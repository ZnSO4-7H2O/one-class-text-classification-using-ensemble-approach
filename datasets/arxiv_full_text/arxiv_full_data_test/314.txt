{"title": "Balanced Excitation and Inhibition are Required for High-Capacity,  Noise-Robust Neuronal Selectivity", "tag": "q-bio", "abstract": " Neurons and networks in the cerebral cortex must operate reliably despite multiple sources of noise. To evaluate the impact of both input and output noise, we determine the robustness of single-neuron stimulus selective responses, as well as the robustness of attractor states of networks of neurons performing memory tasks. We find that robustness to output noise requires synaptic connections to be in a balanced regime in which excitation and inhibition are strong and largely cancel each other. We evaluate the conditions required for this regime to exist and determine the properties of networks operating within it. A plausible synaptic plasticity rule for learning that balances weight configurations is presented. Our theory predicts an optimal ratio of the number of excitatory and inhibitory synapses for maximizing the encoding capacity of balanced networks for a given statistics of afferent activations. Previous work has shown that balanced networks amplify spatio-temporal variability and account for observed asynchronous irregular states. Here we present a novel type of balanced network that amplifies small changes in the impinging signals, and emerges automatically from learning to perform neuronal and network functions robustly. ", "text": "neurons networks cerebral cortex must operate reliably despite multiple sources noise. evaluate impact input output noise determine robustness single-neuron stimulus selective responses well robustness attractor states networks neurons performing memory tasks. robustness output noise requires synaptic connections balanced regime excitation inhibition strong largely cancel other. evaluate conditions required regime exist determine properties networks operating within plausible synaptic plasticity rule learning balances weight conﬁgurations presented. theory predicts optimal ratio number excitatory inhibitory synapses maximizing encoding capacity balanced networks given statistics aﬀerent activations. previous work shown balanced networks amplify spatio-temporal variability account observed asynchronous irregular states. present novel type balanced network ampliﬁes small changes impinging signals emerges automatically learning perform neuronal network functions robustly. response properties neurons many brain areas including cerebral cortex shaped balance co-activated inhibitory excitatory synaptic inputs excitation-inhibition balance diﬀerent forms diﬀerent brain areas species emergence likely arise multiple mechanisms. theoretical work shown that externally driven circuits recurrently connected excitatory inhibitory neurons strong synapses settle rapidly state population activity levels ensure balance excitatory inhibitory currents experimental evidence systems indicates synaptic plasticity plays role maintaining balance address question computational beneﬁts conferred excitation-inhibition balance properties balanced unbalanced neuronal circuits. although shown networks balanced states advantages generating fast linear response changing stimuli advantages disadvantages excitation-inhibition balance general information processing elucidated compare computational properties neurons operating without excitation-inhibition balance present constructive computational reason strong balanced excitation inhibition needed neurons generate selective responses robust output noise crucial stability memory states associative memory networks. novel balanced networks present naturally automatically emerge synaptic learning endows neurons networks robust functionality. begin analysis considering single neuron receiving input large number aﬀerents. characterize basic task discriminating patterns input activation respond ﬁring action potentials patterns leave quiescent. neurons implement form response selectivity applying threshold inputs presynaptic aﬀerents. simplest model captures basic elements binary model neuron studied extensively used model variety neuronal circuits work based including analyzing implications four fundamental neuronal features previously considered together non-negative input corresponding fact neuronal activity characterized ﬁring rates; membrane potential threshold neuronal ﬁring resting potential sign-constrained bounded synaptic weights meaning individual synapses either excitatory inhibitory total synaptic strength limited; sources noise input output noise representing ﬂuctuations arising variable stimuli inputs processes within neuron. shown features imply that number input aﬀerents large synaptic input must strong balanced neuron’s response selectivity robust. extend analysis recurrently connected networks storing long-term memory similar balanced synaptic patterns required stability memory states noise. addition maximizing performance neurons networks balanced state yields prediction optimal ratio excitatory inhibitory inputs cortical circuits. model neuron binary unit either active quiescent depending whether membrane potential ﬁring threshold. potential labeled vpsp weighted inputs represent aﬀerent ﬁring rates thus non-negative vrest resting potential neuron n-component vectors elements respectively. weight represents synaptic eﬃcacy i’th input. vpsp neuron active state otherwise quiescent state. implement segregation excitatory inhibitory inputs weight constrained input excitatory input inhibitory. function properly circuit neuron must respond selectively appropriate inputs. characterize selectivity deﬁne exemplar input vectors randomly assign classes denoted ‘plus’ ‘minus’. neuron must respond inputs belonging ‘plus’ class ﬁring ‘minus’ class remaining quiescent. means neuron acting perceptron assume input activations drawn i.i.d. distribution non-negative means covariance matrix simplicity assume stimulus average activities input neurons within population ¯xexc diagonal equal variances within population exc. note synaptic weights units membrane potential input activity levels hence measured units /σexc. call weight vectors correctly categorize exemplar input patterns solutions categorization task presented neuron. describing detail properties solutions outline broad distinction types possible solutions. type characterized weak synapses i.e. individual synaptic weights inversely proportional total number synaptic inputs enable neuron cross threshold). solution type total excitatory inhibitory parts membrane potential order neuron’s threshold. alternative scenario solution individual synaptic weights relatively strong case total excitatory inhibitory parts potential individually much greater threshold make approximately equal contributions excitation inhibition tend cancel mean vpsp close threshold. call ﬁrst type solution unbalanced second balanced. note norm weight vector serves distinguish types solutions. norm order unbalanced solutions order balanced case. weights norms stronger lead membrane potential values much larger magnitude neuron’s threshold. biological neurons postsynaptic potentials magnitude result high unreasonable ﬁring rates therefore impose upper bound weight norm mentioned above neurons central nervous system subject multiple sources noise performance must robust eﬀects. distinguish biologically relevant types noise input noise resulting ﬂuctuations stimuli sensory processes generate stimulus related input output noise arising aﬀerents unrelated particular task biophysical processes internal neuron including ﬂuctuations eﬀective threshold spiking history adaptation sources noise result trail-by-trial ﬂuctuations membrane potential vpsp robust solution probability changing state output neuron relative noise-free condition must low. sources noise diﬀer dependence magnitude synaptic weights. input noise ﬁltered synaptic weights signal eﬀect membrane potential sensitive magnitude weights. speciﬁcally trial-to-trial variability input characterized standard deviation ﬂuctuations generates membrane potential standard deviation |w|σin hand eﬀect output noise independent synaptic weights output noise characterized standard deviation σout induces membrane potential ﬂuctuations standard deviation σout types solutions appreciate basis diﬀerence noise robustness types solutions. unbalanced solutions diﬀerence potential induced typical although ﬂuctuations induced input noise order output noise yields ﬂuctuations membrane potential order much larger magnitude weak signal contrast balanced solutions signal diﬀerentiating ‘plus’ ‘minus’ patterns order order ﬂuctuations induced types noise thus important observation balanced solution provides hope producing selectivity robust types noise. however guaranty robust balanced solutions exist found maintained manner implemented biological system. question therefore conditions balanced solution selectivity task exist detail robustness properties. below derive conditions existence balanced solution analyze properties study implications single-neuron network computation. show that subject small reduction total information stored network robust balanced solutions exist emerge naturally learning occurs presence output noise. begin presenting results analytic approach determining existence conditions analyzing properties weights generate speciﬁed selectivity independent particular method learning algorithm used weights figure balanced solutions robust input output noise. panel depicts membrane potentials resulting diﬀerent input patterns classiﬁcation task. left column) balanced right column). neuron active state membrane potential greater threshold vth. input pattern class speciﬁed squares underneath horizontal axis. input pattern determines membrane potential ﬂuctuates presentation another input noise output noise vertical bars depict magnitude noise case. variability mean vpsp across input patterns proportional |w|. result mean vpsp’s unbalanced solutions cluster close threshold balanced solutions mean vpsp’s larger spread input noise produces membrane potential unbalanced solutions balanced solutions output noise produces membrane potential ﬂuctuation independent magnitude solution types. thus balanced unbalanced solutions robust input noise balanced solutions also robust substantial output noise. number patterns large solutions exist. maximal value permits solutions proportional number synapses useful measure ratio call load. capacity denoted maximal load permits solutions task. capacity depends relative number ‘plus’ ‘minus’ input patterns. simplicity assume throughout classes equal size classic result perceptron weights sign constrained capacity ‘constrained perceptron’ considered here depends also fraction excitatory aﬀerents denoted fexc. fraction important architectural feature neuronal circuits varies diﬀerent brain systems. fexc namely purely inhibitory circuit capacity vanishes input neuron inhibitory vpsp cannot reach threshold neuron quiescent stimuli. circuit includes excitatory synapses task solved appropriate shaping strength excitatory inhibitory synapses ability increases larger fraction excitatory synapses therefore fexc increases fexc maximum fractions equal greater critical fraction fexc exc. dependence summarized capacity curve bounding range loads admit solutions diﬀerent excitatory/inhibitory ratios. depends statistics inputs denote coeﬃinterestingly cient variation excitatory inhibitory input activities cvexc σexc/¯xexc cvinh σinh/¯xinh respectively. these measure degree stimulus tuning aﬀerent populations. terms quantities critical excitatory fraction words critical ratio number excitatory inhibitory aﬀerents exc)) equals ratio degree tuning. understand origin result note maximize encoding capacity relative strength weights inversely proportional standard deviation aﬀerents ¯wexc /σexc implying mean total synaptic inputs proportional fexc ¯wexc ¯xexc finh ¯winh ¯xinh fexc/cvexc finh/cvinh finh fexc. excitatory fraction fexc mean total synaptic inputs positive allowing voltage reach threshold neuron implement required selectivity task optimally scaled weights. thus capacity fexc excitatory neuron unaﬀected changes fexc range fraction fexc neuron cannot remain responsive optimally scaled weights thus capacity reduced. cortical circuits inhibitory neurons tend higher ﬁring rates thought broadly tuned excitatory neurons implying consistent abundance excitatory synapses cortex. however input statistics make change qualitative behavior discuss figure balanced unbalanced solutions. perceptron solutions function load fraction excitatory weights. capacity line black line) solution exists. balanced solutions exist balanced capacity line dashed gray line). balanced capacity maximum capacity lines unbalanced solutions exist hand balanced capacity line unbalanced solutions coexist balanced ones norm synaptic weight vector typical solutions function load /σexc). norm clipped upper bound norm collapses order input imbalance index typical solutions function load. note sharp onset imbalance fexc yielding methods parameters used. simulation results fig. describe properties diﬀerent solutions. particular investigate parameter regimes balanced unbalanced solutions exist. unbalanced solutions weights vector norm order exist load values balanced solutions weight vector norms order exist critical value smaller speciﬁcally fexc balanced solutions exist load values capacity i.e. fexc smaller decreases fexc vanishes fexc absence balanced solutions fexc clear inhibition balance excitatory inputs. furthermore synaptic excitatory weights must weak ensure vpsp remains close threshold fexc predominance excitatory aﬀerents precludes balanced solution load high i.e. argued shown below balanced solution robust unbalanced solution. hence identify optimal fraction excitatory input fraction excitatory aﬀerents capacity balanced solutions maximal. loads balanced unbalanced solutions exist raising question would character weight vector sampled randomly space possible solutions. theory predicts whenever balance solution exists vast majority solutions balanced furthermore weight vector norm saturated typical solution undergoes transition upper bound thus fexc balanced unbalanced weights crosses balanced capacity line point norm solution collapses explained above balanced solutions expect near cancellation total excitatory inhibitory inputs. theory conﬁrms expectation. measure degree cancellation solution introduce imbalance index symbol denotes average input patterns whereas unbalanced solution order balanced solution small order thus typical solution zero imbalance imbalance increases sharply increases beyond characterize eﬀect noise diﬀerent solutions introduce measures inputrobustness output robustness κout characterize robustness noise-free solutions addition types noise. ensure robustness output noise noise-free membrane potential closest threshold must suﬃciently thus deﬁne minimum taken input patterns task threshold /σexc). second measure characterizes robustness input noise must take account fact ﬂuctuations membrane potential induced form noise scale size synaptic weights. hence κout/|w| eﬃcient algorithms ﬁnding solution maximum studied extensively developed novel eﬃcient algorithm ﬁnding solutions maximum κout possible values input output robustness unbalanced balanced solutions. theory predicts majority balanced unbalanced solutions vanishingly small values κout thus sensitive noise. however given load robust solutions exist spectrum robustness values maximal values κmax since magnitude sensitive hence scales signal noise inputs κmax proportional unbalanced balanced solutions. hand κmax |w|. thus expect κmax unbalanced solutions exist. addition expect increasing load number constraints need satisﬁed reduce value κmax synaptic weights increases. load. expected fig. present values κmax reach zero load approaches capacity however κmax substantial proportional balanced solutions exist contrast remains order full capacity properties κmax ‘optimal’ solutions achieve maximal robustness either input output noise? solutions achieve maximal output robustness κmax balanced norm saturates upper bound interestingly wide range input parameters solutions achieve maximal input robustness κmax unbalanced solutions nevertheless critical balance load values balanced maximal κout solutions order indeed close κmax fact balanced solution maximal κout also posses maximal value possible balanced solutions. conclude solutions robust input output noise exist loads less smaller however long fexc close fexc reduction capacity imposed requirement robustness small. neurons typically receive input communicate output action potentials. thus fundamental question introduction spike-based input spiking output aﬀect results. show main properties balanced unbalanced synaptic eﬃcacies discussed above remain inputs spike trains model neuron implements spiking membrane potential reset mechanisms. figure maximal values input output robustness. maximal value κout load solutions exist maximal κout line output robustness order balanced solutions exist. maximal value κout κmax loads range unbalanced solution exist maximal κout values scale load parameters used solutions σexc). solutions exist maximal line κmax attained solutions maximize κout panels theory numerical results depicted black gray lines gray dots respectively. error-bars depict standard error mean. methods parameters used. simulation results fig. consider leaky integrate-and-ﬁre neuron required perform binary classiﬁcation task considered using perceptron. input characterized vector ﬁring rates aﬀerent generates poisson spike train interval time mean rate neuron integrates input spikes emits output spike whenever membrane potential crosses ﬁring threshold. output spike membrane potential reset resting potential integration inputs continues. deﬁne output state neuron using total number output spikes nspikes neuron quiescent nspikes nthr active nspikes nthr nthr chosen maximize classiﬁcation performance. discuss properties learning neurons instead test properties solutions obtained perceptron model used neuron. particular compare performance balanced maximal κout solution unbalanced maximal solution. synaptic weight neuron according perceptron solutions mean output neuron correctly classiﬁes input patterns consistently results perceptron output noise performance solutions good even presence substantial input noise caused poisson ﬂuctuations number input spikes timings output noise magnitude increased however performance unbalanced maximal solution quickly deteriorates whereas performance balanced maximal κout solution remains largely unaﬀected thus spiking model recapitulates general results found perceptron. thus considered selectivity single neuron results also important implications recurrently connected neuronal networks particular recurrent networks implementing associative memory functions. models associative memory stable ﬁxed points network dynamics represent memories memory retrieval corresponds dynamic transformation initial state memory-representing ﬁxed points major focus memory research many years network function associative memory memory states must large basins attraction network perform pattern completion recalling memory initial state similar identical addition memory retrieval must robust output noise. show variables κout synaptic weights projecting onto individual neurons network closely related sizes basins attraction memories robustness output noise respectively. consider network consists fexc excitatory inhibitory recurrently connected binary neurons. network operates discrete time steps step state randomly chosen neuron updated according figure selectivity spiking model. panels depict output neuron output noise balanced maximal κout solution unbalanced maximal solution panels depict curves solutions output noise high output noise conditions obtained decision threshold modiﬁed consistently results perceptron performance solutions output noise similar slight advantage maximal solution. higher levels output noise performance unbalanced maximal solution quickly deteriorates whereas performance balanced maximal κout solution slightly aﬀected. balanced solution chosen equalize mean output spike count across patterns solutions methods parameters used. deviation σout. randomly chosen binary activity patterns {sµ} representing stored memories encoded recurrent synaptic matrix ﬁxed points network dynamics. achieved treating neuron perceptron weight vector {jij}j=i maps inputs neurons desired output memory state creates attractor network memory states stable ﬁxed points dynamics noise-free condition capacity memory network deﬁned maximal load memory patterns stable ﬁxed points dynamics. capacity single neuron perceptron depends statistics desired output since statistic diﬀerent excitatory inhibitory populations single neuron capacity populations vary hence global capacity recurrent network minimum single-neuron capacities neuron types. long smaller critical capacity recurrent weight matrix exists memory states stable ﬁxed points noiseless dynamics. however solutions unique choice particular matrix endow network diﬀerent robustness properties. stated above properly function associative memory ﬁxed points large basins attraction. corruption initial state away parent memory pattern introduces variability inputs neuron subsequent dynamic iterations hence equivalent injecting input noise single-neuron feedforward case. therefore large basin attraction achieved matrix yields large input noise robustness neuron ﬁxed points requirement memory states retrieval robust output noise satisﬁed yields large output noise robustness neuron ﬁxed points. therefore consider types recurrent connections weight vector maximizes hence chosen parameter regime necessarily unbalanced; second rows connection matrix correspond balanced solutions maximize κout. estimate basins attraction memory patterns numerically initializing network states corrupted versions memory states observing network σout converges parent memory state diverges away deﬁne size basin attraction maximum distortion initial state assures convergence parent memory high probability. comparing basins attraction types networks mean basin attraction unbalanced network moderately larger balanced consistent slightly lower value balanced case hand behavior networks strikingly diﬀerent presence output noise. illustrate this start network memory state determine stable despite noise dynamics estimate output noise tolerance network measuring maximal value σout memory states stable memory states balanced solution maximal κout stable noise levels order magnitude larger unbalanced network maximal figure recurrent associative memory network constructed using single neuron feedforward learning. fully connected recurrent network excitatory inhibitory neurons particular memory state. active neurons shown black excitatory inhibitory synaptic connections shown yellow blue respectively lines symbolize axons synapses shown small circles. appropriate postsynaptic weights neuron using memory-state activities neurons input memory state desired output. example neuron implement desire memory state modiﬁcation weights show fraction erroneous neurons network function time. network dynamics σout initial state network either converge memory state diverge network states probability converging memory state steps initial pattern distortion network unbalanced maximal weights network balanced maximal κout weights network balanced maximal κout weights unlearned inhibition network dynamics σout network initialized memory state. dynamics stable unstable probability stable dynamics least time steps networks initialized memory state presence output noise σout. colors maximal output noise magnitude load networks balanced synaptic weights matrix maximizing κout. similarly κout maximal output noise magnitude order even though solutions exists extremely sensitive output noise. results shown fexc fexc methods parameters used. finally noise robustness memory states balanced network depends number memories. shown fig. ﬁxed level load capacity memory patterns stable long levels noise remain threshold value denote σmax stability memory states rapidly deteriorates. critical noise function σmax decreases smoothly large value small zero level load load coincides maximal load excitatory inhibitory neurons balanced solution loads solutions unbalanced hence magnitude stochastic dynamical component order associative memory network model assumed excitatory inhibitory neurons code desired memory states network connections modiﬁed learning. previous models associative memory separate excitation inhibition assume memory patterns restricted excitatory population whereas inhibition provides stabilizing inputs address emergence balanced solution scenarios inhibitory neurons represent long-term memories studied architecture connections random sparse matrices large amplitudes resulting inhibitory activity patterns driven excitatory memory states. conditions inhibitory subnetwork exhibits irregular asynchronous activity overall mean activity proportional mean activity driving excitatory population although mean inhibitory feedback provided excitatory neurons balance mean excitation variability feedback injects substantial noise onto excitatory neurons degrades system performance variability stems diﬀerences inhibitory activity patterns generated diﬀerent excitatory memory states additional noise caused temporal irregular activity chaotic inhibitory dynamics. next whether system’s performance improved plasticity connections experimental evidence exist indeed appropriate plasticity rule pathway suppresses spatio-temporal ﬂuctuations inhibitory feedback yielding balanced state behaves similarly fully learned networks described interestingly case basins attraction balanced network comparable even larger basins unbalanced fully learned network despite fact explicit memory patterns assigned inhibitory populations inhibitory activity plays computational role goes beyond providing global inhibitory feedback; weights connections shuﬄed network’s performance signiﬁcantly degrades thus presented analytical numerical investigations solutions support selectivity associative memory provide substantial robustness noise. however address robust solutions could learned biological system. fact stated above majority solutions tasks vanishingly small output input robustness. therefore important question whether noise robust weights emerge naturally synaptic learning rules appropriate neuronal circuits. actual algorithms used learning neural circuits generally unknown especially within supervised learning scenario. experiments suggest learning rules depend brain area post synaptic neuron types reviews regardless particular learning algorithm used however theory suggests simple ensure learning arrives robust solution introduce noise learning. indeed common practice machine learning increasing generalization abilities rationale learning algorithms achieve error presence noise necessarily lead solutions robust noise levels least large present learning. case considering learning presence substantial input noise lead solutions substantial introducing output noise learning lead solutions substantial κout. note large even κout remains small vice versa order well). therefore learning presence signiﬁcant output noise lead solutions robust input output noise whereas learning presence input noise alone lead unbalanced solutions sensitive output noise depending details learning algorithm. therefore predict performing successful learning presence output noise suﬃcient condition emergence excitation-inhibition balance. demonstrate robust balanced solutions emerge presence output noise consider variant perceptron learning algorithm forced sign constraints weights addition added weight decay term implementing soft constraint magnitude weights supervised learning rule possesses several important properties required biological plausibility on-line weights modiﬁed incrementally pattern presentation; history independent weight update depends current pattern error signal; lastly simple local weight updates function error signal quantities available locally synapse learning rule applied train selectivity task presence substantial output noise resulting solution balanced weight vector substantial κout contrast learning occurs weak output noise resulting solution unbalanced small κout large substantial input noise present learning learning rule applied load regime unbalanced solutions exist learning fails achieve reasonable performance applied presence large output noise. noise scaled value allowed learning yields unbalanced solutions robustness values order κmax maximum allowed region figure emergence balance learning presence output noise. panels show outcome perceptron learning noisy neuron under high output noise conditions except σout model learning parameters identical conditions mean training error learning cycle. cycle input patterns learned presented once. error decays plateaus minimal value high output noise conditions mean imbalance index learning cycle. remains order output noise conditions drops close zero high output noise conditions. mean input robustness learning cycle. input robustness high output noise conditions. mean output robustness learning cycle. output robustness substantial high output noise learning condition. results demonstrate robust balanced solutions naturally emerge learning presence high output noise. methods parameters used. results presented come imposing fundamental biological constraints ﬁxed-sign synaptic weights non-negative aﬀerent activities positive ﬁring threshold input output forms noise. amit studied maximal margin solution sign-constrained perceptron showed half capacity unconstrained perceptron. however previous work considered aﬀerent activities centered around zero neuron zero ﬁring threshold features preclude presence novel behavior exhibited biologically constrained model studied here. chapeton studied perceptron learning sign-constrained weights preassigned level robustness considered solutions unbalanced regime which shown extremely sensitive output noise. learning neural circuits involves trade-oﬀ exhausting system’s capacity implementing complex input-output functions hand ensuring good generalization properties other. well-known approach machine learning search solutions training examples maximizing distance samples decision surface strategy known maximizing margin margin maximized case corresponds framework κin. work computational neuroscience implicitly optimized robustness parameter equivalent κout knowledge approaches distinguished shown result solutions dramatically diﬀerent noise sensitivities. particular wide parameter range shown maximizing κout leads balanced solution minimal sensitivity output noise robustness input noise almost good maximal margin solution modest trade-oﬀ capacity. hand maximizing margin often leads unbalanced solutions extreme sensitivity output noise. perceptron long considered model cerebellar learning computation recently brunel investigated capacity robustness perceptron model cerebellar purkinje cell taking weights excitatory. view analysis presented here balanced solutions possible case solutions maximize either input-noise output-noise robustness κout types solutions diﬀer weight distributions experimentally testable consequences predicted circuit structure considered solutions maximize κout). output robustness unbalanced solutions increased making input activity patterns sparse. denoting mean fraction active neurons input maximum output robustness scales κout thus high sparsity input activation cerebellum relative modest sparsity neocortex consistent former dominated excitatory modiﬁable synapses. interestingly results suggests optimal ratio excitatory inhibitory synapses. capacity balanced regime optimal fexc determined coeﬃcients variation excitatory inhibitory inputs thus optimality predicts simple relation fraction excitatory inhibitory inputs degree tuning. estimating cv’s existing data diﬃcult would interesting check input statistics connectivity ratios diﬀerent brain areas consistent prediction. commonly observed value cortex fexc would optimal input statistics cvexc/cvinh general expect cvexc/cvinh implies work assumed inhibitory neurons learn represent speciﬁc sensory long-term memory information excitatory ones synaptic pathways learned using similar learning rules. plasticity excitatory inhibitory pathways observed accumulating experimental evidence indicates high degree cell type synaptic type speciﬁcity plasticity rules. addition synaptic plasticity tight control neuromodulatory systems. present unclear interpret learning rules terms concrete experimentally observed synaptic plasticity. functional models neural learning assume learning within excitatory population inhibition acting global stabilizing force. case sensory processing approach consistent observation similar stimulus tuning epscs ipscs many cortical sensory areas. role inhibitory neurons memory representations less known importantly shown main results valid also case inhibitory neurons explicitly participate coding memories. interestingly work suggests even inhibitory neurons passive observers learning processes learning inhibitory synapses onto excitatory cells amplify memory stability system ﬂuctuations inhibitory feedback. given diversity inhibitory cell types likely real circuits inhibition plays multiple roles including conveying information providing stability. several previous models associative memory incorporated biological constraints sign synapses dale’s assuming variants hebbian plasticity synapses capacity hebbian models relatively poor basins attractions small except extremely sparse activity levels. contrast model applies powerful learning rule that keeping sign constraints synapses exhibits signiﬁcantly superior performance high capacity even moderate sparsity levels large basins attraction high robustness output noise. dynamical systems perspective associative memory networks construct exhibit unusual properties. associative memory network models large basins attractions endow memory state robustness stochasticity dynamics here found that ﬁxed-point memories synaptic weights largest possible basins sensitive even mild levels stochasticity whereas balanced synaptic weights somewhat reduced basins substantially increased output noise robustness. network level single-neuron level imposing basic features neural circuitry positive inputs bounded synapses ﬁxed sign positive ﬁring threshold sources noise force neural circuits balanced regime. recent class models showing computational beneﬁts balanced inputs extremely strong synapses outside range discussed models stabilized instantaneous transmission signals neurons required range synaptic strength consider. previous models balanced networks highlighted ability network strong excitatory inhibitory recurrent synapses settle state total input dynamically balanced without special tuning synaptic strengths. state characterized high degree intrinsically generated spatio-temporal variability mean population activities respond fast linear fashion external inputs. typically networks lack population level nonlinearity required generate multiple attractors. contrast explored capacity balanced network support multiple stable ﬁxedpoints tuning synaptic strengths appropriate learning. despite dynamic functional diﬀerences classes networks balancing excitation inhibition plays similar role both. ﬁrst scenario synaptic balance ampliﬁes small changes spatial temporal properties external drive. similarly present scenario balanced synaptic architecture leads enhanced robustness amplifying small variations synaptic inputs induced changes stimulus memory identity. would interesting combine fast dynamics robust associative memory capabilities. conclusion uncovered fundamental principle neuronal learning basic biological constraints. work reveals excitation-inhibition balance critical computational role producing robust neuronal functionality insensitive output noise. showed balance important single neuron level spiking non-spiking neurons level recurrently connected neural networks. further theory suggests excitation-inhibition balance collective self-maintaining emergent phenomena synaptic plasticity. successful neuronal learning process presence substantial output noise lead strong balanced synaptic eﬃcacies noise robustness features. fundamental nature result suggests apply across variety neuronal circuits learn presence noise. number numerical methods choosing weight vector generates speciﬁed selectivity numerical simulations developed algorithms maximal κout maximal solutions obey imposed biological constraints. solutions found directly solving conic programing optimization problems eﬃcient algorithms exist widely available details methods. memory states networks trained implement memory states speciﬁed stable ﬁxed points noise free dynamics. memory states randomly chosen i.i.d. binary distributions parameter pexc/inh according type i’th input aﬀerent i.e. pexc/inh. initial pattern distortion start network close memory state initial state network randomly chosen according initial pattern distortion level otherwise. procedure ensures mean activity levels excitatory inhibitory neurons initial state mean activity levels memory state random variable zero mean variance |wt| out. error signal deﬁned zero otherwise. pattern presentation synapses updated. synaptic weights excitatory inputs updated according wit+ ρetxit]+ weights inhibitory inputs updated according wit+ ρetxit]− weight decay constant constant learning rate. learning cycle patterns presented sequentially random order numerical experiments fig. fig. fig. fig. excitatory inputs random patterns drawn i.i.d exponential distribution unity mean standard deviation. inhibitory inputs drawn gamma distribution shape parameter scale parameter input spike-trains input pattern input spike trains input aﬀerent drawn randomly poisson processes rate duration synaptic input given input spike trains {ti} contribution synaptic eﬃcacy synapse i’th input aﬀerent post synaptic potential kernel. given membrane synaptic time constants respectively maximal value output noise output noise added neuron’s randomly drawn zero mean gaussian distribution standard deviation randomly drawn uniform distribution. voltage reset threshold crossing membrane potential reset it’s resting potential. given output spike times {tspike} total contribution voltage reset membrane potential vrest neuron’s resting threshold potential respectively implements post-spike voltage reset. form ensures voltage given reset resting potential immediately output spike. membrane potential finally neuron’s membrane potential given vrest +vsyn +vo.n. +vreset vreset computed given vsyn vo.n.. σinh/σexc cvexc/cvinh even split responsive/unresponsive labels. fexc fig. panels fexc even split responsive/unresponsive labels. numerical results averaged samples. fig. panels fraction ‘plus’ patterns pout fexc vrest vthr msec msec msec random patterns drawn described maximal κout solutions found units /σexc. output noise added panels panels output noise added nnoise nnoise fig. panels fexc results averaged networks patterns network. methods parameters inhibitory connectivity learned inhibition networks panel maximal output noise magnitude deﬁned value σout stable pattern probability minimize ﬁnite size eﬀects simulations used pattern probability load noise level estimated averaging networks patterns network. fig. random patterns binary pattern equal probabilities even split ‘plus’ ‘minus’ patterns. learning algorithm parameters results averaged samples. acknowledge contribution scientiﬁc work cite open source projects directly crucially contributed current work. computational aspects work done using python stack scientiﬁc computing jupyter/ipython others). convex conic optimization performed using cvxopt parallelization simulations cluster computer performed using ipyparallel. thank misha tsodyks helpful discussions. research supported grant gatsby charitable foundation gatsby initiative brain circuitry columbia university gatsby program theoretical neuroscience hebrew university simons foundation swartz foundation kavli institute brain science columbia university", "year": "2017"}