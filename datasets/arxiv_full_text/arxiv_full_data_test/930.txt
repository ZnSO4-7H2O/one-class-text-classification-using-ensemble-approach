{"title": "Preserved Structure Across Vector Space Representations", "tag": "q-bio", "abstract": " Certain concepts, words, and images are intuitively more similar than others (dog vs. cat, dog vs. spoon), though quantifying such similarity is notoriously difficult. Indeed, this kind of computation is likely a critical part of learning the category boundaries for words within a given language. Here, we use a set of 27 items (e.g. 'dog') that are highly common in infants' input, and use both image- and word-based algorithms to independently compute similarity among them. We find three key results. First, the pairwise item similarities derived within image-space and word-space are correlated, suggesting preserved structure among these extremely different representational formats. Second, the closest 'neighbors' for each item, within each space, showed significant overlap (e.g. both found 'egg' as a neighbor of 'apple'). Third, items with the most overlapping neighbors are later-learned by infants and toddlers. We conclude that this approach, which does not rely on human ratings of similarity, may nevertheless reflect stable within-class structure across these two spaces. We speculate that such invariance might aid lexical acquisition, by serving as an informative marker of category boundaries. ", "text": "certain concepts words images intuitively similar others though quantifying similarity notoriously difﬁcult. indeed kind computation likely critical part learning category boundaries words within given language. here items highly common infants’ input imageword-based algorithms independently compute similarity among them. three results. first pairwise item similarities derived within image-space word-space correlated suggesting preserved structure among extremely different representational formats. second closest ‘neighbors’ item within space showed signiﬁcant overlap third items overlapping neighbors later-learned infants toddlers. conclude approach rely human ratings similarity nevertheless reﬂect stable within-class structure across spaces. speculate invariance might lexical acquisition serving informative marker category boundaries. keywords vector space models; semantic similarity; word learning infants presented challenge carve world distinct lexical entities process learning ﬁrst language. they’re provided little supervision mapping territory william james famously dubbed great blooming buzzing confusion. determine aspects world attend service goal area ongoing research debate relatedly features objects environments varyingly informative regards object segmentation category structure. researchers suggested categorization along fundamentally perceptual grounds later development conceptual knowledge incorporated nascent perceptual categories others suggest fact distinct processes work perceptual categories computed automatically sensory systems conceptual categories independently formed conscious action tr¨auble pauen provide evidence functional information inﬂuencing early category judgements. gelman markman explicitly sources category cues preschoolers override perceptual overlap reasoning functional similarity natural kinds. important open question. model hopes explain mechanics human categorization must address potentially disparate information-sources interface mental representations degree interact. indeed evidence human learners suggests integrate perceptual linguistic information categorization learning take deliberately different approach. separate computations images words compare overlap similarity among items systems deduce. using highly familiar common words concepts large infant corpus compare output image-based similarity analysis word co-occurrence similarity analysis items. algorithms learn feature representations without hand engineering purely byproduct separate training objectives comparing representations algorithms learn provides window structure visual semantic forms. terminology area research challenging. delineating differences words concepts categories abstract processes underlie identifying understanding comparing particular instances trivial. present purposes stick concrete ‘basic level’ nouns early-acquired since underlying question concerns words learned. assume nouns refer concepts categorical boundaries acknowledging multiple nouns refer given concept different concepts called mind given word. assume speciﬁc instances words speciﬁc referents concept word picks used learn word’s meaning concept’s category boundaries. term ‘item’ refer words/concepts examine. intuitively word similarity image similarity likely overlap degree since describe underlying entity. explore whether similarity spaces generated disparate algorithms give rise similar similarities among high-frequency items. supports notion underlying invariance across representational formats capturable models. examine whether neighboring items picked within spaces. might imagine properties render images similar words similar different enough overlap minimal; contrast high overlap would suggest true invariance being captured wordimage-tokens. finally examine whether neighbors within wordimage-space inﬂuences early learning. given similarity makes word-learning category-learning difﬁcult hypothesize items neighbors later-learned items analyze high-frequency items infants’ early visual linguistic input aggregated part seedlings project including longitudinal audio video data infants’ home environments months brieﬂy describe larger study relay items chosen. larger study infants tested every month common nouns using looking-while-listening eyetracking design images shown screen named. words experiments chosen dint high frequency well known across infants samples e.g. brent corpus wordbank concrete nouns heard infant’s home recordings preceding months. images displayed words tested chosen library prototypical images images infants’ items seen home videos enter current analysis images occur times image library high frequency concrete nouns derived eyetracking sessions words heard extremely often daylong audio-recordings hour-long video recordings infants thus words images used provide ecologically-valid item-set present modeling purposes. images items used derive average category image-vectors pixel photos single object gray background. items correspond words found wordbank compilation macarthurbates communicative development inventories used proxy acquisition vector representations generate sets vector representations earlylearned items. ﬁrst taken pretrained glove representations modern distributional semantic vector space model. second taken ﬁnal layer activations pretrained image recognition model google’s inception representations generally referred embeddings. objects medium metric space distances points computed function similarity measures. code used generating vectors subsequent analysis found github. word vectors word vectors based glove’s instantiation distributional hypothesis co-occurring words share similar meaning thus capturing covariance tokens large corpora capture aspects semantic structure. vectors pretrained glove authors common crawl corpus billion tokens resulting dimensional vectors million unique words. vectors shown promise modeling early semantic networks thus word vector space items represented -dimensional vector word assigned unique point common vector space. image vectors image embeddings taken ﬁnal layer activations whose objective function tunes network parameters service object recognition computing loss reference labeled training images. tuned parameters determine value vectors transforming input image signal passes network. ﬁnal layer network encodes abstract integrated visual features serving basis classiﬁcation different classes. model trained ilsvrc--cls challenge dataset deﬁned imagenet category subset unlike word vectors different images containing type item varying vector representations passing layers neural network. presents problem comparing forms representation. thus ﬁrst deﬁne prototypical image vector given category object generate -dimensional representation items image vector space given images containing objects belonging single category deﬁne prototypical vector generalized median within representational space vector minimal distances members vectors space products images passed neural network argminx∈u ∑y∈u deﬁne cosine distance measure https//nlp.stanford.edu/projects/glove/ while corpus best-suited goal modelling ’how words behave’ writ large also conducted analyses vectors trained north american english childes corpora smaller. observe qualitative patterns. next examined degree words shared overlapping ‘neighbors’ vector spaces deﬁned neighbor ﬁrst determining mean similarity distance item items. items whose distance target z-score less considered neighbor. within word-space items average neighbors within image-space items neighbors next tested whether spaces picked overlapping neighbors overlap signiﬁcantly greater complements correlational analysis showing distances given pair tend similar values imageword-space similar words/images also consistent across spaces. degree convergence across image word spaces expected given different manifestations underlying concept/word/item next queried whether invariance related learnability. hypothesized words overlapping neighbors would harder children learn since visual linguistic spaces occur ‘cluttered.’ test this looked relative rates acquisition items wordbank using children’s data english. since clear predictions speciﬁc ages tradeoffs comprehension production used both. i.e. used comprehension norms production norms found number overlapping neighbors given word negatively correlated proportion children reported understand produce word figure words overlapping neighbors later-learned words fewer overlapping neighbors. test whether speciﬁc overlap examined number image-only word-only neighbors found correlations word knowledge like euclidean distance less susceptible differences norm inﬂuencing measure similarity. thus principle cosine similarity corrects frequency effects inherent training data. validate image vectors benchmarked classiﬁcation accuracy ﬁnding inception indeed learning useful representations highly childcentric images model’s prediction correct item-class time; cases even incorrect predictions tended reﬂect idiosyncrasies child-relevant items e.g. guess cartoon puppy teddy. comparing spaces computed sets vectors compare pairwise distances items within single space across two. comparing across spaces correlation pairwise distances implies inter-object distances conserved. example close together word space mutually apart chair table space maintaining relationship pairwise distances vector space means global inter-object structure preserved across mapping. despite strikingly different spaces terms dimensionality virtue using completely different algorithms inputs establish vector representations items. absolute locations might transformed correlation would measure degree invariance positioning relative other. test whether imageword-based similarity converged conducted several analyses. first tested whether pairwise cosine distances items word-space correlated pairwise distances image-space pairs distances. simplicity report pearson’s plot linear fig. non-parametric correlations reveal pattern. analyses using randomly generated vectors identical dimensionality showed preserved structure across spaces signiﬁcantly smaller overlap ratio critically correlations learning. results revealed notable correspondence representations learned different algorithms operating inputs fundamentally different encodings relative distances among common early-learned items correlated across wordimage-space even itemlevel closest words spaces overlap well. moreover words corresponding items neighbors reportedly less well known young children. notably common ground representations real life concepts model. particularly noteworthy given dimensionality feature spaces algorithms placed pressure homologous representations. said suggest algorithms learn representations children though note architectures originally inspired primary visual cortex glove recently used decode semantic representations brain activity clear none corpora used train models meant represent given child’s input representations. meant study structure inherent learning data namely degree visual linguistic information purely separable. table neighbors imageword-space. overlapping neighbors bold red; italicised words imageneighbors only underlined words word-neighbors only. overlap ratio reﬂects shared total neighbors. notion make inferences aspect object given another aspect surprising controversial. rather considering multiple dimensions real learners must show even experiences parcelled separately visual linguistic spaces ‘similarity’ conserved degree. said limitation current work image vectors trained images context; real life images occur informative context extending approach decontextualized images using unsupervised algorithms would provide cleaner demonstration ‘purely’ visual similarity; save future work. relatedly used common infant-oriented items wider item-space would fruitfully extend research. metrics learning algorithm indeed human establish gradations likeness? necessarily metrics form basis category boundaries? fundamental questions ﬁeld current results sufﬁcient support speciﬁc mechanism suggests special role invariance given unifying thread algorithms inputs common objects represent. underneath diversity visual statistics token distributions stable entities world give rise regularity across measurements different vantage points idea dating back helmholtz recent ﬁndings examining mechanics generalization dnns lend modern information theoretic support notion said many things together’ visually similar rather hierarchical functional associative relations leave future work. principle would expect words greater invariance across different representational dimensions would learned earlier since representations likely easier make categorical inferences over. indeed many words lack visual correlates e.g. grammatical markers unobservables words less consistent visual features generally learned later contrast helpful scenario where example visibly round objects occur ‘roll’; indeed correlated perceptual linguistic cues child learner wordbank-based analysis speaks this highlighting concrete nouns space cluttered along visual linguistic dimensions learning slower. notably effect wordimageneighbors suggesting overlap-neighbor effect indeed clutter across spaces. account keeping research ﬁnds displays semantically-similar items word comprehension reduced similar feature-space cluttering effects standard visual search literature target-to-distractor similarity reduces speed accuracy search interestingly learning structure objects adults learning chinese characters perform better visual search trained characters force attention points conﬂuence features perhaps infants same. said concrete nouns appropriate target current analyses given demonstrably early acquisition work abstract words clear next step. results provide in-principle proof vector space models words images fruitfully combined linked early language concept learning. approach could readily extended examine learningrelevant properties like animacy shape color landau smith jones along lines exploratory analyses items splitting item-pairs animate inanimate mixed categories suggests imageword-based correlation particularly strong inanimate items though items conclusions as-yet unwarranted. however imagine larger database children’s visual linguistic experiences tests overlapping similarity relative degrees within-class structure conservation provide informative leverage predicting acquisition across early vocabulary. evidence links visual linguistic features learned distinct machine learning algorithms operate drastically different inputs trained service unrelated ends. links suggest conserved structure separable information sources indeed seems algorithms converge items ‘closer’ similarity within group oft-heard seen concrete nouns children sensitive overlapping cross-word relationships well. process created wordimage-spaces examine certainly meant cognitively plausible. nevertheless results suggest vector-space approach tied language acquisition provides promising avenues uncovering cross-representational inﬂuences early word concept learning.", "year": "2018"}