{"title": "Spatio-Temporal Backpropagation for Training High-performance Spiking  Neural Networks", "tag": "q-bio", "abstract": " Compared with artificial neural networks (ANNs), spiking neural networks (SNNs) are promising to explore the brain-like behaviors since the spikes could encode more spatio-temporal information. Although pre-training from ANN or direct training based on backpropagation (BP) makes the supervised training of SNNs possible, these methods only exploit the networks' spatial domain information which leads to the performance bottleneck and requires many complicated training skills. Another fundamental issue is that the spike activity is naturally non-differentiable which causes great difficulties in training SNNs. To this end, we build an iterative LIF model that is more friendly for gradient descent training. By simultaneously considering the layer-by-layer spatial domain (SD) and the timing-dependent temporal domain (TD) in the training phase, as well as an approximated derivative for the spike activity, we propose a spatio-temporal backpropagation (STBP) training framework without using any complicated technology. We achieve the best performance of multi-layered perceptron (MLP) compared with existing state-of-the-art algorithms over the static MNIST and the dynamic N-MNIST dataset as well as a custom object detection dataset. This work provides a new perspective to explore the high-performance SNNs for future brain-like computing paradigm with rich spatio-temporal dynamics. ", "text": "weight modiﬁcation spike timing dependent plasticity considers local neuronal activities achieve high performance. second ﬁrstly trains transforms version network structure spiking rate neurons acts analog activity neurons bio-plausible explore learning nature snns. promising method obtain high-performance training recent direct supervised learning based gradient descent theory error backpropagation. however method considers layer-by-layer spatial domain ignores dynamics temporal domain therefore many complicated training skills required improve performance ﬁxed-amount-proportional reset lateral inhibition error normalization weight/threshold regularization etc. thus general dynamic model learning framework snns highly required. paper propose direct supervised learning framework snns combines training phase. firstly build iterative model snns dynamics friendly gradient descent training. consider spatial direction temporal direction error backpropagation procedure spatio-temporal backpropagation signiﬁcantly improves network accuracy. furthermore introduce approximated derivative address non-differentiable issue spike activity. test snns framework using fully connected convolution architecture static mnist custom object detection dataset well dynamic n-mnist. many complicated training skills generally required existing schemes avoided fact proposed method make full information captures nature snns. experimental results show proposed method could achieve best accuracy either static dynamic dataset compared existing state-of-the-art algorithms. inﬂuence dynamics different methods derivative approximation systematically analyzed. work shall open explore high-performance snns future brain-like computing paradigms rich dynamics. abstract—compared artiﬁcial neural networks spiking neural networks promising explore brain-like behaviors since spikes could encode spatiotemporal information. although existing schemes including pretraining direct training based backpropagation make supervised training snns possible methods exploit networks’ spatial domain information leads performance bottleneck requires many complicated training techniques. another fundamental issue spike activity naturally non-differentiable causes great difﬁculties training snns. build iterative model friendlier gradient descent training. simultaneously considering layer-by-layer spatial domain timing-dependent temporal domain training phase well approximated derivative spike activity propose spatio-temporal backpropagation training framework without using complicated skill. design corresponding fully connected convolution architecture evaluate framework static mnist custom object detection dataset well dynamic n-mnist. results show approach achieves best accuracy compared existing state-of-the-art algorithms spiking networks. work provides perspective explore high-performance snns future brain-like computing paradigm rich spatio-temporal dynamics. deep neural networks achieved outstanding performance diverse areas seems brain uses another network architecture spiking neural networks realize various complicated cognitive functions compared existing dnns snns mainly superiorities spike pattern ﬂowing snns fundamentally codes spatio-temporal information dnns lack timing dynamics especially widely used feedforward dnns; event-driven paradigm snns make hardware friendly adopted many neuromorphic platforms however remains challenging training snns quite complicated dynamics non-differentiable nature spike activity. summary exist three kinds training methods snns unsupervised learning; indirect supervised learning; direct supervised learning. ﬁrst origins biological synaptic plasticity fig. illustration spatio-temporal characteristic snns. besides layer-by-layer spatial dataﬂow like anns snns famous rich temporal dynamics non-volatile potential integration. however existing training algorithms consider either spatial domain supervised ones backpropagation temporal domain unsupervised ones timing-based plasticity causes performance bottleneck. therefore build learning framework making full spatio-temporal domain fundamentally required high-performance snns forms main motivation work. information facts snns also deep architectures like dnns neuron neuronal dynamic properties. former grants snns rich spatial domain information later offers snns power encoding temporal domain information. however currently uniﬁed framework allows effective training snns implementing backpropagation dnns considering spatio-temporal dynamics. challenged extensive snns various applications. work present framework based iterative leaky integrate-and-ﬁre model enables apply spatio-temporal backpropagation training spiking neural networks. neuronal membrane potential time time constant denotes pre-synaptic input determined pre-neuronal activities external injections synaptic weights. membrane potential exceeds given threshold neuron ﬁres spike resets potential ureset. shown figure forward dataﬂow propagates layerby-layer like dnns self-feedback injection neuron node generates non-volatile integration whole runs complex dynamics codes spatio-temporal information spike pattern. existing training algorithms consider either supervised ones backpropagation unsupervised ones timing-based plasticity causes performance bottleneck. therefore build learning framework making full however obtaining analytic solution model directly makes inconvenient/obscure train snns based backpropagation. whole network shall present complex dynamics address issue following event-driven iterative updating rule well used approximate neuronal potential based last spiking moment pre-synaptic input membrane potential exponentially decays neuron receives pre-synaptic inputs update round start neuron ﬁres spike. neuronal states co-determined spatial accumulations leaky temporal memory know efﬁciency error backpropagation training dnns greatly beneﬁts iterative representation gradient descent yields chain rule layerby-layer error propagation backward pass. motivates propose iterative based iterations occur follows direction thus states different time steps distinguished enables chain rule iterative propagation. similar idea found bptt algorithm training rnns depends error propcase derivative agation direction. help proposed iterative model eq.- unfolding state space acquire required derivative based chain rule follows formulas upper index denotes moment time denote layer number neurons layer respectively. synaptic weight neuron pre-synaptic layer neuron post-synaptic layer neuronal output neuron denotes spike activity denotes nothing occurs. simpliﬁed representation pre-synaptic inputs neuron similar original model. neuronal membrane potential neuron bias parameter related threshold vth. actually formulas also inspired lstm model using forget gate control memory output gate spike. forget gate controls leaky extent potential memory output gate generates spike activity activated. speciﬁcally small positive time constant approximated original model could since transformed iterative version recursive relationship clearly describe friendly following gradient descent training std. combining equations together seen function thus obtain derivative respect required stbp algorithm based gradient descent. assume obtained layer time derivative essential step obtain ﬁnal figure describes error propagation single-neuron level network level single-neuron level propagation decomposed vertical path horizontal path dataﬂow error propagation similar typical dnns i.e. neuron accumulates weighted error signals upper layer iteratively updates parameters different layers; dataﬂow shares neuronal states makes quite complicated directly obtain analytical solution. solve problem proposed iterative model unfold state space fig. error propagation std. single-neuron level vertical path horizontal path represent error propagation respectively. similar propagation occurs network level error requires multiply-accumulate operation like feedforward computation. depends error propagacase derivative ∂otn tion side neuron accumulates weighted error signals upper layer like case side neuron also receives propagated error self-feedback dynamics iteratively unfolding state space based chain rule like case based four cases error propagation procedure shown figure. propagation decomposed vertical path horizontal path network level dataﬂow error propagation similar typical dnns i.e. neuron accumulates weighted error signals upper layer iteratively updates parameters different layers; neuronal states unfolded iteratively timing direction enables chain-rule propagation. finally obtain derivatives respect follows previous sections presented obtain gradient information based stbp issue nondifferentiable points spiking time addressed. actually derivative output gate required stbp training eq.-. theoretically nondifferentiable dirac function greatly challenges effective learning snns zero value everywhere except inﬁnity value zero causes gradient vanishing exploding issue disables error propagation. existing method viewed discontinuous points potential spiking times noise claimed beneﬁcial model robustness directly address non-differentiability spike activity. introduce four curves approximate derivative spike activity denoted figure.b test snns model stbp training method various datasets including static mnist custom object detection dataset well dynamic n-mnist dataset. input ﬁrst layer spike train requires convert samples static datasets spike events. bernoulli sampling original pixel intensity spike rate used paper. spatio-temporal fully connected neural network static dataset. mnist dataset handwritten digits custom dataset object detection chosen test method. mnist comprised training labelled hand-written digits testing labelled digits generated postal codes digit sample grayscale image. object detection dataset two-category image dataset created pedestrian detection. includes training samples testing samples grayscale image. detecting whether pedestrian image sample labelled illustrated figure.a. upper lower sub-ﬁgures figure.c spike pattern input neurons converted center patch pixels sample example object detection dataset mnist respectively. figure.d illustrates example spike pattern output layer within stbp training stimulus digit beginning neurons output layer randomly ﬁres training neuron coding digit ﬁres intensively indicates correct inference achieved. tableii compares method several advanced results similar architecture mnist. although complex skill proposed stbp training method also outperforms reported results. achieve testing accuracy performs best. tableiii compares model typical object detection dataset. contrast model typical artiﬁcial neural networks i.e. snns following ’non-spiking network’ distinguish them. seen model achieves better performance non-spiking mlp. note overall ﬁring rate input spike train object detection dataset higher mnist dataset increase threshold simulation experiments. dynamic dataset. compared static dataset dynamic dataset n-mnist contains richer temporal features therefore suitable exploit snn’s potential ability. n-mnist database example evaluate capability stbp method dynamic dataset. n-mnist converts mentioned static mnist dataset dynamic version spike train using dynamic vision sensor original sample mnist work controls move direction three sides isosceles triangle turn collects generated spike train determines curve shape steep degree. fact derivative rectangular function polynomial function sigmoid function gaussian cumulative distribution function respectively. consistent dirac function introduce coefﬁcient ensure integral function obviously proven candidates satisfy initialization parameters weights thresholds parameters crucial stabilizing ﬁring activities whole network. simultaneously ensure timely response pre-synaptic stimulus avoid much spikes reduces neuronal selectivity. known multiply-accumulate operations prespikes weights threshold comparison steps computation forward pass. indicates relative magnitude weights thresholds determines effectiveness parameter initialization. paper threshold constant neuron simpliﬁcation adjust weights control activity balance. firstly initial weight parameters sampling standard uniform distribution presented tablei. furthermore throughout simulations work complex skill longer required ﬁxed-amount-proportional reset error normalization weight/threshold regularization etc. fig. derivative approximation non-differentiable spike activity. step activation function spike activity original derivative function typical diract function inﬁnite value zero value points. non-differentiable property disables error propagation. several typical curves approximate derivative spike activity. fig. static dataset experiments. custom dataset object detection. dataset two-category image built pedestrian detection. detecting whether pedestrian image sample labelled images yellow boxes labelled rest ones marked mnist dataset. raster plot spike pattern input neurons converted center patch pixels sample example object detection dataset mnist raster plot presents comparison output spike pattern stbp training digit mnist dataset. triggered intensity change pixel. figure.a records saccade results digit sub-graph records spike train within represents saccade period. possible change directions pixel intensity could capture corresponding kinds spike events denoted on-event off-event respectively since n-mnist allows relative shift images saccade process produces pixel range. spatio-temporal representation ﬁgure.c fig. dynamic dataset n-mnist. sub-picture shows ms-width spike train saccades. spike train generated moving dynamic vision sensor turn towards direction spatio-temporal representation spike train digit where upper lower denote on-events off-events respectively. tableiv compares stbp method state-ofthe-art results n-mnist dataset. upper results based anns lower results including method uses snns. anns methods usually adopt frame-based method collects spike events time interval form frame image conventional algorithms image classiﬁcation train networks. since transformed images often blurred frame-based preprocessing harmful model performance abandons hardware friendly event-driven paradigm. seen tableiv models generally worsen models snns. contrast snns could naturally handle event stream patterns better spatio-temporal feature event streams proposed stbp method achieves best accuracy compared reported anns snns methods. greatest advantage method complex training skills beneﬁcial future hardware implementation. spatio-temporal convolution neural network extending framework convolution neural network structure allows network going deeper grants network powerful information. framework establish spatio-temporal convolution neural network. compared spatio-temporal fully connected network main difference processing input image convolution place weighted summation. speciﬁcally convolution layer convolution neuron receives convoluted input updates state according model. pooling layer binary coding snns inappropriate standard pooling average pooling instead. spiking model also tested mnist dataset well object detection dataset mnist network contains convolution layers kernel size average pooling layers alternatively followed hidden layer. like traditional elastic distortion preprocess dataset. tablev records state-of-the-art performance spiking convolution neural networks mnist dataset. proposed spiking model obtain accuracy outperforms reported spiking networks slightly lighter structure. furthermore conﬁgure network structure custom object detection database evaluate proposed model performance. testing accuracy reported training epochs. tablevi indicates spiking model could achieve competitive performance non-spiking section ii-b introduce different curves approximate ideal derivative spike activity. analyze inﬂuence different approximation curves testing accuracy. experiments also conducted mnist dataset network structure testing accuracy reported training epochs. firstly compare impact different curve shapes model performance. simulation mentioned shown figure.b. figure.a illustrates results approximations different shapes. observe different nonlinear curves present small variations performance. rectangular approximation example explore impact width experiment results. corresponding results plotted ﬁgure.b. different colors denote different values. large small value would cause worse performance simulation achieves highest testing accuracy implies width steepness rectangle inﬂuence model performance. combining ﬁgure ﬁgure indicates point approximating derivation spike activity capture nonlinear nature speciﬁc shape critical. full spatio-temporal dynamics snns enables high-performance training. quantitatively analyze impact item. experiment conﬁgurations keep previous section also report testing results training epochs. existing termed sdbp. tablevii records simulation results. testing accuracy sdbp lower accuracy stbp different dataset shows time information beneﬁcial model performance. speciﬁcally compared stbp sdbp loss accuracy objective tracking dataset times larger loss mnist. results also imply performance sdbp stable enough. addition interference dataset itself reason variation unstability snns training. actually training snns relies heavily parameter initialization also great challenge snns applications. many reported works researchers usually leverage special skills mechanisms improve training performance lateral inhibition regularization normalization etc. contrast using stbp training method much higher performance achieved network. speciﬁcally testing accuracy stbp reaches mnist object detection dataset. note stbp achieve high accuracy without using complex training skills. stability robustness indicate dynamics fundamentally includes great potential snns computing work indeed provides idea. work uniﬁed framework allows supervised training spiking neural networks like implementing backpropagation deep neural networks built exploiting spatio-temporal information networks. major contributions summarized follows presented framework based iterative leaky integrate-and-ﬁre model enables implement spatio-temporal backpropagation snns. unlike previous methods primarily focused spatial domain features framework combines exploits features snns spatial domain temporal domain; designed stbp training algorithm implemented architectures. stbp veriﬁed static dynamic datasets. results shown model superior state-of-the-art snns relatively small-scale networks spiking cnns outperforms dnns network size dynamic nmnist dataset. attractive advantage algorithm doesn’t need extra training techniques generally required existing schemes easier implemented large-scale networks. results also revealed spatio-temporal complexity solve problems could fulﬁll potential snns better; introduced approximated derivative address non-differentiable issue spike activity. controlled experiment indicates steepness width approximation curve would affect model’s performance point approximations brain combines complexity temporal spatial domains handle input information also would like claim implementing stbp snns bioplausible applying dnns. property stbp doesn’t rely many training skills makes hardware-friendly useful design neuromorphic chip online learning ability. regarding future research topics issues believe quite necessary important. apply framework tackle problems timing characteristics dynamic data processing video stream identiﬁcation speech recognition. accelerate supervised training large scale snns based gpus/cpus neuromorphic chips. former aims exploit rich spatio-temporal features snns deal dynamic problems later greatly prompt applications large scale snns real life scenarios. hinton deng dahl deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. simard steinkraus platt best practices convolutional neural networks applied visual document analysis international conference document analysis recognition zhang henriquez ferrari spike-based indirect training spiking neural network-controlled virtual insect decision control ieee annual conference ieee kasabov capecci spiking neural network methodology modelling classiﬁcation understanding spatio-temporal data measuring cognitive processes information sciences vol. benjamin mcquinn choudhary chandrasekaran bussat alvarez-icaza arthur merolla boahen neurogrid mixed-analog-digital multichip system large-scale neural simulations proceedings ieee vol. merolla arthur alvarezicaza cassidy sawada akopyan jackson imam nakamura artiﬁcial brains. million spiking-neuron integrated circuit scalable communication network interface. science vol. furber galluppi temple plana spinnaker project proceedings ieee vol. isbell oros krichmar self-driving robot using deep convolutional neural networks neuromorphic hardware arxiv.org esser merolla arthur cassidy appuswamy andreopoulos berg mckinstry melano barch convolutional networks fast energy-efﬁcient neuromorphic computing proceedings national academy sciences united states america vol. querlioz bichler dollfus gamrat immunity device variations spiking neural network memristive nanodevices ieee transactions nanotechnology vol. kheradpisheh ganjtabesh masquelier bio-inspired unsupervised learning visual features leads robust invariant object recognition neurocomputing vol. perezcarrasco zhao serrano acha serranogotarredona chen linaresbarranco mapping framedriven frame-free event-driven vision systems low-rate rate-coding coincidence processing. application feed forward convnets. ieee transactions pattern analysis machine intelligence vol. diehl neil binas cook fast-classifying highaccuracy spiking deep networks weight threshold balancing international joint conference neural networks o’connor welling deep spiking networks arxiv.org delbruck pfeiffer training deep spiking neural networks using backpropagation frontiers neuroscience vol.", "year": "2017"}