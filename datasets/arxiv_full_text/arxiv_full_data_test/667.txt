{"title": "Hierarchical internal representation of spectral features in deep  convolutional networks trained for EEG decoding", "tag": "q-bio", "abstract": " Recently, there is increasing interest and research on the interpretability of machine learning models, for example how they transform and internally represent EEG signals in Brain-Computer Interface (BCI) applications. This can help to understand the limits of the model and how it may be improved, in addition to possibly provide insight about the data itself. Schirrmeister et al. (2017) have recently reported promising results for EEG decoding with deep convolutional neural networks (ConvNets) trained in an end-to-end manner and, with a causal visualization approach, showed that they learn to use spectral amplitude changes in the input. In this study, we investigate how ConvNets represent spectral features through the sequence of intermediate stages of the network. We show higher sensitivity to EEG phase features at earlier stages and higher sensitivity to EEG amplitude features at later stages. Intriguingly, we observed a specialization of individual stages of the network to the classical EEG frequency bands alpha, beta, and high gamma. Furthermore, we find first evidence that particularly in the last convolutional layer, the network learns to detect more complex oscillatory patterns beyond spectral phase and amplitude, reminiscent of the representation of complex visual features in later layers of ConvNets in computer vision tasks. Our findings thus provide insights into how ConvNets hierarchically represent spectral EEG features in their intermediate layers and suggest that ConvNets can exploit and might help to better understand the compositional structure of EEG time series. ", "text": "lately deep convolutional neural networks shown promising results decoding. convnets exploit hierarchical structure present many natural signals shown groundbreaking results computer vision speech recognition first results suggest perform least good already wellestablished decoding approaches notably convnets reach good accuracies learning endto-end manner without hand-designed feature extraction. however deep convnets notoriously hard interpret. visualizing convnets trained studies used weights outputs different layers convnets computed saliency maps show small changes input would affect decoding decision schirrmeister developed visualization method computes correlations amplitude changes different frequency bands signal changes resulting ﬁnal output convnet showing convnets frequency-speciﬁc spectral amplitude. still many aspects well understood especially convnets multiple computational stages extract spectral features amplitude phase different frequency bands aspects might shed light convnets perform decoding. therefore investigate several basic fully understood aspects internal representation data convnets trained end-to-end manner. this investigated convnet architecture schirrmeister recently showed reach good accuracies decoding task-related information eeg. obtain insights features learned point convnet’s hierarchical structure apply perturbation method described schirrmeister intermediate representations convnet layers. furthermore extend method also investigate strongly abstract—recently increasing interest research interpretability machine learning models example transform internally represent signals brain-computer interface applications. help understand limits model improved addition possibly provide insight data itself. schirrmeister recently reported promising results decoding deep convolutional neural networks trained end-to-end manner causal visualization approach showed learn spectral amplitude changes input. study investigate convnets represent spectral features sequence intermediate stages network. show higher sensitivity phase features earlier stages higher sensitivity amplitude features later stages. intriguingly observed specialization individual stages network classical frequency bands alpha beta high gamma. furthermore ﬁrst evidence particularly last convolutional layer network learns detect complex oscillatory patterns beyond spectral phase amplitude reminiscent representation complex visual features later layers convnets computer vision tasks. ﬁndings thus provide insights convnets hierarchically represent spectral features intermediate layers suggest convnets exploit might help better understand compositional structure time series. recently increasing interest research interpretability machine learning models. interpretability example important brain signal decoding braincomputer interfaces many situations practitioners ﬁeld want understand machine learning model extracts information brain-signal. aided understanding practitioners better understand brain-signal features model uses develop ideas improve information extraction. recent interest interpretability also motivated emerging machine learning applications safety critical environments example application lets user control wheelchair. convnet’s internal representations react changes phase features signal. additionally look mostactivating inputs lead largest activations speciﬁc ﬁlters. investigate resemble sinusoidal shapes also visualize activating inputs directly. analyses better understanding convnets learn extract spectral information brain signal. ﬁndings present study shed light internal representation spectral features convnets allow learn frequency-speciﬁc spectral features end-to-end manner. consistent pattern representations computed earlier layers change strongly phase perturbed representations later layers react strongly amplitude changes. interestingly convolution-pooling-block ﬁrst internal representation becomes phaseinvariant classical frequency bands time starting high gamma beta alpha band. also preliminary evidence later internal representations might represent complex patterns beyond simple sinusoidal shapes. dataset schirrmeister consists -electrode signals recorded electromagnetically shielded speciﬁcally optimized reduce environmental electromagnetic noise. resulting data therefore especially well suited extracting information higher frequencies dataset contains recordings subjects roughly trials each. trial duration subject tasked perform movements either left hand right hand feet rest. downsampled data achieve faster training times common average re-reference data obtain easily interpretable visualizations. amplitude perturbation detect phase-invariant response ﬁlters changes amplitude speciﬁc frequencies used perturbation correlation approach described schirrmeister adapted using multiplicative instead additive noise obtain perturbations similar relative strength across different frequencies. case ﬁlter extracts amplitude certain frequency perturbation amplitude frequency evoke consistent change activity units ﬁlter. amplitude increase evoke either activation increase activation decrease units. opposite happen amplitude decrease. independent gaussian-distributed perturbation facξci∼n multiplied amplitors tudes trial channel frequency resulting perturbed amplitudes perturbed trials reconstructed inverse fourier transform using original phases perturbed amplitudes. absolute perturbation correlations averaged ﬁlters channels subjects several repetitions procedure resulting mean absolute amplitude perturbation correlation network architecture proposed schirrmeister showed architecture perform well task-related data. network details shown figure ﬁrst layer takes channels time points input. trials cropped inputs time points using sliding windows maximum overlap. performed analysis convolutional layers denoted figure omitted ﬁrst convolutional layer non-linearities activations following convolutional layer makes activations individual ﬁlters harder interpret. training performed manner described schirrmeister subject individually resulting different models. roughly trials subject used training. mean test accuracy subjects phase perturbation response ﬁlters changes phase certain frequencies calculated similarly amplitude perturbation correlations. however cyclic nature phase features change activations ﬁlter resulting phase shift quantiﬁed using mean activation difference. instead taking difference original perturbation activations calculated correlation them. case ﬁlter sensitive speciﬁc phase speciﬁc frequency phase shift frequency evoke temporal shift unit activations ﬁlter corresponding phase shift. units ﬁlters whose receptive ﬁeld contained fig. convnet architecture decoding proposed schirrmeister blue rectangles layer inputs yellow rectangles kernels. receptive ﬁeld size activation units layer noted layer name samples convolutional layers total. ﬁrst convolutional layer followed non-linear activation function passes activations directly next convolutional layer following convolutional layers followed layer exponential linear units max-pooling layer second convolutional kernel spans channels therefore time ﬁlter dimensions left subsequent layers. investigated convolutional layers denoted speciﬁc phase original signal activate less units whose receptive ﬁeld contains speciﬁc phase perturbed signal activate more. therefore original activations activations perturbed input decreased correlation activation correlation remain similar phase-insensitive ﬁlters. additionally wanted study effect changing overall phase signal independent effect increased decreased phase synchronicity across channels. perturb phase channels individually applied phase perturbation certain frequency channels equally. ξi∼n perturbed phases calculated shifting phase perturbed signals constructed inverse fourier transformation. correlation original perturbation ﬁlter activations ﬁlter trial denoted ρyfiyp fi). corr. additionally hierarchical structure network ﬁlters learned later layers applied directly input applied representations previous layers. therefore decided common approach investigate input windows evoked highest activations ﬁlter ﬁlter determined highest unit activations training trials enforcing trial contributes highest unit activations. highest unit activations determined receptive ﬁeld input signal call input window. correspondingly call input windows highest unit activations most-activating input windows ﬁlter. visualize signals input windows median impression signal characteristics ﬁlter sensitive sine wave ﬁtting interested test ﬁlters sensitive particular phase input signal investigated learned features resembled parts complete sinusoidal curves. quantify this standardized individual most-activating input windows ﬁlter performed least squares o+a∗cos resulting standard scores. also performed simple linear compared mean squared errors sinusoidal linear ﬁts. sinusoidal mses lower linear mses ﬁlters sensitive input windows resembling sinusoidal speciﬁc phase. additionally also ﬁtted medians most-activating input windows. median most-activating input windows already resemble sinusoid certain frequency input windows share sinusoid similar phase fig. mean absolute phase amplitude perturbation correlations individual frequencies. correlation types different scales denoted left right y-axis. figure clearly inverse relation amplitude phase correlations visible. visualization additionally showed alpha beta high gamma frequency ranges speciﬁc layer phase correlation vanishes amplitude correlation saturates perturbation analysis showed earlier layers represent phase-speciﬁc features later layers later layers represent phase-invariant amplitude features early layers. figure shows average absolute phase perturbation correlation amplitude perturbation correlation convolutional layers. ﬁgure shows clearly opposing development decreased respective average values across layers. increased increasing layer depth showing almost exactly inverse relation correlations decrease/increase heavily ﬁrst second layer less following layers. fig. mean phase amplitude perturbation correlations layers. curves show mean perturbation correlation frequencies layer. scales different written left right y-axes. error bars show standard error subjects. standard errors phase amplitude similar much higher relatively amplitude correlation scale therefore visible there. respective scale curves show clearly inverse behavior layers increasing amplitude correlations decreasing phase correlations. perturbation correlations individual frequencies showed strong phase perturbation correlation earlier layers phases alpha beta high gamma range overall phase perturbation correlation highest layer gradually became lower layers interestingly frequency band speciﬁc layer phase perturbation correlations vanished completely. phase perturbation correlations high gamma vanished layer correlations beta vanished layer correlations alpha vanished layer vanishing phase correlations high gamma layer could observed subjects. subjects alpha already vanish together beta layer opposite behavior could observed amplitude notable phase insensitive amplicorrelations tude perturbation correlations emerging layer amplitude correlations individual frequency bands peaked saturated layers phase correlation vanished high gamma amplitude perturbation correlation saturated layer beta layer alpha layer mean squared error most-activating input window sinusoidal linear shown figure still relatively large sinusoidal consistently lower linear means most-activating input windows could better approximated sinusoid layers. sinusoidal most-activating input windows lowest layers sinusoidal approached linear last layer. sinusoids ﬁtted median mostactivating windows also shown figure medians layers consistently better approximated sinusoidal compared linear linear median especially high layers relatively layer sinusoidal layers. also distributions individual squared errors fig. sinusoidal linear most-activating input windows. shown distributions mean squared errors mean layer ﬁtted data individual input windows ﬁtted median across them. distributions differed statistically signiﬁcantly sinusoidal linear layers individual signals median signals fig. histogram phases ﬁtted sinusoids. sinusoids ﬁtted medians activating input windows ﬁlter. histograms shown phases sinusoidal frequency alpha beta band layer subjects. clear bimodal distribution visible frequency ranges. distributions frequencies ﬁtted sinusoids strongly resembled phase correlation plots previous section layer frequencies ﬁtted sinusoids alpha beta high gamma bands. high gamma band frequencies disappear layer beta band frequencies layer alpha frequencies strongly reduced step layer furthermore distributions phases sinusoids ﬁtted medians ﬁtted frequency alpha beta band layer clearly showed bimodal distributions opposing peaks shifted however distributions frequency bands layers clearly bimodal resembled uniform distributions. investigate features frequency-speciﬁc phase amplitude sinusoidal signals visually inspected most-activating input windows ﬁlter median values timepoint. figure shows most-activating input windows randomly sampled ﬁlter subject layer. show activating input windows representative electrode. several ﬁlters clearly deﬁned structure present median. median plots layer show several examples sinusoidal shapes different frequencies. medians fig. signals representative most-activating input window randomly sampled ﬁlter subject layer. blue points standard scores most-activating input windows ﬁlter. median shown black interquartile range gray shaded area. medians respect earlier layers often resemble parts complete sinusoids medians later layers resemble complex patterns. higher frequencies sharper variance across input windows larger. medians layer revealed several smooth alpha sinusoids also examples beta waves. addition that medians without easily interpretable periodicity temporal structure. layer mostly examples alpha waves. medians layer complex temporal patterns emerged. medians relatively beginning input windows showed oscillatory pattern increasing amplitude later parts input windows. also oscillatory patterns examples resembled waves closely pure sinusoids. patterns found several subjects. ﬁndings study provide insight internal representation convnet trained decoding representations develop different convolutional layers. schirrmeister showed correlation spectral amplitude perturbations activation changes ﬁnal classiﬁcation layer. expanded approach investigating inﬂuence spectral amplitude phase sequence intermediate layers. showed ﬁlters later layers convnet extract information spectral amplitudes signal combining information ﬁlters detect phase-speciﬁc periodic often sinosoidal patterns certain frequency early layers. spectral amplitude highly informative case trial-wise motor decoding whereas spectral phase informative continuous motor behavior decoding visualizations revealed relation typical frequency bands intermediate representations trained convnets. different intermediate layers showed specialization either alpha beta high gamma band. data information amplitude higher frequency bands extracted early network forwarded later layers. additionally observed bimodal distributions phases ﬁtted sinusoids distance peaks. notable difference bases conventional fourier transformation output coefﬁcients frequency deﬁned product signal sinusoid frequency suggests networks using exactly bases fourier transformation. size temporal receptive ﬁelds grows early later layers spectral decomposition learned networks might higher time resolution higher frequencies lower time resolution lower frequency bands similar wavelet spectral analyses. investigations would however necessary clarify properties spectral decomposition learned convnets relates classical spectral estimation techniques. addition clear sensitivity ﬁlters basic spectral features visual analysis average data most-activating input windows showed preference complex features ﬁlters last layer. thus ﬁlters later layers networks trained decoding appear learn detect basic spectral phase amplitude features also complex combinations those. filters later layers could analyzed reveal complex features thereby possibly increase understanding underlying signals. would similar networks trained image classiﬁcation ﬁlters later layers shown detect complex structures like faces parts objects abstract categorical features study provided insights internal representation spectral features convolutional networks trained decoding. described possible mechanisms convnets learn hierarchically combine lowhigher-level features sequence convolutional layers network. future steps would interesting examine representations spectral features depend network architecture decoding task. regarding network architecture could investigate convolutional layer physiologically meaningful frequency band natural choice resulting potential performance interpretability advantages. regarding decoding task could train network decode continuous movement parameters phase play important role classiﬁcation task examined furthermore examining internal spectral representations convnets relate classical spectral estimation techniques analysis might possibly provide alternative classical spectral analyses. progress along lines help optimizing convnets decoding visualization exploring complex features. haufe meinecke g¨orgen d¨ahne j.-d. haynes blankertz bießmann interpretation weight vectors linear models multivariate neuroimaging neuroimage vol. schirrmeister springenberg fiederer glasstetter eggensperger tangermann hutter burgard ball deep learning convolutional neural networks decoding visualization human brain mapping vol. abdel-hamid a.-r. mohamed jiang deng penn convolutional neural networks speech recognition ieee/acm transactions audio speech language processing vol. sainath a.-r. mohamed kingsbury ramabhadran deep convolutional neural networks lvcsr ieee international conference acoustics speech signal processing ieee shamwell kwon marathe lawhern nothwang single-trial rsvp classiﬁcation using convolutional neural networks international society optics photonics hammer fischer ruescher schulze-bonhage aertsen ball role ecog magnitude phase decoding position velocity acceleration continuous motor behavior frontiers neuroscience vol.", "year": "2017"}