{"title": "Markov Brains: A Technical Introduction", "tag": "q-bio", "abstract": " Markov Brains are a class of evolvable artificial neural networks (ANN). They differ from conventional ANNs in many aspects, but the key difference is that instead of a layered architecture, with each node performing the same function, Markov Brains are networks built from individual computational components. These computational components interact with each other, receive inputs from sensors, and control motor outputs. The function of the computational components, their connections to each other, as well as connections to sensors and motors are all subject to evolutionary optimization. Here we describe in detail how a Markov Brain works, what techniques can be used to study them, and how they can be evolved. ", "text": "markov brains class evolvable artiﬁcial neural networks differ conventional anns many aspects difference instead layered architecture node performing function markov brains networks built individual computational components. computational components interact other receive inputs sensors control motor outputs. function computational components connections other well connections sensors motors subject evolutionary optimization. describe detail markov brain works techniques used study them evolved. artiﬁcial neural networks become staple classiﬁcation problems well common tool neuro-evolution research. assume reader knows simple three layer perceptron input hidden output layers works recurrent neural network used control robot embodied virtual agent perception action loop discuss methods evolve markov brains assume reader general knowledge genetic algorithms digital evolution. level genotype. also possible encode directly means network evolves experiencing rewiring functional changes well additions subtractions computational components governed speciﬁc rules. therefore distinguish genetically directly encoded mbs. regardless encoding always deﬁned state buffer contains input hidden output states. buffer sometimes referred state vector call individual components vector states nodes. buffer experience update computational components read buffer compute next state buffer brain updates happen parallel agent controlled computing agent senses feeding data input states performing brain update reading output states buffer using outputs control agent environment. constitutes whole perception-action loop repeated allows control agent deﬁned period lifetime. explicit restriction deﬁnition number brain world updates computed within whole action-perception loop. example possible update world once followed brain updates. experimenter needs decide this different applications might require different settings. logic components necessarily read states buffer instead component subset connections select subset states buffer. addition logic gate write many different states buffer. connections change course evolution theory conceive connecting outputs computational components arbitrarily hidden states creates situations single state buffer becomes updated computational component. computational components being executed sequentially would lead states overridden. however computational components updated parallel thus states receive multiple inputs integrate signal summation. think computational components roughly analogous neurons zeros ones binary logic operations absence presence action potential. along axon either action potential thus think summation resembles phenomenon best. computational components allowed write input states buffer happens input states overridden world percept computed. reading output states sometimes desired output states brain update computed. state buffer vector continuous variables time many earlier implementations used discrete binary values states therefore gates require binary inputs discretizer interprets values larger values become type computational component needs types values deﬁned properly. values deﬁne function connectivity well ranges within values vary. evolution optimizes values deﬁne computational component ranges experimenter example experimenter could limit number input connections number output connections mutation number inputs outputs occurs value kept within predeﬁned conditions. values encoded genome vary implementations also varies different kinds components. computational components made different types computational components interact outside world. many different components available units designed added document regular basis. earlier publications using technology used fairly simple components technically limit components traditionally used deterministic probabilistic logic gates computational building blocks. added components simulate classical artiﬁcial neural networks well thresholding functions timers counters. deﬁned logic gates figure illustration recurrent artiﬁcial neural network markov brain. recurrent artiﬁcial neural network input layer contains additional recurrent nodes. nodes layer connected nodes next layer. anns hidden layers requirement. connections nodes typically specify connection weight cases nodes updated using transfer threshold function also requirement. special class anns topology neat hyperneat anns also evolvable topology well evolvable transfer threshold function. deﬁned state buffer element represents node. inputs written part buffer. brain update applies computations deﬁned computational components buffer. outputs read subset buffer. since inputs generally overridden environment considered part recurring. however outputs last update generally accessible computational components thus part recurrence. computational components illustrated using shape however practice evolved using different types computational components. process feedback perform lifetime learning well gates resemble biological neurons gates perform ternary logic operations. present components. gates read multiple inputs write multiple outputs extends logic conventional logic gate explained section deterministic logic gates applies gates stated otherwise. deterministic logic logic gates commonly known perform essential functions computation nand etc. gates typically receive binary inputs return result computation single bit. constitutes call -in--out gate. however logic gates built inputs outputs. multi-and might eight inputs returns eight inputs also possible input cases. would example -in--out gate. three-bit multiplexer three inputs eight outputs three inputs control eight output wires similarly deterministic logic gates arbitrary number inputs outputs. typically limit four inputs four outputs maximally require least input output. becomes complicated table multiple outputs involved. imagine -in--out deterministic logic gate inputs well outputs resulting logic table would become table logic table -in--out gate output following logic inputs output logic. output noted column. example inputs outputs must well indicated column labeled containing input similarly inputs output must logic output must logic. indicated output seen column title probabilistic logic gates probabilistic logic gates work similarly deterministic logic gates compute mapping inputs outputs. however mapping probabilistic. instead using deterministic logic table probabilistic means table deterministic mappings swapped stochastic ones case output table interpreted matrix probabilities. probability matrix -in--out table deﬁning would look like this table probabilistic logic table -in--out gate output follows logic inputs output follows logic. note probabilities thus even though probabilities logic remains deterministic. column represents probability four possible outputs occur. range since inputs binary range deﬁnes possible combinations. similarly possible outputs call every update gate receives input needs produce output deﬁned probability matrix call table dimension element matrix deﬁnes likelihood input resulting output consequence matrix needs matrix read genome example table ﬁlled arbitrary values read genome automatically normalized. ever would ﬁlled everywhere would become equal probabilities. type gate also ﬂexible number inputs outputs. implements single-layer artiﬁcial neural network straddles inputs outputs gate. output node connected input node gate connection weight. transfer function node weight input products imax deﬁnes number inputs threshold gate discretizes inputs accumulates them. accumulated values reach genetically determined threshold outputs return internal accumulator reset zero. otherwise gate returns outputs. note gate contains hidden value state. feedback gates general feedback gates similar probabilistic gates regarding inputs outputs well probability table. however additional inputs used detect positive negative feedback. given update gate receives speciﬁc input returns speciﬁc output depending evaluation probability table. resulting outputs typically directly control actuator result changes state buffer time affect actuators thus behavior agent. turn actions agent might beneﬁcial detrimental. information directly given agent; instead agent needs also evolve machinery assesses sensory input positive negative feedback. means time point output gate computed clear output good one. therefore gate keeps track previous input-to-output mappings order apply feedback later stages. number previous mappings remembered genetically encoded thus evolvable. speciﬁcally input received time point output generated time point since gate technically similar probabilistic logic gate also probability matrix input output pair generated probability matrix speciﬁcally element pitot+. technically entire responsible deﬁne pitot+ responsible. similar process q-learning delayed rewards probability would deﬁned computation gate performed resulted positive feedback probability pitot+ increased whereas case negative feedback decreased. however relevant pertinent feedback might arrive much later time point hand information takes time propagate hand action agent might delayed rewards. therefore input-output pairs range buffer evolvable property gate. case positive feedback received gate sequentially increases entry probability matrix deﬁned pairs buffer. increases values using random number drawn uniform random number maximum value evolvable. increase pitot becomes normalized again next change applied. ternary logic gates ternary logic gates ﬁrst discretize inputs differently. values considered values lower interpreted values interpreted that gates function like deterministic logic gates except compute ternary logic return three different values outputs. consequence logic table probabilistic gate example determined probabilities different reasons using ternary logic gates mbs. example voltage-gated channels three states closed open blocked implies neurons cannot immediately ﬁred. furthermore good evidence information sometimes stored ternary logic using three different ﬁring rates reichardt detectors sense motion visual system listed possible components have easily imagine more. components encoded genetically crucial manner encoding determine component reacts mutations. principle could direct encoding mutations applied brain itself. example might randomly select probabilities gate table change gates rewire randomly gates disappear gates added. well-known direct encodings usually fragile prior implementations used genetic encoding. encoding varies slightly implementation implementation experience signiﬁcant change evolvability example. encode vector elements interpreted beginning analogous strand site codes particular nucleotide. here instead nucleotides numbers parsing vector pairs numbers used identify sections encode single gate. idea inspired beginning genes marked namely start codons mark point transcription initiation. theoretically possible transcriptional regulation expression genes either activates deactivates genes/gates based environmental signals idea tested. start codon found genome stretch sites following used deﬁne values relevant gate. stretch called gene stay within biological metaphor. since wide variety gates gene decoded subject gate deﬁnes. generally ﬁrst pair sites deﬁnes number inputs outputs gate has. since minimal maximal number connections inputs outputs deﬁned experimenter value read site transformed value allowed minimum maximum. next sections deﬁne nodes state buffer read written into. that diversity gates becomes large common denominator except following stretch responsible encode function gate. case probabilistic logic gate would deﬁne probability table whereas timer gate interprets next value interval signal. also implies different gates different sizes genome. general description gene encodes gate figure start codon used probabilistic logic gates followed whereas deterministic logic gates followed choice start codons arbitrary picked homage douglas adams’s hitchhikers guide galaxy represents answer ultimate question life universe everything. unwise choose start codon would imply large sections probability table would interpreted start signals thus giving rise overlapping genes hard interpret. similarly would unwise create start codons consisting three bytes gates would formed rarely. start codon found close genome missing bytes read beginning making genome effectively circular. start codon found inside coding region another gate another gate made accordingly leading overlapping reading frames. device often found nature protect genetic information also useful evolution mbs. principle genomes organized many chromosomes well diploid polyploid even including genders variations known occur nature. mutations genome since encoded genome genome becomes transmitted next generation genome experience mutations. mutations applied genome becomes translated brain again changes manifest differences figure encoding scheme illustration. sequence sites genome read beginning end. sites non-coding start codon found following stretch sites deﬁning connections function gate start codon next sites encoding number inputs outputs followed sites encode speciﬁc nodes state buffer gate receives inputs outputs gate write. last stretch sites used encode speciﬁc values needs specify function gate. cally chance section genome deleted randomly. size section sites deletions occur genome larger bytes. similarly chance section sites copied randomly inserted anywhere genome allowed genome already sites. limits insertions deletions prevent genomes disappearing becoming intractably large. note lower upper limit genome size experimenter. principle kinds mutations possible hard-coded lower upper limits genome size practical values changed arbitrarily. typically used control agent classiﬁer decision maker. order optimize performance performance needs quantiﬁable ranked performance. generally speaking perform better given offspring poor performers. essential dynamic darwinian evolution implemented many different ways standard ‘moran process’ ‘tournament selection’ examples. genome selected replication experiences mutations described above. quantity quality changes applied controlled experimenter. typically evolution needs thousands generations population converge solution. types gates utilized plays important role shaping adaptive trajectory. example experienced behavioral tasks deterministic evolve faster made stochastic elements. visualizing several different ways visualize typically idea give user information placement gates connect nodes show user nodes relay information other. ﬁrst illustrates state buffer time point ﬁgure updated buffer time point bottom. gates added boxes gate types matter either colors additional kinds shapes used. input connections gate drawn upper state buffer output connections drawn bottom state buffer. symbolizes information changes update update. alternatively gates function type lesser importance represented graph showing states inﬂuence type illustration also removes nodes connection therefore meaningless computation. order illustrate nodes sensors ones actuators colors shapes used. state-to-state transition diagrams lifetime state constantly changes. recording state brain every update allows reconstruction state-to-state transition matrix depicts probability particular state followed particular state. case continuous values used brain states discretization step performed ﬁrst. deterministic state-to-state transitions affected sequence inputs probabilities computational components. fig. shows example state-to-state transition graph deterministic single auditory sensor simple case transition inﬂuenced whether tone present absent input usually moves brain particular state another often states labelled decimal equivsparse thus states much interdependent conditional other. consequence state changed much easily makes experience better substrate ﬁne-grained behavior well better suited integration temporal information. brains receive inputs multiple sources stateto-state transition diagrams focusing single source reveal important dynamics either must keep state inputs ﬁxed else show different arrows combination inputs. brain size complexity estimates natural organisms weigh brain estimate number cells involved really cannot easily estimate brain’s complexity. trivial estimate size complexity particular either many different quantities measure different aspects brain’s performance. number gates analogous number neurons size state buffer determines brain’s performance? fact problem different quests deﬁne functional complexity. could brain size proxy complexity size state buffer predeﬁned thus limited also maximum size genome limited thus affects complexity well. lastly elements brain cost anything increasing number components reﬂect functionality. regardless caveats possible count number gates number hidden states used diameter causal graph number connections size genome proxy assess brain complexity. principle would useful deﬁne equivalent vapnik-chervonenkis dimension measure capacity brain learn functions priori clear capacity alent binary state vector. thus prior experiencing input brain begins state example fig. hearing moves brain state state stands activation pattern important note state-to-state diagram implies brain’s future behavior strongly inﬂuenced state therefore computations highly dependent past experience. example particular sequence tones brain particular state determine respond future tones anticipates different future tones state. state-dependence computations makes qualitatively different standard anns. recurrent anns also perform state-dependent computations networks nodes layer connected nodes next implies current state best described conglomerate gives rise large basins attraction precisely makes networks good classiﬁcation tasks. hand connections figure state-to-state transition diagram -node brain listens periodic tone delivered single input. state labels constructed decimal equivalent binary state vector functional brain including neuron determines transitions whose state indicated boxes next arrows. state-to-state diagram depicts transitions relevant brain’s ﬁtness transitions shaped evolution. brain initialized quiescent state translates state brain density estimates addition brain size might interconnected brains are. general idea sparsely connected brain relay information computational units whereas fully connected brain allows computational units access information. clear distinction actual impact computations performed. deﬁne density actual number connections states total possible number connections states. connection context number connections nodes state buffer. speciﬁcally count nodes connected gate relays information node other. multiple connections states counted connection otherwise inﬁnite amount connections would possible. nodes receive inputs gates nodes read gates removed count connections since state either always contribute computation. ponent’s function comparing wild-type behavior system behavior system component knocked classical genetics. knockouts performed different levels. sections genome removed gates removed states silenced. obvious gates parts genome knocked simple modifying states. state cannot removed since would leave gates connections nowhere. therefore next plausible thing would states alternatively state could random number matter. loss function would depend kind knockout performed. neuro-correlate ability integrate information measured using neuro-correlate measure well particular network integrates information calculating network rather complex time-consuming procedure requires user record state-to-state transitions lifetime later φ-measures connectivity however instrumental validating measure possible measure brain complexity neuro-correlate another possible estimating brain complexity quantify much world represented within mb’s internal states meaning much brain knows world without consulting sensors. shown quantitative measure representation highly predictive brain’s ﬁtness important note work representation showed evolve single state store speciﬁc piece information instead information smeared–or distributed–over many states concepts represented within groups states generally simple concepts rather compound concepts. modularity modularity assessed using graph representation brain however found gives meaningful results brains larger nodes. also note modularity measures would quantify topological modularity functional modularity latter nodes would need assigned particular functions. performing functional analysis knockouts states contribute many functions makes impossible generic modularity measures nodes must assigned function only. dilemma modularity measure allows user measure modularity systems whose components several modules time. mutational robustness mutational robustness measure seeks assess tolerate multiple mutations brittle. end–either exhaustively sampling–the ﬁtness measured experiencing single double triple even mutations time. plotting mean ﬁtness number mutations assess robustness system also works arbitrary sources noise mutations. cases would ascertain robustness respect particular form noise. quantiﬁcation behavior beyond visualizing agent controlled movie behavior also quantiﬁed differently. highly recommend experimenter observes behavior agent visually possible need point assessment subjective quantitative best anecdotal. figure illustration mutational robustness average ﬁtness shown function number mutations noise experienced curve remains high mutations indicates system robust. quick loss functionality indicates brittle system beyond sequence actions also deployed measures assess behavior agents example much swarm collectively group. measures distance travelled food collected property recorded might useful. important concept relay type quantiﬁable data together properly executed statistical analysis better subjective observation interpretation behavior much standard behavioral biology. clear search literature extent already deﬁned particular before whether used evolved earlier others. concept probabilistic ﬁnite state machine controlled inputs implicit hidden markov model process. standard example hidden markov models trained recognize–or even synthesize–particular molecular sequences name chose technology refers directly heritage. development owes great deal insights jeff hawkins correctly views human brains prediction machines deeply fundamentally conditioned past experience. recurrent nature well idea create complex system extreme bandwidth still made individual computational units inspired book strange fact many different computational implementations result turing complete machines implies always possible implement functionality system using different technology. network deterministic gates could principle implement chip therefore simulate universal computer. whether evolution ever would optimize towards architecture however extremely unlikely. ultimately think ﬁnite state machine whose current state deﬁned state buffer becomes updated according inputs current states access internal models shaping priors. computational components deterministic would resemble deterministic ﬁnite state machine made probabilistic computational components would resemble probabilistic ﬁnite state machine. consequence difﬁcult proper classiﬁcation mbs. strange implementation ﬁnite state machine weird implementation hidden markov process? also deﬁnition start end? necessarily include genetic encoding ﬂexible varied could even ignored case direct encoding? regardless actual answer method evolve implemented multiple times already independently various purposes. give brief historic overview implementations. integrated information ﬁrst implementation written jeffrey edlund installment used stack genes without non-coding regions instead full genome uses start codons thus could signiﬁcant stretches non-functional code. implementation quickly changed genetic encoding project. without performed quantitative comparison systems consensus genetic encoding evolved faster higher ﬁtness average. implementation also used type start codon probabilistic logic gates. published ﬁrst article using implementation representations original work representation expanded includes multiple different start codons deﬁne gates deterministic probabilistic logic gates well precursor artiﬁcial neuronal network gates. framework written arend hintze expanded adapted several times several experiments adding threshold swarming allowing evolution swarms well predator prey dynamic required expansion framework mentioned above. work involves swarms agents performed using extension written randal olson made work also resulted python implementation brains deterministic probabilistic logic gates https //github.com/rhiever/markovnetwork. learning studying evolution learning expanded framework. work added feedback gates study agents able adapt environment within lifetime performed leigh sheneman arend hintze ealib also added david knoester ealib software library develop wide range different evolutionary computational experiments. framework used several publications since implementation allowed much larger brains used somedifferent encoding gates. mabe insights software development emerged within last decade importance platform independence importance code recycling enable environment code easily re-purposed akin horizontal transfer genetic code biological organisms. creating platform speciﬁc runs risk producing results germane particular implementation thus question generality results obtained. therefore ideally experiments repeated using types computational brains modern platform provide level modularity makes easy. secondly code developed experiment used experiments either control improvement. works code designed modular explicitly allow kind exchange without breaking earlier constraints. prior experiments contains examples. platform designed support vast variety future experiments importantly designed contributions experimenter used others. match philosophy already allowed experiments suggest future experiments done using mabe. work supported part nsf’s beacon center study evolution action contract dbi- paul allen family foundation. grateful christof koch giulio tononi support discussions crucial insights. thank fred dyer frank bartlett lars marstaller devin mcauley heather eisthen nikhil joshi samuel chapman pleskac ralph hertwig attempting keep honest respect neuroscience well intelligence behavior real animals", "year": "2017"}