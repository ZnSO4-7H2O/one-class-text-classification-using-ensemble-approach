{"title": "Scalable Inference of Ordinary Differential Equation Models of  Biochemical Processes", "tag": "q-bio", "abstract": " Ordinary differential equation models have become a standard tool for the mechanistic description of biochemical processes. If parameters are inferred from experimental data, such mechanistic models can provide accurate predictions about the behavior of latent variables or the process under new experimental conditions. Complementarily, inference of model structure can be used to identify the most plausible model structure from a set of candidates, and thus gain novel biological insight. Several toolboxes can infer model parameters and structure for small- to medium-scale mechanistic models out of the box. However, models for highly multiplexed datasets can require hundreds to thousands of state variables and parameters. For the analysis of such large-scale models, most algorithms require intractably high computation times. This chapter provides an overview of state-of-the-art methods for parameter and model inference, with an emphasis on scalability. ", "text": "words parameter estimation uncertainty analysis ordinary differential equations large-scale models abstract ordinary differential equation models become standard tool mechanistic description biochemical processes. parameters inferred experimental data mechanistic models provide accurate predictions behavior latent variables process experimental conditions. complementarily inference model structure used identify plausible model structure candidates thus gain novel biological insight. several toolboxes infer model parameters structure smallmedium-scale mechanistic models box. however models highly multiplexed datasets require hundreds thousands state variables parameters. analysis large-scale models algorithms require intractably high computation times. chapter provides overview state-of-the-art methods parameter model inference emphasis scalability. systems biology ordinary differential equation models become standard tool analysis biochemical reaction networks models derived information underlying biochemical processes allow systematic integration prior knowledge. models particularly valuable used predict temporal evolution latent variables moreover provide executable formulations biological hypotheses therefore allow rigorous falsiﬁcation hypotheses thereby deepening biological understanding. furthermore models applied derive model-based biomarkers enable personalized design targeted therapies precision medicine. construct predictive models model parameters inferred experimental data. inference requires repeated numerical simulation model. consequently parameter inference computationally demanding required computation time numerical solution high. many applications smallmedium-scale models i.e. models consisting small number species belonging fabian fröhlich institute computational biology helmholtz zentrum münchen neuherberg germany chair mathematical modeling biological systems center mathematics technische universität münchen garching germany e-mail fabian.froehlichhelmholtz-muenchen.de carolin loos institute computational biology helmholtz zentrum münchen neuherberg germany chair mathematical modeling biological systems center mathematics technische universität münchen garching germany e-mail carolin.looshelmholtz-muenchen.de hasenauer institute computational biology helmholtz zentrum münchen neuherberg germany chair mathematical modeling biological systems center mathematics technische universität münchen garching germany e-mail jan.hasenauerhelmholtz-muenchen.de core pathway accurate prediction hypothesis testing low-dimensional models derived directly obtained large-scale models model reduction e.g. lumping multi-step reactions one-step reactions assuming time-scale separation smallmedium-scale models analyzed using established toolboxes implementing state-of-the-art methods models describing conditions e.g. response single cell line small stimulations lumping ignoring processes might appropriate. model used wide range conditions e.g. describe responses many cell lines many different stimuli detailed mechanistic description required simpliﬁcations hold selected conditions. detailed mechanistic generalizing models appear particularly valuable precision medicine model must accurately predict treatment outcomes many different patients comprehensive models typically describing species multiple different pathways including respective crosstalk easily describe thousands molecular species involved thousands reactions thousands parameters. models parameter inference often intractable prohibitively computationally expensive beyond model parameters also model structure might unknown e.g. biochemical reactions regulations might unknown then inference model structure used generate mechanistic insights. models achieved constructing multiple model candidates corresponding different biological hypotheses. hypotheses falsiﬁed using model selection criteria akaike information criterion bayesian information criterion large models high number mutually non-exclusive hypotheses uncommon typically leads combinatorial explosion number model candidates computing model candidates comparison would require parameter inference model candidate seem futile given parameter inference single model already challenging. chapter review scalable methods render model parameter model structure inference tractable large-scale models hundreds thousands molecular species biochemical reactions parameters. parameter inference focus different gradient-based optimization schemes describe scaling properties respect number molecular species number parameters. inference model structure focus complexity penalization schemes allow simultaneous inference model structure parameters thus scale better linearly number model candidates. experiments usually provide information different observables depend linearly nonlinearly concentrations direct measurement usually possible. dependence observable concentrations parameters described build predictive models model parameters inferred experimental data. inference problem usually formulated optimization problem. optimization problem objective function describing difference measurements simulation minimized. following ﬁrst formulate optimization problem discuss methods solve efﬁciently. experimental data subject measurement noise. common assumption measurement noise time-points observables additive independent normally distributed timepoints time-points different measurements recorded experimental data standard deviation measurement noise potentially unknown model yields likelihood function plausible noise assumptions include log-normal distributions correspond multiplicative measurement noise distributions heavier tails laplace distribution used increase robustness outliers data model inferred experimental data maximizing likelihood yields maximum likelihood estimate however evaluation likelihood function involves computation several products numerically unstable. thus negative log-likelihood often used objective function minimization. logarithm strictly monotonously increasing function minimization −log) equivalent maximization therefore corresponding minimization problem infer parameters. noise variance depend parameters weighted least-squares objective function. discuss later least-squares structure exploited several optimization methods. according bayes’ theorem posterior probability deﬁned optimization problem convex usually non-convex thus objective function possess local minima saddle points. local minima problematic optimization algorithms stuck yielding suboptimal agreement experimental data model simulation. interestingly recent literature suggests saddle points might affect efﬁciency optimization severely local minima unconstrained problems saddle points local minima stationary points gradient vanishes fig. overview numerical methods scaling properties parameter inference. schematic scaling properties optimization. icons properties local global optimizations shown right. examples gradient-based methods determine parameter updates. computation times different approaches gradient computation shown right. schematic scaling properties simulation algorithms. dense sparse direct linear solvers compared right. reordering performed using approximative minimum degree ordering algorithm dependence number local minima saddle points models number parameters poorly understood. deep learning problems exponential increase number local minima number parameters primarily attributed parameter permutation symmetries rare models. deep learning problems saddle points also problematic affect performance local optimization methods arguments exponential increase stationary points number parameters often based random matrix theory rely strong assumptions distribution entries hessian objective function assumptions rigorously checked lead wrong conclusions shown stability odes objective function depends solution validity assumptions evident difﬁcult assess rigorously. saddle points aware rigorous evaluation. thus exact dependence number local minima saddle points parameter dimensionality remains elusive. infamous free lunch theorem optimization states exists single optimization method performs best classes optimization problems. accordingly empirical evidence well careful analysis problem structure considered selecting suitable optimization method. plethora different optimization methods commonly used systems biology classiﬁed local global methods well gradient-based derivative-free methods. local methods search local optima global methods search global minima. separation local global methods often clear-cut. thus methods simulated annealing sometimes classiﬁed local sometimes global methods therefore following paragraph contains many soft statements serve guidelines. gradient-based methods exploit ﬁrst potentially higher order derivatives objective function derivative-free methods solely objective function. local methods construct sequence trial points successively decrease objective function values procedure usually faster global methods stuck local minima local derivative-free methods direct search methods contrast local methods global methods often rely population trial points iteratively reﬁned increase chance reaching global minimum usually slower global derivative-free methods mostly employ stochastic schemes often inspired nature global gradient-based methods usually perform repeated local optimizations. examples local global well respective derivative-free gradient based methods given table global methods guaranteed converge global minimum convergence global minimum guaranteed rigorous complete global methods branch-and-bound grid search long local information i.e. function values respective parameter derivatives available termination methods require exponentially expensive dense search termination global methods crucial might relatively easy global minimum comparably hard guarantee indeed global minimum. global information lipschitz constants rarely available problems dense i.e. exhaustive search would necessary guaranteed convergence. parameters generally continuous dense search rarely possible. instead meta-heuristics termination optimization employed. many meta-heuristic methods exists little theoretical justiﬁcation convergence proofs others converge probability arbitrarily close might inﬁnitely many function evaluations. practice many meta-heuristic algorithms even fail work reliably smooth convex problems parameters fact algorithms non-convergence even proven mathematically eventually even rigorous convergence guarantee useless convergence rate slow practical purposes. methods exists plethora disparate variants renders comprehensive analysis convergence proofs convergence rates challenging. quite unsatisfying theoretical perspective. practice reasonable results obtained using global optimization methods usually guarantees global optimality given. remainder chapter primarily focus global gradient-based optimization methods typically rely repeated local optimization. meta-heuristic methods local optimization started points parameter space. termination methods usually relies speciﬁed maximum number local optimizations also bayesian methods applied convergence global minimum local optimization started region attraction global optimum. thus probability convergence depend relative volume region attraction respect search domain several adaptive methods scatter search clustering search improve chance starting local optimization region attraction global optimum rely embeddedness global optimum embeddedness global optimum characterizes well local minima cluster determines indicative objective function value starting point chance converging global minimum. aware analysis embeddedness global minimum models systems biology likely problem dependent. resulting rate convergence determined rate convergence local method probability sample starting point region attraction global optimum. methods employ repeated local optimization individual local optimization runs trivially parallel enables efﬁcient high performance computing structure. moreover multiple global runs asynchronously parallelized enhance efﬁciency cooperativity following recent studies deem repeated local optimization suitable candidate scalable optimization following discuss properties respective local gradient based methods detail. controls step size. line-search update direction ﬁxed ﬁrst suitable determined. alternative line-search methods trust-region methods ﬁrst deﬁne maximum step length suitable update direction. discuss trust-region methods detail following subsection. classiﬁcation line-search trust-region methods ambiguous depend speciﬁc implementation method. discussed detail subsection. chapter follow classiﬁcation nocedal wright also provide excellent discussion topic line-search methods particularly appealing reduce possibly high-dimensional optimization problem sequence one-dimensional problems ﬁnding good values ensure convergence meet certain conditions methods determine step size satisfy conditions sometimes referred globalization techniques. note enforcing conditions guarantees convergence local stationary point i.e. local minimum local maximum saddle point global minimum less well established local optimization gradient-based methods used applicable. kolda state review direct search methods \"today people’s ﬁrst recommendation solve unconstrained problem accurate ﬁrst derivatives obtained would direct search method rather gradient-based method\" lewis claim \"the success quasi-newton methods applicable undisputed\" gradientbased methods quasi-newton methods applicable gradient objective function continuous computed accurately. deﬁnition gradient continuous respect parameters objective function continuously differentiable. higher order continuous differentiability corresponds continuity respective higher order derivatives hessian. many noise distributions normal log-normal distribution negative log-likelihood inﬁnitely often continuously differentiable respect observables given ﬁnite number measurements considered. thus continuity derivatives objective function depends continuity derivatives model output however particular noise distributions laplace distribution negative log-likelihood differentiable respect model outputs. following assume both twice continuously differentiable respect parameters. optimization methods using search direction called gradient descent methods. locally gradient provides steepest descent respect euclidean norm. means gradient points direction unit length euclidean norm yields strongest decrease neighborhood around depending objective function neighborhood might arbitrarily small resulting small values this instance case objective functions curved ridges e.g. rosenbrock function arise dependencies parameters. moreover gradient descent methods take small steps vicinity saddle points lead high iteration numbers premature termination individual optimization runs. classical newton’s method step size ﬁxed however many modern implementations implement newton’s method line-search using default value adapting step size necessary hessian symmetric highly efﬁcient methods cholesky factorization used solve problem convex optimization problems. however models computation hessian usually computationally expensive computation newton step. thus computational cost solving usually negligible objective functions depending models. non-convex problems computation newton step ill-posed even well-deﬁned problem moreover newton step might descent direction. newton step direction descent scalar product gradient negative globally satisﬁed inverse hessian positive deﬁnite i.e. problem convex. previously discussed typically non-convex thus simple newton steps always yield direction descent. moreover vicinity saddle point newton step point direction saddle point thus attracting optimizer saddle points literature several modiﬁcations newton’s method always yield descent directions proposed gauss-newton method exploits least-squares structure objective function constructs positive semi-deﬁnite approximation levenberg marquardt independently extended method introducing dampening term step equation. yields levenberg-marquardt method positive semi-deﬁnite gauss-newton approximation hessian dampening factor identity matrix. magnitude dampening factor regulates conditioning geometric interpretation allows interpolation gradient gaussnewton step corresponds pure approximate newton step. positive-deﬁniteness gauss-newton approximation respective methods cannot follow directions negative curvature thus attracted saddle points limited small step-sizes vicinity saddle points gauss-newton method limited least-squares problems traditional formulation levenberg-marquardt method limitation. however possible apply dampening hessian without gauss-newton approximation. resulting algorithms often still referred levenberg-marquardt method setting dampening often chosen according smallest negative eigenvalue using e.g. lanczos method ensure construction direction descent. vicinity saddle points methods modiﬁed also follow directions negative curvature alternative determine dampening factor trust-region method. trust region method ﬁxes determines approximately matching thus levenberg-marquardt algorithms implemented line-search methods trust-region methods depending computed. following discuss trust-region methods generally. line-search methods determine search direction ﬁrst identify good step size trust region methods converse specifying maximum step-size ﬁrst identiﬁng good search direction allows trust region methods make large steps close saddle points always yield descent directions.within trust-region replaced local approximation giving rise trust-region subproblem. trust-region algorithms objective function derivatives construct quadratic trust-region subproblem trust region. trust region ellipsoid radius scaling matrix around current parameter size trust region adapted course iterations. trust-region methods quadratic approximations local approximations e.g. radial basis functions trust-region methods solve subproblem exactly attracted saddle points limited small step sizes however quadratic problem usually difﬁcult solve exactly approximatively solved instead convex problems dogleg method employs linear combination gradient newton step applied. non-convex problems two-dimensional subspace minimization method used. two-dimensional subspace minimization method dampens hessian seen trust-region variant levenberg-marquardt method. dogleg two-dimensional subspace minimization method reduce trust-region subproblem dimensional problem renders computational cost determining update step given gradient independent number parameters underlying problem. feature makes particularly suited large-scale problems. however dampening hessian lead small step-sizes close saddle points state-of-the-art parameter inference toolboxes computational biology pesto meigo copasi feature local global methods include derivative-free gradient-based methods terms global derivative-free methods toolboxes provide interfaces particle swarm pattern search methods. terms local gradient-based methods toolboxes feature various ﬂavors trust-region algorithm. matlab toolboxes provide interfaces fmincon lsqnonlin routines matlab optimization toolbox. copasi provides implementation basic algorithms levenberg-marquardt truncated newton steepest descent. terms global optimization schemes toolboxes employ either multistart scatter search algorithms choosing particular optimization method plethora choices easy task. exhaustive studies compare full range different optimization methods large problems. studies gradient-based optimization algorithms perform best others also show derivative-free methods perform well general rigorous evaluation optimization algorithms highly involved small differences implementations various algorithms. example strscne resnei nlsol lsqnonlin fmincon implement trustregion algorithms even expert users difﬁcult pinpoint differences individual implementations. recent study suggests substantial differences efﬁciency various implementations trust-region algorithms identiﬁed lsqnonlin best performing algorithm even single implementation speciﬁcation hyper-parameters substantial impact performance. table implementations interfaces optimization methods popular computational biology toolboxes. toolboxes feature variants cited algorithms. entries names functions feature multiple different algorithms. many algorithms require user speciﬁcations technical parameters. finding good values hyperparameters challenging non-expert users default values work problems. researchers experts large number different optimization methods rigorous evaluation multiple different algorithms challenging. circumvent problem recent study suggested benchmark problems researchers invited evaluate algorithms. algorithms evaluated benchmark complementarily suggests construction statistical models assess performance methods effect hyper-parameters. different optimization algorithms outlined section rely evaluations objective function gradient even hessian. following sections discuss methods evaluate terms. objective function gradient typically available closed form computed numerically. large-scale models computational cost computing objective function gradient high makes parameter estimation computationally demanding. depending class employed model simulation algorithm computation time depend different features underlying model discuss detail following. timescales biochemical processes span multiple orders magnitude comprehensive models often cover large variety different biological processes particularly prone possess multiple timescales results stiffness corresponding odes stiffness equations typically depends choice parameters rarely possible assess stiffness priori. consequently always advisable implicit solvers adequately handle stiffness parameter inference stiff problems implicit differential equation solvers fully implicit runge-kutta solver family singly diagonally implicit runge-kutta solver family rosenbrock solver famsingle-step methods runge-kutta type solvers function depend previous values. implicit runge-kutta solvers system linear equations equations solved every iteration number stages particular property runge-kutta solver determines order method. multi-step methods function also depend previous values popular implementation multi-step method implicit linear multi-step backwards differentiation formula implemented cvodes solver every iteration solves equation form order method coefﬁcients determined every iteration. order step size determine local error numerical solution often chosen adaptively. implicit equation typically solved using newton’s method function depends thus consequently newton solver computes multiple solutions linear systems deﬁned jacobian right hand side differential equation every integration step. linear system equations contrast nx·s equations single-step methods computational cost solving linear system increases less strongly number state variables computation time method primarily depends factors evaluation time function jacobian usually scales linearly time solve linear systems deﬁned matrix typically symmetric neither positive negative deﬁnite. unstructured problems decomposition factorizes lower-triangular matrix upper-triangular matrix method choice solve linear system long stored memory. performing decomposition solution linear systems computed matrix multiplication. additional structure matrix exploited computational complexity matrix multiplication state-of-the-art algorithms increases least exponent respect thus dominates computation time sufﬁciently large models arising discretization partial differential equations jacobian usually brought banded form. banded matrices specialized solvers scale number offdiagonals jacobian developed unfortunately models biochemical reaction networks cannot generally brought banded structure. example polymerization reactions include dissociation monomers monomer species always inﬂuenced species number off-diagonals jacobian equal frequently occurring motifs feedback loops single highly interactive species also increase number necessary off-diagonals. alternative banded solvers sparse solvers introduced context circuit simulations sparse solvers computation time depends number non-zero entries scales number biochemical reactions. sparse solver relies approximate minimum degree ordering graph theoretical approach used minimize ﬁll-in matrices lu-decomposition currently formulas expected speedup general scaling respect non-zero entries exist. biochemical reaction networks application sparse solver seems reasonable rigorous evaluation scaling performed. toolboxes cvodes lsoda simulation contrast cvodes implements implicit solver lsoda dynamically switches explicit implicit solvers. best knowledge comparison lsoda cvodes ever published. however lsoda provide interface clark kent solver sparse solver thus might perform poorly large-scale problems sparsity structure. annotable difference toolboxes analytically compute jacobian right hand side provide solver. symbolic processing necessary sparse representation also likely beneﬁcial dense solvers. thus amici also general purpose simulation libraries systems biology allow sparse solver. explicit solvers linear system solved algorithm largely consists elementary operations efﬁciently parallelized gpus explicit solvers also parallelized solvers available however computational overhead parallelization usually high unless models several thousand state variables considered considered toolboxes allow deﬁnition models systems biology markup language also allows deﬁnition models terms biochemical reactions. copasi libroadrunner full support sbml features amici support subset sbml features. sparse numerical solvers used efﬁciently compute numerical solution required objective function evaluation. also used compute objective function gradients solution odes. several different gradient computation approaches exist following discuss three common approaches. providing accurate gradient objective function essential gradient-based methods constrained optimization problems gradient objective function computed based parametric derivative solution ode. derivatives often called sensitivities model. several approaches compute sensitivities models exist including ﬁnite differences well direct approach forward sensitivity analysis unit vector practice forward differences backward differences central differences widely used. evaluation require additional solutions model scaling ﬁnite differences respect number parameters also linear. thus forward sensitivity analysis requires computation solution system size model every gradient entry. consequently scaling respect number parameters linear linear scaling forward sensitivity analysis ﬁnite differences computationally prohibitively demanding large-scale models thousands parameters. alternative adjoint approach computes objective function gradient adjoint sensitivity analysis long deemed computationally efﬁcient systems many parameters research ﬁelds e.g. partial differential equation constrained optimization problems adjoint sensitivity analysis adopted past decades. contrast systems biology community isolated applications adjoint sensitivity analysis mathematics engineering community adjoint sensitivity analysis frequently used compute gradients functional respect parameters functional depends solution differential equation applications measurements continuous time assumed functional solution differential equation. however approach also applied discrete-time measurements contrast forward sensitivity analysis adjoint sensitivity analysis rely state sensitivities discrete-time measurements usual case systems computational biology adjoint state piece-wise continuous time deﬁned sequence backward differential equations adjoint state zero starting value trajectory adjoint state calculated backwards time last measurement initial time measurement time points adjoint state reinitialized usually results discontinuity starting value deﬁned adjoint state evolves backwards time next measurement point initial time reached. evolution governed time dependent linear solved dimensionality original model allows computation gradients cost roughly solutions original model. practice adjoint sensitivity approach almost constant scaling respect number parameters. many toolboxes rely ﬁnite differences compute gradients amici notable examples allow computation gradients sensitivity analysis amici allows adjoint sensitivity analysis. addition gradient newton-type methods also require hessian objective function. numerical evaluation hessian challenging dependence computational complexity number parameters order higher gradient computation time ﬁnite differences forward sensitivities scale quadratically number parameters adjoint sensitivities computation time depends linearly number parameters independent normally distributed measurement noise assumed known noise parameters optimization problem least squares type. structure exploited using gaussnewton type algorithms ignore second order partial derivatives hessian. respective approximations hessian coincide fisher information matrix respective parameter estimate. advantage approach computed cost gradient using forward sensitivity analysis. problems least-squares type quasi-newton methods broyden-fletchergoldfarb-shanno algorithm used. bfgs algorithm iteratively computes approximations hessian based updates derived outer products gradient objective function. resulting approximation guaranteed positive deﬁnite long wolfe condition satisﬁed every iteration initial approximation positive deﬁnite. previously discussed positive deﬁniteness ensures descent directions line-search methods generally lead small step sizes vicinity saddle points. symmetric rank algorithm addresses problem allowing negativeindeﬁnite approximations procedure facilitates application optimization methods avoid saddle points allowing directions negative curvature quasi-newton versions generally cheap compute require simple algebraic manipulations gradient. algorithms based limited memory variants l-bfgs l-sr applied machine learning problems millions parameters table implementations hessian computation methods popular computational biology toolboxes. bfgs list function option allows respective approximation. sensitivity equations ﬁnite differences gauss-newton applicable implementation methods computation hessians quite disparate across toolboxes amici toolbox allows sensitivity-based computation hessian. toolboxes fim/gauss-newton bfgs approximations. iterative approximations bfgs implemented part optimization algorithm possible methods. provides relatively ﬂexible implementation approximation exact hessian computations implementations usually transferable optimization methods. theory computation exact hessian adjoint sensitivity analysis forward sensitivity analysis scale linearly number parameters exact hessian used construct methods avoid saddle points practice effect using hessian efﬁciency respective optimization methods studied systems biology problems. problems thousands parameters computation challenging bfgs approximations become appealing. section split parameter inference problem three parts optimization simulation gradient computation discussed respective scaling properties. techniques generalize model analysis techniques require optimization gradient computation uncertainty analysis experimental design inference model structure. detailed discussion methods beyond scope book chapter following discuss inference model structure detail. many applications apparent biochemical species reactions necessary describe dynamics biochemical process. case structure model i.e. vector ﬁeld initial condition inferred experimental data. selection compromise goodness-of-ﬁt complexity. following concept occam’s razor tries control variability associated over-ﬁtting protecting bias associated underﬁtting. following formulate problem model structure inference. introduce discuss criteria select models candidate models describe approaches reduce number candidate models. outline scalability approaches computational complexity. given candidate models model inference model models describe data available generalize datasets choice model made several selection criteria differing among others asymptotic consistency asymptotic efﬁciency computational complexity. true model included candidate models consistent criterion asymptotically select true model probability efﬁcient criterion select model minimizes mean squared error prediction. concepts previous sections followed frequentist approach concepts presented section bayesian. approaches prior knowledge parameters incorporated posterior probability analyzed instead likelihood function. popular criterion bayes factor shown asymptotically consistent broad range models however case general models proofs asymptotic efﬁciency consistency available criteria presented section. bayes’ theorem yields posterior model probability bayes factor describes much likely data generated instead bayes factor often considered decisive rejecting model bayes factor intrinsically penalizes model complexity integrating whole parameter space model. bayes factors approximated laplace approximation computational complexity provides local approximation. enable precise computation bayes factors bridge sampling nested sampling thermodynamic integration related methods employed. approaches evaluate integral deﬁning marginal likelihood approaches require large-number function evaluations methods usually computationally demanding computational complexity highly problem-dependent. thus efﬁcient sampling methods required. high-dimensional computationally demanding problems calculation bayes factors might intractable computationally less expensive model selection criteria need employed. model selection criterion based instead marginal likelihood bayesian information criterion value model incorporating prior information parameters priors conceptually treated additional data points thus part likelihood still allow aic. also extensions criteria exist corrected provides correction ﬁnite sample sizes. also extended versions criteria developed however discussion beyond scope chapter. comparison nested models i.e. subset likelihood ratio test applied efﬁcient test likelihood ratio deﬁned rather simple compute among others available pesto previously discussed toolboxes review pesto also provides sampling methods employed calculate bayes factors parallel tempering. toolboxes employed computing bayes factors amongst others biobayes multinest toolbox models computing bayes’ factors computationally demanding compared optimization evaluation likelihood ratio. number candidate models large even evaluation become limiting optimization problems solved. nonnested models model selection criterion choice needs calculated model determine optimal model. section consider nested candidate models. case candidate models special case comprehensive model constructed ﬁxing subset parameters speciﬁc values remainder chapter assume split model parameters general parameters present models difference parameters encode nesting models. moreover without loss generality assumed corresponds simplest model corresponds complex model difference parameters could example kinetic rates hypothesized reactions scaling factors possibly cell-type condition-speciﬁc parameters settings yield total candidate models limited thus models high number parameters also high number nested models possible. high inference model parameters thus inference model structure challenging. fig. illustration methods model reduction. candidate models varying existence connections nodes total models least parameters. illustration forwardselection starting minimal model. ﬁrst iteration model selected second iteration model full model rejected based selection criteria. apply penalization minlp problem needs solved comprising continuous parameters discrete parameters {}nr penalization reduces number potential models preselected models increasing penalization thus forcing parameters zero. illustration model averaging. thickness arrows corresponds posterior probability akaike weight weight indicates contribution model averaged model properties. statistics step-wise regression often-used approach reduce number models need tested. comprises forward-selection backward-elimination combinations forward-selection bottom-up approach starts least complex model successively activates individual difference parameters sufﬁcient agreement experimental data achieved evaluated using model selection criterion contrast backward-elimination top-down approach starting complex model successively deactivating individual difference parameters required good data. forward-selection backward-elimination reduce number models need compared model selection criteria described however greedy approaches guarantee globally least complex candidate model explains data. penalization supports sparsity i.e. reduces model minimum number difference parameters used. models contain least parameters contributes changes obcomplexity. objective function divided two. jective function divided two. accordingly minimization provide best model according different information criteria. directly assess predictive power also determined using cross-validation. real-valued integer-valued {}nr. optimization done simultaneously parameters ηrq. objective function neither differentiable continuous respect thus gradients respect discrete parameters informative optimization. limits choice optimization algorithms derivative-free specialized gradient-based solvers misqp algorithm besides described commonly used methods approaches among others belief propagation iteratively reweighted least squares employed certain model assumptions. minlp resulting penalized objective functions comprehensive model estimated. this however results complex optimization problem suffers high dimensional parameter space. forced zero higher corresponding situations parameter effect. linear regression penalization commonly known lasso signal processing usually referred basis pursuit norm continuous differentiable zero. thus specialized solvers developed handle non-differentiability zero special case linear regression models convex. norm convex relaxation norm resulting objective function also convex. thus shown estimated parameters unique continuous respect moreover shown piecewise linear respect allows implementation efﬁciently compute solutions values models generally non-convex non-unique discontinuous challenging numerical methods. thus equation usually minimized varying penalization strengths reduced model candidates selected. computational complexity penalization depends number different penalization strengths used. higher number tested penalizations likely obtain globally optimal model lower number tested penalizations decreases computational effort. toolboxes implement methods allow simultaneous inference model parameters structure. meigo implements misqp algorithm implements penalization modiﬁcation fmincon routine large sets candidate models limited data frequently happens single model chosen model selection criterion. instead models plausible cannot rejected considered subsequent analysis. case model averaging employed predict behaviour process chapter provided overview methods parameter inference structure inference models biochemical systems. parameter inference discussed local optimization methods identiﬁed number stationary points determinant computational complexity. context local optimization identiﬁed gradient-based optimization methods suitable method computational complexity determining parameter update optimization gradient objective function independent number state variables number model parameters. still numerical optimization requires computation solution scales number molecular species computation respective derivatives scales number parameters. cases discussed scaling properties state-of-art algorithms identiﬁed adjoint sensitivity analysis sparse solvers suitable methods large-scale problems. believe challenges improve scalability parameter inference treatment stationary points objective function local minima saddle points. contrast deep learning problems dependence number stationary points underlying model remains poorly understood evaluated. local optimization methods account saddle points local minima developed lack implementations computational biology toolboxes evaluations models biochemical systems. structure inference large-scale models often also give rise large different model candidates. many model comparison criteria require parameter inference model candidates rarely feasible number model candidates high. discussed penalization based approach allows simultaneous inference model parameters structure. believe challenges improve scalability structure inference treatment non-differentiability norm prohibits application standard gradient-based optimization algorithms. methods iteratively reweighted least-squares developed decades adopted models. anticipate that advent whole cell models large-scale models demand scalable methods drastically increase coming years. however already medium-scale models much commonplace parameter inference particular structure inference challenging. accordingly growing demand novel methods better scaling properties.", "year": "2017"}