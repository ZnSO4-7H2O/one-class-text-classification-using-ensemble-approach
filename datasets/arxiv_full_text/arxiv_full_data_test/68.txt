{"title": "The Neural Particle Filter", "tag": "q-bio", "abstract": " The robust estimation of dynamically changing features, such as the position of prey, is one of the hallmarks of perception. On an abstract, algorithmic level, nonlinear Bayesian filtering, i.e. the estimation of temporally changing signals based on the history of observations, provides a mathematical framework for dynamic perception in real time. Since the general, nonlinear filtering problem is analytically intractable, particle filters are considered among the most powerful approaches to approximating the solution numerically. Yet, these algorithms prevalently rely on importance weights, and thus it remains an unresolved question how the brain could implement such an inference strategy with a neuronal population. Here, we propose the Neural Particle Filter (NPF), a weight-less particle filter that can be interpreted as the neuronal dynamics of a recurrently connected neural network that receives feed-forward input from sensory neurons and represents the posterior probability distribution in terms of samples. Specifically, this algorithm bridges the gap between the computational task of online state estimation and an implementation that allows networks of neurons in the brain to perform nonlinear Bayesian filtering. The model captures not only the properties of temporal and multisensory integration according to Bayesian statistics, but also allows online learning with a maximum likelihood approach. With an example from multisensory integration, we demonstrate that the numerical performance of the model is adequate to account for both filtering and identification problems. Due to the weightless approach, our algorithm alleviates the 'curse of dimensionality' and thus outperforms conventional, weighted particle filters in higher dimensions for a limited number of particles. ", "text": "institute neuroinformatics university zurich zurich zurich switzerland institute software engineering theoretical computer science technische universit¨at berlin berlin germany robust estimation dynamically changing features position prey hallmarks perception. abstract algorithmic level nonlinear bayesian filtering i.e. estimation temporally changing signals based history observations provides mathematical framework dynamic perception real time. since general nonlinear filtering problem analytically intractable particle filters considered among powerful approaches approximating solution numerically. algorithms prevalently rely importance weights thus remains unresolved question brain could implement inference strategy neuronal population. here propose neural particle filter weight-less particle filter interpreted neuronal dynamics recurrently connected neural network receives feed-forward input sensory neurons represents posterior probability distribution terms samples. specifically algorithm bridges computational task online state estimation implementation allows networks neurons brain perform nonlinear bayesian filtering. model captures properties temporal multisensory integration according bayesian statistics also allows online learning maximum likelihood approach. example multisensory integration demonstrate numerical performance model adequate account filtering identification problems. weightless approach algorithm alleviates ’curse dimensionality’ thus outperforms conventional weighted particle filters higher dimensions limited number particles. every brain facing challenge making sense rich dynamical stream sensory inputs. inputs often ambiguous noisy sometimes even conflicting. nevertheless able make sense surrounding naturally points important question estimates real-world variables perceptive input e.g. position velocity object formed. further unknown computational task real-time state estimation implemented realistic neuronal architecture. here propose algorithm neural particle filter performs state estimation captures essential properties perception takes account prior knowledge environment weights different sensory modalities according reliability able dynamically adapt last decade increasing number studies stated brain performs probabilistic inference perceptual tasks bayesian inference perception relies noisy incomplete data needs integrated across multiple sensory modalities weighted according sensory reliability. addition perception makes strong statistical regularities objects environment forming prior beliefs world. since environment fundamentally dynamic ability adapt changes real time essential perception. bayesian brain hypothesis supported ample experimental evidence ranging psychophysical findings neuronal recordings line bayesian computation. however studies concerned theory perception consider fairly simple tasks observations created either static hidden variables hidden variables discrete state-space underlying dynamics considered linear online history observations bayesian inference commonly referred ‘filtering’. general nonlinear bayesian filtering challenging task even without imperative plausible implementation neuronal architecture. prior distribution gaussian noisy observations depend linearly hidden states inference problem solved kalman filter received substantial attention signal processing community turns increasing importance neuroscientific phenomenological modeling e.g. sensorimotor integration task estimating motor disturbances adaptive gain solutions general nonlinear i.e. non-gaussian filtering problem analytically intractable thus approximated. sampling-based approaches proven powerful tool solve nonlinear filtering problem numerically. principle enable posterior distribution represented accuracy depends number samples. hand called particle methods well suited dynamical priors suffer high dimensions degeneracy importance weights still unclear implement inference scheme neuronal network. hand langevin sampling related techniques ‘fast sampler’ provide promising ground biologically plausible implementation neural synaptic sampling restricted static generative models. could perform filtering noisy sensory stimuli considering marr’s three levels computational level algorithmic representational level implementation level. first level computational task dynamical state estimation context continuous-time continuous-state nonlinear filtering theory. motivated rigorous mathematical theory propose weight-less particle filter neural particle filter approximates posterior time step sampling algorithm tuned maximum likelihood learning thus allows rigorous corrections algorithmic ansatz well learning model parameters. exhibits properties considered crucial perception. implementation level interpret biologically plausible neuronal dynamics identify particle states activities task-specific neurons. results presenting subdivided parts first introduce neural particle filter conceptual result. first part cover first marr’s three levels namely computational level generative model layout task description ii.) algorithmic level outlines choice representation approximate solution nonlinear filtering problem based representation. second part demonstrate properties illustrate might serve model neuronal dynamics involved perception. nonlinear deterministic drift function stochastic diffusion governed uncorrelated brownian motion process noise covariance represent sensory input. observation dynamics modeled function brownian motion diffusion modulated sensory noise covariance observation dynamics task solved kalman-bucy filter continuous-time version well-known kalman filter. however solution nonlinear filtering problem general analytically intractable suffers so-called closure problem therefore introducing suitable approximation inevitable step approaching nonlinear filtering problem. filtering algorithms representing posterior sampling-based manner commonly referred particle filters. standard particle filters update rules trajectories despite asymptotic convergence true posterior infinite number particles approach disadvantages first finds numerically finite number time-steps particle weights decay zero depletes number effective samples. weight decay degeneration undesirable trait weighted particle methods general. stated convergence theorem upper bound divergence true posterior posterior estimated weighted particle system function time hence might growing weight decay. second problem exacerbated number dimensions hidden state large. case number particles needed good numerical performance grows exponentially number dimension variant ‘curse dimensionality’ representation posterior received much attention although experimental support considered relevant according neural sampling hypothesis therefore would like explore approach further. overcome difficulties encountered weighted approaches consider particle ansatz serves sampling-based approximation nonlinear filtering problem consider stochastic processes particle true posterior every time thus expectations precisely dynamics first posterior moment shares important properties classical filtering methods first governed dynamics hidden process correction proportional so-called innovation term innovation term compares sensory input current prediction according single particle position thus seen predictive error signal second gain matrix determines emphasis laid information observations dyt. conceptually similar kalman gain linear model adjusts according reliability single multiple observations. consecutive section identify particles neuronal activities call neural particle filter. though name similar confused ‘neural filtering’ approach unsupervised learning algorithm artificial neural network. serves measure peakedness likelihood. decoding weight large dynamics entirely determined innovation term inter-particle variability governed diffusion term negligible. therefore limit deterministic observation limit single sample suffices represent posterior. hand decoding weight zero information disregarded sample evolves like i.i.d. process case resulting probability distribution simply equals stationary prior distribution motivated mean dynamics formal solution. gain adjusts according observation noise well spatial ambiguity measured empirical i.e. instantaneously estimated particle positions covariance state observation function although choice rather heuristic achieves numerical performance comparable standard particle filter demonstrated below moreover straightforward implement empirically estimate covariance particle positions. turn computed directly approximated filtering distribution itself. shown maximizing likelihood equivalent minimizing prediction error continuous time further model parameters eqs. also decoding parameters i.e. components decoding weight gain matrix learned maximum likelihood approach instead setting according empirical estimate particle positions. alternative corrects heuristic ansatz equation determining decoding weights rigorously. fact shown parameter learning maximum likelihood approach able make even poor filtering ansatz setting parameters accordingly section computational task context perception base implementation algorithm neuronal architecture. simple example illustrate algorithm captures following properties perception relies noisy incomplete sensory data uses prior knowledge dynamic structure environment efficiently combines information several sensory modalities dynamically adapt changes environment. independent brownian motions model noise sensory channels making conditionally independent. nonlinearity auditory channel motivated fact sound localization depends interaural difference resembles sigmoid example. order localize frog perform task nonlinear filtering compute sensory streams. note nonlinear dynamics hidden observation processes example analytically intractable thus requires approximation. posterior distribution thereby acting weight-less particle filter successfully tracks position insect state estimate read population averaging activities filtering neurons simple state estimation interested first moment. particularly sampling-based approximation posterior allows convenient estimation relevant quantities. example frog might want know branch insect sitting order catch easily. frog could directly deduce certainty level left right branch respectively counting number neurons within certain activity range. approximated samples correspond neuronal activities. even though approximated moments exact overall posterior shape captured considerable extent. nonlinearities proposed model therefore superior models relying approximation first moments distribution. instance extended kalman filter definition figure model perception multisensory perception. frog filtering order dynamically track position insect switching branches. model behavior stochastic dynamics gives rise bimodal prior distribution order track insect frog make prior optimally combine noisy input sensory modalities tracking simulation filtering neurons sensory noise upper panel shows true trajectory insect particle densities. regions dotted black lines denote branches certainty levels certainty levels middle panel correspond relative number particles whose states within branches. time sensory gains lower panel computed according eqs. performance terms time-averaged figure estimation posterior variance single-cue observation. simulations correspond example model visual auditory i.e. generative model eqs. respectively. gain tuned according covt according gradient ascent likelihood respectively benchmarking standard performance nearly indistinguishable addition compare performance ekf. posterior variance average exhibits bias bias compared here solid lines integration decoding weights gain factors exemplary essential multisensory integration. balance relative effects sensory modalities prior dynamics filtering neurons thus quantify reliability sensory channels. here consider cov)σ− posterior variances covariances estimated empirically neuronal activity distribution. gains adjust according sensory noise levels spatial ambiguity evoked sigmoid observation function auditory channel particular gains become large channel particularly reliable extreme cases dominate dynamics filtering neurons corresponding deterministic observation limit discussed earlier. appropriate weighting sensory information allows neurons solve filtering task near-optimally comparable standard reflected simulation results fig. adapting changes insect could access generative model parameters i.e. knew prior dynamics insect aware sensory percepts generated true state insect. also knowledge model parameters crucial determining sensory weights external world i.e. model parameters change time successful perception adapt accordingly i.e. model parameters adjusted methods appendix details). time frog relies visual channel addition tracking insect also learn generative factor function relates position insect visual input. simultaneously also learns gain implicitly estimate reliability visual input. figure shows identification problem solved efficiently gradually approaches benchmark estimate parameters accurate find true wide range observation noise factor tend exhibit slight negative bias observation noise still stay %-region true generative weight. findings suggest learning rule refer ‘hebbian’ reasons illustrate below leads estimator generative weight slightly negatively biased across initial conditions absolute value bias decreases smaller observation noise learning rule becomes exact moreover bias seem affect filtering activities represent samples posterior line neural sampling hypothesis thus analog neuronal activities identified continuous particle state instance terms instantaneous firing rate. internal dynamics self interaction filtering neurons governed nonlinear function incorporates prior knowledge state-dependent leak. addition recurrently connected populations novelty neurons feedforward synaptic weights feedback connections whose strength governed nonlinearity dynamics novelty neurons governed innovation term input network consider neuronal population whose rates evoked underlying hidden stimulus generative dynamics figure model parameters learned stochastic gradient ascent likelihood. simulations shown correspond example model visual i.e. generative model eqs. sensory gain equation filtering neurons tuned according maximum likelihood decoding weight learned either maximum likelihood hebbian learning rule valid approximation small sensory noise. benchmark standard particle filter true model parameters. parameters learning starts filtering online learning generative approaches true weight sensory gain sensory noise value resembles standard bias estimated generative weight filtering performance affected bias hebbian learning interpretation corresponds matrix synaptic weights connects novelty neurons filtering neurons function linear denotes matrix feedback weights connects filtering neurons novelty neurons. general learning rules weights local i.e. rely state whole network however deterministic limit learning rule generative weight matrix replaced learning rule hebbian local relies multiplication prepostsynaptic activity further small observation noise replaced constant matrix without affecting filtering performance. therefore least limit network presented fig. implementable neuronal dynamics recurrent network local hebbian synaptic plasticity. alleviates ‘curse dimensionality’. example frog estimate single hidden state namely one-dimensional position insect. realistic setting large number hidden states ranging position object three-dimensional space relative presence features making visual scene. therefore filtering algorithm employed neuronal population brain economical resources i.e. number neurons needed solve filtering task certain performance level scale well number hidden variables. particular decoding weight determined maximum likelihood able solve filtering task higher dimensions limited number filtering neurons. thus alleviates curse dimensionality would devastating realistic implementation. want illustrate point numerically example comparing filtering performance terms small number particles standard general find performs well even limited number samples. allow learn decoding weight matrix maximum likelihood filter able solve filtering task single particle almost good particles. hand single-particle scenario using cov)σ− particle filter unsurprisingly exhibit corresponds independent trajectory prior. upon increasing number samples outperforms again owing approximations employed however crossover-point tends move towards larger number particles higher dimensions also surprising trajectories evolve according prior higher dimensions particles correct spatial domain fitting figure alleviates ‘curse dimensionality’. example nonlinear hidden dynamics linear observation dynamics. solid lines hidden dimensions dashed lines hidden dimensions comparison. error indicates normalized trace prior variance make error dimension-free. black marker indicates approximate cross-over point neural filter gain matrix obtained maximum likelihood particle filter hidden dimensions tends move towards larger particle numbers dimensionality grows. note number filtering neurons equal mainly direct influence observations trajectories samples correspond neuronal activities. neuronal activity seen mini approximation true posterior terms δ-function approximation becomes exact small observation noise course larger becomes less true posterior resembles δ-function particles needed account shape general. paper formulated computational task nonlinear bayesian filtering. based theory nonlinear filtering proposed itˆo posterior process derived learning rule parameters filter well generative weights underlying generative model. thus forward algorithm allows approximate filtering continuous time continuous-valued hidden processes. algorithm allows hidden dynamics well observation dynamics nonlinear thus model flexible representing large class general signal emission statistics. sampling-based framework central aspect well line ‘neural sampling hypothesis’ neural filter equation propose particularly suited model perception phenomenologically shares important properties perception. first perception relies noisy incomplete sensory data uses make sense world model reflected inferring hidden state variable. second prior dynamics directly enter neuronal dynamics prior knowledge note weight decay iterative re-weighting rather simple consequence importance sampling high dimensions illustrating ‘curse dimensionality’ weighted particle methods. environment automatically incorporated principle learned. third information different sensory modalities efficiently combined weighted input population filtering neurons. lastly perception adapt changes environment taken account dynamical gain online parameter updates. implementation biologically realistic architecture imposes constraints algorithm well interpret elements structure. always aware constraints describe highly simplified version real biological underpinning successfully applied network models qualitatively understand core computations brain first neurons communicate among discrete spikes i.e. digital signal. contrast term ‘neuronal activity’ denote analog quantity. cases instance large number neurons ‘analog quantity’ instance correspond instantaneous firing rate. take account ‘negative’ neuronal activities could also consider deviations baseline firing rate membrane potential neuron logarithm firing rate. secondly computations neurons performed weighted inputs cells connected inputs transformation presynaptic activity. third hallmark neural circuits synaptic connectivity neurons connection strength quantified connective weights. synaptic weights modified learning rules simple case local i.e. depend prepostsynaptic neuronal activity. learning rules model general local fact filter neuron know state every filter neuron and/or novelty neuron. apart that parameters learned online clear filter derivative implemented network. however shown learning rules model become hebbian local small observation noise making learning rules biologically plausible limit. lastly number neurons brain finite computations clearly rely finite number neurons fact taking account representing probability distributions samples. requirements proposed network structure consider neuronal dynamics filtering line standard network models. filtering algorithm continuous-time continuous-state generative model nonlinear hidden observation dynamics. filtering algorithms based linear generative models subject extensive research. mainly study analytical solution problem kalman filter implemented neurons however posterior resulting kalman filter always gaussian highly restrictive properly reflect activity distributions observed neurons unlike various extensions kalman filter unscented kalman filter applied nonlinear systems restricted approximate posterior gaussian parametrized first second moment. rather nonlinearity network dynamics represent probability distribution given time step. important aspect work sampling-based representation probability distributions whereby activity neuron considered single sample. main competing proposals probability distributions underlying bayesian computations might represented brain. firstly suggested probability distributions expressed probabilistic population codes neuron represents state encoded random variable activities proportional probability corresponding state. filtering approaches based population codes neuronal activity directly relates posterior posterior explored literature large models e.g. representation neurons directly correspond parameters distribution thus critical factor accuracy number neurons. further suffer ‘curse dimensionality’ multimodal distribution. second proposal called neural sampling hypothesis uses inference scheme activity neuron represents sample underlying probability distribution. since filtering algorithm based unweighted samples findings line advantages sampling-based representation outlined represent distribution without need parametric form mitigates ‘curse dimensionality’ well-suited learning. filtering approaches implementing markov-chain monte carlo algorithms received attention lately since rely discrete state space assume different coding scheme suggested advantages listed necessarily emerge models. filtering approaches. ansatz seen particle filter particles carry weight which therefore avoids numerical pitfalls weight degeneracy. problem notorious standard mcmc particle filters becomes even severe number hidden dimensions grows. curse dimensionality i.e. exponential growth approximation error dimension underlying model inevitable nuisance standard mcmc approaches. tricks deal problems instance particle resampling using refined propagator particles neither solution able properly circumvent weight decay general. moreover currently proposed implementation weighted particle methods neural architecture. instance need renormalize weights time step introduces coupling particles trajectories independent weights certainly not. hand neural filter relying importance weights first place suffer numerical pitfalls related implementational issues. curse dimensionality seems avoided least mitigated fact observations directly enter particle trajectories. however particles following dynamics completely independent either. coupling particles mediated decoding weight matrix whose learning rule influenced particles. could avoided fixing constant consistent observational noise e.g. learning. even numerically think ‘real’ decoding weight filtering performance seriously affected particle trajectories effectively decoupled. importance weights derived rigorously mathematical filtering theory approaches so-called feedback particle filter based similar particle trajectories propose shown underlying distribution feedback particle filter evolves exactly according kushner equation whereas approach merely approximates however computation gain function feedback particle filter needs access full filtering distribution itself order avoid numerical issues algorithm relies regularization scheme would hard justify biologically. though formally multivariate version feedback particle filter exists gain function cannot solved closed form. neural filter though exact particle algorithm overcomes drawback readily applicable higher dimensions dynamics according structurally similar proposed model represent neuronal activities terms instantaneous firing rate approximation spiking nature biological neurons. predictive coding model central role assigned predictive error signal compared dynamics novelty neurons novelty signal model. accordingly equations neuronal dynamics learning generative weight small observation noise limit structurally similar. however model generalizes sense allow dynamics prior directly reflected dynamics filtering neurons. three central aspects work namely sampling-based representation filtering algorithm adaptive gain structure recurrent neuronal network result following implications neuronal network first implication follows directly sampling-based representation namely robustness neuronal failure. example distribution represented particles removing significantly decrease ability particles represent probability distribution. extreme case could consider single neuron represent whole distribution given activity state take values range hidden state allow sample distribution time. apart that seen numerically ability perform filtering reduced number particles affected particle removal large extent either least particular algorithm propose. however degree plasticity rewiring would necessary order read expectations decimated neuronal population. hand parametric representations neuron determines height particular tuning curve assigned actually rely tuning curves cover space densely neuronal failure devastating neuron breaks down particular point state space cannot represented directly anymore. clearly single neuron would never able account distribution resembling tuning curve. decoding weight determines emphasis laid observations. demonstrated gain model increases sensory reliability according sensory noise ambiguity input generation putting emphasis observations versus internal model. absence observations observation noise maximal neuronal dynamics follow hidden state comprises internal model world. availability observations sensory reliability naturally increases variability across samples decrease dynamics influenced stimulus gain indeed found spontaneous neuronal activities relate prior expectations stimulus visual cortex further shown inter-trial variability neuronal responses declines upon stimulus onset experimental findings nicely line theoretical predictions. neural particle filter come algorithm allows neurons perform nonlinear bayesian filtering sampling-based manner. specifically neuronal implementation based network recurrently connected analog neurons whose dynamics governed algorithm. future work biological plausibility recurrent network model addressed. first observing learning rules general i.e. nonvanishing observation noise fulfill requirement locality needed biologically plausible learning rule model could enhanced individual filtering neurons obey different rather identical dynamics. could instance consider different subnetworks locally determine weights subnetworks possibly taking account global modulation factor. approach would also effectively decouple particles smaller timescale. second including theory filtering identification point processes algorithm could extended spike-based representation accounted for. choice inspired dynamics formal solution. particular consider dynamics first posterior moment comparison equation directly motivates particular choice decoding weights. thereby covariance covt estimated empirically samples neural filter tuned maximum likelihood. learning rule given corresponds online update decoding weight time step. peculiar choice decoding rather generative model parameter learned illustrating inference learning intertwined fact actually possible rephrase filtering problem terms learning problem giving ansatz decoder whose parameters learned perform filtering consider rather extreme case here. examples fig. employ linear sigmoid observation dynamics thereby simulating multisensory integration model plots fig. sensory modality row. observation noise varied dimension independent dimensions corresponds chosen unit matrix. stationary distribution dynamics thus multimodal distribution peaks dimensions. linear generative function given rotation matrix rotating hidden state vector denotes true benchmark value. unless stated otherwise mses normalized respect trace stationary prior variance σprior make performance comparable needed independent number hidden dimensions. throughout simulations. simulations time steps corresponding time units. unless stated otherwise mses biases averaged last time units equaling time steps. would like stress generative model chose examples nonlinear prior well observation dynamics. implies closed form solution problem exist thus approximations employed. therefore assessing performance compare approximate filtering algorithms widely used approximating posterior distribution nonlinear filtering problems standard particle filter continuous-time version extended kalman filter information algorithms appendix. continuous-time continuous state-space generative model given eqs. learning mathematical literature commonly referred ‘system identification’ tough problem hardly looked fact reference gives explicit cost function identification setting technical report moura mitter based change probability measure propose cost function equivalent likelihood parameter learning implemented maximizing likelihood gradient ascent respect model parameters giving rise following online learning rule parameters function thus take account implicit change posterior distribution called filter derivative respect model parameters. filter derivative general hard even impossible compute analytically many identification problems deal estimating model make approximated posterior dynamics order derive dynamics filter derivative parameter learning. equation approximated taking samples equation order express posterior estimates work interested learning decoding weight matrix linear observation dynamics learning generative matrix respectively. resulting learning rules components decoding weight matrix learning rate given fij) denotes jacobian nonlinear hidden dynamics denotes unit vector i-th direction. implies that take plastic decoding weight matrix learned observations become available least three equations needed infer hidden state time step first evolve states filter neurons second eqs. update weights filter equation. addition term proportional filter derivative learning rule contains second term emerges explicit dependence likelihood generative weight. filter derivatives given bias sampling derivation learning rules used sampling-based representation approximated posterior order estimate log-likelihood gradients. introduces bias estimations correct computing parameter estimates. unfortunately bias general analytically accessible least linear generative model shown vanishes approximation small observation noise learning rules obtain decoding weights generative weights local implying weights computed knowing state filter neuron time. however small observation noise learning rule generative weight approximated local learning rule hebbian structure. first neglect filter derivative decays zero fast limit decoding weight generally large thus first term vanishes. second limit posterior approach δ-distribution around true hidden state approximated posterior approximate learning rule", "year": "2015"}