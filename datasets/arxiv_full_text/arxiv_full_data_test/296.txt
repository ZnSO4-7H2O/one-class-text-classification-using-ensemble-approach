{"title": "A dynamic connectome supports the emergence of stable computational  function of neural circuits through reward-based learning", "tag": "q-bio", "abstract": " Synaptic connections between neurons in the brain are dynamic because of continuously ongoing spine dynamics, axonal sprouting, and other processes. In fact, it was recently shown that the spontaneous synapse-autonomous component of spine dynamics is at least as large as the component that depends on the history of pre- and postsynaptic neural activity. These data are inconsistent with common models for network plasticity, and raise the questions how neural circuits can maintain a stable computational function in spite of these continuously ongoing processes, and what functional uses these ongoing processes might have. Here, we present a rigorous theoretical framework for these seemingly stochastic spine dynamics and rewiring processes in the context of reward-based learning tasks. We show that spontaneous synapse-autonomous processes, in combination with reward signals such as dopamine, can explain the capability of networks of neurons in the brain to configure themselves for specific computational tasks, and to compensate automatically for later changes in the network or task. Furthermore we show theoretically and through computer simulations that stable computational performance is compatible with continuously ongoing synapse-autonomous changes. After reaching good computational performance it causes primarily a slow drift of network architecture and dynamics in task-irrelevant dimensions, as observed for neural activity in motor cortex and other areas. On the more abstract level of reinforcement learning the resulting model gives rise to an understanding of reward-driven network plasticity as continuous sampling of network configurations. ", "text": "functional synapses time point vector encodes current values potential synaptic connections stochastic dynamics high-dimensional vector deﬁnes markov chain inhibitory neurons. potential synaptic connections excitatory neurons shown keep ﬁgure uncluttered. synaptic connections inhibitory neurons assumed ﬁxed simplicity. reward landscape parameters several local optima. z-amplitude color indicate expected reward given parameters example prior prefers small values posterior distribution results product prior panel expected discounted reward panel illustration dynamic forces sampling step sampling posterior distribution. deterministic term consists ﬁrst terms directed single trajectory policy sampling posterior distribution panel starting black dot. parameter vector ﬂuctuates diﬀerent solutions moves primarily along task-irrelevant dimension exponential function turns useful relating dynamics experimental data dynamics synaptic weights. volume image brightness ca-imaging dendritic chosen proportional logarithm justiﬁed model spontaneous dynamics ornstein-uhlenbeck process. done model explain demonstrate fig. logarithmic transformation also ensures additive increments yield multiplicative updates observed experimentally altogether model needs create dynamics consistent experimental data spontaneous spine dynamics case also consistent rules rewardmodulated synaptic plasticity suggests look plasticity rules process mentioned above. stochastic term inﬁnitesimal step random walk precisely wiener process wiener process standard model brownian motion physics. presence stochastic term makes unrealistic expect converges particular value dynamics deﬁned fact contrast many standard diﬀerential equations highly rewarded network conﬁgurations. observation ﬁrst term right-hand-side written potential synaptic connections form arbitrary given distribution denotes proportionality positive normalizing constant. encode structural priors network scaﬀold example encode preference sparsely connected networks. happens mass near fig. illustration. could also convey genetically encoded previously learnt information preference strong synaptic sure term vanishes potential synaptic connections currently functional i.e. chooses gaussian distribution prior dynamics amounts ornstein-uhlenbeck process. currently generally accepted stochastic dynamics used simulations gaussian distribution prefers small nonzero weights prior hence model consistent previous ornstein-uhlenbeck models spontaneous spine dynamics. figure reward-based routing input patterns. illustration network scaﬀold. population model msns receives input excitatory input neurons model cortical neurons. potential synaptic connections populations neurons subject reward-based synaptic sampling. addition ﬁxed lateral connections provided recurrent inhibitory input msns. caption continued next page... caption fig. continued msns divided groups projecting exclusively target areas reward delivered whenever network managed route input pattern primarily group msns projected target area illustration model spine dynamics. five potential synaptic connections diﬀerent states shown. synaptic spines represented circular volumes diameters proportional functional connections assuming linear correlation spine-head volume synaptic eﬃcacy dynamics weights log-scale potential synaptic connections activity-dependent equal zero). consistent experimental date dynamics case consistent ornstein-uhlenbeck process logarithmic scale. weight values plotted relative initial value time dynamics model synapse reward-modulated stdp pairing protocol applied. reward delivery repeated ﬁring presynaptic neuron postsynaptic neuron resulted strong weight increase eﬀect reduced without reward prevented completely presynaptic stimulus applied. values represent percentage weight changes relative pairing onset time compare fig. dependence resulting changes synaptic weights model function delay reward delivery. gray shaded rectangle indicates time window stdp pairing application. reward delays denote time paring reward onset. compare figure average reward achieved network increased quickly learning according synaptic parameters kept changing throughout experiment magnitude change synaptic parameter vector change peaks onset learning remains high even stable performance reached. spiking activity network learning. activities randomly selected input neurons msns shown. salient input neurons highlighted. neurons learnt higher rate input pattern corresponds target area projecting. bottom reward delivered network. dynamics network rewiring throughout learning. snapshots network conﬁgurations times indicated plots shown. gray lines indicate active connections neurons; connections present preceding snapshot highlighted green. output neurons subsets input neurons strongly pattern shown numbers denote total counts functional connections pools. connectivity initially dense rapidly restructured became sparser. rewiring took place time throughout learning. analysis random exploration task-irrelevant dimensions parameter space. projection parameter vector dpca components best explain variance average reward. explains reward variance single trajectory high-dimensional synaptic parameter vector hours learning projected onto shown. amplitude y-axis denotes estimated average reward converging region high reward network continues explore task-irrelevant dimensions expected discounted reward increased parameter dynamics follows gradient i.e. small learning rate. parameter dynamics recover case given solely second term parenthesis activity patterns upstream neurons cortex routed diﬀerent ensembles msns thereby diﬀerent downstream targets msns assumed upstream activity pattern particular subset upstream neurons active hence routing task amounted routing synaptic input msns project downstream neuron downstream target areas cortical input modeled poisson spike trains input neurons instantaneous rates deﬁned prototype rate patterns fig. task learn activate t-projecting neurons silence t-projecting neurons whenever pattern presented cortical input. pattern activation reversed activate t-projecting neurons silence projecting desired function deﬁned reward signal proportional ratio mean ﬁring rate msns projecting figure reward-based self-conﬁguration compensation capability recurrent neural network. network scaﬀold task schematic. symbol convention fig. recurrent network scaﬀold excitatory inhibitory neurons subset excitatory neurons received input aﬀerent excitatory neurons caption continued next page... caption fig. continued remaining excitatory neurons pools randomly selected control lever movement bottom inset stereotypical movement generated receive reward. spiking activity network learning onset hours learning. activities random subsets neurons populations shown bottom lever position inferred neural activity pools rewards indicated bars. gray shaded areas indicate presentation. task performance quantiﬁed average time presentation onset movement completion. network able solve task less seconds average hours learning. task change introduced time quickly compensated network. using simpliﬁed version learning rule re-introduction non-functional potential connections approximated using exponentially distributed waiting times yielded similar results connectome kept ﬁxed task change performance signiﬁcantly worse trial-averaged network activity lever movements activity traces aligned movement onsets y-axis trial-averaged activity plots sorted time highest ﬁring rate within movement various times learning sorting ﬁrst second plot based activity third fourth ﬁfth resorted activity network activity clearly restructured learning particularly stereotypical assemblies sharp upward movements. bottom average lever movement individual movements turnover synaptic connections experiment shown y-axis clipped turnover rate ﬁrst turnover rate observed task change time eﬀect forgetting parameter diﬀusion simulated days. application reward stopped hours network learned reliably solve task. parameters subsequently continue evolve according onset forgetting observed simple consolidation mechanism triggered days reliably prevents forgetting. histograms time intervals disappearance reappearance synapses exact approximate learning rule. relative fraction potential synaptic connections stably non-functional transiently decaying transiently emerging stably function re-learning phase experiment shown random subset parameters plot suggests continuing dynamics task-irrelevant dimensions learning goal reached function neuron pools switched synaptic parameters migrated region. plots show means independent runs observed wondered whether simple consolidation mechanism could prevent forgetting model. test used prior distribution stabilize synaptic parameters. simulated days mean prior current value synaptic parameters reduced postsynaptic density sizes spine volumes cisd synapses weakly correlated correlation coeﬃcients thus even conservative estimate corrects possible figure contribution spontaneous neural activity-dependent processes synaptic dynamics evolution synaptic weights plotted time pair synapses non-ci synapses temperature pearson’s correlation coeﬃcient computed synaptic weights non-ci synapses network learning fig. synapses weakly signiﬁcantly stronger correlated non-ci synapses. impact correlation synapses learning performance represents averaged data particular temperature value indicated color. values caption continued next page... caption fig. continued values proportional small vertical bars color bar. performance measured hours learning experiment fig. network changed completely good performance achieved range temperature values high values impaired learning. means s.e.m. independent trials shown. synaptic weights pairs synapses emerged pearson’s correlation comparable experimental data fig. estimated contributions activity history dependent spontaneous synapse-autonomous neuron-wide processes synaptic dynamics resulting fractions similar experimental data fig. evolution learning performance total number active synaptic connections diﬀerent temperatures compensation task perturbation signiﬁcantly faster higher temperatures. temperatures larger prevented compensation. overall number synapses decreasing temperatures increasing demands formalized expected discounted reward prior multiplicative manner priors represent structural constraints well results preceding learning experiences innate programs. since model samples distribution proportional discount rate simplicity equal paper. time constant immediately related experimentally studied time window eligibility trace inﬂuence dopamine synaptic plasticity property true general model reward-gated network plasticity introducing binary random variable represents currently expected future discounted reward probabilistic manner. likelihood denotes constant assures correctly normalized probability distribution. thus reward-based network optimization formalized maximizing likelihood optimization amounts theoretical perspective learning posterior distribution bayes’ rule deﬁned therefore learning goal formalized compact form evaluating posterior distribution network parameters constraint abstract learning goal achieved. suitable normalization constant temperature parameter controls sharpness given original posterior emphasizes parameter values high probability posterior leads parameter distributions dynamics converges well-deﬁned unique stationary distribution limit large practical relevance so-called burn-in time distribution parameters close output spike train neuron deﬁned dirac delta pulses positioned spike times potential synaptic connections also indexed arbitrary order integers ksyn ksyn denotes number potential synaptic connections network. denote prei posti index prepostsynaptic neuron synapse respectively unambiguously speciﬁes connectivity network. further deﬁne synk index synapses project neuron note indexing scheme allows include multiple synaptic connections given pair neurons. included denotes slowly changing bias potential neuron yprei denotes trace postsynaptic potentials neuron prei leaves postsynaptic synapses time precisely deﬁned yprei zprei given spike trains ﬁltered kernel bias potential implements slow adaptation mechanism intrinsic excitability ensures output rate neuron stays near ﬁring threshold neuron maintains time constant adaptation mechanism desired output rate neuron. simulations bias potential initialized followed dynamics given found regularization signiﬁcantly increased performance furthermore denote fposti ﬁring rate neuron postsynaptic synapse stated otherwise refractory time tref addition subset neurons clamped given ﬁring rates input neurons given arbitrary function. denote spike train neurons network input. time constant eligibility trace. recall prei denotes index presynaptic neuron posti index postsynaptic neuron synapse zposti denotes postsynaptic spike train fposti denotes instantaneous ﬁring rate postsynaptic neuron yprei denotes postsynaptic potential synapse low-pass ﬁltered version variable combines eligibility trace reward averages time scale constant oﬀset reward signal. parameter arbitrary value without changing stationary dynamics model scaled current synaptic weight weight-dependence update equations induces multiplicative synaptic dynamics consequence pulls synaptic parameters towards unused synapses tend disappear synapses permanently formed. throughout simulations used independent gaussian priors classical goal reinforcement learning maximize function discounted expected rewards policy gradient algorithms perform gradient ascent changing parameter direction gradient v/∂θi. here show parameter dynamics approximate gradient i.e. v/∂θi. average taken long sequence network activity starts time ends time here systematic diﬀerence batch setup cannot guarantee time-invariant distribution change slowly time-scale chosen signiﬁcantly longer time constant eligibility trace estimator works reliably require time constant estimate average reward figure impact prior distribution synaptic dynamics. task performance total number active synaptic connections throughout learning prior distributions distribution initial synaptic parameters. synaptic parameters initially drawn gaussian distribution mean µinit gaussian prior distribution diﬀerent parameters compared parameter used experiments addition laplace prior diﬀerent parameters tested prior distribution initial synaptic parameters marked eﬀect task performance overall network connectivity. zero mean scale parameter leads constant negative drift term parameter active synaptic connections. convergence sparse solutions faster prior good task performance reached networks less active connections description temperature time constant eligibility trace time constant gradient estimator time constant estimate average reward oﬀset reward signals learning rate mean prior prior distribution temperature synaptic delays synaptic parameter changes clipped synaptic parameters allowed exceed interval sake numerical stability. experiences input neurons assigned gaussian tuning curves centers independently equally scattered unit cube. sensory experiences represented diﬀerent randomly selected points -dimensional space. stimulus positions constants reward oﬀset temperature synapse model chosen qualitatively match results fig. fig. value estimation average reward chosen equal based theoretical considerations found parameters prior relatively small eﬀect synaptic figure drifts neural codes performance remained constant. trial-averaged network activity fig. evaluated three diﬀerent times selected time window network performance stable column shows trial-averaged activity plot subject diﬀerent sorting. rows correspond sorting criterion based evaluation time. infer lever position network activity weighted spikes neuron pool spikes summed ﬁltered long kernel input pattern realized method used generate patterns outlined above. trial completed successfully reward signal otherwise. trial short holding phase inserted consolidation mechanism fig. used modiﬁed version algorithm introduced synaptic parameter independent mean prior distribution simulated days current value synaptic parameter standard deviation simulation synaptic parameter dynamics continued subsequent hours. connection became functional time reset eligibility trace gradient estimator zero continued synaptic dynamics according histograms fig. computed bins hours width. pearson correlation ci-synapses given non-ci synapses therefore estimated fraction contributions speciﬁc activity histories synaptic changes spontaneous synapse-autonomous processes remaining (measured correlation non-ci", "year": "2017"}