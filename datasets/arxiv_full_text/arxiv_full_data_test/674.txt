{"title": "Variational Encoding of Complex Dynamics", "tag": "q-bio", "abstract": " Often the analysis of time-dependent chemical and biophysical systems produces high-dimensional time-series data for which it can be difficult to interpret which individual features are most salient. While recent work from our group and others has demonstrated the utility of time-lagged co-variate models to study such systems, linearity assumptions can limit the compression of inherently nonlinear dynamics into just a few characteristic components. Recent work in the field of deep learning has led to the development of variational autoencoders (VAE), which are able to compress complex datasets into simpler manifolds. We present the use of a time-lagged VAE, or variational dynamics encoder (VDE), to reduce complex, nonlinear processes to a single embedding with high fidelity to the underlying dynamics. We demonstrate how the VDE is able to capture nontrivial dynamics in a variety of examples, including Brownian dynamics and atomistic protein folding. Additionally, we demonstrate a method for analyzing the VDE model, inspired by saliency mapping, to determine what features are selected by the VDE model to describe dynamics. The VDE presents an important step in applying techniques from deep learning to more accurately model and interpret complex biophysics. ", "text": "often analysis time-dependent chemical biophysical systems produces high-dimensional time-series data difﬁcult interpret individual features salient. recent work group others demonstrated utility time-lagged covariate models study systems linearity assumptions limit compression inherently nonlinear dynamics characteristic components. recent work ﬁeld deep learning development variational autoencoders able compress complex datasets simpler manifolds. present time-lagged variational dynamics encoder reduce complex nonlinear processes single embedding high ﬁdelity underlying dynamics. demonstrate able capture nontrivial dynamics variety examples including brownian dynamics atomistic protein folding. additionally demonstrate method analyzing model inspired saliency mapping determine features selected model describe dynamics. presents important step applying techniques deep learning accurately model interpret complex biophysics. introduction simulations biomolecules provided insight molecular processes increasing timelength-scales advances algorithms hardware. simulations thousands degrees freedom making crucial meaningful statistically robust methods extract underlying dynamical processes. dynamics molecular systems often represented using dynamical propagator approach. given ensemble particles time distributed phase space given probability distribution seek describe propagator operator describe distribution ensemble given time chosen probabilities independent history system model said markovian. many methods developed compute approximations propagator molecular system simulation data including time-structure-based independent component analysis extensions markov state models vampnets soft-max msms diffusion maps. method approximating dynamics complex system objectives adequately represent complexity form model nonlinearity interpretable readily analyzable feature importance. figure indicate several commonly-used methods dimensionality reduction dynamical systems compare terms achieving aims. complexity interpretability often come expense other. instance kernel methods kernel tica improve ability capture nonlinear effects features dynamics linear methods; however identifying biophysical meaning coordinates implicit kernel space remains challenge. conversely standard tica sparse tica allow precisely identifying relevant biophysical features linearity tica limits complexity dynamics represent. alternative technique dimensionality reduction autoencoder framework autoencoder deep unsupervised learning algorithm aims learn low-dimensional representation high-dimensional data autoencoder components encoder network decoder network. encoder network reduces input data low-dimensional representation referred latent space autoencoder decoder network reconstructs latent representation original dimensionality. difference original data reconstruction used update train network. variational autoencoders regularization encoder framework applying gaussian noise latent space. term variational stems stochasticity autoencoder implementation figure overview subset methods used analyze protein dynamics terms model interpretability ability capture non-linear motions– here deﬁne interpretability ease scientist analyze model feature importance respect dynamics. example principal component analysis arguably simplest model mentioned typically ill-suited analyze complex dynamics therefore principal components reliably meaningful. meanwhile able leverage deep learning model non-linear relationships time-dependent observables saliency mapping understand observables contribute model. recently autoencoder framework extended model time-series data–. analysis applications typically involves mapping time-series data latent spaces dimensionality length initial time-series data focused approximating propagator time-series data; however couple notable exceptions. doerr fabritis recently compared simple autoencoder methods dimensionality reduction biophysical simulation data. wehmeyer introduced time autoencoder framework describe dynamics. interestingly demonstrate limit single linear hidden layer tica solution attained. work extend traditional architecture approximate propagator time-series data architecture denoted variational dynamics encoder represents ﬁrst time within variational autoencoder knowledge. additionally introduce novel autocorrelation loss function inspired variational approach conformational dynamics demonstrate approach yields models explanatory power linear dimensionality reduction techniques m¨uller-brown potential folding landscape villin headpiece subdomain. also explore generative capability propagator dynamics show that implemented unable reliably capture thermodynamics differing temperatures. finally demonstrate novel analysis method inspired saliency mapping neural nets visual classiﬁcation– lend interpretation models. combination using saliency mapping creates framework enables nonlinear combinations features remaining interpretable. architecture architecture seen figure closely resembles vae; however training procedure slightly modiﬁed suit time-series data signiﬁcant modiﬁcation featurized data timepoint network order make prediction state system future timepoint user-selected time dynamics system markovian. traditional network subdivided three parts encoder network; variational layer decoder network. figure schematic vde. features timepoint network order make prediction state system future timepoint markovian time. traditional network subdivided three parts encoder network; variational layer decoder network labeled. encoder network non-linear activation functions hidden layers eventually bottlenecks one-dimensional latent space latent space slightly perturbed gaussian noise λ-layer generate described kingma welling. finally decoder network also mirrors encoder network architecture using generate encoder network deep neural network non-linear activation functions user-selected number hidden layers eventually bottlenecks one-dimensional latent space encoder network functions non-linear dimensionality reduction latent space perturbed gaussian noise within λ-layer mean parameter variance parameter arbitrary scaling generate described kingma welling. finally decoder network also mirrors encoder network architecture using generate prediction system evolve duration trained used dimensionality reduction synthetic trajectory generation. dimensionality reduction encoder network necessary provides direct mapping trajectory generation entire network needed. initial features network generate predicted state duration done iteratively generate arbitrarily long trajectory features exhibiting thermodynamics consistent original system used training. overcome model’s insensitivity λ-layer used training recommend noise scaling increased αgeneration αtrain. ﬁrst reconstruction loss attempts quantify well approximates state system given true state system time essentially evalutating ability latent space approximate markovian propagator single time. done considering mean squared error predicted propagation true propagation xt+τ along kullback–leibler divergence latent space priors generate learnable parameters representing mean standard deviation gaussian prior applied latent space respectively. together complementary loss terms capture trade-off model complexity simplicity gaussian prior. reconstruction loss pushes model towards high ﬁdelity training data kullback–leibler divergence acts regularization term ensure latent space behaves gaussian emission. autocorrelation loss attempts optimize network towards complete representation long time-scale kinetics observed within time-series. although minimization reconstruction loss potential recover dynamical processes cases section alone sufﬁcient. order improve model convergence borrow speciﬁc application variational principle quantum mechanics adapted markov modeling useful tool parameter selection. variational principle states that limit inﬁnite data process identiﬁed data slower true process. interpret variational principle measure quality approximation phase-space decomposition leads linear model larger leading dynamical eigenvalues consequently better phase-space decomposition. limit single linear decomposition phase-space eigenvalue consider equivalent autocorrelation sample mean population mean latent space particular batch data respectively. linear models leads ﬁrst-order approximation slowest process; however incorporating vde’s loss function take advantage deep encoder general approximator slowest processess found within data. algorithm outlines losses calculated used backpropagation practice. note data split many smaller batches training input variable xt+τ target variable take advantage stochastic gradient descent methods. also recommend pre-processing features—either standardization median-centering scaling interquartile ranges—to prevent reconstruction loss overpowering autocorrelation loss. ﬁrst apply framework well-studied m¨uller-brown potential demonstrate adequately describe dynamics simple system. figure shows results tica pca. note tica identify dominant linear coordinate representing diffusion minor major basin generates non-linear projection able distiguish basins clearly well transition region. figure m¨uller-brown potential overlaid colormap projections one-dimensional tica coordinates. tica identify strictly linear mode approximates slowest dynamical process non-linear better able basins intermediate state note region outside contours energetically unfavorable color projections space extrapolations method respectively. establish unbiased assessment vde’s performance compared tica constructed msms using identical hyperparameters compared generalized matrix raleigh quotients slowest process scoring metric. achieves slightly higher mean gmrq score tica held-out data suggesting better able represent slowest timescale system. behave true propagator vaes regarded generative model consider relationship propagator function. trained m¨uller-brown potential joules able generate fake trajectories similarities thermodynamics original simulations seen figure furthermore modulate effect λ-layer adjusting scaling parameter also able mimic effects changing simulation temperature without re-train vde. figure demonstrates decreasing increasing able adjust barrier heights similar fashion observed simulation shown figure however ﬁdelity simulation lacking transition regions previously unobserved regions phase-space poor recapitulating true thermodynamics. figure one-dimensional free energy projections generated coordinate true brownian-dynamics simulations different temperatures fake trajectories generated trained joules different scaling values proxy temperature within λ-layer. although true one-to-one comparison free energy barriers lowered expected temperature increased within m¨uller-brown potential; however free energies transition boundary regions phase-space cannot reproduced reliably. note selected values rigorously ﬁtted best match different values shown instead evenly sampled ﬁxed interval similar thermodynamics simulation observed. also note case behaves essentially indicator function reporting basin given frame eventually diffuse towards temperature simulation. increased must decide basin push heat-bathed system towards realistic dynamics observed. behavior interpreted temperature-dependent propagator whereby learned underlying thermodynamic characteristics system; although seems strong attraction certain basins observed simulation. attraction increasing leads raising intermediate basins towards realistic free energies rather lowering might expect raising temperature simulation. recommend greatly increasing described section generating synthetic trajectories trend. simple encoding villin headpiece dynamics next apply pairwise alpha-carbon contacts order model folding process villin headpiece subdomain. here assess quality dimensionality reduction technique protein folding quantifying well constructed vde-transformed data separates relevant timescales distinguishes basins within landscape. metrics mind appears represent folding landscape well even out-perform tica using similar hyperparameters. figure depicts trajectory data projected onto slowest tica processes optimized tica model colored projection onto latent coordinate. optimal tica model tics needed capture folding prominent misfolding process. ﬁrst unable completely separate unfolded folded state whereas second distinguishes folded unfolded state unable distinguish folded misfolded state. contrast latent coordinate able discriminate three states folded unfolded misfolded. comparing free energies latent space ﬁrst observe coordinate narrower basin folding ﬁrst indicating latent coordinate sharply resolves folding basin ﬁrst does. figure folding coordinate villin approximated comparison ﬁrst tica solutions. villin trajectory data projected onto ﬁrst tics optimized tica model order compare coordinate slowest coordinates tica colored latent projection. inset yellow orange purple structures depict representative folded unfolded misfolded structures respectively. villin trajectory data projected onto ﬁrst tics optimal tica model colored latent projection without using autocorrelation loss. free energy latent projection. free energy ﬁrst tica coordinate. comparison timescales msms constructed model optimized tica model tics tica model tic. error bars represent range bootstrapped replicates. error bars visible error small see. reconstruction loss autocorrelation loss. projection optimal model autocorrelation loss portrayed figure projection minimal differentiation different parts landscape. highlights necessity incorporating autocorrelation loss loss function. msms villin landscape constructed model optimized tica model. comparing models indicates model identiﬁes slower timescale tica model. figure portrays timescales slowest processes identiﬁed msms built projection optimized tica model tica model built component. timescale slowest process projection nanoseconds whereas timescale slowest process optimal tica model nanoseconds. according variational approach conformational dynamics described section model longer timescales closer modeling true dynamics system. protein saliency maps enable interpretation noted figure nonlinear methods time-series analysis tend sacriﬁce model interpretability. linear tica provides loadings input feature slow mode. thus absolute magnitude loadings used understand holistic protein dynamics atomic scale. make vdes interpretable designed novel variant saliency maps gain insight network operates propagates protein conﬁgurations particular time. figure protein saliency maps used gain insight vde. saliency maps distances predicted targeted output propagated back network input contact distances order gain insight network learns. repeated large batch possible conﬁgurations. villin folded state characterized contact distances central asparagine residue. misfolded state residue close ﬁrst helix forming non-native contacts. green lines denote contacts highest median saliency scores. integrating saliency atomic level allows infer importances individual residues certain state transitions making prime candidates biophysical characterization. distributions computed transitions. perform saliency analysis computed median value derivative residual villin’s misfolded folded basin respect input contact features. shown figure saliency maps villin found important contacts. contacts involve contacts residues around asn. remarkably also integrate saliency scores atomic feature infer feature importances residue scale. residue importance figure used potentially design molecular simulations biophysical experiments. example case villin model predicts distances critical movement misfolded partially helical state mutating residue proline glycine could potentially used prevent system sampling misfolded state. potential drawback method requires sufﬁcient knowledge system identify relevant path investigate corresponding initial ﬁnal conformations. accomplished either empirical analysis clustering simply sampling conformations minima maxima latent space. discussion work introduced variational autoencoder analyze dynamical processes incorporating time traditional autoencoder structure introducing autocorrelation loss training leveraging gaussian noise introduced latent space training dimensionality reduction synthetic trajectory analysis. furthermore introduced saliency mapping approach inspired advances deep learning order interpret features contribute identiﬁed reaction coordinate. demonstrate able outperform state-of-the-art methods tica describing slow dynamics m¨uller-brown potential protein folding. using system probe generative nature showing generate realistic thermodynamics ability extrapolate dynamics temperatures observed. although portion study quantitatively rigorous expect better understanding parameters relate simulation parameters might lead using cheaply generate reliable data different simulation conditions perhaps even protein mutants. complex case protein folding show utility understanding conformational landscape villin headpiece domain non-trivial prominent misfolded state observed. latent space captures transitions among misfolded unfolded folded constructed using projection exhibits signiﬁcantly longer timescale slowest process optimized constructed tica-transformed data. also showed peer black protein saliency mapping provides biophysical insights network’s decision-making. villin identify important contacts predict potentially play role misfolding-folding transitions. anticipate results prove useful experimental design fret experiments decide effectively probe protein observe conformational change. shows much promise reasons cannot recommend completely replace previous methods tica yet. first training deep autoencoders using autocorrelation loss noticeable dependence batch size arises training. autocorrelation well related variational loss attempts calculate global equilibrium statistics exchange timescale slowest process. however ﬁnite batch sizes might observe single event process within given batch. lead underestimating computed statistics since network information rest dataset. problem arise tica msms timescales global statistics estimated data already processed. another issue using autocorrelation loss implemented arises reality many processes occur similar timescales. processes assigned highly similar autocorrelations thus might lead volatile training; although believe compound loss function somewhat attenuate issue since network designed keep track global transition dynamics addition ﬁtting slow processes. vdes recent related work herald exciting opportunities bridging markov models deep learning. believe expressive power neural networks provides natural solution choice-of-basis problem plagues many markovian analyses strong theoretical underpinnings behind msms allow select potentially even validate cross-validate neural architectures ultimately allowing address fundamental questions biophysics. suggested m¨uller brown. using euler-maruyama method numerical integration time step produced unique trajectories time steps saved every steps. initial positions sampled uniform distribution vdes m¨uller-brown potential trained time time steps; hidden layers nodes each; swish activation function; α-value batch size dropout rate learning rate note parameters optimized using automated hyperparameter selection. gradient descent performed adam optimizer. models trained epochs point losses observed converged. prior training trajectories preprocessed subtracting overall median values scaling inter-quartile ranges. constructing msms m¨uller-brown potential scaled trajectories subject dimensionality reduction using principal component analysis time-structure based independent component analysis pretrained order generate one-dimensional representations system’s dynamics. partitioned representations twelves clusters using mini-batch k-means algorithm finally clusters used construct maximum-likelihood estimated reversible markov state model timescale. time time steps chosen construction dimensionality reduction resulting models provided optimal convergence implied timescales. msms evaluated randomly seeded hold-out datasets generate unbiased gmrq scores standard errors. trajectory generation analyses performed msmbuilder msmexplorer. finally order generate fake trajectories using randomly sampled initial positions uniform distribution described above iteratively propagated coordinates steps equivalent integrator steps. done scaling values evenly sampled logspace understand λ-layer affects propagation. villin headpiece domain demonstrate utility method characterizing folding landscape villin headpiece domain widely-studied -residue fast-folding protein referred henceforth villin. simulation data villin generated lindorff-larsen al.. simulation length strided analysis. contacts used featurization vdes villin trained time selected optimal tica model. expanding number hidden layers nodes training procedure identical section compared optimized tica model villin featurized contacts identiﬁed hyperparameter optimization. husic indicated contacts useful featurization representing folding processes hence selection featurization. model tica time tica components kinetic mapping selected according hyperparameter optimization. construct msms tica-transformed vde-transformed data analogous steps m¨uller potential section performed. mini-batch k-means clustering performed clusters sets data. optimal cluster number identiﬁed tica hyperparameter searching showed little inﬂuence cluster number msms vde-transformed data. msms ticavdetransformed models constructed time. obtain error estimates equilibrium populations timescales rounds bootstrapping performed original trajectories. resulting ranges values used error bars. protein saliency maps saliency maps– originally proposed looking spatial support varied classiﬁcation problems. image data spatial features network looks classiﬁcation i.e. asking much individual pixel contribute ﬁnal prediction. done back-propagating desired class score network image pixels. similar tica loadings magnitude derivative used gauge feature importance output class. alternative closely related method namely guided back-propagation propagates positive derivatives network. saliency maps designed classiﬁcation algorithms thus needed modiﬁed application brieﬂy ﬁrst generate faux two-step trajectory starting random protein conformation instance misfolded state going desired protein conformation instance folded state. misfolded state propagated network residual folded state propagated back obtain loadings individual distances. ideally done large number possible misfolded folded transitions obtain robust saliency maps. median values feature across maps integrated obtain residue level statistics rank ordered important features.it worth noting method different classical saliency scoring whereby desired class label score propagated backwards. note vde’s noise parameter autocorrelation loss consistent results numerical stability. also recommend computing saliency scores multiple times across many conﬁgurations averaging results. lastly note protein saliency maps used variety different protein deep learning algorithms including vampnets tae. availability source code work available open-source license accessible https//github.com/ msmbuilder/vde. complete examples found jupyter notebooks https//github.com/msmbuilder/ vde/tree/notebooks/. acknowledgements would like thank chodera eastman feinberg sharma insightful discussions. thank peck help copy-editing. acknowledge funding grants support work. hkws acknowledge support grfp m.m.s would like acknowledge support national science foundation grant nsf-mcb-. work used xstream computational resource supported national science foundation major research instrumentation program well sherlock cluster maintained stanford research computing center. disclosures consultant member schrodinger globavir sits board directors apeel freenome omada health patient ping rigetti computing general partner andreessen horowitz. references shirts pande screen savers world unite sci. shaw anton special-purpose machine molecular dynamics simulation. commun. shukla hern´andez weber pande markov state models provide insights dynamic modulation prinz j.-h. markov models molecular kinetics generation validation. chem. phys. naritomi fuchigami slow dynamics protein ﬂuctuations revealed time-structure based independent coifman lafon diffusion maps. appl. comput. harmon. anal. harrigan pande landmark kernel tica conformational dynamics. biorxiv kingma welling auto-encoding variational bayes. arxiv preprint arxiv. rezende mohamed wierstra stochastic backpropagation approximate inference deep generative hecht-nielsen replicator neural networks universal optimal source coding. sci. hinton salakhutdinov reducing dimensionality data neural networks. sci. bengio alain vincent generalized denoising auto-encoders generative models. advances wang deep reconstruction model dynamic images. plos doerr ariz harvey fabritiis dimensionality reduction methods molecular simulations. arxiv simulations kinetic experiments. proceedings national academy sciences harrigan msmbuilder statistical models biomolecular dynamics. biophys. husic mcgibbon sultan pande optimized parameter selection reveals trends markov state mcgibbon osprey hyperparameter optimization machine learning. open source softw. m¨uller brown location saddle points minimum energy paths constrained simplex optimization ramachandran zoph searching activation functions. corr abs/. kingma adam method stochastic optimization. arxiv preprint arxiv. sculley web-scale k-means clustering. proceedings international conference world wide pedregosa scikit-learn machine learning python. mach. learn. res. metzner no´e sch¨utte estimating sampling error distribution transition matrices functions lindorff-larsen piana dror shaw fast-folding proteins fold. sci. mcgibbon mdtraj modern open library analysis molecular dynamics trajectories. biophys.", "year": "2017"}