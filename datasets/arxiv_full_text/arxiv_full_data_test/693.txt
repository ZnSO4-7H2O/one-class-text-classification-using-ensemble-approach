{"title": "Attention based convolutional neural network for predicting RNA-protein  binding sites", "tag": "q-bio", "abstract": " RNA-binding proteins (RBPs) play crucial roles in many biological processes, e.g. gene regulation. Computational identification of RBP binding sites on RNAs are urgently needed. In particular, RBPs bind to RNAs by recognizing sequence motifs. Thus, fast locating those motifs on RNA sequences is crucial and time-efficient for determining whether the RNAs interact with the RBPs or not. In this study, we present an attention based convolutional neural network, iDeepA, to predict RNA-protein binding sites from raw RNA sequences. We first encode RNA sequences into one-hot encoding. Next, we design a deep learning model with a convolutional neural network (CNN) and an attention mechanism, which automatically search for important positions, e.g. binding motifs, to learn discriminant high-level features for predicting RBP binding sites. We evaluate iDeepA on publicly gold-standard RBP binding sites derived from CLIP-seq data. The results demonstrate iDeepA achieves comparable performance with other state-of-the-art methods. ", "text": "rna-binding proteins play crucial roles many biological processes e.g. gene regulation. computational identiﬁcation binding sites rnas urgently needed. particular rbps bind rnas recognizing sequence motifs. thus fast locating motifs sequences crucial timeefﬁcient determining whether rnas interact rbps not. study present attention based convolutional neural network ideepa predict rna-protein binding sites sequences. ﬁrst encode sequences one-hot encoding. next design deep learning model convolutional neural network attention mechanism automatically search important positions e.g. binding motifs learn discriminant high-level features predicting binding sites. evaluate ideepa publicly gold-standard binding sites derived clip-seq data. results demonstrate ideepa achieves comparable performance state-of-the-art methods. rna-binding proteins take eukaryotic proteome closely associated many biological processes identify whether binds important analyzing rnas’ functions. many experimental technologies developed. clip-seq. however still time-consuming high-cost. thus computational identiﬁcation binding sites urgently needed. many machine learning based methods proposed. example graphprot encodes sequences structures graph support vector machine classify bound sites unbound sites ionmf integrates multiple sources data predict binding sites using orthogonal matrix factorization recently deep learning successfully developed predict binding sites. example deepnet-rbp applies deep belief network integrate k-mer frequency features sequences structures model targets deepbind applies convolutional neural network identify binding sequence speciﬁcity. ideep uses multimodal deep learning integrate different sources data infer binding sites sequence motifs ideeps infers sequence structure motifs simultaneously using convolutional neural network long short temporal network core methods demonstrates high accuracy identifying binding sites. commonly assumed sequence bound contains least binding subsequence rbp. therefore fairly intuitive consider putting attention motif subsequence along sequence. better model characteristics binding sites attention mechanism introduced attention mechanism allows deep learning models focus selectively important features. deep models augmented attention mechanisms obtained great success machine translation computational biology study propose attention-based convolutional neural network model ideepa predict binding sites sequences alone. ideepa combines learned features cnns levels attentions locate important subsequences. graphprot contains periments rbps. thousands bound subsequences variable length almost number negative sequences selected evidence showing bound rbp. study present based method attention mechanism classify bound sites unbound sites ﬁrst encode sequences one-hot encoding showing presence nucleotide acgu. one-hot encode matrix involves convolution activation max-pool operations. layer preserves spatial information output feature maps subsequent processing. inspired introduce attention mechanism attend differentially related motifs locate important positions predicting binding sites. extract three levels abstract features output feature maps cnn. outputs attention model sequence dimension whose input copy two-dimensional hidden states cnn. outputs another attention model feature dimension whose input transposition hidden states cnn. attention models structure feedforward neural network decoder generate representation vector. output attention model augmenting attention mechanism learns soft transformation input output sequences. finally outputs layer attention models connected fully connected layers. last layer sigmoid layer used classify bound sites unbound sites. optimize categorical entropy loss function using rmsprop number epochs ideepa implemented using keras library https//github.com/fchollet/keras. compare ideepa state-of-the-art methods graphprot deepnet-rbp deepbind milcnn. negative sequence binding site positive sequence contains least binding sites rbp. intuitive consider sequence whose subsequence instance. inspired characteristics milcnn ﬁrst breaks sequence multiple overlapping ﬁxed-length subsequence subsequence instance sequence instances. next milcnn trains multiple instance learning framework. multiple instance learning used predicting protein-dna interactions figure ﬂowchart ideepa. ideepa ﬁrst encodes sequence one-hot matrix output feature maps. next input last hidden states attention model transposition another attention model. outputs attention models combined fully connected layers predict binding sites. graphprot deepnet-rbp milcnn deepbind ideepa achieve average across experiments respectively. ideepa deepbind yield similar average higher three methods. addition ideepa improves rbps small training deepbind achieve high auc. example ideepa obtains corf training samples increase compared deepbind. results indicates introducing attention mechanism enhance learning ability small dataset deepbind fast focus important subsequences. however introducing attention mechanism improve performance rbps large number training samples possible feeding samples model training make model converge optimum model. addition milcnn yields lower performance methods maybe training sequences subsequence anchored peak center derived clip-seq breaking subsequence also break binding sites. study present attention-based method predict binding sites. method ideepa yields comparable performance state-of-the-art methods. however still investigate whether attention used identify interpretable motifs. future work expect obtain interpretablitity ideepa comprehensively evaluate ideepa larger dataset rbps.", "year": "2017"}