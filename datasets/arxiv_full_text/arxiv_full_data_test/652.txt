{"title": "Survival analysis of DNA mutation motifs with penalized proportional  hazards", "tag": "q-bio", "abstract": " Antibodies, an essential part of our immune system, develop through an intricate process to bind a wide array of pathogens. This process involves randomly mutating DNA sequences encoding these antibodies to find variants with improved binding, though mutations are not distributed uniformly across sequence sites. Immunologists observe this nonuniformity to be consistent with \"mutation motifs\", which are short DNA subsequences that affect how likely a given site is to experience a mutation. Quantifying the effect of motifs on mutation rates is challenging: a large number of possible motifs makes this statistical problem high dimensional, while the unobserved history of the mutation process leads to a nontrivial missing data problem. We introduce an $\\ell_1$-penalized proportional hazards model to infer mutation motifs and their effects. In order to estimate model parameters, our method uses a Monte Carlo EM algorithm to marginalize over the unknown ordering of mutations. We show that our method performs better on simulated data compared to current methods and leads to more parsimonious models. The application of proportional hazards to mutation processes is, to our knowledge, novel and formalizes the current methods in a statistical framework that can be easily extended to analyze the effect of other biological features on mutation rates. ", "text": "abstract antibodies essential part immune system develop intricate process bind wide array pathogens. process involves randomly mutating sequences encoding antibodies variants improved binding though mutations distributed uniformly across sequence sites. immunologists observe nonuniformity consistent mutation motifs short subsequences aﬀect likely given site experience mutation. quantifying eﬀect motifs mutation rates challenging large number possible motifs makes statistical problem high dimensional unobserved history mutation process leads nontrivial missing data problem. introduce -penalized proportional hazards model infer mutation motifs eﬀects. order estimate model parameters method uses monte carlo algorithm marginalize unknown ordering mutations. show method performs better simulated data compared current methods leads parsimonious models. application proportional hazards mutation processes knowledge novel formalizes current methods statistical framework easily extended analyze eﬀect biological features mutation rates. introduce proportional hazards model approach study mutation processes. study motivated somatic hypermutation mutation process occurs sequences encode b-cell receptors proteins recognize neutralize pathogens. bcrs secreted cells known antibodies. immune system relies somatic hypermutation process generate diversity bcrs bind large continually evolving variety pathogens. starting material mutation process sequence formed recombination sequence complex system enzymes introduces mutations random pattern known highly sensitive sequence motif sequence bases surrounding mutating position goal develop solid statistical framework estimates mutation rates motifs provides interpretable results mutation process. better understanding somatic hypermutation help designing vaccines challenging viruses furthering understanding biological mechanisms play gaining insight natural selection process occurring immune system several strategies used model motif’s mutability likely position mutate given motif position. general approach compare mutated sequence inferred ancestor sequence model diﬀerences them. cohen kleinstein louzoun elhanati model mutabilities motifs product mutabilities short subsequences using log-linear model ﬁrst-order terms keep parameter count miss interactions positions. yaari log-linear assumption allow separate parameter possible ﬁve-nucleotide motif ad-hoc methods handle motifs observations. rather restrictive ad-hoc approaches data-adaptive variable selection method desirable. another drawback methods ignore mutations occur neighboring positions even though events carry important information highly mutable motifs. indeed methods require counting number times motif observed mutate mutations occur neighboring positions cannot attribute mutation correct motif. settings high rates mutation methods estimating mutabilities poorly. properly estimate mutabilities needs account diﬀerent possible orders mutations occurred previous work developed methods performing various types inference mutation order unknown inference procedures make parametric assumption mutation process follows continuous time markov process. relax model assumption semiparametric model instead. paper advance modeling motif mutabilities several directions. propose method mutabilities using survival analysis mutation motifs called samm. formalize problem using proportional hazards mutations failure events investigated. although survival models used implicitly computational immunologists simulation believe ﬁrst time used inference. estimate motif mutabilities method uses monte carlo expectation– maximization algorithm since orders mutations occur unobserved data expectation–maximization allows perform maximum likelihood averaging unknown orders. however e-step requires calculating expected log-likelihood analytically intractable since must average possible mutation orders; thus estimate expectation using gibbs sampling. approach similar used goggins model interval-censored failure-time data order failure events occur unknown. method also handles high-dimensional settings many predictors observations important many motifs hypothesized aﬀect mutation rate speciﬁc ones unknown. instance yaari consider motifs length lasso improve estimation perform variable selection. provide measure uncertainty estimates two-step approach -penalized proportional hazards model perform variable selection reﬁt unpenalized model selected variables obtain ﬁnal estimates along approximate conﬁdence intervals. section describes estimation methods starting simpliﬁed logistic regression model progressing full estimation method. section presents simulation results. section apply method model somatic hypermutation sequences compare results previous methods. mutation natural selection. data obtained example immunization experiments transgenic mice designed segment carried along mutated expressed part though focus modeling somatic hypermutation process bcrs approach framed generally problem modeling sequencevalued mutation process. refer original unmutated sequences na¨ıve descendants mutated sequences. throughout suppose na¨ıve sequences known. case restrict attention computationally-identiﬁed na¨ıve segment coded germline described vector-valued stochastic process indexed time {xj} represents mutation process position sequence. given time state space nucleotides state space lengthp nucleotide sequences t}p. start mutation process context-sensitive model probability position mutates time depends current nucleotide sequence work assume local context matters mutation rate position aﬀected local nucleotide sequence called motif. motif denote length motif typically much smaller number nucleotides function binary indicator whether motif appears sequence positions len. formally deﬁned nucleotide position motif known len-mer i.e. motif length len. example -mer motif length special case checks motif centered position call centered motif; cases checking oﬀset motif. deﬁne motif dictionary sequence features aﬀect mutation rate. example dictionaries include -mers oﬀset -mers central oﬀset -mers central -mers. also consider possible unions dictionaries. suppose selected ease exposition choose arbitrarily assigned ﬁxed order example feature vectors generated believe mutation rate position depends -mer starting position left feature vector position one-hot encoding sequence appears position formally element feature vector position indicates whether motif appears start position position start positions derived aligning position sequence position motif. course framework present generalizes types dictionaries including dictionaries specify bases subset positions restrict above-described dictionaries paper concreteness. simpliﬁed approach modeling mutation process ignore time component logistic regression. model position sequence independent probability position mutating depends initial nucleotide sequence i.e. yaari essentially take approach; logistic model formalizes intuition within statistical framework allows generalize method applicable feature vector mapping. moreover penalized logistic regression handling high-dimensional models encode various structural assumptions regarding mutation rates; discuss detail later section logistic regression ignores time component mutation process ignores mutation rate position change positions mutate assumption mutation rate depends initial nucleotide sequence problematic mutation rate high. also logistic regression ignores censoring method estimates average mutation probability respect particular sampling process. estimates diﬀerent tend sample sequences mutate propose using survival analysis framework model mutation process. view positions single sequence subjects observed time period. mutation event position occurs time nucleotide immediately time lims→t− diﬀers nucleotide time position never mutates consider mutation time censored. hazard rate position instantaneous risk mutating time given conserved time successive mutation times position constant hazard rate mutates independently positions. dependence positions introduced mutation occurs upon mutation event hazard rate neighboring position change accounting sequence change time complicates estimation procedure. since observe order mutation events data observe pairs na¨ıve mutated sequences many possible mutation orders could explain mutated sequence arose na¨ıve sequence; mutation order corresponds distinct sequence hazard rates. ease exposition present estimation method mutation process single pair na¨ıve mutated nucleotide sequences. method readily applies estimating rates given many independent mutation processes part modeling framework assume position mutate mutation process. simpliﬁcation somatic hypermutation process since possible position mutate once though data na¨ıve mutated sequence typically diﬀer positions. think assumption reasonable makes problem easier handle computational standpoint. discuss assumption aﬀects performance model misspeciﬁcation section appendix. using survival analysis framework implicitly assume mutation occur every position given suﬃciently long period time. assumption reasonable somatic hypermutation complex system enzymes ability mutate position along sequence assumption hold mutation processes method need modiﬁed accordingly. survival analysis sequences positions mutated indicated light gray squares mutated indicated dark gray squares. context-dependent mutation model mutation event change mutation rates positions. suppose hazard rate position depends position’s neighboring bases. then example third position mutates hazard rate fourth position changes original motif motif. changes motif potential mutating position thus hazard rate indicated change solid dashed lines. process mutates speciﬁc nucleotide target nucleotide. previous work suggests context-dependent mutation process biased favor mutations particular bases. take account preferences considering per-target model. model additionally ready present maximum likelihood estimation method model. assume hazard rate follows per-target model straightforward extension simpler case. observed data namely single pair na¨ıve mutated nucleotide sequences denoted sobs suppose positions mutated. transformed arbitrary increasing function form hazard function would still form consequently estimating involves maximizing likelihood observing mutation order. suppose observe order mutations occurred position mutation denote positions ﬁrst mutation deﬁned empty set. deﬁne nucleotide sequence positions mutate. thus result looks like marginal likelihood derived equation except derived general assumptions whereas assume covariates ﬁxed assume covariates ﬁxed events. derivation given appendix. marginal likelihood implies mutation order simulated drawing positions successive multinomial distributions. simulate mutation position draw position risk group fact gupta procedure simulate somatic hypermutation process though provide statistical justiﬁcation. unfortunately mutation order observed problem. instead maximize observed data likelihood complete data likelihood marginalized admissible mutation orders assuming positions mutate once possible mutation orders. number mutated positions small enumerate possible mutation orders maximize using nonlinear optimization algorithm however data sets much large direct enumeration computationally tractable maximize using mcem. mcem extends traditional algorithm approximating expectation e-step using monte carlo sampling method. full mutation order. gibbs sampler algorithm sample {sobs given full mutation order partial mutation order mutation removed full mutation order consistent instance partial mutation order consistent since gibbs sweep index cycles random order. gibbs step eﬃciently calculate probability full mutation order given partial mutation order reusing previous computations. particular partial mutation order calculate probabilities consistent full mutation order starting full mutation order position mutates ﬁrst position mutates last. ordering consistent full mutation orders consistent full mutation order consistent full mutation order except mutations swapped. ratio conditional probabilities given already pr|π) divide quickly obtain pr|π). moreover eﬃciently calculate storing previous computational results instance summation risk group shares many elements summation risk group similar ideas used speed calculations required mcem. given monte carlo samples e-step m-step maximizes mean log-likelihood complete data. suppose e-step generates monte carlo samples m-step solve rule deciding proposed mcem estimate iteration accepted monte carlo sample size increased. number monte carlo samples increases standard error estimated expected likelihood decreases. suﬃciently large number monte carlo samples ensure observed data likelihood increases high probability. many cases desirable model eﬀects many features. instance yaari estimate -mer model parameters. estimating parameters per-target model increases number parameters additional factor four. number sequences dataset small compared number features optimization problem ill-posed. high-dimensional settings common regularization stabilize estimates encourage model structure. particular believe small subset features aﬀects mutation rate. yaari assume nucleotides closest position signiﬁcant eﬀect mutation rate -mer motifs small number observations estimate mutation rate using oﬀset -mer motif. method lasso perform variable selection. incorporate lasso estimation procedure requires steps. ﬁrst step maximizes observed log-likelihood lasso penalty thereby performs variable selection. second step aims quantify uncertainty model parameter estimates reﬁt model parameters maximizing unpenalized objective conﬁdence intervals unpenalized model assessment uncertainty. penalty parameter. solve variant mcem e-step before maximize penalized surrogate function m-step. penalized surrogate function simply lasso penalty tune penalty parameter training-validation split. typical ideal case choose penalty parameter maximizes likelihood observed validation data. unfortunately likelihood observed data computationally intractable. instead property that diﬀerence log-likelihoods observed data bounded diﬀerence expected log-likelihoods complete data follows directly jensen’s inequality. expectation taken respect conditional distribution mutation orders given observed data sobs model parameter thus right-hand side estimated sampling mutation orders gibbs sampler algorithm right-hand side positive higher log-likelihood validation set. however right-hand side negative know parameters compare. proposal tuning penalty parameter algorithm based algorithm searches across one-dimensional grid penalty parameters largest smallest. consecutive penalty parameters estimate right-hand side determine smaller penalty parameter higher observed log-likelihood. keep shrinking penalty parameter estimate right-hand side negative. since check based conservative choosing penalty parameter slightly larger desired. nonetheless simulation results suggest procedure works well practice. algorithm easily extended incorporate multiple training-validation splits k-fold cross-validation average estimates righthand side across training-validation splits stop shrinking penalty parameter average negative. selecting penalty parameter obtain ﬁnal parameter support k-fold procedure reﬁtting penalized model whole training set. move second step goal quantify uncertainty estimated model parameters. unfortunately estimating conﬁdence intervals model selection diﬃcult problem even much simpler case linear models hence papers approach ﬁtting penalized model reﬁtting unpenalized model based selected variables using conﬁdence intervals generated using traditional inference procedures unpenalized models proceed manner reﬁt model maximizing unpenalized observed log-likelihood entire dataset respect selected variables constraining others zero; construct conﬁdence intervals unpenalized model ignoring fact already peeked data ﬁrst step. though conﬁdence intervals asymptotically valid restrictive conditions provide measure uncertainty ﬁtted parameters; show simulation section coverage intervals close nominal. highlight intervals truly conﬁdence intervals refer uncertainty intervals uncertainty intervals constructed using intervals nominal coverage. obtain uncertainty intervals calculate standard error estimates using estimate observed information matrix. louis shows observed information matrix related complete data likelihood following identity finally caveat method two-step procedure guaranteed give estimates standard errors/uncertainty intervals ﬁrst step procedure choose penalty parameter estimated information matrix second step positive deﬁnite. behavior small number simulations section though observe behavior data analysis. avoid issue suggest combining k-fold cross-validation algorithm average estimate lower gplv-licensed python implementation samm available http// github.com/matsengrp/samm. repository includes code used generating plots manuscript well tutorial samm. output sections well appendix available http//zenodo. org/record/ ./zenodo.. varying motif dictionary procedure diﬀerent models mutation process. section list example models using procedure discuss interplay motifs included feature-selection step. simplest case analogous existing work estimate k-mer model letting practice instead modeling eﬀects k-mers ﬁxed believe hazard rate position aﬀected positions closer case model eﬀect z-mers varying length e.g. k-mer motifs refer model hierarchical elements relate nested fashion. including motifs hierarchical fashion lasso penalty encourages z-mers inner -mer share mutation rate. model formalizes intuition used yaari estimate mutation rates -mers fall back using -mer sub-motif -mer appear enough times data. mentioned before oﬀset motifs motif dictionary previous work suggests mutation rates depend upstream downstream motifs instance include oﬀset motifs overlap mutating position motif dictionary. refer models oﬀset k-mer models. many example models overparameterized order obtain desired sparsity pattern. overparameterized models singular information matrices reﬁtting procedure. however problem since truly interested conﬁdence intervals parameters θagg associated simple k-mer model matrix aggregates hierarchical motifs single k-mer. since aggregate k-mer model identiﬁable uncertainty intervals θagg calculate pseudo-inverse information matrix ai−a estimate covariance matrix θagg. -mer model hazard rate modeled motif dictionary -mer per-target model hazard rate modeled motif -mer model hazard rate modeled motif dictionary according hierarchical structure model consider. model parameters corresponding motif corresponding motif target nucleotide m→n. obtain desired sparsity level randomly select portion parameters zero out. per-target parameters instead setting probability mutating zero possible values indicating mutation preference. scale model parameters appropriately control eﬀect size. goal simulations obtain synthetic data reﬂects different possible settings encounter analyzing experimental data. experimental data analyzed section template alter various underlying properties dataset simulate data replicates typical real-world datasets look like. ﬁrst generate na¨ıve sequences using partis drawing genes imgt database simulating observation frequency each. antibodies composed units heavy light chain. further light chains classed either depending encoded sequence came genome. mice humans antibodies structured way. select κ-light chain mouse bcrs simulation reﬂects experimental data section generate true parameters randomly draw values mouse somatic hypermutation targeting model rsnf refer parameters rsnf model collection mutabilities substitution probabilities -mer κ-light chain mouse data. average length na¨ıve sequences around nucleotides. survival model mutate positions na¨ıve sequence obtaining collection simulated sequences. conditional na¨ıve sequences sequences mutate independently. vary sparsity eﬀect size sample size follows. generate true parameters non-zero elements. also consider variance diﬀerent eﬀect sizes scaling finally model using variance values mutated sequences. main manuscript report simulation settings vary simulation setting settings middle value report result running hundred replicates setting. remaining possible settings separate model takes average hour complete replicates report results appendix section determine optimal penalty parameter samm split data gene subgroups externally-deﬁned categorization groups genes share least identity nucleotide level reserving subgroups validation remainder training. splitting gene subgroup ensures training validation sets look suﬃciently diﬀerent; otherwise sequences validation look nearly identical training select penalty parameter small. apply algorithm decreasing sequence penalty parameter-values starting value sequence penalty parameter values pre-tuned smaller smaller eﬀect sizes sample sizes. particular chose eﬀect size sample size eﬀect size sample size otherwise. penalty parameter-value maximum mcem iterations. mutation orders sampled gibbs sampler every eight sweeps initial burn-in period gibbs sweeps. e-step sample four mutation orders continue double number sampled mutation orders proposed estimate accepted ascent-based mcem. estimate support model reﬁt unpenalized model obtain uncertainty estimates. mcem model converged variance estimates estimated model parameters nonnegative. assess performance procedure using three measures. performance metrics calculated respect aggregate model since complete model overparameterized design. calculate relative true parameter also calculate kendall’s coeﬃcient well procedure ranks motifs terms mutabilities. finally calculate coverage approximate uncertainty intervals. deﬁne average coverage proportion aggregate model parameters uncertainty intervals covered true value. coverage calculations involve aggregate parameters zeroed models. simulations demonstrate estimation procedure performs expected sample size eﬀect size increase relative error decreases rank correlation increases. hand percent non-zero elements increases relative error rank correlation increase. error increases parameters estimate. increase rank correlation likely artifact metric calculated kendall’s removes ties calculations. particular percent non-zero elements increases number ties data decreases rank correlation seems increase. plots -mer per-target model tends diﬃcult estimate. expected contains parameters whereas -mer model parameters. simulations show coverages -mer -mer models close surprising uncertainty intervals ignore double-peeking issue zhao shojaie witten explain procedure might work certain assumptions variables selected lasso deterministic high probability using lasso select variables really constitute peeking data twice. however coverage -mer per-target much lower dropping certain settings suspect coverage mainly lack data coverage improves number samples. small number samples compared number parameters method provide reasonable ranking mutable motifs provide good estimates uncertainty intervals. across simulation runs twenty estimated information matrices positive deﬁnite therefore uncertainty intervals cannot calculated believe occurs selected penalty parameter small; small penalty parameters support ﬁtted model becomes large. case reﬁt model penalty parameter problem ill-posed therefore estimated information matrix positive deﬁnite. avoid issue recommend using k-fold cross-validation practice rather training/validation split. section compare performance samm shazam penalized logistic regression simulated data. since shazam estimates eﬀect -mer motifs simulate data mutation rate speciﬁc site depends -mer centered position target nucleotide. simulate sequences mice. mouse generate separate na¨ıve sequences using procedure section na¨ıve sequences simulate mutation process independently generate sequences. methods simulate mutation process survival simulation generate model parameters resampling values -mer per-target model structure. mutate na¨ıve sequences according survival model. shmulate function shazam package shmulate simulates mutation process using procedure similar survival model. however exact calculations differ somewhat shazam advantage shmulate simulations since estimated using shazam separate dataset shazam uses prior assumptions model structure. particular shazam assumes -mer motifs share certain upstream/downstream nucleotides similar mutabilities. simulations sequence mutated. mutation rate aﬃnity-matured sequences even though shazam advantage simulate data using dense model shmulate. logistic regression shazam tended produce similar estimates though logistic regression tended better simulated using survival model shazam tended better used shmulate model. present results model ﬁtting detail figure negative values methods biased towards zero though shazam logistic regression tend positive values samm nearly unbiased shazam logistic regression somewhat biased towards zero. methods probably trouble estimating negative values since observe small number mutations sequence data informative ﬁnding motifs high mutation rates rather mutation rates. based results section expect bias samm shrink number training observations increases. models sequence data obtained vaccination study four transgenic mice published experimental setting substitutions present κ-light chain sequences unlikely aﬀected natural selection function. thus restrict analysis κ-light chain data order estimate somatic hypermutation rates rather combination somatic hypermutation selection single na¨ıve comparison samm shazam penalized logistic regression given simulated b-cell receptor sequences mice. relative error kendall’s computed separately replicates. monte carlo standard errors calculated estimates sequence give rise many diﬀerent b-cell receptors somatic hypermutation forming so-called clonal family varying levels shared evolutionary history. partis assign mutated sequences clonal families infer likely na¨ıve sequence family. sequencing clonal family inference possibility error propagation; begin analysis assuming bcrs accurately sequenced assigned clonal families. resulting data composition shown table mitigate double-counting mutations sample single mutated sequence clonal family. though discards data believe gives accurate estimates approaches data estimate mutation history; analyze issue depth section appendix. -mer model using samm using settings though -fold cross-validation determine optimal parameter support. estimate block-like -fold-repetitive pattern many -mer motifs zeroed lasso step. uncertainty intervals suggest many motifs marked nonzero eﬀect. mutable -mer motifs match classical spot motif reverse complement also conﬁrm many less mutable -mer motifs match canonical cold spot syc/grs example -mers estimate high mutability aagct form nngyw ends -mer gyw. example nucleotide example aagct example spot motif gyw. model also reveals shortcomings current cold spot definitions. estimates show signiﬁcant variability mutabilities motifs even contain cold spot motif. instance established literature atggc motif considered cold spot since form grs. estimate value large relative values suggesting actually spot. also shazam estimates motifs form cccnn negative mutability examples known cold spot syc. estimates samm show cccgn positive mutability even though also form indicating inner -mer increase mutation rate nucleotides left mutating position. addition classic spots central nucleotide actually mutability estimates; suggests using well-known wa/tw identify spots appropriate. finally model suggests samm used discover cold spots. example consider motifs central base mutating. mutabilities -mer cacgc -mers higher motif form wrc. motifs form indicates nucleotide immediately preceding mutating aﬀect mutation rate nucleotide bases away. well-deﬁned inferential procedure determine signiﬁcant collections cold spots ample support data require additional future work. comparison shazam data without sampling single sequence clonal family done yaari also logistic model data samm. models data determine degrees freedom ﬁtting resulting number unique values less saturated model size -mer model. shazam estimated unique values maximum samm estimated unique values logistic estimated visually estimates three models look similar similar hotcold-spots though shazam spiky samm logistic terms model interpretability samm logistic regression seem preferable shazam produce much parsimonious models. logistic model seems model intermediate samm shazam terms parameter support. ideally would able compare diﬀerent methods terms observed data likelihood test set. however methodological diﬃculties incompatibilities methods unable come concrete compare methods. particular shazam likelihood-based method. addition observed data likelihood samm computationally intractable makes diﬃcult compare likelihood-based methods. hope come good solution assessing samm real-world data future. modeled somatic hypermutation sequences using proportional hazards. context-dependence mutation rates must take account unknown mutation order compute full likelihood. deal estimated somatic hypermutation model mouse light chains using samm -mer motifs centered bases motif corresponding x-axis position read bottom top. plots depict estimated aggregate -mer motifs estimating model -mer model aggregating estimates using procedure outlined section negative value means reduced mutation rate relative baseline hazard whereas positive means enhancement. well-known spots wrc/gyw wa/tw colored green respectively. well-known cold spot syc/grs colored blue. motifs colored grey. uncertainty intervals estimates depicted black lines center bar. comparison ﬁtted aggregate values shazam samm logistic regression -mer motifs central base samm figure samm logistic -mer aggregated -mer models. samm logistic tend parsimonious models compared shazam left plot looks spiky middle right ones. samm produces parsimonious among three methods. missing data used mcem marginalize possible mutation orders using markov chain monte carlo. unlike current methods regression framework model eﬀect arbitrary features varying motif lengths sequence positions. paper lasso perform feature selection stabilize estimates high-dimensional settings. easily extend approach sparsity-inducing penalties reﬂect prior beliefs model structure. show samm achieves better performance state-of-the-art method variety simulation settings. limitations current method. currently subsample data signiﬁcantly ensure training composed independent observations. would necessary able perform accurate phylogenetic ancestral sequence estimation using context-sensitive models. addition method returns uncertainty intervals rather conﬁdence intervals since guarantees nominal coverage. simulations show uncertainty intervals close nominal coverage levels suﬃcient amount data better methods available. present analysis considers sequence context biologicallymotivated features informative nucleotide position proximity contexts etc. incorporating types features model able help verify problems currently accepted model somatic hypermutation finally model used contexts model biological processes. instance method could used model rate singlenucleotide polymorphisms transcriptionfactor binding grateful duncan ralph assistance performing sequence annotation clustering simulating germline repertoires. would like thank kleinstein generously sharing sequences especially jason vander heiden providing preprocessed versions sequence data. jean feng supported grants dpod tca. noah simon supported grant dpod. david shaw vladimir minin frederick matsen supported grants u-ai r-gm; david shaw frederick matsen also supported rai. research frederick matsen supported part faculty scholar grant howard hughes medical institute simons foundation.", "year": "2017"}