{"title": "Causality Testing: A Data Compression Framework", "tag": "q-bio", "abstract": " Causality testing, the act of determining cause and effect from measurements, is widely used in physics, climatology, neuroscience, econometrics and other disciplines. As a result, a large number of causality testing methods based on various principles have been developed. Causal relationships in complex systems are typically accompanied by entropic exchanges which are encoded in patterns of dynamical measurements. A data compression algorithm which can extract these encoded patterns could be used for inferring these relations. This motivates us to propose, for the first time, a generic causality testing framework based on data compression. The framework unifies existing causality testing methods and enables us to innovate a novel Compression-Complexity Causality measure. This measure is rigorously tested on simulated and real-world time series and is found to overcome the limitations of Granger Causality and Transfer Entropy, especially for noisy and non-synchronous measurements. Additionally, it gives insight on the `kind' of causal influence between input time series by the notions of positive and negative causality. ", "text": "causality testing determining cause eﬀect measurements widely used physics climatology neuroscience econometrics disciplines. result large number causality testing methods based various principles developed. causal relationships complex systems typically accompanied entropic exchanges encoded patterns dynamical measurements. data compression algorithm extract encoded patterns could used inferring relations. motivates propose ﬁrst time generic causality testing framework based data compression. framework uniﬁes existing causality testing methods enables innovate novel compression-complexity causality measure. measure rigorously tested simulated real-world time series found overcome limitations granger causality transfer entropy especially noisy non-synchronous measurements. additionally gives insight ‘kind’ causal inﬂuence input time series notions positive negative causality. causality testing used widely various disciplines including neuroscience physics climatology econometrics epidemiology number causality testing methods exist applied estimate magnitude direction causality between time series listed fig. methods make diﬀerent model assumptions input time series making appropriate choice often diﬃcult given context. thus need unifying framework explain working measures suitably guide application. according wiener incorporating past time series helps improve prediction time series causes existing methods based notions improved predictability reduction uncertainty dynamical modeling estimation based proximity attractor manifold notions closely related information transfer form other. establishing relation information reality world basis causality information transfer increasingly shown rigorous physical foundation information transfer complex systems fundamentally linked energy entropy ﬂows. case living complex systems information created self-organization resulting re-ordering entropy reduction interior corresponding increase external environment. causal relationships based entropic exchanges encoded patterns dynamical measurements. ‘data compression’ framework captures patterns seems natural approach extract causal relationships. inspired decades work data compression propose ﬁrst time generic framework causality testing. data compression concerned encoding information either modeling statistical redundancy learning patterns algorithmically reducing resources store transmit data. framework well founded mathematically owing close link data compression information entropy algorithmic complexity fig. gives block diagram framework list possible choices block below. table describes work-ﬂow various causality testing methods indicating choice block. describe three methods examples. granger causality uses blocks ‘model’ ‘testing criteria’ transfer entropy uses three blocks ‘pre-processing’ etc.) ‘model’ ‘testing criteria’ convergent cross mapping uses blocks ‘model’ ‘testing criteria’ methods clearly within framework makes assumptions nature data ﬂexible easily conﬁgurable given application appropriate choice block. furthermore framework lends invention novel methods causality testing. work propose measure compression-complexity causality described below. minimum description length principle formalizes occam’s razor states best hypothesis given data leads best compression. extending principle causality estimation compressibility time series remains unchanged even upon incorporating information time series conclude causal inﬂuence however change compressibility included model infer causality compression-complexity either lz/etc compression-complexity measure could used compute work found perform better short noisy time series given series ﬁrst binned converted sequence symbols using uniformly sized bins application complexity measures. binned time series length determine whether causes consider moving window length deﬁne compression-complexity rates follows compressibility estimated based windows immediate past values xpast ypast taken respectively. refers appending e.g. time series gives compressioncomplexity rate deﬁned eﬀort-to-compress knowing recent past alone. compression-complexity rate knowing recent pasts refer here cccy cccx→y cases depend linearly past. depends linearly past dependence non-linear. kind information transferred whereas this infer compressibility reduced non-linear inﬂuence case happens kind information brought past compress diﬀerent kind information brought past itself. here cccy negatively causes hand case cccy positively causes speak causing also infer kind information transferred based sign cccy case certain kinds non-linear inﬂuence time series lead negative causality could mechanisms well. diﬀerence time averaged compression-complexity rates entire length time series window slided step-size cccy statistically zero implying causal inﬂuence cccy statistically signiﬁcant diﬀerent zero infer causes higher magnitude cccy implies higher degree causation. formulation striking resemblance transfer entropy fact terms asymptotically approach entropy rates used formulation stationary ergodic processes computed using optimal lossless data compression algorithm however important diﬀerences ccc. said cause reduction entropy rate included model however limited fact model strictly assumes markovian property valid given data entropy rate included never increase contrast model non-linear generic since based optimal lossless data compression algorithms compression-complexity rate included either decrease indicating positive causality increase indicating negative causality notion positive negative causality propose analogous concept positive negative correlation. richer characterization causality discussed literature. consider following cases. case minimally coupled autoregressive processes sampling period noise terms noise intensity follows standard normal distribution. here cccy cccx→y case non-linearly coupled deterministic processes sampling period second test involved simulation linearly non-linearly coupled chaotic tent maps independent process linear coupling dependent process non-linear coupling strength coupling varied simulations. fig. shows mean values causality trials estimated using linear non-linear coupling settings used assumption linear model estimation proved erroneous trials hence values displayed. increased linear non-linear coupling increases positive direction falls zero series become completely synchronized trend magnitude values similar however cccy increment negative direction. non-linear inﬂuence results negative causality. parameter selection selection parameters simulations done based investigations nature computation compression-complexity time series. criteria rationale discussed detail section supplemental material. criteria applied case real-world data results discussed below. testing real-world data applied estimate causality measurements real-world systems compared system comprised short time series dynamics complex ecosystem point recording predator prey populations reported originally acquired ﬁrst points series removed eliminate transients settings used seen aptly capture higher causal inﬂuence predator prey population lower inﬂuence opposite direction latter expected owing indirect eﬀect change prey population predator. results line obtained using hand fails capture correct causality direction. system comprised single-unit neuronal membrane potential recordings squid giant axon response stimulus current recorded made available test causation three axons labeled ‘at’ ‘at’ ‘at’ extracting points recording. settings used testing simulations performance compared case minimally coupled processes simulated deﬁned before. spurious causalities using case noise temporal resolution discussed literature here move step ahead present results non-uniformly sampled/non-synchronous measurements common real-world physiological data acquisition jitters/motion-artifacts well economics realistically simulate scenario non-uniform sampling introduced eliminating data random locations dependent time series presenting data knowledge time-stamps missing data. percentage non-uniform sampling/non-synchronous measurements percentage missing data points. mean causality estimated trials using three measures increasing noise intensity shown fig. increasing shown fig. length time series settings used estimation markovian models order assumed throughout paper. estimates positive causality statistically zero opposite direction. values stable cases show mildly increasing trend causation increased. contrast show confounding values estimated causality directions increasing conclusion important contribution work unifying data compression framework causality testing absorbs diverse methods information etc. framework also enabled propose novel compression-complexity causality measure outperforms noisy non-uniformly sampled simulated stochastic data real-world time series. negative case linearly non-linearly coupled chaotic maps giving rise notion negative causality. insightful characterization absent existing measures including existing methods blocks framework hoped framework would inspire novel measures invented future. indicate possibilities. example started developing causality testing based popular compression algorithm measure blocks framework except quantizer. initial testing measure gave promising preliminary results needs experimentation. anexample novel measure would compressed sensing based measure sparse signals eﬃciently model wide class compressible signals. futuristic example would deep learning based causality measure also conceivable within framework. futuristic measures suggested merely illustrative limited imagination. ccci→v less approximately equal cccv values less zero three axons indicating negative causality directions. implies bidirectional non-linear dependence values capture similar causality magnitude relationship squid axons ‘at’ ‘at’ however fails ‘at’. causality covered main paper. explain idea compressioncomplexity computed individual pair time series. also describe criteria rationale choosing parameters details matlab implementation made available free download use. single unique deﬁnition complexity. noted shannon entropy popular intuitive measure complexity. value shannon entropy indicates high redundancy structure data high value indicates redundancy high randomness ergodic sources owing shannon’s noiseless source coding theorem compressibility data directly related shannon entropy. however robustly estimating compressibility using shannon entropy short noisy time series challenge recently notion compression-complexity introduced circumvent problem. compression-complexity deﬁnes complexity time series using optimal lossless data compression algorithms. well acknowledged data compression algorithms useful compression data eﬃcient transmission storage also models learning statistical inference lempel-ziv complexity eﬀort-to-compress measures fall category. deﬁned eﬀort compress input sequence using lossless compression algorithm known non-sequential recursive pair substitution demonstrated outperform shannon entropy accurately characterizing dynamical complexity stochastic deterministic chaotic systems presence noise further shown reliably capture complexity short time series even fails analyzing short tachograms healthy young subjects peerj. thus make deﬁning though possible deﬁne using since expects symbolic sequence input given time series binned appropriately generate sequence. symbolic sequence available proceeds parsing entire sequence pair symbols sequence highest frequency occurrence. pair replaced symbol create symbolic sequence procedure repeated iteratively terminates constant sequence since length output sequence every iteration decreases algorithm surely halt. number iterations needed convert input sequence constant sequence deﬁned value complexity. example input table criteria rationale choosing parameters ccc. values parameter chosen autoregressive tent squid giant axon system predator prey ecosystem enlisted rightmost column. please refer main paper details four systems. sequence gets transformed follows thus achieves minimum value constant sequence maximum value length sequence distinct symbols. thus normalize complexity value dividing thus normalized note normalized values always values indicating complexity high values indicating high complexity. perform straightforward extension mentioned procedure computing joint measure pair input time series length. every iteration algorithm scans simultaneously sequences replaces frequent jointly occurring pair symbol pairs. illustrate table summarize criteria rationale choosing four parameters proposed measure ccc. based preliminary investigations explorations nature compression-complexity various time series. expect reﬁne criteria future. parameter length moving window ﬁxed datasets used work. chosen contains suﬃcient number data points rate reliably estimated. earlier studies revealed able reliably capture complexity even short time series step size well xpast window moved chosen based criteria suﬃcient overlap successive xpast windows length number bins used generate symbolic sequence input time series chosen suﬃcient capture underlying dynamics. found processes suﬃcient whereas time series chaotic tent requires least chosen choose window length xpast. this analyze curves measure varies diﬀerent appended well non-appended time series estimate individual well joint compression-complexities detailed description selection criteria discussed below. figure averaged curves left subﬁgure curves right subﬁgure linearly coupled tent maps causing incremented value data points time. using ﬁrst criteria selection discussed table given time series ﬁrst plot well separate graphs. choose value curves graphs well separated. work start figs. show curves plotted linearly non-linearly coupled tent figure averaged curves left subﬁgure curves right subﬁgure linearly coupled tent maps causing incremented value data points time. using ﬁrst criteria selection figure averaged curves left subﬁgure curves right subﬁgure predator prey ecosystem representing didinium population representing paramecium population. incremented value data points time. using ﬁrst criteria selection separation curves exhibits heterogeneous time series diﬀerent values homogeneous time series respectively. clear separation curves cannot merely accounted statistical variations likely traced kind causal relationship. figure averaged curves left subﬁgure curves right subﬁgure squid giant axon system representing applied stimulus current representing observed voltage. incremented value data points time. using ﬁrst criteria selection lower values used despite suﬃcient separation avoid making computation based transient stage values. figs. rationale behind criteria similar encountered case ﬁrst criteria. here idea ensure joint complexities heterogeneous time series diﬀerent values homogeneous time series respectively. given separation safely compute diﬀerences rate estimation conviction diﬀerence arising statistical diﬀerences. have encountered case criteria fail. sure whether plausible. even case independent uniformly distributed real time series though ﬁrst criteria fails second valid value used estimated range compute result value statistically close zero figure averaged curves independent processes incremented value data points time. using second criteria selection based ﬁgure fig. avoiding range giving transient values ccc.", "year": "2017"}