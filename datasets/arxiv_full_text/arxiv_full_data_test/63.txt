{"title": "A comparative study of divisive hierarchical clustering algorithms", "tag": "q-bio", "abstract": " A general scheme for divisive hierarchical clustering algorithms is proposed. It is made of three main steps : first a splitting procedure for the subdivision of clusters into two subclusters, second a local evaluation of the bipartitions resulting from the tentative splits and, third, a formula for determining the nodes levels of the resulting dendrogram. A number of such algorithms is given. These algorithms are compared using the Goodman-Kruskal correlation coefficient. As a global criterion it is an internal goodness-of-fit measure based on the set order induced by the hierarchy compared to the order associated to the given dissimilarities. Applied to a hundred of random data tables, these comparisons are in favor of two methods based on unusual ratio-type formulas for the splitting procedures, namely the Silhouette criterion and Dunn's criterion. These two criteria take into account both the within cluster and the between cluster mean dissimilarity. In general the results of these two algorithms are better than the classical Agglomerative Average Link method. ", "text": "abstract. general scheme divisive hierarchical clustering algorithms proposed. made three main steps first splitting procedure subdivision clusters subclusters second local evaluation bipartitions resulting tentative splits third formula determining nodes levels resulting dendrogram. number algorithms presented. algorithms compared using goodman-kruskal correlation coefficient. global criterion internal goodness-of-fit measure based order induced hierarchy compared order associated given dissimilarities. applied hundred random data tables comparisons favor methods based unusual ratio-type formulas splitting procedures namely silhouette criterion dunn's criterion. criteria take account within cluster cluster mean dissimilarity. general results algorithms better classical agglomerative average link method. papers relative hierarchical clusterings four popular agglomerative methods namely single link method average link method complete link method ward’s method. goal methods represent proximities dissimilarities objects tree objects situated branches generally bottom graph junctions branches called nodes tree; node levels supposed represent intensity ressemblance objects clusters joined. agglomerative procedure tree constructed bottom-up beginning object considered cluster called singleton. step procedure consists creating cluster merging closest clusters. implies compute dissimilarity distance clusters. instance usual average link method distance clusters mean value between-cluster distances value used node level junction branches issued indeed shown that usual procedures monotonic. means cluster included cluster associated node levels increasing order ensures hierarchical tree built without branch crossings. thus formula used first criterion merging clusters second determining node levels hierarchy. divisive hierachical algorithms built top-down starting whole sample unique cluster split cluster subclusters turn divided subclusters step clusters make so-called bipartition former. well known ways splitting objects subsets. therefore time consuming base splitting protocol trial possible bipartitions. present paper proposes evaluate restricted number bipartitions make working algorithm. idea developed long time macnaughton-smith reused kaufman rousseeuw present work aims treatment small moderate size datasets search quality results. framework complete binary hierarchies looked choice cluster split relevant clusters including objects split turn remain singletons. three points studied following applying principles gives rise family algorithms described section practical benchtest made used comparing algorithms classical average link procedure. main results gathered concluding section terminates paper number splitting procedures designed past oldest williams lambert procedure said monothetic sense object sets split according values variable. idea updated using principal component instead single variable another approach around complexity splitting extract several objects split. macnaughton-smith proposed select distant object cluster seed separate cluster. aggregate seed objects closer subset rest current cluster similar idea developed hubert suggested pair objects seeds bipartition. choice select objects dissimilar build subclusters according distances seeds. exploiting idea roux considered bipartitions generated pairs objects retaining bipartition best evaluation priori criterion. procedure applied following. finally another divisive algorithm based usual k-means method partitioning objects. called bisecting k-means procedure builds successive dichotomies -means algorithm either random initial partition partition using procedures. step usual agglomerative method candidate clusters merging step considered bipartition instance case classical average link method bipartition evaluated mean value between-cluster distances. divisive methods cluster split selected next step study number bipartitions between-cluster average distances used evaluating split another criterion suggested agglomerative framework moreover number criteria designed evaluation partition used. described section used applications ward’s criterion also considered distance-like criterion. indeed paper székely rizzo exists infinite family algorithms similar ward’s. present study focus them. original algorithm described j.h. ward second defined parameter székely-rizzo family. between-cluster distances involved algorithms designated respectively murtagh legendre main idea using criteria take account betwen cluster distances also distances neighboring objects clusters studied. popular though ancient criterion dunn designed evaluate partition number elements used agglomerative scheme usual five criteria examined section used without problem representation results criterion value becomes level corresponding node drawing hierarchical tree show cross-over branches. unfortunately divisive procedures general enjoy property nonoptimality successive splittings. rule needed obtain consistent node levels true tree representation. kaufman rousseeuw program diana diameter successive clusters node levels. evident diameter subset included less than equal diameter fulfilling monotonic condition thus subsets created splitting always associated lower node levels. another settle consistent node levels would associate ranks nodes according order created starting rank level last created node. method satisfying; effect happen small homogeneous susbet would separated early stage bulk objects. corresponding node would associated high rank spite homogeneity. another ranks would renumber nodes bottom completion splittings free difficulties either. indeed present discussion little general purpose comparing clustering algorithms since global evaluation results based rank correlation methods however users could interested getting coherent node levels hence working representation. present experiment node levels determined program diana diameters. algorithms divisive algorithms studied though exhaust possibilities algorithms following principles section following list enumerated type bipartition criteria. basic reference usual average link agglomerative method included study. present work divisive algorithms based two-seeds splitting procedure except macnaughton-smith method pddp algorithm. contrary rule applied agglomerative methods highest value criterion indicates divisive scheme bipartion use. case ties first appeared split selected. however small modification adopted complete link divisive method splitting criterion smallest values diameters candidate subsets instead highest between-cluster distance. variants dunn's formula used silhouette formula also applied. distance-like formulas successive splits ratio-type criteria maximize criteria. sake comparisons existing algorithms added list together classical average link agglomerative method. first principal directions divisive partitioning second algorithm proposed macnaughton-smith listed table pddp method first designed analysis observations variables data tables readily adapted using principal coordinates analysis technique akin principal components analysis pddp algorithm first principal coordinate axis used create dichotomy objects whose coordinates negative first subset dichotomy objects positive null coordinates make second subset. however step taken move objects actual assignment long closer latter former. pcoa recomputed cluster objects achieving hierarchical divisive procedure. method proposed macnaughton-smith could considered one-seed procedure. split cluster choose seed object whose average distance elements maximum. building bipartition begins comparison algorithms restricted quality results measured goodness-of-fit results data. first benchmark made random data sets described second thoughts goodness-of-fit criteria developed. third tentative estimation algorithmic complexity studied finally summary comparisons sample rectangular matrices generated uniform distribution variables generated independently according distribution. matrix usual euclidean distance applied order obtain distance matrices input clustering programs. although data real life data constitute harsh benchmark allow real competition among programs. ultrametric distances associated dendrogram; cpcc usual correlation coefficient input distances ultrametric distances values laid long vectors. present work focus rather oriented toward rank correlation methods. indeed user examines hierarchy issued data user focuses mainly groups subgroups disclosed algorithm; words interest mostly structure topology tree rather exact values within groups distances. addition results hierarchical algorithms given terms distances; case particular original ward’s method node levels represent variations variance. kendall’s goodman-kruskal‘s coefficient based ranks values compared. input distance objects ultrametric distance objects resulting clustering algorithm. index number concordant pairs distances number discordant pairs pairs objects constitute quadruple said concordant number distance pairs equal number objects. coefficients differ denominator. goodman-kruskal denominator number quadruples really taken account kendall’s denominator equal number quadruples including possible ties. seems reasonable take account tied pairs numerous ultrametric distances induced hierarchy. addition number pairs really comparable much lower case true correlation coefficient. instance dendrogram figure pairs compared cluster including included cluster including hand cannot compared pairs included again relation could established pairs reason. remarks make computation goodman-kruskal coefficient little complicated applying existing software function. computation non-standard cluster dissimilarities implies preserve initial distance matrix computer memory. step divisive process needs examination objects pairs potential seeds dichotomy. since updating formula like agglomerative algorithms evaluating dichotomy implies recompute splitting criterion elements corresponding bipartition. evaluation order number bipartitions therefore complexity divisive step construction full binary hierarchy needs steps overall complexity proposed divisive algorithms involves heavy computer task still possible moderate size target data. experiment conducted according conditions results table rows columns cell table includes goodmankruskal coefficient relative data algorithm. higher coefficient better corresponding algorithm since coefficient akin correlation coefficient. table gathered average values coefficients datasets. algorithms sorted average values goodman-kruskal coefficient decreasing order. best first algorithms appear silhouette based dunn variant methods. next come principal direction divisive method divisive average link method. none declared best algorithm since them turn show best value goodman-kruskal coefficient depending data hand. silhouette based dunn’s variant principal direction average link divisive macnaughton-smith ward székély-rizzo variant ward’s original formula average link agglomerative single link dunn’s original formula complete link lower part ranking appear three algorithms namely based single link complete link respectively based dunn’s original formula. average three algorithms perform less well usual agglomerative average link method. study compared divisive hierarchical clustering algorithms taking input dissimilarity datasets. usual average link agglomerative method added basic reference. five algorithms based popular formulas used pairwise aggregative procedures namely single link complete link average link methods versions ward’s algorithm. divisive algorithms based splitting criteria involving ratio between-group dissimilarities within-group dissimilarities. important argument present work possible separate computation hierarchical node levels criterion used splitting cluster. question readable dendrogram solved using diameters clusters. hamper internal evaluation results comparison hierarchy initial data thanks goodman-kruskal correlation coefficient. comparing order relation induced successive inclusions clusters order relation associated input dissimilarities correlation coefficient provides evaluation independant node levels concentrates shape dendrogram certainly main interest user. applied sample hundred random datasets principles allow ranking algorithms. best ones based ratio-type splitting criteria silhouette formula variant dunn’s formula partitions. lower ranking appear various ward’s procedures dunn’s original formula together complete link single link based procedures dissuades them. finding confirms previous considerations ward’s agglomerative hierarchical algorithm", "year": "2015"}