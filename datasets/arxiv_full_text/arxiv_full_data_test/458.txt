{"title": "Data-driven Advice for Applying Machine Learning to Bioinformatics  Problems", "tag": "q-bio", "abstract": " As the bioinformatics field grows, it must keep pace not only with new data but with new algorithms. Here we contribute a thorough analysis of 13 state-of-the-art, commonly used machine learning algorithms on a set of 165 publicly available classification problems in order to provide data-driven algorithm recommendations to current researchers. We present a number of statistical and visual comparisons of algorithm performance and quantify the effect of model selection and algorithm tuning for each algorithm and dataset. The analysis culminates in the recommendation of five algorithms with hyperparameters that maximize classifier performance across the tested problems, as well as general guidelines for applying machine learning to supervised classification problems. ", "text": "bioinformatics ﬁeld grows must keep pace data algorithms. contribute thorough analysis state-of-the-art commonly used machine learning algorithms publicly available classiﬁcation problems order provide data-driven algorithm recommendations current researchers. present number statistical visual comparisons algorithm performance quantify eﬀect model selection algorithm tuning algorithm dataset. analysis culminates recommendation algorithms hyperparameters maximize classiﬁer performance across tested problems well general guidelines applying machine learning supervised classiﬁcation problems. bioinformatics ﬁeld increasingly relying machine learning algorithms conduct predictive analytics gain greater insights complex biological processes human body. example algorithms applied great success gwas proven eﬀective detecting patterns epistasis within human genome. recently deep learning algorithms used detect cancer metastases high-resolution pathology images levels comparable human pathologists. results among others indicate heavy interest development analysis bioinformatics applications. owing development open source packages active research ﬁeld researchers easily choose dozens algorithm implementations build predictive models complex data. although several readily-available algorithm implementations advantageous bioinformatics researchers seeking move beyond simple statistics many researchers experience choice overload diﬃculty selecting right algorithm problem hand. result ml-oriented bioinformatics projects could improved simply better algorithm. researchers aware challenges algorithm selection presents practitioners. result eﬀorts empirically assesses diﬀerent algorithms across sets problems beginning statlog project. early work ﬁeld also emphasized bioinformatics applications. recently caruana fern´andez-delgado analyzed several supervised learning algorithms coupled parameter tuning. aforementioned literature often compared many algorithms relatively example problems using upwards example problems. time since assessments researchers moved towards standardized open source implementations algorithms number publicly available datasets used comparison skyrocketed leading creation decentralized collaboration-based analyses openml project. however value focused reproducible experiments still paramount. observations motivated work conduct contemporary open source thorough comparison algorithms across large publicly available problems including several bioinformatics problems. paper take detailed look popular open source algorithms analyze performance across supervised classiﬁcation problems order provide data-driven advice practitioners wish apply datasets. part comparison full hyperparameter optimization algorithm. results highlight importance selecting right algorithm problem improve prediction accuracy signiﬁcantly problems. further empirically quantify eﬀect hyperparameter tuning algorithm demonstrating marked improvements predictive accuracy nearly algorithms. show underlying behaviors various algorithms cluster terms performance might expected. finally based results experiments provide reﬁned recommendations algorithms parameters starting point future researchers. study compared popular algorithms scikit-learn widely used library implemented python. algorithm hyperparameters described table algorithms include na¨ıve bayes algorithms common linear classiﬁers tree-based algorithms distance-based classiﬁers ensemble algorithms non-linear kernel-based strategies. goal represent common classes algorithms used literature well recent state-of-the-art algorithms gradient tree boosting. algorithm hyperparameters tuned using ﬁxed grid search fold cross-validation. results compare average balanced accuracy folds order account class imbalance. used expert knowledge reasonable hyperparameters specify ranges values tune algorithm. worth noting attempt control number total hyperparameter combinations budgeted algorithm. result algorithms parameters advantage sense training attempts dataset. however goal report close best performance possible algorithm dataset reason chose optimize algorithm thoroughly possible. algorithms compared supervised classiﬁcation datasets penn machine learning benchmark pmlb collection publicly available classiﬁcation problems standardized format collected central location easy access pythona. although limited problems biology medicine pmlb includes many biomedical classiﬁcation problems including tasks disease diagnosis post-operative decision making exon boundary identiﬁcation among others. sample biomedical classiﬁcation tasks contained pmlb listed table prior evaluating algorithm scaled features every dataset subtracting mean scaling features unit variance. scaling step necessitated algorithms distance-based classiﬁers assume features datasets scaled appropriately beforehand. entire experimental design consisted million algorithm parameter evaluations total resulting rich data analyzed several viewpoints section additional contribution work provided complete code required conduct algorithm hyperparameter optimization study well access analysis resultsb. allows researchers easily compare algorithm performance datasets similar conduct analysis pertaining research. section analyze algorithm performance results several lenses. first compare performance algorithm across datasets terms best balanced accuracy section look eﬀect hyperparameter tuning model selection section finally analyze algorithms cluster across tested problems present algorithms maximize performance across datasets section simple bulk measure compare performance algorithms plot mean rankings algorithms across datasets figure ranking determined -fold balanced accuracy algorithm given dataset lower ranking indicating higher accuracy. rankings show strength ensemble-based tree algorithms generating accurate models ﬁrst second fourth-ranked algorithms belong class algorithms. three worst-ranked algorithms also belong class na¨ıve bayes algorithms. order assess statistical signiﬁcance observed diﬀerences algorithm performance across problems non-parametric friedman test. complete experiments indicate statistically signiﬁcant diﬀerences according test present pairwise post-hoc analysis table post-hoc test underlines impressive performance gradient tree boosting signiﬁcantly outperforms every algorithm except random forest level. spectrum multinomial signiﬁcantly outperformed every algorithm except gaussian strong statistical results interesting given large problems algorithms loss loss function optimized. penalty whether lasso ridge elasticnet regularization. alpha regularization strength. learning rate shrinks contribution successive training update. intercept whether intercept linear classiﬁer computed. ratio ratio lasso ridge reguarlization use. used ‘penalty’ elasticnet. initial learning rate. power exponent inverse scaling learning rate. kernel ‘linear’ ‘poly’ ‘sigmoid’ ‘rbf’. penalty parameter regularization. gamma kernel coef. ‘rbf’ ‘poly’ ‘sigmoid’ kernels. degree degree ‘poly’ kernel. coef independent term ‘poly’ ‘sigmoid’ kernels. weight fraction leaf minimum number samples node considered leaf. controls depth complexity decision tree. features number features consider computing best node split. criterion function used measure quality split. estimators number decision trees ensemble. weight fraction leaf minimum number samples node considered leaf. controls depth complexity decision trees. features number features consider computing best node split. criterion function used measure quality split. estimators number decision trees ensemble. learning rate shrinks contribution successive decision tree ensemble. loss loss function optimized gradient boosting. depth maximum depth decision trees. controls complexity decision trees. features number features consider computing best node split. compared here. free lunch theorem guarantees algorithms perform average possible classes problems diﬀerentiated results imply problems pmlb belong related subset classes. initial pmlb study also noted similarity properties several publicly available datasets could lead allbp allhyper allhypo ann-thyroid biomed breast-cancer-wisconsin breast-cancer diabetes w-a-.n w-a-.n liver-disorder molecular-biology promoters postoperative-patient-data diagnosis diagnosis diagnosis diagnosis diagnosis diagnosis diagnosis diagnosis locating exon boundaries simulated gwas simulated gwas diagnosis identify promoter sequences choose post-operative treatment inﬂated statistical signiﬁcance. nevertheless cannot denied results relevant classiﬁcation tasks encountered real-world biological contexts since vast majority datasets used taken contexts. given bulk results tempting recommend top-ranked algorithm problems. however neglects fact top-ranked algorithms outperform others problems. furthermore simpler algorithms perform complex often preferable choose simpler two. mind investigate pair-wise outperformance calculating percentage datasets algorithm outperforms another shown figure algorithm outperforms another dataset least higher -fold balanced accuracy represents minimal threshold improvement predictive accuracy. terms outperformance worth noting algorithm performs best across datasets. example datasets multinomial performs well better gradient tree boosting despite overall worstbest-ranked algorithms respectively. therefore still important consider diﬀerent algorithms applying datasets. algorithms contain several hyperparameters aﬀect performance signiﬁcantly experimental results allow measure extent hyperparameter tuning grid search improves algorithm’s performance compared baseline settings. also measure eﬀect model selection improving classiﬁer performance. gorithm hyperparameters tuning often improves algorithm’s accuracy depending algorithm. cases parameter tuning accuracy improvements figure shows improvement -fold accuracy attained model selection hyperparameter optimization compared average performance dataset. results demonstrate selecting best model tuning leads approximately increase accuracy improvement certain datasets. thus selecting right algorithm tuning parameters vitally important problems. fig. heat showing percentage datasets given algorithm outperforms another algorithm terms best accuracy problem. algorithms ordered bottom based overall performance problems. algorithms considered performance problem achieved accuracy within other. modeling techniques common. assess whether holds cluster performance diﬀerent algorithms across datasets. perform hierarchical agglomerative clustering -fold balanced accuracy results leads clusters shown figure indeed algorithms similar underlying assumptions methodologies cluster terms performance across datasets. example na¨ıve bayes algorithms perform similarly other linear algorithms also cluster. ensemble algorithms extra trees random forests ensembles decision trees also cluster. support vector machines gradient tree boosting appear quite diﬀerent algorithms given able capture nonlinear interactions variables less surprising cluster well. algorithms parameters maximize coverage benchmark datasets meaning perform within best -fold balanced accuracy obtained maximum number datasets experiment. datasets pmlb algorithms associated parameters cover datasets within balanced accuracy. notably datasets covered tuning parameters listed algorithms. based available evidence recommended algorithms good starting point achieving reasonable predictive accuracy dataset. empirically assessed supervised classiﬁcation algorithms supervised classiﬁcation datasets order provide contemporary recommendations bioinformaticians wish apply algorithms data. analysis demonstrates strength state-of-the-art tree-based ensemble algorithms also showing problemdependent nature algorithm performance. addition analysis shows selecting right algorithm thoroughly tuning parameters lead signiﬁcant improvement predictive accuracy problems critical step every application. made full experiments results available online encourage bioinformaticians easily gather information pertinent area study. parameters loss=deviance learning rate=. estimators= depth= features=log estimators= features=. criterion=entropy gamma=. kernel=poly degree= coef=. estimators= features=log criterion=entropy penalty=l intercept=true settings strong amount generality. starting point provided recommendations diﬀerent algorithms parameters based collective coverage datasets pmlb. however important note algorithms parameters work best supervised classiﬁcation problems used starting points. nuanced approach similarity dataset applied datasets pmlb could quantiﬁed algorithms performed best similar datasets could used. lieu detailed problem information could also automated tools ai-driven platforms perform model selection parameter tuning automatically. course bioinformaticians value properties algorithms aside predictive accuracy. example algorithms often used microscope model better understand complex biological systems data sampled. case bioinformaticians value interpretability model case black predictive models cannot interpreted little use. although logistic regression decision tree algorithms often outperformed tree-based ensemble algorithms terms predictive accuracy linear models shallow decision trees often provide useful trade-oﬀ predictive accuracy interpretability. furthermore methods lime show promise explaining complex black models make individual predictions also useful model interpretation. quantitative trait genetics. addition experiments take account feature preprocessing feature construction feature selection although shown learning better data representations signiﬁcantly improve performance. plan extend work analyze ability various feature preprocessing construction selection strategies improve model performance. addition experimental results contain rich information performance diﬀerent learning algorithms function datasets. future work take deeper look properties datasets inﬂuence performance speciﬁc algorithms. relating dataset properties speciﬁc areas bioinformatics able generate tailored recommendations algorithms work best speciﬁc applications. thank andreas m¨uller valuable input development project well penn medicine academic computing services computing resources. work supported grants p-es well warren center network data science university pennsylvania.", "year": "2017"}