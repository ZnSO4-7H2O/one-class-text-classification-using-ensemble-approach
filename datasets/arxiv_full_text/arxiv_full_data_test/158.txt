{"title": "Supervised Learning in Spiking Neural Networks with FORCE Training", "tag": "q-bio", "abstract": " Populations of neurons display an extraordinary diversity in the behaviors they affect and display. Machine learning techniques have recently emerged that allow us to create networks of model neurons that display behaviours of similar complexity. Here, we demonstrate the direct applicability of one such technique, the FORCE method, to spiking neural networks. We train these networks to mimic dynamical systems, classify inputs, and store discrete sequences that correspond to the notes of a song. Finally, we use FORCE training to create two biologically motivated model circuits. One is inspired by the zebra-finch and successfully reproduces songbird singing. The second network is motivated by the hippocampus and is trained to store and replay a movie scene. FORCE trained networks reproduce behaviors comparable in complexity to their inspired circuits and yield information not easily obtainable with other techniques such as behavioral responses to pharmacological manipulations and spike timing statistics. ", "text": "populations neurons display extraordinary diversity behaviors aﬀect display. machine learning techniques recently emerged allow create networks model neurons display behaviours similar complexity. here demonstrate direct applicability technique force method spiking neural networks. train networks mimic dynamical systems classify inputs store discrete sequences correspond notes song. finally force training create biologically motivated model circuits. inspired zebra-ﬁnch successfully reproduces songbird singing. second network motivated hippocampus trained store replay movie scene. force trained networks reproduce behaviors comparable complexity inspired circuits yield information easily obtainable techniques behavioral responses pharmacological manipulations spike timing statistics. human beings naturally learn perform wide variety tasks quickly eﬃciently. examples include learning complicated sequence motions order take slap-shot hockey learning replay notes song music class. approaches solving diﬀerent problems ﬁelds machine learning control theory etc. humans distinct tool solve problems spiking neural network. recently broad class techniques derived allow enforce certain behavior dynamics onto neural network top-down techniques start intended task recurrent spiking neural network perform determine connection strengths neurons achieve this. dominant approaches currently force method spike-based predictive coding networks neural engineering framework spike-based coding approaches yielded substantial insights network functioning. examples include networks organized solve behavioral problems process information robust metabolically eﬃcient ways spike-based coding approaches create functional spiking networks agnostic towards underlying network neurons recent advances). second order apply either approach task speciﬁed terms closed-form diﬀerential equations. constraint potential problems networks solve recent advances). despite constraints spike-based coding techniques resurgence top-down analysis network function fortunately force training agnostic towards network consideration tasks network solve. originating ﬁeld reservoir computing force training takes high dimensional dynamical system utilizes systems complex dynamics perform computations unlike techniques target behavior speciﬁed closed form diﬀerential equation training. required training supervisor provide error signal. high dimensional dynamical system reservoir makes force method applicable many network types error signal expands potential applications networks. unfortunately force training directly implemented networks rate equations phenomenologically represent neurons’ ﬁring rate varies smoothly time little work done spiking network implementations literature recent advances. show force training directly applied spiking networks robust diﬀerent implementations neuron models potential supervisors. demonstrate potential networks force train spiking networks biologically motivated. ﬁrst circuits corresponds zebra ﬁnch hvc-ra circuit reproduces singing behavior second circuit inspired hippocampus encodes replays movie scene. circuits perturbed determine robust circuit functioning post-training. explored potential force method training spiking neural networks perform arbitrary task. networks synaptic weight matrix static weights learned weights qηφt static weights initialize network chaotic spiking learned component weights determined online using supervised learning method called recursive least squares quantity also serves linear decoder network dynamics. parameter deﬁnes tuning preferences neurons learned feedback term. components static always randomly drawn. parameters control relative balance chaos inducing static weight matrix learned feedback term respectively. parameters discussed greater detail materials methods. goal minimize squared error network dynamics target dynamics method successful network dynamics mimic target dynamics turned considered three types spiking integrate-and-ﬁre model theta model leaky integrate-and-ﬁre izhikevich model parameters models found table networks considered constrained intermediate size post-training average ﬁring rates synaptic time constants typically values considered supplementary material. code used paper found modeldb accession number demonstrate basic principle method compare spiking network simulations applied force training network rate equations trained network learn simple sinusoidal oscillator. static weight matrix initializes highdimensional chaotic dynamics onto network rate equations dynamics form suitable reservoir allow network learn target signal quickly. original formulation force method rates heterogeneous across network varied strongly time activated short initialization period network. learning appropriate weights network reproduces oscillation without guidance teacher signal albeit slight frequency amplitude error ascertain networks learn perform target dynamics computed eigenvalues resulting weight matrix learning unlike top-down techniques force trained weight matrices always high rank demonstrate dominant eigenvalues large sought determine whether force method could train spiking neural networks given impressive capabilities technique training smooth systems rate equations previously demonstrated implemented force method diﬀerent spiking neural networks integrate-and-ﬁre neurons order compare robustness method across neuronal models potential supervisors. first demonstrate chaotic nature networks deleted single spike network deletion resulting spike trains immediately diverged indicating chaotic behavior determine onset chaos occurred function simulated networks neurons range parameters computed coeﬃcients variation interspike-interval distributions izhikevich theta neurons immediate theta neuron model transition chaotic spiking small value neuron models exhibited bimodal interspike-interval distributions indicative possible transition rate chaos suﬃciently high values finally perturbation analysis revealed three networks neurons contained ﬂux-tubes stability force train chaotic spiking networks used original work guide determine parameter regime contributions static learned synaptic inputs magnitude. similarly scale ensure appropriate balance finally force training works quickly stabilizing rates. subsequent weight changes devoted stabilizing weight matrix. fast learning applied faster time scale spiking networks versus rate networks respectively). learning rate taken fast enough stabilize spiking basis ﬁrst presentation supervisor. modiﬁcations force method successfully trained networks spiking theta neurons mimic various types oscillators. examples include sinusoids diﬀerent frequencies sawtooth oscillations oscillators addition teaching signals noise present decay time constant product sines oscillator presented challenge theta neuron learn. however could resolved network larger decay time constants oscillators figure trained successfully izhikevich neuron models furthermore force training robust possible types initial chaotic network states finally three parameter sweep parameter space reveals parameter regions convergence contiguous. parameter sweep conducted sinusoidal supervisors diﬀerent frequencies oscillators higher frequencies learned larger parameter regions networks faster synaptic decay time constants finally compared eigenvalues trained weight matrices varied spiking networks observe cases systems without dominant eigenvalues performed better systems dominant eigenvalues cases opposite true. dynamics consideration izhikevich model greatest accuracy fastest training times partially fact izhikevich neuron spike frequency adaptation operates longer time scale long time scale aﬀords reservoir greater capability memory allowing learn longer signals. additionally oscillators simple dynamical systems wanted assess force train spiking neural network perform complicated tasks. thus considered additional tasks reproducing dynamics low-dimensional chaotic system statistically classifying inputs applied network neurons. trained network theta neurons using lorenz system supervisor network could reproduce butterﬂy attractor lorenz-like trajectories learning. supervisor complex training took longer required neurons still successful. furthermore lorenz system could also trained network neurons quantify error compare rate network developed attractor density based metric marginal density functions attractor. spiking network comparable performance rate network further spiking rate networks able regenerate stereotypical lorenz tent albeit superior performance rate network finally showed populations neurons force trained statistically classify inputs similar standard feed forward network wanted determine force training could adapted generate weight matrices respect dales constraint neuron excitatory inhibitory both. unlike previous work opted enforce dales dynamically network trained error test interval supervisor comparable values supplementary fig. dales implemented summarize three diﬀerent neuron models demonstrated force method used train spiking network using supervisor. supervisor oscillatory noisy chaotic training occur manner respects dales law. neurons encode complicated temporal sequences mating songs song birds learn store replay wondered whether force could train spiking networks encode similar types naturally occurring spatiotemporal sequences. formulated long oscillations repeatedly presented network. ﬁrst pattern considered sequence pulses -dimensional supervisor. pulses correspond notes ﬁrst beethoven’s joy. network izhikevich neurons successfully force trained reproduce average ﬁring rate network training variability neuronal responses replay replay forming stable peri-simulus time histogram furthermore could learned neuron models larger synaptic decay time constants network displayed error replaying song errors random stereotypical. errors primarily located non-unique e-note repetitions occur ﬁrst third addition non-unique sequences occur second beginning fourth bar. example non-trivial train pales complexity singing zebra ﬁnch. constructed circuit could reproduce bird song recorded adult zebra ﬁnch. learned singing behavior birds owed primary nuclei robust nucleus arcopallium projecting neurons form chain spiking activity projecting neuron ﬁres speciﬁc time song chain ﬁring transmitted activates circuit. neuron bursts multiple precisely deﬁned times song replay neurons project lower motor nuclei stimulate vocal muscles generate bird song. simplify matters focus single network neurons receiving chain inputs area chain inputs modeled directly series successive pulses force trained simplicity. pulses feed network izhikevich neurons successfully force trained reproduce spectrogram recorded song adult zebra ﬁnch additionally varying parameters izhikevich model parameters spiking statistics neurons easily reproduced qualitatively quantitatively figure neurons burst regularly multiple times song replay thus trained model network match song generation behavior spiking behavior consistent experimental data. wondered network would respond potential manipulations underlying weight matrices. particular considered manipulations excitatory synapses alter balance excitation inhibition network found network robust towards downscaling excitatory synaptic weights. network could still produce song albeit lower intensity. possible even excitatory synapses reduced amplitude however upscaling excitatory synapses little drastically reduced song performance. resulting song output diﬀerent spectral structure supervisor opposed downscaling excitation. finally upscaling excitatory weights suﬃcient destroy singing replacing high intensity seizure like activity. interestingly similar result found experimentally injection bicuculine large doses bicuculine resulted strong bursting activity accompanied involuntary vocalizations. smaller doses resulted song degradation increased noisiness duration appearance syllables. surprised robust performance songbird network given high dimensionality complicated structure output. hypothesized performance network strongly associated precise clock-like inputs provided similar inputs could encoding replay types information. test hypothesis removed input pattern found replay learned song destroyed similar experimental lesioning area adult canaries temporal information high-dimensional signals provide subsequently refer high-dimensional temporal signals explore beneﬁts hdts might provide force trained network izhikevich neurons internally generate hdts simultaneously also training reproduce ﬁrst joy. entire supervisor consisted notes used ﬁrst song addition components. correspond sequence pulses partition time. network learned hdts song simultaneously less training time greater accuracy without hdts. hdts helped learning replaying ﬁrst song wondered signals could help encoding longer sequences. test hypothesis force trained network learn ﬁrst four bars corresponding second song addition -dimensional hdts. network successfully learned song post-training without error sequence notes spite length complexity sequence thus internally generated hdts make force training faster accurate robust learning longer signals. refer networks internally generated hdts. given improvements hdts confers supervisor duration training time accuracy wanted know input signals would help populations neurons learn natural high dimensional signals. test this trained network izhikevich neurons learn dimensional supervisor corresponds pixels second scene movie component supervisor corresponds time evolution single pixel. hdts’s either generated separate network directly input encoding replay network songbird example refer cases internally generated externally generated hdts respectively. former case demonstrate hdts easily learned network latter case freely manipulate hdts. long example hdts could also learned simultaneously movie scene constituting dimensional supervisor note thought spontaneous replay opposed cued recall network reconstructs partially presented stimulus. able successfully train network replay movie scene cases furthermore found hdts inputs necessary training replay network could still replay individual frames movie scene without hdts however order scenes incorrect appeared chaotic thus hdts input facilitates learning spontaneous replay high dimensional signals. surprised despite complexity high dimensionality encoding signal histogram spike times across replay network displayed strong modulation conferred hdts. histogram interpreted mean network activity. unsurprisingly reducing hdts amplitude yields steady decrease replay performance. initially mirrored decrease amplitude oscillations mean population activity surprisingly however emergence slower oscillations corresponds sharp decline replay performance scenes longer replayed chronological order spikes also non-uniformly distributed respect mean-activity wanted determine removing neurons replay network would aﬀect mean population activity replay performance. found replay performance decreased approximately linearly proportion neurons lesioned amplitude mean activity also decreasing hdts network however much sensitive neural lesioning. found lesioning random selection neurons hdts network suﬃcient stop hdts output. thus hdts network critical element circuit drive accurate replay sensitive damaging perturbations neural loss. finally wondered frequency amplitude hdts would alter learning accuracy network optimal input frequency located range large regions parameter space. robust diﬀerent neuronal parameters speculated compressed reversed replay event might important memory consolidation thus wondered networks trained hdts could replay scene accelerated time compressing hdts post-training. training compressed external hdts time network able replay dropping compression. eﬀect time compression mean activity introduce higher frequency oscillations frequency oscillations scaled linearly amount compression. however increasing compression frequency large waves synchronized activity also emerged mean population activity reverse replay also successfully achieved reversing order hdts components presented network loss accuracy fact temporal dynamics network reversed within segment hdts. thus network generalize robustly compress reverse replay despite trained tasks. compression task dependent sequence ﬁring experimentally found numerous sources example authors found recorded sequence neuronal cross correlations rats elicited spatial sequence task reappeared compressed time sleep. compression ratios compressed found networks without incurring appreciable error replay time compression dilation also experimentally found striatum here authors found neuronal ﬁring sequences striatal neurons directly compressed time indeed also found accelerated replay movie scene compressed spiking behavior neurons replay network summarize hdts necessary encoding replay high dimensional natural stimuli. movie clips thought proxies episodic memories. compression reversal hdts allows compressed reversed replay memory proxy. lower compression ratios mean population shown force training take initially chaotic networks spiking neurons mimic natural tasks functions demonstrated populations neurons. example networks trained learn low-dimensional dynamical systems oscillators heart generating rhythmic rhythmic motion found force training robust spiking model employed initial network states synaptic connection types. additionally showed could train spiking networks display behaviors beyond dimensional dynamics altering supervisor used train network. example trained statistical classiﬁer network izhikevich neurons could discriminate inputs. extending notion oscillator even allowed store complicated sequence form notes song reproduce singing behavior songbirds encode replay movie scene. tasks aided inclusion high-dimensional temporal signal discretizes time segregating neurons assemblies. force training reminiscent songbirds learn stereotypical learned songs juvenile songbirds typically presented species speciﬁc song repertoire songs parents members species. birds internalize original template song subsequently error signal vocalization. model reproduced singing behavior songbirds force training error correction mechanism. spiking statistics area song spectrogram accurately reproduced force training. furthermore demonstrated altering balance excitation inhibition post training degrades singing behavior post-training. shift excess excitation alters spectrogram highly non-linear shift excess inhibition reduces amplitude frequencies. inspired clock-like input pattern song birds learning replay used similar high-dimensional temporal signal encode longer complex sequence notes addition scene movie. found signals made force training faster subsequent replay accurate. furthermore manipulating hdts frequency found could speed reverse movie replay robust fashion. found compressing replay resulted higher frequency oscillations mean population activity. attenuating hdts decreased replay performance transitioning mean activity oscillation slower episodic memory network associated particular hippocampal region tempting conjecture results might interpreted within context hippocampal literature. particular found hdts conferred slow oscillation mean population activity reminiscent slow theta oscillations observed hippocampus. theta oscillation strongly associated memory however computational role fully understood many theories proposed example theta oscillation proposed serve clock memory formation here show concrete example natural stimuli serve proxies memories bound underlying oscillation population neurons. oscillation forces neurons discrete temporal assemblies. oscillation sped even reversed resulting identical manipulation memory. additionally found reducing hdts input severely disrupted replay underlying mean population oscillation. mirrors experimental results showed theta power predictive correct replay furthermore blocking hdts prevents learning prevents accurate replay networks trained hdts present. blocking hippocampal theta oscillation pharmacologically optogenetically also found disrupt learning. role hdts reminiscent recent discovery time cells also serve partition across time interval episodic memory tasks time cells formed ongoing research however dependent medial septum thus hippocampal theta oscillation time cells found temporally selective cells occur entorhinal cortex broader context force trained networks could used future elucidate hippocampal functions. example future force trained networks make biological constraints dale’s eﬀort reproduce veriﬁed spike distributions diﬀerent neuron types regards phase force training powerful tool allows suﬃciently complicated dynamical system basis universal computation. primary diﬃculty implementing technique spiking networks appears controlling orders magnitude chaos inducing weight matrix feedback weight matrix. chaotic weight matrix large magnitude chaos longer controlled feedback weight matrix however chaos inducing matrix weak chaotic system longer functions suitable reservoir. resolve this derived scaling argument scale successful training based network behaviors observed interestingly balance ﬂuctuations could related fading memory property necessary criterion convergence force trained rate networks furthermore succeeded implementing technique neuron types izhikevich model accurate terms learning arbitrary tasks dynamics. presence spike frequency adaptation variables operate much slower time scale neuronal equations. biologically relevant forces increase capacity network reservoir longer time scale dynamics synaptic depression nmda mediated currents example furthermore found inclusion high-dimensional temporal signal increased accuracy capability spiking network reproduce long signals. another type highdimensional supervisor used train initially chaotic spiking networks. here authors supervisor consisting components details). diﬀerent approach involving construction hdts serves partition neurons assemblies lower dimensionality however work here increasing dimensionality supervisor force training accuracy capability. finally possible hdts would facilitate faster accurate learning networks rate equations general reservoir methods well. although force trained networks dynamics starting resemble populations neurons present top-down procedures used construct functional spiking neural network need work become biologically plausible learning rules example force trained networks require non-local information form correlation matrix however dismiss ﬁnal weight matrices generated techniques biologically implausible simply techniques biologically implausible. aside original rate formulation force trained rate equations recently applied analyzing reproducing experimental data. example authors used variant force training train rate network reproduce temporal sequence activity mouse calcium imaging data. pinning uses minimal changes balanced weight matrix architecture form neuronal sequences. authors combine experimental manipulations force trained networks demonstrate preparatory activity prior motor behavior resistant unilateral perturbations experimentally force trained rate models. authors demonstrate dynamics reservoirs explain emergence mixed selectivity primate dorsal anterior cingulate cortex authors modiﬁed version force training implement exploration/exploitation task also experimentally performed primates. authors found force trained neurons similar dynamic form mixed selective experimentally recorded neurons dacc. finally authors train network rate neurons encode time scale seconds. network subsequently used learn diﬀerent spatio-temporal tasks cursive writing task. force trained networks able account psychophysical results weber’s variance response scales like square time since start response. cases force trained rate networks able account predict experimental ﬁndings. thus force trained spiking networks prove invaluable generating novel predictions using voltage traces spike times neuronal parameters. top-down network training techniques diﬀerent strengths uses. example neural engineering framework spike-based coding approaches solve underlying weight matrices immediately without training solutions analytical spike based coding approach numerical approach. furthermore weight matrix solutions valid entire regions phase space force training uses individual trajectories supervisors. multiple trajectories force trained single network yield comparable level global performance region. sets solutions yield diﬀerent insights structure dynamics functions spiking neural networks. example brain scale functional models constructed networks spike-based coding networks demonstrate higher order error scaling possible utilizing spiking sparsely eﬃciently balanced network solutions. spike based coding approaches provide immediate weight matrix solutions techniques diﬃcult generalize types networks types tasks. spike based coding approach require system closed form diﬀerential equations determine static weight matrix yields target dynamics. summary showed force used train spiking neural networks reproduce complex spatio-temporal dynamics. method could used future mechanically link neural activity complex behaviors animals. work funded canadian national sciences engineering research council postdoctoral fellowship wellcome trust leverhulme trust bbsrc would like thank frances skinner chris eliasmith larry abbott raoul-martin memmesheimer brian depasquale dean buonomano comments. finally would like especially thank anonymous referees. comments suggestions greatly improved manuscript. preference phase space target dynamics. quantities constants scale static weight matrix feedback term respectively. ﬁring rates given correspond type-i normal form ﬁring. variable scales ﬁring rates. decoders determined recursive least mean squares technique iteratively described greater detail next section. supplementary fig. supplementary fig. also implemented tanh continuous variable equations comparison purposes. equations described greater detail currents given ibias ibias constant background current near rheobase value. currents dimensionless case theta neuron dimensional izhikevich models. note absorbed unit resistance current model. quantities voltage variables adaptation current izhikevich model. voltage variables instantly reset potential neurons membrane potential reaches threshold voltage peak parameters neuron models found table parameters izhikevich model slight modiﬁcation parameter. model refractory time period τref neuron cannot spike. adaptation current izhikevich model increases discrete amount every time neuron ﬁres spike serves slow ﬁring rate. membrane time constant model given variables izhikevich model include synaptic rise time synaptic decay time. ﬁlters simple exponential type double exponential alpha synapse type however primarily consider double exponential ﬁlter rise time decay time longer shorter ﬁlters considered supplementary materials. weight matrices explicitly theta neuron models counterbalance resulting ﬁring rate heterogeneity. necessary izhikevich model. variable controls chaotic behavior value varies neuron model neuron model. encoding variables drawn randomly uniformly dimensionality teaching signal. encoders contribute tuning preferences neurons network. variable increased better allow desired recurrent dynamics tame chaos present reservoir. weights determined dynamically time minimizing squared error approximant intended denominator. term adds slight increase accuracy stability change overall order convergence rls. comparison purposes modiﬁcation applied supplementary fig. implementations used equations learned recurrent component static feedback term order magnitude furthermore input stimulus amplitude small chaotic dynamics could tamed. operating hypothesis static term learned term require ﬂuctuations order magnitude derive following equations using standard arguments balanced networks high-dimensional temporal signals serves stabilize network dynamics organizing neurons assemblies. assemblies precise intervals time. elaborate further signals constructed discretization time sub-intervals. subinterval generates extra component supervisor within subinterval pulse upward deﬂection supervisor genend eﬀect signals divide network series assemblies. neurons participation assembly function static weight matrix addition ηin. assembly activated preceding assemblies propagate long signal network. assemblies activated speciﬁc frequency network dictated width pulses hdts. sinusoidal hdts used ﬁgures exception supplementary fig. gaussian hdts used. network rate equations used learn dynamics teaching signal sinusoidal oscillation. network consists rate neurons network second initially next seconds shut second mark. network subsequently another seconds ensure learning occurs. smooth ﬁring rates static matrix connectivity. induces chaotic spiking network. allow network settle onto chaotic attractor period seconds. initial period activated turn force training duration seconds. period time deactivated remainder seconds simulation time teaching signals except signals product sinusoids. signals required seconds training time tested longer duration post training determine theta neuron parameters figure sinusoidal oscillator. network consisted neurons connectivity static weight matrix integration time step average ﬁring rate izhikevich network consisted neurons connectivity static weight matrix average ﬁring rate parameter used taken small fraction integration time step figure izhikevich model integration time step training time networks considered teaching signal constructed using positive component sinusoid frequency quarter notes half notes. pulse corresponds presence note also used amplitude/envelope harmonics corresponding note generate audio-signal network network consisted izhikevich neurons teaching signal continually network seconds corresponding repetitions signal. training network simulated period seconds. interval correct replay song corresponded duration distinct correct replays song within signal. correct replays automatically classiﬁed constructing moving average error function seconds length song. vary teaching signal comes alignment correct playbacks network output ˆx). reduces error creating local minima correspond correct replays times automatically classiﬁed correct critical value. average ﬁring rate network used training. teaching signal consisted spectrogram second long recording singing behavior male zebra ﬁnch generated window. data obtained crcns data repository network consisted neurons force trained network tested another turned test learning successful. note songbird example network could also learn output static chaos inducing weight matrix feedforward inputs however network behavior similar classic liquid state regime. teaching signal consisted second movie clip movie predator. frames smoothed interpolated could applied time point instead ﬁxed times corresponding simulated another turned test learning successful. internal hdts case separate hdts network trained neurons. supervisor network consisted -dimensional hdts pulses duration. hdts identical replay network external hdts case. replay network trained simultaneously hdts network external hdts case. applied train hdts replay network identical parameters external hdts case longer training time note movie example network could also learn output static chaos inducing weight matrix feedforward hdts inputs however network behavior similar classic liquid state regime. force method spiking neural network contains backbone static strong synaptic weights scale like induce network level chaos secondary weights added weight matrix decoders determined time optimization procedure recursive least squares technique used subsequent simulations. force training requires supervisor estimate prior learning ﬁring rates random neurons network rate equations chaotic regime. chaos controlled converted steady state oscillations. allows network represent sinusoidal input learning network still displays sinusoidal oscillation macroscopic dynamics training successful. total training time seconds. decoders randomly selected neurons network voltage trace randomly selected neurons networks integrate-and-ﬁre spiking neurons. models consideration theta neuron leaky integrate-and-ﬁre neuron izhikevich model spike frequency adaptation networks consideration spike deleted neurons. caused spike train diverge post-deletion clear indication chaotic behavior. network theta neurons initialized chaotic regime trained mimic diﬀerent oscillators force training. oscillators included sinusoid harmonic relaxation regimes non-smooth sawtooth oscillator oscillator formed taking product pair sinusoids frequencies product sinusoids gaussian additive white noise distortion standard deviation three networks diﬀerent integrate-and-ﬁre neurons initialized chaotic regime trained using force method mimic sinusoidal oscillator network theta neurons trained force method mimic lorenz system. used learn decoders using second long trajectory lorenz system supervisor. turned seconds resulting trajectory chaotic attractor bearing strong resemblance lorenz system. network attractor notes song beethoven converted continuous -component teaching signal. presence note designated upward pulse signal. quarter notes positive portion sine wave form half notes represented positive part sine wave. component teaching signal corresponds musical notes cdef teaching signal presented network izhikevich neurons continuously start ﬁnish network learns reproduce sequence force training. network output seconds force training applications target signal. seconds simulation time training song played correctly times comprising seconds signal randomly selected neurons network resulting dynamics force training. voltage traces taken time approximant decoders force training. resulting eigenvalues weight matrix qηiφj learning. note onset chaos occurs izhikevich network series pulses constructed positive portion sinusoidal oscillator period used model chain ﬁring output projection neurons hvc. neurons connect neurons trigger singing behavior. force training used train network izhikevich neurons reproduce spectrogram second audio recording adult zebra ﬁnch. syllables song output teaching signal network spike raster plot neurons repetitions song. spike raster plot aligned syllables distribution instantaneous ﬁring rates network post training. parameter corresponds up/down regulation factor excitatory synapses diﬀerent colours graphs excitation dominates inhibition reverse true inset reproduced note y-axis model data diﬀer normalization density functions. interspike interval histogram. correlation network output teaching signal excitatory synapses upscaled downscaled spectrogram teaching signal network output diﬀerent scaling factors excitatory synaptic weights. panels correspond plots performance measure types networks trained replay second clip movie. internally generated hdts case hdts replay network simultaneously trained. hdts projects onto replay network similar neurons project onto neurons birdsong circuit. hdts confers oscillation mean population activity. externally generated hdts case network receives hdts supervisor trained simple manipulate confers oscillation mean population activity. teaching signal displayed addition network output separate times correspond distinct scenes movie. hdts consists pulses generated positive component sinusoidal oscillator period colour pulses denotes order temporal chain equivalently dimension hdts. external hdts compressed time results speeding replay movie clip. time-averaged correlation coeﬃcient teaching signal network output used measure performance. hdts amplitude network internal hdts reduced. network robust decreasing amplitude hdts. neurons replay network removed replay performance measured. replay performance decreases approximately linear fashion proportion replay network removed. mean population activity replay networks hdts compression removal replay network lesioning.", "year": "2016"}