{"title": "Deep Active Inference", "tag": "q-bio", "abstract": " This work combines the free energy principle from cognitive neuroscience and the ensuing active inference dynamics with recent advances in variational inference in deep generative models, and evolution strategies to introduce the \"deep active inference\" agent. This agent minimises a variational free energy bound on the average surprise of its sensations, which is motivated by a homeostatic argument. It does so by optimising the parameters of a generative latent variable model of its sensory inputs, together with a variational density approximating the posterior distribution over the latent variables, given its observations, and by acting on its environment to actively sample input that is likely under this generative model. The internal dynamics of the agent are implemented using deep and recurrent neural networks, as used in machine learning, making the deep active inference agent a scalable and very flexible class of active inference agent. Using the mountain car problem, we show how goal directed behaviour can be implemented by defining appropriate priors on the latent states in the agent's model. Furthermore, we show that the deep active inference agent can learn a generative model of the environment, which can be sampled from to understand the agent's beliefs about the environment and its interaction therewith. ", "text": "abstract. work combines free energy principle cognitive neuroscience ensuing active inference dynamics recent advances variational inference deep generative models evolution strategies eﬃcient large-scale black-box optimisation technique introduce deep active inference agent. agent tries minimize variational free energy bound average surprise sensations motivated homeostatic argument. changing parameters generative model together variational density approximating posterior distribution latent variables given observations acting environment actively sample input likely generative model. internal dynamics agent implemented using deep neural networks used machine learning recurrent dynamics making deep active inference agent scalable ﬂexible class active inference agents. using mountaincar problem show goal-directed behaviour implemented deﬁning sensible prior expectations latent states agent’s model fulﬁl. furthermore show deep active inference agent learn generative model environment sampled understand agent’s beliefs environment interaction active inference normative theory brain function derived properties required active agents survive dynamic ﬂuctuating environments. theory able account many aspects action perception well anatomic physiologic features brain hand encompasses many formal theories brain function terms functional form rests minimisation upper variational bound agents average surprise. formally similar state algorithms variational inference deep generative models however optimising bound active agents introduces dependency true dynamics world agent usually access whose true pre-print article published biological cybernetics. ﬁnal authenticated version available online https//doi.org/./s--- author archived personal copy accepted manuscript personal homepage https//kaiu.me. functional form coincide functional form agent’s generative model. solve problems using deep neural networks recurrent neural networks ﬂexible function approximators allow agent’s generative model learn good approximation true world dynamics. futhermore apply evolution strategies estimate gradients variational bound averaged population agents. formalism allows obtain gradient estimates even non-diﬀerentiable objective functions case here; since agent neither know equations motions world partial derivatives. approach pairs active inference state machine learning techniques create agents successfully reach goals complex environments simultaneously building generative model surroundings. work want basic form called deep active inference agents illustrate dynamics using simple well known problem reinforcement learning namely mountain problem utilising models optimisation techniques applied successfully real-world data large-scale problems agent scaled complex rich environments. paper publish full implementation resulting deep active inference agent together scripts used create ﬁgures publication https//www.github.com/ kaiu/deepai_paper. section brieﬂy recapitulate active inference principle. section describe mountain environment introduce deep active inference agent able solve problem. section show agent solve mountain problem simultaneously learning generative model environment. section discuss possible directions approach relation approaches. active inference rests basic assumption agent exchange dynamic ﬂuctuating environment keep certain inner parameters within well deﬁned range. otherwise would sooner later encounter phase transition would loose deﬁning characteristics therefore disappear. thus agent must restrict small volume state space. agents whose sensory organs good mapping relevant physical states appropriate sensory inputs would last long. mapping agents true states sensations assumed almost constant sensitivity makes last term approximately constant allows upper bounding entropy agent’s distribution state space entropy sensory states plus constant term thus ensure keep physiological variables within well-deﬁned bounds agent minimize sensory entropy able eﬃciently this agent needs statistical model sensory inputs evaluate since world live hierarchically organised dynamic features complex noise assume agent’s model deep recurrent latent variable model furthermore assume model generative using observation able imagine certain situations perceptions without actually experiencing experienced them. thus work generative model sensory observations latent variables represent hidden true states marginalising possible states could lead given observation. dimensionality latent state space high integral extremely hard solve. therefore assumption free energy principle agents arbitrary called variational density space hidden states belongs family distributions parameterised time-dependent i.e. fast changing parameter corresponding means standard-deviations. parameter encoded internal states agent e.g. neural activity brain. thus upper bound depends quantities agent direct access. namely states sensory organs synapses encoding generative model world neural activity representing suﬃcient statistics variational density. using deﬁnition kullback-leibler divergence linearity integral bayes’ rule manipulation logarithms derive following equivalent forms free energy functional passive observer world around thing could minimize would change parameters generative model suﬃcient statistics inner representation. looking ﬁrst form free energy optimizing would correspond minimizing kullback-leibler divergence variational distribution true tions thus variational density seen approximation true posterior density. therefore minimising agent automatically acquires probabilistic representation approximate posterior hidden states world given sensory input. optimization suﬃcient statistics variational density therefore call perception. optimized fast timescale quickly changing sensory input likely represented terms neural activity. might explain hallmarks bayesian computations probabilistic representations sensory stimuli brain variational free energy upper bounds surprise minimising free energy respect parameters generative model simultaneously maximise evidence agent’s generative model world. resulting optimisation parameters generative model call perceptual learning might implemented changing synaptic connections neurons brain. second form given demonstrate name free energy originated from since form similar helmholtz free energy canonical ensemble statistical physics. core idea active inference equip agent actuators allow actively change state environment. thereby sensory observations become function current past states agents actuators inﬂuence state world generating sensory inputs. agent minimize free energy bound learning perception also changing observations makes. posterior state world. thus agent seek states conform expectations i.e. high likelihood generative model world. encode speciﬁc goals priors generative model agent assigning higher priori probability speciﬁc state agent seek state often. ﬁrst term agent maximise using actions also called accuracy bayesian statistics second term known complexity. complexity measures degree posteriors adjusted relation priors provide accurate account sensory data outcomes. triple optimisation problem formalised using following dynamics parameters agent’s generative model internal states encoding suﬃcient statistics variational density states actuators paper consider variational density beliefs hidden states world. general formulations density would cover hidden states parameters generative model. usually beliefs factorised mean ﬁeld approximation true posterior estimates states parameters section introduce agent uses recent advances stochastic optimisation deep generative models evolution strategies eﬃcient scalable optimisation algorithm nondiﬀerentiable objective functions implement active inference. show ability reach speciﬁc goal concurrently building generative model environment using well-known mountain problem ﬁrst start basics well known deep learning community similar diﬀerently used nomenclature might lead confusion among computational neuroscientists. deep neural networks. talk deep neural networks term sense deep learning literature sense neural networks nothing ﬂexible function approximators. excellent comprehensive introduction state deep learning given goodfellow neurons i.e. dimensionality input space represents number output neurons i.e. dimensionality output space. subscript represents parameters function tuned approximate optimise given objective function. elementwise nonlinear transfer function. could example parameters consists matrix called weight matrix weights bias vector functional form loosely inspired weighted ﬁring rates presynaptic neurons) ﬁring rate modelled population nonlinear thresholded activation function represented using bias parameters nonlinear transfer function output function called output layer input called input layer intermediate values called hidden layers since explicitly constrained target objective function. ﬁnite dimensions intermediate outputs regarded number hidden neurons layer. contrast previous approaches active inference deep feedforward networks allow specify agents whose generative models environment need model family true generative process. ﬂexibility deep feedforward networks. shown feedforward network linear output layer least hidden layer nonlinear activation function tanh sigmoid activation function approximate borel measurable function ﬁnite-dimensional space another arbitrary accuracy given suﬃcient dimensionality hidden layer although wide variety complex powerful architectures goodfellow simple prototypical recurrent neural network recently shown able learn long-range temporal dependencies given sensible initialization using deterministic type recurrent neural network directly basic idea feeding back samples distribution current network state inputs calculate next network state. similar approximation theorem deep feedforward networks shown recurrent neural networks able learn arbitrary algorithms turing complete environment. agent discrete time version mountain world start bottom potential landscape given current position velocity current state eﬀector organs. case humans would state muscles state describes car’s steering direction throttle car’s engine. dynamics environment given update equations summarised symbolically motor limited interval agent strong enough climb slope results downhill force without additional momentum. thus overcome slope agent move uphill left away target position ﬁrst. acquire required additional momentum allows overcome opposite slope. mountaincar environment although simple completely trivial agent learn direct approach target position succeed needs acquire strategy initially leads away target. deep active inference agent. describe agent follows active inference principle laid section adapted mountain environment sensory inputs motor outputs inner workings implemented optimised using recent advances deep learning denotes gaussian probability density mean standarddeviation show agent indeed learn complex nonlinear generative model sensory input another sensory channel nonlinear non-bijective transformation note direct sense action state necessary active inference agent successfully control environment e.g. reﬂex arcs increase likelihood sensory inputs given generative model world feature closed loop neural dynamics level spinal cord. direct access action states muscles lift expecting lift. however adding channel allow later directly sample agents proprioceptive sensations generative model check understands action environment. factorisation means next state agents generative model depends current state inner model world. thus transition distribution encodes agent’s model dynamics world i.e. concrete implementation model distribution diagonal gaussian means standard deviations calculated current state using neural networks. i.e. fully connected single layer network tanh nonlinearity calculate another fully connected single layer network softplus means nonlinear transfer function calculate standard deviations encompass parameters generative model going optimise. practice means weights bias parameters neural networks calculate means standard-deviations. diagonal gaussians prior leads hidden abstract representations within independent causes observations well separated. calculate suﬃcient statistics gaussian distributions state using deep feedforward network three hidden layers neurons each using tanh nonlinear transfer function. linear output layer calculate means gaussian variables second output layer softplus nonlinear transfer function calculate standard deviations. although structure ﬁrst glance diﬀerent hierarchical dynamical models developed friston deep nonlinear structure feedforward network also allows structured noise enter diﬀerent levels hierarchy. agent acquired generative model sensory inputs optimising parameters sample model propagating prior state space using learned likelihood function generate samples described variational density. following kingma welling rezende explicitly represent suﬃcient statistics variational posterior every time step. would require additional costly optimisation variational parameters individual time step. instead inference network approximates dependency variational posterior previous state current sensory input parameterised time-invariant parameters. allows learn parameters together parameters generative model action function also allows fast inference later following factorisation approximation variational density states given suﬃcient statistics calculated using deep feedforward network hidden layers neurons each using tanh nonlinearities. means calculated using linear standard deviations using softplus output layer. diagonal gaussians prior leads hidden abstract representations within independent causes observations optimally separated i.e. favourable properties choice practical considerations. fact choose variational density prior density diagonal gaussians later allow closed formula calculate kullback-leibler-divergence densities. however ﬂexible posterior required normalizing ﬂows allow series nonlinear transformations diagonal gaussian used variational density approximate complex multimodal posterior distributions device learning mapping inputs suﬃcient statistics posterior hidden states known amortisation. great advantage extremely eﬃcient fast. hand assumes mapping approximated static non-linear function learned neural network. somewhat limits context sensitivity active inference schemes depending upon parameterisation mapping. short amortisation enables convert deep inference deep deconvolution problem deep learning problem ﬁnding static non-linear mapping inputs posterior beliefs states generating inputs. action states. general action state agent could minimised directly timestep thereby minimizing free energy driving sensory input towards agents expectations terms likelihood function. however would require costly optimisation timestep thus rationale kingma welling rezende used variational density approximate complex dependence action state agent’s current estimate world ﬁxed ﬂexible functions i.e. deep neural networks. yields explicit functional dependency whose suﬃcient statistics calculated using deep feedforward network fully connected hidden layers neurons linear output layer mean softplus output layer standard deviation. optimise time-invariant parameters neural networks together parameters generative model variational density. approximation makes learning propagating agent fast allowing eﬃcient simultaneous learning generative model variational density action function. approximation both suﬃcient statistics variational density action states deep neural networks reason call class agents deep active inference agents. goal directed behavior. propagate optimise agent look stable equilibrium environment settle there. however practically applicable real-life problems instil concrete goals agent. achieve deﬁning states expect action states fulﬁl expectations. concrete case want propagate agent timesteps want least last timesteps. agent’s priors hidden states introduce hard-wired state represents agents current position. explicitly encoding agent’s sense position ﬁrst dimension state vector seen homeostatic i.e. vitally important state parameter. e.g. concentration blood extremely important tightly controlled opposed possible brightness perceived individual receptors retina vary orders magnitude. though might directly change behavior depending visual stimuli slight increase concentration blood concurring decrease trigger chemoreceptors carotid aortic bodies turn increase activity respiratory centers medulla oblongata pons leading fast strong increase ventilation might accompanied subjective feeling dyspnoea respiratory distress. hard-wired connection vitally important body parameters direct changes perception action might similar approach encode goal-relevant states explicitly. besides explicitly encoding relevant environmental parameters hidden states agent’s latent model also specify corresponding prior expectations figure graphical representation causal dependencies. remember solid lines correspond factors agent’s generative model dashed lines correspond variational density dotted lines correspond environmental dynamics true generative densities wiggly line describes dependency action hidden states kind hard-coded inference dynamics expectations might ﬁxed individual agents class within lifetimes mappings optimised longer timescale populations agents evolution. fact evolution might seen similar learning process diﬀerent spatial temporal scales evaluate expression simulate several thousand processes parallel allows approximate variational density single sample process timestep analogous stochastic gradient descent variational autoencoder sample data point enough since gradients depend entire batches datapoints sampling occurs following algorithm closed form kl-divergence diagonal gaussians minimisation free energy respect parameters improve agents generative model lower bounding evidence observations given generative model. simultaneously make variational density better approximation true posterior seen following optimization using evolution strategies. without action model objective would similar objective functions kingma welling rezende chung could gradient descent algorithm sampling based approximation free energy cost. concrete samples values variable marked using e.g. ˆst. sampling-based approximation expected likelihood observations variational density i.e. accuracy term. kl-divergence variational density prior i.e. complexity term. sample action propagate environment using draw single observations ˆoxt observation ˆoht draw single sample calculate calculate dkl||pθ) increment free energy carry next timestep. estimated free energy respect parameters using gradient-based optimisation algorithm adam however here would backpropagate partial derivatives free energy respect parameters dynamics world. i.e. agent would know equations motions environment least partial derivatives. obviously case many environments even diﬀerentiable resort another approach. introduce distribution space parameters called population density. optimise suﬃcient statistics minimize expected free energy distribution. population density seen population individual agents whose average ﬁtness optimised hence name. expected free energy population function suﬃcient statistics population density using gradient estimates optimise expected free energy bound population density using adam gradient based optimiser corresponding pseudocode shown algorithm constrained sampling learned generative model. agent optimised propagate environment draw unconstrained samples generative model world also markov-chain monte carlo algorithm draw constrained samples generative model hand used impute missing inputs. e.g. sampling agents estimate homeostatic sensory channel given proprioceptive spatial sensory channels. using multiple samples even sensible bounds uncertainty estimates approximating full distribution. required algorithm developed appendix rezende described algorithm basic idea de-noising properties autoencoders learned abstract robust representation ability generate lowdimensional representations capturing regularity systematic dependencies within observational data. thus workings algorithm understood follows first given sensory channels randomly initialised. partly random sensory observations encoded using variational distribution resulting state tries represent observation within low-dimensional robust representation learned agent thereby able remove noise randomly initialised channels line classic idea autoencoder variational distribution sample drawn used generate sensory samples already less noisy. known observations reset respective values denoised observations encoded using variational density iteratively encoding denoised samples algorithm markov-chain monte carlo algorithm constrained sampling agent’s generative model. concrete samples values variable marked using e.g. ˆst. together given sensory inputs iterative samples abstract robust representation converge probable cause actually observed sensations generative model. variational density generative model capture regularities dependencies within observations observations generated representation converge distribution unknown observations given observed channels. rezende provided proof true given unobserved channels initialised away actual values. experimental parameters. experiments performed desktop equipped nvidia titan gpu. parameter values used optimisation algorithm required estimation free energy bound using algorithm shown table note approximate free energy single process sample population density. possible draw many samples population density. using processes sample keeping total number ntot npopnf simulated processes constant results worse convergence behavior coverage parameter space reduced total variability stochastic approximation total bound stays approximately constant. full code implementation scripts reproduce ﬁgures paper downloaded here https//www.github. evolution strategies based optimisation procedure used less memory took less iteration. figure shows convergence free energy bound within training steps quickly converges random starting parameters plateau tries directly climb hill gets stuck steep slope. however updates population density discovers higher ﬁrst moving opposite direction thereby gaining momentum overcome steep parts slope insight leads sudden rapid decline free energy rapid development agent’s strategy quick adoption insight initial movement away target position beneﬁcial illustrated ﬁgure agent’s trajectory training steps shown ﬁgure takes short left swing gain required momentum overcome steep slope directly swings target position stays applying right force counteract non-zero downhill force target position agent environment ﬁgure generated sampling agents generative model environment without interaction environment. agent learn timecourse proprioceptive sensory channel sense position also setting irrelevant channel nonlinear transformation position. note panel ﬁgure shows processes sampled generative model described algorithm note approximating density single sample timestep process. thus although estimates seem noisy consistent actual behavior agent variability easily reduced averaging several processes. learned generative model environment propagate freely also test beliefs agent given priori assumptions timecourse certain states sensory channels using algorithm described section sampled agent’s prior beliefs trajectory given proprioceptive inputs i.e. using example took average timecourse proprioceptive channel true interaction environment shifted back timesteps. results shown ﬁgure first sampled processes converge. might markov-chain-monte-carlo-sampling approach chain initialised close enough solution guarantee convergence. however processes results look similar true propagation unconstrained samples generative model shifted back timesteps. scalable flexible active inference implementation. paper shown free energy objective proposed active inference framework cognitive neuroscience allows agent solution mountain environment concurrently building generative model figure agent’s performance training steps using mean parameters population density. realised moving uphill left reach higher position directly going upwards shown agent’s proprioception sense position environment. implementing internal dynamics agent using deep neural networks recurrent dynamics able approximate true generative process environment generative model. furthermore using eﬃcient black-box optimiser non-diﬀerentiable objective functions agent require direct access information regarding true generative process derivates given environment. implementation optimisation agent uses methods applied complex large scale problems machine learning artiﬁcial intelligence hope class agent figure agent’s performance training steps using mean parameters population density. tightly sticks strategy shown terms proprioceptive sensory channel resulting trajectory ﬁrst leads uphill left away target position gain momentum overcome steep slope nonlinearly modiﬁed sensory channel shown lower left homeostatic hidden state lower right. figure processes sampled agent’s generative model world training steps using mean parameters population density. shown prior expectations proprioceptive channel agent’s sense position nonlinear transformation position agent’s prior expectation homeostatic state variable note distribution approximated single sample timestep process. figure processes sampled agent’s generative model world training steps constrained given trajectory proprioceptive input using mean parameters population density. shown constrained expectations proprioceptive channel agent’s sense position nonlinear transformation position agent’s constrained expectation homeostatic state variable note distribution approximated single sample timestep process. demonstrate active inference able solve complex realistic problems atari robotic environments openai atari environments require agent learn play atari games pixel input robotic environments mujoco physics engine accurately simulate robotic control problems. comparison original implementation. contrast original implementation friston implementation formulated discrete timesteps without explicit representation generalised coordinates. i.e. agent learn observations position related successive timesteps form representation velocity. furthermore agent’s generative model world contrast former work functional form true generative process. i.e. agent also learn approximation true dynamics terms generative model. possible implementation agents generative model terms high-dimensional recurrent neural network. structures shown able eﬃciently learn represent underlying structure complex time-series data real-life applications agent also access partial derivatives sensory inputs respect actions also depend knowledge true generative process real agents usually possess. using evolution strategies able derive stochastic estimates gradients enable train full active inference agent despite constraints. furthermore structure agent utilised transfer functions optimisers directly adopted large-scale machine learning hope agent scale realistic rich environments showcasing potential active inference terms rich emergent behavior simultaneous optimisation generative model environment. comparison recent action-oriented implementation active inference. another implementation active inference recently introduced baltieri buckley similar approach authors give agent access generative process neither terms functional form agent’s generative model providing partial derivatives full sensory inputs respect actions. however work diﬀers several crucial aspects ours agent formulated continuous time using partial diﬀerential equations. equations simple enough explicitly discuss dynamics. contrast discrete time model high-dimensional highly nonlinear transfer functions precluding classical analytical treatment resulting dynamics. however ﬂexible form generative model approach allows agent learn almost perfect approximation true dynamics shown ﬁgure furthermore circumvent lack explicit partial derivatives sensory inputs respect agent’s actions subdividing sensory inputs exteroceptive proprioceptive channels action suppress prediction errors proprioceptive channels. call action-oriented approach. terms model would equivalent ignoring tion density. hand leads arbitrary subdivision sensory channels necessitates proprioceptive channel approach really necessary successfully build generative model environment reach goals deﬁned agent’s prior expectations hand might severely hinder least delay agent’s learning complex realistic environments third full active inference framework underlines crucial role partial derivatives hints possible approximations implemented brain. example retinotopic maps early visual system e.g. superﬁcial deep layers superior colliculus allow quick simple inversion sensory input respect small movements note active inference action selection generally restricted sorts inputs action system access however learning parameters action mapping work clearly accountable sorts inputs. eﬀectively observe setup action function state-action policy. ﬁxed mapping approximate posterior states world given agents observations distribution possible actions known state-action policy control theory. presupposes bayes optimal action every state world. common forms policies; however universal policies sense many actions depend upon previously encountered states. however allow agent develop ﬂexible representation current state world basically include compressed representation history previous states information might required guide action. indeed agent shows behavior developed representation current velocity regarded diﬀerence current previous position. otherwise would able successfully solve mountain problem agent learn move left initial position acquire additional momentum later accelerate right exact position acquired required additional momentum allowed climb steep slope emergent curiosity. optimising generative model world gives agent sense epistemic value i.e. seek states expects also improve general understanding world. similar recent work artiﬁcial curiosity however contrast work agent interested environmental dynamics directly inﬂuence action also tries learn statistics environment beyond control. constrained sampling understanding learned models. able demonstrate constrained sampling agent’s generative model world. comparing constrained samples actual interaction environment unconstrained samples look reasonable deviate true dynamics would expect given conditioned time course constrained sampling mean timecourse agent’s true interaction world shifted back time time steps. lead samples variables also shifted back time. however ﬁrst time steps agent believes would stay initial position despite strong push left. might fact relative weighting agent’s prior expectations position quite strong. accordingly tightly sticks optimal trajectory soon learned thus dynamics infers deviate true dynamics ever experiences small subset phase space. however agent noisy environment larger stochastic ﬂuctuations forced explore encounter wider variety dynamic states learn complete realistic model environmental dynamics. hope also rests fact taking away eﬀector organs agent reduced generative recurrent latent variable model. class models able model generate complex time series data spoken language basis audio waveform thinking autonomous agents real environment constrained sampling agent’s generative model might good understand beliefs world might react given actions events opening black-box associated classical deep reinforcement learning algorithms. beneﬁts evolution strategies optimiser. evolution strategies allow exploration environment without requiring agent’s action functions probabilistic using standard reinforcement-learning algorithms deep q-learning agent discover states correspond current local optimum requiring stochastically take actions time time. might sampling completely random actions lower bounding standard-deviation actions. here artiﬁcially forced exploration required evolution strategies algorithm explores solutions parameter space agent settle fully deterministic policy policy exists respective environment. moreover gradient estimates depend full free energy completion individual simulation also resilient long time horizons sparse rewards variational bayesian perspective evolution strategies. although using evolution strategies mainly black-box optimiser non-diﬀerentiable objective function namely sampling based approximation free energy bound agent’s surprise subtle important reinterpretation population density highlights ubiquitous role variational bayes following minimisation variational bayes appendix view population dynamics aﬀord general robust ensemble based scheme optimising model parameters respect variational free energy beliefs model parameters latent states neglecting prior information parameters. similar general formulation friston also absorbed deep inference deep learning problems imperative solve dual estimation problem. subtlety free energy beliefs parameters minimises path time integral free energy parameters assumed change. towards ﬂexible expectations. right direct hardcoding expectations agent terms explicit prior distributions ﬁrst dimension agent’s latent space seems ad-hoc restricted. however could show ﬂexibility agent’s internal dynamics robust optimisation strategies agent quickly learns reach narrowly deﬁned goal able build realistic model environmental dynamics encounters. future work plan look complex priori beliefs. could done sampling agent’s generative model part optimisation process. e.g. could sample processes generative model calculate quantity constraint terms prior expectations agent placed calculate diﬀerence sampled target distribution e.g. using kl-divergence. could diﬀerence penalty free energy. enforce constraint could example basic diﬀerential multiplier method similar lagrange multipliers used gradient descent optimisation schemes. idea penalty term equal zero constraint fulﬁlled function minimised scaled multiplier combined objective would look like this optimise objective forcing penalty term zero perform gradient descent fconstrained parameters gradient ascent penalty parameter prospective sampling optimise goals agent might actually parts prefrontal cortex thinking future reach certain goals. ﬁrst step artiﬁcial life artiﬁcial general intelligence. present ﬂexible scalable implementation general active inference agent able learn generative model environment simultaneously achieving prespeciﬁed goals terms prior expectations perceived states world. hope implementation prove useful solve wide variety complex realistic problems. this could show general intelligent behaviour follows naturally free energy principle. principle turn derived necessary properties dynamic systems exchange changing ﬂuctuating environments allow sustain identity keeping inner parameters within viable bounds i.e. basic homeostatic argument thus hope work contributes concrete experimental examples intelligent behaviour necessary follows hard separate basic imperative survive adapt changing environments. author would like thank karl friston insightful comments earlier version manuscript participants organisers computational psychiatry course stimulating lectures discussions.", "year": "2017"}