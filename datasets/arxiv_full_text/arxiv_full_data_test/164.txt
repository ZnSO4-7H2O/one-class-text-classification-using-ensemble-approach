{"title": "BioEM: GPU-accelerated computing of Bayesian inference of electron  microscopy images", "tag": "q-bio", "abstract": " In cryo-electron microscopy (EM), molecular structures are determined from large numbers of projection images of individual particles. To harness the full power of this single-molecule information, we use the Bayesian inference of EM (BioEM) formalism. By ranking structural models using posterior probabilities calculated for individual images, BioEM in principle addresses the challenge of working with highly dynamic or heterogeneous systems not easily handled in traditional EM reconstruction. However, the calculation of these posteriors for large numbers of particles and models is computationally demanding. Here we present highly parallelized, GPU-accelerated computer software that performs this task efficiently. Our flexible formulation employs CUDA, OpenMP, and MPI parallelization combined with both CPU and GPU computing. The resulting BioEM software scales nearly ideally both on pure CPU and on CPU+GPU architectures, thus enabling Bayesian analysis of tens of thousands of images in a reasonable time. The general mathematical framework and robust algorithms are not limited to cryo-electron microscopy but can be generalized for electron tomography and other imaging experiments. ", "text": "cryo-electron microscopy molecular structures determined large numbers projection images individual particles. harness full power single-molecule information bayesian inference formalism. ranking structural models using posterior probabilities calculated individual images bioem principle addresses challenge working highly dynamic heterogeneous systems easily handled traditional reconstruction. however calculation posteriors large numbers particles models computationally demanding. present highly parallelized gpu-accelerated computer software performs task eﬃciently. ﬂexible formulation employs cuda openmp parallelization combined computing. resulting bioem software scales nearly ideally pure cpu+gpu architectures thus enabling bayesian analysis tens thousands images reasonable time. general mathematical framework robust algorithms limited cryo-electron microscopy generalized electron tomography imaging experiments. program title bioem. journal reference catalogue identiﬁer licensing provisions programming language cuda. operating system linux. problem dependent supplementary material online supplementary material. external routines/libraries boost fftw mpi. subprograms used situs routine reading .mrc images. distribution format repository archive. nature problem analysis electron microscopy images. solution method gpu-accelerated bayesian inference numerical grid sampling. cryo-electron microscopy revolutionized structural biology providing structures chemical-motors synthase ion-channels transporters atomic-level resolution near-native conditions single-molecule character cryo-em powerful technique great potential. fact structures many biomolecules diﬃcult characterize using x-ray crystallography nuclear magnetic resonance resolved cryo-em. advances feasible technologies time-resolved direct electron detection cameras development novel image processing methods accelerated computing capacities multi-core processors hardware accelerators gpus direct electron detection cameras record image rapidly ensemble time-dependent frames unprecedentedly electron dose capture evolution individual particles time time-resolution makes possible characterize eﬀects beam-induced motion radiation damage novel image processing algorithms ﬁlter contributions individual frame according precalculated energy ﬁlter in-situ b-factor assignment discard damaged frames. however single time-frames discarded also many complete particle images. successful classiﬁcation algorithms group images classes generate diﬀerent maps images determine highest resolution correspond cases small fraction total. despite many improvements still several scenarios cryo-em algorithms face challenges acquire high-resolution information. methods rely hypothesis molecular orientations sampled equally case hydrophobic large systems acquire preferred orientations carbon grid disordered heterogeneous complexes diﬃcult obtain suﬃciently many particles cover orientations conformation computationally challenging assign single orientation multiple conﬁgurations major diﬃculty lies generating initial models algorithms initial alignment references. thus despite enormous advances eﬀort extend reach cryoem ongoing. work provide computational tool harness power cryo-em true single-molecule technique. algorithm performs bayesian inference electron microscopy accelerated gpus calculate posterior probability structural models given experimental images. contrast standard reconstruction algorithms perform forward calculation building realistic image model compare unmodiﬁed particle image using likelihood function. bioem posterior probability calculated allows accurately rank discriminate sets structural models. paper organized follows ﬁrst describe mathematical framework create calculated image model bayesian technique obtain posterior probability particle images. then introduce bioem algorithm describe main routines well parallelization scheme. demonstrate performance scalability bioem program high-performance computing cluster using benchmark sets particle images. lastly discuss integrated out. requirement models projection image calculated them. models thus represented variety ways atomic coordinates electron density maps simple geometric shapes. idea perform forward calculations projection-image intensities given models compare observed image intensities calculation posterior takes account relevant factors experiment molecule orientation interference eﬀects uncertainties particle center normalization oﬀset intensities noise. account interference inelastic scattering eﬀects imaging contrast transfer function envelop function respectively assumed radially dependent simplicity. product fourier-space equivalent real-space point spread function translation vector shifting image thus particle center pixel pixel scales intensity intensity oﬀset. likelihood function model image parameters figure rotation projection point spread function convolution center displacement integrated-out parameters normalization oﬀset standard deviation noise likelihood function establishes similarity calculated image observed experimental image. dividual observed calculated images ﬁxed parameters contrast maximum likelihood techniques determine single optimal parameter perform bayesian analysis covering wide range possible parameter sets. range weight individual sets deﬁned prior probability combined prior model bayesian posterior probability model given image weighted integral product prior parameters shown ref. integrate noise normalization oﬀset analytically. resulting analytical expression posterior probability function remaining parameters shown supplementary information. average mean squared averages intensities observed calculated images precomputed. additionally involves estimation cross-correlation calculated observed images remaining integrals orientations parametrized parameters center displacements evaluated numerically. importantly parameter combination cross-correlation calculated computationally demanding step. fig. model must ﬁrst rotated projected convoluted displaced. loop experimental images principle independent best nest loop inside loop model rotations convolutions. then rotation projection convolution computed repeatedly every observed image. listing presents pseudo-code bioem algorithm. here subroutine compute probability computes posterior probability norm images array precomputed rotations space readily implemented evaluate integral image translation vectors implemented alternative approaches suitable small large sets respectively. real space formulation cross-correlation simply calculated diﬀerent displacements fourier space fact cross-correlation calculated observed images shifted given overline indicates complex conjugate. take advantage fact convolution carried fourier space produces precalculated. hence computationally intense part fourier backtransformation compare algorithm variants real-space requires computation many cross-correlations displacements. fourier variant requires fourier backtransformation irrespective number displacements. naturally ﬁrst version advantage displacements variant superior many displacements. fact fig. shows fourier version superior cases gpu. fourier variant diﬀers real-space version wraps around borders experimental image instead cropping however comparing results certain larger datasets conclude pose problem particle generally located center image region around consists mostly noise. moreover using fourier variant advantage convert calculated image back real space computing cross-calculation saves fourier transforms. figure processing time real-space fourier-space variants compute cross-correlation calculated image observed image function number center displacements measurement taken -image benchmark system speciﬁed table perform fourier-transformation using fast fourier transform libraries fftw cufft gpu. since images real fast real-hermitian variant oﬀered libraries. computing time algorithms depends image size box-sizes powers dimension follow standard em/fft suggestions best performance. computed numerical integral using single double precision without kahan summation algorithm found options lead numerical result mainly likelihood function sharply peaked around maxima parameter sets contribute summation. thus default fastest setup uses single precision without kahan summation. benchmarks shown paper based single-precision setting. common technique optimize memory access patterns nested loops blocking improves cache usage. alternatively memory access pattern depend loops place loop innermost loop. apply optimization real-space variant. loop displacements shifts memory access entry iteration loop innermost loop since fourier variant ffts computationally dominant need blocking optimization case. real-space version written code support compiler auto vectorization. checking disassembly object veriﬁed compiler vectorized code exactly intend. since optimized libraries vectorization take action respect fourier variant. order speed processing compute-intense bioem task parallelize processing vectorization. consider parallelization cores inside compute node parallelization multiple compute nodes usage parallel accelerator devices like gpus speed processing. desirable parallelize inner loop images shared memory architecture. need compute rotation projection convolution once reuse calculated image comparison observed images obs. employ openmp process comparison images parallel. outermost loop rotations dependencies hence chosen parallelize message passing interface support utilization many compute nodes parallel. fig. shows schematic representation parallelization approach. grid cell represents single rotation single experimental image inside integrals projection convolution center displacement performed fourier algorithm calculates figure representation double parallelization scheme used bioem algorithm. used parallelization diﬀerent model rotations cuda and/or openmp used parallelization diﬀerent images. pair model orientation particle image loop convolution kernels performed cross-correlation observed image calculated using fast-fourier algorithm. center displacement cross-correlation experimental image simultaneously. experimental images also rotation projection fourier transformation take negligible time order speed bioem openmp precalculate fourier transformed images parallel. recent years graphics processing units shown signiﬁcant speedup many applications among heavily used bioem. order leverage potential adapted bioem gpus. bioem cuda cross-correlation step essentially consists image multiplication fourier space fourier back-transformation. consider bringing steps parts time critical processed well cpu. pipeline prepare next rotations convolutions runs comparison last observed images. also arrange remote direct memory access transfer data host pipeline figure processing time real-space fourier variants calculation posterior probability function number observed images. results shown variants real-space fourier-space measurement taken -image benchmark system speciﬁed table asynchronously cuda streams. bioem keeps executing kernels time idle time. order full extent bioem splits work uses processor types jointly comparison step. fig. shows computational time function number observed images. except small sets images processing time depends linearly number observed images. transition linear scaling occurs images thus would encounter typical applications. table summarizes evolution code’s performance subsequent development phases starting ﬁrst serial prototype version executed single core full gpuaccelerated multicore node. would like emphasize huge overall speedup large degree algorithmic optimizations pointed sections openmp parallelization version. latter deﬁnes performance baseline fair comparison version delivers speedup factor case table performance evolution bioem subsequent optimization phases measured workstation intel nehalem core -core nvidia titan gpu. test ﬂexibility chaperonin groel orientation center displacement. third column states absolute runtime forth ﬁfth column give relative speedup compared previous initial code version respectively. using cuda proﬁler measured compute time spent cufft library datasets. fraction time spent fftw even larger. majority remaining time used pixel-wise multiplication images fourier space deﬁnition memory-bandwidth limited particular gpu. small ineﬃciency respect images stored memory multiplication read hardly avoided libraries. libraries already well optimized thus margin additional speedup limited. section presents performance evaluation bioem software focusing parallel eﬃciency performance obtainable typical high-performance compute cluster employed production runs bioem. tests performed high-performance system hydra max-planck-society operated max-planck computing data facility garching germany. consists dual-socket nodes equipped intel xeon cpus interconnected high-performance network subset nodes equipped nvidia kepler gpus each. benchmarks hydra system intel compiler suite cuda fftw used linux operating system sles. density used reference model. numerical integrals performed grids orientations psfs center displacements. parameter setup consistent case without prior knowledge symmetry system orientations particle images. thus take advantage -fold symmetry complex reduce orientational search. however practical applications easily implemented searching restricted euler angles corresponding quaternions. fig. shows achieve almost perfect linear scaling number physical cores node datasets. bioem allows parallelize cores inside node several ways observed images openmp orientations combination openmp hybrid setup. ﬁgure compares openmp scalability. eﬀects limit pure openmp scaling. first unavoidable non-uniform memory access eﬀects common global data stored thus scattered numa domains. becomes apparent fig. shows large small setup show good scalability maximum cores numa domain. second particular small dataset synchronization computation likelihood limits performance. contrast aspects aﬀect pure setup process mapped individual core. conﬁgurations exhibit nearly perfect scaling maximum physical cores node datasets suﬃciently many orientations parallelization. thus note side result memory bandwidth limiting factor context. since bioem compute bound ffts hyperthreading yields small non-negligible improvement. figure bioem speedup compared single thread function number employed cores using openmp observed images orientations parallelization small large datasets. shaded region indicates hyper-threading. important advantage openmp parallelization smaller memory footprint. openmp case threads share copy observed images process case copy. large dataset example requires already prohibitive today’s clusters memory requirement increases larger datasets. moreover plain parallelization total number orientations poses strict upper limit number tasks. would ultimately limit strong scalability bioem particular smaller problems orientations. steadily increasing number cores stagnating per-core performances exacerbate constraints future. fig. provide overview performance deﬁned inverse runtime obtained employing diﬀerent parallelization gpuacceleration options implemented bioem small large datasets respectively multiple nodes hydra cluster. ﬁgures show multiple curves diﬀerent execution conﬁgurations. curves distinguish between cpu-only conﬁgurations gpu-only conﬁgurations combined conﬁguration uses gpus cores node. gpu-only case curves gpus node shown. present three curves cpu-only case diﬀer parallelization approach. show curves pure openmp parallelization inside nodes pure parallelization hybrid mpi-openmp conﬁguration processes openmp threads node. conﬁgurations obtain close-to-perfect linear scaling number nodes absence communication load-imbalances implementation. hybrid mpi-openmp parallelization adds ﬂexibility parallelization helps contain memory footprint. reduce number processes node multiple cores process used eﬃciently openmp parallelization. instance assigning processes two-socket node avoids numa limitations reduces memory footprint enormously that shifts strong scaling limit number orientations number orientations multiplied number cores socket. thus hybrid approach speed computations employing larger computational resources number orientations limits figure bioem wall-clock performance function number compute nodes small dataset observed images employing diﬀerent parallelization gpu-acceleration options. solid lines cpu-only conﬁgurations squares dashed lines gpu-only conﬁgurations. combined workload conﬁguration observed images devices remaining openmp threads node. scaling. respect intel xeon many-core threads scarce resource high-bandwidth memory presumably separated four numa domains might powerful energy-eﬃcient architecture operating bioem software hybrid mpi-openmp setup. large dataset comprising images allows better exploitation parallel architecture. small benchmark task fast -core ivy-bridge processors. processing large dataset runs twice eﬃciently achieving performance full ivy-bridge node processors. cases gpus reach twice performance single gpu. figure bioem wall-clock performance function number compute nodes large dataset observed images employing diﬀerent parallelization gpu-acceleration options. solid lines cpu-only conﬁgurations squares dashed lines gpu-only conﬁgurations. combined workload conﬁguration observed images devices remaining openmp threads node. compared processor reading images global memory saturates memory bandwidth. here processor play strength larger caches. note whereas cpus easily hold tens thouhowever limited memory size real restriction processing images takes place independently. process subsets images step-by-step host combine results later since processing subset images takes order minutes overhead additional transfers repeated projections etc. model negligible. bioem also allows split workload among observed images split sets goes analyzed using openmp. workloadsharing improves performance signiﬁcantly. full capacity node utilized proﬁting devices cores node. optimal splitting ratio depends speciﬁc problem hardware. setups fastest setting small large datasets. synchronization issues combined conﬁguration achieve individual performances. performance assessment shows optimal execution setup depends problem. trend parallelization works better many orientations openmp needs many observed images. hybrid setup often best compromise scales almost perfectly linear number cores. cases recommend employing process numa domain. instance complete analysis images orientations psfs takes approximately minutes ivy-bridge nodes. analysis performed within minutes nodes accelerated gpus. demonstrates software eﬃciently handle analysis large amounts experimental particles used electron microscopy. estimate grid point using fourier-algorithm code takes approximately bioem method provides alternative approach structurally characterize biomolecules using electron microscopy images. calculating posterior probability model respect individual image avoids information loss averaging classiﬁcation allows compare structural models according posterior probability. bayesian analysis methods relion enormously successful contributing much resolution revolution however main relion reconstructing densities projection images rank compare existing structural models. also diﬀers bioem integration scheme optimization algorithms bioem requires relatively images discriminate correct model within pool plausible structures whereas full reconstructions relion typically requires tens hundreds thousands particles costly computational resources implement multiple methods select classify polish particles well reﬁne maps. beyond applications studies highly dynamic systems envision bioem complement traditional reconstruction techniques ﬁrst steps classiﬁcation assigning accurate orientations single-particle estimations last steps reﬁnement validating ﬁnal models. addition bioem method applied problems reconstruction techniques fail particle images acquire preferred orientations system ﬂexible. mathematical framework also extended analyze individual time-dependent frames direct electron-detection cameras electron tomography tilt-series foresee bioem method generalized types imaging experiments atomic force light microscopy appropriate modiﬁcations forward calculation cal. possible limitation method structural models required. however models constructed using low-resolution data hybrid modeling combining coarse-grained maps components homologous domains models simulations. however models incomplete bioem cannot give absolute estimation posterior probability rather relative value. thus model comparison essential bioem framework. bioem software scales almost ideally number cores excellent performance architectures. code optimized fast accurate analysis tens thousands images required electron microscopy suﬃciently ﬂexible adjust diverse research necessities. order cope growing heterogeneity gpu-accelerated systems plan future autotuning feature bioem dynamically chooses optimal distribution workload cpus gpus node. distribution continuously adjusted based measurement current image processing rate. moreover autotuning could suggest good setting number processes node hybrid mpi-openmp mode. performance bioem dominated great extent libraries well optimized leaving little margin performance improvements. specifically three consecutive steps computing cross-correlation posterior probability multiplication fourier space fast fourier backtransformation evaluation analytic formula. multiplication images fourier space saturate memory bandwidth gpu. since cufft library performs black intermediate data must stored fft. however need certain fourier coeﬃcients cufft computes them. could possible modify cufft extract relevant coeﬃcients. however pursued considering challenges required modify maintain code. diﬀerent optimization promising. possible parallelize projections openmp instead using mutexes control probability updates observed images. implementation achieve performance version keep small memory footprint openmp version. would optimal analyze images molecular orientations distributed randomly correlated. example electron tomography correlations arise diﬀerent tilt images particle. authors acknowledge prof. k¨uhlbrandt vonck allegretti availability micrographs feedback experimental techniques. p.c. f.b. m.r. g.h. supported planck society. bioem software downloaded webpage https//gitlab.mpcdf.mpg.de/mpibp-hummer/bioem jointly manual tutorial.", "year": "2016"}