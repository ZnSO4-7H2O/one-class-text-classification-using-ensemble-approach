{"title": "Linear response for spiking neuronal networks with unbounded memory", "tag": "q-bio", "abstract": " We establish a general linear response relation for spiking neuronal networks, based on chains with unbounded memory. This relation allows quantifying the influence of a weak amplitude external stimuli on spatio-temporal spike correlations, in a general context where the memory in spike dynamics can go arbitrarily far in the past. With this approach, we show how linear response is explicitly related to neuron dynamics with an example, the gIF model, introduced by M. Rudolph and A. Destexhe [91]. This illustrates the effect of the stimuli, intrinsic neuronal dynamics, and network connectivity on spike statistics. ", "text": "establish general linear response relation spiking neuronal networks based chains unbounded memory. relation allows quantifying inﬂuence weak amplitude external stimuli spatio-temporal spike correlations general context memory spike dynamics arbitrarily past. approach show linear response explicitly related neuron dynamics example model introduced rudolph destexhe illustrates eﬀect stimuli intrinsic neuronal dynamics network connectivity spike statistics. neurons communicate short-lasting electrical signals called action potentials spikes allowing rapid propagation information throughout nervous system minimal energy dissipation spike shape remarkably constant given neuron contemporary view consider spikes quanta information consequence information presumably encoded spike timing simplest quantitative characterize spiking activity neuron ﬁring rate probability neuron spikes small interval inﬂuence external stimulus ﬁring rate changes. classical ansatz coming volterra expansions write variation ﬁring rate neuron convolution form exponent recalls consider ﬁrst-order eﬀect stimulus stimulus weak enough higher order terms volterra expansion neglected. example linear response variation rate proportional stimulus. here convolution kernel constrained underlying network dynamics. example sensory neurons functions space time convolution takes explicit form general response spiking neuronal networks time dependent stimulus aﬀect rates also impact higher order correlations neurons neurons connected. situation sketched fig. figure time-dependent stimulus applied time neurons spiking neuronal network result spiking activity modiﬁed well spike correlations neurons even neurons directly stimulated interactions. particular sensory neurons convey collectively brain information external stimuli using correlated spike patterns resulting conjunction stimulus inﬂuence intrinsic neurons dynamics neurons interactions synapses correlated ﬁring linked stimulus encoding stimulus discrimination intrinsic properties network remain absence stimulus however disentangling biophysical origins correlations observed spiking data still central diﬃcult problem neuroscience consequence correlations spiking neuronal networks attracted considerable amount attention last years experimental data analysis perspectives well theoretical modeling viewpoint hand novel experimental recording techniques permit measure collective spiking activity larger larger populations interacting neurons responding external stimuli recordings allow particular better characterize link stimuli correlated responses living neuronal network paving better understand \"the\" \"neural-code\" neuronal responses highly variable even single neuron level presenting repetitions stimulus controlled experimental conditions neural activity changes trial trial thus researchers seeking statistical regularities order unveil probabilistic causal relation stimuli spiking responses hand mathematical models spiking neuronal networks oﬀer complementary approach biological experiments based biophysically plausible mechanisms controlling dynamics neurons mathematical modeling provides framework characterize population spike train statistics terms biophysical parameters synaptic connectivity history previous spikes stimuli. hope understanding aspects model allow better process extract information real data. large learned model experiments real neurons. theoretical computational point view characterizing response neuronal network model stimulus relating correlated response spike trains measured recordings involves several modeling steps. modeling spontaneous activity. goal characterize collective spiking activity absence external stimuli. situation spike correlations assumption neuronal dynamics interactions. modeling response stimuli. assume spiking neuronal network receives time-dependent stimulus time time fig. even stimulus applied subset neurons network inﬂuence eventually propagate neurons directly indirectly connected. stimulus spikes timing modifying spike correlations. particular relation ought extend general statistical indicators rates. statistical indicator rate correlation generally observable deﬁned section expects volterra expansion time-dependent variation action stimulus would take form convolution kernel depends well network dynamics determining explicit mathematical form diﬃcult general example based ruelle’s linear response theory applied amari-wilson-cowan model experimental characterization spontaneous activity response stimulus. experiments underlying neuronal network known. access spike trains. therefore level deﬁne eﬃcient operational characterize spike statistics spontaneous regime well stimulated regime. last point particularly tricky implies non-stationary response whereas prominent standard statistical methods like maximum entropy boltzmann machines heavily rely stationarity assumption addition processes generating spikes trains causal memory suggesting markovian dynamics. fact memory depth dependent neurons dynamics variable length markov dynamics could realistic don’t know work addressing points simultaneously seems long toward achievement. paper step direction. contrast relatively large literature considering links points. modeling collective neural response stimulus large body theoretical work linking spike responses neuronal network models structural properties presence stimuli. example relation stimuli ﬁring rate function parameters model obtained network homogeneous leaky integrate-and-fire neurons considered mean-ﬁeld limit computing ﬁring rate time-varying stimuli however much diﬃcult problem beyond ﬁring rates also results concerning correlations. toyoizumi develop mean-ﬁeld methods approximating stimulus-driven ﬁring rates autocross-correlations stimulus-dependent ﬁltering properties spiking neuronal networks markov refractoriness. authors obtain formulas cross-correlations arbitrary delay terms motifs neuronal connectivity while correlation structure networks interacting hawkes processes investigated. general neuronal networks dynamics involves interactions neurons time delay strongly depends network history. statistics general spike events involving distinct neurons spiking diﬀerent times ought aﬀected stimuli could shed light coding process. obviously thorough characterization events gets rapidly reach experimental data number neurons involved event time delays increases. opposite characterization achieved neuronal network models using analytic expressions show. statistical models recordings wide literature modeling inference approaches recently developed describe correlated spiking activity populations neurons recent review rather complete although mentioning mathematical literature subject). approaches cover variety models describing correlations pairs neurons well larger groups synchronous delayed time without explicit inﬂuence stimulus including hidden latent variables. distinguish main trends. ﬁrst inspired statistical physics seeks characterize spike correlations restricted form gibbs distribution i.e. derived maximum entropy principle form below. term \"restricted\" maximum entropy principle requires stationarity models used literature consider spike time correlations; instance ising model extensions higher order spatial correlations successive times independent. fact gibbs distributions limitations discussed detail section second trend consists building stochastic processes reproducing spike correlations based family transition probabilities taking account causality history. prominent examples generalized linear model model hawkes processes discussed reviewed next section probability distributions generated well gibbs distributions considered general setting standard statistical physics courses. time-dependent non-stationary spike population response stimulus assume reasonable characterization spike train statistics e.g. techniques described previous paragraphs spontaneous activity predict spike correlations modiﬁed under inﬂuence time-dependent stimulus weak enough amplitude neglect higher order corrections? show paper derive quite general linear response theory somewhat linking extending notion linear response theory used non-equilibrium statistical physics ergodic theory spiking processes inﬁnite memory. particular generalizes general observables. however theory stays formal level without concrete example convolution kernel explicitly computed neurons dynamics. show achieved generalized integrate fire model introduced rudolph destexhe theory develop roots non-equilibrium statistical physics ergodic theory linear response determines expectation value observable dynamical system changes upon weakly perturbing dynamics. seminal result context ﬂuctuation-dissipation theorem linear response depends correlation functions unperturbed system. approach proceeds along similar lines meaning linear response predicted spikes correlations unperturbed system. paper organized follows. section brieﬂy review linear response statistical physics ergodic theory allowing make link neuronal networks considered dynamical systems statistics spikes. section introduce formalism chains unbounded memory allowing handle non-stationary spike distribution unbounded memory. context derive general linear response formula used throughout paper. equation expresses time-dependent variation average observable time series speciﬁc correlation functions computed respect spontaneous activity result reminiscent ﬂuctuation-dissipation theorem statistical physics applied spike statistics. section introduce spiking neuronal network model instantiate analysis. model presented associate spiking activity discrete stochastic process deﬁned transition probabilities memory unbounded. probabilities written function parameters model. this able explicitly write discrete time form convolution kernel explicit function model parameters especially synaptic weights. expression relies markovian approximation chain decomposition theorem spike observables introduced general context hammersley cliﬀord main result linear response theory? response system originally equilibrium time-dependent stimulus proportional stimulus coeﬃcients obtained correlations functions computed equilibrium. derive result type model addressing central question correlations matter related synaptic interactions? neuronal networks considered either dynamical systems spike generating processes characterized transition probabilities computed spike train observations. ﬁrst case natural seek linear response dynamics itself using possible approximations second case deﬁne probability distribution spike trains order investigate eﬀect perturbation. section show approaches related making link classical statistical physics approach linear response dynamical systems ergodic theory neuronal networks. introduce general formalism chains unbounded memory allowing handle non-equilibrium linear response spiking neuronal networks. material section known diﬀerent domains statistical physics ergodic theory stochastic processes neuronal networks presented better understanding next sections. simplicity consider introductory section dynamical system taking ﬁnite number \"states\" state denoted statistical physics linear response theory addressed terms. system thermodynamic equilibrium probability observe state given boltzmann-gibbs distribution magnetic ﬁeld conjugated parameters correspond intensive quantities like temperature electric potential pressure chemical potential magnetic susceptibility general depend location physical space equilibrium uniform space though. form i.e. choice constrained physical properties system. also constrained boundary conditions. standard statistical physics courses gibbs distribution form obtained consequence principle maximum entropy principle probability measure states statistical entropy denote expectation respect maximum entropy principle seeks probability distribution maximizing statistical entropy constraint average energy constant i.e. probability measure states. probability exists unique states ﬁnite; inﬁnite additional summability conditions required ensure existence uniqueness non-equilibrium situation arises uniform space generating gradients gradients result currents general currents nonlinear function gradients. onsager linear response theory assumes currents linear combinations gradients known examples ohm’s electric current proportional gradient electric potential fourier’s heat proportional temperature gradient fick’s etc. several gradients simultaneously involved like peltier eﬀect. proportionality coeﬃcients called onsager coeﬃcients property interests onsager coeﬃcients obtained correlations functions computed equilibrium thus knowledge correlations equilibrium allows infer non-equilibrium response system perturbations. maximum entropy principle powerful tool allows establish link description system terms states thermodynamics characterized macroscopic averages. however many scientists starting boltzmann tried construct equilibrium non-equilibrium statistical physics microscopic dynamics system. equilibrium problem formally stated way. \"given autonomous dynamical system compact phase space natural probability measure characterizing trajectories sample phase space? conditions probability take exponential form autonomous means vector ﬁeld dynamical system independent time natural context address question ergodic theory. ﬂows preserving volume phase space natural measure liouville measure dissipative systems natural measure so-called sinai-ruelle-bowen measure whose existence guaranteed speciﬁc class dynamical systems given weak-limit lebesgue measure dynamics systems exist addition remarkable ﬁnite partition phase space called markov partition allowing trajectories dynamical system trajectories markov chain measure invariant measure markov chain. gibbs distribution potential determined jacobian obeys well maximum entropy principle. measure extends time-dependent uniformly hyperbolic systems context david ruelle provided proof linear response consequence diﬀerentiability measures respect smooth perturbations established linear response formula onsager coeﬃcients depending jacobian ﬂow. kubo relations onsager coeﬃcients obtained correlations equilibrium. non-uniformly hyperbolic systems exist linear response like henon map. however violation detected quantiﬁed numerically neuronal networks modeled dynamical systems therefore linear response theory addressed using tools brieﬂy presented previous section. example done discrete time amari-wilson-cowan model convolution kernel appearing explicitly computed especially compute response neuron weak harmonic perturbation another neuron exhibiting speciﬁc resonances functional connectivity distinct synaptic graph. dealing spiking models integrate fire dynamics diﬀerentiable anymore still markov chain formalism used developed below. particular exhibit example transition probabilities explicitly computed directly related dynamics. generally formalism presented section extends situation dynamics unknown rasters observed access spike correlations idea inherent linear response theory correlations absence stimulus infer linear response time-dependent stimulus weak amplitude. leads speciﬁc questions intrinsic neuronal dynamics kubo-like relations spiking neural networks? linear response coeﬃcients related spontaneous spiking dynamics? statistical physics wisdom expect linear response given spikes correlations natural basis expand linear response? question related previous one. many possible spike correlations space time pairs triplets clearly expects high correlations play less important role than pairs correlations possible quantify depends neurons dynamics. show paper dependence lays transfer matrix constructed transition probabilities depending dynamics. important consequence derived formalism exponential correlation decay. fairly general conditions spectrum transfer matrix largest eigenvalue remainder spectrum implies exponential correlations decay ensures existence linear response also implies existence resonances power spectrum strong consequences linear response harmonic signals. developed section below. synaptic versus eﬀective connectivity. modeling spike train statistics experimental data without knowing underlying dynamics leads partial knowledge causality spike trains. approach makes appear notions \"eﬀective connectivity\" build stationary correlations neurons eﬀective connectivity related synaptic connectivity? another notion eﬀective connectivity arises neuronal response. excite single neuron stimulus check whether neurons respond. provides third notion connectivity. shown discrete time version amari-wilson-cowan model notions connectivity lead completely diﬀerent graphs connectivity situation spiking neurons? partial answer question found paper mapping integrate fire model ising considered statistical physics methods. however ising model considers instantaneous pairwise spike events whereas spike interactions involve delays. thus better-adapted notion eﬀective connectivity include delays. provide general formalism paper. show particular linear response connectivity diﬀers general pairwise correlations connectivity except simpliﬁed models like ising ﬂuctuation-dissipation theorem establishes case proportionality susceptibility instantaneous pairwise correlations plasticity. another advantage linear response theory handle eﬀect synaptic plasticity learning. assume neuronal network submitted stimulus synaptic weights evolving time according learning rule involving spike pairwise correlations response stimulus depends correlations correlations depend data. important application linear response correlations taken respect statistics spontaneous activity approximated experimental recordings neuronal network spiking absence external stimuli using maximum entropy principle. thus linear response formula could used anticipate response biological neuronal network weak stimulus knowing structure spontaneous spike correlations. neurons variables membrane potential ionic currents described continuoustime equations. contrast spikes resulting experimental observation discrete events binned certain time resolution paper jointly consider time descriptions. consider network neurons labeled index deﬁne spike variable neuron emitted spike time interval otherwise. denote network time call spiking pattern. note state space spiking patterns network neurons; spike block denoted sequence spike patterns blocks elements product an−m also denoted text. last notation consider processes inﬁnite memory want explicit notation an−∞ corresponding events. time-range block number time steps call spike train inﬁnite sequence spikes past future. spike trains thus alleviate notations note spike train shift operator allows step forward time along raster notion observable. function associates real number spike-train. observable range follows hammersley-cliﬀord theorem range-r observable written form neuron index thus raster neuron spikes time neuron spikes time otherwise number degree monomial; degree monomials form degree monomials form ωiωi thus monomials similar physicists call interactions; case interactions involve time delay spikes. monomials neurons range index integer one-to-one correspondence pairs advantage monomial representation focus spike events natural spiking neuronal dynamics. decomposition straightforward. however would like give simple illustrative example. consider neuron times spiking patterns function patterns takes values have called index block. deﬁne block inclusion bits convention block degree included blocks. extension means bits block corresponding integer included block corresponding result shows coeﬃcient monomial linear combination function values number terms combination increases monomial degree. integer part function depends raster spikes preceding current time range-r time dependent observable function −d). decomposition holds well time dependent range-r observable \"natural\" characterize statistics observed spike trains associate markov chain transition probabilities transition probabilities depend time approach \"natural\" captures causality conditioning past spikes. call memory depth chain perron-frobenius theorem markov chain unique invariant probability chapman-kolmogorov relation constructs transition probabilities probability measure where block range viewed transition block \u0001\u0001). extension block blocks range transition legal \u0001\u0001). basis construct transfer matrix block positive entries follows perron-frobenius theorem unique real positive eigenvalue strictly larger modulus eigenvalues positive right eigenvector left eigenvector moreover range-r observable establishes ﬁrst relation gibbs distributions form strong diﬀerence though. whereas assumed hold ﬁnite state characterizing system given time trajectory system describing time evolution. addition probability block conditioned upon past which statistical physics would correspond determine probability block binary variables next section. dropped boltzmann constant plays role here. then shown satisﬁes variational principle maximizes invariant probability ﬁxed amounts maximizing entropy constraint average energy ﬁxed. finally supremum corresponds free energy generating function cumulants. therefore shown potential form associated homogeneous markov chain invariant probability extends notion gibbs distribution introduced section systems memory probability state depends ﬁnite history. extension inﬁnite history made next section. important remark note didn’t detailed balance property here. detailed balance completely unnecessary deﬁne gibbs distributions markov chains. reciprocally associate markov chain strictly positive transition probabilities function form fact inﬁnitely many functions minimal number monomials actually raises serious problem dealing so-called maximum entropy models handle neuronal spike trains. energy known priori. contrast thermodynamics entering deﬁnition energy known ﬁrst principles principle guide order reduce number terms exist methods reduce complexity still remain large number terms. example exactly generalized linear model depending parameters maximum entropy model number terms model generically exponential stage still dealing time-translation invariant systems therefore possible representations handle spike train statistics maximum entropy approach generic potential form markov chain approach. approaches equivalent ﬁrst agnostic underlying model many redundant irrelevant terms hard interpret; second requires know transition probabilities either inferring data general diﬃcult many blocks appear sample cannot reliably estimate conditional probabilities guessing form models like however possible establish form analytically models developed section correlation decay replacing construct matrix which perron-frobenius theorem largest eigenvalue eigenvector rest spectrum bounded away important property resulting this used throughout paper exponential correlation decay. observables integer times deﬁne correlation present case time translation invariant depends write simplicity cfg. using fact adjoint time shift operator using spectral decomposition theorem obtains that γkfg complex numbers. note that real eigenvalues complexconjugated. coeﬃcients γkfg combines together produce real correlations functions exponentially decaying part illatory part note starts term corresponding removed deﬁnition correlations. follows spectrum therefore correlations decay exponentially fast characteristic rate however considering neural networks memory necessarily neither constant bounded consider e.g. integrate fire model memory goes back last time past neuron ﬁred. general possible bound time. general formalism consider chains unbounded memory course discuss below markovian approximations possible useful. still needs properly control approximations. addition want consider case system submitted time-dependent stimulus dynamics time-translation invariant. represent probability time observes spiking pattern given network spike history. non-markovian stochastic process known chain complete connections \"chain unbounded memory\" deﬁned detail here. section follows close deﬁnition system transition probabilities family {pn}n∈z functions an−−∞ following conditions hold every every function measurable respect f≤n−. f≤n-measurable functions probability measure exists called chain complete connections consistent system transition probabilities {pn}n∈z. possible multiple measures consistent system transition probabilities. intuitive meaning continuity following. quantity varm corresponds maximum variation observe probability spike state time given history ﬁxed time thus continuity implies variation tends zero tends inﬁnity past spike sequence ﬁxed less probability given past varies present. equations emphasize connection gibbs distributions statistical physics acts \"energy\" instead term \"potential\". correspondence case consider time -dimensional lattice boundary conditions past ωm−−∞ stochastic process. contrast statistical physics potential deﬁned transition probabilities normalization factor equal reason call normalized gibbs potential. similar essential diﬀerence memory inﬁnite potential inﬁnite range. well known statistical physics inﬁnite range potentials require speciﬁc conditions associated unique gibbs distribution. mathematically well founded correspondence chains complete connections gibbs distributions however chain complete connections deﬁne probability transitions present conditioned upon past gibbs distributions allow well condition \"upon future\" leads diﬀerent notions \"gibbsianness\" equivalent shall develop distinctions call gibbs distribution chain complete connection. consider neural system spike statistics characterized time-translation invariant gibbs distribution \"sp\" stands \"spontaneous\". suppose that absence stimulus spontaneous dynamics stationary. assume stimulus applied time conditions existence uniqueness chain complete connection more generally gibbs distributions statistical physics extend probability distributions probability observe certain conﬁguration spins restricted region space constrained conﬁguration boundaries region. therefore deﬁned terms speciﬁcations determine ﬁnite-volume conditional probabilities exterior volume known. spatial dimension identifying time axis corresponds conditioning past future. contrast families transition probabilities exponential continuity rate deﬁne so-called left-interval speciﬁcations goal establish explicit equation function stimulus. done volterra-like expansion powers stimulus ﬁrst order obtain linear response terms convolution stimulus convolution kernel depending obtain relation proportionality coeﬃcient linear response speciﬁc correlations functions computed equilibrium provides kubo relation holding case neuronal networks unbounded memory arbitrary range-r observable contrast volterra expansion formalism allows explicit dependence neuronal network characteristics example provided next section. here note explanation necessary. functions random functions randomness comes determined probability average continuous time dependent observable averaged discrete time spike train discrete time depend spike events occurring times posterior equation expresses time-dependent variation average observable expressed ﬁrst order time series correlation functions time-dependent variation normalized potential computed respect equilibrium distribution. main result. similar ﬂuctuation-dissipation theorem statistical physics here holds gibbs distributions inﬁnite range potential crucial point convergence series initial time perturbation tends holds correlations decay suﬃciently fast typically exponentially. come back point next section. advantages relation averages taken respect case experimental data averages approximated empirical averages spontaneous activity. still unknown. show constraint. although inﬁnite range memory dependence decay fast typically exponentially. property independent application stimulus holds well hence case inﬁnite range potentials approximated ﬁnite-range one. classical result ergodic theory potential exponentially decaying variation well approximated ﬁnite-range potential norm equivalently chain inﬁnite memory replaced markov chain memory depth therefore approximated range-r potential eigenvalues matrix introduced section coeﬃcient γkll depends projection eigenvector follows expor=−∞ converges exponential rate controlled second eigenvalue ensures convergence linear response equation addition series truncated keeping time order considering projection monomials highest eigenvectors. also linear response equation involves monomials whose probability decreases fast degree. consequently truncate sums degree monomials note correlation monomial degree involves product already monomial degree monomials probability general. resonances denote matrix diagonalize matrix simplicity consider time-independent observable note π−f. assume vector form aleiπνr real frequency. happens example stimulus harmonic weak enough compute ﬁrst order expansion dealing linear response stimulus take complex thus linear response harmonic perturbation harmonic. particular response exhibits resonance complex frequency domain λie−ßπν therefore resonance observed dynamics directly related eigenvalues discrete convolution. close form diﬀerence time discrete coming spike trains discretization. stimulus explicit hidden give illustration next section. fundamental result convolution kernel deﬁned linear combination monomial correlations functions. therefore form kubo equation monomials play role physical quantities introduced section mentioned above main essential diﬀerence that contrast physics priori idea monomials important known principle guide choice. example application results consider so-called generalized integrate-and-fire introduced rudolph destexhe analyzed consider model despite complex dynamics depending spike history allows analytic treatment giving access collective spike statistics. step toward applying linear response theory introduced previous section understanding consequences. model rather complex dynamics conductances current depend spike history. sake clarity introduce leaky integrate-and-fire model. brieﬂy recall main results necessary apply formalism characterization transition probabilities note therefore material section published elsewhere time membrane potential neuron reaches ﬁring threshold neuron ﬁres action potential i.e. emits spike membrane potential neuron reset ﬁxed reset value vres instantaneously. without loss generality vres neuron’s membrane potential remains value time denoted called refractory period i.e. vres equation reset condition deﬁnes model ﬁrst introduced lapique model synaptic conductance pre-synaptic neuron post-synaptic neuron depends spike history holds pre-synaptic spike-times here maximal conductance zero synaptic connection neurons so-called α-proﬁle mimics curse post-synaptic potential pre-synaptic spike function somewhat summarizes complex dynamical process underlying generation post-synaptic potential emission pre-synaptic spike. take figure sample time trajectory membrane potential neuron plotted continuous time. membrane potential reaches ﬁxed threshold reset ﬁxed value vres spike recorded discrete time otherwise respectively leak conductance leak reversal potential reversal potential characterizing synaptic transmission finally white noise introducing stochasticity dynamics. intensity depends network spike history contains stochastic term. reversal potential positive negative synaptic weights deﬁne oriented signed graph whose vertices neurons. model conductances depend explicitly membrane potential explicitly integrate sub-threshold dynamics. ﬁxed equation characterizes membrane linear equation potential evolution neuron threshold i.e. neuron spike time interval easily extend deﬁnition including reset denoting last time past neuron ﬁred spike-train ﬁrst term r.h.s corresponds \"spontaneous\" dynamics independent external stimulus perturbation noise. spontaneous contribution divided part corresponding network eﬀects part corresponding leak voltage model depends history ways. time integrals start time last time past anterior neuron spiked. time unbounded arbitrarily past. second history dependence constrained raster history conductance voltage reset neuron spikes history dependence conductance not. thus memory model inﬁnite. however thanks exponential decay synaptic response memory dependence decays exponentially fast ensuring exponential continuity transition probabilities necessary ensure existence uniqueness chain unbounded memory best knowledge literature linear response integrate-and-fire neurons relies mean-ﬁeld assumptions previous spikes averaged over. here contrast average previous spikes condition them. nevertheless mean-ﬁeld approximation used well developed section model allows approximate limit small family transition probabilities network model. show model presented here conditionally independent i.e. factorizes neurons spike history ﬁxed equation terms. ﬁrst corresponds requires assumed binning time small enough replace computation spiking condition becomes likewise second term correv sponds normalized potential model separated into spontaneous part time independent stimuli time and; perturbation part depending time-dependent stimuli non-zero time mathematically achieved adding extra term spontaneous potential time thus term refers average respect unperturbed system show variation explicitly written terms variation normalized potential induced introduction stimulus. note external stimuli switched time spike statistics still constrained previous spontaneous activity since transition probabilities memory. eﬀect especially salient model unbounded memory. however quantity becomes large absolute value consider terms expansion approach suﬃciently well function well known property function taylor expansion converges slowly near inﬁnity expansions eﬃcient here want consider eﬀect perturbation range function saturate. addition want restrict cases ﬁrst order taylor expansion suﬃcient characterize response. ensured conditions form limitation approach instead limitation linear response applied neuronal systems response neuron characterized saturating function. away linear part sigmoid nonlinear eﬀects dominate response. ﬁrst order variation normalized potential neurons submitted weak time dependent stimulus approximation contributions. integral includes eﬀect stimulus dynamics ﬂow; term given contains eﬀect network given terms note dependence synaptic weights non-linear non-linear. out. dots functions stand recall correlations correspond averaging functions spontaneous gibbs distribution thus expected linear response theory ﬁrst variation average reads series correlation functions computed respect spontaneous dynamics. discuss main parts equation decomposes neurons time-correlation functions. note correlations decay exponentially fast model allows hand truncate necessary. hand allows take limit corresponding start stimulation arbitrary past. case well write correlation involves time integral stimulus weight depending spike history conductance. integral weighted term integrating inﬂuence noise containing network contribution. thus linear response arbitrary observable include neurons function external stimulus depends also network connectivity spike history. therefore piecewise continuous function jumps integer values exponential decay integer values depending fractional part obviously time discretization spike trains. note fractional part term obtain explicit equation allowing make link between linear response model formulation terms correlations monomials. indeed limit characteristic time integration faster mean-interspike interval replace time integral average addition replace function negligible depends whole history even approximation synaptic term. markov approximation order simplify term further. truncate history dependence integer time steps past. truncation justiﬁed exponential correlation decay exposed section makes link formal expansion proposed section marginal past depth explicit value thus hammersley-cliﬀord decomposition obtain coeﬃcients δφl. however explicit tractable computation requires additional approximations done next section. random variable here space cardinality constrained likerandom variables type.we hammersley-cliﬀord transformation convert variables where contrast spikes occur time still degree monomial quite large number neurons corresponding spiking patterns neurons spike time nevertheless possible make monomial expansion functions truncating certain monomial degree. instantiates example terms entering general form linear response explicitly computed. sees clearly correlations between observable spike events hierarchy. term weighted coeﬃcients integrating eﬀect synaptic interactions history-dependent conductances. repeat term would obtain isolated neuron characteristic time τdk. note response neuron stimulus applied neuron depend synaptic weight general synaptic weights dynamics creates complex causality loops build response neuron procedure generalizes ﬁnite memory depth using write synaptic contribution terms spike events using hammersley-cliﬀord decomposition write function monomial expansion coeﬃcients explicitly computed. involve pre-synaptic spikes events linear response series correlation functions converges thanks spectral property. clearly terms series involve monomials increasing order whose probability expected decrease fast order increases. thus markovian approximation memory depth involving pairwise triplets might sufﬁcient characterize statistics model especially dealing ﬁnite samples obtained numerical simulations. remark deserves though thorough investigations subject future studies. object moves across visual ﬁeld generates transient spiking activity retina conveyed thalamus visual cortex trajectory object general quite complex moving constant speed involves long-range correlations space time. local information motion encoded retinal ganglion cells. decoders based ﬁring rates cells extract motion features lateral connectivity retina especially amacrine cells connecting bipolar cells plays central role motion processing addition ganglion cells directly connected electric synapses role lateral connectivity motion processing? clearly expect induce spatial temporal correlations spiking activity echo trace object trajectory. correlations cannot read variations ﬁring rate; cannot read well synchronous pairwise correlations propagation information lateral connectivity necessarily involves delays. example raises question information extracted spatio-temporal correlations network connected neurons submitted transient stimulus. eﬀect stimulus correlations? handle information data measure transient correlations? paper addressed ﬁrst questions theoretical setting using linear response theory probability distributions unbounded memory generalizing basic deﬁnition gibbs distribution statistical physics courses. goal settle general mathematical formalism allowing handle spike correlations result neuronal network activity response stimulus. salient result work variations observable average response external stimulus weak amplitude computed knowledge spontaneous correlations i.e. dynamics without stimulus. result surprising non-equilibrium statistical physics perspective however best knowledge ﬁrst time established spiking neuronal networks. novelty approach provides consistent treatment expected perturbation higher-order interactions going beyond known linear perturbation ﬁring rates instantaneous pairwise correlations; particular extends time-dependent correlations. addition wanted explicit linear response kernel terms parameters determining individual networks dynamics neurons connectivity. provided explicit example well-known class models integrate fire where case synaptic conductances depend spike history. makes explicit role neuronal network structure spiking response. show expected stimulus-response dynamics entangled complex manner. example response neuron stimulus applied neuron depend synaptic weight general synaptic weights dynamics creates complex causality loops build response neuron formally obtained linear response function terms parameters spiking neuronal network model spike history network. although linear treatment seem strong simpliﬁcation results suggest already case connectivity architecture neglected. presence stimuli whole architecture synaptic connectivity history dynamical properties networks playing role correlations perturbed potential. agrees well results recent study exhibiting exact analytical mapping neuronal network models maximum-entropy models showing that order accurately describe statistical behavior observable maximum entropy model synaptic weights needed even predict ﬁring rates single neurons does. moreover hammersley-cliﬀord decomposition allows obtain coeﬃcients weighting monomials terms parameters constraining dynamics. case model allowed show explicit dependence coeﬃcients terms synaptic weights. although basis monomials quite huge standard results ergodic theory transfer matrices/operators state neglect high order terms exponential correlation decay. decay rate controlled spectral matrix depending network parameters. setting cannot exclude situations spectral tiny leading slow correlations decay reminiscent second-order phase transitions beyond models also wanted characterize linear response biological neurons knowledge spontaneous activity. show raises question spontaneous correlations relevant. obviously natural start lowest orders nevertheless higher order terms also play signiﬁcant role spatial terms shown also temporal terms. indeed argued along paper neurons interactions involve delays integrated model attempting explain spike statistics note however contrarily usually believed detailed balance absolutely unnecessary properly handle time correlations also remark binning convenient remove short-range time-correlations analysis dramatically changes nature process investigation rendering non-markovian mean ﬁeld assumption. ﬁrst would like comment mean-field assumption allowed obtain explicit form response terms correlations network parameters. approximation holds particular condition generally neurons often silent active allowed simplify general equation replacing history dependent history independent one; replacing last ﬁring time moment idea working without i.e. handling depends spike trajectory. point seems quite hard handle even numerically idea deviation hypothesis could diagnosed. concerning opposite limit ﬁring rate quite faster characteristic time ﬂow. essentially means neurons ﬁring high rate thus reset fast dynamics time settle. didn’t consider case found uninteresting. general case random variable whose constrained stationary probability able handle case. paper focused analysis linear beyond linear response. response main goals establish mathematical setting allowing derive kernel convolution equation terms spontaneous spatio-temporal correlations. questions pending higher order terms? type volterra-like expansion adapted handle non-stationary collective spike response stimulus? concerning higher order terms appear places. first expansion easily access higher order terms expansion exponential. diﬃculty express terms terms correlations respect stationary probability. systematic expansion smooth dynamical systems done ruelle second higher order terms come taylor expansion potential expansion beyond formal results requires explicit knowledge potential issue. concerning exist indeed ways handling non-linearity spike response. typical case so-called linear-non-linear model generalized linear model linear response term corrected static non-linearity relation ln-glm form potential form studied actually potential static non-linearity parameters constraining model explicitly known. ln-glm characterize ﬁring rates infer correlations requires work. ln-glm deﬁne fact family transition probabilities possibly non-stationary which developed section construct gibbs distribution giving access spike statistics. stationary case handled perron-frobenius theorem don’t general handle non-stationary response except volterra expansion. monomial expansion. monomials expansion results plethora terms many irrelevant. mentioned several times text expansion advantage generic; drawback general principle tells monomials signiﬁcant. fact even worse. shown starting model potential like constrained parameters monomial expansion memory depth generically terms therefore related hidden non-linear relations. serious criticism maximum entropy related modeling approach except something unexpected happens biological neural networks somewhat pruning large part monomials. evidence this. point related another question method remove irrelevant monomials observation data? addition classical akaike bayesian information criterion occam’s factors investigated method based information geometry exponential measures like gibbs constitute suitable space probability measures fisher metric closely related pairwise spatio-temporal correlations. setting pruning methods monomials proposed eﬀective connectivity. network architecture together stimuli inﬂuences statistics spike correlations produced. recent years considerable eﬀorts dedicated construction detailed connection maps neurons multiple scales paper introduced general formalism allowing compute neuron responds excitation submitted another neuron deﬁning notion eﬀective connectivity based stimulus-response causality. follows analysis eﬀective connectivity depends stimulus already observed eﬀective connectivity deﬁned ising model depends also synaptic weights complex manner. observed existence resonances power spectrum generates stimuli dependent graphs e.g. controlled frequency harmonic stimulus. would interesting check whether property holds well. data. another possible application result comes fact correlations taken respect statistics spontaneous activity approximated experimental recordings neuronal network spiking absence external stimuli using maximum entropy principle. correlations monomials also computed data assuming neuronal tissue spikes recorded modeled values parameters model needed compute predict linear response. similar approach used quite interesting results. work extends interactions delays allowing future work make step answering important questions raised authors \"first eﬀective couplings depend stimulus? second extent inferred couplings aﬀected incomplete sampling activity temporal spatial points views? third couplings strongly depend model used inference?\" approach raises issues handling terms expansion using empirical correlations becoming noisy order monomials increases. note however spontaneous gibbs distribution pairwise interactions well approximated data shown", "year": "2017"}