{"title": "Characterizing optimal hierarchical policy inference on graphs via  non-equilibrium thermodynamics", "tag": "q-bio", "abstract": " Hierarchies are of fundamental interest in both stochastic optimal control and biological control due to their facilitation of a range of desirable computational traits in a control algorithm and the possibility that they may form a core principle of sensorimotor and cognitive control systems. However, a theoretically justified construction of state-space hierarchies over all spatial resolutions and their evolution through a policy inference process remains elusive. Here, a formalism for deriving such normative representations of discrete Markov decision processes is introduced in the context of graphs. The resulting hierarchies correspond to a hierarchical policy inference algorithm approximating a discrete gradient flow between state-space trajectory densities generated by the prior and optimal policies. ", "text": "hierarchies fundamental interest stochastic optimal control biological control facilitation range desirable computational traits control algorithm possibility form core principle sensorimotor cognitive control systems. however theoretically justiﬁed construction state-space hierarchies spatial resolutions evolution policy inference process remains elusive. here formalism deriving normative representations discrete markov decision processes introduced context graphs. resulting hierarchies correspond hierarchical policy inference algorithm approximating discrete gradient state-space trajectory densities generated prior optimal policies. stochastic policy inference discrete markov decision processes discrete states denoted reward associated state assumed actions deterministic transitions state-space thus summarized directed graph state trajectory arbitrary horizon objective sum-over-paths perspective policy state trajectories induced policy embedded linearly solvable subtracting policy description length penalty state trajectory allows objective re-formulated free energy functional incorporating policy entropy maximization objective entropy energy terms denoted respectively. policy functional encapsulates trade-off policy value policy complexity space state trajectories considering inverse temperature parameter limit recovers standard objective maximizing expected cumulative reward simplicity inverse temperature well-known trick formulae explicitly reﬂect balance control complexity penalty reward optimal transition policy computed calculation standard approximate inference methods applied however goal study compute transition policy gain insight hierarchical structure mdp. accomplished characterizing dynamics trajectory policy generated optimal policy inference algorithm transformed prior πprior optimal policy question addressed exploiting physical interpretation free energy functional energy functional stochastic system particles taking positions discrete space state trajectories initialized accord density πprior evolve time inﬂuence potential langevin dynamics associated potential cause particles attracted state trajectories higher rewards. trajectory policy reﬂects density particles trajectory space time unique thermal equilibrium system optimal policy given eqn. conceptually planning accomplished initializing particle system prior policy πprior allowing particles evolve according non-equilibrium thermodynamics described above. instead simulating stochastic dynamics particles evolution density studied. given fokker-planck equation optimal policy unique stationary solution doubles notationally policy deﬁnes trajectory density). furthermore equation following remarkable property time evolution according equation guaranteed reach optimal policy steepest descent free energy functional global minimum equal optimal policy particles remain equilibrium. implies equation gradient free energy therefore evolution πprior described equation generated optimal policy inference algorithm. since stochastic process considered level trajectories described hierarchical policy inference respect state transitions since sensitive dependencies states temporal horizons. next section optimal probability density trajectories re-expressed function states summing total state planning timepoint returns dynamic state hierarchy reﬂecting magnitude policy state optimal policy ﬂow. applying equation context variational optimization requires temporal discretization here spatial discretization also required since state-spaces considered discrete. spatially discretized form fokker-planck equation constructed based markov chain approximation underlying langevin dynamics retains properties continuous case current trajectory policy deﬁne generalized potential markov stochastic matrix trajectories generated current policy laplacian kernel. since trajectories conditionally independent policy density increases inﬂow policy particles attracted lower energy state associated generalized potential policy along velocity ﬁeld deﬁned driven ﬂuxes pulls policies towards action sequences large expected rewards pushes policy away policies large description lengths. summing trajectories total although total across trajectories zero total associated particular state original state-space reveals state-space hierarchy. section formulae recorded total values reﬂect state’s rank optimal hierarchy pair trajectories initial state fundamental matrix markov chain generated policy ultimately discrete fokker-planck equation re-expressed geometrically using discrete laplacian operator deﬁned role discrete laplacian operator intuitive physical meaning. generalized potential independent across states analogous dependence example gravitational potential across distinct positions real world. typical operation mathematical physics identify independent sources generate physical ﬁeld accomplished taking divergence ﬁeld equivalent laplacian ﬁeld’s potential. considering local energy potential generating planning complexity ﬁeld suggests equation interpreted dynamics solving planning problem targeting independent sources generate planning complexity. simple expository application regular graph previously used behavioral experimentation considered. fig. node represents state edge available transition. regular graphs provide challenging scenario elucidating hierarchy homogeneous local structure despite this obvious bottleneck rooms fig. participants tasked solving shortest-path problems uniformly drawn states. fig. darker node color reﬂects higher ∆djπ thus observe bottleneck recovered. participants also asked identify bottleneck states state-space despite never observed global structure. hierarchical state ranking measure ∆djπ approximately matched full distribution bottlenecks chosen explicitly subjects previously predicted model. implication analysis presented optimal planning process prioritize identiﬁcation transitions hierarchical order measured ∆djπ reﬂects steepest descent trajectories. information-theoretic perspective transition bottleneck state communicates amount information regarding shortest path states average second highest ranked states give next highest amount information prediction tested simulating oracle planner simply identiﬁes optimal transition given state shortest path reveals state shortest path. procedure iteratively reﬁnes candidate trajectories shortest path identiﬁed. entropy remaining candidate trajectories measures expected amount information remaining speciﬁed planning algorithm. plotted function number oracle samples fig. b-e. fig. trajectory entropy plotted optimal oracle sampling order fig. average shortest path length presented. expected random oracle sampling order outperforms optimal hierarchical sampling order average across tasks preliminary study discrete gradient context hierarchical policy inference presented. amongst possible policy inference processes equation describes expected time evolution take steepest descent free energy objective thus establishing principle optimality. considering policy inference trajectory space implies state-space structure scales integrated policy inference process. resulting policy provides normative perspective optimal hierarchical policy inference however remains seen whether principles established usefully embedded within practical algorithms. basic example provided establishes link hierarchical representations human cognition thus potentially fruitful research direction examine whether aspects behavioral neural dynamics natural planning problem-solving explained within framework. konrad rawlik marc toussaint sethu vijayakumar. stochastic optimal control reinforcement learning approximate inference. robotics science systems evangelos theodorou krishnamurthy dvijotham todorov. information theoretic dualities path integral kullback leibler control continuous discrete time formulations. wardetzky saurabh mathur felix kaelberer eitan grinspun. discrete laplace operators free lunch. eurographics symposium geometry processing pages equation describes evolution random vector drift diffusion diffusion coefﬁcient scales wiener process describes accumulation gaussian noise general n-dimensional random vectors -dimensional matrix m-dimensional vector. fokker-planck equation describes time-evolution probability density random vector special case drift conservative time-homogeneous vector ﬁeld corresponds langevin dynamics. vector ﬁeld generated scalar potential −∇ψ). assuming white noise diffusion temperature results stochastic dynamics energy entropy terms respectively. free energy lyapunov function dynamics conversely remarkably equation gradient free energy functional speciﬁcally equation deﬁnes trajectory steepest descent free energy respect -wasserstein metric space densities probability densities whose marginals equal respectively. although derivations work rely deﬁnition wasserstein metric interesting point appearance metric juncture related theoretical foundations optimal transport theory here primary interest lies discrete markov decision problems thus translation continuous theory discrete domain reviewed discrete state-space states density state-space discrete distribution discrete potential deﬁned mapping states real numbers discrete form fokker-planck equation based markov chain approximation langevin stochastic dynamics retains properties continuous case results derived used derivations main paper. absorbing markov chain stochastic matrix considered. function state trajectories chain induced probability distribution trajectories. assume expressed states which expected simply expected number times chain state starting state multiplied probability transitioning similarly letting state identity function implies returns entry fundamental matrix equals expected number times state encountered applying discrete fokker-planck dynamics requires develop analogous arguments pairs trajectories. linear states trajectories contribution speciﬁc state pair weighted multiplicity state combination trajectories according distribution ππ). analogous previous arguments equivalent taking expectation respect probability generating trajectory according appearing arbitrary timesteps", "year": "2017"}