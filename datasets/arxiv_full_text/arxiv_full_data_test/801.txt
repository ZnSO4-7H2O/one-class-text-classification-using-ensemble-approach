{"title": "Solving constraint-satisfaction problems with distributed  neocortical-like neuronal networks", "tag": "q-bio", "abstract": " Finding actions that satisfy the constraints imposed by both external inputs and internal representations is central to decision making. We demonstrate that some important classes of constraint satisfaction problems (CSPs) can be solved by networks composed of homogeneous cooperative-competitive modules that have connectivity similar to motifs observed in the superficial layers of neocortex. The winner-take-all modules are sparsely coupled by programming neurons that embed the constraints onto the otherwise homogeneous modular computational substrate. We show rules that embed any instance of the CSPs planar four-color graph coloring, maximum independent set, and Sudoku on this substrate, and provide mathematical proofs that guarantee these graph coloring problems will convergence to a solution. The network is composed of non-saturating linear threshold neurons. Their lack of right saturation allows the overall network to explore the problem space driven through the unstable dynamics generated by recurrent excitation. The direction of exploration is steered by the constraint neurons. While many problems can be solved using only linear inhibitory constraints, network performance on hard problems benefits significantly when these negative constraints are implemented by non-linear multiplicative inhibition. Overall, our results demonstrate the importance of instability rather than stability in network computation, and also offer insight into the computational role of dual inhibitory mechanisms in neural circuits. ", "text": "computation neural systems division biology biological engineering california institute technology pasadena cedars-sinai medical center departments neurosurgery neurology biomedical sciences angeles nonlinear systems laboratory department mechanical engineering department brain cognitive sciences massachusetts institute technology cambridge institute neuroinformatics university zurich zurich switzerland finding actions satisfy constraints imposed external inputs internal representations central decision making. demonstrate important classes constraint satisfaction problems solved networks composed homogeneous cooperativecompetitive modules connectivity similar motifs observed superﬁcial layers neocortex. winner-take-all modules sparsely coupled programming neurons embed constraints onto otherwise homogeneous modular computational substrate. show rules embed instance csps planar four-color graph coloring maximum independent sudoku substrate provide mathematical proofs guarantee graph coloring problems convergence solution. network composed non-saturating linear threshold neurons. lack right saturation allows overall network explore problem space driven unstable dynamics generated recurrent excitation. direction exploration steered constraint neurons. many problems solved using linear inhibitory constraints network performance hard problems beneﬁts signiﬁcantly negative constraints implemented non-linear multiplicative inhibition. overall results demonstrate importance instability rather stability network computation also oﬀer insight computational role dual inhibitory mechanisms neural circuits. ability integrate data many sources make good decisions action essential perception cognition well industrial problems scheduling routing. process integration decision often cast constraint satisfaction problem technological systems csps solved algorithms implement strategies backtracking constraint propagation linear optimization. contrast algorithmic methods view algorithm computational deﬁned sequence discrete operations carried data computer achieve desired end. contrast view neural networks modeled approximations physical neuronal networks cerebral cortex processing sequential algorithmic control. primary interest paper understand natural networks process csp-like problems. explore behavior networks context reference classes well understood algorithmic point view -coloring planar graphs minimum independent game sudoku classes network must decide suitable color assignment node graph given total number available colors choose graph-coloring problems topologies lend implementation networks constraints expressed simple equal not-equal relations. moreover four-coloring planar graphs interesting problem computationally hard solution applied practical tasks decision making cognition hand sudoku interesting involves many constraints dense non-planar graph also hard input constraints values nodes makes solution signiﬁcantly diﬃcult simple graph coloring valid coloring acceptable. show neuronal networks solve arbitrary instances three problem classes. many problems decision making reduced classes showing networks solve them implies principle solve problems reducible reference classes. problems course solved also algorithmical methods however important contribution explain principles dynamics allow network distributed interacting neurons achieve eﬀect without relying centralized sequential control inherent well known algorithms. modules behaves special kind variable initially able entertain simultaneously many candidate values eventually selects single best value according constraints imposed processing evolving related variables. collective selection behavior driven signal gain developed recurrent excitatory connections neurons participating circuits described below. circuits appealing network modules pattern connectivity resembles dominant recurrent excitatory general inhibitory feedback connection motif measured physiologically anatomically superﬁcial layers mammalian neocortex. also substantial growing evidence circuit gain modulation circuits neocortex ﬁnding consistent recurrent connectivity required wtas convergence however recently described crucial computational implications unstable expansion dynamics inherent circuits instabilities drive selection processing therefore computational process network performs. explored computational instability networks linear thresholded neurons unbounded positive output. consequently networks ltns recurrent excitation must rely feedback inhibition rather output saturation achieve stability. also means networks ltns change mode operation unstable stable principles develop paper depend concepts described previously apply contraction theory understand dynamics collections coupled wtas convenience ﬁrst summarize brieﬂy relevant points work. first note contraction theory applicable dynamics discontinuous derivatives introduced activation function utilize. switched systems dynamics remain continuous function state suitable parameter regimes networks enter unstable subspaces dynamics. expansion within unstable subspace active neurons steered constrained negative divergence dynamics. ensures current expanding sub-network soon switch diﬀerent sub-network diﬀerent subspace dynamics space might stable unstable. unstable network switch stable solution space found. refer unstable spaces ’forbidden’ spaces network quickly exit them. exit guaranteed dynamics forbidden spaces eigenvectors associated dominant eigenvalue system jacobian always mixed. means activity least neuron falling toward threshold soon pass changing space network dynamics proof.) stable spaces hand said ’permitted’. spaces eigenvalues negative network converge toward steady state space. critically negative divergence ensures dynamics space entered next lower dimensionality previous space regardless whether stable unstable. thus sequence transitions sub-spaces causes network compute progressively better feasible solutions. crucial feature process direction expansion determined automatically eigenvectors jacobian currently active neurons network. thus direction expansion change according particular neurons active given forbidden subspace. sense network able actively asynchronously systematically search forbidden spaces suitable solution subspace encountered. process constitutes network computation extend concepts show utilized construct networks solve certain classes constraint satisfaction problems. show using mathematical proofs simulations problems embedded systematically networks modules coupled negative positive constraints. overall approach ensure dynamical spaces constraint violated ’forbidden’. achieved adding additional neurons enforce necessary constraints. importantly analytical proofs guarantee networks complete correct solution also form inhibitory mechanism used implement negative constraints aﬀects performance network. diﬀerent types inhibition used implement negative constraints linear subtractive non-linear multiplicative inhibition. problem classes could solved using standard subtractive inhibition modules found solution diﬃcult problems greatly facilitated using multiplicative non-linear inhibition instead. recent experimental observations implicated kinds inhibition diﬀerent modes cortical computation work presented oﬀers theoretical foundation computational roles types inhibition also neuronal circuits neocortex. consists variables assigned discrete continuous values constraints variables. constraints unary binary higher order. constraint establishes allowable combinations values variables state problem assignment values variables assignment complete partial. csps instantiated neuronal networks embedding speciﬁc problem ﬁeld identical modules. modules standard connection pattern recurrent excitation inhibition supports functionality. embedded network coupling modules neurons implement negative positive constraints. show below performance network aﬀected form negative constraint inhibition onto wtas; either linear nonlinear. begin describing ’standard’ related networks make linear inhibitory negative constraints; thereafter describe extended networks make non-linear inhibitory negative constraints. standard consists point neurons excitatory remaining inhibitory examples express single active unit thus excitatory neurons xi=n receive self-feedback excitation neuron receive also external input normally distributed noise mean standard deviation added external inputs figure connectivity network architecture. single comprising excitatory neurons encode possible winners connections. bottom simpliﬁed notation. modules implement shown connected another additional inhibitory cells cells enforce negative constraint wtas cannot reach winner constraint inhibition linear. maintain analogy neurons inhibition shown applied dendrite. however point model neuron could well applied directly soma. simulation constraint problem. non-linear inhibitory synapse provides on-path inhibition suppress dendritic somatic excitatory inputs. example circuit negative positive constraint cells implemented non-linear inhibitory synapses. nodes graph represent possible states problem location edges graph represent constraints acting nodes. node module patterns activation excitatory neurons represent allowable states node. since winner permitted wtas represent many solution states constraints wtas implemented additional neurons additional excitatory inhibitory interactions relevant states interacting nodes. graph-coloring problems constraints negative selection particular color neuron node suppresses selection corresponding color neuron neighboring node. however problems require also positive constraints. example state also state maximal independent problem considered requires positive constraints. section ﬁrst describe dynamics connectivity negative-and positive contraint cells whereas speciﬁc wiring implement particular described later separately class considered. negative constraints competition states diﬀerent wtas introduced inhibitory feedback negative constraint cells implementation provides inhibitory output onto excitatory cells receives input contrast inhibitory cells enforce competition states within cells enforce competition speciﬁc state neurons across multiple participating wtas. positive constraints hints implemented excitatory positive constraint cells receives input speciﬁc excitatory cell provides excitation onto speciﬁc excitatory neurons wtas. total number negative positive constraint cells provide input cell respectively. strength inhibitory synapse made negative constraint cell onto cell strength excitatory synapse made positive hint cell onto cell consider choose parameters network analyze stability convergence consider example network shown network implements negative constraints modules possible solutions cells enforce same constraint wtas state. dynamics circuit converge steady-state solutions wtas depend addition local constraints example unit wtas receive largest inputs independently winner either would however constraint dependency express whereas winner node despite receiving lower amplitude input. arguments rely concept eﬀective jacobian expresses dynamics currently active subset neurons consider network form here matrix weights describes connections neurons non-linearity diag diagonal matrix containing dissipative leak terms neuron. eﬀective jacobian system diag diagonal matrix derivatives activation function evaluated current state neuron. case neurons ltns resulting derivatives equal either depending whether state neuron threshold. activation function slope whenever threshold. therefore remains long neuron crosses threshold. consequently eﬀective connectivity network changes whenever neuron crosses threshold. ’eﬀective connectivity’ indicates connections arise inactive neurons cannot inﬂuence target neurons provide output. therefore rows jacobian corresponding silent neurons zeroed out. however columns zeroed silent neurons still receive process input. mathematical tool assess describe network computation. used previously show single circuit permitted forbidden sets active neurons existence forbidden sets provides computational power forbidden sets highly figure enforcing constraints wtas using negative constraint cells. simple example using two-node circuit circuit diagram network nodes winners each. negative constraint cells enforce not-same constraint. weight matrix full network. gray boxes mark nodes connections outside boxes correspond ncs. inputs network. activity nodes activity negative constraint cells unstable drives network enter diﬀerent currently expressing. unstable negative feedback ensures dynamics forbidden states steers network activity towards suitable solution. forbidden must satisfy conditions divergence must negative eﬀective jacobian must positive deﬁnite networks conditions enforced shared inhibition excitatory self-recurrence respectively. together guarantee individual exit forbidden sets exponentially fast. arguments summarized results derived rest contraction theory powerful analytical tool allows systematically reason systematically stability instability non-linear networks networks use. contraction theory applicable non-linear networks switched networks provided dynamics remain continuous function state contraction metric remains thus applicable networks composed units ltns activation functions whose derivatives discontinuous. case consider following network max. note describes dynamics network continuous function despite discontinuous derivatives respect furthermore metric networks remains throughout processing thus conditions application contraction theory satisﬁed. present proofs together provide important insights operation network. first prove adding cells creates forbidden sets. thereby able inﬂuence dynamics network computation. second prove adding cells creates permitted sets. third prove networks wtas connected nc/pc cells remain stable despite presence forbidden sets. together results generalize previous ﬁndings individual wtas networks wtas coupled implement speciﬁc constraints. finally provide rules allow instances three classes csps implemented networks wtas installing suitable coupling connections. thus combinations circuits negative divergence always negative divergence. next additional inhibitory neuron enforces additional competition sub-circuits constraint create forbidden subspace must expanding. system described conclusion additional feedback loop introduced adding negative constraint cell creates forbidden subspace negatively divergent well expanding. method applied recursively arbitrary numbers inhibitory feedback loops collection circuits. positive constraints create additional forbidden sets. instead positive constraint kind state state reinforces sets already permitted. thus required regard positive constraints proof demonstrating permitted sets remain permitted preserved variety combinations subsystems importantly property includes kinds combination consider here introduction negative positive constraint cells. following simply outline grounds inclusion. detailed proof preservation contraction combination subsystems presented stability computational power depends following parameters excitatory local recurrence inhibitory recurrence inhibitory excitatory recurrence wtas implements constraints. provided parameters values within permitted range network allow winner emerge solution depends pattern input constraints equations derived equation derived above. note constraints inhibitory feedback loops established inhibitory neurons apply local inhibitory neuron well additional inhibitory neurons establish inhibitory feedback wtas simulations chose parameters within permitted ranges given equations within restrictions parameters chosen optimize performance problem class network type note parameters used identical instances particular problem class network type optimized particular problem instance next applied architecture problem graph coloring. here node must time express ﬁxed number diﬀerent colors. selected color represents node’s current state. coloring constraint nodes share edge forbidden color. finding assignment colors graph nodes respects constraint edges solves graph coloring problem smaller number permitted colors harder problem. speciﬁcally chose investigate problem coloring planar graphs colors arbitrary number nodes undirected edges planar graph embedded plane means drawn edges cross another. here restrict planar graphs guaranteed colorable colors. topology graph coloring naturally framed topologically distributed constraint satisfaction problem implemented networks circuits manner described above. color state node represented single network implementation requires many modules graph nodes. problems reported here smallest number colors required color graph constant given. thus many state neurons chromatic number. local competition states unbiased wtas internal connection architecture. edges graph constraints kind same implemented using cells most constraints across edge require many diﬀerent cells node colors. however single cell enforce constraint across arbitrary numbers neighbor wtas. thus suﬃcient cell color given node. cell able assert constraint across edges connected node figure solving graph coloring problems networks wtas. example -node graph possible color solution computed network indicated weight matrix module network implementing graph shown neurons conﬁgured separate wtas four excitatory neurons encode possible colors node global inhibitory neuron. neurons impose inhibitory constraint edge nodes color. dynamics network leading solution note relatively small modulations constraint neuron activity required achieve solution. shown previously symmetrical asymmetrical networks fundamental operation modules active selection process whereby activities neurons driven threshold receiving support either local remote excitatory input. partitions active neurons inconsistent stability forbidden. partition left exponentially quickly unstably high gain generated neurons forbidden partitions drive recurrent inhibition suﬃciently strongly soon drive neuron beneath threshold bring partition being. process continues consistent permitted partition found. previous work concerned relationship inhibitory feedback driven excitatory members local existence forbidden sets. demonstrated existence forbidden subspaces provides computational power networks negative constraints provide additional source negative feedback routed remote wtas tested performance network solving randomly generated planar graphs four color nodes cases constraints acceptable color speciﬁc node. thus solution satisﬁes constraints acceptable. consequently many equally valid solutions graph goodness solution measured metrics average time network took converge number edges satisﬁed function time. found networks solve graphs nodes correctly within however larger graphs incompletely solved take longer computational process evolves number errors decreases exponentially fast means networks given size solve majority random graphs quickly minority taking much longer. results heavy-tailed distributions respect figure performance solving planar graph four coloring problem example solution. weight matrix network units performance cumulative probability network convergence function processing time network size. average number errors function time. performance notation average time ±s.e. solution function network size architecture. time solution large problems signiﬁcantly shorter network comparison distribution times solution function network size. scaling time solved networks converged function network size. parameters noise parameters same except section network parameters chosen. results simulations network size. random planar graph density generated simulation. positive constraint cells demonstrated solving second class graph coloring problems maximal independent sets problems second fundamental class computational problems solvable type network present here. related graph coloring requires diﬀerent constraints. case node must take possible colors nodes connected cannot further node takes color must connected least node color problem ﬁnds practical application many distributed algorithms used automatic selection local leaders. translated problems networks negative positive constraints. ﬁrst constraint neighbors color implemented cell connected pair second constraint node color encourages neighbors color implemented positive feedback hint cells pair connected nodes positive feedback active conditional node figure performance solving maximal independent problem. example maximal independent node graph. node connected nodes green node connected least node. connectivity simple two-node problem. node possible winners enforces notes red. enforces node green red. weight matrix -node graph illustrated dashed indicates connection submatrix wtas remaining entries indicate constraint units connections. performance random problems diﬀerent size graphs randomly generated planar graphs density. converges quickly problem sizes. performance comparison except smallest problem converged signiﬁcantly quickly parameters noise parameters same except color thus node color positive constraint inactive. found network solves manner speed similar described graph coloring networks solve problems large size fairly quickly however small number large problems remain unsolved even long times. standard networks also able solve non-planar graph coloring problems color states many nodes ﬁxed assignment. initial assignment constraints make graph coloring signiﬁcantly diﬃcult case valid solution acceptable. canonical example problem class popular game sudoku. constraints problem color appear column box. neural network implements sudoku consists units. those units implement nodes implement constraints addition initial constraints form speciﬁed inputs subset nodes describe speciﬁc problem solved sudoku network excitatory cell receives constant noisy excitatory input network. input stands broader network context particular network embedded. addition excitatory cell receives several negative constraint inputs. positive hint constraints. forward inputs bias enforce ﬁxed color assignments nodes required speciﬁc problem solved. unbiased neurons receive bias found standard networks able solve many sudoku problems. performance poor. average converge time network able solve networks maximal time permitted. case graph coloring number violated constraints decreases exponentially function simulation time sought improve fraction correct solutions rate reduction errors modifying network conﬁguration. reasoned exponential form network convergence reﬂects exploration combinatorial solution space; convergence would rapid network could made sensitive constraints. therefore negative constraint rather discouraging selection particular state could prevent state ever selected subtractive inhibition negative constraints cells suﬃciently strong cell driven activation threshold become insensitive positive inputs. hypothesized that eﬀects constraints scaled multiplicatively rather applied subtractively equation network would able separate function state selection node constraints biased selection change architecture might promote rapid convergence. non-speciﬁc contextual background excitation number negative constraint cells synapsing cell number positive constraint cells synapsing cell number contextual inputs function inverse sigmoid-type non-linearity form figure sudoku graph coloring problem solved example hard identical hard example used values given. circuit implementation sud. node possible winners column constrains enforced negative constraint cells). pre-deﬁned winners enforced bias currents soma. weight matrix network implements sud. network consists units. those units implement nodes implement constraints performance column constraints color; i.e. network sudoku shown runs network diﬀerent initial conditions. required average converge maximal duration number violated constraints decreases exponentially function simulation time simulations shown simulations diﬀerent sudoku problems varying diﬃculty average convergence time respectively. parameters contextual inputs s.d. parameters identical except extended version inhibitory constraints enter argument non-linear function scales eﬀect network excitatory sources thus excitatory unit wtae receives kinds excitatory input forward input bias constraint input cons always positive. contrast standard input also negative. eﬀect negative constraints never overwrite forward input. tested performance extended networks constraint satisfaction tasks. unlike standard networks networks solve problems presented. graph coloring architecture converged signiﬁcantly faster reduced number errors rapidly diﬀerence became apparent larger problem size. example networks nodes converged average contrast problems required average greatest advantage network problems. solved sudoku problems quickly convergence average hard problem shown diﬀerent sudoku standard problems varying diﬃculty contrast network able solve instances hard problem diﬀerent problems time solved problems. note solution times heavy tailed distribution even small minority runs take much longer. example whereas mean convergence diﬀerent problems problems required results indicate large distributed constraint satisfaction problems processed computational substrate composed many stereotypically connected neuronal modules together smaller number speciﬁc ’programming’ neurons interconnect wtas thereby encode constraints particular problem solved. architecture networks consistent strong recurrent excitatory recurrent inhibitory connection motifs observed physiological anatomical connections neurons superﬁcial layers mammalian neocortex therefore results relevant understanding biological circuits might operate. however consider problems solved actual neuronal cortical circuits. instead choose canonical examples properties applications well understood computational literature graph-coloring csps intriguing computational problems structure requires simultaneous distributed satisfaction constraints. however practice solved sequential localized algorithms example csps solved exhaustive search candidate solutions systematically generated tested validity. however approach scale well problem size node graph strategy would require approximately conﬁgurations tested respectively. various heuristic algorithms improve performance beyond obtainable exhaustive search. contrast neural network present solves csps eﬃciently without relying domain-speciﬁc heuristics. however performance would property physically realized network algorithmic simulation model network obliged here. moment estimates network performance terms model time steps stand proxy physical performance measurements. previous approaches solving csps using artiﬁcial neural networks relied saturating neurons maintain global stability neglected important role instability. output saturation observed biological networks neurons typically operate well beneath maximum discharge rate. computational implications well-recognized fact recently received little attention. example becoming clear non-saturating ’relu’ activation functions advantageous deep learning networks show novel principles network computation depend non-saturating activation. case stability relies shared inhibition allows transient periods highly unstable dynamics. kind instability exist networks utilize saturating neurons majority neurons either positive negative saturation. networks non-saturating ltns derivative activation function appears supra-threshold neurons hence currently active neurons contribute expansion contraction network dynamics. neurons saturation course also active derivative saturated activation function near zero contribute little nothing expansion contraction network dynamics. analyzing properties thus powerful tool understand networks switch diﬀerent states autonomously driven dynamics unstable parts dynamics. tool analogous energy function used work pioneered analysis provided great insight saturating networks compute. approach consists rules allows systematic ’programming’ biologically plausible networks. thus able program desired computational processes onto uniform substrate scalable manner approach technological beneﬁts conﬁguring large scale neuromorphic hardware truenorth dynap chip instantiate physical network rather simulating here. deal continously-valued networks shown types wta-networks also implemented using spiking neurons search forbidden sets likely fundamental spontaneous computation types networks. addition work provides insight analogous hardware engineered. practical implications contrast general theoretical frameworks often lack circuit-level implementation cannot make predictions necessary computational roles cell types here. note also computational properties networks describe preserved regardless network size. aspects network rely simple computational motif replicated many times needed particular problem without make modiﬁcations depend network size. scalability contrast attractorbased computational approaches shown solve small problems cannot easily generalized larger ones fundamental operation network involves simultaneous interactive selection values across modules. selection process drives activities neurons threshold using signal gain developed neurons receive support either local remote excitatory input. dynamics network forbid partitions active neurons inconsistent network stability. partitions left exponentially quickly unstably high gain generated neurons active forbidden partition increase recurrent inhibition least active driven beneath activation threshold consequence this partition lower divergence entered. next partition forbidden exited; might permitted stable. exploiting instability manner thought taking path least resistance state space exploration state space continues consistent permitted partition found absence noise external inputs transition states results reduction divergence. reduction implies network cannot return previously occupied forbidden state introduces form memory network prevents cycling. presence noise cycling becomes theoretically possible unlikely solving csps sequentially exploring diﬀerent network subspaces interesting similarities algorithmic linear optimization methods particular simplex related methods critical step simplex pivot algorithmic manipulation improves subset problem variable maximized holding others constant. involves decision followed change basis maximization along newly chosen dimensions. process similar process whereby network transitions forbidden sets relaxation labeling networks saturating units). driven exponentially shrinking volumes implied negative divergence unstable dynamics rapidly cause switch diﬀerent state lower dimensionality state space. direction expansion proceeds described subset eigenvectors eﬀective jacobian positive eigenvalues network step similar maximization chosen variable simplex. furthermore whenever network switches forbidden another network performs ‘pivot’ changing basis functions among dynamics evolve. contrast mechanism network implements search principles fundamentally diﬀerent linear programming similar approaches including generalization csps based polymatrix games based lemke’s algorithm firstly network performs steps fully asynchronously autonomously every module. secondly network require access global cost function require access current values variables depend external controller decide suitable pivots. instead network moves along best directions search proceeds wtas parallel. constraint connections aﬀect mutual selection process coupled variables. constraints implemented directed weighted connections provide immediate distributed update appropriate values aﬀected variables. within subspace network behaves piecewise linear system computationally powerful partial derivatives system update many interacting variables simultaneously consistently described eﬀective jacobian importantly updates possible mixtures values evolving variable rather replacement discrete value another. eventually variable enhance signal candidate value suppressing competitors. process radically diﬀerent back-tracking constraint-propagation method implemented digital algorithms procedurally generate test consequences alternative discrete value assignments particular variables. instead network approaches solution successive approximation candidate quality unlikely compute towards false assignment optimization process follows computational dynamics network computational trajectory follows successively less unfavorable forbidden subspaces permitted subspace entered. important distinction algorithmic network rests assignment initial candidate conﬁguration. algorithmic approaches begin candidate conﬁguration every variable initialized legal value. contrast initial network assignment eﬀectively null neurons zero. however values soon aﬀected stochastic context signals applied almost immediately amplitude mixture variables across network network dynamics bootstrap better estimates mixtures constraints network ﬁnally converges towards complete consistent assignment. network oﬀers novel approach dynamic balance candidate generation validation progressive reﬁnement mixture values variable. certain degree noise essential operation networks. stochasticity bias enters network process inputs embedding network. contextual excitatory inputs moment introduce randomness biases enable network gain access otherwise computationally inaccessible solution subspace provide degree innovation search process. also inputs modulated multiplicative inhibition. realistic scenario would replaced input parts brain specify priors. network would responsive constraints parts network sensory input internal states. figure distribution times inﬂuence initial conditions. probability simulation correct solution certain amount simulation time. simulation runs random graphs nodes parameters majority simulations solution within data well log-normal distributions characteristic heavy-tailed phenomena assessment using log-log plot. robustness y-axis cumulative rather frequency. tail observed data falls theoretical distributions indicating tail heavier expected log-normal less heavy expected generalized extreme value. identical node graph diﬀerent initial conditions random initial conditions partially informative iii) partially uninformative candidates repetitions runs using random seed initial state conﬁrm non-deterministic processing nevertheless gives rise distinct distributions solution times easier harder problems case prior information feedforward inputs bias implement required values variables. larger number biases harder problem. however hard problem biases given game sudoku priori known compatible solution therefore network expected ﬁnally complete solution. however general coloring problems input biases desirable known compatible complete assignment. cases network approximate incomplete assignment network solves majority random graphs quickly minority taking much longer thus distribution times required networks solve heavytailed form distribution depend hardness problem heavy-tail persists even problem solved multiple times initial conditions indicating probabilistic processing allows network follow diﬀerent trajectories diﬀer substantially length alternative routes successive forbidden subspaces hand network seeded initial conditions favor simple solutions median processing time shorter seed biased towards invalid solutions non-linearity improved performance diﬃcult problems global well local constraints sud. multiple types inhibition prominent feature nervous systems particular neocortex non-linear linear mechanisms associated gabaa chloride- gabab potassium-mediated inhibition respectively. actions separable intracellular recordings vivo eﬀects processing mixed inhibition mediated distinct types neuron encountered superﬁcial layers neocortex large group basket inhibitory neurons horizontally disposed axons target predominantly soma proximal dendritic segments pyramidal neurons. somatic bias synapses neurons make likely candidates implementing somatic selective mechanism. another large group bitufted cells double-bouquet cells vertically disposed axons target predominantly distal dendritic segments pyramidal neurons. neurons candidates non-linear cells model. although particular conductance mechanisms unknown non-linear inhibitory eﬀects considered play important role processing synaptic inputs dendrites pyramidal cells negative constraints implemented direct subtractive inhibition applied somatic compartment degrade local selection process falsely contributing inhibitory normalization overcame disadvantage introducing second dendritic compartment receives positive constraint contextual input whose output dendrite soma governed non-linearity somatic compartment receives standard inputs bias output dendritic compartment. thus addition local recurrence excitatory unit wtae receives kinds input direct somatic input bias dendrite. conﬁguration multiplicative inhibitory constraints quench various sources excitation received interfere directly local decision best supported advantage explains improved performance networks depicted figs dfi; def; def. non-linearity provides ’on-path’ multiplicative ’shunting’ inhibition previously described biological dendrites type inhibition veto excitatory input arriving dendritic branch much less inﬂuence excitatory inputs arrive branches soma important implications computation. firstly somata strongly activated little dendritic sensitivity strong feedback inhibition drives towards ignore dendritic excitatory input thereby reducing dimensionality problem. second dendritic sensitivity graded somatic activation varies neurons somatic activation sensitive remote excitatory input figure behavior dendritic non-linearity improves performance network solving heavily constrained problems sudoku separation processing dendritic somatic compartment non-linearity compartments receive excitation inhibition nearby neurons well external inputs. shape nonlinearity equal total inhibitory dendritic input dendritic branch histogram values across dendritic branches simulation sudoku correct solution found. note bimodality compartments making insensitive dendritic input. somatic inputs ibias strong. eﬀectively reduces dimensionality problem. simulation diﬀerent values ibias used results tri-modal distribution mode corresponding units non-zero weak ibias. units thus remain sensitive dendritic input much less units ibias mechanism provides weighting importance diﬀerent dimensions problem making easier solutions little evidence switch alternative solutions comparison evidence single dendritic compartment generalized multiple compartments governed nonlinearity thereby allowing localized inhibitory modulation speciﬁc excitatory input manner dendritic tree total dendritic input dendrite currents provided dendritic branches branches total. branch receives inhibitory inputs negative constraint cells well kinds excitatory inputs contextual inputs inputs positive constraint cells branch receives inputs respectively. shown large distributed constraint satisfaction problems processed computational substrate composed stereotypically connected neuronal modules smaller number speciﬁc ’programming’ neurons embody constraints particular problem solved. rules network construction accompanying mathematical proofs guarantee instance three types networks combination unstably high gain network noise drive search consistent assignment values problem variables. organization network imposes constraints evolving manifold system dynamics result computational trajectory network steered toward progressive satisfaction problem constraints. process takes advantage non-saturating nature individual neurons results eﬀective jacobian driven neurons currently threshold. search performance greatly improved mechanism value selection variable reduce sensitivity constraints according conﬁdence selection. achieved using subtractive inhibition selection modulating constraint inputs using multiplicative inhibition. arrangement allows constraint satisfaction network solve diﬃcult problems solve problems quickly. ﬁndings provide insight operation neuronal circuits neocortex fundamental patterns connection amongst superﬁcial neurons consistent networks described ﬁndings also relevant design construction hybrid analog digital neuromorphic processing systems provide general principles whereby physical computational substrate could engineered utilized. boost graph library matlab interface matlabbgl used graph-theoretical algorithms conﬁrmation graphs planar generation random graphs chrobak-payne straight line drawing etc. description network generated automatically based speciﬁes graph. format used jflap graph decomposed fully connected non-overlapping subgraphs cell added projecting particular pyramid cell normalized constant equal external input pyramid cells normally distributed i.i.d. noise", "year": "2018"}