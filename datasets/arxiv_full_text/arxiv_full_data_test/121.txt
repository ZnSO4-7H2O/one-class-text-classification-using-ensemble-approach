{"title": "CaMKII activation supports reward-based neural network optimization  through Hamiltonian sampling", "tag": "q-bio", "abstract": " Synaptic plasticity is implemented and controlled through over thousand different types of molecules in the postsynaptic density and presynaptic boutons that assume a staggering array of different states through phosporylation and other mechanisms. One of the most prominent molecule in the postsynaptic density is CaMKII, that is described in molecular biology as a \"memory molecule\" that can integrate through auto-phosporylation Ca-influx signals on a relatively large time scale of dozens of seconds. The functional impact of this memory mechanism is largely unknown. We show that the experimental data on the specific role of CaMKII activation in dopamine-gated spine consolidation suggest a general functional role in speeding up reward-guided search for network configurations that maximize reward expectation. Our theoretical analysis shows that stochastic search could in principle even attain optimal network configurations by emulating one of the most well-known nonlinear optimization methods, simulated annealing. But this optimization is usually impeded by slowness of stochastic search at a given temperature. We propose that CaMKII contributes a momentum term that substantially speeds up this search. In particular, it allows the network to overcome saddle points of the fitness function. The resulting improved stochastic policy search can be understood on a more abstract level as Hamiltonian sampling, which is known to be one of the most efficient stochastic search methods. ", "text": "synaptic plasticity implemented controlled thousand diﬀerent types molecules postsynaptic density presynaptic boutons assume staggering array diﬀerent states phosporylation mechanisms. prominent molecule postsynaptic density camkii described molecular biology memory molecule integrate auto-phosporylation ca-inﬂux signals relatively large time scale dozens seconds. functional impact memory mechanism largely unknown. show experimental data speciﬁc role camkii activation dopamine-gated spine consolidation suggest general functional role speeding reward-guided search network conﬁgurations maximize reward expectation. theoretical analysis shows stochastic search could principle even attain optimal network conﬁgurations emulating well-known nonlinear optimization methods simulated annealing. optimization usually impeded slowness stochastic search given temperature. propose camkii contributes momentum term substantially speeds search. particular allows network overcome saddle points ﬁtness function. resulting improved stochastic policy search understood abstract level hamiltonian sampling known eﬃcient stochastic search methods. calcium-calmodulin dependent protein kinase frequently occurring complex molecule postsynaptic density molecule implementation synaptic plasticity described molecular biology memory molecule creates somewhat persistent autophosphorylated state short term memory pass ﬁlter time scale dozens seconds calcium inﬂux fig. fig. calcium inﬂux typical feature induction longterm plasticity nmda receptors. speciﬁcally incoming calcium transforms camkii calmodulin active state maintained autophosphorylation among subunits. furthermore camkii triggers activated state changes synaptic eﬃcacy phosphorylation ampa receptors anchoring additional ampa receptors postsynaptic density dopamine-gated stabilization spines although numerous experimental data show camkii activated state essential contribution network plasticity still unclear. address article question activation dynamics camkii could contribute rewardbased network optimization speciﬁc computational tasks. since molecular processes involve camkii give rise contain strong stochastic component natural view optimization deterministic stochastic search good network parameters. view also consistent numerous experimental data show synaptic connections even adult cortex subject continuous coming going dendritic spines appears inherently stochastic independent prepostsynaptic ﬁring absence functional synaptic connection theoretical framework stochastic network plasticity introduced termed synaptic sampling. there shown neural network parameters subject stochastic plasticity rules samples stationary distribution network conﬁgurations sampling process known langevin sampling machine learning literature. means network visit long network conﬁgurations often large probability index distribution denotes temperature search depends amount noise plasticity process. exact shape determined plasticity rule context reward-based learning chosen prefer network conﬁgurations lead frequently large rewards. words framework synaptic plasticity shown implement ongoing stochastic policy search however synaptic sampling carries langevin sampling convergence stationary distribution rather slow ﬁxed temperature general undesirable implies slow learning. particular search high ﬁtness regions gradient-based optimization techniques langevin sampling hindered local optima even severely recently suggested dauphin presence saddle points slowness generic impediment implementation global optimization strategy simulated annealing detailed below. article show camkii activation dynamics ease issues. compared synaptic sampling framework activation dynamics camkii gives rise additional dynamic variable basically low-pass ﬁlters parameter updates. low-pass ﬁltering implements momentum term method well-known improve gradient-based optimization many circumstances example vicinity saddle points. abstractly fig. camkii dynamics. camkii crystal structure units fold rings domains. incoming calcium/calmodulin transform camkii active state activate neighboring subunits autophosphorylation. leads persistent calcium-independent activation camkii time scale dozens seconds local concentration camkii activated state changes response diﬀerent input pulse. camkii activation jumps upward single calcium input pulse otherwise decays exponentially. poisson calcium input pulses camkii activation irregular frequency input almost steady high frequency input note time constant camkii here. stochastic framework show resulting dynamics gives rise parameter sampling algorithm known hamiltonian sampling however still samples stationary distribution well-known advantage hamiltonian sampling langevin sampling faster convergence stationary distribution faster convergence properties model camkii driven plasticity allows create link reward-based learning optimization theory establishes conditions neural circuit could attain functionally attractive locally optimal network conﬁgurations principle even global optimum. simulated annealing arguably powerful algorithmic approach nonlinear optimization. evolutionary algorithms also work well cases require large control overhead many competing networks parallel biological evidence exists far. show reward-based network plasticity principle reach even globally optimal network conﬁgurations amount stochasticity suﬃciently slowly decreased learning similar simulated annealing continuous time. theoretical result provides gold standard reward-based network learning. results consider network receives certain times reward signals e.g. form dopamine. dynamics synaptic connection network modeled parameter determines synaptic eﬃcacy. therefore assume behavior network determined parameter vector biological neuronal networks neurons either excitatory inhibitory fact commonly refered dale’s principle. implies outgoing synaptic connections exclusively excitatory inhibitory synaptic weights cannot change sign plasticity processes. ﬁrst introduce version model allows sign-switch synaptic weights demonstration purposes. later introduce slightly modiﬁed version model excitatory synapses plastic weights constrained non-negative. processes achieve network seeks network conﬁgurations provide large expected discounted reward. mathematically expected discounted reward given integral integrates future rewards discounting remote rewards exponentially discount rate expectation average multiple learning episodes episode realization reward trajectory encountered given parameters according distribution addition biological network needs satisfy structural constraints sparse connectivity formulated prior network conﬁgurations hence network learning regarded search policies satisfy structural constraints provide large expected discounted reward. stated formally sampling posterior distribution parameters network reaches unique stationary distribution given posterior describes inﬁnitesimal stochastic increments decrements wiener process standard model brownian motion dimension amplitude integrate role camkii plasticity processes model previously sketched transient role camkii pass ﬁlter induction synaptic plasticity potential synapse introduce another dynamic variable determines change time found both require activated form camkii switch determined mechanisms therefore interpret absolute value local concentration camkii activated state. interaction variables modeled stochastic diﬀerential equation form change parameter directly depends value hidden camkii-related variable dynamics turn determined three terms. ﬁrst term gradient parameter posterior. reward-based learning gradient estimated rule depends prepost-synaptic spike times global reward decay camkii activation time constant detailed experimental studies suggest time constant depends variety factors e.g. inactivation time constant camkii activity mobility camkii last term models noise camkii activation stochastic opening n-methyl-d-aspartate receptor channels words camkii-enriched dynamics gives rise reward-optimizing distribution network conﬁgurations direct dynamics considered importantly however turns dynamics actually posesses advantageous properties compared direct dynamics noise-less case dynamics corresponds gradient ascent comparison dynamics introduces momentum term well-known improve gradient descent many circumstances example presence small local optima vicinity saddle points. case noise dynamics corresponds langevin sampling dynamics hamiltonian sampling friction. knwon hamiltonian sampling typically shows much faster convergence stationary distribution rather slow langevin sampling fact similar low-pass ﬁltering gradient updates already implemented improve learning performance without clean mathematical background biological motivation. arrive concrete plasticity rules determine concrete neuron model prior hand. example used subsequent simulations consider stochastic spiking neuron model independent zero-mean gaussian priors variance parameter obtain derivative prior. using fig. spiking neuron learns emulate sigmoidal neuron. illustration network architecture. target ﬁring activity spiking neuron deﬁned output sigmoidal neuron four inputs pre-deﬁned weights. spiking neuron receives inputs pools spiking neurons each ﬁring rates proportional sigmoidal neurons’ inputs. distribution input pattern used learning input-output plane sigmoidal neuron output sigmoidal neuron ﬁring probability spiking neuron function weighted inputs learning hamiltonian dynamics. spiking neuron approximates smooth behavior sigmoidal neuron learning. comparison average rewards synaptic sampling without hamiltonian dynamics throughout learning yprei activation synapse fposti denotes ﬁring probability postsynaptic neuron zposti binary variable postsynaptic neuron spiked time zero else. synaptic plasticity rule acts related camkii activation instead acting directly synaptic parameter learning rule simple version reward-modulated spike-timing dependent plasticity similar rules derived previously context reward-based learning current work extends rules include prior network conﬁgurations stochastic parameter updates camkii-induced hamiltonian dynamics. learning recurrent networks spiking neurons notoriously hard particular reward-based learning. example interesting functionality acquired rewardbased synaptic plasticity recurrent networks smooth non-spiking neurons. recently proposed functionality non-spiking network ported spiking network previously learned exhibit smooth dynamics wondered whether smoothing network responses obtained reward-based learning. considered simple basic setup task reproduce single spiking neuron behavior artiﬁcial sigmoidal neuron model target ﬁring rate spiking neuron given output sigmoidal neuron four inputs pre-deﬁned weights. fig. shows desired input-output behavior. spiking neuron received inputs pools spiking neurons each ﬁring rates proportional sigmoidal neurons’ inputs input patterns presented spiking neuron continuously weights adapted reward-based plasticity presentation input pattern lasted presentation followed phase reward delivered given minus absolute diﬀerence spiking neuron sigmoidal neuron output reward delivery delay period introduced input neurons silent followed another pattern presentation. learning ﬁring rate spiking neuron rather random whole range inputs pattern presentations neurons’ ﬁring rate approximated smooth behavior sigmoidal neuron well fig. shows average reward throughout learning hamiltonian sampling comparison non-hamiltionian dynamics hamiltonian dynamics speeds learning signiﬁcantly. next investigated whether beneﬁt learning performance hamiltonian sampling scales biologically realistic network architectures larger size less structured. fig. hamiltonian synapse dynamics improves learning blind reaching task. illustration network architecture learning task. recurrent network inhibitory excitatory neurons input population aﬀerent neurons. arrows indicate symbolically connectivity excitatory neurons inhibitory neurons pool neurons used control position cursor space. aﬀerent input neurons provide indicates phase movement performed. reward delivered network cursor reaches goal location starting start location activity random subsets network neurons example cursor trajectories learning onset time hours learning. black horizontal bars indicate presentation pattern. vertical bars show reward windows successful trials. network responses cursor movements become stereotyped goal-directed throughout learning. comparison learning curves without hamiltonian synaptic dynamics. reward quantiﬁed mean fraction successful trials time point. hamiltonian dynamics included network learns task signiﬁcantly faster better. average results independent trials shown shaded area indicates std. applied hamiltonian synaptic sampling framework outlined learn blind reaching task simple model motor cortex. reward-guided changes network activity task-induced spine dynamics well documented motor cortex used network recurrently connected excitatory neurons inhibitory neurons control cursor space connectivity parameters cortical network motif taken addition recurrent connections random subset excitatory neurons received input aﬀerent neurons. remaining excitatory neurons randomly selected neural pool neurons control cursor position. controlling cursor adopted population vector model brieﬂy neuron assigned randomly selected preferred direction cursor space. time point cursor moved direction population vector neurons trial started cursor centered start area cursor held initiate movement phase trial. movement phase indicated presentation pattern reward given network cursor moved target area fig. held success presentation pattern stopped reward window initiated network failed reach target within seconds failed hold cursor trial aborted time window without reward presented. note nontrivial reinforcement learning task since neurons know whether belonged population also network receive feedback cursor position binary information trial phase provided. also true preferred directions assigned neurons could observed neurons. furthermore neurons receive input directly routing information learned reaching task. information discovered random exploration global sparse binary reward signal. used synaptic sampling framework without hamiltonian momentum term learn task. synaptic plasticity active excitatory synapses whereas inhibitory synapses ﬁxed. order guarantee synapses didn’t change role i.e. become inhibitory learning used model synaptic plasticity allow synaptic weights become negative. done applying mapping synaptic parameters synaptic eﬃcacies used exponential mapping oﬀset parameter positive value show methods inserting equation general hamiltonian learning framework arrive slightly modiﬁed version eligibility trace given fig. show hamiltonian momentum term rule signiﬁcantly enhances learning task. network responses learning hamiltonian momentum term shown fig. initially rewarded goal reached occasionally learning hours network able reach target trials fig. compare learning progress without hamiltonian sampling. found task hard learn without hamiltonian momentum term traditionally believed gradient-based non-convex optimization high-dimensional spaces hampered presence local optima ﬁtness landscape. recently dauphin argued high-dimensional spaces typically local optima local optima nearly good global optimum. importantly noted authors saddle points much numerous high-dimensional ﬁtness landscapes. hence stochastic procedures high dimensional spaces like synaptic sampling tend ineﬃcient time consuming presence saddle points much local optima. generally accepted method speed convergence learning sampling presence saddle points hamiltonian dynamics therefore hypothesized camkii-induced hamiltonian parameter dynamics provide beneﬁt respect. test hypothesis considered three-layer neural network input neurons hidden neurons output neurons. task learn classify images handwritten digits mnist dataset large computational demands task consider spiking network rather network consisting stochastic perceptrons pattern presentation digit chosen randomly mnist dataset presented input. binary reward delivered depending activity output neurons. output neuron corresponding target current example larger ﬁring probability output neurons reward delivered otherwise reward note eligibility trace used network obtained feedback immediately ﬁrst network non-hamiltonian synaptic sampling behavior network learning showed typical signs saddle points. particular test accuracy tended stuck plateau value slight increases longer periods. then point performance increased signiﬁcantly another plateau reached similar behavior observed hamiltonian dynamics however case network tended escape saddle points much faster test whether hamiltonian dynamics escape saddle points faster non-hamiltonian synaptic sampling considered parameter setting obtained synaptic sampling close putative saddle point fig. hamiltonian dynamics improves network behavior saddle points. network architecture. hamiltonian synaptic sampling ease saddle point problem. network performance mnist task learning hamiltonian dynamics nonhamiltonian synaptic sampling initial weights hamiltonian dynamics escape saddle point quickly. dashed vertical line indicates time non-hamiltonian dynamics switched hamiltonian dynamics network. comparison average accuracy test data hamiltonian synaptic sampling non-hamiltonian synaptic sampling continued simulation hamiltonian dynamics observed network escaped current saddle point much faster hamiltonian dynamics. considering average performance independent learning trials found hamiltonian sampling accelerates learning signiﬁcantly obtains better result within reasonable learning times virtually previous approaches reward-based learning spiking neural networks based policy gradient method parameters network gradually adjusted direction increases expected reward locally. hence suﬃciently long learning parameter setting network converges local optimum stays local optimum thereafter. proposed mathematical framework hamiltonian sampling allows create link reinforcement learning nonlinear optimization theory simulated annealing algorithm. link implies neural networks principle attain learning functionally attractive locally optimal network conﬁgurations principle even global optimum. theoretical result hence reveals fundamental advantage hamiltonian synaptic dynamics previous approaches reward-based network optimization. link nonlinear optimization becomes apparent takes closer look temperature parameter plasticity dynamics scales amount noise parameter updates. since given network samples decreased temperature concentrates parameter samples values lead large rewards therefore increases expected reward network. converges uniform distribution optimal denotes measure methods. further expected reward also assumes global optimum limit. attempt attain optimum start large temperature reduce slowly towards annealing procedure used simulated annealing non-linear optimization technique cooling technique however needs convergence associated stationary distribution temperature within reasonable time. data suggest genetic program developmental learning features reminiscent cooling schedule hamiltonian sampling dynamics likely improve convergence speed temperature. network used task shown fig. consisted input neurons hidden neurons output neuron. input neuron encoded binary input variable. produced poisson spike train output rate input input fig. cooling improves reward-based learning spiking neural networks. illustration network architecture. network consist input neurons hidden neurons output neuron. task learn function. comparison average reward obtained learning hamiltonian dynamics without cooling temperature evolution synaptic weights neurons hidden layer output neurons ﬁrst hours learning. average ﬁring rate output neuron four input patterns learning. fraction learning trials network ﬁnds optimal solution without cooling. learning pattern chosen randomly presented network time output network compared target output binary reward delivered accordingly. speciﬁcally every reward recomputed delivered network output neuron spiked past target otherwise. pattern presentation followed delay period input reward delivered network. then another randomly chosen pattern presented evolution synaptic weights learning shown fig. weights layers change signiﬁcantly throughout learning contribute learning task. synapses also remain plastic throughout whole learning time explore diﬀerent solutions. network responses learning shown fig. fig. learning average ﬁring rate output neuron input patterns respectively learning hours output neuron maximized ﬁring rate input patterns signiﬁcantly reduced patterns task considered seung also considered stochastic spiking neuron model however zero refractory time. further model positive negative reward delivered network every millisecond. noted learning work reliably positive negative rewards balanced. fact using highly unbalanced reward schedule rewards either network often achieve optimal performance constant temperature chosen learning case optimal results obtained learning trials introduced cooling schedule temperature decreased learning ratio increased superiority annealed optimization also visible average reward attained learning fig. shows parameter optimization annealed noise signiﬁcantly improve performance spiking neural networks. similar observation reported deep artiﬁcial neural networks theoretical framework hamiltonian sampling provides explanation phenomenon optimization annealed sampling similar simulated annealing thus opens door apply toolkit stochastic optimization gradient-based neural network learning principled manner. presented theoretical framework reward-based neural network optimization integrates hidden synaptic parameter plasticity process. suggest synaptic parameter could implemented synapse camkii abundantly present postsynaptic density acts pass ﬁlter induction synaptic plasticity. shown camkii-enriched dynamics supports special type ongoing stochastic policy search hamiltonian sampling friction convergences stationary distribution much faster langevin sampling david marr famously proposed treat brain computation three distinct complementary levels analysis today known marr’s tri-level hypothesis. interest realize biological data activation dynamics kinase camkii corresponds implementational/physical level marr’s tri-level hypothesis. proposed model network plasticity suggests camkii enables brain perform hamiltonian sampling algorithm level speciﬁc biological networks neurons able approximate hamiltonian sampling network conﬁgurations rather slower langevin sampling gradient descent. demonstrated several advantages hamiltonian sampling previously considered approaches reward-based learning spiking neural networks. shown fig. hamiltonian synaptic sampling framework learn smooth responses spiking neurons reward-based learning scale learn recurrent networks spiking neurons fig. shown synaptic sampling prone slow near saddle points objective function hamiltonian synaptic sampling signiﬁcantly speed learning cases. finally demonstrated reward-based network plasticity principle able acquire down-regulation stochastic component parameter updates full power simulated annealing optimizing network. allows neural networks attain learning locally optimal network conﬁgurations principle even global optimum. theoretical result provides gold standard reward-based network learning. camkii dynamics previously studied work focused detailed molecular dynamics implications stdp level pairing protocols treated camkii dynamics current study abstractly low-pass ﬁltering process studied implications system level reward-based learning. interesting note low-pass ﬁltering eﬀect also predicted model addition proposed role camkii binary-state behavior synapses hippocampus. underlying hypothesis synaptic eﬃcacies attain possible states depressed state potentiated state question recent experimental data model makes number experimentally testable predictions. shown previous work synaptic spine dynamics modeled stochastic process time-constants temporal scale several days. model includes hamiltonian momentum term suggests also short time scales models synaptic dynamics time constants provide better ﬁts. moreover proposed role camkii suggests time constant correspond rates dephosphorylation. result network optimization fig. suggests biological networks able control level stochasticity stochasticity decreases long lasting learning processes experimental results revealed learning behavioral task accompanied increased synaptic spine numbers spine dynamics analyzed simple model synaptic turnover found statistics spine regrowth task acquisition explained brief increase learning temperature ﬁndings suggest brain employs addition deterministic synaptic updates mechanism regulate speed random exploration high-dimensional space synaptic parameters several hours days. article introduced mathematical framework provides step towards understanding complex interplay deterministic stochastic strategies employed brain solve complex learning problems. present general mathematical framework synaptic parameter dynamics derive emerging stationary distribution network conﬁgurations results dynamics. generalized model includes hamiltonian synaptic sampling synaptic sampling without momentum special cases given following sdes posterior distribution network parameter given equation distribution camkii-related hidden synaptic parameter. notation denotes derivative evaluated parameter vector results dynamics describes general noisy ﬁrst-order interaction visible synaptic parameters determine eﬃcacy synapse hidden synaptic parameters absolute value model local concentration camkii activated state. dynamics thus seen generalization standard gradient-based synaptic plasticity rules includes structural constraints camkii activation stochastic plasticity. general dynamics joint distribution sets parameters converge produce samples result formalized following theorem theorem p∗p∗ strictly positive continuous probability distributions parameters respectively twice continuously diﬀerentiable respect postitive constants. stochastic diﬀerential equations leaves distribution sampling dynamics. stationary distribution parameters dynamic assumption strictly positive stationary distribution also unique. matrix diﬀusion coeﬃcients invertible potential conditions satisﬁed stationary distribution obtained simple integration. since matrix diﬀusion coeﬃcients diagonal model diﬀusion coeﬃcient matrix trivially invertible diagonal elements i.e. strictly positive. also potential conditions fulﬁlled veriﬁed substituting eqs. equation synaptic sampling deﬁned obtained choosing remark various types gradient descent also recovered generalized dynamics e.g. gradient descent momentum noiseless hamiltonian dynamics. equation seen continuous version hamiltonian sampling metropolis update performed simulating hamiltonian dynamic. equation also seen extension stochastic gradient hamiltonian monte carlo friction case temperature used shape static distribution synk index synapses project neuron prei denote index presynaptic neuron synapse denotes bias potential neuron recurrent network fig. used slowly changing bias potential ensures output rate neuron stays within ﬁnite bounds experiments used constant bias potential. yprei denote trace postsynaptic potentials presynaptic neuron synapse time throughout paper used standard double-exponential kernels brief ﬁnite rise exponential decay form time constants deﬁned dirac delta pulses positioned spike times neuron ﬁres according link function denotes ﬁring probability neuron time lasting eﬀects psps ﬁring probability depend history past spiking activities consider recurrent network section reward-guided network plasticity example show parameter dynamic approximate gradient. actually compute gradient expected reward feed-forward neural network simulations similar learning rule. order simplify notation represent history dependence result applying chain rule using exponential mapping function linear mapping used term vanishes learning rules similar previous ones found context maximum likelihood reinforcement learning neural networks average episodes average time time point treated start episode. average taken long sequence network activity starts time ends time t+τg. here systematic diﬀerence batch setup cannot guarantee time-invariant distribution initial network conditions since depend current network parameter setting. however assumption inﬂuence initial conditions decays quickly compared time scale environmental dynamics reasonable assume induced error negligible. thus rewrite form length sequence network activity empirical expectation taken. finally combine second third integral single rearrange terms substitute integrals past rather future obtain similar learning rule already proposed seung fact learning rule estimate based reward time ignores outer integral thus can’t approximate accurately. better estimation given kappel improve learning performance without biological plausible motivation. actually hamiltonian synaptic sampling framework camkii works momentum term computes average gradient corresponds outer integral thereby supporting better estimate gradient expected reward. build relationship hamiltonian synaptic sampling synaptic sampling show synaptic sampling included hamiltonian synaptic sampling. simplicity brevity consider version parameters dynamics discrete time. according parameter change ∆θsyn synaptic sampling small discrete time step written denotes learning rate controls speed parameter dynamics.vt represents gaussian noise zero mean variance noises independent parameter update time seems diﬀerent actually build relationship assumption momentum term transient time constant speciﬁc parameter large tends thus rewrite discrete version hamiltonian synaptic sampling attained network parameters converged stationary distribution reward temperature network reached stationary distribution expected reward given respect words sampling posterior decreased temperature concentrates parameter samples values lead large rewards therefore increases expected reward network. temperature annealing global optimization small temperatures posterior concentrated global optimum reward landscape. practice sampling process mixes extremely slowly temperatures probabilities non-optimal states. hence annealing schedule decreases temperature slowly time employed order give synaptic sampling enough time settle global optimum similar simulated annealing optimization algorithm. show limit network achieves maximal possible performance simulated annealing). sopt denote optimal circuit parameters i.e. sopt rmax} rmax maxθ{pn poisson spiking neurons divided pools. pool neurons encodes input number. order generate input patters ﬁrst generate random vectors dimension sampling uniform distribution choose vectors input. fig. shows examples distributed along input output plane sigmoidal neuron. node xcoordinate represents weighted four input numbers y-coordinate represents output sigmoidal neuron. mapping deﬁned +e−x sigmoidal neuron. presentation input patterns chosen randomly input converted poisson spike trains space-rate coding. maximum ﬁring rate neuron presenting example reward delivered denotes ﬁring probability sigmoidal neuron denotes scaled ﬁring rate spiking neuron time window pattern presentation. note equals rate total ﬁring times spiking neuron time window maximum ﬁring times time constants eligibility trace momentum respectively. details reward-guided network plasticity network connectivity excitatory inhibitory neurons suggested excitatory inhibitory neurons randomly connected connection probabilities given table connections include lateral inhibition excitatory inhibitory neurons. connectivity inhibitory neurons kept ﬁxed connection probability excitatory inhibitory neurons given synaptic weights drawn gaussian distribution inhibitory neurons connected targets probability refractory period excitatory inhibitory neurons. postsynaptic potentials excitatory neurons used time constants inhibitory synapses used faster kernel form bias potential initialized followed dynamics time constant adaptation mechanism desired output rate neuron. simpliﬁed version mechanism proposed balance activity networks excitatory inhibitory neurons. found regularization signiﬁcantly increased performance learning speed network model. synaptic dynamics used gaussian prior synaptic parameters initially drawn form gaussian distribution synaptic parameter changes clipped synaptic parameters allowed exceed sake numerical stability. weights synapses synaptic parameters became smaller zero clamped previous model temperature parameter kept constant time constants eligibility trace momentum respectively. generate pattern adapted method aﬀerent inputs given representations simple symbolic sensory environment. inputs randomly generated realizations inhomogeneous poisson spike trains. generate spike patterns input neurons assigned gaussian tuning curve tuning curve centers independently equally scattered unit cube. represented randomly selected point -dimensional space covered tuning curves input neurons. stimulus positions overlaid small-amplitude jitter presentation ﬁring rate individual input neuron given support sensory experience input neuron’s tuning curve. maximum ﬁring rate input neuron addition oﬀset background noise added. present input neurons homogeneous poisson ﬁring. cursor movement implemented using simple version population vector method spike neuron caused cursor jump direction neuron’s preferred direction trial cursor position reset start location details hamiltonian dynamic improves network behavior saddle points three-layer perceptron network consists input neurons hidden neurons output neurons used learn mnist data set. shown fig. activation function hidden layer layer sigmoid function winner-take-all respectively. trial digital chosen randomly mnist data input. network gets immediate reward eligibility trace used here. speciﬁc gradients expected synapse connected neurons hidden layer reward otherwise binary reward denotes outputs presynaptic postsynaptic neurons synapse weighted inputs postsynaptic neuron synapse simulation learning rate hamiltonian sampling synaptic sampling. parameters chosen order test whether hamiltonian dynamic help overcome saddle point problem ﬁrst train network synaptic sampling continue train hamiltonian sampling current parameter setting. note initial value momentum term result shows network escaped current saddle point much faster hamiltonian dynamics. details reward-based learning global network optimization three-layer perceptron network consists input neurons hidden neurons output neurons used learn mnist data set. shown fig. activation function hidden layer layer sigmoid function soft winner-take-all respectively. trial digit chosen randomly mnist data input. network gets immediate reward eligibility trace used here. speciﬁc estimator gradients expected reward directly given according simulation used learning rate hamiltonian sampling synaptic sampling. parameters chosen ﬁrst trained network synaptic sampling trials. continued training hamiltonian sampling using parameter setting time point", "year": "2016"}