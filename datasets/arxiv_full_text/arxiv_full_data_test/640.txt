{"title": "GRIM-Filter: Fast Seed Location Filtering in DNA Read Mapping Using  Processing-in-Memory Technologies", "tag": "q-bio", "abstract": " Motivation: Seed location filtering is critical in DNA read mapping, a process where billions of DNA fragments (reads) sampled from a donor are mapped onto a reference genome to identify genomic variants of the donor. State-of-the-art read mappers 1) quickly generate possible mapping locations for seeds (i.e., smaller segments) within each read, 2) extract reference sequences at each of the mapping locations, and 3) check similarity between each read and its associated reference sequences with a computationally-expensive algorithm (i.e., sequence alignment) to determine the origin of the read. A seed location filter comes into play before alignment, discarding seed locations that alignment would deem a poor match. The ideal seed location filter would discard all poor match locations prior to alignment such that there is no wasted computation on unnecessary alignments.  Results: We propose a novel seed location filtering algorithm, GRIM-Filter, optimized to exploit 3D-stacked memory systems that integrate computation within a logic layer stacked under memory layers, to perform processing-in-memory (PIM). GRIM-Filter quickly filters seed locations by 1) introducing a new representation of coarse-grained segments of the reference genome, and 2) using massively-parallel in-memory operations to identify read presence within each coarse-grained segment. Our evaluations show that for a sequence alignment error tolerance of 0.05, GRIM-Filter 1) reduces the false negative rate of filtering by 5.59x--6.41x, and 2) provides an end-to-end read mapper speedup of 1.81x--3.65x, compared to a state-of-the-art read mapper employing the best previous seed location filtering algorithm.  Availability: The code is available online at: https://github.com/CMU-SAFARI/GRIM ", "text": "*correspondence jeremiekimgmail.com calkancs.bilkent.edu.tr onur.mutluinf.ethz.ch department electrical computer engineering carnegie mellon university pittsburgh department computer science z¨urich z¨urich full list author information available article motivation seed location ﬁltering critical read mapping process billions fragments sampled donor mapped onto reference genome identify genomic variants donor. state-of-the-art read mappers quickly generate possible mapping locations seeds within read extract reference sequences mapping locations check similarity read associated reference sequences computationally-expensive algorithm determine origin read. seed location ﬁlter comes play alignment discarding seed locations alignment would deem poor match. ideal seed location ﬁlter would discard poor match locations prior alignment wasted computation unnecessary alignments. results propose novel seed location ﬁltering algorithm grim-filter optimized exploit d-stacked memory systems integrate computation within logic layer stacked memory layers perform processing-in-memory grim-filter quickly ﬁlters seed locations introducing representation coarse-grained segments reference genome using massively-parallel in-memory operations identify read presence within coarse-grained segment. evaluations show sequence alignment error tolerance grim-filter reduces false negative rate ﬁltering .x–.x provides end-to-end read mapper speedup .x–.x compared state-of-the-art read mapper employing best previous seed location ﬁltering algorithm. availability code available online https//github.com/cmu-safari/grim introduction understanding human genomes today aﬀected ability modern technology quickly accurately determine individual’s entire genome. human genome comprised sequence approximately billion bases grouped deoxyribonucleic acids today’s machines identify short sequences determining genome requires three stages cutting genome many short reads identifying sequence read mapping read reference genome order analyze variations sequenced genome. paper focus improving third stage often referred read mapping major computational bottleneck modern genome analysis pipeline. read mapping performed computationally read mappers read identiﬁed. steps used seed-and-extend mapper. first mapper obtains read second mapper selects smaller segments read serve seeds third mapper indexes data structure seed obtain list possible locations within reference genome could result match fourth possible location list mapper obtains corresponding sequence reference genome fifth mapper aligns read sequence reference sequence using expensive sequence alignment algorithm determine similarity read sequence reference sequence. improve performance seed-and-extend mappers utilize seed location ﬁlters recently introduced seed location ﬁlter eﬃciently determines whether candidate mapping location would result incorrect mapping performing computationally-expensive sequence alignment step location. long ﬁlter eliminate possible locations would result incorrect mapping faster time takes perform alignment entire read mapping process substantially accelerated result several recent works focused optimizing performance seed location ﬁlters advent seed location ﬁlters performance bottleneck read mapping shifted sequence alignment seed location ﬁltering unfortunately seed location ﬁlter requires large amounts memory bandwidth process characterize candidate locations. goal reduce time spent ﬁltering thereby improve speed read mapping. present algorithm grim-filter eﬃciently ﬁlter locations high parallelism. design grim-filter well-suited implementation d-stacked memory exploiting parallel low-latency processing capability logic layer memory. d-stacked dram technology integrates logic memory three-dimensional stack dies large internal data transfer bandwidth. enables bulk transfer data memory layer logic layer perform simple parallel operations data. conventional computing requires movement data long slow energy-hungry buses processing cores memory cores operate data. contrast processing-in-memory -enabled devices d-stacked memory perform simple arithmetic operations close data resides high bandwidth latency. carefully designed algorithms application performance often greatly improved relatively narrow long-latency cores memory longer impedes speed computation data. goal develop seed location ﬁlter exploits high memory bandwidth processing-in-memory capabilities d-stacked dram improve performance read mappers. knowledge ﬁrst seed location ﬁltering algorithm accelerates read mapping overcoming memory bottleneck using d-stacked memory technologies. grim-filter used read mapper. however work demonstrate eﬀectiveness grim-filter hash table based mapper mrfast fasthash improve performance hash table based read mappers maintaining high sensitivity comprehensiveness mechanism. grim-filter provides quick method determining whether read match given location thus allowing read mapper skip expensive sequence alignment process location. grim-filter works counting existence small segments read genome region. count falls threshold indicating many small segments read present grim-filter discards locations region alignment. existence small segments region stored bitvector easily predetermined region reference genome. bitvector reference genome region retrieved read must checked match region. regional approximation technique enables high performance boost high parallelism also improves ﬁltering accuracy state-of-the-art. ﬁltering accuracy improvement comes ﬁner granularity grim-filter uses counting subsequences read region genome compared state-of-the-art ﬁlter results. evaluate grim-filter qualitatively quantitatively state-of-the-art seed location ﬁlter fasthash results show grim-filter provides .x–.x smaller false negative rate best previous ﬁlter zero false positives grim-filter provides end-to-end performance improvement .x–.x state-of-the-art read mapper mrfast fasthash real genomic reads sequence alignment error tolerance also note increase sequence alignment error tolerance performance improvement ﬁlter state-of-the-art increases. makes grim-filter eﬀective relevant future-generation error-prone sequencing technologies nanopore sequencing motivation mapping reads reference genome enables analysis variations sequenced genome. throughput read mapping increases largescale genome analyses become possible. ability deeply characterize analyze genomes large scale could change medicine reactive preventative personalized practice. order motivate method improving performance read mappers pinpoint performance bottlenecks modernday mappers focus acceleration eﬀorts. across data state-of-the-art read mapper mrfast fasthash average spends execution time performing sequence alignment locations found match execution time performing sequence alignment locations discarded found match goal implement seed location ﬁlter reduces wasted computation time spent performing sequence alignment false locations. seed location ﬁlter would quickly determine location match read would avoid sequence alignment altogether. ideal seed location ﬁlter correctly ﬁnds false locations without increasing time required execute read mapping. ideal seed location ﬁlter would improve average performance mrfast speedup primarily reduced number false location alignments. contrast prior works gain speedups implementing part read mapper specialized hardware gpus focusing mainly acceleration sequence alignment process avoidance sequence alignment. works accelerate sequence alignment provide orthogonal solutions could implemented together seed location ﬁlters including grim-filter additional performance improvement grim-filter describe proposal seed location ﬁlter grim-filter. high level idea grim-filter store utilize metadata short segments genome i.e. segments order several hundred base pairs long order quickly determine read result match genome segment. genome metadata representation figure shows reference genome associated metadata formatted eﬃcient operation grim-filter. reference genome divided short contiguous segments order several hundreds base pairs refer bins. grim-filter operates granularity bins performing analyses metadata associated bin. metadata represented bitvector stores whether token i.e. short sequence order base pairs present within associated bin. refer bitvector existence bit. account possible tokens length bitvector must bits length denotes existence particular token instance. figure highlights bits token instances bin’s bitvector shows token gacag exists i.e. existence associated token gacag bitvector; token ttttt present i.e. existence associated token ttttt bitvector. bitvectors associated reference genome bitvectors need generated reference used number reads individuals species. order generate bitvectors genome must sequentially scanned every possible token length selected token size. binx contains token bitvector corresponding token must binx contain token left unset bitvectors saved stored later mapping reads reference genome i.e. part genome’s metadata. figure grim-filter data structure <row column> indicates token present corresponding grim-filter divides genome overlapping bins. grim-filter’s metadata associated reference genome. columns indexed number location. rows indexed token value. ﬁgure token size=. grim-filter operation sequence alignment grim-filter checks contains potential mapping location read based list potential locations provided read mapper. contains location grim-filter checks location likely match read sequence operating bitvector bin. relies entire read contained within given thus requires bins overlap construction metadata shown figure grim-filter uses described bitvectors quickly determine whether match within given error tolerance impossible. done running expensive sequence alignment algorithm order reduce number unnecessary sequence alignment operations. location associated seed grimfilter loads bitvector containing location; operates bitvector quickly determine match discards location determines poor match. grim-filter discard location sequence location must aligned read determine match similarity. using circled steps figure explain detail grim-filter determines whether discard location read sequence indicate number contains location grim-filter extracts every token contained within read sequence then grimfilter loads bitvector binbin tokens contained grim-filter extracts existence token bitvector whether token exists somewhere within bin. grim-filter sums extracted existence bits together refer accumulation location accumulation represents number tokens read sequence present binbin num. larger accumulation indicates tokens present therefore location likely contain match finally grim-filter compares sumz constant accumulation threshold value determine whether location likely match read sequence sumz greater equal threshold likely match read mapper must perform sequence alignment reference sequence location sumz less threshold match read mapper skips sequence alignment location. explain determine accumulation threshold section figure flow diagram seed location ﬁltering algorithm. grim-filter takes read sequence sums existence tokens within determine whether read sequence must sequence aligned reference sequence discarded without alignment. note token size example. grim-filter ﬁnishes checking location returns control read mapper performs sequence alignment locations pass ﬁlter. process repeated seed locations signiﬁcantly reduces number alignment operations ultimately reducing end-to-end read mapping runtime implementation grim-filter ensures zero false positive rate grim-filter passes seed location whose contains enough tokens read sequence. grim-filter also account errors sequence tokens match perfectly therefore using grim-filter ﬁlter seed locations aﬀect correctness read mapper. integration full read mapper figure shows integrate grim-filter read mapper improve read mapping performance. read mapper begins sequence alignment sends read sequence along potential seed locations found hash table sequence grim-filter. then filter bitmask generator grim-filter performs seed location ﬁltering algorithm describe section checking bins include potential seed location contains tokens read sequence location save output threshold decision within seed location ﬁlter bitmask means location’s accumulation greater equal threshold means accumulation less threshold. bitmask passed seed location checker locates reference segment corresponding seed location passed ﬁlter sends reference segment read mapper. read mapper performs sequence alignment reference segments receives seed location checker outputs correct mappings read sequence. figure grim-filter integration read mapper. filter bitmask generator uses bitvectors determine whether locations within potential matches read sequence saves potential match information seed location filter bitmask seed location checker uses bitmask retrieve corresponding reference segments seed locations match sent read mapper sequence alignment. determining accumulation threshold discuss detail determine threshold used evaluate accumulation threshold used determine whether seed location sent read mapper sequence alignment greater value sumz indicates seed location likely good match read sequence however cases sumz high read sequence results poor match seed location simple example poor match read sequence consists entirely base pairs resulting aaaaa tokens seed location consists entirely base pairs except single aaaaa token. example aaaaa tokens read sequence locate aaaaa token seed location resulting accumulation even though location contains aaaaa token. cases occur even though occur probability grim-filter cannot guarantee high accumulation seed location corresponds good match read sequence. hand grim-filter guarantee accumulation indicates reference sequence within poor match read sequence. lower means fewer tokens read sequence present translates directly greater number errors potential match. enough guarantee potential read sequence alignment would many errors good match. change worst case aﬀect tokens. figure shows example substitution aﬀects four diﬀerent tokens therefore error tolerate must assume worst-case error case tokens match read sequence even location actually contains read sequence. figure equation calculate accumulation threshold read sequence token length sequence alignment error tolerance. impact substitution error four separate tokens single deletion substitution error propagates consecutive tokens single insertion error propagates consecutive tokens. equation figure gives accumulation threshold accounting worst-case scenario sequence alignment error tolerance means maximum number allowable errors equal ceiling read size multiplied sequence alignment error tolerance. sequence alignment error tolerance less widely used allowable error assume worst-case number tokens aﬀected error. also assume worst case error aﬀects diﬀerent tokens within read results greatest possible number tokens match. calculate multiplying maximum number allowable errors equation. finally subtract largest possible number tokens match total number tokens candidacy d-stacked memory implementations identify three characteristics ﬁlter bitmask generator grim-filter make strong candidate implementation d-stacked memory requires simple operations highly parallelizable since operated independently parallel; highly memory-bound requiring single memory access approximately every three computational instructions attempts bridge well-known disparity processor speed memory bandwidth. next describe grim-filter easily mapped utilize memory technology disparity processor speed memory bandwidth increases memory becomes bottleneck computing stack terms performance energy consumption along d-stacked dram enables much higher bandwidth lower latency compared conventional dram disparity between processor memory alleviated re-emergence concept processing-in-memory integrates processing units inside near main memory leverage high in/near-dram bandwidth intra-dram latency; reduce energy consumption reducing amount data transferred processor. section brieﬂy explain required background technologies leverage implement grimfilter highly-parallel manner. d-stacked memory main memory implemented using dram technology today’s systems conventional dram chips connected processors using long slow energy-hungry interconnects conventional dram chips incorporate logic perform computation. detail modern dram operation architecture refer reader previous works d-stacked dram dram technology much higher internal bandwidth conventional dram thanks closer integration logic memory using through-silicon interconnects seen figure tsvs vertical interconnects pass silicon wafers stack dies much smaller feature size traditional interconnect enables d-stacked dram integrate hundreds thousands wired connections stacked layers. using large number wired connections d-stacked dram transfer bulk data simultaneously enabling much higher bandwidth compared conventional dram. figure shows dstacked dram based system consists four layers dram dies logic stacked together connected using tsvs processor silicon interposer connects stacked dram processor. vertical connections stacked dram wide short results high bandwidth power consumption respectively many diﬀerent d-stacked dram architectures available today. high bandwidth memory already integrated radeon series graphics cards high bandwidth memory integrated radeon vega series graphics cards nvidia tesla accelerators hybrid memory cube developed number diﬀerent contributing companies like also enables logic layer underneath dram layers perform computation already integrated sparc xifx chip technologies enable processing-in-memory also already prototyped real chips micron’s automata processor tibco transactional application servers figure d-stacked dram example. high bandwidth memory consists stacked memory layers logic layer connected high bandwidth throughsilicon vias microbumps d-stacked memory connected processor interposer layer provides high-bandwidth logic layer processing units package substrate. processing-in-memory technique improve performance reduce energy consumption memory system place computation units inside memory system data resides. today processing capabilities appearing inside near dram memory computation inside near dram signiﬁcantly reduces need transfer data to/from processor memory bus. provides signiﬁcant performance improvement energy reduction compared conventional system architecture must transfer data to/from processor since processor entity performs computational tasks. d-stacked dram pim. combination technologies d-stacked dram enables promising opportunities build high-performance low-power systems. promising design d-stacked dram consists multiple stacked memory layers tightly-integrated logic layer controls stacked memory shown figure many prior works show logic layer d-stacked dram utilized managing stacked memory layers also integrating application-speciﬁc accelerators simple processing cores. since logic layer already exists enough space integrate computation units integrating application-speciﬁc accelerators logic layer requires modest design implementation overhead little hardware overhead various analyses). importantly d-stacked dram architecture enables fully customize logic layer acceleration applications using processing-inmemory mapping grim-filter d-stacked memory grim-filter good candidate implement using processingin-memory ﬁlter memory-intensive performs simple computational operations figure shows implement grim-filter d-stacked memory. center block shows layer example d-stacked memory architecture multiple dram layers stacked logic layer. layers connected together several hundred tsvs enable high data transfer bandwidth layers. dram layer subdivided multiple banks memory. bank dram layer connected banks dram layers using tsvs. interconnected banks along slice logic layer grouped together vault. inside d-stacked memory store bitvector within bank follows bitvector placed diﬀerent consecutive manner bits bitvector placed column entire bitvector column design place customized logic perform grim-filter operations within logic layer slice vault perform independent grim-filter operations parallel every vault. next discuss organize bitvectors within bank. afterwards discuss customized logic required grim-filter associated hardware cost. figure left block grim-filter bitvector layout within dram bank. center block dstacked dram tightly integrated logic layer stacked underneath tsvs high intra-dram data transfer bandwidth. right block custom grim-filter logic placed logic layer vault. grim-filter reads data bank dram buﬀers within bank’s buﬀer resides dram layer bank. data copied data register sits logic layer grim-filter logic read data. data organization allows vault compute accumulation multiple bins simultaneously. thus grim-filter quickly eﬃciently determine across many bins whether seed location needs discarded sequence aligned bins. right block figure shows custom hardware logic implemented grim-filter vault’s logic layer. design small logic module grimfilter consists incrementer accumulator comparator operates bitvectorx single incrementer adds value accumulator stores accumulated order hold ﬁnal accumulator must least bits wide. comparator must width accumulator comparator used check whether accumulated exceeds accumulated threshold. arrange bitvectors dram single read operation vault retrieves many existence bits parallel many bitvectors token. existence bits copied dram bank’s buﬀer data register within logic layer slice vault. order maximize throughput grim-filter logic module logic layer slice. allows grimfilter process existence bits multiple bitvectors parallel. integration system low-level operation. grim-filter starts sends read sequence in-memory grim-filter logic along range consecutive bins check match. grim-filter quickly checks range bins determine whether discard seed locations within bins. logic layer grim-filter filter bitmask generator iterates token read sequence token grim-filter reads memory vault contains existence bits token bins checked buﬀer inside dram layer. then grim-filter copies data register logic layer. grim-filter logic module assigned single bin. logic module examines bin’s existence buﬀer incrementer adds value accumulator existence set. process repeated tokens tokens processed logic module uses comparator check accumulator holds accumulated assigned greater equal accumulated threshold. sumz greater equal threshold seed location ﬁlter indicating read sequence sequence aligned locations read mapper. maintain amount parallelism present bitvector operations place seed location ﬁlter bits seed location ﬁlter bitmask logic module writes bitmask performs accumulator threshold comparison. seed location ﬁlter bitmask written dram layer. seed location checker starts executing reads seed location ﬁlter bitmasks dram performs sequence alignment bits whose seed location ﬁlter bits hardware overhead. hardware overhead grim-filter implementation d-stacked memory depends available bandwidth memory layer logic layer. bandwidth bits cycle across vaults grim-filter exploits parallelism completely place grim-filter logic modules across vaults within logic layer. total memory read mapper processes reads consisting base pairs grimfilter requires incrementer lookup tables seven-bit counters comparators enough buﬀer space hold seed location ﬁlter bitmasks. larger bandwidth logic memory layers would able compute seed location ﬁlter bits bins parallel would also incur larger hardware overhead logic layer. read mapper performs sequence alignment seed locations speciﬁed seed location ﬁlter bitmask grim-filter generates seed location ﬁlter bitmasks diﬀerent seed locations. bitmask buﬀer size provides enough capacity ensure grim-filter read mapper never stall lack buﬀer space. overall memory footprint bitvectors reference genome calculated multiplying number bins size single bin. section show parameters results eﬀective ﬁlter memory footprint experimental methodology evaluated read mappers. evaluate proposal incorporating grimfilter state-of-the-art hash table based read mapper mrfast fasthash choose mapper evaluations provides high accuracy presence relatively many errors required detect genomic variants within across species grim-filter plugs extension mrfast using simple series calls application programming interface however note grim-filter used read mapper. major evaluation metrics. report grim-filter’s false negative rate end-to-end performance improvement read mapper using grim-filter. measure false negative rate ﬁlter ratio number locations passed ﬁlter result mapping locations passed ﬁlter. note implementation grim-filter ensures zero false positive rate thus grim-filter aﬀect correctness read mapper. performance evaluation. measure performance improvement grimd comparing execution time read mappers. develop methodology estimate performance grim-d since real hardware systems enable in-memory computation unavailable point time. estimate grim-d’s execution time need time spent three components obtain measure performance grim-software software-only version grim-filter take advantage processing d-stacked memory. grim-software mrfast measure grim-software-filtering-time. estimate validated simulator similar ramulator provides time spent grim-d ﬁltering using processing-in-memory. simulator models time spent in-memory logic produce seed location ﬁlter bitmask store bitmask buﬀer accessible read mapper. data sets. used real data sets genomes project used data sets used original evaluation mrfast fasthash order provide fair comparison baseline. table lists read length size data set. evaluation results ﬁrst proﬁle reference human genome order determine range parameters reasonable grim-filter. determine points diminishing returns several parameter values. data presented section using preliminary data reduce required experiments reasonable range parameters. implementation grim-filter enables variation runtime parameters within ranges values determine experimentation best possible results. quantitatively evaluate grim-filter’s improvement false negative rate mapper runtime baseline mrfast fasthash sensitivity grim-filter parameters order determine range parameters experiments series analyses fundamental characteristics human reference genome. perform initial experiments determine eﬀective parameters grimfilter compute memory footprint. memory footprint grim-filter depends directly number bins divide reference genome into since requires bitvector hold token existence bits. since bitvector must contain boolean entry permutation token size bitvector must contain bits. total memory footprint obtained multiplying bitvector size number bins. section sweep number bins token size error tolerance grim-filter considering memory footprint. understand diﬀerent parameters aﬀect performance grim-filter study sweep parameters range values result memory footprint average read existence. figure shows varying number diﬀerent parameters aﬀects average read existence across bins. deﬁne average read existence ratio bins seed locations pass ﬁlter bins comprising genome representative reads. would like value possible reﬂects ﬁlter’s ability ﬁlter incorrect mappings. lower average read existence means fewer bins must checked mapping representative reads. across three plots vary token size within plot vary number bins split reference genome into denoted diﬀerent curves x-axis shows error tolerance used y-axis shows average read existence. plot average min/max across data sets indicated respectively triangle whiskers. figure eﬀect varying token size error tolerance count average read existence. representative reads collect data. lower value average read existence represents eﬀective ﬁlter. note scale y-axis diﬀerent three diﬀerent graphs. make three observations ﬁgure. first looking across three plots observe increasing token size provides large reduction average read existence increasing token size provides much smaller reduction average read existence. reduction average read existence fact that random pool probability observing certain substring size distribution base pairs across reference genome across random larger token size always result large decrease seen changing token size note increasing token size causes grim-filter memory footprint. second observe three plots increase number bins results decrease average read existence. size decreases number bins increases smaller bins smaller sample size reference genome given substring could exist within. third observe plot increasing error tolerance results increase average read existence. fact allow errors fewer tokens entire read sequence must present seed location pass ﬁlter. increases probability seed location random read passes ﬁlter random bin. poor sequence alignment location passes ﬁlter categorized false negative. conclude ﬁgure using tokens size provides quite good ﬁltering eﬀectiveness without requiring much memory footprint using token size false negative rate. choose ﬁnal bitvector size sweeping number bins error tolerance figure shows varying parameters aﬀects false negative rate grim-filter. x-axis varies number bins diﬀerent lines represent diﬀerent values when sweeping number bins multiples even multiple number tsvs logic memory layers today’s d-stacked memories want multiple utilize tsvs time copy data buﬀer memory layer corresponding data register logic layer. maximizes grim-filter’s internal memory bandwidth utilization within d-stacked memory. negatives error tolerance values. second observe that increase error tolerance regardless parameters false negative rate increases. also number bins minimally aﬀects runtime grimfilter linearly increases memory footprint. based memory footprint. larger number bins results bitvectors must keep parameter reasonable value order retain reasonable memory footprint grim-filter. since chosen token size grim-filter requires bitvectors length equals number bins segment reference genome into. conclude employing bins results best trade-oﬀ memory footprint ﬁltering eﬃciency runtime. parameters results total memory footprint approximately storing bitvectors mechanism reasonable size today’s d-stacked memories grim-filter parallelization. grim-filter operates every independently parallel using separate logic module bin. thus grimfilter’s parallelism increases additional operates simultaneously. refer consecutive bins grim-filter logic modules currently assigned window internal bandwidth enables copying bits memory layer logic layer every cycle allowing grim-filter operate many consecutive bins parallel grim-filter must check windows contain least seed location contrast consecutive bins contains many seed locations grim-filter operate every parallel quickly determine seed locations within bins safely note time generate bitvectors included ﬁnal runtime results need generated reference genome either user distributor. that genome length generate bitvectors order understand grim-filter’s ability parallelize operations many bins analyze grim-filter using window size takes advantage full memory bandwidth available memory. discuss section read mapper generates list potential seed locations read sequence sends list grim-filter ﬁlter starts. several bins call empty bins contain potential seed locations. logic module module assigned empty grim-filter immediately moves next without computing accumulation sum. however some logic modules assigned empty bins. happens order simplify hardware grim-filter operates logic modules lockstep thus logic module assigned empty must wait logic modules ﬁnish move onto another bin. result grim-filter faster grim-filter quantify beneﬁts parallelization compare performance grim-filter window sizes using representative reads. seeds grim-filter reduces ﬁltering time compared grim-filter remaining seeds grim-filter reduces ﬁltering time thus even though many logic modules assigned empty bins given cycle grim-filter reduces ﬁltering time operating many bins contain potential seed locations parallel. overlapping grim-filter computation sequence alignment cpu. addition operating multiple bins parallel beneﬁt implementing grim-filter d-stacked memory ﬁltering operations parallelized sequence alignment happens since ﬁltering longer uses cpu. every cycle window size grimfilter’s filter bitmask generator reads bits memory updates accumulation sums bins within window contain potential seed location. accumulation sums computed compared threshold grim-filter’s seed location checker discard seed locations bins whose accumulation sums meet threshold seed locations discarded sent read mapper sequence alignment ending grim-filter’s work current window. read mapper aligns sequences passed ﬁlter completed window grim-filter’s filter bitmask generator moves onto another window computing seed location ﬁlter bits bins. grim-filter exploit enough parallelism provide enough bins keep sequence alignment step busy least long time needed filter bitmask generator process window. would allow ﬁltering latency overlap completely alignment eﬀect hiding grimfilter’s latency. window bins provides enough parallelism completely hide ﬁltering latency read mapper running performs sequence alignment. false negative rate. figure shows false negative rate grim-filter compared baseline fasthash ﬁlter across real data sets evaluate. plots ﬁgure show false negative rates error tolerance values ranging increments make three observations ﬁgure. first grim-filter provides much lower false negative rate baseline fasthash ﬁlter data sets error tolerance values. error tolerance false negative rate grim-filter lower fasthash ﬁlter averaged across read data sets. second grim-filter’s false negative rate increases error tolerance increases decreases error tolerance increases least conﬂicting reasons. first error tolerance increases accumulation threshold decreases thus grim-filter discards fewer locations results higher false negative rate. second error tolerance increases number acceptable mapping locations increases number candidate locations remains same results lower false negative rate. interaction conﬂicting reasons results initial increase subsequent decrease false negative rates observe. third observe higher error tolerance values grim-filter reduces false negative rate compared fasthash ﬁlter larger fraction. shows grimfilter much eﬀective ﬁltering mapping locations increase error tolerance. conclude grim-filter eﬀective reducing false negative rate. execution time. figure compares execution time grim-d mrfast fasthash across diﬀerent read data sets error tolerance values used figure make three observations. first grim-d improves performance data sets error tolerance values. error tolerance average performance improvement across data sets. second error tolerance increases grimd’s performance improvement also increases. grim-filter safely discards many mapping locations fasthash ﬁlter higher error tolerance values thus grim-filter saves signiﬁcantly execution time fasthash ﬁlter ignoring many unnecessary alignments. third based analysis execution time breakdown grimd grim-d’s performance gains mainly reduction average computation time spent false negatives compared using fasthash ﬁlter seed location ﬁltering. conclude employing grim-filter seed location ﬁltering state-of-the-art read mapper signiﬁcantly improves performance read mapper. related work knowledge ﬁrst paper exploit d-stacked dram processing-in-memory capabilities implement seed location ﬁltering algorithm mitigates major bottleneck read mapping pre-alignment section brieﬂy describe related works accelerate pre-alignment algorithms accelerate sequence alignment hardware support. accelerating pre-alignment. recent prior work implements seed location ﬁlter fpga shows signiﬁcant speedup prior ﬁlters. however shown work fpga still limited memory bandwidth bottleneck. grim-filter overcome bottleneck fpga well. accelerating sequence alignment. another recent prior work exploits high memory bandwidth reconﬁgurable logic layer d-stacked memory implement accelerator sequence alignment many prior works fpgas also accelerate sequence alignment. works accelerate sequence alignment using customized fpga implementations diﬀerent existing read mapping algorithms. example arram accelerate soap tool fpga engine achieving speedup compared houtgast present fpga-accelerated version bwa-mem faster compared software implementation. works gpus purpose accelerating sequence alignment. example accelerate bowtie respectively. contrast grim-filter accelerators focus accelerating sequence alignment whereas grim-filter accelerates pre-alignment hence grim-filter orthogonal works combined performance improvement. figure execution time mappers grim-d mrfast fasthash across real data sets diﬀerent error tolerance values. note scale y-axis diﬀerent diﬀerent graphs. future work shown grim-filter signiﬁcantly reduces execution time read mappers reducing number unnecessary sequence alignments taking advantage processing-in-memory using d-stacked dram technology. believe many possible applications employing d-stacked dram technology within genome sequence analysis pipeline signiﬁcant additional performance improvements obtained combining future techniques grim-filter. grim-filter essentially seed location ﬁlter employed sequence alignment read mapping used read mapper along acceleration mechanisms genome sequence analysis pipeline. identify three promising major future research directions. believe promising explore beneﬁts combining grim-filter various read mappers ﬁeld show eﬀects mapping varying sizes reference genomes examine grim-filter scale process greater number reads concurrently. conclusion paper introduces grim-filter novel algorithm seed location ﬁltering critical performance bottleneck genome read mapping. grim-filter three major novel aspects. first preprocesses reference genome collect metadata large subsequences genome stores information whether small subsequences present bin. second grimfilter eﬃciently operates metadata quickly determine whether discard mapping location read sequence prior expensive sequence alignment thereby reducing number unnecessary alignments improving performance. third grim-filter takes advantage logic layer within d-stacked memory enables eﬃcient processing-in-memory overcome memory bandwidth bottleneck seed location ﬁltering. examine trade-oﬀs various parameters grim-filter present parameters result signiﬁcant performance improvement state-of-the-art seed location ﬁlter fasthash. running sequence alignment error tolerance show grim-filter ﬁlters seed locations .x–.x lower false negative rates fasthash; improves performance fastest read mapper mrfast fasthash .x–.x. grim-filter universal seed location ﬁlter applied read mapper. believe promising potential designing read mapping algorithms memory technologies processing paradigms hope results paper provides inspiration works design sequence analysis bioinformatics algorithms take advantage memory technologies processing paradigms processing-in-memory using d-stacked dram. acknowledgments earlier version paper appears arxiv.org earlier version work presented short talk recomb-seq thank anonymous reviewers feedback. work supported part semiconductor research corporation national institutes health intel samsung vmware. author details department electrical computer engineering carnegie mellon university pittsburgh usa. department computer science carnegie mellon university pittsburgh usa. nvidia research austin usa. department computer engineering bilkent university bilkent ankara turkey bilkent department computer engineering tobb university economics technology sogutozu department computer science z¨urich z¨urich", "year": "2017"}