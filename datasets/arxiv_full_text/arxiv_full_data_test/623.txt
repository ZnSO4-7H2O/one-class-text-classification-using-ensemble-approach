{"title": "A Supervised STDP-based Training Algorithm for Living Neural Networks", "tag": "q-bio", "abstract": " Neural networks have shown great potential in many applications like speech recognition, drug discovery, image classification, and object detection. Neural network models are inspired by biological neural networks, but they are optimized to perform machine learning tasks on digital computers. The proposed work explores the possibilities of using living neural networks in vitro as basic computational elements for machine learning applications. A new supervised STDP-based learning algorithm is proposed in this work, which considers neuron engineering constrains. A 74.7% accuracy is achieved on the MNIST benchmark for handwritten digit recognition. ", "text": "neural networks shown great potential many applications like speech recognition drug discovery image classiﬁcation object detection. neural network models inspired biological neural networks optimized perform machine learning tasks digital computers. proposed work explores possibility using living neural networks vitro basic computational elements machine learning applications. supervised stdp-based learning algorithm proposed work considers neuron engineering constraints. accuracy achieved mnist benchmark handwritten digit recognition. artiﬁcial neural network spiking neural network brain inspired computational models shown promising capabilities solving problems face detection image classiﬁcation computer programs based neural networks defeat professional players board game relies numerical abstractions represent states neurons connections among them whereas uses spike trains represent inputs outputs mimics computations performed neurons synapses models extracted biological neuron behaviors optimized perform machine learning tasks digital computers. contrast proposed work explores whether biological living neurons vitro directly used basic computational elements perform machine learning tasks. living neurons perform computation naturally transferring spike information synapses. energy consumption efﬁcient hardware evaluation living neurons small sizes adapt changes. precise control living neural networks challenging recent advances optogenetics genetically encoded neural activity indicators cell-level micropatterning open possibilities area optogenetics label individual neurons different types optically controlled channels equip vitro neural networks optical interfaces. patterned optical stimulation high-speed optical detection allow simultaneous access thousands vitro neurons. addition invention micropattern enables modularized system design. best knowledge work ﬁrst explore possibility using living neurons machine learning applications. considering neuron engineering design constraints algorithm proposed easy training future biological experiments. fully connected spiking neural network evaluated mnist dataset using neuron simulator accuracy obtained based biologically-plausible model promising result demonstrates feasibility using living neuron networks compute. spiking neural network model closely represents biological neuron behavious. biological neural network neurons connected plastic synapses. spike pre-synaptic neuron change membrane potential post-synaptic neuron. impact spikes different time pre-synaptic neurons accumulated post-synaptic neuron. post synaptic spikes generated membrane voltage neuron exceeds certain threshold. information represented series spikes accumulative effect pre-synaptic spikes also modeled. work models biological neural networks using hodgkin-huxley neuron model spike-based data representation. major difference proposed work prior based models work aims explore potential using biological living neurons functional devices prior works focused computational capability neuron model. neuron engineering design constraints lead different design choice input encoding network topology neuron model learning rule model parameters. neural connectivity human brain complex different types topologies different parts nervous system. understand network functionality simple network topology built input neurons connected output neurons synapses images mnist dataset pixels compressed pixels. result simpliﬁcation network input neurons corresponding pixel. black pixels generate spikes input spikes occur simultaneously. output network vector spiking state output neurons represents spike means spike. output neuron associated group index deﬁned artiﬁcially. index group largest number spiking neurons considered network output. experiment output neurons used. every consecutive neurons belong group example ﬁrst belong group capture realistic neuron dynamics hodgkinhuxley model used simulation models electro-chemical information transmission biological neurons electrical circuit. model successfully veriﬁed numerous biological experimental data biologically accurate simpliﬁed model integrate fire model spike generated membrane potential neuron exceeds certain threshold. however strong current pulse excites spike period current pulse amplitude cannot generate anspikewhich referred refractory period learning rule plasticity synapses neurons important learning. connection strength changes based precise timing prepost-synaptic spikes. phenomenon called spike timing dependent plasticity describe stdp rule used work. rule weight changes proportional spike trace tpre tpost denote time prepost-synaptic respectively spikes arrive; tpre tpost represent arrival time previous prepost-synaptic spikes respectively. amplitudes trace updating potentiation depression respectively; potentiation depression learning rate respectively. pre-synaptic spike arrival update pre-synaptic trace according postsynaptic spike changes post-synaptic trace according pre-synaptic spike happens post-synaptic spike weight decrease given post-synaptic spike occurs pre-synaptic spike weight increase given stdp rule enough learning. unsupervised supervised training algorithms based stdp rule proposed focus computational aspect neuron model. algorithms poisson based spike trains input include bio-inspired mechanisms like winner-take-all homoeostasis since artiﬁcial stimuli precisely applied optically stimulated neurons synchronous inputs used proposed work. considering feedforward network synchronous inputs lead problem. weight decrease network based basic stdp rule neurons eventually. because input neurons time post-synaptic neuron pre-synaptic spike. order solve problem make living neural network easy train propose supervised stdp training algorithm. four basic operations algorithm shown fig. discussed below. algorithm stimuli applied input output neurons artiﬁcially generate spike. fig. external stimuli generate spike input output neurons shown pink blue respectively. without artiﬁcial output stimuli output neuron reach action potential response network inputs shown yellow. output neuron spikes input stimuli weights in-out pairs naturally potentiated. referred network’s natural increase tpre tpost besides natural increase stimuli directly applied input output neuron pairs artiﬁcially change weights. output stimulus given input stimulus given weight pair increased referred artiﬁcial increase. output stimulus given input stimulus artiﬁcial decrease happen weight decrease. increased decreased make network converge. weights already reaching convergence need kept same. however weight synapse large enough make output neuron weight increase naturally training process move network away convergence. therefore training step separated phases input stimuli corresponding input image given phase. ﬁrst phase weight kept actually increased stimulus added corresponding output neurons tn+t input stimulus tn+t second phase decrease increased weight. refractory period natural increase second phase. time interval between input stimuli adjusted time interval natural output spike input stimuli ﬁrst phase amount decrease increase matches. approach weight kept roughly same. process referred artiﬁcial hold. stdp training algorithm described algorithm image observed network prediction made applying input stimuli network checking natural response outputs index group contain largest number spiking neurons predicted result train network correct label input image actual spike pattern output neurons compared generate control signals select neurons different lists require external stimuli based selected neurons stimuli applied network update weights three tunable parameters training protrainstep number training steps network cess. goes image. larger trainstep means higher effective learning rate. order correct group ﬁring neurons groups intarget detarget targets number ﬁring neurons group. intarget represents desired number ﬁring neurons observed output group matches correct label. detarget represents desired number ﬁring neurons observed incorrect groups. numspike number spiking neurons group matches spiking neurons group added holdlist keep weights since respond correctly number ﬁring neurons less intarget intarget-numspike neurons randomly chosen among non-spiking ones added inlist output groups number ﬁring neurons detarget numspike-detarget neurons randomly chosen among spiking ones added delist selecting inlist delist holdlist corresponding stimuli applied time sequence shown fig. training step apply stimuli delist apply stimuli input neurons apply stimuli inlist apply stimuli holdlist tn+t apply stimuli delist tn+t apply stimuli input neurons tn+t apply stimuli inlist tn+t parameters used network listed table timing parameters fig. impact learning rate trainstep. small time interval prepost-synaptic neuron spikes leads greater weight changes. however optical stimuli cannot spaced arbitrarily close other. work ﬁxed interval used between prepost-synaptic spikes artiﬁcial increase decrease. prepost-synaptic spike interval three tunable parameters network trainstep intarget detarget. trainstep kept intarget detarget conﬁgured base line. sensitivity study done tuning parameter time. trainstep increases prediction accuracy increases. however learning rate large accuracy drops. larger effective learning rate lead fast convergence large overshooting happen moving towards global optimum point leads oscillations hurts performance. intarget better results achieved middle range shows that training nearly half neurons provide enough information avoid divergence. example images large overlapping ﬁring neurons inputs different labels strengthening connections ﬁring inputs corresponding outputs image likely lead mis-prediction image. best performance achieved intarget=. decreasing weights associated ﬁring neurons incorrect groups achieves best performance. detarget larger performance drops dramatically. intarget results. number ﬁring neurons incorrect group needs make sure correct group greatest number ﬁring neurons. however training steps cannot guarantee intarget detarget reached. best accuracy sensitivity study larger dataset evaluated based best parameters trainstep= intarget= detarget=. accuracy larger dataset compared single-layer fully connected achieves accuracy mnist dataset proposed supervised stdp-based still accuracy gap. unlike weights inputs directly used prediction models rely output spikes exact membrane potential make prediction. loss information leads accuracy drops. single-layer work knowledge derives extra mathematical function extract informations timing relationships output spikes consider biological properties neurons synapses hence unrealistic scheme living neuron experiment. works based neuron science simulations three-layer design supervised stdp achieved accuracy digit recognition task mnist dataset similar proposed single-layer network paper. works better results mnist however networks least layers larger number neurons works also preprocess input images achieve better accuracy major difference proposed work prior works prior works optimized solid state computers. proposed supervised scheme bioengineering constraints considered. input data compressed applied synchronous spike trains model used instead integrate model. biological limitations maintaining synaptic weights lead design training phases multiple training steps. explore possibility using living neuron machine learning tasks supervised stdp training algorithm proposed simulated fully-connected neural network based model. accuracy achieved digit recognition task mnist dataset. result demonstrates feasibility using living neurons computation elements machine learning tasks. haoxiang xiaohui shen jonathan brandt gang convolutional neural network cascade face detection proceedings ieee conference computer vision pattern recognition taras iakymchuk alfredo rosado-mu˜noz juan guerreromart´ınez manuel bataller-mompe´an jose franc´esv´ıllora simpliﬁed spiking neural network architecture stdp learning algorithm applied image classiﬁcation eurasip journal image video processing vol. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot sander dieleman dominik grewe john nham kalchbrenner ilya sutskever timothy lillicrap madeleine leach koray kavukcuoglu thore graepel demis hassabis mastering game deep neural networks tree search nature vol. jan. karen simonyan ioannis antonoglou huang arthur guez thomas hubert lucas baker matthew adrian bolton yutian chen timothy lillicrap laurent sifre george driessche thore graepel demis hassabis david silver julian schrittwieser mastering game without human knowledge nature zidong daniel ben-dayan rubin yunji chen liqiang tianshi chen zhang chengyong olivier temam neuromorphic accelerators comparison neuroscience machine-learning approaches yevgeny berdichevsky helen sabolek john levine kevin staley martin yarmush microﬂuidics multielectrode array-compatible organotypic slice culture method journal neuroscience methods vol. erkin seker yevgeny berdichevsky matthew begley michael reed kevin staley martin yarmush fabrication low-impedance nanoporous gold multipleelectrode arrays neuralelectrophysiology studies nanotechnology vol. gordon shepherd jason mirsky matthew healy michael singer emmanouil skoufos michael hines prakash nadkarni perry miller human brain project neuroinformatics tools integrating searching modeling multidisciplinary neuroscience data trends neurosciences vol. jason allred kaushik unsupervised incremental stdp learning using forced ﬁring dormant idle neurons neural networks international joint conference ieee damien querlioz olivier bichler philippe dollfus christian gamrat immunity device variations spiking neural network memristive nanodevices ieee transactions nanotechnology vol. amirhossein tavanaei anthony maida minimal spiking neural network rapidly train classify handwritten digits binary -digit tasks international journal advanced research artiﬁcial intelligence vol. michael beyeler nikil dutt jeffrey krichmar categorization decision-making neurobiologically plausible spiking network using stdp-like learning rule neural networks vol. david meyer david kieras computational theory executive cognitive processes multiple-task performance part accounts psychological refractory-period phenomena. psychological review vol. bernhard nessler michael pfeiffer wolfgang maass stdp enables spiking neurons detect hidden causes inputs advances neural information processing systems rebecca lewis katie asplin gareth bruce caroline dart mobasheri richard barrett-jolley role membrane potential chondrocyte volume regulation journal cellular physiology vol.", "year": "2017"}