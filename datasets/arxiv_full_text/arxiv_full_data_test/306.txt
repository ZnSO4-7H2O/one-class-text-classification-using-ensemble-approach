{"title": "Determining the minimum embedding dimension for state space  reconstruction through recurrence networks", "tag": "q-bio", "abstract": " The analysis of observed time series from nonlinear systems is usually done by making a time-delay reconstruction to unfold the dynamics on a multi-dimensional state space. An important aspect of the analysis is the choice of the correct embedding dimension. The conventional procedure used for this is either the method of false nearest neighbors or the saturation of some invariant measure, such as, correlation dimension. Here we examine this issue from a complex network perspective and propose a recurrence network based measure to determine the acceptable minimum embedding dimension to be used for such analysis. The measure proposed here is based on the well known Kullback-Leibler divergence commonly used in information theory. We show that the measure is simple and direct to compute and give accurate result for short time series. To show the significance of the measure in the analysis of practical data, we present the analysis of two EEG signals as examples. ", "text": "analysis observed time series nonlinear systems usually done making time-delay reconstruction unfold dynamics multi-dimensional state space. important aspect analysis choice correct embedding dimension. conventional procedure used either method false nearest neighbors saturation invariant measure correlation dimension. examine issue complex network perspective propose recurrence network based measure determine acceptable minimum embedding dimension used analysis. measure proposed based well known kullback-leibler divergence commonly used information theory. show measure simple direct compute give accurate result short time series. show signiﬁcance measure analysis practical data present analysis signals examples. nonlinear time series analysis important area applied mathematics many practical applications. method involves reconstruction underlying dynamics scalar time series embedding higher dimension using time delay co-ordinates quantiﬁers nonlinear dynamics chaos theory regularly employed quantitative characterization dynamical system underlying time series. example grassberger-procaccia algorithm commonly employed computing widely used invariant correlation dimension important aspect whole analysis identiﬁcation minimum embedding dimension reconstructing dynamics especially real world data information known priori. embedding dimension less actual dimension system computed measures tend inaccurate since dynamics completely unfolded. hand embedding dimension used large number data points time series needs corespondingly large leading excessive computation. also enhances computational error presence additional unwanted dimension dynamics operating. methods commonly adopted present information regarding appropriate embedding dimension. method false nearest neighbors method looks behavior nearest neighbors reference point attractor changes embedding dimension changes number nearest neighbors studied increasing attractor unfolded completely change number nearest neighbors second method compute invariant measure correlation dimension time series increasing shows saturation beyond value chosen minimum dimension compute nonlinear measures. methods eﬃcient commonly employed nonlinear time series analysis though accuracy methods decrease short time series. ∗electronic address kp.hkgmail.com †electronic address rinku.jacob.vallanatgmail.com ‡electronic address rmisraiucaa.in §electronic address g.ambikaiiserpune.ac.in statistical measures based complex network theory increasingly applied analysis approach embedded attractor time series ﬁrst transformed complex network using suitable scheme point attractor identiﬁed node network. transformation done properly show structural properties embedded attractor characterized statistical measures derived complex network. important advantage approach network measures derived accurately small number nodes network hence analysis becomes reasonably accurate even short time series. method frequently employed convert time series complex network makes property recurrence trajectory points state space. this again requires embedding appropriate dimension using delay co-ordinates. nodes corresponding points embedded attractor within recurrence threshold denoted considered connected resulting network. details regarding construction network presented next section. recently shown value used closely connected hence knowledge correct value important accurate implementation scheme converting time series network. network based scheme estimate minimum embedding dimension time series analysis proposed literature. main work propose scheme. measure present based well known kullback-leibler divergence used diﬀerentiate probability distributions whose details discussed next section. measure ﬁrst tested using synthetic time series known dimension standard chaotic systems practical utility explicitly shown using real world data. paper organized follows essential details recurrence network construction discussed kullback-leibler measure introduced. measure tested using time series standard chaotic systems without adding noise practical utility measure also presented using examples real world data. conclusions drawn ﬁrst step compute required measure construct recurrence network time series. un-weighted un-directed complex networks several authors discussed detail convert time series recently proposed scheme used study eﬀect noise chaotic attractors follow scheme. basically parameters involved construction recurrence threshold decides whether nodes network connected embedding dimension scheme choice closely linked value basic criterion used selection resulting network bcomes single giant component. constructed several statistical measures derived directly related structure embedded attractor. speciﬁcally concentrate measure namely clustering coeﬃcient deﬁne local clustering coeﬃcient node degree node number triangles attached node triangle simplest motif present complex network. value measures many nodes connected node also mutually inter-connected value normalized averaging nodes entire network global network. work main focus compute probability distribution local clustering coeﬃcient nodes entire network. procedure repeated increasing embedding dimension beyond since time series limited length compare probability distributions successive values check convergence distributions increases. order quantify diﬀerence probability distributions make measure based well known kullback-leibler divergence widely used information theory diﬀerentiate probability distributions. compute function check convergence respect measure shows convergence beyond taken required embedding dimension system. note basic idea behind procedure much analogous ﬁnding false nearest neighbors. attractor embedded lower dimension actual dimension system false neighbors within recurrence threshold reference node gets connected many nodes real neighbors. consequently clustering coeﬃcient aﬀected. true nodes. beyond actual dimension value remains genuine making probability distribution converge approximately. advantage measure simple straightforward compute derived relatively small number nodes network. divergence usually applied information theory diﬀerentiate probability distributions speciﬁcally divergence denoted measure amount information lost used approximate discrete probability distributions divergence deﬁned average value dimension dimension values added capture diﬀerence displacement proﬁle other. calculation repeated taking distributions successive values time plotted function measure saturates beyond embedding dimension structure attractor unfolds completely. taken minimum dimension embedding. words computes measure instead actual value measure diﬀerent. however ﬁnal result still remains diﬀerence measure successive values starts diverging actual dimension system. checked conﬁrmed this. ﬁrst test measure using synthetic data standard lorenz attractor time series also data added diﬀerent percentage noise applying real world data. time series standard lorenz attractor generated time step data length analysis paper done time series length show potential proposed measure analysis short time series. time series embedded dimension varying time delay equal ﬁrst minimum autocorrelation. constructed embedded attractors using scheme presented case probability distribution also computed. results values shown fig. expected three probability distributions converge approximately since dimension attractor hand computation repeated random time series vary randomly whole interval without showing convergence. results values random also shown fig. latter case attractor tends phase space every changing connections node continuously hence varying chaotic attractor hand global tend saturate beyond certain another important aspect distribution seen fig. varies cases. chaotic attractor inherent geometric structure clustering generally high nodes degree isolated nodes rare. consequently nodes negligible. moreover average number connections node also high. nodes connected node also inter-connected probability low. this turn makes nodes also negligible. contrary random probability extremes comparatively high. hence chaotic attractor generally varies small range probability distribution well deﬁned proﬁle random distributed whole interval. distributions given fig. cumulative distributions shown fig. note distributions values random better diﬀerentiated latter ﬁgure. checked distributions several standard dimensional chaotic attractors found converge quantify convergence compute measure taking distributions successive values time cases. results standard chaotic attractors random shown fig. evident measure correctly identiﬁes dimension attractor. checked saturated values remains constant increase number nodes also shown fig. important issue analysis observed time series contamination noise. important nonlinear measure aﬀected noise added data measure applied real world data. study performance proposed measure noisy conditions apply chaotic data added diﬀerent noise. standard lorenz attractor time series generates data adding white noise undertake analysis time series constructing computing klm. fig. show cumulative distributions values time series added noise noise note that noise level increases convergence distributions shifted higher value. better idea convergence compute function four percentages noise results shown fig. actual converged value value convergence occurs increase noise. moreover beyond noise level convergence seems disappear least upto maximum value used. words measure unable give proper embedding dimension data contaminated moderate high amount noise. however true measures well. example case precise value known clouded noise finally example application real world data consider typical signals healthy person person epileptic seizure. data consist data points shown fig. data studied earlier details regarding generation analysis found andrzejak indications nonlinear signature dynamical properties human brain’s electrical activity particularly epileptic seizure making signal dimensional. construct signals compute distributions diﬀerent embedding dimensions cumulative distributions signals shown fig. measure distributions computed function signals variation shown fig. clear measure signal seizure shows convergence tendency absent healthy signal. indicates latter either high dimensional involves fair amount noise. test this compute value correlation dimension function signals using counting scheme results also shown fig. expected state seizure shows nonlinearity saturating value healthy state keeps increasing shows measure fairly accurate determining embedding dimension dimensional signals. network based measures increasingly applied nonlinear analysis time series data. important advantage measures improved accuracy compared conventional measures analysis short non-stationary data usually obtained real world. present based measure determine necessary embedding dimension used delay-embedding method normally used reconstruct dynamics higher dimension time series. method involves computing probability distributions local clustering coeﬃcients nodes constructed successive embedding dimensions. measure based divergence used determine convergence probability distributions value happens chosen required dimension embedding. eventhough divergence well known measure information theory application nonlinear time series analysis novel. show measure accurately determine dimension standard dimensional chaotic attractors data small amount noise contamination highlight importance measure analysis practical data apply sample signals healthy person epileptic seizure. proposed method analogous method fnn. consider approach fnn. because consider nodes connected reference node deﬁning number nearest neighbors reference point attractor. local clustering coeﬃcient ﬁnding many nodes connected reference node also mutually connected. words many nearest neighbors point also mutually nearest neighbors. thus chaotic attractors number nodes given local remains constant beyond actual dimension. this turn makes probability distribution converge function convergence quantiﬁed using standard measure. finally claim measure accurate enough give proper embedding dimension diﬀerent types data real world. example checked case data high dimension require time series data length large. another possible issue amount noise involved data which priori knowledge. distribution show convergence ﬁrst possible pair values data either high dimensional involves fair amount noise. however dimensional data relatively noise level method simple accurate even limited data length. acknowledge ﬁnancial support science engineering research board govt. india form research project sr/s/hep-/. acknowledges computing facilities iucaa pune. packard crutchﬁeld farmer shaw geometry time series phys. rev. lett. sauer yorke casdagli embedology stat. phys. hilborn chaos nonlinear dynamics abarbanel analysis observed chaotic data grassberger procaccia measuring strangeness strange attractors physica kennel brown aberbanel determining minimum embedding dimension using geometrical eckmann kamphorst ruelle recurrence plot dynamical systems europhys. lett. jacob harikrishnan misra ambika uniform framework recurrence-network analysis chaotic kullback leibler information suﬃciency annals math. statistics kullback information theory statistics donner small donges marwan xiang kurths recurrence based time series analysis fig. probability distributions local clustering coeﬃcients nodes constructed time series lorenz attractor random time series results three diﬀerent embedding dimensions shown cases. fig. panel shows variation kullback-leibler measure computed successive embedding dimensions constructed time series lorenz attractor ueda attractor asterisks corresponding variations measure random time series. cases number nodes used indicated. values chaotic attractors saturated upto shown function bottom panel. fig. figure shows cumulative distributions local aﬀected adding noise chaotic time series. panel obtained adding noise bottom panel added noise time series lorenz attractor. variation embedding dimensions shown fig. variation computed probability distributions signals solid triangle healthy solid circle signal seizure. variation corresponding signals shown bottom panel.", "year": "2017"}