{"title": "The cognitive roots of regularization in language", "tag": "q-bio", "abstract": " Regularization occurs when the output a learner produces is less variable than the linguistic data they observed. In an artificial language learning experiment, we show that there exist at least two independent sources of regularization bias in cognition: a domain-general source based on cognitive load and a domain-specific source triggered by linguistic stimuli. Both of these factors modulate how frequency information is encoded and produced, but only the production-side modulations result in regularization (i.e. cause learners to eliminate variation from the observed input). We formalize the definition of regularization as the reduction of entropy and find that entropy measures are better at identifying regularization behavior than frequency-based analyses. Using our experimental data and a model of cultural transmission, we generate predictions for the amount of regularity that would develop in each experimental condition if the artificial language were transmitted over several generations of learners. Here we find that the effect of cognitive constraints can become more complex when put into the context of cultural evolution: although learning biases certainly carry information about the course of language evolution, we should not expect a one-to-one correspondence between the micro-level processes that regularize linguistic datasets and the macro-level evolution of linguistic regularity. ", "text": "regularization occurs output learner produces less variable linguistic data observed. artiﬁcial language learning experiment show exist least independent sources regularization bias cognition domain-general source based cognitive load domain-speciﬁc source triggered linguistic stimuli. factors modulate frequency information encoded produced productionside modulations result regularization formalize deﬁnition regularization reduction entropy entropy measures better identifying regularization behavior frequency-based analyses. using experimental data model cultural transmission generate predictions amount regularity would develop experimental condition artiﬁcial language transmitted several generations learners. eﬀect cognitive constraints become complex context cultural evolution although learning biases certainly carry information course language evolution expect one-to-one correspondence micro-level processes regularize linguistic datasets macro-level evolution linguistic regularity. languages evolve pass mind another. immersed world inﬁnite variation cognitive architecture constrains perceive process produce. cognitive constraints learning biases shape languages evolve help explain structure language early debate nature biases polarized chomsky’s nativism program explained linguistic structure product language-speciﬁc acquisition device behaviorists claimed generalpurpose learning mechanisms reinforcement learning could explain language acquisition recent experimental research found domain-general learning mechanisms underpin many aspects language learning statistical learning involved word segmentation infants memory constraints modulate learners’ productions probabilistic variation language however likely mixture domain-general domain-speciﬁc mechanisms involved language learning natural languages rarely exhibit free variation regularization behavior language learners likely cause. regularization documented extensively natural language laboratory. natural language regularization occurs children’s acquisition language formation creole languages highly variable pidgin languages formation signed languages historical trends language change laboratory regularization studied depth artiﬁcial language learning experiments children adults focus regularization lexical variation adult learners artiﬁcial language learning paradigm. future research explore whether results generalize regularization child learners. behavioral experiments oﬀer special insight regularization process allow researchers present participants controlled linguistic variation precisely measure participants transform variation test hypotheses causes participants alter patterns variation. example hudson newport investigated regularization pseudodeterminers artiﬁcial language learning experiment. experiment adult participants trained language consisted several verbs several nouns main determiners zero noise determiners training language noun occurred main determiner exposures; remaining exposures equally divided across noise determiners. testing phase participants described scenes using language learned. participants encountered noise determiners training regularized slightly producing main determiners nouns rather observed training language. regularization increased number noise determiners reaching highest level noise determiners main determiners produced nearly nouns. experiment hudson newport showed adult participants regularize artiﬁcial language less noise determiners conditioned particular nouns predictable consistent way. results consistent newport’s less-ishypothesis. originally conceived explanation children regularize adults states learners limited memory capacity regularize inconsistent input diﬃculty storing retrieving forms lower frequency used less consistently. regularization behavior varies considerably children adults however regularization memory limitations also apply adults albeit lesser degree overall less-is-more hypothesis constitutes domain-general account linguistic regularization terms cognitive constraints memory encoding retrieval. hypothesis describes truly domaingeneral eﬀect expect kind regularization behavior non-linguistic domains. gardner conducted frequency prediction experiment adult participants predict several lights would ﬂash given trial. participants observed lights ﬂashing random ratio probability matched ratio predictions meaning guesses light would ﬂash next guesses light also probability matched observing ratio. however participants trained three lights regularized over-predicting frequent light under-predicting less frequent lights similar behavior hudson newport’s participants. another experiment kareev report eﬀect individual diﬀerences working memory capacity participants’ perception correlation probabilistic variables. participants lower memory capacity overestimated common variant whereas participants higher capacity not. similarly dougherty hunter show participants lower working memory less likely consider alternative choices eight-item prediction task also less likely consider low-frequency alternatives participants higher working memory. cases identiﬁed regularization higher-frequency variants over-represented participants’ behavior. therefore strong evidence existence domain-general drivers regularization extent account level regularity observe language clear. domainspeciﬁc learning mechanisms play role interact general mechanisms. example perfors presented seven carefully controlled manipulations cognitive load encoding stage artiﬁcial language learning task found eﬀect regularization behavior. suggests less-ishypothesis apply retrieval storage eﬀects working memory found non-linguistic experiments kareev dougherty hunter operate strongly language learning. furthermore reali griﬃths show eﬀect domain regularization behavior participants reduce variation learning words increase variation learning coin ﬂips. however cognitive load lower coin ﬂipping condition unclear whether higher cognitive load linguistic domain caused participants regularize word learning task. vary number stimuli learner must track concurrently. manipulate task domain manipulating type stimuli learner must track objects named words marbles drawn containers method closely based artiﬁcial language learning experiment reali griﬃths high load linguistic condition replicates experiment although compare regularization behavior particular linguistic task particular non-linguistic task diﬀerences regularization behavior revealed comparison constitute existence proof general language-speciﬁc drivers regularization behavior. little known regularization behavior compares across diﬀerent levels language systematic study date reports global diﬀerence regularization behavior across levels two-by-two design easily extended various linguistic tasks diﬀerent levels language appropriately matched nonlinguistic tasks determine generalizability present results. based work reviewed above predict regularization behavior increase cognitive load raised. also predict regularization behavior increase task presented linguistic stimuli. however clear prediction existence interaction domain cognitive load relative amount variation removed data load domain. knowing relative contribution domain-general domain-speciﬁc biases structure language important tells much ground theories language learning general mechanisms memory statistical learning. order address questions need principled measure regularization comparable across diﬀerent distributions variation stimuli domains. section provide measure formalizing definition regularization reduction entropy data set. readers skip section willing accept following statement amount variation participant regularizes equal drop entropy productions relative observations. section present experimental method design. section present main result followed three supporting analyses explore regularization behavior greater depth. section empirical data investigate evolution regularity learners’ biases repeatedly applied model cultural transmission. gives sense predictive known regularization biases level regularity found culturally-transmitted behaviors languages. existing literature regularization described elimination reduction free variation. therefore deﬁne regularization terms lost variation quantify amount variation lost learners’ productions compared data learners observed. amount variation data quantiﬁed information-theoretic notion entropy growing number studies using entropy measures analyze regularization behavior linguistic variants question probability distribution variants probability variant set. example take probability distribution determiners used noise determiner condition hudson newport artiﬁcial language learning distribution shannon entropy bits change variation bits. means bits variation among determiners regularized participant. intuitively determiners regularized participant. knows class noun better able predict determiner speaker language noun. variation distribution items conditioning variable taken account quantiﬁed conditional entropy entropy variants context weighted probability context. assume moment distribution determiners conditioned noun class meaning determiners probabilities regardless noun class conditional entropy mapping bits identical entropy determiners themselves noun class carries information determiner used. contrast another mapping mapping determiner conditioned noun exclusively produced noun class third fourth determiners exclusively produced noun class conditional entropy mapping entropy determiners remains bits. participant trained language mapping produced language mapping would regularized bits variation mapping joint probability observing variant context together. looking figure possible imagine joint entropy system increase moving circles away another. circles move apart carry less information another. eﬀect increasing conditional entropy values reducing mutual information mutual information measure variation structure measures much uncertainty reduced deﬁne regularization reduction space figure regularization occur eliminating linguistic variants eliminating conditioning contexts increasing degree variants contexts conditioned analways decreases loss variation. mutual information hand necessarily change regularization occurs. following experiment construct stimuli lexical items variants objects refer contexts. matched non-linguistic stimuli marbles variants containers drawn contexts. ation mapping linguistic variants contexts. figure shows quantities relevant complete description variation linguistic mapping. largest quantity total amount variation mapping equal area covered overlapping circles. joint experiment manipulate cognitive load task domain allowing quantify amount variation participants regularize source. participants observe input mapping among stimuli participants recruited amazon’s mechanical turk crowdsourcing platform completed experiment online. informed consent obtained experimentation. participant location restricted veriﬁed post-hoc check participant address location. participants excluded basis following criteria failing ishihara color vision test self-reporting pencil task exit questionnaire reporting self-reporting previously participated related experiments determined user mturk participants recruited necessary expectation would excluded criteria. predetermined number participants condition last participants excluded participants received full monetary reward participating task one-item conditions six-item conditions average time taken complete oneitem conditions minutes seconds standard deviation minute seconds. average time complete six-item conditions minutes seconds standard deviation minutes seconds. ﬁnal participants reported female reported male mean years standard deviation years. experiment coded java applet participant’s browser x-pixel ﬁeld. photographs diﬀerent containers computer-generated images marbles diﬀerent colors served non-linguistic stimuli. photographs diﬀerent novel objects diﬀerent nonsense words served linguistic stimuli. stimuli chosen similar visual complexity across domain marbles words organized ﬁxed pairs maximized distinctiveness stimuli pair. stimuli lists appear order pairings marble colors paired diﬀer brightness. withinpair diﬀerences greater within-pair brightness diﬀerences greater words paired contrastive. within-pair words utilized diﬀerent letters vowels within-pair consonants diﬀered place articulation. stimuli closely based word stimuli used reali grifﬁths selected look sound like existing words pronounced american english speaker. words presented visually accompanied auditory stimuli. non-linguistic single frequency learning participants observed marble colors drawn container particular ratio participants asked demonstrate another several draws container likely look like. asked predict speciﬁc future draws thus feedback given. participants observed marble draws produced marble draws. participant observed draws possible ratios constitute input ratio conditions. refer ratio participant observed input ratio ratio participant produced output ratio. participants input ratio condition totaling participants marbles. container stimuli randomized across participants participant containers. equal numbers participants container. marble pairs also randomized across participants participant marble pairs. equal numbers participants marble pair. variant pair randomly assigned majority variant full details observation production regimes found section figure non-linguistic multiple frequency learning condition similar marbles condition diﬀerence participants observed produced draws diﬀerent containers container diﬀered ratio marble colors. containers marble pairs input ratios randomly assigned another without replacement assignments randomized participants. participant containers marble pairs input ratios participants condition yielding data input ratios. figure schema experiment’s observation production phases. cognitive load condition. bottom high cognitive load condition. examples shown linguistic condition. non-linguistic condition containers shown place object marbles shown place words. linguistic single frequency learning condition similar marbles condition diﬀering linguistic stimuli instead non-linguistic stimuli minimal adaptation instructions linguistic domain. participants observed object named words particular ratio asked name object like observed being named. asked predict speciﬁc future namings thus feedback given. participants observed namings produced namings. possible input ratios observed participants totaling participants. linguistic multiple frequency learning condition similar marbles condition diﬀering linguistic stimuli minimal adaptation instructions linguistic domain. condition constitutes replication word learning experiment reali griﬃths diﬀerent object stimuli modiﬁed word stimuli participants completed experiment online rather laboratory. participants condition yielding data input ratios. figure screen shot sliders page high cognitive load linguistic condition showing three answers selected. participants could change answers save answers clicked. back took participants back question instruction sliders. load condition slider shown. observation production trials would observation phase marble/word stimuli presented random order. high load conditions containers/objects presented random order. production trial left-right location marbles/words randomized. participant moused answer pixel displayed around choice. clicked remained table co-occurance frequencies among twelve variants contexts experimental stimuli set. cell gives number times participant observed varianti along contextj button appeared equidistant choices. participants could change answer clicked conﬁrm ﬁnal response. choice shown container/object next trial began. button served re-center participant’s cursor trials. production phase participants asked estimate generating ratio underlies input ratio saw. accomplished asking many marbles color container often word said object artiﬁcial language. participants provided response discrete slider options relative percentages participant observes stimuli composed co-occurrances marbles containers words objects. purpose quantifying variation stimuli sets consider marbles words variants consider containers objects contexts. table shows co-occurrance frequencies contexts variants. high cognitive load conditions table describes complete stimuli participant trained observation phase. cognitive load conditions participant trained table. figure shows entropy values associated table describes population-level variation stimuli. values across conditions allowing direct comparison mean change entropy conditions. entropy stimuli participant observes high cognitive load condition identical figure however entropy stimuli cognitive load condition lower varies input ratio observed condition figure entropy training stimuli linguistic condition distribution words distribution objects. non-linguistic condition distribution marbles distribution containers. refer back section deﬁnition quantity. experiment designed participants change size outer circle only. first describe participant behavior present main result cognitive load linguistic stimuli elicit regularization behavior. next three supporting analyses explore regularization behavior greater depth section shows participants’ regularization behavior production biases rather encoding bias section analyzes individual diﬀerences regularization experiment section shows primacy effects help explain individuals regularized minority variant rather majority variant. analyzing data terms entropy ﬁrst visually inspect participants changed input ratio. figure panel shows distribution ratios participants produced response input ratio observed experimental condition. ﬁrst shows clear probability matching behavior mean mode participant responses near input ratio. participants condition tended successfully reproduce input ratio small amount error. second shows clear regularization behavior. participants condition moved distributional mass away input ratio toward maximally regular ratios responses input ratio seem combination probability matching behavior regularization behavior figure shows results experimental condition. column corresponds input ratios ranging pane contains distribution output ratios participants produced response input ratio. output ratios displayed x-axis number times participant produced variant input ratio variant corresponds whatever marble/word majority observation phase. input ratios indicated dashed line. roughly half participants appear probability matched error rates similar marbles roughly half participants appear regularized levels comparable marbles. input condition none participants choose unseen word production trial. fourth shows similar regularization proﬁle marbles extreme movement distributional mass edges majority participants produced maximally regular ratios. condition constitutes successful replication ﬁrst experiment reported reali griﬃths section report diﬀerences regularization behavior within four experimental conditions. calculating change shannon entropy pair input-output ratios obtained participants. example participant observes ratio orange blue marbles produces ratio orange blue marbles shannon entropy pair input-output ratios assess signiﬁcance diﬀerences regularization within conditions linear mixed eﬀects regression analysis performed using dependent variable change entropy input-output ratios. experimental condition independent variable. participant entered random eﬀect obvious deviations normality homoscedasticity apparent residual plots. within-condition changes assessed re-leveling model obtain intercept value condition. intercept equals condition’s mean change entropy regression analysis provides t-statistic figure entropy drops learners regularize. shows average change shannon entropy pairs input-output ratios condition. stars indicate signiﬁcant diﬀerence zero. error bars indicate conﬁdence intervals computed bootstrap percentile method signiﬁcant drop entropy means participants regularized condition. nonsigniﬁcant diﬀerences zero obtained participants probability match. lower upper bounds mean entropy change figure changes frequency fail capture regularization behavior. shows average diﬀerence number times participants observed majority variant training number times produced variant testing phase. error bars indicate conﬁdence intervals computed bootstrap percentile method values significantly higher zero indicate population-level trend overproducing majority variant. values signiﬁcantly lower zero indicate population-level trend over-producing minority. evaluate whether mean signiﬁcantly diﬀerent zero. three four experimental conditions elicited signiﬁcant amount regularization behavior participants regularized average bits marpairwise comparison regularization conditions also obtained re-leveled model. pairwise comparisons showed signiﬁcant diﬀerence regularization behavior level except words marbles eﬀects experimental manipulations assessed constructing full linear mixed eﬀects model three independent variables interaction domain cognitive load entropy input ratio. dependent variable change entropy input-output ratios. participant entered random eﬀect signiﬁcance ﬁxed eﬀect determined likelihood ratio tests full model reduced model omits eﬀect question. signiﬁcant eﬀect domain cognitive load input ratio interactions ﬁxed eﬀects also determined likelihood ratio tests comparing reduced model includes interaction interest. interactions found signiﬁcant cognitive load input ratio domain input ratio interaction domain cognitive load signiﬁcant therefore best-ﬁt model contained interaction domain input ratio interaction cognitive load input ratio additive relationship domain cognitive load input entropy increases bits output entropy much linguistic regularization literature date regularization measured terms stimulus frequency rather entropy. section repeat analyses section diﬀerent dependent variable change frequency majority variant illustrate diﬀerence approaches. figure shows mean change frequency majority variant example participant produces ratio response input ratio change majority variant frequency pair input-output ratios. input condition random variant encoded majority variant. positive changes mean participants over-produced majority variant negative changes mean participants over-produced minority variant. figure none conditions elicit overproduction majority variant average despite fact participants marbles words words clearly regularizing input ratios ever frequency-based analysis reveal something entropy-based analysis unable capture signiﬁcant over-production minority variant marble-drawing domain marbles determine eﬀects experimental manipulations apply analysis section change majority variant frequency signiﬁcant eﬀect domain input frequency change majority variant frequency signiﬁcant eﬀect cognitive load also signiﬁcant interaction domain input frequency therefore bestﬁt model contains eﬀect domain input frequency interaction domain input frequency summary frequency analysis fails capture eﬀect cognitive load regularization behavior fails capture fact participants eliminating variation linguistic domain. reason mean change frequency diﬀerent zero linguistic domain participants sometimes regularized majority variant times regularized minority variant tends cause frequency changes average zero. however clear data would incorrect conclude participants probability matching linguistic domain. figure production bias encoding bias drives regularization. dark grey average diﬀerence regularity input ratios participants actually observed estimates underlying ratio generated input ratio. signiﬁcant increase entropy means participants estimated underlying ratio variable input ratio signiﬁcant decrease means estimated regular. light grey average diﬀerence production ratio regularity estimated ratio regularity. error bars indicate conﬁdence intervals computed bootstrap percentile method discussed introduction regularization behavior often explained result general cognitive limitations memory encoding and/or retrieval. high cognitive load manipulation experiment affected observation production phases phases consisted interleaved trials. therefore regularization behavior observed could encoding multiple frequencies load and/or retrieving frequencies load furthermore possible linguistic domain speciﬁc eﬀect encoding frequency information. determine whether encoding errors contribute participants’ regularization behavior experiment asked participants estimate underlying ratio generated marble draws naming events observed container object participants’ estimates signiﬁcantly diﬀerent ratios observed assume frequency encoding unbiased. result would point productionside driver regularization. figure shows average change entropy participants’ estimates actual input ratios observed. linear mixed eﬀects regression analysis described section applied data using change input estimate entropy dependent variable. condition marbles elicited signiﬁcant diﬀerence input ratios estimates condition participants estimated generating ratio signiﬁcantly variable ratio observed indicating slight encoding bias toward variability. none conditions show bias toward regularity participants’ estimates. eﬀects experimental manipulations assessed procedure described section using change input estimate entropy dependent variable. best-ﬁt model contained signiﬁcant eﬀect domain cognitive load input ratio interaction found signiﬁcant predictor participants’ estimates cognitive load input ratio interactions domain input ratio domain cognitive load signiﬁcant. although estimate data shows bias toward regularity factors aﬀected regularization behavior also aﬀect participants’ estimates. additionally cognitive load manipulation resulted noisier estimates whereas domain manipulation suggests high load condition indeed diﬃcult load condition domains wellmatched terms diﬃculty stimuli complexity. figure shows diﬀerence entropy ratio participants produced estimate ratio i.e. extent productions regular estimate input data. linear mixed eﬀects regression analysis described section applied data using diﬀerence entropy produced estimated ratios dependent variable. conditions production ratios signiﬁcantly regular estimates participants made means regularization occurs during production phase likely involved retrieval frequency information. interestingly production-side regularization occurs four conditions even marbles participants probability matched productions inputs suggests regularity broadly associated frequency production behavior even cases lead overt regularization behavior. summary raising cognitive load resulted noisier encoding however noise biased direction regularity. estimates linguistic domain biased toward regularity either. appears bulk regularization occurs production-side experiment likely involve processes frequency retrieval use. bimodal distributions output ratios suggest individual diﬀerences frequency learning strategies. break frequency learning behavior three categories regularizing probability matching variabilizing. many participants fall category? high load conditions participants respond item consistent responses strategy? deﬁne probability matching sampling input ratio replacement. leads output ratios binomially distributed mean although single likely output ratio participant could sample input ratios itself probability matchers sample ratio higher lower entropy input ratio. classify participants produced ratios within conﬁdence interval sampling replacement behavior probability matchers. classify participants variabilizers produced ratios signiﬁcantly higher entropy likely probability matching behavior. could participants attempting produce maximally variable randomly selecting among choices production trial. likewise classify participants regularizers produced ratios signiﬁcantly lower entropy likely probability matching behavior. important note participant weak bias regularity variability consistently produce data falls within conﬁdence range probability matching. however take conservative approach grouping individuals regularizers variabilizers probability matching probability. load conditions participants sample ratio conﬁdence intervals output ratios determined clopper-pearson exact method. high cognitive load conditions participants sample ratios classify ratios according conditional entropy figure linguistic non-linguistic stimuli evoke diﬀerent frequency learning strategies. data high cognitive load conditions marbles words x-axis shows participant number sorted conditional entropy y-axis shows frequency majority variant participant’s output; point represents performance single container/object therefore points participant. shaded region contains participants classiﬁed probability matchers. participants left shaded region classiﬁed regularizers participants right classiﬁed variabilizers. table shows number participants fell frequency learning category condition. strategies represented within experimental condition. signiﬁcant eﬀect cognitive load domain distribution frequency learning strategies meaning experimental manipulations elicit diﬀerent frequency learning strategies participants. fewer data points collected participants load condition probability matching behavior easily ruled hence high number participants classiﬁed probability matchers marbles words. possible diﬀerence dataset size high conditions responsible signiﬁcant eﬀect load. eﬀect domain however reliably experimental manipulation. therefore remainder section focuses high load data. found non-linguistic domain probability matchers seem equally likely found either domain. extreme left x-axis subset regularizers numbering participants marbles words produced maximally regular participants produced maximally variable participants likely maximally regularize linguistic condition although participants regularized majority variant exclusively participants regularized minority variant exclusively. points range y-axis correspond output ratios contained large number minority variant productions participants regularized minority variants majority variants. summary found frequency learning strategies regularizing probability matching variabilizing present condition linguistic stimuli causes participants consistently regularize. studies regularization often participants regularize over-producing over-predicting majority variant serves standard deﬁnition regularization however many studies report participants regularize minority variant causes participants regularize majority variant others regularize minority variant? previous section minority regularization individual diﬀerences frequency learning behavior. minority regularization feature individuals feature training data received. possible data-driven explanation minority regularization lies eﬀects stimulus’s primacy recency participant behavior. observation phase participants presented randomly-ordered sequence variants probability particular variant occurring beginning input sequence proportional frequency sequence. therefore participants would received minority variants toward beginning and/or sequence whereas others would not. many experiments serial recall lexical items show participants better recalling ﬁrst last items list words eﬀect also extends learning mappings between words referents poepsel weiss found participants cross-situational learning task confronted several possible synonyms object conﬁdence correct mapping positively correlated primacy mapping observation phase. therefore investigated eﬀect minority variant’s position input sequence participants’ tendency regularize minority variant. unlike research primacy recency input sequences consist variants presented several times each. therefore quantify strength minority primacy imbalance variants across input sequence. this notion torque. analogy consider input sequence weightless lever length consider minority variant weight unit placed lever according observation trial number assume lever balanced fulcrum center. distance weights located right center minus distance weights left center torque. following standardization torque refer primacy score sequence weights distance weight start sequence. input sequences random variant coded minority variant. length total number minority variants sequence. positive values mean minority variants occur toward beginning sequence negative values mean occur toward sequence. maximum primacy score minimum average primacy score obtained sequence balanced example primacy score sequence table number regularized production sequences condition. parentheses show number minority-regularized sequences percentage regularized sequences. primacy analyses restricted input sequences participants regularized. table shows breakdown number regularized production sequences experimental condition participants regularized total input sequences. previous sections showed learners regularize novel word frequencies domain-general domain-speciﬁc constraints. accomplished analyzing cycle learning spans perception processing production variants. although informs relevant constraints underpin regularity word learning even much regularity constraint imposes given data necessarily tell much regularity expect linguistic variants time. languages transmitted generations learners therefore subject multiple learning cycles individual opportunity impose amount regularity language. section address complex relationship regularization biases level regularity found culturally transmitted data. particular focus evolution regularity marbles words conditions conditions elicited similar amounts regularization behavior diﬀerent causes domain-general domain-speciﬁc constraints frequency learning. would data culturally transmitted conditions high cognitive load linguistic framing ultimately acquire amount regularity? answer question explore dynamics change existing data using iterated learning model cultural transmission output learner serves input several cycles iterated learning result walk complex landscape constraints shape transmitted behavior several walks used estimate landscape likely evolutionary trajectories. griﬃths kalish shown iterated learning equivalent markov process discrete-time random process sequence values random variable vt=n random variable determined recent value describes memoryless time-invariant process previous value inﬂuence current value case iterated learning chains learners observe behaviors previous generation. possible values random variable constitute state space system. markov process fully speciﬁed probabilities state lead every state probabilities states represented transition matrix probabilities landscape culturally transmitted dataset evolves. figure plots primacy scores sequences regularized majority variant sequences regularized minority variant constructed logit mixed eﬀects model regularization type function primacy score. participant entered random eﬀect likelihood ratio test performed model reduced model omits primacy score predictor. found signiﬁcant eﬀect primacy score regularization type average primacy score points higher sequences regularized minority. means participants likely regularize minority variant toward beginning input sequence however minority regularization entirely explained minority primacy. seen figure minority regularization obtained across primacy scores even minority maximally recent summary found participants minority variant toward beginning observation phase likely regularize minority variant. helps explain individual diﬀerences regularization behavior grounding diﬀerences properties data participant observed. figure data experiment used predict cultural evolution regularization. estimated transition matrices experimental condition contain probabilities learner produces given output ratio given input ratio shading cells denote transition probabilities states. matrix corresponds distribution output ratios produced response input ratio example marbles transition matrix corresponds upper left panel figure probability transitioning st−= equivalent proportion participants produced ratio trained ratio. likewise rows correspond panel figure distribution ﬂipped display results terms minority variant. bottom stationary distribution shows percentage learners produce output ratio ratios evolved arbitrarily large number generations. stationary distribution solution matrix input ratio output ratio. experiment designed could estimated four experimental conditions collecting data participants eleven possible states. figure shows estimated transition matrix experimental condition. estimation consists data condition smoothed small value length cell matrix gives transition probability state si=t− state sj=t. transition matrices used estimate regularity data arbitrarily large number learning cycles. matter start state used initialize iterated learning chain arbitrarily large number iterations converge stationary distribution stationary distribution deﬁned meaning data take form stationary distribution serve input output distribution subsequent generations data change anymore. stationary distribution probability distribution states system probability corresponds proportion time system spend state solved matrix decomposing matrix eigenvalues eigenvectors proportional ﬁrst eigenvector. figure shows stationary distribution transition matrix. distributions arbitrarily long iterated learning chain produce maximally regular ratios approximately time participants learning marbles containers approximately time participants learning words object diﬀerence stationary distributions means evolutionary dynamics experimental conditions diﬀer. calculate level regularity stationary distribution multiplying shannon entropy ratio probability observing state psi). results bits conditional entropy marbles bits marbles bits words bits words. compare values results experiment bits marbles bits marbles bits words bits words. figure plots values terms entropy change difference mean input entropy mean regularization individual learners. looking data individual learners even infer cognitive load linguistic domain inject similar amounts regularity language. however fact words higher stationary regularity marbles means least terms present data amount regularity ultimately expect language simply predicted learner’s biases. instead process cultural transmission indispensable piece puzzle explaining learning biases shape languages. regularity language rooted cognitive apparatus learners. paper shown linguistic regularization behavior results least independent sources cognition. ﬁrst domaingeneral involves constraints frequency learning cognitive load high. second domain-speciﬁc triggered frequency learning task framed linguistic stimuli. cognitive load manipulated varying number stimuli frequency learning task. participants observed produced stimuli regularized stimuli frequencies average observing producing fewer stimuli. result held stimuli non-linguistic stimuli linguistic previously observed separate non-linguistic linguistic experiments shown within experimental setting identical distributions variation increasing cognitive load causes participants regularize non-linguistic linguistic stimuli. furthermore shown participants regularize similar amount variation cases eliminating variation marbles conditioned containers variation words conditioned objects. similarity suggests learners general limits amount variation process reproduce independent learning domain. quite possible cognitive load makes ﬁxed contribution regularization behavior however remains seen whether result holds diﬀerent learning domains cognitive load manipulations. possible alternative explanation cognitive load eﬀect regularization behavior diﬀering length tasks. design kept duration stimulus constant across conditions rather total task duration unknown stimuli presentation length affects regularization behavior. however possible participants’ attention lower high cognitive load tasks causing over-produce stimuli early training phase. given ﬁnding primacy aﬀects minority regularization recency could case although eﬀect quite small. future research address eﬀect figure learning biases lead diﬀerent degrees regularization many generations cultural transmission. dark grey average change entropy learning cycle light grey average change entropy variants convergence stationary distribution error bars indicate conﬁdence intervals computed bootstrap percentile method resamples transition matrix matrix solved stationary distribution mean change entropy. output entropy learning cycle convergence stationary distribution that despite showing similar mean entropy change experiment regularization biases involved marbles words ultimately produce diﬀerent levels regularity cultural transmission diﬀerent distribution probabilities within transition matrices. probabilities constitute different landscapes attract iterated learning chains diﬀerent regions state space. reason words data regularizes data sets markedly lower probability transitioning states trapping generations learners highly regular region longer amounts time. summary shown regularity elicited diﬀerent constraints frequency learning similar generation learners displays diﬀerent evolutionary dynamics simulated cultural transmission. ﬁnding important implications relationship learning biases structure language means culturally transmitted systems language necessarily mirror biases learners previously showed cognitive load linguistic domain independent sources domain manipulated varying type stimuli used frequency learning task. participants observed produced mappings words objects regularized participants observed produced mappings marbles containers. participants appear higher baseline regularization behavior learning linguistic stimuli additional variation regularized linguistic domain cognitive load condition linguistic stimuli trigger number domain-speciﬁc learning mechanisms production strategies. possibility stimuli manipulation changed participants’ pragmatic inferences frequency learning task. artiﬁcial language learning task perfors showed participants regularize believe variation labels objects result typos suggesting participants likely maintain variation think meaningful. possible participants make diﬀerent assumptions importance variation marbles versus words required demonstrate learned experimenter. however clear assumptions another possibility linguistic stimuli encourages participants consider communicative consequences variation. participants artiﬁcial language learning tasks regularize allowed communicate another even erroneously believe communicating another participant suggests participants strategically regularize variation situations potentially communicative reason regularization observed wide range language learning tasks including present study. non-linguistic stimuli fully participants non-linguistic task framing participants saying orange blue observe marbles verbalizing rule there blue marbles jar. humans rely language solving complex problems trigger linguistic representations stimuli become complex regardless learning domain. following logic increase cognitive load could make task linguistic. would change interpretation results degree-diﬀerences regularization behavior degree-diﬀerences amount linguistic representation involved. adopting interpretation experiment however would require overhaul deﬁnition domain-general learning mechanisms statistical learning. container object observed. found factors aﬀected production data also aﬀected estimates however estimates signiﬁcantly regular input ratios participants observed. suggests participants access somewhat accurately encoded frequency information making estimates. participants regularized productions without showing corresponding bias estimates implies bulk regularization occurred production phase task. production-side interpretation line results hudson chang showed adult participants regularize stimuli retrieval made harder; perfors found adult participants regularize encoding made harder; schwab show children regularize production despite demonstrated awareness word forms used training. result also suggests less-is-more hypothesis states learners regularize fail encode store retrieve lower-frequency linguistic forms applies retrieval less encoding. however possible biased encoding could result complex mappings used experiment. alternative explanation diﬀerence estimates productions could types data elicited participants. likely estimation question elicited explicit knowledge stimuli frequencies production task elicited implicit knowledge case would mean participants’ explicit knowledge observed frequencies accurate implicit knowledge imply regularization behavior closely associated implicit knowledge retrieval. paper also explored topic minority regularization depth. found minority regularization result individual diﬀerences. although participants diﬀer frequency learning strategies participants regularized minority variants. therefore investigated diﬀerences randomized stimuli participant found participants signiﬁcantly likely regularize minority variant occurs toward beginning observation sequence. primacy eﬀect line results poepsel weiss showed participants vouloumanos found learners able encode retrieve ﬁne-grained diﬀerences statistics low-frequency mappings words objects failed encode retrieve ﬁne-grained diﬀerences complex stimuli joint entropy high cognitive load mappings bits within vouloumanos demonstrated threshold accurate frequency representation. cross-situational learning task higher conﬁdence correctness mapping words referents items co-occurred early observation phase. also demonstrated minority regularizers confound regularization analyses based majority variant’s change frequency argue regularization deﬁned exclusively overproduction highest-frequency dominant form. alternative analyses overcome issue perfors regularization index entropy-based analyses several pros cons using entropybased analyses. regularization occurs whenever learners increase predictability linguistic system therefore directly equates system’s decrease entropy. entropy measures allow quantify linguistic variation directly mathematically-principled based predictability. also allow quantify variation linguistic variants once. analyses based majority-variant frequency tell changes subset variants language. overproduction majority forms certainly cause language’s entropy drop regularity also increase minority forms overproduced forms maintained conditioned linguistic contexts meanings. entropy measures also force explicit type linguistic variation analyzing allow direct comparison between diﬀerent experiments however entropy frequency analyses sensitive diﬀerent aspects linguistic data. entropy better quantifying regularization positively identifying whereas frequency better detecting populationlevel trend overunder-producing particular variant. example frequency method used section capture eﬀect cognitive load frequency learning behavior capture interesting domain diﬀerence entropy analysis missed marble drawers overproduced minority variant average whereas word learners not. methods also show diﬀerences classiﬁcation probability matching behavior entropy method identiﬁed marbles consistent probability matching behavior frequency method raises important questions nature probability matching deﬁned reproducing amount variation reproducing amount variation along correct mapping variation stimuli overall paper explored various cognitive constraints frequency learning give rise regularization behavior. detailed knowledge constraints tell regularity languages? possibility relationship constraints learning structure languages straightforward learning biases directly read typology languages world probability learner ends speaking given language read probability language prior conditions however cultural transmission distorts eﬀects learners’ biases data transmit making impossible simply read learning biases language universals often distortion increases eﬀects bias time weak biases strong eﬀects structure culturally transmitted data however opposite also occur biases weaker eﬀects eﬀects suggests cultural transmission increases complexity relationship individual learning biases structure language. plugging data obtained population participants model cultural transmission also found complex relationship regularization biases regularity culturally transmitted datasets. although participants produced similar amounts regularity response domain load manipulations domain manipulation resulted signiﬁcantly higher regularity data culturally transmitted. future research explore whether similar eﬀects domain demand regularization behavior seen child rather adult learners. possible child learners would show diﬀerent relative contributions domain general domain speciﬁc biases adults therefore extent language change shaped speciﬁcally biases child acquisition implications role languagespeciﬁc domain-general biases shaping evolution linguistic regularity. learners observe reproduce probabilistic variation regularize cognitive load high stimuli linguistic. conclude linguistic regularization behavior coproduct domain-general domain-speciﬁc biases frequency learning production. furthermore load domain aﬀect participants encode frequency information. however encoded frequencies regular data participants observed bulk regularization occurs participants produce data. finally show relative contributions load domain regularity linguistic variants change data transmitted culturally. order understand various regularity biases create regularity language experiments quantify learning biases need coupled cultural transmission studies. research supported university edinburgh’s college studentship sorsas award engineering physical sciences research council. writeup supported omidyar fellowship. reported experiment conducted approval linguistics english language ethics committee university edinburgh. thank bill thompson griﬃths florencia reali simon dedeo luke maurits members centre language evolution feedback project. thank daniel richardson providing experimental stimuli thank reviewers perfors anonymous excellent comments.", "year": "2017"}