{"title": "Role of zero synapses in unsupervised feature learning", "tag": "q-bio", "abstract": " Synapses in real neural circuits can take discrete values, including zero (silent or potential) synapses. The computational role of zero synapses in unsupervised feature learning of unlabeled noisy data is still unclear, thus it is important to understand how the sparseness of synaptic activity is shaped during learning and its relationship with receptive field formation. Here, we formulate this kind of sparse feature learning by a statistical mechanics approach. We find that learning decreases the fraction of zero synapses, and when the fraction decreases rapidly around a critical data size, an intrinsically structured receptive field starts to develop. Further increasing the data size refines the receptive field, while a very small fraction of zero synapses remain to act as contour detectors. This phenomenon is discovered not only in learning a handwritten digits dataset, but also in learning retinal neural activity measured in a natural-movie-stimuli experiment. ", "text": "synapses real neural circuits take discrete values including zero synapses. computational role zero synapses unsupervised feature learning unlabeled noisy data still unclear thus important understand sparseness synaptic activity shaped learning relationship receptive ﬁeld formation. here formulate kind sparse feature learning statistical mechanics approach. learning decreases fraction zero synapses fraction decreases rapidly around critical data size intrinsically structured receptive ﬁeld starts develop. increasing data size reﬁnes receptive ﬁeld small fraction zero synapses remain contour detectors. phenomenon discovered learning handwritten digits dataset also learning retinal neural activity measured natural-movie-stimuli experiment. sparsity either neural activity synaptic connectivity plays important role sensory information processing across brain areas sparsity constraints imposed neural activity sparse coding model reproduce gabor-like ﬁlters resemble receptive ﬁelds simple cells mammalian primary visual cortex. sparse representation also applied deep belief networks model hierarchical representations natural image statistics capture higher order features deeper levels cortical hierarchy. addition perspective optimal information storage must exist large fraction silent potential synapses consistent existence synapses cortex cerebellum synapses vital plasticity learning therefore sparse representation synaptic connectivity also appealing optimal neural computation. artiﬁcial neural networks trained supervised learning requires large library images prelabeled categories. however unsupervised learning gives humans non-human animals ability make sense external world themselves without additional supervision. thus unsupervised learning aims extracting regularities sensory inputs without speciﬁc labels. ﬁgure learning algorithms used modeling external world unsupervised important designing machine intelligence. however understanding computational mechanisms unsupervised learning sensory representation extremely challenging although zero synapses observed real neural circuits computational role concept formation unsupervised learning remains unclear lacks simple model explain. furthermore previous theoretical eﬀorts focused random models neural networks distribution synaptic values preﬁxed. here propose simple model unsupervised learning zero synapses two-layered neural network introduced learn synaptic values sensory inputs thus practical random models. bottom layer composed visible neurons receiving sensory inputs layer contains hidden neuron response speciﬁc features inputs. bottom layer connected layer synapses. note exist lateral connections bottom layer. kind neural network called one-bit restricted boltzmann machine binary synapses also experimentally observed real neural circuits one-bit binary synapses studied model unsupervised feature learning analytically tractable mean-ﬁeld level ternary synaptic connections provide hidden feature representation input. conﬁguration ternary synapses forms feature also called receptive ﬁeld hidden neuron layer. study one-bit deﬁned learn speciﬁc features sensory inputs unlabeled data. machine required internally create concepts inputs. process thus called unsupervised learning. here sensory inputs given handwritten digits taken mnist dataset image dataset notes neighbors data node except feature node auxiliary quantity µb→i indicates probability contribution data node feature node given value products stem weak correlation assumption. j→b). cavity magnetization deﬁned mj→b ξjpj→b second moment feature component deﬁned mj→b pj→b. thus intractable replaced integral normal distribution. furthermore ternary variable pi→a parametrized cavity ﬁelds ehi→a +e−hi→a +e−gi→a hi→a gi→a pi→a combining representation following iterative learning equations probability non-zero synapses i.e. pi→a also ˆmi→a according deﬁnition. sense sparsity synapses described single parameter potential feature inferred computing px=±qb∈∂i well number synapses input dimensionality. synaptic values characterized component takes ternary values. one-bit thus described boltzmann distribution activity hidden neuron denotes inverse temperature synaptic strength scaled factor ensure corresponding statistical mechanics model extensive free energy. marginalization hidden activity obtains distribution visible activity inference model given input one-bit possible synaptic conﬁgurations describe sensory input. however machine choose potential candidates feature map. process naturally modeled bayes’ rule condition last product becomes clear introducing zero synapses bayesian viewpoint amounts sort gaussian-like regularization discrete support. bayesian method able capture uncertainty learned parameters thus avoids over-ﬁtting able reduce necessary data size learning well follows compute maximizer posterior marginals estimator maxξi data dependence probability omitted. hence task compute marginal probabilities e.g. computationally hard problem interaction among data constraints however mapping original model onto graphical model data constraints synaptic values treated factor nodes variable nodes respectively estimate marginal probability running message passing algorithm shall explain below. assumption synapses graphical model weakly correlated called bethe approximation physics. ﬁrst deﬁne cavity probability pi→a weak correlation assumption self-consistent starting randomly initialized messages learning equations converge ﬁxed point corresponding thermodynamically dominant minimum bethe following part study learned feature fraction zero synapses change data size. particular focus machine develops internal concept input handwritten digits computational role zero synapses feature selectivity receptive ﬁeld formation. fig. learning behavior handwritten digits dataset formation receptive ﬁelds learning. left right bottom corresponding data size increases colors black white gray indicate inactive active zero synapses respectively. gray border separating subﬁgures refer synapses. fraction zero synapses overall strength cavity messages function data size. marker plot averaged random selections training images equal number. asymptotic curve small limit mcav also shown. neurobiological counterpart still unclear deserves tested future experiments feature learning. particular monotonic behavior sparsity level synapses intimately related overstrength cavity messages deﬁned tanh hi→a. model originally formation symmetry spontaneously broken indicated mcav around critical cavity messages start polarize without maintaining trivial values more mean ﬁeld theory study unsupervised feature learning zero synapses. following simulations unless otherwise stated. simplicity consider digits combinations diﬀerent digits yield similar results. ﬁrst study receptive ﬁeld hidden neuron develops learning number training images increases. shown fig. data severely scarce apparent structure feature map. number training images increases around intrinsically structured feature starts develop. nevertheless still large fraction zero synapses. learning proceeds intrinsic structure concentrates center feature indicating machine already created internal perception external stimuli. kind perception shown excellent discriminative capability diﬀerent stimuli precision-recall analysis distribution weighted inputs hidden neuron receives develops well-separated peaks diﬀerent digits. then study computational role zero synapses. shown fig. sparsity level synapses decreases data size around critical sparsity decreases abruptly suggesting structured feature beginning develop. here learning indeed induces fraction zero synapses decrease since zero synapses required adopt non-zero values capturing characteristics input data. data size increased feature reﬁned sparsity decreases slowly around critical region. large values small fraction zero synapses still maintained. zero synapses stage seem form approximate boundary active inactive regions feature therefore zero synapses behave like contour detectors. eﬀect predicted model fig. feature maps diﬀerent inversetemperatures left right bottom feature maps obtained fraction zero synapses versus inversetemperature diﬀerent values marker plot averaged random selections training images equal number. guish diﬀerent stimuli. note that large free energy ceases extensive seen last product second equality qualitative change results competition data constraints biases introduced zero synapses critical determined value ceases decrease starts increase. observed fig. decreases increases. ganglion cells. neural activity measured during natural-movie-stimuli experiment salamander retina retina early visual system performing decorrelaaccompanied rapid decrease number zero synapses. moreover asymptotic behavior small limit message strength derived ρasympt denotes small strength denotes image statistics expressed well trend small next study machine creates perception digit learning proceeds. receptive ﬁeld formation displayed fig. around rough structure receptive ﬁeld starts emerge learning process structure becomes apparent data added. meanwhile fraction zero synapses decreases. become active become inactive reﬁning developed receptive ﬁeld feature map. belief stimulus image continuously updated sensory inputs. around clear concept digit created unsupervised learning combining likelihood prior interestingly small fraction zero synapses remain serve contour detectors. zero synapses specify boundary active inactive regions feature map. next study eﬀect inverse-temperature receptive ﬁeld formation. thought scalar tuning global contrast level input image increasing observes qualitative change feature activesynapses-dominated phase small zero-synapses-dominated phase high surprisingly zero-synapses-dominated phase still maintains discriminative capability distintour detectors. addition zero synapses sensitive contrast level sensory inputs. predictions guide future neurobiological experiments. particular fact number zero synapses acts indicator concept formation intimately related spontaneous symmetry breaking model. ﬁndings also implications promising deep neuromorphic computation discrete synapses would interesting challenging generalize current framework neural networks multiple hidden neurons furthermore hierarchical multi-layered architectures. previous studies showed parallel retrieval memory possible random dilution connections random rbms connections current ﬁndings sense zero synapses oﬀer possibility simultaneously recall multiple patterns. furthermore ﬁndings unsupervised learning zero synapses consistent results reported supervised learning perceptron model cerebellar purkinje cells studied. intuitive explanation that learning stretches synaptic-weight distribution pushing synapses towards limit values recent work derived paramagnetic-spin-glass transition line generalized spin weight priors interpolating gaussian binary distributions connect results spontaneous symmetry breaking concept formation real data analysis. thank taro toyoizumi comments silent synapses jack raymond james humble careful reading manuscript adriano barra drawing attention previous works. research supported amed grant number jpkm. fig. fraction zero synapses overall strength cavity messages function data size neural activity. marker plot average random selections neural spike patterns equal number. tion computation redundant visual inputs downstream brain areas directly model structure population activity upstream area without reference external sensory inputs thus important test theory kind unsupervised learning retinal neural activity. fig. observe similar behavior sparsity synapses found learning handwritten digdataset. learned feature spontaneous symmetry breaking critical data size sparsity synapses changes rapidly well. spontaneous symmetry breaking feature possible phases synapses either all-active allinactive fraction zero synapses becomes nearly zero. hidden neuron model thought unit downstream circuit along ventral visual pathway polarization receptive ﬁeld show intrinsic structures similar already observed learning handwritten digits dataset. retina circuit bottom level visual hierarchy concept visual input formed higher level cortical hierarchy conclusion build physics model sparse unsupervised feature learning based one-bit model sparseness synaptic activity automatically learned noisy data. rapid decrease number zero synapses signals concept formation neural network remaining zero synapses reﬁne learned concept serving con-", "year": "2017"}