{"title": "Noisy independent component analysis of auto-correlated components", "tag": "q-bio", "abstract": " We present a new method for the separation of superimposed, independent, auto-correlated components from noisy multi-channel measurement. The presented method simultaneously reconstructs and separates the components, taking all channels into account and thereby increases the effective signal-to-noise ratio considerably, allowing separations even in the high noise regime. Characteristics of the measurement instruments can be included, allowing for application in complex measurement situations. Independent posterior samples can be provided, permitting error estimates on all desired quantities. Using the concept of information field theory, the algorithm is not restricted to any dimensionality of the underlying space or discretization scheme thereof. ", "text": "present method separation superimposed independent auto-correlated components noisy multi-channel measurement. presented method simultaneously reconstructs separates components taking channels account thereby increases eﬀective signal-to-noise ratio considerably allowing separations even high noise regime. characteristics measurement instruments included allowing application complex measurement situations. independent posterior samples provided permitting error estimates desired quantities. using concept information ﬁeld theory algorithm restricted dimensionality underlying space discretization scheme thereof. separation independent sources multichannel measurements fundamental challenge large variety diﬀerent contexts ﬁelds science technology. large interest methods comes bio-medicine namely neural-science investigate brain activities also analysis ﬁnancial time series separation astrophysical components universe name few. mainly distinct approaches component separation exist namely principle component analysis independent component analysis performs linear transformation data obtain mutually uncorrelated orthogonal directions calls principle components. diﬀerent principle components covariance vanishes averaged data useful situations data described orthogonal processes. however imply independence therefore higher order correlations vanish number principle components obtains depends dimension involved data spaces. components processes generating data others might noise. drawing line classes components requires careful consideration context data obtained algorithms estimate independent components maximizing measure independence. several measures used kurtosis negentropy mutual information name view. rely non-gaussian statistics components. mixture gaussian components still gaussian non-orthogonal relevant directions used traditional ica. therefore often assumed non-gaussianity prerequisite ica. however exploitation autocorrelations temporal spatial domain breaks example method used rather similar setting discussed work multivariate singular spectrum analysis also used noisy multi-channel measurement situations taking auto-correlations account. done extending original channel measurements number time-delayed versions vectors. calculates correlation matrix possible channels time delays. diagonalizing matrix leads orthogonal principle components incorporating temporal correlations time delays. relevant principle components used describe main features data allowing analyze dynamical properties underlying system. side independent component analysis large variety widely used algorithms popular ones include fastica jade rely mentioned independence measures noise free environments. often inherent temporal spatial correlation individual components also used. algorithm uses amuse exploits time structure noise-free scenario. problem methods often presence measurement noise. noise prohibits unique recovery individual components demands probabilistic describtion problem. several approaches made solve problem using maximum likelihood methods gaussian mixtures essence method follow similar path. general advice literature however ﬁrst de-noise measurement treat results noiseless processing suitbale methods approach severely suﬀers high noise regime limited signal-tonoise ratio individual measurements. thereby overcomes restriction reconstructing separating components simultaneously combining information across measured channels thereby vastly increases eﬀective signal-tonoise ratio taking spatial temporal correlations individual components account. using method improve result adding additional channels satisfying results obtained even high noise environments. achieve following bayesian framework consistently include auto-correlations posterior estimate components. posterior however accessible analytically maximum posterior estimate insuﬃcient problem. therefore present approximation true posterior capable capturing essential features. kullback-leibler divergence optimally estimate model parameters information theoretical context. furthermore formulate components physical ﬁelds without requirement specifying discretization. allows language information ﬁeld theory develop abstract algorithm free limitations used speciﬁc grid speciﬁc number dimension. information theory ﬁelds generalizing concept probability distributions functions continuous spaces. framework formulate prior distribution encoding auto-correlation components. fist describe generic problem noisy independent component analysis. next section formulate auto-correlations continuous spaces include model. ways approximate model feasible fashion discussed sec. order infer parameters draw samples approximate posterior. describe procedure obtain samples. brieﬂy stating full algorithm discuss convergence demonstrate performance numerical examples showcasing diﬀerent measurement situations. noisy describes situation multiple measurements components diﬀerent mixtures presence noise. individual measurement position time results data noise contribution well linear combination components also spatial temporal structure. data equation process given summation convention multiple indices. mixture acts positions equally therefore depend position index. simplify notation equation simply dropping position index interpreting quantities vectors. remains even introducing multimeasurement vector vector vectors containing individual measurements noise well multi-component vector consisting components. usual matrix multiplication mixture index-free formulation equation want modify expression ways. ﬁrst describe components vectors ﬁelds. hand true components usually resemble physical reality limited discretization therefore best described continuous ﬁeld therefore hand data never continuous ﬁeld inﬁnite resolution would correspond inﬁnite amount information. therefore necessary introduce description measurement process kind instrument probes physical reality form mixed continuous components. general instrument linear operator continuous physical domain discrete target data space. including response operator data equation becomes decoupled domains data components. also represent components inﬁnite resolution want numerical calculations somehow specify discretization introducing response operator allows choose representations completely independent data measurement process. response operator also allows consider linear measurements using diﬀerent instruments individual channels. easily include masking operations convolutions transformations linear instrument speciﬁc characteristics consistent way. furthermore apply wiener-khinchin theorem identify eigenbasis correlation associated harmonic domain spaces corresponds fourier basis. convenient implementation algorithm allows apply correlation operator fourier space diagonal operation eﬃcient implementations fourier transformation components available well. approach stays feasible even high resolutions components representation covariance scales roughly linearly fourier space quadratically position space. components correlations dimension might also advantageous assume statistical isotropy. this correlation depends absolute value distance points. express correlation structure one-dimensional power spectrum. paper assume correlation structure component known. principle could also inferred data critical ﬁltering idea critical ﬁltering parametrize power spectrum additionally infer parameters. allows separate auto-correlated components without knowing correlation structure beforehand. critical ﬁltering successfully applied multiple applications included straightforwardly model. order keep model simple choose discuss case detail here. known correlation structure construct prior distribution components informing algorithm auto-correlation. least informative prior property gaussian prior vanishing mean covariance conceptually gaussian distribution continuous ﬁeld numerical application represent ﬁeld discretized distribution becomes regular multivariate gaussian distribution again. assuming independence individual components prior distributions factorize write product gaussian distributions written compact form combined gaussian multi-component vector blockdiagonal correlation structure expressing independence diﬀerent components other i.e. components want separate exhibit autocorrelation want exploit essential property. component value location continuous domain. deﬁne scalar product ﬁelds expresses complex conjugate ﬁeld position express two-point autocorrelation respect parameters. step over-ﬁt aﬀects consecutive minimization. accumulate errors parameters leading unrecognizable strongly correlated components. minimization algorithm approaches reasonable component separations converge continues accumulate errors converging somewhere else. behavior seen figure showing deviation current estimates true components case well algorithm discussed following. strategy solve problem choose richer model approximate posterior distribution capable capture uncertainty features reduce over-ﬁtting. instead using deltadistribution describe posterior components choose variational approach using gaussian distribution whose parameters estimated. posterior mixture stay initial description point estimate turns sufﬁcient many applications. therefore approximate true posterior distribution form approximation describe posterior knowledge mixture pointestimate components gaussian distribution mean covariance estimate posterior components covariance describes uncertainty structure estimate. compared prior covariance posterior covariance diagonal harmonic domain likelihood typically breaks homogeneity. main problem approximation point-estimate posterior mixture assume perfect knowledge mixture absolute certainty. certainly justiﬁed probabilistic nature problem true every point-estimate context. approximation also aﬀects posterior covariance components contain mixture. assume uncertainty consider errors mixture therefore underestimate true uncertainty components. noise regime eﬀect negligible become larger signal-to-noise ratio numerical examples. model however seems perform reasonably well relatively high noise regimes take error estimates caution keep mind underestimated. could easily think complex model performing even better accurate high noise case. example also approximating mixture gaussian distribution using large gaussian distribution also accounting cross-correlations components mixture. models come cost dramatically increased analytical numerical complexity. analytic form posterior available best solution possible obtained sampling posterior become computationally exillustration purposes without changes algorithm generalizes three n-dimensional situations. even correlations curved spaces sphere considered replacing fourier basis corresponding harmonic basis. constructed likelihood data model prior distribution components encoding auto-correlation. using bayes theorem derive posterior distribution components mixture discuss prior distribution mixture want restrict way. problem-speciﬁc insights mixture expressed right here. prior distributions case written typical approach problem like take likely posterior value estimate parameters. achieved minimizing information hamiltonian above. interpreted approximation posterior distribution delta distributions peaked informative position sense minimal kullback-leibler divergence true approximated posterior. latter written approximation turned insuﬃcient meaningful separation components illustrate sect. viii. iterating minimization respect components mixture obtain satisfying results. maximum posterior estimate known over-ﬁt noise features. severe consequences component separation relies iterative minimization hamiltonian start posterior component mean comparing terms hamiltonian containing ones containing analogous structure. given mixture minimum identical. solve posterior mean setting derivative kl-divergence respect zero divergence vanish contains mixture gives rise required uncertainty corrections regularizing mixture therefore making algorithm converge. unfortunately term numerically challenging. trace operator extracted operator probing involves multiple numerical operator inversions using conjugate gradient method computationally expensive. choose another approach avoids trace expressions taking implicitly account. purpose still solve multiple linear systems found approach numerically stable general also applied cases explicit expressions. order obtain analytic expression kldivergence calculated expectation value information hamiltonian respect approximate posterior gaussian distribution gave rise trace terms ﬁrst place. avoid tremely expensive dimensionality problem scales resolution components. choose approximation given capture relevant quantities simple possible. integration mixture replaces every order keep expressions shorter drop star calculations symbol calculate gaussian expectation values total information hamiltonian. perform calculation cyclical property trace operation identity drawing samples approximate posterior distribution components challenging direct access eigenbasis correlation structure diagonal. could draw independent gaussian samples mean zero variance dimension weight square root eigenvalue adjust correct variance apply transformation position space given eigenvectors. point sample correct correlation structure adjusted correct mean adding obviously access true components prior belief them. correlation diagonal fourier domain component easily generate samples using description above. given mixture instrument response noise covariance prior signal covariance reconstruct quantity data would obtained real components. therefore simulate measurement process arbitrary sample using linear data equation consider kl-divergence performing averaging approximating gaussian keep derivation gradient. estimate resulting expressions approximate averaging replacing average samples minimization respect mixture assume posterior mean covariance ﬁxed calculate derivative expression respect mixture ignoring expectation value used maximum posterior approximation main diﬀerence method becomes apparent. maximum posterior approach point estimate components used. approach replaces minimization hamiltonian minimization mean hamiltonian approximated gaussian taking uncertainty structure components account. obtain estimate mixture allows estimate component means covariances allows draw samples mixture algorithm converges. discuss converges next section. however algorithm converged samples calculate posterior quantity interest involving components estimate uncertainty. example would spatial uncertainty component reconstruction evaluating estimate parameter reduce remaining divergence approximated posterior true posterior least stochastically. stochasticity noise introduced sampling reduced using samples price high computational cost. brieﬂy discuss symmetries structure minima kullback-leibler divergence stated start likelihood contributions brieﬂy summarize approach approximate posterior sampling. start sample drawn independently component prior mock observation provides mock data wiener ﬁlter data posterior mean thing inter−m ested calculations residual allows construct sample mean distribution actually interested samples draw better sampling distribution approximates true distribution. however want samples possible calculation computationally expensive sampling procedure also usage calculations gradient estimations. alternating minimization respect mean components mixture permanently recalculate samples mean mixture constantly changing. found practical start samples increase number inference. note divergence fully calculated also estimated samples therefore estimate inherits stochastic variations. order algorithm need knowledge characteristic noise behavior encoded correlation structure well statistical properties individual components described prior covariance addition specify number components want infer. addition this individual mixtures component means terms exhibit symmetries multiply mixtures components arbitrary factors dividing corresponding components according factors. introduces submanifold minimal energy. finally interchange components also swapping entries mixing matrix. depending number components additional times many minima number components. quadratic terms respectively positive sign therefore introduce additional minima eliminate degeneracy. first terms constrain null space degeneracy single point. multiplicative degeneracy also broken quadratic terms regularize like norms. remains degeneracy multiplication case prior covariances individual components identical interchange symmetry broken minima divergence anymore. therefore using gradient descent method necessarily global minimum. solved discrete optimization steps trying possible permutations mixture components picking smallest divergence. seen case prior correlation structures components minima divergence global minima therefore converge optimal solution irrespective starting position. speed convergence however hard estimate rely iteration consecutive minimizations parameters. individual minimization converges rather quickly depending condition numbers matrices involved invert conjugate gradient method. total convergence rate depend correlation component means mixtures divergence. less correlated faster individual parameters reach minimum. strong correlations however allow large steps therefore slower. components mixture diﬀerent dimensionality main source computational cost. dimensionality component part scales linearly number components resolution least one-dimensional case. higher dimensional components resolution scales accordingly. costly part minimization numerical inversion implicit operator order solve wiener filter problem. minimization respect mixture rather cheap dimension number components times number data channels. drawing posterior samples however requires wiener filter complexity ﬁrst part. therefore want keep number samples possible least beginning inference. increase number samples towards reducing statistical sampling noise. entire algorithm consists large number consecutive minimizations. accuracy performed greatly eﬀects overall performance. want avoid unnecessary accuracy wherever possible parameters changing constantly mixture divergence statistical estimate uncertainties itself. therefore would waste computation high accuracy initially. towards number samples increases might also increase accuracy. optimally steer rather diﬃcult currently requires case case optimization. implemented algorithm outlined python using package nifty allowing coordinate free implementation. numerical examples synthetic data generated according model. ﬁrst describe rather simple case moderate present noise. components recovered suggested solutions highly anti-correlated. demonstrates necessity uncertainty corrections emerging presented model represented averages second example used setup components data channels. modiﬁed measurement instrument resemble typical properties true sensors. randomly masked total area sequences measurement points each. additionally assign sensor individual noise covariance. noise level signiﬁcantly higher case ranging factor times variance compared previous example. data shown figure hard identify components hints correlated structures recognized. encode failing sensors instrument response operator masks varying noise noise operator exactly algorithm before. result seen figure corrected signs compared true corresponding components. morphological structure recovered despite signiﬁcantly hostile conditions. overall uncertainty consequently higher therefore small scales well resolved. masking observe modulations uncertainty structure. parts uncertainty fully cover deviations true components. take uncertainty structure mixture account probably underestimated error. mixture also observe larger deviations correct mixture general recover well. convergence behavior three examples seen figure shows mean deviation current estimate components true components iteration step corrected degeneracy. calculated according number sites given resolution components. would expect quantity become smaller expected deviations originating error estimate ﬁnal result therefore sets lower limit. shown horizontal lines high noise case. inference mean deviation declines towards limit cases reach indicates error estimate result underestimates error slightly ﬁnding surprising take uncertainties mixture account. higher noise level eﬀect becomes relevant whereas noise case almost negligible. also observe statistical nature minimization sampling noisy trajectory. compared maximum posterior minimization follows smooth line. plot also nicely second example challenge algorithm realistic measurement. model randomly failing measurement sensors masking areas data set. addition sensor exhibit individual noise covariance signiﬁcantly increased strength. comparison component realizations mixture used before. examples measure diﬀerent mixtures independent components. channel consists data points probing equally spaced locations unit interval periodic components live. ﬁrst example measurements corrupted noticeable noise zero mean diagonal covariance response operator case identity operator describes spatial correlation falling power fourier space typical many physical processes.this function shown figure choosing power spectrum components ignore problem multimodality probability distributions minima equally global minima. values mixture entries drawn independently gaussian distribution vanishing mean unit variance. afterwards entries corresponding component normalized multiplicative degeneracy between mixture component. number samples used estimate mixture initially iteration increased reconstruction. iterated algorithm times reconstruction converged. results analysis shown figure reconstructions corrected degeneracy signs compared true corresponding components mixtures keeping product constant. clearly recover morphological structure distinct components high accuracy. sigma uncermated error reasonably well. structure mixture recovered small deviations true mixture present. even recover relatively small structures components algorithm uses combined information channels simultaneously increasing eﬀective signal-to-noise ratio leading higher resolutions. denoising individual channel ﬁrst applying noise free method cannot reach resolution limited signal noise ratio individual channels. also show result maximizing posterior respect scenario. initial divergence using maximum posterior. starts approaching true components roughly speed kl-approach situation slows starts accumulate errors clearly diverges method continues converging. derived method allows separation independent components noisy measurements exploiting auto-correlation. done ﬁrst describing measurement process linear mixture component ﬁelds observed linear measurement instruments under additive gaussian noise. model derived likelihood. assuming homogeneity auto-correlation components could express correlation structure diagonal operator fourier basis. assumption derived least informative prior distribution components form gaussian distribution. prior assumptions mixture entries made could added easily. using model likelihood component prior derived expression posterior probability distribution applying bayes theorem. expression accessible analytically approximated product gaussian distribution components delta distribution mixture entries. order infer parameters approximation proposed scheme minimize kullback-leibler divergence distribution true posterior. involved iterative wiener ﬁltering components mixture. estimating mixture considered uncertainty corrections originating gaussian approximation component maps. turned essential obtaining accurate estimates mixture matrices. joint estimate ﬁelds mixtures tends provide incorrect results. order evaluate corrections outlined approach draw independent samples approximate gaussian posterior distribution. numerical examples demonstrated applicability derived algorithm. ﬁrst case involved moderate noise recovered true components mixtures high accuracy. estimated error components reliable. second example models randomly failing sensors signiﬁcantly higher varying noise level applied components. morphology mixture components recovered well error slightly underestimated involved point estimate mixture. overall algorithm delivered satisfying results also applied complex measurement situations high noise regime. acknowledge helpful discussions comments manuscript martin dupont reimar leike sebastian hutschenreuter natalia porqueres daniel pumpe anonymous referees.", "year": "2017"}