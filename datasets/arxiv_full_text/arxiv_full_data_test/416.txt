{"title": "A probabilistic model for learning in cortical microcircuit motifs with  data-based divisive inhibition", "tag": "q-bio", "abstract": " Previous theoretical studies on the interaction of excitatory and inhibitory neurons proposed to model this cortical microcircuit motif as a so-called Winner-Take-All (WTA) circuit. A recent modeling study however found that the WTA model is not adequate for data-based softer forms of divisive inhibition as found in a microcircuit motif in cortical layer 2/3. We investigate here through theoretical analysis the role of such softer divisive inhibition for the emergence of computational operations and neural codes under spike-timing dependent plasticity (STDP). We show that in contrast to WTA models - where the network activity has been interpreted as probabilistic inference in a generative mixture distribution - this network dynamics approximates inference in a noisy-OR-like generative model that explains the network input based on multiple hidden causes. Furthermore, we show that STDP optimizes the parameters of this model by approximating online the expectation maximization (EM) algorithm. This theoretical analysis corroborates a preceding modelling study which suggested that the learning dynamics of this layer 2/3 microcircuit motif extracts a specific modular representation of the input and thus performs blind source separation on the input statistics. ", "text": "previous theoretical studies interaction excitatory inhibitory neurons proposed model cortical microcircuit motif so-called winner-take-all circuit. recent modeling study however found model adequate data-based softer forms divisive inhibition found microcircuit motif cortical layer investigate theoretical analysis role softer divisive inhibition emergence computational operations neural codes spike-timing dependent plasticity show contrast models network activity interpreted probabilistic inference generative mixture distribution network dynamics approximates inference noisy-or-like generative model explains network input based multiple hidden causes. furthermore show stdp optimizes parameters model approximating online expectation maximization algorithm. theoretical analysis corroborates preceding modelling study suggested learning dynamics layer microcircuit motif extracts speciﬁc modular representation input thus performs blind source separation input statistics. winner-take-all-like circuits constitute ubiquitous motif cortical microcircuits previous models theories competitive hebbian learning wta-like circuit based assumption strong wta-like lateral inhibition. several theoretical studies showed spike-timing dependent plasticity supports emergence bayesian computation winner-take-all circuits analyses based probabilistic generative model approach. particular shown network implicitly represents distribution input patterns generative mixture distribution stdp optimizes parameters mixture distribution. analysis assumed input explained point time single neuron strong lateral inhibition among pyramidal cells ensures basically ﬁxed total output rate wta. assumptions however suitable context realistic activity dynamics cortical networks. fact recent modeling results show model adequate softer form inhibition reported cortical layer softer form inhibition often referred feedback inhibition lateral inhibition termed abstractly based inﬂuence pyramidal cells divisive inhibition stems dense bidirectional interconnections layer pyramidal cells nearby parvalbumin-positive interneurons e.g. simulations results also indicate blind source separation emerges computational function microcircuit motif stdp applied input synapses circuit. results raise question whether understood perspective corresponding probabilistic generative model could replace mixture model underlies analysis emergent computational properties microcircuit motivs hard wta-like inhibition. propose model based gaussian prior number active excitatory neurons network noisy-or-like likelihood term. develop novel analysis technique based neural sampling theory show microcircuit motif model approximates probabilistic inference probabilistic generative model. further derive plasticity rule optimizes parameters generative model online expectation maximization arguably powerful tool statistical learning theory optimization generative models. show plasticity rule approximated stdp-like learning rule. theoretical analysis strengthens claim blind source separation also referred independent component analysis emerges fundamental computation assembly codes stdp microcircuit motif. computational operation enables network disentangle separately represent superimposed inputs result independent assembly activations different upstream networks. furthermore theoretical analysis reveals ability cortical microcircuit motif perform blind source separation facilitated either normalization activity patterns input populations homeostatic mechanisms normalize excitatory synaptic eﬃcacies within neuron. figure data-based network model microcircuit motif. network anatomy. circles denote excitatory inhibitory pools neurons. black arrows indicate excitatory connections. lines dots indicate inhibitory connections. numbers connections denote corresponding connection probabilities. network physiology. connection delays indicated. synapses modeled shape using decay time constant indicated right. input synapses subject stdp. data-based microcircuit motif model interaction pyramidal cells inhibitory neurons layer introduced based study analyzed computational properties emerge microcircuit motif synaptic plasticity. ﬁrst brieﬂy introduce microcircuit motif model analyzed discuss properties. subsequently present theoretical analysis network motif based probabilistic generative model proposed speciﬁc model interacting populations pyramidal cells inhibitory neurons cortical layer based data petersen fig. refer speciﬁc model microcircuit motif model model consists reciprocally connected pools neurons excitatory pool inhibitory pool. stochastic spiking neurons constitute excitatory pool. dynamics given stochastic version spike response model ﬁtted experimental data instantaneous ﬁring rate neuron depends exponentially current membrane potential excitatory neurons reciprocally connected pool recurrently connected inhibitory neurons. connection probabilities model taken excitatory neurons receive excitatory synaptic inputs corresponding synaptic eﬃciencies input neuron neuron aﬀerent connections subject standard form stdp. thus membrane poim denotes indices inhibitory neurons project neuron denotes weight inhibitory synapses. denote synaptic input inhibitory neurons input neurons respectively above. inhibitory contributions membrane potential pyramidal cells neuron model divisive eﬀect ﬁring rate. seen substituting also methods thus implementing divisive inhibition recent review). divisive inhibition shown ubiquitous computational primitive many brain circuits recent review). mouse visual cortex divisive inhibition implemented inhibitory neurons although inhibitory signal common neurons pool excitatory neurons contrary inhibition modeled normalize ﬁring rates neurons exactly therefore total ﬁring rate excitatory pool variable depends input strength. importantly contrast inhibition strictly enforced single neuron excitatory pool active given time data-based model allows several neurons active concurrently. computational properties data-based network model extensively studied simulations order compare properties data-based network model previously considered models examined emergence orientation selectivity brieﬂy discuss here. details please pixel-representations noisy bars random orientations provided external spike inputs input spike trains generated pixel arrays converting pixel values poisson ﬁring rates input neurons randomly oriented bars presented network presented fig. resulting network response shows emergence assembly codes oriented bars. resulting gaussian-like tuning curves excitatory neurons densely cover orientations resembling experimental data orientation pinwheels also consistent experimental data inhibitory neurons exhibit orientation selectivity contrast previously considered models idealized strong inhibition wtacircuits show clearly distinct behavior fig. model single neuron could moment time result neurons responded corresponding learning protocol increased ﬁring rate given orientation simulations data-based model average neurons responded orientation increased ﬁring rate. suggests emergent computational operation layer microcircuit motif divisive inhibition better described k-wta computation winners emerge simultaneously competition. figure emergent computational properties data-based network model network inputs given images randomly oriented bars input neuron spike patterns presence input orientation indicated panel indicated gray shading. spike responses subset excitatory neurons input learning tuning curves excitatory neurons preferred orientations degrees. orientation tuning curves model figure modiﬁed fig. number however strict constraint data-based model actual number winners depends synaptic weights external input. computation thus better describe adaptive k-wta operation. k-wta characterisitcs layer microcircuit motif quite attractive since known computational complexity theory k-wta computation powerful simple computation fig. demonstrates signiﬁcantly diﬀerent computational properties emerge data-based model stdp compared previously considered models main article understand diﬀerent emergent computational capability theoretically particular since analysis terms mixture distributions applicable circuits. novel analysis technique summarized follows. first using simpliﬁcations network dynamics formulate network dynamics neural sampling framework allows deduce distribution activities excitatory neurons network given input given network weights show distribution approximates posterior distribution generative probabilistic model generative model mixture distribution case complex distribution based noisy-orlike likelihood. make nature approximation explicit evaluate severity simulations. finally derive plasticity rule implement online generative model thus implementing blind source separation. plasticity rule approximated stdp-like learning rule. neural sampling framework provides ability determine stationary distribution network states given network parameters given network input. order able describe probabilistic relationships input network activity describe network inputs binary vectors responses excitatory neurons binary vectors vectors capture spiking activity ensembles spiking neurons continuous time according common convention introduced spike neuron ensemble time sets corresponding component vector default value duration methods. note diﬀerence vectors output traces used methods). former constitute abstract convention describe momentary state network based current ﬁring activity latter describe impact neurons postsynaptic targets terms real-valued double-exponential epsps. want describe distribution network states given inputs network parameters terms probability distribution distribution activities network inputs excitatory neurons represented vectors binary random variables network state time interpreted speciﬁc realization random variables. order make mapping network activity distribution network states feasible make three simplifying assumptions dynamics neural network models similar first psps inputs network neurons rectangular length network neurons refractory time span spike. second synaptic connections idealized sense synaptic delay ﬁnally weights recurrent synaptic connections symmetric necessitates lateral inhibition implemented pool inhibitory neurons. instead network dynamics deﬁned pool network neurons since inhibitory neurons show linear response properties inhibition network depends linearly activity excitatory neurons network. therefore model inhibition network direct inhibitory connections excitatory neurons weight clarity provide full description approximate dynamics following. approximate dynamics described network neurons. network neurons instantaneous ﬁring rates depend exponentially membrane potential given whenever neuron spikes output trace neuron period duration input neurons). emitting spike neuron enters refractory period duration instantaneous spiking probability zero. note deﬁnition output trace state vector identical vector output traces lateral inhibition network established direct inhibitory connections excitatory neurons leading membrane potentials denotes neuron-speciﬁc excitability neuron independent input network activity. network neuron receives feedforward synaptic inputs whose contribution membrane potential neuron time depends synaptic eﬃciency input neuron network neuron network neurons all-to-all recurrently connected. second term speciﬁes recurrent input inhibitory recurrent weight connection network neuron network neuron shown membrane potentials distribution network states given boltzmann distribution figure relationship data-based model generative probabilistic model a-c) schema response model superimposed bars. network inputs schematically arranged array clarity argument. black indicates highly active inputs neurons. vertical added noise presented input activates excitatory neuron similar hard models. another neuron activated horizontal also similar hard models. combination basic input patterns activates neurons response inconsistent model generative model based mixture distributions. still viewed approximate inference posterior distribution hidden causes given inputs probabilistic model shown schema probabilistic model joint deﬁned prior likelihood synaptic eﬃcacies implicitly deﬁne likelihood inputs given hidden causes likelihood model given input high probability large least active hidden cause depicted example belongs bars corresponding active hidden causes. nonlinear behavior likelihood probability comparable hidden causes active. inset right depicts gaussian prior hidden causes prior implicitly incorporates impact inhibitory feedback data-based model fig. illustrates putative stochastic computation performed model i.e. network input leads network activity model. assume probabilistic model inputs deﬁned prior likelihood distributions describe generate input samples ﬁrst drawing hidden state vector drawing input vector therefore probabilistic model also called generative model network performs probabilistic inference probabilistic model inference task described assumes given hidden causes inferred. inference intuitively also described providing explanation current observation according generative model following describe probabilistic model show approximates posterior distribution model. implies simpliﬁed dynamics data-based model approximate probabilistic inference probabilistic model probabilistic model deﬁned distributions prior hidden variables conditional likelihood distribution input variables describes probability input given network state network parameters distributions deﬁne joint distribution hidden visible variables since fig. speciﬁc forms distributions probabilistic model considered discussed following deﬁned eqs. below. previously considered hard models strong lateral inhibition assumed. corresponded prior single component hidden vector active time. biologically realistic divisive inhibition allows several simultaneously. corresponds prior induces sparse activity soft manner enforce strict ceiling number z-neurons within time interval length tries keep number within desired range. hence prior gaussian distribution zprior normalizing constant parameter shifts mean distribution deﬁnes variance note gaussian restricted integers runs binary random variables likelihood input figure likelihood model proposed likelihood model noisy-or likelihood likelihood input depends hidden causes weighted contribution hidden causes input. large likelihood approaches likelihood proposed likelihood model whereas noisy-or model. approximation log) used probabilistic model hard circuits explain input variable single hidden cause contrast probabilistic models soft inhibition prior several hidden causes active simultaneously explain together input variable deﬁne vector weights deﬁne likelihood variable consider following likelihood model deﬁned +exp likelihood function shown figure together often used noisy-or likelihood. note none hidden causes active i.e. probability active hidden cause increases probability input variable assumes value also fig. likelihood allows generative model deal situations input neuron context diﬀerent hidden causes example pixels network inputs intersection diﬀerent patterns fig. soft gaussian prior allows internal model develop modular representations diﬀerent components complex input patterns. likelihood quite similar frequently used noisy-or model diﬀerence probability input zero proposed likelihood become noisy-or model. model reﬂect situation network inputs noisy ﬁring rates never zero. analyze relationship probabilistic model description data-based model neural sampling framework. approximates probabilistic inference finally show adaptation network parameters stdp understood approximate stochastic expectation maximization process corresponding probabilistic model normalizing constant terms including stem prior last term stems normalization likelihood. compare posterior posterior network model quite similar denoting strength inhibitory connections neural excitabilities. last term problematic since ai’s depend thus whole posterior boltzmann distribution therefore computed model turns however last term approximated quite well term linear note zero last term evaluates log. increases neglect logarithm expression quickly approaches thus write term brackets right l-norm weight vector neuron note given weight matrix increased leads better approximation. approximation log) illustrated fig. hence ﬁrst approximate posterior consider given boltzmann distribution form last term accounts neuronspeciﬁc homeostatic bias depends incoming excitatory weights. matching terms equation terms performing match membrane potential membrane potential neurons note wnorm incorporated constant weight could enforced biological network synaptic scaling mechanism normalizes incoming weights neuron. data-based model used uniform excitabilities excitatory neurons homeostasis simplicity argue conditions approximation justiﬁed. consider posterior distribution circuits second approximation exact posterior note case wnorm eﬀectively leads smaller mean gaussian prior. detailed discussion parameters generative probabilistic model mapped parameters data-based microcircuit motif model provided network parameter interpretation methods. evaluated impact approximations f¨oldiak’s superposition-ofbars problem standard blind-source separation problem also used evaluate data-based network model problem input patterns two-dimensional pixel arrays horizontal vertical bars superimposed fig. input patterns generated superposition bars distribution used performed inference hidden causes sampling approximate posterior distribution given network hidden-cause neurons. performed approximate stochastic online order optimize parameters model. synaptic update rule used parameter updates discussed detail below. compared approximated posterior exact computing kullback-leibler divergence dkl||pa). divergence small throughout learning slight decrease process fig. evaluate kl-divergence means inference considered hidden state vector zmax maximum posterior probability learning exact approximate posterior used reconstruct input pattern computing σls. reconstructed inputs example inputs fig. shown fig. exact posterior approximate posterior approximate reconstructions resemble exact ones many cases occasional misses basic pattern. ﬁnal weights neurons shown fig. two-dimensional layout input facilitate interpretability. note basic patterns represented individual neurons additional neurons specialized combined patterns. figure empirical evaluation approximations superposition-of-bars task. sample input patterns depicted grid. patterns consist varying number superimposed horizontal vertical bars. evolution kullback-leibler divergence exact posterior posterior approximation learning comparison kl-divergence uniform distribution indicated blue. example reconstruction inputs panel posterior according hidden states maximum probability exact posterior posterior approximation learning scale weights vectors network neurons depicted grid input panel scale evolution kullback-leibler divergence exact posterior posterior approximation learning posterior approximation adjusted sparsity prior learning. comparison kl-divergence uniform distribution indicated blue. approximate posterior equivalent approximate posterior synaptic weights neuron normalized common norm. turned normalization strictly necessary. work data-based model managed perform blind source separation posterior best described without normalization synaptic eﬃcacies. found superposition-of-bars problem posterior distribution diﬀers signiﬁcantly exact posterior wnorm line fig. diﬀerence mostly induced tendency prefer many hidden causes missing last term prior corrected reduce number hidden causes found approximation signiﬁcantly improved particular network weight vectors approached ﬁnal norm values yellow line fig. closer inspection comparison shows approximation eﬀective basic patterns consist similar number active units otherwise patterns strong activity preferred weakly active ones compensates for). conclude analysis microcircuit motif model approximates probabilistic inference noisy-or-like probabilistic model inputs. prior network activity favors sparse network activity strictly enforce predeﬁned activity level. ﬂexible regulation network activity obviously important network input composed varying number basic component patterns. show network behavior combination stdp allows microcircuit motif model perform blind source separation mixed input sources. analysis shown computation blind source separation facilitated either normalization activity input populations homeostatic mechanisms normalize excitatory synaptic eﬃcacies within neuron. established link well-deﬁned probabilistic model spiking dynamics microcircuit motif model analyze plasticity network. probabilistic model deﬁnes likelihood distribution inputs depends parameters propose stdp viewed adaptation parameters deﬁned probabilistic model parameters approximates actually encountered distribution spike inputs within constraints prior since prior typically deﬁned favor sparse representations tends extract hidden sources patterns operation called blind source separation precisely show stdp approximates stochastic online given external distribution synaptic inputs adapts model parameters model likelihood distribution approximates given distribution formally kullback-leibler divergence likelihood inputs internal model empirical data distribution brought local minimum. theoretically optimal learning rule contains non-local terms hard interpret biological point view. following derive local approximation yield simple stdp-like learning rule. goal algorithm parameters minimize kullback-leibler divergence likelihood inputs probabilistic model empirical data distribution distribution inputs experienced network equivalent maximization average data log-likelihood ep∗]. given training data corresponding unobserved hidden variables corresponds maximizing optimization done iteratively performing steps. given parameters posterior distribution hidden variables determined using distribution performs m-step maximized respect obtain better parameters model. steps guaranteed increase lower bound steps iterated convergence parameters local optimum computation m-step probabilistic model hard. generalized algorithm m-step replaced procedure improves parameters without necessarily obtaining optimal ones single m-step. done example changing parameters direction gradient since model assume synaptic eﬃcacy changes instantaneous pre-post spike pair need consider online-version generalized algorithm. stochastic online data example sample posterior drawn parameters changed according samplepair. shown above network implements approximation stochastic e-step. m-step parameter updated direction gradient ∆wim prior depend likelihood given equivalent ∆wim derivative given learning rate. learning rule local requires information activation output neurons well values synaptic weights originating input neuron order make biologically plausible approximate rule rule uses locally available information synapse. consequences approximation learning? single neuron network active approximation exact. otherwise approximation ignores neurons contribute explanation input component means weight increased even already fully explained network activity. decrease general smaller exact rule note however magnitudes weigh changes aﬀected weights change sign change. hence conclude angle approximate parameter change vector exact parameter change vector degrees. words inner product vectors always non-negative updates performed correct direction. conﬁrmed simulations. learning experiment described fig. compared approximate update update proposed exact rule every update step. angle exact approximate update vector mean indicated spike network neuron pattern presented input. therefore update following synaptic plasticity rule postsynaptic spike update weight according according learning rule presynaptic neuron spikes shortly postsynaptic neuron results long-term potentiation weight dependent according term σls. weight dependence large weights lead small weight changes vanishing changes large weights. post-synaptic spike neuron preceded presynaptic spike neuron results long term depression also weight dependent much lesser extent weight-dependent factor varies behavior mimicked standard stdp rule implemented data-based model standard weight dependence updates exponentially decreased depend ltd. hence dynamics synaptic plasticity data-based model understood approximation probabilistic model creates internal model distribution network inputs. internal probabilistic model deﬁned noisy-or-like likelihood term sparse prior hidden causes current input pattern. hence stdp understood optimizing model parameters observed distribution input patterns explained basic patterns assumed input time point described combination sparse subset patterns. words stdp microcircuit motif model performs blind source separation input patterns. provided novel theoretical framework analyzing understanding computational properties emerge stdp prominent cortical microcircuit motif interconnected populations pyramidal cells interneurons layer computer simulations based data indicate computational operation network motif cannot captured adequately model. instead work suggests k-wta model varying number excited neurons become active. since circuit model turns inadequate capturing dynamics interacting pyramidal cells interneurons needs replace probabilistic model previously used analyze impact stdp computational function network motif. mixture models proposed inseparably tied dynamics drawing sample mixture model ﬁrst decides stochastically component mixture model sample drawn shown quite diﬀerent generative model similar noisy-or model captures impact soft lateral inhibition emergent network codes computations much better noisy-or model well-known machine learning apparently previously considered computational neuroscience. probabilistic model suggests varying number active neurons circuit depend prior encoded network parameters familiarity network input. shown evolution dynamics computational function network motif stdp understood theoretical perspective approximation expectation maximization ﬁtting noisy-or based generative model statistics high dimensional spike input stream. link helpful theoretical perspective since useful theoretical principles known understanding self-organization processes. particular theoretical framework allows elucidate emergent computational properties network motif spike input streams contain superimposed ﬁring patterns upstream networks. disentangles patterns represents occurrence pattern component separate sparse assembly neurons already postulated established relationship network probabilistic model allows relate network parameters parameters generative model brieﬂy excitability pyramidal cells proportional strength inhibitory connections pool pyramidal cells proportional hence large combination small leads large spontaneous activity tightly regulated strong inhibitory feedback. hand broad prior leads weaker inhibitory feedback thus allowing network attain broader range activities. related theoretical study circuits performed extended sheets circuits assumed models inhibition normalizes network activity exactly leading strict behavior. analysis present work much complex necessarily include number approximations. analysis reveals softer type inhibition studied provides network additional computational functionality. exists also structural similarity proposed learning rule reported insofar signiﬁcant raises question application almost learning rule motif leads learning extraction single hidden cause another extraction multiple causes. answer likely lies interplay prior knowledge model learning rule inhibition strength multiple neurons proposed microcircuit motif model spike response input adapt synaptic weights increase likelihood spiking whenever similar input pattern presented future possibly conjunction diﬀerent input components. manifested increased total input strength neurons pattern seen again. results also increased total inhibition neurons thereby eﬀectively limiting number winners. ﬁxed normalization ﬁring rates soon input strength caused single feature component strong enough trigger spike neuron neuron respond pattern consists particular feature. average force neurons specialize single feature component. therefore learning spike interpreted indication particular feature component. noisy-or model tightly related likelihood model used article. basic likelihood models allows combine basic patterns. noisy-or related models previously used machine learning literature models nonlinear component extraction basic elements belief networks linked cortical processing. extraction reoccurring components input patterns closely related blind source separation independent component analysis previous work direction includes implementations artiﬁcial neural networks also abstract models loosely connected computation cortical network motifs. investigated context spiking neurons. theoretical rules intrinsic plasticity derived enable neurons combination input normalization weight scaling stdp extract independent components inputs. interesting diﬀerence inhibition acts decorrelate neuronal activity. intrinsic plasticity hand enforces sparse activity probabilistic model sparse network activity enforced prior network activities implemented inhibitory feedback models experimentally found network connectivity inhibition naturally acts fast time scale time scale intrinsic plasticity unclear layer microcircuit motif modeled data-based model model described brieﬂy completeness. thorough deﬁnition. model consists reciprocally connected pools neurons excitatory pool inhibitory pool. inhibitory network neurons recurrently connected. excitatory network neurons receive additional excitatory synaptic input pool input neurons. fig. summarizes connectivity structure data-based model together connection probabilities. connection probabilities chosen according experimental data described denote spike times input neuron output trace input neuron given temporal unweighted postsynaptic potentials synaptic response kernel i.e. shape psp. given doubleexponential function rise time constant fall time constant given spike times output traces excitatory network neurons inhibitory network neurons deﬁned analogously denoted respectively. network consists excitatory neurons modeled stochastic spike response model neurons eqs. sec. motivation network parameter values theoretical perspective. here numerator includes excitatory contributions ﬁring rate denominator term ﬁring rate describes inhibitory contributions thereby reﬂecting divisive inhibition apart excitatory neurons minh inhibitory neurons network. inhibitory neurons also modeled stochastic spike response neurons instantaneous ﬁring rate given denotes synaptic input excitatory network neuron denotes indices excitatory neurons project inhibitory neuron denotes excitatory weight inhibitory neurons. synaptic connections input neurons excitatory network neurons subject stdp. standard version stdp employed exponential weight dependency potentiation section interpretation dynamics light established relationship parameters probabilistic model network parameters. relationship however derived based simpliﬁed network model included example rectangular epsps direct inhibitory connections without explicit inhibitory neurons. nevertheless also determine reasonable parameter settings data-based model based prior network activity deﬁned probabilistic model parameters excitability synaptic weights section start assuming prior parameters well ﬁtting parameter deduce parameters used simulations. shown above neural excitability given obtain chosen inhibition strength approximate dynamics replaced weight inhibitory neurons excitatory neurons probabilistic model determined assumption rectangular inhibitory psps. double-exponential psps instead rectangular ones. therefore correct diﬀerences integrals. using correction obtains ﬁrst consider weights excitatory inhibitory neurons assumption i-to-i connections. case order obtain number spikes inhibitory neurons excitatory neurons spike excitatory neuron induce average spike within inhibitory neurons without i-to-i connections guarantees excitation balanced inhibition. however single spike occur average delay interestingly i-to-i connections help decrease delay. particular demands inhibitory spike elicited delay less average spike excitatory population induces inhibitory population approximately total rate leading average delay without i-to-i connections would however lead many successive spikes within i-to-i connections compensate large excitation. approximately correct compensation ﬁrst inhibitory spike balance excitation approximately achieved providing exactly amount inhibition inhibitory neurons leading creation basic patterns basic patterns -dimensional vectors representing horizontal vertical two-dimensional pixel array. deﬁned basic patterns total corresponding possible horizontal vertical bars width pixel array. horizontal pixels array attained value pixels entries basic pattern vectors deﬁned values corresponding pixels array. superposition basic patterns generate input pattern basic rate patterns superimposed follows. number superimposed basic patters nsup chosen drawn distribution .k.−k .l.−l basic pattern superimposed drawn uniformly basic patterns without replacement. corresponds distribution used input denotes basic since posterior step intractable approximated assuming maximum hidden causes active posterior distribution allowed compute partition function therefore sample hidden state vectors straight-forward manner. further consider hidden state vectors active hidden state since would lead parameter changes. initialized values drawn uniform distribution weights clipped minimal value maximal value constant learning rate used. training performed updates. network characteristics computed every update. figure every update computed kl-divergence true posterior approximate posterior dkl||pa). addition also computed kl-divergence uniform distribution state vectors. divergences considered distribution vectors active hidden causes tractability fig. data smoothed using box-car ﬁlter size figure considered input patters given panel computed hidden state zmax maximum posterior probability learning exact approximate posterior. hidden state vector used reconstruct input pattern computing σls. note sample deﬁnes probability individual pixel figure every update simulation described above computed kldivergence dkl||pa) true posterior posterior according approximation curve used wnorm given original model. yellow curve corrected prior model kl-divergence uniform distribution computed described above.", "year": "2017"}