{"title": "Optimal control of eye-movements during visual search", "tag": "q-bio", "abstract": " We study the problem of optimal oculomotor control during the execution of visual search tasks. We introduce a computational model of human eye movements, which takes into account various constraints of the human visual and oculomotor systems. In the model, the choice of the subsequent fixation location is posed as a problem of stochastic optimal control, which relies on reinforcement learning methods. We show that if biological constraints are taken into account, the trajectories simulated under learned policy share both basic statistical properties and scaling behaviour with human eye movements. We validated our model simulations with human psychophysical eye-tracking experiments ", "text": "study problem optimal oculomotor control execution visual search tasks. introduce computational model human movements takes account various constraints human visual oculomotor systems. model choice subsequent ﬁxation location posed problem stochastic optimal control relies reinforcement learning methods. show biological constraints taken account trajectories simulated learned policy share basic statistical properties scaling behaviour human movements. validated model simulations human psychophysical eye-tracking experiments. human oculomotor system performs hundreds thousands eye-movements execution diﬀerent behavioral tasks. order details visual scene related tasks humans direct foveal vision informative locations saccades high-velocity conjugate gaze shifts. saccades followed visual ﬁxation human oculomotor system generates ﬁxational movements involuntarily. despite remarkable achievements modelling ﬁxational movements interpretation fundamental properties comprehensive generic model ﬁxation selection takes account underlying mechanisms visual attention qualitatively describes statistical properties saccadic eye-movements execution visual tasks server gathers information world ﬁxation integrates information ﬁxations belief state makes choice next location ﬁxate. choice governed policy gaze allocation function speciﬁes action decision-maker certain belief state. shown policy based information maximization criteria generates trajectories share basic statistical properties human movements. research goal developing control model ﬁxation selection capable interpreting scaling behaviour human eye-movements provides human level performance computational agent. contrast previous research control models take account inherent uncertainty human oculomotor system duration saccadic movements. it’s well known motor action humans executed random error increases movement magnitude despite oculomotor system having developed correction mechanism saccade errors result inevitable temporal costs. furthermore duration saccades empirically correlated magnitude well factors result situations observer choose informative remote locations nearby show constraints taken consideration trajectories simulated learned policy share basic statistical properties scaling behavior human movements achievable conventional infomax model formulation biologically plausible model gaze allocation human observer point view stochastic optimal control. representation model form partially observable markov decision process proposal heuristic policy. development robust high performance algorithms simulation po-mdp. implementation reinforcement learning algorithms policy optimization numerical estimation optimal policy gaze allocation. comprehensive statistical analysis simulated trajectories data psychophysical experiments. policy learned policy gradient reinforce algorithm shows highest level statistical similarity human eye-movements. experiments discovered dependency mean saccade length q-order hurst exponent visibility target explained model. figure flow chart model ideal observer. visual search recurrent process starts initialization world state. consequent step observer receives observation vector used estimation belief state using bayesian inference update belief state observer makes decision ﬁxate next according policy gaze allocation next ﬁxation location deﬁned execution function observer ﬁxates location target process visual search terminated otherwise next step starts updated values variables. beginning episode target object appears randomly possible locations. assume target placed background noise surrounded distractors placed vacant locations. world state represented tuple kronecker delta value gaussian function mean variance argument euclidean distance between locations current ﬁxation fovea-peripheral operating characteristic fpoc function represents dependence signal-to-noise ratio eccentricity. figure demonstrates fpoc calculated several values contrast background noise single value contrast target calculation based analytical expressions signal-to-noise ratio peak fovea decreases rapidly eccentricity. simulations consider case rotationally symmetric fpoc. assumption correct human observers better generic model fpoc found broken circular symmetry fpoc inevitably results asymmetry visual search process simplify model fpoc research focus attention temporal structure eye-movements rather spatial distribution. visual task considered accomplished. formulation terminal state reﬂects necessity foveate target order extract much information identity details possible. location target doesn’t change trial. decision-making observer modeled po-mdp belief state discreet probability distribution function target location given observations received step observer instructed target appears randomly initial belief state discrete uniform distribution. step observer receives observation vector whose elements represent perceptual evidence target corresponding locations. probability distribution function updated using bayesian inference index location observation model. order take account uncertainty processing perceptual information within neural circuits observer follow noisy observation paradigm paradigm observation model reﬂects presence observer’s internal sources ineﬃciency physical neural noise stages information processing. according perceptual model observation represented random variable gaussian distribution mean depending location center target lattice time intervals empirically correlated magnitude saccade preceding ﬁxation duration saccadic eyemovements θsac range magnitudes possible approximate τsac besides magnitude saccade ﬁxation duration inﬂuenced various factors discriminability target complexity visual task observer however observer correctly informed targets’ properties task execution performs visual task without interruptions contribution factors ﬁxation duration constant trial. eye-tracking experiments ﬁxations tasks found dependence ﬁxation duration saccade amplitude linear values parameters used simulations consistent estimates eye-tracking experiments .ms/ ±ms. within range parameters’ values didn’t substantial diﬀerence estimates learned policy gaze allocation. figure fovea-peripheral operating characteristic calculated several values contrast background noise single value contrast target signal-to-noise ratio peak fovea decreases rapidly eccentricity. next step po-mdp starts transition location an+. decision-making model easily extended order take account extraction visual information between moment making decision ﬁxate next completion saccade. duration step duration time step considered total time required relocation gaze previous location current extraction visual information location therefore consider durations ﬁxation saccade θsac. according literature represents function expected information gain blue cross corresponds location current ﬁxation step observer makes decision ﬁxate location deﬁned policy decision results saccadic eye-movements location marked green cross. receiving observation step observer updates belief state evaluate information gain next decision. particular situation target absent vicinity observation resulted decline probability area around green cross area eﬀectively inhibited subsequent ﬁxations probability. size area deﬁned values fpoc call policy infomax greedy text below. trajectories generated infomax greedy policy match basic properties human movements however policy doesn’t consider correlation magnitude saccades durations steps mdp. show later policy inferior policy optimizes expected rate information gain i/θ] policy called \"infomax rate text below. performance heuristic policies compared performance policy learned reinforcement learning algorithms section appendix a... formulation cost function real time sets study separately previous works show below policy optimized cost function reward deﬁned generates sequences actions statistical characteristics close human saccadic eye-movements. section describe heuristic policies related model entropy limit minimization searcher deﬁne information gain step shannon entropy. heuristic policy deﬁned policy chooses decision maximizes expected information gain sign denotes convolution operator fpoc represented radially symmetric function expression gives approximate value expected information gain case stochastic saccadic placement figure decision-making process infomax greedy policy colour represents function expected information gain blue cross corresponds location current ﬁxation step observer makes decision ﬁxate location deﬁned policy decision results saccadic eye-movement location marked green cross. receiving observation step observer updates belief state evaluates information gain next decision. particular situation target absent vicinity observation resulted decline probability area around ﬁxation area eﬀectively inhibited subsequent ﬁxations probability pn+. size area deﬁned values fpoc approach optimization problem algorithm named \"reinforce optimal baseline according procedure described supplementary materialappendix performance reinforce compared optimization algorithms named \"policy gradient parameter exploration adopted algorithm reinforce optimal baseline belongs class likelihood ratio methods whereas pgpe related ﬁnite diﬀerence methods. despite distinction approaches algorithms give close estimation optimal policy appendix a... simulated trajectories data analysis section using solution provided reinforce better performance comparing pgpe. figure demonstrates decision-making process policy learned fpoc corresupplementary materials appendix justify choice policy evaluate form kernel function allows eﬀectively solve optimization problem policy gradient algorithms. task search kernel function corresponds policy optimizes cost function figure decision-making process policy learned fpoc corresponding conditions step observer ﬁxates location marked blue cross. policy deﬁnes probability density function decision ﬁxate next observer chooses decision according policy results saccadic eye-movement location well case dynamics heuristic policy previously visited locations inhibited subsequent ﬁxations. note movements remote locations inhibited radial function. results co-directed short movements also characteristic human observer. sponding conditions step observer ﬁxates location marked blue cross. policy deﬁnes probability density function decision ﬁxate next observer chooses decision results saccadic eye-movement location well case dynamics heuristic policy previously visited locations inhibited subsequent ﬁxations. section discuss statistical properties trajectories generated learned policy heuristic policies simulations performed grid size corresponds visual ﬁeld size psychophysical experiment. order justify computational model reproduced psychophysical experiments detailed description experiments found appendixappendix although computational model designed exact prediction response time human observers demonstrates high level consistency performance visual task execution human observers. performance measured average time reach target percentage correct ﬁxations target’s location n-alternative forced choice task unsuccessful trials psychophysical experiments excluded consideration. found number unsuccessful trials grows contrast noise corresponding numbers contrast figure demonstrates percentage correct ﬁxations target location experimental conditions means standard errors response time human observers presented figure together means response time three policies estimated episodes po-mdp. learned policy outperforms heuristics human observers mean response time percentage correct ﬁxations experimental conditions. human observers significantly outperformed infomax rate experimental conditions infomax greedy conditions mean response time previously found time learned policy outperformed human observers signiﬁcantly condition conditions t-test didn’t reject hypothesis distributions equal means signiﬁcance level. figure shows length distributions saccades human observers simulated agents performing visual search task corresponding experimental conditions distributions policies human observers exhibit ascent maximum around deg. diﬀerence behaviour distributions starts deg. experimental conditions share saccades human observer length larger whereas value length distribution stabilizes interval observed earlier work found length \"stability\" interval increases linearly grid size. reason behind uniform radial ranking policy locations constant radial function decline probability starts distance compared size visual ﬁeld. hand length distributions trajectories concave interval also characteristic human eye-movement behaviour radial function reﬂects non-uniform radial ranking locations. result remote locations signiﬁcantly lower probabilities chosen next destinations. table statistics test experimental distribution saccade length simulated distributions diﬀerent policies. ﬁrst second columns show values contrast psycho-physical experiment corresponding critical values test statistics signiﬁcance level next three columns show test statistics distribution saccades simulated diﬀerent policies. simulated saccades. results test summarized table ﬁrst second columns show values contrast psycho-physical experiment corresponding critical values test statistics signiﬁcance level critical values diﬀerent diﬀerence number saccades experimental condition. next three columns show test statistics distributions saccades simulated diﬀerent policies. test indicated higher statistical similarity distributions experimental saccades saccades simulated learned policy cases case infomax rate learned policy explained experimental distribution equally well. this make conclusion simulations learned policy explained best length distribution human eye-movements. mean length saccades estimated episodes po-mdp three policies compared mean length saccades human observers according results mean length saccades decreases consistent simulations. it’s immediate consequence decrease values fpoc increase contrast noise illustrated ﬁgure amplitude signal exceeds figure performance human observers simulated agents. learned policy outperforms heuristics mean completion time percentage correct responses n-afc task experimental conditions dependence mean completion time learned policy resembles human observer. figure histograms length distribution saccadic events trajectories generated policies µconv human eye-movements corresponding experimental conditions data binned resolution deg. distribution function policies human observer exhibits ascent maximum around deg. distribution length corresponding tothe infomax greedy stabilizes declines deg. consistent length distribution human saccadic eye-movements concave interval mean length saccades decreases it’s immediate consequence decrease width fpoc deﬁnes area inhibition subsequent ﬁxations. amplitude noise within circle area radius satisﬁes condition circle area eﬀectively inhibited subsequent ﬁxations information already gathered suﬃcient level conﬁdence. however found model provides close estimates mean length high values contrast noise. experimental ﬁndings consistent previously reported results visual search experiments several levels contrast background noise. future works plan incorporate complex saccade execution model takes account bias toward optimal saccade length order explain lower variability saccade length experiments. section analyze distribution directional angle human saccadic eye-movements simulated trajectories. directional angle angle consequent saccades therefore deﬁned tan− tan− coordinates ﬁxation. according deﬁnition movement related persistent directional angle close angles values close correspond anti-persistent movements. distributions directional angle calculated trajectories generated markov decision process policies figure demonstrates distribution directional angle saccadic events human observers simulated trajectories infomax greedy policy generates trajectories stable anti-persistent movements policy chooses next ﬁxation location without taking current location consideration. inhibitory behavior infomax it’s much less likely choose nearby location instead remote relatively unexplored ones. geometrical borders limit choice next ﬁxation results ﬁxations contrast decision process learned policy tends preserve direction movement. dynamic system policy quite similar self-avoiding random walk model described asymptotic behavior kernel function reward gain remote locations suppressed meanwhile locations already visited also inhibited results shortrange self-avoiding movements demonstrate persistent behavior therefore probability distribution directional angle biased towards values according figure dynamics heuristics also characterized persistent random walk. learned policy general stronger radial ranking locations results shorter range saccades repulsion caused inhibition becomes relevant. distribution average length saccades depending shown figure average co-directed movements shorter reversal ones policies. experiments discovered geometrical persistence depends visibility target measured share saccades retain direction previous movement quantity called persistence coeﬃcient. ﬁgure demonstrates dependency persistence coeﬃcient contrast background noise human observers simulated trajectories. mentioned previously average saccade length decreasing growth contrast therefore linear term duration steps becomes less relevant decision-making becomes agnostic temporal costs decline persistent coeﬃcient also characteristic human movements covered previous research. figure histograms directional angle distributions mean length saccades directional angle data histograms binned resolution deg. infomax greedy policy generates trajectories stable anti-persistent movements high degree separation large small movements contrast decision-making process infomax rate policy tends preserve direction movement. dynamics learned policy also characterized persistent random walk. previous section analyzed geometrical persistence human eye-movements trajectories simulated three diﬀerent policies. however statistical property doesn’t give insight long-range correlation time-series. section show dynamics learned policy multifractal behavior similar human eye-movements execution visual search task contrast previous research analysis distinguish diﬀerent types multifractality calculation generalized hurst exponent shuﬄed time series. separate time series ﬁxational saccadic eye-movements allows demonstrate fundamental diﬀerence temporal structure types eye-movements. shown behaviour generalized hurst exponent consistent basic statistical properties eyemovements. demonstrate dynamics optimal policy gaze allocation explains changes scaling behaviour eyemovements diﬃculty visual task qualitative quantitative levels. figure share saccades retain direction previous movement called \"persistence coeﬃcient\". quantity demonstrates dependence persistence visibility target. mentioned previously average saccade length decreasing growth contrast therefore linear term duration steps becomes less relevant decision-making becomes agnostic temporal costs decline persistent coeﬃcient also characteristic human movements covered previous research. statistical analysis simulated trajectories multifractal detrended ﬂuctuation analysis widely-used method detection long-range correlations stochastic time-series. found successful applications ﬁeld bioinformatics nano geophysics method based approximation trends time-series subtraction detected trends original data diﬀerent scales. detrending allows deducting undesired contribution long-range correlation result non-stationarities physical processes. package provided espen ihlen estimations generalized hurst exponent section. appendix appendix thoroughly explain details multifractal analysis. subsection appendix presents details mfdfa algorithm. subsections appendix appendix explain mf-dfa performed simulated trajectories. results multifractal analysis human eyemovements presented subsection summarizes ﬁndings compares generalized hurst exponent simulated trajectories human eye-movements diﬀerent experimental conditions. perform mf-dfa diﬀerence time series human gaze positions order compare estimated generalized hurst exponent simulations. diﬀerentiated time series estimated data coordinates gaze ﬁxations }with resolution represent diﬀerentiated time series following .... correspond sequences movements time interval i-th ﬁxation saccade respectively separate diﬀerentiated time series ﬁxational saccadic time series ﬁgure demonstrates scaling qorder ﬂuctuation function graph result application mf-dfa horizontal concatenated diﬀerentiated time series human scan-paths experimental conditions blue green lines correspond linear approximation function orders scaling exhibits crossover time scale crossover separates \"lower \"upper regimes mentioned according amor crossover caused presence diﬀerent generative mechanisms eyemovements. lower regime related ﬁxational eye-movements upper regime saccadic ones. crossover scaling observed experimental conditions. value generalized hurst exponent obtained linear regression estimates consistent ones amor directions regimes. order distinguish diﬀerent types multifractality calculated generalized hurst exponent hshuf shuﬄed diﬀerentiated time series. ﬁrst type multifractality consequence broad probability density figure scaling q-order ﬂuctuation function generalized hurst exponent computed linear regression graph result application mf-dfa horizontal diﬀerentiated time series concatenated human scan-paths experimental conditions blue green lines correspond linear approximation function orders scaling exhibits crossover crossover time scale crossover separates \"lower\" \"upper\" regimes mentioned lower regime related ﬁxational eye-movements upper regime saccadic ones. crossover scaling observed experimental conditions. function values time series. multifractality ﬁrst type presents time series hshuf second type multifractality caused diﬀerence correlation large small ﬂuctuations scenario described case hshuf hcorr hcorr positive long-range correlation. types multifractality present time series hshuf hcorr. ﬁgure demonstrates estimates hurst exponent shuﬄed time series hshuf correlational hurst exponent hcorr horizontal vertical components estimated exponents saccades upper lower regimes scales respectively. well previous graph result application mf-dfa concatenated diﬀerentiated time series human eye-movements experimental conditions behaviour similarity function distribution amplitude saccadic events humans amplitude distribution saccades demonstrates power behavior interval probability distribution function also reﬂects absence saccades length lower minimal one. therefore ﬁrst type multifractality saccadic time series caused figure hurst exponent shuﬄed time series hshuf correlational hurst exponent hcorr horizontal vertical components human eye-movements. well previous graph result application mf-dfa concatenated human scan-paths experimental conditions behaviour hshuf horizontal vertical shifts full scales corresponds mentioned assume multifractality ﬁrst type caused asymptotic behaviour amplitude distribution saccades diﬀerence long-range correlation large small ﬂuctuations reﬂected hcorr properties ﬂuctuation function positive q-orders main contribution coming segments containing small ﬂuctuations positive long range correlation therefore characteristic small ﬂuctuations upper full scales regimes directions. general results consistent distribution average length saccades directional angle also indicates diﬀerence persistence large small saccades. diﬀerence long-range correlation large small ﬂuctuations reﬂected hcorr properties ﬂuctuation function positive q-orders main contribution coming segments containing large ﬂuctuations positive long-range correlation therefore characteristic small ﬂuctuations upper regime saccadic full time series. results consistent distribution average length saccade directional angle also indicates diﬀerence persistence large small saccades. therefore conﬁrm small saccadic eyemovements demonstrate long-range correlations well ﬁxational eye-movements. monofractal behaviour positive correlations lower regime scales however behaviour hcorr hshuf full time series lower regime indicates presence multifractalities types. present moment explanation multifractality lower regime leave problem future work. section present comparison generalized hurst exponent human eyemovements upper regime simulated trajectories learned policy. well case geometrical persistence claim quantitative properties statistical persistence depend visibility target. estimated correlational hurst exponent hcorr hurst exponent shuﬄed time series hshuf diﬀerentiated trajectories human eye-movements levels contrast background noise figure shows hcorr simulated trajectories under learned policy correlational hurst exponent human eye-movements averaged directions hcorr corr) upper regime. stabilized values human eye-movements simulated trajectories correspondingly. general correlations weaken growth contrast consistent decline geometrical persistence decline hurst exponent increase diﬃculty visual search task also observed previous work hurst exponent shuﬄed time series well correlational hurst exponent demonstrates decline growth contrast negative q-orders human eye-movements simulated trajectories. subsection mentioned behaviour hshuf resembles related time series random values power distribution average value time series equals increase results decrease average value time series decrease value hshuf therefore average value time series values hshuf negative q-orders correlated assumption power-law distribution. previously found decrease average saccade length growth background noise consistent decrease values hshuf negative q-orders. assume correlation caused power-law asymptotic behaviour length distribution human eye-movements upper regime. correlational hurst exponents negative q-orders declines growth contrast noise human eye-movements simulated trajectories. positive q-orders correlational hurst exponent less aﬀected change visibility target. general correlations weaken growth contrast consistent decline geometrical persistence hurst exponent shuﬄed time series well correlational hurst exponent demonstrates decline growth contrast negative q-orders human eye-movements simulated trajectories. subsection mentioned behaviour hshuf resembles related time series random values power distribution tively describes human visual behaviour execution visual search task. basis model observer’s representation constraints visual oculomotor systems. demonstrated consideration temporal costs uncertainty execution saccades results dramatic change basic statistical properties scaling behavior simulated time series. performed multifractal analysis data discovered presence types multifractality time series human eyemovements model simulations. multifractality caused broad amplitude distribution saccades makes signiﬁcant contribution multifractal behaviour time series covered previous work estimation correlational part hurst exponent conﬁrmed presence long-range positive correlations small saccades upper regime. contrary large saccades exhibit weak long-range anti-correlations model simulations human eye-movements upper regime. well case geometrical persistence found long-range correlations eye-movements weaken decline target’s visibility consistent previous work topic research focused attention persistence eye-movements rather spatial distribution. that’s didn’t consider factors directly related trade-oﬀ temporal costs expected information gain. estimate optimal policy assumption visual search process characterized shift-rotational symmetry observed previous work similar experimental settings symmetry visual search broken angular dependency fpoc cases normal controls patients vision disabilities plan include angular dependency radial smoothing functions policy order consider asymmetry visual ﬁeld future works. expect estimation higher order terms result improvement performance policy. current observation model based independent inputs individual location results independence values probability distribution suﬃciently large grid size. therefore higher order terms don’t provide additional information location target. function expected reward computed taking account current location gaze considering rotational invariance general form function a)p. softmax policy function expected reward together equations form policy keeps evolution system invariant distance-preserving transformation. convolution probability distribution kernel function general form diﬃcult optimize problem effectively solved separable approximation call radial smoothing functions correspondingly. ﬁrst characterizes dependence expected reward intended saccade length. motivation behind introduction radial function growing uncertainty ﬁxation placement duration step length saccade. assume radial function equals zero outside interval amin amax minimal maximal saccade length correspondingly. minimal saccade framework provides elegant explanation scaling persistent dynamic voluntary saccades optimality point view. clearly demonstrates control models able describe human eye-movements beyond basic statistical properties. assume process visual search characterized shift-rotational invariance research focus attention persistence eye-movements rather spatial distribution. that’s approximation shift-rotational invariance don’t need consider factors directly related trade temporal costs expected information gain asymmetry fpoc. coeﬃcients dynamic equations unaltered distance preserving transformations. last dynamic equation policy gaze allocation shift-rotational invariant well. policy determined function expected reward property shift invariance represent function expected reward volterra series called volterra kernels. constant eliminated equation therefore considered. dimensionality volterra kernels scales number potential locations estimation volterra kernel computationally unfeasible grid size simulations reason consider linear term observer ﬁxate location make unlikely large saccades condition smoothing function corresponds absence information gain remote locations therefore irrelevance process ﬁxation selection. policy represented parameters solve optimization problem value function policy gradient algorithm adopted optimization procedure represented iterative process gradient estimation update policy parameters training epoch sequence episodes. second approach optimization problem parameter exploring policy gradient presented well previous section estimate gradient update policy parameter training epoch. symmetric sampling policy parameters gradient estimation. beginning step length amin chosen magnitude shortest possible voluntary movement. equal maximal saccade length amax length diagonal stimulus image experiments. smoothing function describes relative contribution surrounding locations reward. smoothing function role term deﬁnition information maximization policy basically deﬁnes meaningful certain location without consideration time costs relocation. zeros bessel function order radii visual ﬁeld. representation allows control dimensionality kernel eﬀectively store policy memory. choice orders bessel functions caused boundary conditions radial smoothing functions boundary conditions radial function forbid model figure shows results optimization radial smoothing functions. reinforce pgpe provide close estimates smoothing radial functions eccentricity smaller order compare solution heuristic policies presented fpoc plot smoothing function. smoothing function provided reinforce monotonously decreasing well fpoc whereas pgpe ﬂuctuating solution decreasing amplitude oscillations. behavior radial function similar solutions higher amplitude oscillations pgpe solution. generate perturbation normal distribution create symmetric parameter samples current values policy parameters training epoch. simulate episode parameter sample denote cost episode generated correspondingly. training epoch policy parameters standard deviation distribution perturbation updated according equations markov decision process deﬁned dynamic equations simulated grid comprises possible target locations beginning optimization procedure pick policy parameters randomly uniform distribution parameter algorithms parametrization policy. training epoch pgpe reinforce consists episodes. learning rate algorithms. figure illustrates performance policy gradient methods used search optimal policy case fpoc corresponding algorithms used fourier-bessel parametrization policy dimensionality radial smoothing functions. reinforce performed better parameter settings. average takes around proﬁle divided segments chosen among linear space {sminsmin smax}. segmentation starts beginning time series therefore residual number elements time-series. order process residual elements segmentation also performed time series. segmentation procedure segments value time step corresponds ﬁxation discrete time step case time corresponds saccadic movement within discrete time interval have +τsac therefore deﬁned function maps discrete sequence real-time sequence perform mf-dfa diﬀerentiated trajectories generated po-mdp heuristic policies learned policy diﬀerentiation trajectories represented real time sequences procedure interpolation appendix model presented devoted can’t describe combined movement saccades. results analysis compared scaling behavior human eye-movements scales corresponds upper regime. therefore minimal time scale smin choice smax corresponds average length episode. assume correlation episodes random location ﬁrst ﬁxation location target. ﬁgure demonstrates scaling q-order ﬂuctuation function simulated trajectory infomax greedy policy conditions blue green lines correspond linear approximation function orders scaling doesn’t exhibit crossover positive q-orders interval scales however behavior deviates linear large scales smax. simulations diﬀerent grid sizes correspond diﬀerent average time task execution shown interval linear behavior always coincides scaling diﬀerent diﬀerent orders therefore trajectories multifractal time series. ﬁgure demonstrates estimates correlational hurst exponent hcorr hurst exponent shuﬄed time series hshuf time series simulated diﬀerent policies. well case human eye-movements types multifractality present simulated time series. behavior hshuf resembles power-law distribution scenario policies except infomax greedy distribution saccade length doesn’t correspond powerlaw demonstrated ﬁgure contrary infomax rate learned policy generate movement distributional multifractality presents human eyemovements well. policies correlational hurst exponent positive negative q-orders. indicates presence long-range correlations small ﬂuctuations. large ﬂuctuations anticorrelated exhibit weak anti-correlation observe last scenario upper regime human eye-movements large ﬂuctuations demonstrate weak anticorrelation contrary figure scaling q-order ﬂuctuation function estimated simulated trajectories infomax rate policy generalized hurst exponent computed linear regression blue green lines correspond linear approximation function orders scaling doesn’t exhibit crossover positive q-orders interval scales figure hurst exponent shuﬄed time series hshuf correlational hurst exponent hcorr trajectories simulated diﬀerent policies. well case human eye-movements types multifractality present simulated time series. behavior hshuf resembles power-law distribution scenario policies except infomax greedy distribution saccade length doesn’t correspond power-law demonstrated ﬁgure contrary infomax rate learned policy generate movement distributional multifractality presents human eye-movements well. group nine patients normal corrected-to-normal vision participated experiment. group included four postgraduate students queen mary university london. group aware experimental settings passed minutes training sessions four diﬀerent experimental conditions correspond certain value contrast background noise. experiments approved ethics committee queen mary university london informed consent obtained. used dell monitor driven dell precision laptop experiments. movements right registered using tracker device smi- sampling frequency tracker device mounted monitor. matlab psychtoolbox used experiments generate stimulus images. participants front monitor heads ﬁxed chin rest distance monitor. monitor subtended visual angle deg. participant shown examples stimulus image experiments instructed ﬁxate target object fast possible press certain button keyboard indicate found target. four participants completed practice session trials experiment. stimuli static images generated before session according description original experiment noise generated square region screen spans visual angle deg. target sine framed symmetric raised cosine. grating target appeared randomly possible location stimuli image within square region. experiments provided level contrast target several levels noise contrast participants completed four experimental sessions trials. experimental session started inbuilt nine-point grid calibration eye-tracking device. participants given minutes rest sessions. stimuli images shown beginning trial. participants assumed perform visual search task ﬁnished pressing \"end\" button. experimental settings signal participants blocked start trial. gaze position measured tracking device vicinity around location target moment participant presses \"end\" button task considered successful. presence temporal delay moments localization target pushing \"end\" button block signal button completion trial central ﬁxation cross shown next trial started stimulus image shown participants. according literature saccade programming assumed two-stage process consists labile non-labile stages labile stage ﬁrst stage saccade programming initial saccade command cancelled favour saccade another location. saccade next location executed non-labile stage. visual input active labile non-labile stages suppressed execution saccade. therefore decision made labile stage visual input received location non-labile stage saccade programming used decision-making next step result observer receives separate observation vectors previous current ﬁxation locations. observation model doesn’t take account duration observation. assume observation vector integrated continuous-time gauswdt time interval detection experiment visibility maps measured. model noise generalizes \"noisy observation\" paradigm variable ﬁxation duration. result integration continuous time noise gaussian white noise mean variance θ·σ. next assume duration non-labile stage θnlb rest ﬁxation duration allocated labile stage using compute mean variance observation inputs successively apply equation evaluate belief state pn+. learned policy gaze allocation extended observation model using reinforce. compared basic characteristic trajectories simulated policy simulations initial model data human observers (look initial model outperformed extended signiﬁcant diﬀerence found. didn’t expect signiﬁcant diﬀerence performance observer receives amount information average models.", "year": "2017"}