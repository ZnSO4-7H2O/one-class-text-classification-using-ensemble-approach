{"title": "Reducing a cortical network to a Potts model yields storage capacity  estimates", "tag": "q-bio", "abstract": " An autoassociative network of Potts units, coupled via tensor connections, has been proposed and analysed as an effective model of an extensive cortical network with distinct short- and long-range synaptic connections, but it has not been clarified in what sense it can be regarded as an effective model. We draw here the correspondence between the two, which indicates the need to introduce a local feedback term in the reduced model, i.e., in the Potts network. An effective model allows the study of phase transitions. As an example, we study the storage capacity of the Potts network with this additional term, the local feedback $w$, which contributes to drive the activity of the network towards one of the stored patterns. The storage capacity calculation, performed using replica tools, is limited to fully connected networks, for which a Hamiltonian can be defined. To extend the results to the case of intermediate partial connectivity, we also derive the self-consistent signal-to-noise analysis for the Potts network; and finally we discuss implications for semantic memory in humans. ", "text": "autoassociative network potts units coupled tensor connections proposed analysed eﬀective model extensive cortical network distinct shortlong-range synaptic connections clariﬁed sense regarded eﬀective model. draw correspondence indicates need introduce local feedback term reduced model i.e. potts network. eﬀective model allows study phase transitions. example study storage capacity potts network additional term local feedback contributes drive activity network towards stored patterns. storage capacity calculation performed using replica tools limited fully connected networks hamiltonian deﬁned. extend results case intermediate partial connectivity also derive self-consistent signal-to-noise analysis potts network; ﬁnally discuss implications semantic memory humans. multi-modular hopﬁeld network potts network storage capacity potts network simulation results discussion calculation replica symmetric free energy self consistent signal noise analysis considerable research eﬀorts recent years driven ambition reconstruct simulate microscopic detail structure human brain possibly scale outcomes questioned complementary perspective forward late neuroanatomist valentino braitenberg many publications argued need understand overarching principles mammalian brain organisation even recourse dramatic simpliﬁcation spirit years braitenberg proposed notion skeleton cortex comprised solely pyramidal cells apical dendrites receive predominantly synapses axons originate pyramidal cells cortical areas travel white matter basal dendrites receive mainly synapses local axon collaterals systems estimated include similar numbers synapses receiving cell. braitenberg detailed could later called small world scheme scheme pyramidal cells allocated modules including cells fully connected cell would receive system connections cell drawn random modules also therefore cell gets connections pyramidal cells systems perfectly balanced average minimal path length cell pair course modules largely ﬁctional construct apart special cases least generality character quite controversial distinction long-range local connections real simple model recapitulates rough square-root scaling systems skeleton cortices mammals range functional counterpart neuroanatomical scheme notion hebbian associative plasticity considered mechanism modulates longshort-range connections pyramidal cells. view autoassociative memory storage retrieval universal processes local global networks operate cortical areas across species would share universal processes whereas information express would speciﬁc constellation inputs area receives simpliﬁed skeleton model attempt describe. underlying diversity higher-order processes cortical cognition comprised would common associative operation multi-modular autoassociative memory. abstract mathematical level hopﬁeld model simple autoassociative memory network opened path quantitative statistical understanding memory implemented network model neurons thorough analyses attractor neural networks. crucially allowed sketch phase diagram approach nature phase transitions associative memory network demonstrate beyond reach quantitative models. initial analyses networks binary units shifted towards networks properties seen cortex modelling cortical connectivity attempts reproduce quantitative observations given apparent lack speciﬁcity single cell level cases models which even without modules probability pyramidal-to-pyramidal connections depends distance neurons rapidly decreasing beyond distance conceptually corresponds radius module models basis strict parcellation modules either speciﬁc assumptions binary synapses network interactions across modules diﬀerent nature within module thereby departing braitenberg assumption associative hebbian plasticity governs intrainter-module interactions. braitenberg’s suggested simpliﬁcation skeleton units system associative enabled powerful statistical-physicsderived analyses successfully applied hopﬁeld model? allowed understanding phase transitions? point. studies multi-modular network models including full connectivity within individual modules sparse connectivity modules could approached basic formulation modules participate every memory sparse connectivity random attempts articulate analytical complexity recourse sparse eﬀectively local coding schemes without yielding plausible quantiﬁcation storage capacity. potts associativ network contrast been early study kanter fully analysed original sparsely coded versions argued oﬀer ever simpliﬁcation cortical network braitenberg’s amenable study also latching dynamics correspondence braitenberg’s notion potts model however discussed. here establishing clearer rationale using potts model study cortical processes. figure braitenberg model regards skeleton cortex pyramidal cells cells each. potts model reduces module comprised multi-state unit state corresponds dynamical attractor local cortical module. number states module thought scale potts neural network studied network units states generalizing hopﬁeld’s binary network units either active quiescent. potts unit introduced statistical physics regarded neuroscience context representing local subnetwork cortical patch real neurons endowed dynamical attractors span diﬀerent directions activity space therefore converted states potts unit schematically illustrated fig. whatever interpretation however deﬁne model autoassociative network potts units interacting tensor connections. memories stored weight matrix network ﬁxed reﬂecting earlier learning phase memory vector list states taken overall activity conﬁguration unit take potts unit possible active states labelled e.g. index well quiescent state unit participate activity conﬁguration memory. therefore take values categorical set. tensor weights read denote units denote states fraction units active memory unit gives input unit number input connections unit kronecker symbols. subtraction mean activity state ensures higher storage capacity units network updated following variable representing input unit state within time scale takes continuous values range whereas note also memories simplicity assumed discrete implying perfect retrieval approached potts model dynamics potts model studied model cortical dynamics often written threshold component speciﬁc unit acting active states varying time time constant threshold intended describe local inhibitory eﬀects cortex relayed gabaa gabab receptors widely diﬀerent time courses short long. discussed elsewhere also dynamical behaviour potts model much interesting fast slow inhibition included. here however treat dynamics beyond sketch stay single time constant sake simplicity. speciﬁc threshold unit state varying time variable constant intended model adaptation i.e. synaptic neural fatigue speciﬁc neurons active state ﬁeld unit state experiences reads note another parameter local feedback term ﬁrst introduced aimed modelling stability local attractors full model. helps network converge towards attractor giving weight active states thus eﬀectively deepens attractors. review hopﬁeld network implementation thresholdlinear units brieﬂy recapitulate order draw correspondence potts network multi-modular version threshold-linear hopﬁeld network considered earlier without globally sparse coding consider underlying network modules comprised neurons connected neurons within module neurons distributed randomly throughout modules make critical hopﬁeld assumption shortlong-range synaptic connections symmetric. activity neuron threshold-linear function summed input modularity ﬁnds expression articulation global activity patterns comprise attractor states network. module retrieve local activity patterns features learned corresponding short range connections. index furthermore global activity patterns consisting combinations features stored dilute long-range connections illustrated fig. total number connections neuron given deﬁne fraction long range connections ca/c. model therefore partially incorporates braitenberg’s assumptions setting implementation would complete also therefore necessary analytical treatment. make simplifying assumption ﬁring rates represent local pattern within module identically independently distributed across units given distribution combination constraint non-zero. denote pa/s average number global patterns represented speciﬁc local pattern given global sparsity assume simplicity integer number. also impose satisﬁes figure cortex comprised modules pyramidal cells receiving sparse inputs modules apical dendrites memory patterns thought comprised features whose values coded local attractors module features bound together tensor connections potts model. sparse coding means features pertain every memory; rest potts units quiescent state example bottom parameters adjust dimensions shortlong-range connections regulate relative strength. note adopt emphasize summation implies repeatedly complex index using local pattern global patterns variable cimjn binary variable cases energy function deﬁned i.e. essentially cimjn cjnim attractor states system correspond minima free energy. hamiltonian multi-modular network proportional cases given avoid introducing additional dimensional parameters assume activity model neuron measured units suitably regulated inhibition local correlations automatically normalized reach maximum value obtain noted absence self-interactions estimated mean pa/s number contributions encoding local attractor state. putting together eqs. neglect last term limit noting nu/c cb/c correct scaling potts hamiltonian implies extra factor present original hamiltonian reabsorbed eﬀective inverse potts temperature otts diverges thermodynamic limit. means potts network taken operate zero temperature relation interactions modules. within modules however eﬀects non-zero noise level underlying multi-modular network persist entropy terms. turn then entropy. here delineating correspondence requires suitable assumptions distribution microscopic conﬁgurations dominate thermodynamic state module expressed entropy terms eﬀective potts model. assumption module mostly states fragmented competing domains units fully correlated corresponding local patterns except ﬁrst spontaneous activity level. would imply that dropping module automatically satisﬁed. number microscopic states characterized +-plet divided subtracted module original hamiltonian entropy term comes microscopic free-energy. becomes eﬀective hamiltonian potts network dividing factor reabsorbed therefore ﬁnds additional shows original inverse temperature retains signiﬁcance local parameter modulates stiﬀness module potts units even though eﬀective noise level long-range interactions modules vanishes. precise entropy formula depends also assumptions microscopic states dynamically accessible other would validated depending dynamics assumed hold within module. alternative assumption individual units practice exchanged fragment correlated local pattern pool uncorrelated units. assumption entropy estimated note that sparse connectivity modules multimodular network translate diluted potts connectivity module potts unit receives inputs modules potts units. consider cases which instead connections potts unit e.g. highly diluted intermediate connectivity considered storage capacity analysis below. gets vague inhibition dynamics arguments indicate local attractors module reinterpreted dynamical variables system interacting potts units. correspondence cannot worked completely however fully equivalent hamiltonian deﬁned anything eﬀects inhibition cannot included given inherent asymmetry interactions hamiltonian formulation. body work neural networks stimulated hopﬁeld model eﬀects ascribed inhibition regarded incapsulated peculiar hebbian learning rule determines contribution stored pattern synaptic matrix subtractive terms. similar subtractive terms argued basis take account inhibitory eﬀects module level lead replace interaction extend approximate correspondence beyond thermodynamics dynamics assume underlying potts network fact network integrate-and-ﬁre model neurons emulating dynamical behaviour pyramidal cells cortex considered simple assumptions concerning connectivity synaptic eﬃcacies reﬂected fact inputs model neuron extended network determined globally deﬁned quantities namely mean ﬁelds weighted averages quantities measure function time eﬀective fraction synaptic conductances open membrane cell given class cluster action presynaptic cells another given class cluster conductance speciﬁc synaptic input. point among clusters deﬁned framework ref. many cluster pairs comprise pyramidal cells share similar biophysical time constant describing conductance dynamics i.e. temporally integrated variable representing activity unit state since varying time scale taken correspond activation pyramidal cells module. conclude summarizes time course conductances opened pyramidal cells inputs pyramidal cells. represents inactivation synaptic conductance like ﬁring rates function overlap function neglecting adaptation consider hamiltonian deﬁned total number units potts network number stored random patterns sparsity i.e. fraction active potts units each a/s. mentioned above time-independent threshold acting units network main diﬀerence analysis included term proportional self-reinforcement term pushes unit active states thus providing positive feedback. apply replica technique compute free energy expressed terms overlap order parameters following refs. measure correlation thermodynamic state network stored memory patterns interested case order parameter diﬀers zero. free energy potts units replica theory reads average quenched disorder quenched average requires introducing additional conjugate order parameters diagonal values appendix compute replica symmetric free energy mean ﬁeld network aﬀects state given unit state condensed pattern interpretation given measures diﬀerence mean square activity given replica coactivation diﬀerent replicas. note zero temperature limit diﬀerence goes always order clariﬁed section separate analysis related derivative output average neuron respect variations mean ﬁeld. diluted networks highly diluted limit biologically plausible case diluted networks number connections unit less speciﬁcally consider connections form cijjij usual symmetric matrix derived hebbian learning. equals according given probability distribution note hciji/n cm/n dilution parameter. general diﬀerent leading asymmetry connections units. performed simulations three types connectivity focus analysis onto type simplest treat analytically. storage capacity curve three models estimated simulations shown later fig. known literature erdos-renyi graphs. many properties known random graph models known critical value essentially connected components graph trees critical value loops present. particular graph almost surely contain isolated vertices disconnected almost surely connected. threshold connectedness graph distinguishing highly diluted limit simpliﬁed analysis storage capacity possible intermediate case next section complete analysis necessary following approach shiino fukai random dilution capacity cannot analysed replica method symmetry interactions necessary condition existence energy function hence application thermodynamic formalism. therefore apply signal noise analysis. local ﬁeld unit state writes highly diluted limit most assumption ﬁeld written simply terms signal noise. signal pushes activity unit network conﬁguration converges attractor noise crosstalk patterns deﬂects network away cued memory pattern. noise term writes statistical independence units used. randomly correlated patterns terms vanish. identiﬁed non-zero term proceed capacity analysis. express ﬁeld using overlap standard gaussian variable. indeed positive constant highly diluted networks l.h.s. i.e. contribution ﬁeld non-condensed patterns approximately normally distributed random variable large number uncorrelated quantities. computed averaging connectivity distribution gaussian noise taking mean ﬁeld equations characterize ﬁxed points dynamics eqs. highly diluted limit however obtain last equation fully connected replica analysis network partial connectivity consider complex case partial connectivity i.e. approached self-consistent signal noise analysis previous section express ﬁeld using overlap parameter single contribution pattern retrieved label high enough connectivity however must revise mean ﬁeld computed reﬁned scsna method recapitulate positive constants represents noise determined self-consistently. ﬁrst term proportional resulting activity unit itself reverberated loops network; second term contains noise propagates units activation function writes connected case. equations found constitute generalization particular highly diluted limit αpnq/s results obtained previous section; fully connected case correspondence variables obvious shown algebraic manipulation. indeed following identity computer simulations conﬁrm analyses above? starting eﬀect setting overall threshold show fig. retrieval performance function threshold simulations solving eqs. figure often fully connected potts network retrieves memories function threshold number stored memories color represents fraction simulations overlap activity state network stored pattern solid line obtained numerical solution eqs.-. dependence diﬀerent values threshold already optimal value subtracting non-zero detrimental capacity adjusted considering lead optimal eﬀective threshold maximizing capacity. clear simulations agree well numerical results. maximum storage capacity found approximately also shown simple signal noise analysis. possible compute approximately standard ﬁeld respect distribution patterns deviation roughly consistent larger replica analysis simulations fig. given optimal value fig. shows eﬀect feedback term storage capacity purely subtractive shift right optimal value. eﬀect network parameters fig. illustrates eﬀect feedback term setting charting storage capacity function sparsity diﬀerent values fully connected highly diluted networks cases decreases monotonically increasing close optimal. increasing reaches region high therefore beneﬁts non-zero even though exact value critical. high sparsity parameter curves except seem coalesce. envelope diﬀerent curves represents optimal threshold setting takes figure storage capacity function sparsity diﬀerent values fully connected highly diluted networks obtained numerical solution eqs. also includes points simulations. parameters connectivity limit cases illustrated fig. shows dependence storage capacity sparsity fully connected diluted networks fig. instead varied fig. corresponding highly sparse limit curves distinct highly sparse network curves coalesce. curves obtained numerically solving eqs. moreover storage capacity curve fully connected case matches well fig. diluted curves always fully connected ones found figure storage capacity function sparsity dots correspond simulations network cm/n curves obtained numerical solution eqs. storage capacity function parameters illustrating limit case. eﬀect diﬀerent connectivity models fig. show simulation results storage capacity three connectivity models introduced earlier. sdrd networks seem almost identical capacity. models capacity fully connected case should. note particular limited decrease p/cm cm/n increasing almost full connectivity three models. particular model already shown analytically degree dilution almost eﬀect already moderate values network eﬀectively sparse coding regime cm/n becomes irrelevant. apparent decrease capacity cm/n values likely artefact small. results contrasted storage capacity connectivity models hopﬁeld model. hopﬁeld model eﬀects investigated found capacity decreases monotonically value highly diluted well-known value fully connected network instead highly diluted limit figure storage capacity curves obtained simulations function mean connectivity unit cm/n three diﬀerent types connectivity namely random dilution symmetric dilution state-dependent random dilution higher capacity capacity three models coalesces fully connected limit models become equivalent. simulations carried sets parameters studied value found. plausibly assume intermediate values interpolate highly diluted fully connected limit cases hopﬁeld network seems higher capacity however important note overlap network retrieves models highly diluted model authors zero temperature undergoes second order phase transition control parameter taking consideration hopﬁeld model increased capacity model respect predicted analytically well precision retrieval models behave similarly. clarify next section making potts-hopﬁeld correspondence exact diﬀerent sense considering multi-modular hopﬁeld model. note ﬁrst term hopﬁeld hamiltonian storing unbiased patterns modulo multiplicative term zero-temperature however overall rescaling energies leaves statistics system unchanged consider ﬁrst term exactly hopﬁeld hamiltonian. last term additive constant neglected second term figure setting threshold unit dependent hamiltonians models become equivalent. dots correspond simulations potts network latter parameters uninterrupted line corresponds analytical results obtained sompolinsky. dashed line read right y-axis corresponds overlap critical capacity. intermediate values connectivity cm/n simulation results analytical curve well particular well-known value fully connected network. higher levels dilution greater capacity predicted analytically. simulations performed network units. considering type hamiltonian considered sompolinsky system hamiltonian given simulated setting parameters potts network results compared analytical results derived latter study. unit-dependent threshold correlates learned patterns equivalence formulations hamiltonians spin binary variables ﬁrst found signiﬁcant storage biased patterns considered paper elaborate correspondence multi-modular neural network coarse grained potts network grounding hamiltonian potts model multi-modular one. units taken threshold-linear multi-modular model fully connected within module hebbian synaptic weights. sparse connectivity links units belong diﬀerent modules synapses cortex impinge primarily apical dendrites axons travelled white matter. relate potts states overlap correlation activity state module local memory patterns i.e. weighted combinations activity threshold-linear units. long range interactions modules roughly correspond suitable assumptions inhibition tensorial couplings potts units potts hamiltonian. becomes apparent w-term initially introduced model positive state-speciﬁc feedback potts units arises short range interactions multi-modular hamiltonian. keeping w-term potts hamiltonian apply replica method derive analytically storage capacity fully connected potts model. simpliﬁed derivation applied also highly diluted connectivity network case intermediate connectivity studied self-consistent signal-to-noise analysis. intermediate results smoothly interpolate limit cases fully high diluted networks limit cases fact similar capacity measured p/cm sparse coding limit limit approached rapidly potts model relevant parameter fact a/s. eﬀect term eﬀectively vicinity memory states reduced altering threshold leads storage capacity suppressed term threshold originally close optimal value. assumes threshold close optimal value taking feedback term account value becomes irrelevant storage capacity still aﬀects network dynamics storage capacity parameters storage capacity potts network primarily function parameters suﬃce broadly characterize model minor adjustments factors. parameters considered reﬂect cortically relevant quantities? critical issue make cortical sense distinct thermodynamic phases analysed potts model develop informed conjectures cortical phase transitions potts network potts variables requires fully connected case nm··s/ connection variables diluted case would variables multi-modular hopﬁeld network shown sect. long-range synaptic weights. diluted connectivity modules summarily represented potts network tensorial weights. therefore number potts weights cannot larger total number underlying synaptic weights represents. cannot larger simple braitenberg model mammalian cortical connectivity motivated multi-modular network model total number pyramidal cells ranges small mammalian brain large one. large e.g. human cortex module taken correspond roughly cortical surface also estimated include pyramidal cells module however cannot plausibly considered fully connected; available measures suggest that even shortest distance connection probability pyramidal cells order therefore write departing assumption simplest version braitenberg’s model .nu. keep approximate equivalence would imply also .nu. inserting inequality above yields constraint argue however another constraint limits value given potts connectivity number local patterns neurons receiving connections given associative storage patterns stringent upper bound compatible small scaling linearly well small scaling intermediate regimes. would lead take proportional order mammalian cortices diﬀerent scale essentially scaling like fourth root total number pyramidal cells appears like plausible rough modelling assumption. yield estimates actual capacity cortex given species. major factor estimates take account however correlation among memory patterns. analyses reported apply randomly assigned memory patterns. case correlations treated elsewhere considerations sound rather vague. neglect inter alia large variability number spines hence probably synapses among cortical areas within species capture however quantitative change perspective aﬀorded coarse graining inherent potts model. simplify argument neglecting sparse coding well exact value numerical pre-factor potts model uses nmcms weights store kcms/ memory patterns containing order bits information therefore storing bits weight. respect keeping frolov conjecture diﬀerent associative memory network based hebb-like plasticity including multi-modular model eﬀectively represents. multi-modular model bits available allocated memory however patterns speciﬁed single-neuron detail hence contain order nunm bits information each. network store retrieve number them argued limited memory glass problem order magnitude number local attractors limited order perhaps argued above glossing single-neuron resolution potts model forfeits locally extensive character information contained pattern losing factor gains factor cms/ number patterns. whether scales between upshot more less informative memories. therefore focusing long range interactions potts model misses information eﬀectively circumvents memory glass issue plagued earlier incarnation braitenberg idea stores patterns. possible potts model reduced description underlying multi-modular model? trick likely hebbian form tensor interactions straightforward reduction implies inhibitory regulation multi-modular model attempted achieve. argument expanded made precise considering again plausible scenario correlated memories. finally separate studies needed also assess dynamical properties potts network also reﬂect strength w-term begun undertake earlier paper elsewhere analysis dynamics reveal unique statistical properties large cortices expressed latching dynamics work supported human frontier collaboration groups naama friedmann rémi monasson analog computations underlying language mechanisms hfsp rgp/.", "year": "2017"}