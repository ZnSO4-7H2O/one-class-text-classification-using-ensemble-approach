{"title": "Deep Reinforcement Learning of Cell Movement in the Early Stage of C.  elegans Embryogenesis", "tag": "q-bio", "abstract": " Cell movement in the early phase of C. elegans development is regulated by a highly complex process in which a set of rules and connections are formulated at distinct scales. Previous efforts have shown that agent-based, multi-scale modeling systems can integrate physical and biological rules and provide new avenues to study developmental systems. However, the application of these systems to model cell movement is still challenging and requires a comprehensive understanding of regulation networks at the right scales. Recent developments in deep learning and reinforcement learning provide an unprecedented opportunity to explore cell movement using 3D time-lapse images. We present a deep reinforcement learning approach within an ABM system to characterize cell movement in C. elegans embryogenesis. Our modeling system captures the complexity of cell movement patterns in the embryo and overcomes the local optimization problem encountered by traditional rule-based, ABM that uses greedy algorithms. We tested our model with two real developmental processes: the anterior movement of the Cpaaa cell via intercalation and the rearrangement of the left-right asymmetry. In the first case, model results showed that Cpaaa's intercalation is an active directional cell movement caused by the continuous effects from a longer distance, as opposed to a passive movement caused by neighbor cell movements. This is because the learning-based simulation found that a passive movement model could not lead Cpaaa to the predefined destination. In the second case, a leader-follower mechanism well explained the collective cell movement pattern. These results showed that our approach to introduce deep reinforcement learning into ABM can test regulatory mechanisms by exploring cell migration paths in a reverse engineering perspective. This model opens new doors to explore large datasets generated by live imaging. ", "text": "measurements made every cell characterize developmental behaviors massive recordings contain hundreds thousands cells hours days development provide unique opportunity cellular-level systems behavior recognition well simulation-based hypothesis testing. agent-based modeling powerful approach analyze complex tissues developmental processes previous effort observationdriven agent-based modeling analysis framework developed incorporate large amounts observational/phenomenological data model individual cell behaviors straightforward interpolations timelapse images ultimate goal model individual cell behaviors regulatory mechanisms tremendous challenges still remain deal scenarios regulatory mechanisms data collection potential mechanistic insights need examined complex phenomena. directional cell movement critical many physiological processes elegans development including morphogenesis structure restoration nervous system formation. known that processes cell movements guided gradients various chemical signals physical interactions cell-substrate interface mechanisms remains open interesting challenge could learn rules mechanisms cell movement movement tracks recorded live imaging. paper presents approach study cell movement adopting deep reinforcement learning approaches within agent-based modeling framework. deep reinforcement learning good dealing high-dimensional inputs optimize complex policies primitive actions naturally aligns complex cell movement patterns occurred elegans embryogenesis. even biological scenarios regulatory mechanisms completely studied deep neural networks adopted characterize cell movement within embryonic system. neural network takes information time-lapse images direct inputs output cell’s movement action optimized collection regulatory rules. since deep reinforcement learning optimize cell migration path considerable temporal spatial spans global perspective overcomes local optimization problem encountered traditional rule-based agent-based modeling uses greedy algorithms. abstract— cell movement early phase elegans development regulated highly complex process rules connections formulated distinct scales. previous efforts demonstrated agent-based multiscale modeling systems integrate physical biological rules provide avenues study developmental systems. however application systems model cell movement still challenging requires comprehensive understanding regulation networks right scales. recent developments deep learning reinforcement learning provide unprecedented opportunity explore cell movement using time-lapse microscopy images. present deep reinforcement learning approach within agent-based modeling system characterize cell movement embryonic development elegans. modeling system captures complexity cell movement patterns embryo overcomes local optimization problem encountered traditional rule-based agent-based modeling uses greedy algorithms. tested model real developmental processes anterior movement cpaaa cell intercalation rearrangement superﬁcial leftright asymmetry. ﬁrst case model results suggested cpaaa’s intercalation active directional cell movement caused continuous effects longer distance opposed passive movement caused neighbor cell movements. learning-based simulation found passive movement model could lead cpaaa predeﬁned destination. second case leader-follower mechanism well explained collective cell movement pattern asymmetry rearrangement. results showed approach introduce deep reinforcement learning agentbased modeling test regulatory mechanisms exploring cell migration paths reverse engineering perspective. model opens doors explore large datasets generated live imaging. recent developments cutting-edge live microscopy image analysis provide unprecedented opportunity systematically investigate individual cells’ dynamics quantify cellular behaviors extended period time. systematic single-cell analysis elegans highly desired quantitative measurement cellular behaviors based time-lapse imaging entire cell lineage automatically traced quantitative tested model representative scenarios elegans embryogenesis anterior movement cpaaa intercalation rearrangement superﬁcial left-right asymmetry. ﬁrst case proposed hypotheses intercalation cpaaa simulation results indicated cpaaa experienced active directional movement towards anterior caused continuous effects longer distance rather passive process squeezed target location neighbors’ movements. second case frequently occurring leader-follower mechanism also supported simulation results asymmetry rearrangement. summary framework presents reverse engineering perspective investigate regulatory mechanisms behind certain developmental process formulating reward functions representation regulatory mechanisms different hypotheses tested reinforcement learning procedures. comparing extent similarities simulation cell migration paths observation data hypotheses either supported rejected facilitate explanations certain cell movement behaviors. model also used study cell migration paths elegans mutants metazoan embryo/tissue systems related data given. modeling framework individual cell modeled agent contains variety information fate size division time group information. wild-type elegans simulation cell fate division information directly derived predeﬁned observation datasets. complicated cases involve gene mutation manipulation developmental landscape incorporated purpose modeling detailed design information agent-based model found study cellular movements treated results inherited genetically controlled behaviors regulated interintracellular signals cell movements also constricted neighbor cells eggshell. assume migration path individual cell optimal path cell migrate collection regulation networks and/or constraints within physical environment. transform cell movement problem neural network construction learning problem using observational and/or predeﬁned rules. therefore neural networks constructed inside cell represent behaviors reinforcement learning method used train neural networks time-lapse imaging training neural networks determine feasible optimal cell migration path dynamic embryonic system migration path still controlled constrained underlying regulation networks physical environment. basic kinds individual cell movements investigated. ﬁrst movement pattern directional movement regulation network presents strong signals results directional individual cell movements. second type cell movement deﬁned passive cell movement represents scenarios explicit movement patterns observed signals regulation networks weak canceled out. directional cell movement stage strong regulation signals regulation networks cell movement mainly controlled potential destination physical pressures neighbor cells eggshell. destination cell movement deﬁned spatial location region within embryonic system regulatory mechanisms well studied deﬁned location next speciﬁc cell. passive cell movement stage without strong overall regulation mechanisms cell movement mainly controlled physical pressures neighbor cells eggshell. therefore deﬁned passive cell movement high level randomness. elegans embryonic system individual cells also part functional group group-speciﬁc communication regulation mechanisms. collective cell migration cell movements directional. however depending role cell movement cells collective migration categorized leading cells following cells. platform adopted present fundamental cell behaviors including cell fate division migration wild-type elegans cell fates predeﬁned. framework retains fundamental characteristics elegans early embryogenesis illustrated fig. terminologies intelligent cell dumb cell represent cell learns migration path move based observation dataset respectively. time step cell ﬁrst moves next location determined either output action neural network observation data that right time division cell hatched. global timer updated cells acted single time step loop repeats process. fig. framework. cells move time step based output neural network reading observed locations cell’s movement right time division cell hatched. process repeats simulation. mentioned modeling approach section cell movement modeled reinforcement learning process agent interacts environment achieve predeﬁned goals. individual cell movement case intelligent cell always tends seek optimal migration path towards destination based regulatory rules. discrete time step cell senses environmental state embryo selects action includes candidate actions state. embryo returns numerical reward cell evaluation action based state. finally cell enters next state repeats process terminal condition triggered. intelligent cell’s objective maximize overall rewards collected process. whole process demonstrated fig. traditionally tabular-based q-learning approaches largely used reinforcement learning tasks modest amounts input states. however dynamic agent-based embryogenesis model usually contains hundreds cells high temporal spatial resolutions. millions different states generated single embryogenesis process cannot handled traditional tabular-based q-learning algorithms. furthermore traditional q-learning algorithm requires large computational resources tightly integrated within agent-based modeling framework large-scale simulations high-dimensional inputs. recent breakthroughs reinforcement learning incorporate deep neural networks mapping functions allow feed high-dimension states obtain corresponding q-values indicate cell’s next movement fig. reinforcement learning framework. cell interacts embryo. time step cell receives state selects action gets reward enters next state st+. cell’s objective maximize total rewards received. framework implemented customized cell movement modeling. contains main loops cell migration loop network training loop time step cell migration loop state tracker used collecting input state representation environmental conditions ǫ-greedy strategy implemented balance exploration exploitation. speciﬁcally hyperparameter random number sampled uniform distribution time selection action. intelligent selects random action obtains reward moves next location. otherwise movement action calculated feeding input state neural network. process repeats terminal condition triggered. training loop established based traditional q-learning algorithms. rather searching q-table maximal value q-values obtained neural network parameterized weights training samples tuples gathered migration loop. update process achieved minimizing loss function backpropagating loss whole neural network update α∇θl therefore intelligent cell gradually select better actions training process proceeds. fig. deep q-network framework cell movement cotnains cell migration loop network learning loop. intelligent cell’s movement selected ǫgreedy mechanism either random sampling possible actions output neural network. gets reward moves next location repeats process. samples generated cell migration loop used update parameters neural network backpropagation. experience replay target network implemented improve performance. network framework. experience replay cuts correlation samples storing movement tuples replay memory sampling randomly training process. capacity replay buffer much larger number samples generated single process randomly selected samples training time come various processes much less related consecutive samples single process. single neural network target gradient descent always shifting updated time step. therefore rather calculating future maximal expected reward maxa updating weights single neural network target network architecture original network parameterized implemented calculation maxa weights remains unchanged iterations updated online network. mechanism reduces oscillations improve stabilities framework. improved process represented reinforcement learning scenario regulatory mechanisms guide cell movements transformed reward functions evaluation well cell moves certain period time based mechanisms. physical constraints cell movement deﬁned following rules rules threshold distance reached terminal condition triggered process ends restarts. directional cell movement explicit destination given simpliﬁed third rule regulatory mechanisms missing rule replaced speciﬁc regulatory mechanisms discovered hypotheses formulated. details reward settings illustrated section supplementary material automated cell lineage tracing technology utilized obtain information cells’ identities locations time-lapse microscopy images. information used model non-intelligent cells’ movement. temporal resolution observation data minute simulation often requires much smaller tick interval linear interpolation implemented consecutive samples calculate next locations cells. additionally added random noise movement sampling normal distribution whose mean value standard deviation averaged locations cells wild-type elegans embryos intelligent cell ǫ-greedy strategy implemented makes based past experiences maximize accumulated rewards time also gives small chance randomly explore unknown states. usually value increase training process proceeds. demands exploration narrows intelligent cell moves towards destination. selection varies case case details demonstrated supplementary material following sub-sections give description fig. example speciﬁc evaluation step single action. list cells pre-selected state cells cell neighbor determination model. locations concatenated sent neural network output action maximal probability selected intelligent cell’s next movement. acceleration training process. reinforcement learning architecture integrated part agent-based model. computations executed dell precision workstation conﬁgured -core intel xeon main memory nvidia quadro gpu. live time-lapse images elegans embryogenesis data used study cell movement. cell lineage traced starrynite manually corrected acetree acetree also used visualize observation data. detailed information live imaging found supplementary material special elegans biological phenomena intercalation cpaaa left-right asymmetry rearrangement investigated. ﬁrst case remarkable process elegans early morphogenesis dorsal hypodermis. cpaaa born dorsal posterior. minutes later birth cpaaa moves towards anterior intercalates branches abarp cells give rise left right seam cells respectively. intercalation cpaaa consistent among wild-type embryos. leads bifurcation abarp cells correct positioning seam cells. second case left-right asymmetry rearrangement. signiﬁcant development scenario -cell stage left-right symmetry broken skew aba/abp spindle. right cell abpr positioned posterior left cell abpl. stage movement abpl abpr cells start restore spatial symmetry i.e. abpl cells move towards posterior abpr cells move towards anterior. -cell stage abpl abpr cells symmetry axis. asymmetry rearrangement achieves superﬁcially symmetric body plan embryo considered ellipsoid volume estimation. mounting technique aligns axis embryo z-axis data lengths axes obtained ﬁnding minimum maximum cell positions along estimation cell radius ratio cell volume input states representing input state accurately efﬁciently issue deep reinforcement learning framework cell movement. besides location intelligent cell indispensable intuitive assumption neighbors represent environment incorporated form input state. implemented neighbor determination model conservative manner purpose. speciﬁcally extracted number candidate cells might inﬂuence intelligent cell relatively loose condition cells would selected guarantee input state sufﬁciently represented. done running agent-based model non-reinforcement learning mode recording neighbors intelligent cell time step. finally combined locations cells ﬁxed order input neural network. output actions intuitive give intelligent cell many candidates actions possible make eligible choice simulation. diversity action includes different speeds directions. however number output nodes grows exponentially take looser strategies select action. based extensive experiments discovered enumeration eight directions action them good enough scenario. moreover ﬁxed speed based estimation average movement speed embryogenesis measured observation data. finally give example speciﬁc evaluation step single action selection process collect locations selected cells neighbor determination model concatenate form vector ﬁxed order feed neural network. output neural network q-values action corresponds maximal probability selected intelligent cell’s next movement. agent-based model implemented mesa framework python used python’s package tkinter purpose visualization. cell movement behavior model built coordinates certain slice whole embryo visualized manner illustrate emergent behaviors speciﬁcally happen. used pytorch achieve reinforcement learning algorithms advantage entire embryo determined based identity. then radius estimated considering cell sphere utilized linear functions deﬁne rewards simulations. speciﬁcally collision rule penalty exerted distance cells reached threshold. distance becomes smaller penalty linearly grows terminal threshold reached similarly boundary rule penalty calculated based distance intelligent cell eggshell. finally destination rule bigger positive rewards given cell moves towards destination. details demonstrated supplementary material environment initialized observation data live imaging automated cell lineage tracing. ﬁrst tested performance framework. platform conﬁgured track movements intercalation cell namely cpaaa ﬁrst process purpose illustration. although embryo measured length dorsal-ventral axis considered space dorsal side cpaaa’s intercalation happens. entire space visualized projecting cells space center plane based result found movement path cpaaa consistent time-lapse images. visualized cell sizes largely consistent observation data except fact them especially located planes away center plane slightly different sizes visually. however differences insigniﬁcant impact cell movement modeling. unlike supervised learning tasks classiﬁcation regression evaluating performance quite challenging deep reinforcement learning tasks. followed evaluation metric quantify general performance system. total rewards cell collects single movement path generally goes upward tends quite noisy since tiny changes weights neural network results large changes actions cell chooses training loss tends oscillate time reason behind implementation experience replay target network correlation training samples. finally extracted states running model nonreinforcement learning collecting state cells’ locations. predeﬁned states neural network training process. turns average action values states grows smoothly training process encounter divergence problems though convergence still active research area. sometimes experienced unstable training scenarios problems could solved implementing learning rate decay strategy. examined hypotheses individual cell movement cpaaa intercalation case speciﬁcally tested whether cpaaa’s intercalation results active directional movement passive movement whether passive movement mechanism sufﬁcient explaining migration path cpaaa’s neighbors. case observed fact ﬁrst four minutes process intercalating cell cpaaa moves randomly. extensive divisions abarp cells cpaaa changes behavior directional movement process. signal triggering switch come newborn abarp cells. directional cell movement process unexpected regularization signals irregular movement patterns considered. study deﬁned possibility selecting directional movement neural network ratio value zero means completely random movement value means completely directional cell movement. regulatory mechanisms cpaaa intercalation case trained individual neural networks directional passive movements different sets regulatory mechanisms. speciﬁcally trained neural network passive movement collision boundary rules directional movement addition destination rule. different behaviors cpaaa accumulated rewards generally goes upward tends noisy. loss tends oscillate implementation experience replay target network. average action value grows smoothly time. directional movement that) controlled manipulating probability random movement action selection procedure. results simulation cpaaa destination rule show ﬁrst four minutes intelligent cell didn’t explicit destination large extent acted randomly. that cpaaa switched behavior began move directionally destination well kept proper distances neighbors eggshell. whole migration path largely reproduced live microscopy images however trained cpaaa without destination rule failed identify migration path fell suboptimal location kept proper distances neighbors also trained neighbor cpaaa namely caaaa passive movement cell process migration path scenario also reproduced images indicated caaaa played passive role cpaaa’s intercalation. veriﬁcation generality model random noises added initial positions cells migration paths dumb cells training process. turns neural networks could still provide proper actions large variety input states policy converges though optimization process took longer converge scenarios without random noises. migration path intelligent cell found qualitatively intelligent cell cpaaa adopted similar migration path destination directional movement setting compared observation case though minute observation movement cpaaa went towards anterior faster simulation path. difference simulation observation results indicates extra regulatory mechanisms could considered control cell movement whole cpaaa intercalation process. hand without destination rule cpaaa’s simulated path quite away observed path used mean square error quantitative measurement simulated path observed path. turns fig. much smaller fig. conclusion results show cpaaa’s intercalation regulated active directional movement mechanism strongly inﬂuenced destination rule rather passive movement mechanism. moreover another interesting ﬁnding standard deviation migration path cpaaa destination rule controlled proper range whereas path without destination rule diverges time goes result indicates intelligent cell achieves error correction mechanism migration path destination. experiment trained neural network test cell movement group migration case left-right asymmetry rearrangement. rather explicitly pointing destination intelligent cell follow leading cell reward setting modiﬁed accordingly distance leading cell following cell proper range positive reward given. results show abplpaapp always moves following leading cell keeps proper distances neighbors. although identify cell leading cell intelligent cell gradually ﬁgure nearby cell leading cell training process following leading cell achieve reward. results consistent observation data shows ﬂexibility model replacing destination rule concrete ones. study presented novel approach model cell movement using deep reinforcement learning within agent-based modeling framework. study showed neural networks adopted characterize cell movement deep reinforcement learning approach used optimal migration path cell certain regulatory mechanisms. comparing fig. results cpaaa intercalation case. observation results visualized acetree time-lapse images. simulation results intercalating cell cpaaa destination rule. simulation results training cpaaa boundary collision rules without destination rule indicate cpaaa fell suboptimal location. simulation results cell caaaa neighbor cpaaa. yellow green circles represent intelligent cell input state cells non-related cells respectively. white circle indicates destination intelligent cell. four sets data collected following time steps heuristic rule-based agent-based models macroscopical behaviors studied model provides point view single cell movements deﬁned optimized considerable period time. cpaaa intercalation case tested hypotheses might explain cpaaa’s migration towards anterior manipulating reward settings simulation results rejected passive movement assumption comparisons simulated observed paths cpaaa. results indicated target site speciﬁcation simpliﬁed representation morphogen gradient effective approach cell migration path learning especially regulatory mechanisms data collection. left-right asymmetry rearrangement case demonstrated framework capability generalize destination rule speciﬁc mechanisms explain certain cell movement behaviors. comparing simulated cell migration path regulated proposed assumptions observed path reverse engineering perspective framework used facilitating hypotheses certain developmental processes elegans model captures main aspects cell movement provides idea represents cell behaviors neural networks trained deep reinforcement learning algorithms. powerful models implemented following aspects multi-agent reinforcement learning used studying cooperative/competitive cell behaviors manipulating rewards framework. extension provide biological insights. example cpaaa intercalation case investigate whether certain group cells works cooperatively neighbors actually competitively rules speciﬁcally observed last minutes process cell abarpaapp moves posterior become neighbor cpaaa. interesting study whether abarpaapp helps cpaaa intercalate towards anterior migration abarpaapp dislocation hierarchical regulatory mechanism another area interest. although destination rule provides simpliﬁed representation morphogen gradient generalized formation certain cell neighbor relationships. cpaaa intercalation case intelligent cell experiences series changes neighbor relationships reaching target site. worth investigating whether relationships play signiﬁcant sub-goals serve ultimate goal. presented deep q-network performs poorly hierarchical tasks. tasks require advanced strategies obtained prior knowledge hardly represented input state. therefore future work immediately needed implement hierarchical deep reinforcement learning architectures meet demands advanced training strategies reinforcement learning algorithms also worth investigating improve performance model learning rate decay continuous control asynchronous methods finally hope incorporate biological domain knowledge model simulate complex cell movement behaviors. previous effort developed developmental landscape mutated embryos mutated cell fate information research integrated part input state study cell’s migration path mutant. faterelated adjustments regulatory mechanisms reward functions behind them veriﬁed/rejected hypotheses certain cell movement behaviors mutant based extent differences simulated path observed path. furthermore comparing simulation observation paths design biological experiments follow-up investigations. concepts cell-cell adhesion environmental factors also fig. migration paths cpaaa directional movement. simulation results training cpaaa boundary collision rules without destination rule. results indicate cpaaa fell suboptimal location. simulation paths averages runs shaded regions indicate ranges standard deviation greater/less average values. horizontal axis represents developmental time minutes. vertical axis represents projected position cpaaa ap-axis center embryo. fig. simulation left-right asymmetry rearrangement. observation data. intelligent cell leading cell circled. simulation results. cyan circle represents leading cell others color coded fig. white circle indicates destination intelligent cell purpose visualization. sets data collected following time steps paper successfully developed cell movement modeling system integrating deep reinforcement learning framework. modeling system learn cell’s optimal path certain regulatory mechanisms thus examine hypotheses comparing similarities simulation cell migration paths observation data. capabilities turn provide opportunities explore large datasets generated live imaging. zhirong john murray thomas boyle siew loon matthew sandel robert waterston. automated cell lineage tracing caenorhabditis elegans. proceedings national academy sciences united states america thomas boyle zhirong john murray carlos araya robert waterston. acetree tool visual analysis caenorhabditis elegans embryogenesis. bioinformatics lucian busoniu robert babuska bart schutter. comprehensive survey multiagent reinforcement learning. ieee transactions systems cybernetics-part applications reviews zhuo anthony santella michael tiongson zhirong bao. novo inference systems-level mechanistic models development live-imaging-based phenotype analysis. cell maxim egorov. multi-agent deep reinforcement learning. claudiu giurumescu sukryool kang thomas planchon eric betzig joshua bloomekatz deborah yelon pamela cosman andrew chisholm. quantitative semi-automated analysis morphogenesis single-cell resolution complex embryos. development ¨urgen hench johan henriksson martin ¨uppert thomas b¨urglin. spatio-temporal reference model caenorhabditis elegans embryogenesis cell contact maps. developmental biology tejas kulkarni karthik narasimhan ardavan saeedi josh tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. advances neural information processing systems koji kyoda adachi eriko masuda yoko nagai yoko suzuki taeko oguro mitsuru urai ryoko arai mari furukawa kumiko shimada wddd worm developmental dynamics database. nucleic acids research timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous control deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature john isaac murray thomas boyle elicia preston dionne vafeados barbara mericle peter weisdepp zhongying zhao zhirong boeck robert waterston. multidimensional regulation gene expression elegans embryo. genome research ralf schnabel harald hutter moerman heinke schnabel. assessing normal embryogenesis incaenorhabditis elegansusing microscope variability development regional speciﬁcation. developmental biology eric wang anthony santella wang dali wang zhirong bao. visualization -dimensional vectors dynamic embryonic systemwormguides. journal computer communications wang benjamin ramsey dali wang kwai wong husheng eric wang zhirong bao. observation-driven agent-based modeling analysis framework elegans embryogenesis. plos wang dali wang husheng zhirong bao. cell neighbor determination metazoan embryo system. proceedings international conference bioinformatics computational biology health informatics", "year": "2018"}