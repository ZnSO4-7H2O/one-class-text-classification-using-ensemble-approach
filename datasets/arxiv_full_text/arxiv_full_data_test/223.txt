{"title": "A Radically New Theory of how the Brain Represents and Computes with  Probabilities", "tag": "q-bio", "abstract": " The brain is believed to implement probabilistic reasoning and to represent information via population, or distributed, coding. Most previous population-based probabilistic (PPC) theories share several basic properties: 1) continuous-valued neurons; 2) fully(densely)-distributed codes, i.e., all(most) units participate in every code; 3) graded synapses; 4) rate coding; 5) units have innate unimodal tuning functions (TFs); 6) intrinsically noisy units; and 7) noise/correlation is considered harmful. We present a radically different theory that assumes: 1) binary units; 2) only a small subset of units, i.e., a sparse distributed representation (SDR) (cell assembly), comprises any individual code; 3) binary synapses; 4) signaling formally requires only single (i.e., first) spikes; 5) units initially have completely flat TFs (all weights zero); 6) units are far less intrinsically noisy than traditionally thought; rather 7) noise is a resource generated/used to cause similar inputs to map to similar codes, controlling a tradeoff between storage capacity and embedding the input space statistics in the pattern of intersections over stored codes, epiphenomenally determining correlation patterns across neurons. The theory, Sparsey, was introduced 20+ years ago as a canonical cortical circuit/algorithm model achieving efficient sequence learning/recognition, but not elaborated as an alternative to PPC theories. Here, we show that: a) the active SDR simultaneously represents both the most similar/likely input and the entire (coarsely-ranked) similarity likelihood/distribution over all stored inputs (hypotheses); and b) given an input, the SDR code selection algorithm, which underlies both learning and inference, updates both the most likely hypothesis and the entire likelihood distribution (cf. belief update) with a number of steps that remains constant as the number of stored items increases. ", "text": "brain believed implement probabilistic reasoning represent information population distributed coding. previous probabilistic population coding theories share basic properties continuous-valued units; fully/densely-distributed codes; graded synapses; rate coding; units innate unimodal tuning functions units intrinsically noisy; noise/correlation generally considered harmful. present radically different theory assumes binary units; small subset units i.e. sparse distributed representation comprises individual code; functionally binary synapses; signaling formally requires single spikes; units initially completely flat units less intrinsically noisy traditionally thought; rather noise resource generated/used cause similar inputs similar codes controlling tradeoff storage capacity embedding input space statistics pattern intersections stored codes epiphenomenally determining correlation patterns across neurons. theory sparsey introduced years canonical cortical circuit/algorithm model achieving efficient spatiotemporal pattern learning/recognition elaborated alternative ppc-type theories. here show that active simultaneously represents similar/likely input entire similarity/likelihood distribution stored inputs given input sparsey’s code selection algorithm underlies learning inference updates likely hypothesis entire likelihood distribution number steps remains constant number stored items increases. like thank people encouraged pursue theory years including bullock jeff hawkins john lisman josh alspector mckenna andrew browning hammerstrom. also like thank colleagues neurithmic systems greg lesher jasmin leveille oliver layton harald ruda nick nowak help developing ideas. work partially supported darpa contracts fa--c- n--c- contract n--c- training grant introduction widely acknowledged brain must implement form probabilistic reasoning deal uncertainty world however exactly brain represents probabilities/likelihoods remains unknown also widely agreed brain represents information form distributed—a.k.a. population cellassembly ensemble—code relevant review]. several population-based probabilistic coding theories forth recent decades including state neurons comprising population i.e. population code viewed representing single likely/probable input value/feature entire probability/likelihood distribution features despite differences approaches share fundamental properties notable exception spike-based model neural activation continuous neurons coding field formally participate active code whether represents single hypothesis distribution hypotheses. representation referred fully distributed representation. individual neurons assumed intrinsically noisy e.g. firing poisson variability. noise correlation viewed primarily problems dealt with e.g. reducing deeper level clear despite framed population models really based underlying localist interpretation specifically individual neuron’s firing rate taken perhaps noisy estimate probability single preferred feature present receptive field models entail method combining outputs individual neurons e.g. averaging neuron viewed providing individual i.e. localist estimate input feature i.e. neuron possesses independent example seen quite clearly fig. wherein first layer cells unimodal therefore viewed detectors value modes pooling cells also -to- correspondence directions. underlying localist interpretation present models referenced well. however several compelling arguments localistically-rooted conceptions. experimental standpoint growing body research suggests individual cell heterogeneous classically conceived also described mixed selectivity greater fidelity heterogeneity modeled less neuronal response variation needs attributed noise leading question appropriateness traditional concept single neuron function invariant plus noise formal standpoint limitation long pointed maximum number features/concepts e.g. oriented edges directions movement stored localist coding field units importantly work shown computational time efficiency features/concepts signaling communicated wave contemporaneously arriving first-spikes afferent code downstream computation thus principal orders magnitude faster rate coding. means sparsey formally spiking model. initial weights afferent synapses neurons comprising field zero i.e. completely flat. roughly unimodal emerge side-effect model’s single/few-trial learning process laying sdrs superposition provided process choosing sdrs preserves similarity neurons less inherently noisy classically supposed. observed noise likely experimental limitations i.e. inability truly closely control input conditions. algorithmic information-bearing level individual neurons simply given computational cycle. make precise spike times assume meta-circuitry organizes transmission masse synaptic signals coding field within small window arrival downstream decoding field. working assumption purpose local gamma cycle neurons inherently less noisy assumed canonical mesoscale circuit explicitly noise resource. noise explicitly generated injected code selection process achieve specific coding goal. goal make code activated response input increasingly random function novelty i.e. minimize intersection activated code previously stored codes. generally goal overall codes stored coding field property similar inputs similar codes similarity measure codes size intersection. straightforward implications correlation described below. admits completely different concept representing computing probabilities possible either localist fully distributed representations. instead representing input/feature state single neuron localism vector real values neurons comprising coding field fully distributed coding represented subset neurons specifically small subset whole coding field refer code. neurons participating code fully active rest coding field’s neurons completely closely consistent combinatorial coding framework described analyzed thus communication probability target computations—i.e. decoding—can achieved neurons participating target computations simply summing binary synaptic inputs field active. thus representing using graded values e.g. probabilities requires binary neurons binary synapses. need explicit localist representation graded values anywhere system/computation. specifically need represent probabilities/likelihoods localistically form firing rates need represent conditional probabilities strengths association continuous/graded weights. moreover although neurons presumably communicate primarily spikes conceiving signal sent active code vector contemporaneously arriving first spikes essentially removes need cast treatment terms spiking model assumed intrinsic noisiness. rather approach character algorithm operating according discrete clock thus approach assume intrinsic noise ‘that dealt with’ rather we’ll describe injects state-dependent amount noise manipulating neurons’ transfer functions specifically nonlinearities achieve certain coding goals discussed below. underscores strongly distinguishing property sparsey. nonlinearity principal cells static true models highly dynamic specifically nonlinearities principal cells comprising coding field modulated masse correlated fashion fast time scale e.g. function global measure familiarity total input coding field. introduced similarity preservation involving continuous measure e.g. euclidean distance also established fully distributed coding models e.g. however knowledge codes interpreted simultaneously representing likely hypothesis entire distribution. case noted above models decoding downstream computations requires either graded synapses rate-coding. above refer coding field rather individual neuron. require that first approximation neurons coding field order assert codes field represent features present constraint relaxed extent facilitates initial exposition analysis. case constraint actually serves underscore massive gulf sparsey previous theories constraint needed even approximately underlying localist conception. sparsey evinces paradigm neuroscience ensemble single neuron considered fundamental functional unit i.e. move away ‘neuron doctrine’ example advocated expect rapidly advancing methods allowing observation activities neurons substantial volumes progressively greater temporal precision issues connectivity assumption well dynamics implied core algorithm addressable. results section present simulation-backed examples showing mechanistic details sparsey’s core algorithm code selection algorithm activates code best-matching likely input also activates entire similarity/likelihood distribution coarsely-ranked fidelity stored inputs. moreover accomplishes number steps remains constant number stored inputs increases property call fixed time complexity. first example shows case purely spatial inputs. second example shows coding field recurrently connected entire distribution updated time consistent statistics domain again fixed time. moreover explained previously also stores learns inputs spatial spatiotemporal fixed time. fact sparsey/csa viewed adaptive hashing method learns locality-sensitive i.e. similarity-preserving hash function data neurally inspired hashing models fixed-time best-match retrieval also fixed-time learning. fact none currently stateof-art hashing models described recent review possess fixed-time learning fixed-time best-match retrieval. although time complexity considerations like generally discussed probabilistic population coding literature essential evaluating overall plausibility models biological cognition uncontentious brain computes probabilistically also need explain extreme speed computations potentially quite large hypothesis spaces occur. crucial reason sparsey achieves fixed time performance learning best-match retrieval unique simple method computing global familiarity input using control selection code input. ‘global’ mean function cells comprising coding field contrast local familiarity cell local measure match input. crucially computing require explicitly comparing input every stored input rather computation whose time complexity dominated single pass fixed number afferent weights field implicitly performs comparisons simultaneously algorithmically parallel fashion. ‘algorithmic parallelism’ means single atomic operations affect multiple represented items. thus operationally ‘algorithmic parallelism’ close identical ‘distributed representation’ cannot without other. emphasize algorithmic parallelism machine parallelism orthogonal resources fully compatible. simplified version sufficient paper’s examples given table briefly summarize here. step computes input sums cells comprising coding field. specifically cell separate computed major afferent synaptic projections e.g. bottom-up horizontal top-down projections latter provide recurrence coding field. step requiring aforementioned single pass field’s afferent weights. step sums normalized step normalized sums multiplied yielding values. steps computed average across cms. remaining steps used nonlinearly transform distribution cells final distribution winner picked. influence distributions summarized follows. high global familiarity detected distributions exaggerated bias choice favor cells high input summations thus high local familiarities acts increase correlation. global familiarity detected distributions flattened reduce bias local familiarity acts increase expected hamming distance selected code previously stored codes i.e. decrease correlation. means sparsey achieves sisc. enforcement sisc learning ultimately makes possible immediate retrieval bestmatching hypothesis simultaneous fixed-time update stored hypotheses input. recent years much discussion nature causes uses correlations noise cortical activity; reviews. investigations neural correlation noise especially context theories assume priori fundamentally noisy neurons e.g. poisson spiking; tuning functions general form e.g. unimodal bell-shaped describe noise/correlation affects coding accuracy populations specifically treatments measure correlation terms either mean spiking rates spikes however noted above theory makes neither assumption. rather theory noise actively injected—implemented g-dependent modulation neuronal transfer function— learning achieve goals described above. thus pattern correlations amongst units simply emerges side-effect cells selected participate codes. overarching goal learning process simply enforce sisc. however enforcing sisc context coding field realizes balance between interestingly exploring implications shifting focus information theory coding theory terms influence upon theoretical neuroscience pointed tradeoff though treatment uses error rate instead storage capacity. point understanding neural correlation ultimately affects things like storage capacity considered largely unknown active area research approach implies straightforward answer. minimizing correlation i.e. maximizing average hamming distance codes stored coding field maximizes storage capacity. increases correlations pairs triples subsets order coding field’s cells decreases capacity. fig. shows sparsey’s particular format. coding field consists winner-take-all competitive modules consisting binary neurons. here thus codes exactly active neurons possible codes. assume input field binary neurons e.g. representing pixel patch visual field completely connected coding field i.e. aforementioned coding field. weights initially zero. fig. shows particular input associated particular code here blue lines indicate bundle weights would increased store association fig. sparse distributed representation coding field consists competitive modules. input field binary neurons completely connected coding field. particular input code consisting active neurons bundle weights would increased form association shown fig. shows strength presence i.e. probability feature coding field’s represented sdr-based theory. figure shows five hypothetical inputs learned i.e. associated codes hand-chose particular codes consistent principle similar inputs similar codes inputs progressively smaller overlaps therefore codes progressively smaller intersections shown statistically enforce sisc spatial spatiotemporal input domains input spaces plausible assume input similarity correlates probability/likelihood single active code therefore also viewed probability/likelihood distribution stored codes. shown lower part figure. leftmost panel bottom fig. shows active codes partially active proportions reflect similarities corresponding inputs thus probabilities/likelihoods inputs represent. remaining four panels show input similarity approximately correlating code overlap. fig. illustration probability/likelihood feature represented fraction code active. fully active hypothesis feature present considered maximally probable. similarities features probable feature correlate codes’ overlaps probabilities/likelihoods features represented fraction codes active. intersection columns black indicates units intersecting either input pattern code gray indicates non-intersecting units example fig. constructed illustrate desired property similarities thus likelihoods stored hypotheses simultaneously physically active whenever single hypothesis fully active. next section demonstrates achieves property purely spatial inputs following section spatiotemporal case. table presents simplified version code selection algorithm minimal steps needed demonstrations. specifically simplifications relative model internal level thus signals; spatial inputs exactly active pixels thus normalizer constant internal level consists single coding field thus normalizer constant fig. shows inputs previously stored model instance depicted fig. model binary pixel input level completely connected cells comprising mac. consists binary cells. second fig. shows novel test stimulus varying overlaps given inputs constrained exactly active pixels simply size intersection divided fig. shows code activated response construction similar black coding cells cells also indicates active cells green indicates inactive cells green cells given viewed substitution error. intention color coding cells retrieval trial model asked return closest matching stored input cells considered errors. note however sub-symbolic scale errors errors scale whole inputs whole inputs collectively represented entire code. example appropriate threshold settings downstream/decoding units would allow model whole return correct answer given cells code activated similar thresholding schemes associative memory models note however learning trial cells would considered errors would simply code assigned represent novel input respects similarity input space. fig. shows first message figure. active fractions codes representing stored inputs highly rank-correlated pixel-wise similarities inputs thus blue fig. represents fact code best matching stored input highest active code fraction cells active cyan next closest matching stored input indicates cells active general many common cells stored hypotheses. shown; intersection sizes matter indicated along right margin chart fig. note even code zero intersection cells common general expected code intersection zero input intersection condition zero chance since case winners chosen uniform distribution thus expected intersection case q/k. also assume occurred exactly training thus prior hypotheses flat. case posterior likelihood proportional other thus likelihoods fig. also viewed unnormalized posterior probabilities hypotheses corresponding stored codes. fig. response input codes learned inputs i.e. hypotheses activated strength correlated similarity current input learned input. test input similar learned input shown intersections panel thus code largest fraction active cells codes active rough proportion similarities associated inputs normalized input summations cells cms. values equal values purely spatial case transformed un-normalized probabilities sigmoid transform whose properties e.g. value depend parameters. values normalized true probabilities winner chosen details values second axis indexes cells values them. single cell much higher ultimately value rest others cells tied acknowledge likelihoods fig. seem high. less half pixels common etc. given particular input patters really reasonable consider high likelihood? bear mind example assumes experience model world single instances inputs shown. assume prior knowledge underlying statistical structure generating inputs. thus really relative values matter could pick parameters notably steps would result much less expansive sigmoid nonlinearity would result lower expected intersections learned codes thus lower likelihoods. main point simply expected code intersections correlate input similarity thus likelihood. fig. shows second message likelihood-correlated pattern activation levels codes apparent fig. achieved independent soft choices cms. fig. shows cells traces relevant variables used determine input summation active pixels indicated paper weights effectively binary though represented hence maximum value possible cell presented model example also assumes inputs exactly active pixels normalized value here thus vi=ui. cell’s value represents total local evidence activated. however rather simply picking cell winner would amount executing steps remaining steps executed distributions transformed described earlier winners chosen soft thus extremely cheap-to-compute global function whole used influence local decision process repeat emphasis part explicitly operates i.e. iterates over stored hypotheses; indeed explicit representations stored hypotheses operate. fig. shows different inputs yield different likelihood distributions correlate approximately similarity. input highest intersection different pattern intersections learned inputs well fig. shows codes stored inputs become active approximate proportion similarities i.e. likelihoods simultaneously physically represented fractions codes active. value case yields steps v-to-µ transform shown fig. applied cms. range given particular distributions shown fig. cell ends greatly favored lower-v cells. shows distribution second abscissa fig. gives within-cm indexes cells corresponding values immediately thus cell maps approximately whereas closest competitors cells maps similar statistical conditions exist cms. however three them cells tied cell contained code wins cell overall presentation activates code cells common manifesting high likelihood estimate finish demonstration spatial input case fig. shows presentation another input half pixels common half fig. shows codes become approximately equally active active codes. thus model representing hypotheses likely approximately equally likely. exact heights fluctuate somewhat across trials e.g. sometimes higher likelihood general shape distribution preserved. remaining hypotheses’ likelihoods also approximately correlate pixelwise intersections qualitative difference presenting readily seen comparing rows fig. seeing latter tied condition exists almost reflecting equal similarity approximately half cell wins intersects half winner intersects fig. three single black indicates codes intersect three cms. fig. details presenting novel inputs cases resulting likelihood distributions correlate closely input overlap patterns. panels show details example input goal section demonstrate moment-to-moment updating similarity/likelihood distribution stored inputs case particular spatiotemporal moments approximate agreement spatiotemporal similarity structure experienced inputs. fig. shows training consisting -item sequences items pixel patterns shown. fig. shows novel test sequences constructed slightly perturbed versions frames comprising training sequences. present details testing begin showing baseline details testing training sequence figures training sequences handcrafted show naturalistic edge motion patterns aperture visual field degree pixel overlap amongst frames low. test sequences constructed first second halves would unambiguously spatiotemporally similar terms measure pixel overlap first second halves training sequences. thus subsequence clearly similar subsequence subsequence clearly similar subsequence etc. overall goal demonstrations show likelihood distribution shifts reflect approximately spatiotemporal similarities stored hypotheses switch mid-sequence slightly noisy/perturbed versions learned sequences. fig. train test sequences used demonstrate frame-by-frame update likelihood distribution case spatiotemporal patterns. prime mark used indicate frame noisy/deformed version corresponding learning frame fig. shows state model i.e. input code activated response four moments test presentation sample active afferent weights moment shown. cell source weight active prior moment. panels show selected cell weights increased throughout learning thus seen selected cell active moment depicted pixels] moments well pixels]. figure also introduces moment notation. ‘moment’ mean particular spatial input context full item sequence preceded indicate enclosing sequence including current item brackets bolding current item. fig. shows processing details training sequence presented test. note model cells. total eight spatiotemporal moments stored learning. first moment presentation first item denoted followed shown fig. similarly moments shown main message fig. that successive item presented likelihoods eight stored hypotheses i.e. hypothesis current input moment etc. updated respects coarsely ranked spatiotemporal similarity structure experienced inputs. coarsely ranked mean following. exact repeat training instance likely moment time step occurred training instance. glance across four likelihood charts right verifies this code correct moment activated strongly others general time step likelihoods seven moments much lower i.e. falling second coarse rank. however distribution third time step quite plausibly described three ranks middle including moment appropriate fact item significant overlap presented first item learning trial prior learning weights occurred third moment learning trial caused high summations cells since first item signals present choice cells depends inputs consequently yielding relatively high intersection{ fig. charts show correct tracking likelihood achieved independent soft choices cms. panels correct cell yields reflects fact test sequence exact duplicate learned many cells either significant significant value reflecting crosstalk cells involved codes multiple moments both. fact incorrect cells appropriately zero near-zero values. thus distribution greatly favors correct cell. errors occur moment moment winners picked using soft occasional instances less likely neuron selected. nevertheless effect global familiarity equals four moments modulates v-to-µ transform cause almost whole stored code activate correctly i.e. increase correlation cells. seen figures lower values decrease correlation. thus theory provides novel causal indeed normative explanation correlation brain. degree correlation moment next learning retrieval effectively modulated deterministic mechanism modulation v-to-µ transform. indeed steps except last deterministic. fig. detailed trace information test presentation four items i.e. train=test sanity test. also training item sequence stored model detects item reinstates traces almost perfectly. v-to-µ function four moments four cases. show details v-to-µ transform upper right panel. likelihoods eight moments stored shown right eight moments panels shown panel reduce clutter. panel traces since relevant first item sequence consider novel sequences evidence hypothesis likelihoods measured active fractions codes track coarsely-ranked spatiotemporal similarity presented sequence moment moment. fig. shows state model four moments sequences noted earlier sequences constructed relative spatiotemporal similarities stored moments current test moment would clear even without exact spatiotemporal similarity metric. first moments must clearly considered closest first moments third moment spatially closest third moment spatiotemporal perspective i.e. considering immediately prior moments context model reasonably consider learned moment also elevated likelihood. likelihood panel fig. indeed shows moments highest likelihoods. precise likelihoods vary across test instances almost always highest. behavior flattened distributions resulting lower fact many cell cell tied nearly tied reasoning allows consider third moment ambiguous i.e. taking temporal context account suggests fourth moment judged less ambiguous fact likely instance learned moment indeed reflected likelihood panel fig. higher global familiarity results expansive v-to-µ transform which combination higher values cells results cells greatly favored thus model seen successfully gone ambiguous state sequence recovering on-line combining evidence yield appropriately less ambiguous internal state. example underscores another crucial capability model namely allowing stored hypotheses physically active allows transiently weaker hypotheses recover based future evidence countermand transiently stronger hypotheses. example fig. strongest hypothesis consistent overarching hypothesis currently unfolding sequence despite inconsistent evidence presented third time step input state consistent overarching hypothesis unfolding sequence however additional evidence inconsistent occurs fourth time step overarching hypothesis currently unfolding sequence becomes strongest. fig. shows results final example whose first moments constructed closest first moments whose second moments close last moments behavior model broadly analogous behavior third moment learned moments deemed likely model reasoning applied plausibly spatiotemporally similar global familiarity lower third moment introduced noise first moments fact spatial input intersection pixels common thus learned moments approximately likelihood fig. although cells comprising originally chosen spatiotemporal context therefore afferent weights increased signals present first item sequence. thus choice winners first moment depends signals thus increases made weights learning. learned moment receives high likelihood moment. fig. shows ambiguity present first moment greatly diminished presentation spatial input quite similar thus making spatiotemporal input moment much spatiotemporally similar stored moment stored moment learned moment. third moment present spatial input much similar spatial input. case third moment leads learned third moments approximately equally likely. however increased noise present first three moments compared lower likelihoods learned moments i.e. fractions codes active appropriately lower case third moment finally present spatial input fourth moment multiplication signals signals appreciable overlap cells comprising highly favored winning them yielding significantly higher likelihood moments seen fig. again model seen negotiate ambiguous moments updating active code moment moment simultaneously represents plausibly likely hypothesis full coarsely-ranked distribution hypotheses. pointed ability i.e. simultaneously representing multiple competing hypotheses example required representing motion transparency problematic theories fully distributed codes theories true numerical detail justifying relative similarities/likelihoods present likelihood distributions figures matter present spatial examples figures however examples demonstrate coarse correlation spatial spatiotemporal similarity likelihood similar capabilities able store successfully recognize/retrieve large numbers complex sequences item occur multiple times varying contexts previously demonstrated described radically different theory prevailing probabilistic population coding theories brain represents computes probabilities. theory avails context sparse distributed representation opposed fully distributed coding context models developed. theory sparsey introduced years model canonical cortical circuit computationally efficient explanation episodic semantic memory sequences interpretation representing computing probabilities emphasized. models share several fundamental properties continuous neurons; neurons formally participate every code; synapses must either graded rate-coding must used allow decoding; generally assume rate-coded signaling; individual neurons generally assumed unimodal e.g. bell-shaped tuning functions individual neurons assumed noisy e.g. firing poisson variability; noise correlation e.g. noise correlation generally viewed degrading computation needs mitigated e.g. averaged out. contrast properties/assumptions sparsey assumes binary neurons; individual codes small sets cells code simultaneously represents best matching stored hypothesis similarity distribution stored hypotheses; effectively binary synapses; signaling waves contemporaneously arriving first-spikes afferent codes; afferent weights coding neurons initially zero i.e. initially completely flat emerge single/few-trial learning reflect cell’s specific history inclusion codes; neurons assumed intrinsically noisy simply given computational cycle noise resource explicitly generated injected code selection process achieve specific coding goal namely overall codes stored coding field sisc property manifests indirectly particular patterns correlation amongst individual units. thus sparsey entails completely different view noise/correlation mainstream. rather viewed problem imposed externalities essentially functions positive sense i.e. resource. specifically showed that model uses coding; model’s process assigning codes preserves similarity input space code space iii) input similarity assumed correlate likelihood then active code simultaneously represents probable hypothesis likelihood/probability distribution stored hypotheses. likelihood/probability hypothesis represented fraction code’s cells active part current fully active code. spatiotemporal case successive sequence item sparsey’s core algorithm code selection algorithm updates entire distribution approximate accord intuitive notions spatiotemporal similarity number computational steps remains fixed number stored hypotheses increases. executing dominated single iteration weights number fixed life system. emphasize algorithmic efficiency learning retrieval shown computational method including hashing methods either neurally-relevant generally although time complexity considerations like generally discussed literature essential evaluating overall plausibility models biological cognition uncontentious brain computes probabilistically also need explain extreme speed computations potentially quite large hypothesis spaces occur. sparsey’s computational speed extremely efficient method computing global familiarity input using adjust individual cell’s transfer function local similarity measure final probability chosen winner viewed directly modulating noise present process code selection. high familiarity detected noise minimized disproportionately increasing bias towards selecting cells correlated input i.e. pattern completion familiarity detected noise maximized making probability cells equal i.e. pattern separation. sketch possible noise modulation mechanisms involving brain’s neuromodulators. emphasize mechanism constitutes radically novel concept noise correlation brain. moreover constitutes novel method combining global local information inference particular suggests possible necessity structural mesoscale facilitate global information local decision processes. opposite standpoint action also viewed controlling amount correlation amongst neurons relatedly controlling cells bound together represent inputs either purely spatial inputs spatiotemporal events thus providing similar functionality binding operations described models coding field homogenous field binary units number chosen particular code also true combinatorial neural codes well binary hashing models reviewed contrast sparsey coding field consists winner-take-all competitive modules comprised binary units. selecting code performed making independent draws cms. thus unlike homogenous-field models sparsey explicit structural mesoscale situated single neuron whole coding field. fact inception sparsey offered generic model cortical macrocolumn proposed analogous minicolumns hence synonymous coding field macrocolumn just mac. important consequence presence explicit structural mesoscale imposes specific fixed sparsity structurally. thus explicit computation need expended control sparsity model’s lifetime either learning retrieval contrasts prevalent technique adding penalty term cost function achieve sparsity sparse coding models entail continual computation throughout model lifetime. point sdr-based model numenta’s hierarchical temporal memory model mesoscale also equated cortical minicolumn however htm’s conception minicolumn differs radically ours. particular cells minicolumn priori cells minicolumn co-activate appropriate feature present minicolumn’s true hubel wiesel’s original results found cells along vertical penetrations visual cortex similar recent studies detailed probes refined observation methods revealing heterogeneous originally thought. suggests approach learned scratch arbitrarily heterogeneous wider applicability. addition htm’s assumption cells minicolumn even volume minicolumn activate simultaneously seems clearly odds experimental data calcium imaging e.g. moreover although uses fixed-density sdrs imposed structurally thus contrast sparsey require explicit computation determine subsets minicolumns activate given instance presumably learning retrieval. additional important point distinction like sparsey possess mesoscale knowledge published results thus involve single coding fields whereas describes results hierarchical sparsey models consisting multiple internal levels consisting multiple coding fields mesoscale architecture functionally crucial substantial evidence subsuming macrocolumn-scale various cortical regions/species crucial functional advantages associated scale well. addition sparsey’s efficiency purely algorithmic standpoint emphasize requires binary neurons synapses require rate coding. rather naturally suited signaling waves contemporaneously arriving first-spikes code next either recurrently downstream coding fields. thus rather reliably decoding spike frequency requires model requires window contemporaneous signals afferent code might arrive downstream coding field integrated. imagine sort macrocircuit-level control apparatus e.g. integration phase gamma-scale envelope might impose window hypothesis would like explore future. also emphasize potentially significantly lower metabolic/energy costs signaling based firstspikes compared signaling rate-coding addition already reduced energy costs sparse coding compared dense coding. sparsey existence proof representing using graded values e.g. probabilities requires binary neurons binary synapses. need explicit localist representation graded values anywhere system/computation. specifically need represent probabilities/likelihoods spatially localized firing rates need represent conditional probabilities strengths association continuous/graded weights. moreover contend replacing what localist model typically single real valued parameter representing relation e.g. conditional probability symbolic-level variables bundle i.e. synapsemble independent binary parameters allows flexible faster learning input domain’s statistics. developing argument near-term future research goals. sparsey distributed memory traces stored superposition therefore interference increase number hypotheses stored. given setting parameters regime number stored hypotheses enough expected interference i.e. expected retrieval error remains tolerable. overall sparsey system internal level level analyses characterizing capacity expected errors vis-à-vis parameters would primary research focus. case. single sparsey module proposed analog cortical macrocolumn. cortex known organized deep hierarchy perhaps cortical levels level patch order hundreds thousands macs. question overall storage capacity relates explaining apparent vast storage capacity typical human lifespan thus depend information distributed throughout dynamically interacts across entire hierarchy single mac. advantages organizing knowledge hierarchically categorically componentially long known. recently advantages many-leveled flat representations described terms efficiency representing highly nonlinear relations constant stream impressive deep learning results strongly bears however deep learning models including lstm thus combined indeed principles paradigms different essentially incompatible. regard must specifically point recently described sparsely-gated mixtures experts model exploiting principle sparsity instance present sparsey models. important difference sparsey sparsely-gated content input select experts respond thus used code input. generally true many instantiations drop-out principle contrast explained throughout sparsey’s implements spatial/spatiotemporal matching mechanism directly uses input control cells code input yields crucial sisc property. also exploring multi-level hierarchies coding fields e.g. model used explanation single-cell continuing path. believe truly capturing essence brain computes requires union hierarchy/heterarchical organization sparse distributed coding places work distinction with hand deep learning models combine densely/fully distributed coding fields hierarchy appears exception] hand hmax models combine localist coding fields hierarchy. point another theory probabilistic computation brain distributed representation essential much closer spirit sparsey despite fact theory shares properties mentioned outset high level sparsey described terms quite similar used describes theory three main parts overlapping patterns population activity proposed encode latent variables observed domain; brain specifies variables related world sparse probabilistic graphical model; recurrent circuitry implements nonlinear message-passing algorithm effectively executes probabilistic inference amongst latent variables represented population codes residing hierarchy coding fields. analog point sparsey ‘overlapping patterns population activity’ correspond subsets cells i.e. intersections occur amongst multiple codes i.e. across multiple contexts. subsets size smaller whole code size crucially emerge course learning. furthermore general intersections subsets well allowing encoding statistics range higher orders. analog point modifications made sparsey’s synaptic projections including recurrent projection projections to/from macs levels hierarchy embed probabilistic relations amongst latent variables. sparsey’s analog point essentially operation csa. characteristics mean models viewed distributed instantiations graphical probability models large localist e.g. hidden markov models bayesian nets dynamic bayesian nets. believe move localist concept distributed concept major implications notably domain’s latent variables mapped distributed codes emerge course on-line learning anonymous latent variables conditional probabilistic relations amongst variables represented partially code intersections partially intersections synaptic mappings also emerge course learning. many decades experimental methods finally reaching point fast time scale activities neurons large e.g. macrocolumn-scale volumes observable. finally allow understand brain’s operation believe native language essentially cell assemblies sequences cell assemblies view assembly functions likely hypothesis distribution memories stored offer sparsey theoretical elaboration concept simple i.e. binary cells synapses single-trial hebbian learning general extremely generic powerful terms computational efficiency. numerous questions pursue notably regarding nature/capacities hierarchical interactions amongst macs time look forward continuing explore them. similarity metric case spatial inputs simply pixel-wise overlap. inputs exactly number active pixels measure spatial similarity simply size intersection divided number examples figures spatiotemporal examples figures pixel overlap spatial measure frame individually semi-quantitative estimation regarding temporal aspects sequences described main text. input patterns spatial spatiotemporal update examples manually created minimize pixel-wise overlap otherwise similar natural inputs subject preprocessing consisting edge filtering binarization skeletonization. inputs used receptive field example created data using aforementioned preprocessing. learning hebbian simultaneous prepost-synaptic activation causes synapse weight binary case weights weight increased prepost-synaptic cells active successive time steps. full sparsey model additional learning principles modeled including decay permanence critical period details. simulations described paper small enough size terms total numbers inputs sequential case means total number frames across training sequences additional learning principles materially affect results/conclusions. nevertheless briefly describe additional principles. following synaptic weight increase pre-post coincidence initial period weight remains near decays approximately inverse logarithmic profile. second pre-post coincidence occurs within relatively small temporal window previous pre-post coincidence weight reset value permanence increases i.e. time scale decay greatly protracted. simulations date principles quantified explicit tabular forms rule-based tables described motivation expected time recurrence pre-post coincidences structural regularities input domain must clearly much shorter pre-post coincidences noise spurious alignments. thus described mechanism preferentially embed codes events structural regularities domain allowing spurious events fade memory. permanence mechanism/protocol form metaplasticity similar spirit cascade model well recent attempts deal catastrophic forgetting simpler models require explicit evaluation synapse’s importance/relevance global objective e.g. accuracy learned tasks. works concert critical period mechanism. discussed coding field finite storage capacity. code space exponential e.g. sparsey’s case codes embedded superposition interference accrues. fraction increased weights grows expected retrieval accuracy falls zero learning frozen i.e. critical period enforced. full model three principles/mechanisms work concert again paper’s simulations small enough none come play. given codes consist binary unit chosen competitive modules comprising coding field code similarity measured hamming distance normalized code size given assumption input similarity correlates likelihood; model preserves input similarity code similarity measure likelihood stored input references abbott dayan \"the effect correlated variability accuracy population code.\" neural computation ahmad hawkins \"properties sparse distributed representations application hierarchical temporal memory.\" aljundi babiloni elhoseiny rohrbach tuytelaars \"memory aware synapses learning forget.\" arxiv e-prints barlow \"single units sensation neuron doctrine perceptual psychology? perception barth poulet \"experimental evidence sparse firing neocortex.\" trends neurosciences bengio challenge learning complex functions. progress brain research. paul cisek john elsevier. volume bengio deep learning representations looking forward. statistical language speech processing first international conference slsp tarragona spain july proceedings. a.-h. dediu martín-vide mitkov truthe. berlin heidelberg springer berlin heidelberg bengio courville vincent representation learning review perspectives montreal. boerlin denève \"spike-based population coding working memory.\" plos computational biology bogacz \"optimal decision network distributed representation.\" neural networks bonin histed yurgenson reid \"local diversity fine-scale organization receptive fields mouse visual cortex.\" journal neuroscience buzsáki \"neural syntax cell assemblies synapsembles readers.\" neuron cohen kohn \"measuring interpreting neuronal correlations.\" neurosci dicarlo \"does learned shape selectivity inferior temporal cortex automatically generalize across retinal position?\" neurosci. ahmad hawkins \"continuous online sequence learning unsupervised neural network model.\" neural computation curto itskov morrison roth walker \"combinatorial neural codes mathematical coding theory perspective.\" neural comp deneve chalk \"efficiency turns table neural encoding decoding noise.\" current opinion neurobiology franke fiscella sevelev roska hierlemann azeredo silveira \"structures neural correlation favor coding.\" neuron fries \"neuronal gamma-band synchronization fundamental process cortical computation.\" annual review neuroscience fusi drew abbott \"cascade models synaptically stored memories.\" neuron fusi miller rigotti \"why neurons high dimensionality higher cognition.\" current opinion neurobiology georgopoulos kalaska caminiti massey relations direction two-dimensional movements cell discharge primate motor cortex.\" journal neuroscience grauman fergus learning binary hash codes large-scale image search. machine learning computer vision. cipolla battiato farinella. berlin heidelberg springer berlin heidelberg hamel elizabeth benjamin grewe jones parker mark schnitzer \"cellular level brain imaging behaving mammals engineering approach.\" neuron hebb organization behavior; neuropsychological theory. wiley. hecht-nielsen confabulation theory mechanism thought. berlin springer-verlag. hochreiter schmidhuber \"long short-term memory.\" neural computation igarashi colgin m.-b. moser moser \"coordination entorhinalhippocampal ensemble activity associative learning.\" nature jazayeri movshon \"optimal representation sensory information neural populations.\" neurosci jercog rogerson schnitzer \"large-scale fluorescence calcium-imaging methods studies long-term memory behaving mammals.\" cold spring harbor perspectives biology kanerva sparse distributed memory. cambridge press. kanerva spatter code encoding concepts many levels. proceedings international conference artificial neural networks sorento italy springer-verlag. kanerva \"hyperdimensional computing introduction cmoputing distributed representation high-dimensional random vectors.\" cognitive computing kirkpatrick pascanu rabinowitz veness desjardins rusu milan quan ramalho grabska-barwinska hassabis clopath kumaran hadsell \"overcoming catastrophic forgetting neural networks.\" pnas kohn coen-cagli kanitscheider pouget \"correlations neuronal population information.\" annual review neuroscience krizhevsky hinton using deep autoencoders content-based image retrieval. esann- bruges belgium. latham \"correlations demystified.\" neurosci lecun bengio hinton \"deep learning.\" nature beck latham pouget \"bayesian inference probabilistic population codes.\" neurosci jazayeri \"neural coding uncertainty probability.\" annual review neuroscience mante sussillo shenoy newsome \"context-dependent computation recurrent dynamics prefrontal cortex.\" nature marr theory cerebellar cortex.\" physiol moll miikkulainen \"convergence-zone episodic memory analysis simulations.\" neural networks moreno-bote beck kanitscheider pitkow latham pouget \"informationlimiting correlations.\" neurosci nandy anirvan jude mitchell monika jadi john reynolds \"neurons macaque area tuned complex spatio-temporal patterns.\" neuron nandy anirvan tatyana sharpee john reynolds jude mitchell \"the fine structure shape tuning area neuron ohki chung ch'ng kara reid \"functional imaging cellular resolution reveals precise micro-architecture visual cortex.\" nature olshausen field \"emergence simple-cell receptive field properties learning sparse code natural images.\" nature osborne palmer lisberger bialek \"the neural basis combinatorial coding cortical population response.\" neurosci. palm schwenker sommer associative memory networks sparse similarity preserving coding. statistics neural networks theory pattern recognition applications. cherkassky friedman wechsler. pearl probabilistic reasoning intelligent systems networks plausible inference mateo morgan kaufmann. perrinet sparse models computer vision. biologically inspired computer vision wileyvch verlag gmbh kgaa pitkow angelaki \"how brain might work statistics flow redundant population codes.\" plate common framework distributed representation schemes compositional structure connectionist systems knowledge representation deduction. maire hayward diederich queensland university technology poggio http//www.scholarpedia.org/article/models_of_visual_cortex. pouget beck latham \"probabilistic brains knowns unknowns.\" neurosci pouget dayan zemel \"information processing population codes.\" neurosci pouget dayan zemel \"inference computation population codes.\" annual review neuroscience rachkovskij kussul \"binding normalization binary sparse distributed representations context-dependent thinning.\" neural computation rajkumar pitkow inference reparameterization neural population codes. riesenhuber poggio \"hierarchical models object recognition cortex.\" neurosci riesenhuber poggio \"neural mechanisms object recognition.\" current opinion neurobiology rinkus combinatorial neural network exhibiting episodic semantic memory properties spatio-temporal patterns. boston rinkus neural model episodic semantic spatiotemporal memory. annual conference cognitive science society chicago lawrence earlbaum. rinkus population coding using familiarity-contingent noise areadne research encoding decoding neural ensembles. santorini rinkus cortical sparse distributed coding model linking minimacrocolumn-scale functionality.\" frontiers neuroanatomy rinkus \"quantum computing sparse distributed representation.\" neuroquantology rinkus cortical theory super-efficient probabilistic inference based sparse distributed representations. paris. rinkus lisman time-invariant recognition spatiotemporal patterns hierarchical cortical model caudal-rostral persistence gradient society neuroscience annual meeting washington rinkus \"sparsey^tm spatiotemporal event recognition deep hierarchical sparse distributed codes.\" frontiers computational neuroscience rosenbaum smith kohn rubin doiron \"the spatial structure correlated neuronal variability.\" neurosci salakhutdinov hinton semantic hashing. sigir workshop information retrieval applications graphical models. salakhutdinov hinton \"semantic hashing.\" international journal approximate reasoning sanger \"neural population codes.\" current opinion neurobiology schneidman \"towards design principles neural population codes.\" current opinion neurobiology serre kouh cadieu knoblich kreiman poggio theory object recognition computations circuits feedforward path ventral stream primate visual cortex. memo mit. shazeer mirhoseini maziarz davis hinton dean \"outrageously large neural networks sparsely-gated mixture-of-experts layer.\" silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis \"mastering game deep neural networks tree search.\" nature smith häusser \"parallel processing visual space neighboring neurons mouse visual cortex.\" nature neuroscience snaider franklin extended sparse distributed memory. bica. snaider franklin \"extended sparse distributed memory sequence storage.\" cognitive computation snaider franklin integer sparse distributed memory. flairs conference. snaider franklin \"modular composite representation.\" cognitive computation srivastava hinton krizhevsky sutskever salakhutdinov \"dropout simple prevent neural networks overfitting.\" mach. learn. res. wang kumar chang \"learning hash indexing data survey.\" proceedings ieee watrous fell ekstrom axmacher \"more spikes common oscillatory mechanisms content specific neural representations perception memory.\" current opinion neurobiology willshaw buneman longuet-higgins \"non holographic associative memory.\" nature s.-c. baker gray \"heterogeneity responses adjacent neurons natural stimuli striate cortex.\" journal neurophysiology yuste \"from neuron doctrine neural networks.\" neurosci zemel dayan pouget \"probabilistic interpretation population codes.\" neural comput.", "year": "2017"}