{"title": "Fast and Scalable Distributed Deep Convolutional Autoencoder for fMRI  Big Data Analytics", "tag": "q-bio", "abstract": " In recent years, analyzing task-based fMRI (tfMRI) data has become an essential tool for understanding brain function and networks. However, due to the sheer size of tfMRI data, its intrinsic complex structure, and lack of ground truth of underlying neural activities, modeling tfMRI data is hard and challenging. Previously proposed data-modeling methods including Independent Component Analysis (ICA) and Sparse Dictionary Learning only provided a weakly established model based on blind source separation under the strong assumption that original fMRI signals could be linearly decomposed into time series components with corresponding spatial maps. Meanwhile, analyzing and learning a large amount of tfMRI data from a variety of subjects has been shown to be very demanding but yet challenging even with technological advances in computational hardware. Given the Convolutional Neural Network (CNN), a robust method for learning high-level abstractions from low-level data such as tfMRI time series, in this work we propose a fast and scalable novel framework for distributed deep Convolutional Autoencoder model. This model aims to both learn the complex hierarchical structure of the tfMRI data and to leverage the processing power of multiple GPUs in a distributed fashion. To implement such a model, we have created an enhanced processing pipeline on the top of Apache Spark and Tensorflow library, leveraging from a very large cluster of GPU machines. Experimental data from applying the model on the Human Connectome Project (HCP) show that the proposed model is efficient and scalable toward tfMRI big data analytics, thus enabling data-driven extraction of hierarchical neuroscientific information from massive fMRI big data in the future. ", "text": "recent years analyzing task-based fmri data become essential tool understanding brain function networks. however sheer size tfmri data intrinsic complex structure lack ground truth underlying neural activities modeling tfmri data hard challenging. previously proposed data modeling methods including independent component analysis sparse dictionary learning provided shallow models based blind source separation strong assumption original fmri signals could linearly decomposed time series components corresponding spatial maps. given convolutional neural network successes learning hierarchical abstractions low-level data tfmri time series work propose novel scalable distributed deep autoencoder model apply fmri data analysis. model aims learn complex hierarchical structures tfmri data leverage processing power multiple gpus distributed fashion. deploy model created enhanced processing pipeline apache spark tensorflow leveraging large cluster nodes cloud. experimental results applying model human connectome project data show proposed model efficient scalable toward tfmri data modeling analytics thus enabling data-driven extraction hierarchical neuroscientific information massive fmri data. sheer complexity brain forced neuroscience community particularly neuroimaging experts transit smaller brain datasets much larger hard-tohandle ones. cutting-edge technologies biomedical imaging field well techniques digitizing lead collect information structural organization functional neuron activities brain rich imaging modalities like fmri projects human connectome project functional connectomes openfmri perfect examples large neuroimaging datasets. primary goal efforts gain better understanding human brain diagnose neurological psychiatric disorders. among various neuroimaging methods functional magnetic resonance imaging tfmri widely used assess functional activity patterns cognitive behavior human brain however main challenges obtain meaningful patterns intrinsic complex structure tfmri also lack clear insight underlying neural activities. given hierarchical structure functional networks human brain previously datadriven methods independent component analysis sparse coding dictionary learning well model-driven approaches general linear model demonstrated disregard information contained rich tfmri data thus shallow machine learning models capable fully understanding deep hierarchical structures functional networks human brain consequently urgent call efficient scalable data analytics knowledge discovery methods crack underlying brain activities. recently data-driven computational intensive neural network approaches deep learning gained increasing interest among researchers efficiency extracting meaningful hierarchical features lowlevel data. particularly convolutional neural network among deep learning methods scientific community especially classifying learning image data context high dimensional data fmri however large size training examples sheer size model parameters drastically impact computational cost accuracy learning fmri signals. furthermore current neural network methods fmri analysis implemented local application without parallelization scheme indicated extensive battery literature many scaling deep learning applications using large-scale clusters gpus solve computational bottleneck efficient effective knowledge discovery fmri data. following previous successes using distributed processing scaling neural network model work design fast scalable distributed framework implement deep convolutional model dubbed distributed deep convolutional autoencoder leverage power distributed optimization distributed data partitioning multiple processing. distributed optimizer based asynchronized stochastic gradient descent method model used multiple replicas single model optimize parameters lead reducing training time significantly. data parallelization utilized apache spark hadoop distributed computationally intensive operations tuning parameter spark acts fast extract transfer load layer optimize data partitioning underlying hadoop ecosystem. accomplished constructing resilient distributed dataset provides functional interface partitioned data across cluster. major contributions work summarized follows. learning compared frameworks dist-keras elephas proposed framework critical advantages migration standalone code distributed version done lines change. despite previous framework framework works efficiently hdfs allowing spark push datasets. integrating model current pipeline easy spark charge parallelizing data. framework easy deploy scale cloud in-house clusters. created amazon machine image combination spark-ec script easily scale cluster. rest paper describes dist-dca model architecture detail. section briefly introduce primary components dist-dca implemented. also review related works domain. thoroughly describe deep convolutional model section section dedicated data parallelism distributed optimization. section describes scalable experiments large clusters explain model easily distributed among dozens nodes reduce computational time efficiently. recent advances building affordable high-performance gpus thousands cores critical factors advancing large-scale deep learning models. breakthrough also encouraged scientific community utilize gpus often cpu’s capacity seem grow proportion rate increasing demand. however limited memory capacity typical gpus market become bottleneck feeding extensive datasets training speed concerned. therefore common approaches data parallelism model parallelism researchers’ interest. implement distributed deep learning framework using tensorflow spark take advantage power distributed gpus cluster. propose distributed deep convolutional autoencoder model gain meaningful neuroscience insight massive amount tfmri data. validate proposed dist-dca model using novel high-level sparse dictionary learning method. model parallelism different portions model computation done different computational devices simultaneously batch examples sharing parameters among devices single model. approach however efficient large models splitting neural network model needs done case-bycase manner time-consuming. data parallelism hand seems straightforward general implementation easily scaled larger cluster sizes. fig. demonstrates data parallelism paradigm. discuss dist-dca data parallelism scheme depths section main motivation behind work implement scalable asynchronous data parallelism model leveraging tensorflow spark efficiently learn meaningful hierarchical abstraction massive size fmri data. tensorflow mathematical software opensource software library machine intelligence developed since google brain team initially aimed machine learning research deep neural networks. tensorflow numerical computation library using data flow graphs enables machine learning experts dataintensive robust implementations conventional deep learning algorithms. offers flexible architecture enables deploying computation cpus gpus standalone parallel distributed fashion. selected tensorflow work efficiently supports distributed parallel processing supports keras. however easy scale running tensorflow applications model data become large. queuing framework seamlessly feed data cluster nodes schedule manage tasks efficiently needed. pipelining pre-processing training inferences steps known challenge addressed tensorflow ecosystem. since spark framework developed university berkeley amplab currently maintained databricks. framework addresses deficiencies mapreduce introducing resilient distributed datasets abstract operations performed memory. spark compiles action lineages operations efficient tasks executed spark engine. spark offers functional programming manipulate resilient distributed datasets rdds represent collection items distributed across many computing nodes manipulated computational engine responsible scheduling distributing monitoring applications. consists many computational tasks across executor node computation node/cluster. spark’s scheduler execute duties across whole cluster. spark minimizes repetition data loading caching data memory crucial complex processes. spark uses hadoop filesystem core distributed file system apache spark active apache projects github. work used combination tensorflow spark leverage data parallelism scheduling spark thus enabling direct tensor communication among tensorflow executors parameter server. process-to-process direct communication enables tensorflow program scale effortlessly. section describe communication details. past years multiple studies adopting neural network methods model fmri data associated applications. instance chen used convolutional autoencoder fmri data aggregation; plis used deep belief network learn physiologically important representations fmri data; combined deep auto-encoder hidden markov model investigate functional connectivity resting-state fmri; huang used restricted boltzmann machine mine latent sources task fmri data; used convolutional neural networks classify fmri-derived functional brain networks used alexnet reconstruct visual semantic experiences using fmri data. context applying deep learning applications fmri data however works focused classification problem using single computation node. focus paper provision unsupervised distributed encoder effectively models tfmri data. enables learn hierarchical feature abstraction lowering spatial temporal noises contained fmri data ensures efficiently reduce model training inferences time easily scaling cluster gpus. fig. illustrates structure proposed dist-dca model validation pipeline based online dictionary learning algorithm. describe pipeline later section neuroinformatics platform used preprocess tfmri signals. keras tensorflow apis used construct model. section explain asynchronous gradient computation reduces model’s training time communicating updating parameters values. non-distributed version model elaborated facilitate understanding model recapitulate model following paragraphs. purpose autoencoder first encode input fmri time series mapping higher level feature maps decode signals reversing process. throughout process obtain hierarchical abstraction fmri signals denoising them. mentioned below assume model consists convolutional layer encoder decoder later extend real model. summary model parameters shown table function. paper except output convolutional layer decoder layer linear activation function rectified linear unit activation functions. advantages choosing relu study first reduce possibility vanishing gradient second represent signal sparsely later sparse representation hidden layer data validation. fully connected layer used encoder match encoder final hidden layer feature size input signal ensure hidden states learned full receptive field input final desired output model mentioned decoder following symmetric pattern attached previous encoder. reconstruct input signal first hidden states mapped reshaped reconstructed version feature maps fully connected layer decoder. equation denote weights bias fully connected layer decoder respectively. asynchronous method robust failure nodes node fails others still train data partitions fetch updates parameter server. example given batch size elements replicas model computes gradient elements combine gradients separate node known parameter server apply parameters updates synchronously order behave exactly running sequential algorithm batch size elements. implemented downpour distributed framework fixed fetch push weights gradients speeding convergence ease comparison simple sgd. experiment shows relaxing consistency requirements remarkably effective. downpour comes intuition view gradient descent water droplet toward minimizing error rate individual executors considered several droplets near other separately flowing valley. moreover practiced warm-up phase wherein single executor node starts training data partition starting executors. significantly decreased probability diverging executor trapped local optima. also chose adagrad optimizer keep learning rate update parameter model training ease extending number executing nodes. adagrad uses separate adaptive learning rate parameter. concept extended model layers transforming input layer different feature convolutional layer chain rule. minimize mean square error fmri signals reconstructions also used regularization term feature maps layer encoder bottom layer decoder. ensures fully connected layer randomly shuffle timing order reconstructing features maps decoder. equation controls significance regularization term experimentally pooling layer convolutional layer. helps first substantially reducing computational cost upper layer second gaining translation-invariance. translation-invariance particularly important tfmri possible time-shift phenomena acquiring signal given invertible property max-pooling utilized switches encoder memorize location local pooling regions applied location corresponding local value original position. validation studies switches available simply traditional up-sampling. next section explain model replicated among spark executor nodes. data parallel approaches copy entire model sent executor parallelizing processing gradient descent partitioning data smaller subsets. parameter server combines results subset synchronize model parameters executor receiving gradient delta executor. done synchronously asynchronously. however homogeneous environments nodes share hardware specifications communicate reliable network communication asynchronous methods outperform reasons first executors wait others commit start processing next batch data. second stage ends shuffle tasks created shuffle-map tasks. tasks particular stage completed driver creates tasks next stage sends executors repeats last stage results return driver. asynchronized implementation ensure model replicas data partitions independently thus reducing delays induced loaded executors. evaluated tensorflow spark performance scalability novel dist-dca model using amazon elastic cloud computing trained model fmri time series human subjects evaluated time series separate subjects. dataset. human connectome project release dataset containing healthy subjects’ tfmri data. dataset advantageous high temporal spatial resolution enables detailed characterization brain’s functional behaviour. motor task fmri data study composed basic motor tasks including visual cues tapping left fingers squeezing left toes moving tongue divided motor task subjects separate subsets training validating subjects. running dist-dca model preparation steps include fmri signal pre-processing implemented using feat. furthermore recruited integrated neuroinformatic platform helpni facilitate pre-processing integrate different steps data acquisition using powerful pipelining ability. cloud platform. dist-dca model deployed amazon service elastic cloud computing clusters highly scalable number executor nodes could adjusted effortlessly within cluster. preprocessed converted fmri data stored cloud amazon accessible clusters. enables pull data newly initialized instances easily. used customized scripts along containing preconfigured instance scale cluster according desire. cluster’s node contains apache spark version hadoop version tensorflow keras python benchmark scalability robustness proposed framework used variety node hardware configurations different number node experiment summarized table configuration nodes follows. nodes equipped high-frequency intel xeon processors nvidia tesla learning rates computed summed squared gradients parameter adagrad easily implemented locally within parameter server. constant scaling factor learning rates larger best fixed learning rate used without adagrad. adagrad extends maximum number model replicas productively work simultaneously. abovementioned optimization procedures ideally address problem ways. empowering process massive fmri data allowing train relatively large model consisting million trainable parameters faster. result proposed dist-dca benefits asynchronous data parallelism main components distributed data partitioning distributed parameter optimization shown fig. hadoop main distributed file system spark tasks scheduling data partitioning. spark executor acts wrapper tensorflow application node handles parameter synchronization rest tensorflow application independently single node setup. executor commits gradient delta parameter server processing batch elements receives latest parameter server. meanwhile spark core efficiently feeds executors hdfs partitioning data based number epochs dataset size. fig. shows spark data partitioning among cluster nodes consisting driver parameter servers executors. spark driver responsible replicating tensorflow model across cluster. stage partition tasks created sent executors. trained models time series data batches steps epoch. fig. demonstrates speed various implementations including standalone distributed ones nodes. since standalone data-parallelism network overhead obviously outperforms two-node cluster. however clusters higher number executor nodes easily exceed regarding computation time. example cluster executors outperforms standalone model almost seven times. observed training speed linearly grows number executor nodes increase. however expect performance drops increase number executor nodes happens network overhead starts rule dist-dca model performance executor nodes fewer tasks waiting fetch parameters. demonstrates scalability implemented distributed framework measured training time distdca previously discussed dataset. trained model different cluster settings executor nodes total gpus respectively. please note sake comparison experiments node used. goal obtain minimum loss minimum amount training time. fig. illustrates training time reduced significantly almost hours four-executor cores compared twoexecutor cluster cores however increased rate hold four-node eightnode cluster cores believe network communication overhead previously discussed warm-up phase. explained section tensorflow application wrapped inside spark executor node. executors independently start train model pushing gradients fetching parameters parameter server stage. recurring network communications cause larger clusters linearly scale-up opposed ones fewer nodes. conclude network always bottleneck larger clusters. parallel processing cores gigabytes video memory gbps aggregate network bandwidth within cluster. nodes come intel sandy bridge processors nvidia grid cuda cores gigabytes memory gpu. aimed investigate performance framework respect mean processing time single mini-batch downpour adagard training function number nodes used single model instance. deployed four clusters instances nodes given broadband network communications except -node cluster parameter servers dedicated parameter server along spark driver. moreover evaluate effect network traffic training speed non-distributed version model single node configuration. experiments validation study shown fig. rationale behind compare detected task-related patterns brain activity sparse dictionary learning method setups. feeding high-level features hidden layer contained dist-dca tfmri signals sparse dictionary learning unsupervised learning algorithm aims finding sparse representation input data form linear combination basic elements known dictionaries along corresponding coefficients. goal achieved aggregating over-complete dictionary matrix corresponding coefficient matrix effective online dictionary learning algorithm time series completed dictionary represents temporal activity brain network corresponding reference weight vector stands spatial every network. method recognized efficient method inferring comprehensive collection concurrent functional networks human brain spatial temporal pattern sample network decomposed results demonstrated fig. also performed another experiment solely evaluate effect cores proposed framework performance. launched clusters nodes train ditdca model data. cluster setup used environmental setup card node clusters utilized cores node respectively. demonstrated results fig. suggests increasing number cpus would benefit training speed significantly. simple comparison results fig. fig. shows increasing number gpus distributed setup reduces training time significantly; conclusion cannot drawn opposed increasing cores. draw fair comparison used parameters runs. adopted parameter-tuning approach suggested setups learned dictionaries sparsity regularizer achieve best performance brain network inference. training high-level features setup decomposed high-level dictionaries corresponding spatial distributions. decoder project high-level dictionaries back signal space. detected patterns visualized fig. shown right side figure although dictionary learning analysis setups detected motor task patterns patterns mixed large number noises setup result correlation values task design pattern quite small. hand setup contained much fewer noises time series patterns spatial maps. consequently conclude proposed model filters noises layer preserves useful information brain activities. sake simplicity page limitation explain theoretical brain model analysis reconstruction error analysis. details comparison found huang work furthermore visualized filters layer. fig. shows filters first layer encoder. first layer filters summarized common sub-shapes tfmri time. example sinuous bowl patterns fmri shown arrows fig. providing effective model represent large scale tfmri data break intrinsic complex structure tfmri signals highly demanded challenging. novel deep learning model along distributed computing keys transforming understanding complicated brain signals work presented novel scalable distributed deep convolutional autoencoder hierarchically models large-scale tfmri time series data gaining higher level abstraction tfmri signal. used apache spark tensorflow computational engines parallelize millions fmri time series train model large cluster gpus. experiment results showed model effectively scale-up dozens computation nodes processing extensive dataset hundreds computational cores. significance network overhead however severely impact training time. furthermore results showed high-level features superior task-related regions detection. proposed autoencoder also able denoise tfmri signal learned dictionary atoms novel high-level sparse dictionary learning suggests. general work contributes novel deep convolution autoencoder framework fmri data modelling significant application potentials cognitive clinical neuroscience future. future work plan perform tests implement parallel version model computational power multi-gpu multi-node distributed setting maximize performance. also plan available subjects releases including acquisitions different types tasks identify brain areas wide range neural systems benefit proposed distributed model enabling datadriven hierarchical neuroscientific discovery massive fmri data future. acknowledgment work supported national institutes health national science foundation tianming corresponding author work; phone e-mail tianming.liugmail.com. references data open science brain lessons learned genomics. frontiers human neuroscience ugurbil wu-minn consortium. wu-minn human connectome project overview. neuroimage making data sharing work fcp/indi experience. neuroimage doi= http//dx.doi.org/http//dx.doi.org/./j.neuroimage... poldrack r.a. barch d.m. mitchell j.p. wager t.d. wagner a.d. devlin j.t. cumba koyejo milham m.p. toward open sharing task-based fmri data openfmri project. frontiers neuroinformatics doi= http//dx.doi.org/./fninf... mining fmri data deep neural network functional brain network classification using convolutional neural networks accepted isbi learning dynamic natural vision.\" arxiv preprint arxiv. andy feng tensorflowonspark github repository https//github.com/yahoo/tensorflowonspark neuroimaging informatics brain informatics essen d.c. smith s.m. barch d.m. behrens t.e.j. yacoub ugurbil wu-minn human connectome project overview. neuroimage doi= http//dx.doi.org/http//dx.doi.org/./j.neuroimage.... corbetta glasser m.f. curtiss dixit feldt function human connectome task-fmri individual differences behavior. neuroimage. neuroimage. zeiler m.d. fergus visualizing understanding convolutional networks. european conference computer vision. springer learning apache spark keras github repository https//github.com/joerihermans/dist-keras online learning stochastic optimization. journal machine learning research sparse coding. proceedings annual international conference machine learning. convolutional autoencoder.\" international conference information processing medical imaging. springer cham nigel cairns robert green danielle harvey \"the alzheimer's disease neuroimaging initiative review papers published since inception.\" alzheimer's dementia e-e. davidson jieping tianming shannon quinn. \"scalable fast rank- dictionary learning fmri data analysis.\" proceedings sigkdd international conference knowledge discovery data mining abadi martí \"tensorflow large-scale machine learning heterogeneous distributed systems.\" arxiv preprint arxiv. https//github.com/maxpumperla/elephas distributed deep learning apache software foundation license http//deeplearningj.org</bib> abolghasemi ferdowsi sanei fast incoherent dictionary learning algorithms application fmri. signal image video processing activated regions functional brain. magnetic resonance imaging component analysis dynamic response measured fmri generalized linear systems framework. magnetic resonance imaging bandettini jesmanowicz wong hyde processing strategies time‐course data sets functional human brain. magnetic resonance medicine fmri data examining assumptions. hum. brain mapp. holistic atlases functional networks interactions reveal reciprocal organizational architecture cortical function. ieee trans. biomed. eng. frackowiak r.s.j. statistical parametric maps functional imaging general linear approach. hum. brain mapp. n.j.a. veltman d.j. aleman zitman f.g. penninx b.w.j.h. buchem m.a. reiber j.h.c. rombouts s.a.r.b. milles hierarchical functional modularity resting-state human brain. hum. brain mapp. hierarchical modularity human brain functional networks. front. hum. neurosci. ideas immanent nervous activity.\" bulletin mathematical biophysics deep convolutional neural networks. advances neural information processing systems. mri.\" advances neural information processing systems cireşan \"mitosis detection breast cancer histology images deep neural networks.\" international conference medical image computing computer-assisted intervention. springer berlin heidelberg generate affinity graphs image segmentation.\" neural computation performance face verification.\" proceedings ieee conference computer vision pattern recognition. images labels captions.\" advances neural information processing systems. masci meier cireşan schmidhuber stacked convolutional auto-encoders hierarchical feature extraction. international conference artificial neural networks. springer networks.\" advances neural information processing systems. simple neural nets excel handwritten digit recognition. corr unsupervised feature learning. aistats optimization methods deep learning. icml elastic averaging sgd.\" advances neural information processing systems. abstraction in-memory cluster computing.\" proceedings usenix conference networked systems design implementation. usenix association subject fmri data aggregation.\" arxiv preprint arxiv. plis s.m. hjelm d.r. slakhutdinov allen e.a. bockholt h.j. long j.d. johnson paulsen turner calhoun v.d. deep learning neuroimaging validation study. front. neurosci. deep learning functional dynamics estimation resting-state fmri. neuroimage.", "year": "2017"}