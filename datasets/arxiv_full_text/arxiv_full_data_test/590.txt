{"title": "Classification and Geometry of General Perceptual Manifolds", "tag": "q-bio", "abstract": " Perceptual manifolds arise when a neural population responds to an ensemble of sensory signals associated with different physical features (e.g., orientation, pose, scale, location, and intensity) of the same perceptual object. Object recognition and discrimination requires classifying the manifolds in a manner that is insensitive to variability within a manifold. How neuronal systems give rise to invariant object classification and recognition is a fundamental problem in brain theory as well as in machine learning. Here we study the ability of a readout network to classify objects from their perceptual manifold representations. We develop a statistical mechanical theory for the linear classification of manifolds with arbitrary geometry revealing a remarkable relation to the mathematics of conic decomposition. Novel geometrical measures of manifold radius and manifold dimension are introduced which can explain the classification capacity for manifolds of various geometries. The general theory is demonstrated on a number of representative manifolds, including L2 ellipsoids prototypical of strictly convex manifolds, L1 balls representing polytopes consisting of finite sample points, and orientation manifolds which arise from neurons tuned to respond to a continuous angle variable, such as object orientation. The effects of label sparsity on the classification capacity of manifolds are elucidated, revealing a scaling relation between label sparsity and manifold radius. Theoretical predictions are corroborated by numerical simulations using recently developed algorithms to compute maximum margin solutions for manifold dichotomies. Our theory and its extensions provide a powerful and rich framework for applying statistical mechanics of linear classification to data arising from neuronal responses to object stimuli, as well as to artificial deep networks trained for object recognition tasks. ", "text": "perceptual manifolds arise neural population responds ensemble sensory signals associated diﬀerent physical features perceptual object. object recognition discrimination require classifying manifolds manner insensitive variability within manifold. neuronal systems give rise invariant object classiﬁcation recognition fundamental problem brain theory well machine learning. study ability readout network classify objects perceptual manifold representations. develop statistical mechanical theory linear classiﬁcation manifolds arbitrary geometry revealing remarkable relation mathematics conic decomposition. show special anchor points manifolds used deﬁne novel geometrical measures radius dimension explain classiﬁcation capacity manifolds various geometries. general theory demonstrated number representative manifolds including ellipsoids prototypical strictly convex manifolds balls representing polytopes ﬁnite samples ring manifolds exhibiting non-convex continuous structures arise modulating continuous degree freedom. eﬀects label sparsity classiﬁcation capacity general manifolds elucidated displaying universal scaling relation label sparsity manifold radius. theoretical predictions corroborated numerical simulations using recently developed algorithms compute maximum margin solutions manifold dichotomies. theory extensions provide powerful rich framework applying statistical mechanics linear classiﬁcation data arising perceptual neuronal responses well artiﬁcial deep networks trained object recognition tasks. fundamental cognitive task performed animals humans invariant perception objects requiring nervous system discriminate different objects despite substantial variability objects physical features. example vision mammalian brain able recognize objects despite variations orientation position pose lighting background. impressive robustness physical changes limited vision; examples include speech processing requires detection phonemes despite variability acoustic signals associated individual phonemes; discrimination odors presence variability odor concentrations. sensory systems organized hierarchies consisting multiple layers transforming sensory signals sequence distinct neural representations. studies high level sensory systems e.g. inferotemporal cortex vision auditory cortex audition piriform cortex olfaction reveal even late sensory stages exhibit signiﬁcant sensitivity neuronal responses physical variables. suggests sensory hierarchies generate representations objects although entirely invariant changes physical features still readily decoded invariant manner downstream system. hypothesis formalized notion untangling perceptual manifolds viewpoint underlies number studies object recognition deep neural networks artiﬁcial intelligence conceptualize perceptual manifolds consider neurons responding speciﬁc sensory signal associated object shown fig. neural population response stimulus vector changes physical parameters input stimulus change object identity modulate neural state vector. state vectors corresponding responses possible stimuli associated object viewed manifold neural state space. geometrical perspective object recognition equivalent task discriminating manifolds diﬀerent objects other. presumably signals propagate processing stage next sensory hierarchy geometry manifolds reformatted become untangled namely easily separated biologically plausible decoder paper model decoder simple single layer network geometrical properties perceptual manifolds inﬂuence ability separated linear classiﬁer. begin introducing mathematical model general manifolds binary classiﬁcation formalism allows generate generic bounds manifold separability capacity limits small manifold sizes large sizes bounds highlight fact large ambient dimension maximal number separable ﬁnite-dimensional manifolds proportional even though consists inﬁnite number points setting stage statistical mechanical evaluation maximal using replica theory derive mean ﬁeld equations capacity linear separation ﬁnite iii) stadimensional manifolds conditions involving manifold anchor point. anchor point representative support vector manifold. position anchor point manifold changes orientations manifolds varied ensuing statistics distribution anchor points play role theory. optimal separating plane intersects fraction manifolds theory categorizes dimension span intersecting sets relation position anchor points manifolds’ convex hulls. mean ﬁeld theory motivates deﬁnition manifold geometry based measure induced statistics anchor points. particular deﬁne manifold anchor radius dimension respectively. quantities relevant since capacity general manifolds well approximated capacity balls radii dimensions interestingly show limit small manifolds anchor point statistics dominated points boundary manifolds minimal overlap gaussian random vectors. resultant gaussian radius dimension related well-known gaussian mean-width convex bodies beyond understanding fundamental limits classiﬁcation capacity geometric measures oﬀer quantitative tools assessing perceptual manifolds reformatted brain artiﬁcial systems. apply general theory three examples representing distinct prototypical manifold classes. figure perceptual manifolds neural state space. firing rates neurons responding images shown various orientations scales response particular orientation scale characterized n-dimensional population response. population responses images form continuous manifold representing complete invariances neural activity space. object images corresponding various poses represented manifolds vector space. linear separability previously studied context classiﬁcation points perceptron using combinatorics statistical mechanics gardner’s statistical mechanics theory extremely important provides accurate estimates perceptron capacity beyond function counting incorporating robustness measures. robustness linear classiﬁer quantiﬁed margin measures distance separating hyperplane closest point. maximizing margin classiﬁer critical objective machine learning providing support vector machines good generalization performance guarantees theories focus separating ﬁnite points underlying geometrical structure applicable problem manifold classiﬁcation deals separating inﬁnite number points geometrically organized manifolds. paper addresses important question quantify capacity perceptron dichotomies input patterns described manifolds. earlier paper presented analysis classiﬁcation manifolds extremely simple geometry namely balls however previous results limited applicability neural manifolds arising realistic physical variations objects exhibit much complicated geometries. statistical mechanics deal classiﬁcation manifolds complex geometry speciﬁc geometric properties determine separability manifolds? paper develop theory linear separaone class consists manifolds strictly smooth convex hulls contain facets exempliﬁed ellipsoids. another class convex polytopes arise manifolds consists ﬁnite number data points exempliﬁed ellipsoids. finally ring manifolds represent intermediate class smooth nonconvex manifolds. ring manifolds continuous nonlinear functions single intrinsic variable object orientation angle. diﬀerences manifold types show clearly distinct patterns support dimensions. however show share common trends. size manifold increases capacity geometrical measures vary smoothly exhibiting smooth cross-over small radius dimension high capacity large radius dimension capacity. crossover importantly many realistic curs cases size smaller crossover value manifold dimensionality substantially smaller computed naive second order statistics highlighting saliency signiﬁcance measures anchor geometry. figure model manifolds aﬃne subspaces. manifold embedded orthogonal translation vector aﬃne space center manifold. scale varied manifold shrinks point expands entire aﬃne space. manifolds aﬃne subspaces model perceptual manifolds corresponding perceptual object. manifold consists compact subset aﬃne subspace aﬃne dimension point manifold parameterized orthonormal bases dimensional linear subspace containing components represents coordinates manifold point within subspace constrained bold notation indicates vectors whereas arrow notation indicates vector rd+. deﬁnes shape manifolds encapsulates aﬃne constraint. simplicity ﬁrst assume manifolds geometry coordinate manifolds; extensions consider heterogeneous geometries provided sec. study separability manifolds classes denoted binary labels linear hyperplane passing origin. hyperplane described weight vector normalized hyperplane correctly separates manifolds margin satisﬁes finally treat important case classiﬁcation manifolds imbalanced labels commonly arise problems object recognition. well known highly sparse labels classiﬁcation capacity random points increases dramatically fraction minority labels. analysis sparsely labeled manifolds highlights interplay manifold size sparsity. particular shows sparsity enhances capacg manifold radius. notably large regime parameters sparsely labeled manifolds approximately described universal capacity function equivalent sparsely labeled balls radii dimensions demonstrated numerical evaluations conversely capacity close dimensionality manifold theory provides ﬁrst time quantitative qualitative predictions perceptron classiﬁcation realistic data structures. however application real data require extensions theory discussed section vii. together theory makes important contribution development statistical mechanical theories neural information processing realistic conditions. bounds number linearly realizable dichotomies considering limit following general conditions. first limit linear separability manifolds becomes equivalent separability centers. leads requirement centers general position second manifolds consider conditions manifolds linearly separable manifolds span complete aﬃne subspaces. weight vector consistently assign label points aﬃne subspace must orthogonal displacement vectors aﬃne subspace. hence realize dichotomy manifolds weight vector must null space dimension n−dtot dtot rank union aﬃne displacement vecgeneral position tors. basis vectors dtot aﬃne subspaces separable required projections orthogonal translation vectors need also separable dtot dimensional null space. under general conditions number dichotomies d-dimensional aﬃne subspaces linearly separated related number dichotomies ﬁnite points relationship conclude ability linearly separate d-dimensional aﬃne subspaces exhibits transition always separable never large separable critical ratio general d-dimensional manifolds ﬁnite size number dichotomies linearly separable lower bounded upper bounded introduce notation denote maximal load randomly labeled manifolds linearly separable margin high probability. therefore considerations follows critical load zero margin bounded bounds highlight fact large limit maximal number separable ﬁnite-dimensional manifolds proportional even though consists inﬁnite number points. sets stage statistical mechanical evaluation maximal number manifolds described following section. position aﬃne subspace relative origin deﬁned translation vector closest origin. orthogonal translation vector perpendicular aﬃne displacement vectors points aﬃne subspace equal projections i.e. assume simplicity normscµ normalized investigate separability properties manifolds helpful consider scaling manifold overall scale factor without changing shape. deﬁne scaling relative center scalar manifold converges point hand manifold spans entire aﬃne subspace. manifold symmetric natural choice center. later provide appropriate deﬁnition center point general asymmetric manifolds. general translation vector center need coincide shown fig. however also discuss later special case centered manifolds translation vector center coincide. bounds linear separability manifolds dichotomies input points zero margin number dichotomies separated linear hyperplane origin given binomial coeﬃcient zero otherwise. result holds input vectors obey mild condition vectors general position namely subsets input vectors size linearly independent. large probability dichotomy linearly separable depends upon exhibits sharp transition critical ratio value aware comprehensive extension cover’s counting theorem general manifolds; nevertheless provide lower upper order make theoretical progress beyond bounds above need make additional statistical assumptions manifold spaces labels. speciﬁcally assume individual components drawn independently identical gaussian distrin butions zero mean variance binary labels randomly assigned manifold equal probabilities. study thermodynamic limit ﬁnite load addition manifold geometries speciﬁed particular aﬃne dimension held ﬁxed thermodynamic limit. assumptions bounds extended linear separability general manifolds ﬁnite margin characterized reciprocal critical load ratio many intergaussian measure esting cases aﬃne dimension large overly loose. hence important derive estimate capacity manifolds ﬁnite sizes evaluate dependence capacity nature solution geometrical properties manifolds shown below. minv average random dimensional vectors whose components i.i.d. normally distributed components vector represent signed ﬁelds induced solution vector basis vectors manifold. gaussian vector represents part variability quenched variability manifolds basis vectors labels explained detail below. inequality constraints written equivalently constraint point manifold minimal projection therefore consider concave support function used write constraint note deﬁnition easily mapped conventional convex support function deﬁned operation. karush-kuhn-tucker conditions gain insight nature maximum margin solution useful consider conditions convex optimization conditions characterize unique solution given πµxµ∈m heaviside function enforce margin constraints along delta function ensure following focus properties maximum margin solution namely solution largest load ﬁxed margin equivalently solution margin maximized given vector subgradient support function i.e. point convex hull minimal overlap support function diﬀerentiable subgradient unique equivalent gradient support function brevity; equation follows pression capacity eqs. relations above self-consistent equations statistics λµand ˜sµ. mean ﬁeld theory derives appropriate statistics self-consistent equations ﬁelds single manifold. this consider projecting solution vector onto aﬃne subspace manifolds deﬁne dimensional vector signed ﬁelds solution aﬃne basis vectors manifold. then reduces represents contribution manifolds since subspaces randomly oriented contribution well described random gaussian vector. finally self consistency requires ﬁxed minimal overlap represents point residing margin hyperplane otherwise contribute margin solution. thus decomposition ﬁeld induced speciﬁc manifold contribution induced speciﬁc manifold along contributions coming manifolds. self consistent equations well relating gaussian statistics naturally follow requirement represents support vector. vectors contributing solution play role theory. denote equivalently aﬃne subspace components manifold anchor points. particular conﬁguration manifolds manifolds could replaced equivalent anchor points yield maximum margin solution. important stress however individual anchor point determined conﬁguration associated manifold also random orientations manifolds. ﬁxed manifold location anchor point vary relative conﬁgurations manifolds. variation captured mean ﬁeld theory dependence anchor point random gaussian vector particular position anchor point convex hull manifold reﬂects nature relation manifold margin planes. general fraction manifolds intersect margin hyperplanes i.e. non-zero manifolds support manifolds system. nature support varies characterized dimension span intersecting conv since support function positively homogeneous thus depends unit vector values diﬀerentiable subgradient unique deﬁned uniquely particular subgradient obeys conditions latter case conv reside itself. capacity written terms scale factor either zero positive corresponding whether positive zero. positive meaning then case multiplying yields thus obeys self consistent equation relations nice interpretation within framework mean ﬁeld theory. maximum margin solution vector always written linear combination support vectors. although inﬁnite numbers input points manifold solution vector decomposed vectors manifold conv vector convex hull µ-th manifold. large limit vectors uncorrelated other. hence squaring equation ignoring correlations diﬀer coordinates µ-th aﬃne subspace µ-th manifold simply conventional polar cone equation interpreted decomposition component vectors component i.e. euclidean projection onto s◦κ; component located cone moreau decomposition theorem states components perpendicular non-zero components need perpendicular obey position vector relation cones cone gives rise qualitatively diﬀerent expressions contributions solution weight vector inverse capacity. correspond diﬀerent support dimensions mentioned above. particular lies inside support dimension hand lies inside cone manifold fully supporting solution mean ﬁeld equations consists stages. first computed particular relevant contributions inverse capacity averaged gaussian distribution simple geometries ellipsoids ﬁrst step solved analytically. however complicated geometries steps need performed numerically. ﬁrst step involves determining given solving quadratic semi-inﬁnite programming problem manifold contain inﬁnitely many points. novel cutting plane method developed eﬃciently solve qsip problem expectations computed sampling gaussian dimensions taking appropriate averages similar procedures mean ﬁeld methods. relevant quantities corresponding capacity quite concentrated converge quickly relatively samples. following sections also show mean ﬁeld theory compares computer simulations numerically solve maximum margin solution realizations manifolds given variety manifold geometries. finding maximum margin planes. support manifolds call touching manifolds intersect margin hyperplane anchor point. support dimension anchor point boundary extreme fully supporting manifolds completely reside margin hyperplane. characterized case parallel translation vector hence points support vectors overlap anchor point case unique point interior conv obeys self consistent equation namely balances contribution manifolds zero components orthogonal case smooth convex hulls manifold support conﬁgurations exist. types manifolds also partially supporting manifolds whose convex hull intersection margin hyperplanes consist dimensional faces associated anchor points reside inside intersecting face. instance implies lies edge whereas implies lies planar -face convex hull. determining dimension support structure arises various explained below. conditions also interpreted terms conic decomposition generalizes notion decomposition vectors onto linear subspaces null spaces euclidean projection. convex cone manifold deﬁned cone margin solution challenging standard methods solving problems limited ﬁnite number input points. recently developed eﬃcient algorithm ﬁnding maximum margin solution manifold classiﬁcation used method present work section address capacity separate manifolds related geometry particular shape within d-dimensional aﬃne subspace. since projections points manifold onto translation vector same convenient parameterize aﬃne basis vectors coordinates dimensional vector representation parameterization convenient since constrains manifold variability ﬁrst components coordinate longitudinal variable measuring distance manifold aﬃne subspace origin. write dimensional vectors lower case vectors denote vectors also refer d-dimensional intrinsic vector anchor point. notation capacity written previous section. this random vector consider qualitative change anchor point decreases interior manifolds suﬃciently positive manifold interior margin plane i.e. corresponding support dimension although contributing inverse capacity solution vector useful associate anchor points manifolds deﬁned closest point manifold margin plane ˜st) mins∈s ∇gt) since deﬁnition ensures continuity anchor point interior regime holds equivalently ttoucht) described change manifold size corresponds scaling every scalar small size manifold shrinks point whose capacity reduces isolated points. however case capacity aﬀected manifold structure even section nevertheless underlying support structure simple. small manifolds support conﬁgurations. manifold interior with manifold becomes touching support dimension case small magnitude thus cases close gaussian vector probability conﬁgurations vanishes. large size limit separatlarge size manifolds equivalent separating aﬃne subspaces. show appendix main support structures. probabil−κ manifolds fully supporting namely underlying aﬃne subspaces parallel margin plane. regime contributes inverse regimes pacity amount d+α− angle aﬃne subspace margin plane almost zero contribute amount inverse capacity. combining contributions obtain large sizes consistent large margin ﬁxed implies larger increases probability supporting regimes. increasing also shrinks magnitude according hence capacity becomes similar random points corresponding capacity given independent manifold geometry. manifold centers theory manifold classiﬁcation described sec. require notion manifold center. however understanding scaling manifold sizes parameter aﬀects capacity center points manifolds scaled need deﬁned. many geometries center point symmetry ellipsoid. general manifolds natural deﬁnition would center mass anchor points averaging gaussian measure adopt simpler deﬁnition center provided steiner point convex bodies figure determining anchor points gaussian distributed vector onto convex hull manifold denoted show vector change decreases strictly convex manifold. suﬃciently positive vector obeys constraint hence conﬁguration corresponds interior manifold intermediate values t−t) violates constraints point boundary manifold maximizes projection manifold vector v−v) closest t−t) obeys finally larger values point interior manifold direction square manifold. here interior touching regimes vertex square. fully supporting regime anchor point interior collinear also partially supporting regime slightly tfs. regime perpendicular edges resides edge corresponding manifolds whose intersection margin planes edges manifolds terms centered manifolds manifolds shifted within aﬃne subspace center orthogonal translation vector coincide i.e. deﬁned relative distance centers origin d-dimensional intrinsic vectors give oﬀset relative manifold center. capacity equation motivates deﬁning geometrical measures manifolds call manifold anchor geometry. manifold anchor geometry based statistics anchor points induced gaussian random vector relevant capacity statistics suﬃcient determining classiﬁcation properties supporting structures associated maximum margin solution. accordingly deﬁne manifold anchor radius dimension manifold anchor radius denoted deﬁned mean squared length unit vector direction anchor dimension measures angular spread corresponding anchor point dimensions. note manifold dimension obeys whenever ambiguity call manifold radius dimension respectively. geometric descriptors oﬀer rich description manifold properties relevant classiﬁcation. since depend general quantities averaged also reason manifold anchor geometry also depends upon imposed margin. point manifold ﬁrst touches hyperplane normal translated inﬁnity towards manifold. aside measure zero touching point unique point boundary conv. procedure similar used deﬁne well known gaussian mean-width nota. note small sizes touching point depend statistics determined shape conv relative center. motivates deﬁning simpler manifold gaussian geometry denoted subscript highlights dependence dimensional gaussian measure gaussian radius denoted measures mean square amplitude gaussian anchor point ˜sgt) figure gaussian anchor points mapping points sgt) showing relation point manifold touches hyperplane orthogonal manifolds shown circle; ellipsoid; polytope manifold. values measure zero ˜sgt) along edge otherwise coincides vertex polytope. cases ˜sgt) interior convex hulls otherwise restricted boundary. important note even limit geometrical deﬁnitions equivalent conventional geometrical measures longest chord second order statistics induced uniform measure boundary conv special case ddimensional balls radius boundary ball direction however general manifolds much smaller manifold aﬃne dimension illustrated examples later. recapitulate essential diﬀerence gaussian geometry full manifold anchor geometry. gaussian case radius intrinsic property shape manifold aﬃne subspace invariant changing distance origin. thus scaling manifold global scale factor deﬁned results scaling factor likewise dimensionality invariant global scaling manifold size. contrast anchor geometry obey invariance larger manifolds. reason anchor point depends longitudinal degrees freedom namely size manifold relative distance center. hence need scale linearly also depend thus anchor geometry viewed describing general relationship signal noise classiﬁcation capacity. also note manifold anchor geometry automatically accounts rich support structure described section particular decreases statistics anchor points change concentrated boundary conv interior. additionally manifolds strictly convex intermediate values anchor statistics become concentrated k-dimensional facets convex hull corresponding partially supported manifolds. illustrate diﬀerence geometries practice suﬃcient. analysis elucidates interplay size dimension namely small needs high dimensional manifolds substantial classiﬁcation capacity. high-dimensional regime mean ﬁeld equations simplify self averaging terms involving sums components ˜si. quantity appears capacity approximated introduce eﬀective manifold margin rm√dm combined obtain capacity random points gain insight result note eﬀective margin center mean distance point closest margin plane roughly mean indicates margin needs scaled input appendix show namely classiﬁcation capacity general high dimensional manifold well approximated balls dimension radius scaling regime implies obtain ﬁnite capacity high-dimensional regime eﬀective margin rm√dm needs order unity requires radius small scaling large scaling regime calculation capacity geometric properties particularly simple. argued above radius small components small hence gaussian statistics geometry suﬃce. thus replace eqs. respectively. note scaling regime factor proportional next order correction overall capacity since small. notably margin regime gaussian mean width convex bodies support structure since manifold size small signiﬁcant contributions arise interior touching supports. beyond scaling regime small anchor geometry cannot adequately described gaussian statistics case manifold margin rm√dm large reduces figure distribution ellipsoids. distribution ball gaussian geometry peaked non-zero probability manifold anchor geometry ellipsoids distribution radii gaussian geometry anchor geometry corresponding distribution ˜s). ball radius vectors parallel angle always zero. manifold anchor geometry inside ball fully supporting region. thus distribution consists mixture delta function corresponding interior touching regions smoothly varying distribution corresponding fully supporting region. fig. also shows corresponding distributions dimensional ellipsoid major minor radius gaussian geometry distribution ﬁnite support whereas manifold anchor geometry support also since need parallel distribution angle varies zero manifold anchor geometry concentrated near zero contributions fully supporting regime. section show gaussian geometry becomes relevant even larger manifolds labels highly imbalanced i.e. sparse classiﬁcation. general linear classiﬁcation expressed depends high order statistics anchor vectors surprisingly analysis shows high mensional manifolds classiﬁcation capacity described terms statistics alone. particularly relevant expect many applications aﬃne dimension manifolds large. speciﬁcally deﬁne high-dimensional manifolds manifolds manifold dimension large i.e. finally fully supporting regime occurs tfst) full expression capacity ellipsoids given appendix section below focus interesting cases ellipsoids high-dimensional ellipsoids instructive apply general analysis high dimensional manifolds ellipsoids distinguish different size regimes assuming radii ellipsoid scaled global factor high dimensional regime self-averaging boundaries touching fully supporting transitions approximated used large margins assumed strictly convex high dimensional manifolds touching regime contributes signiﬁcantly geometry hence capacity. manifolds strictly convex partially supporting solutions also contribute capacity. finally large fully supporting regimes contribute geometry case manifold anchor dimension approaches aﬃne dimen sion eqs. reduce expected. family ellipsoids examples manifolds strictly convex. strictly convex manifold smooth boundaries contain corners edges ﬂats thus description anchor geometry support structures relatively simple. reason anchor vectors correspond either interior touching fully supporting partial support possible. ellipsoid geometry solved analytically; nevertheless less symmetry sphere exhibits salient properties manifold geometry nontrivial dimensionality non-uniform measure anchor points. assume ellipsoids centered relative symmetry centers described sec. parameterized points siuµ components ellipsoid centers i.i.d. gaussian distributed zero mean variorthonormal large ance limit. radii represent principal radii ellipsoids relative center. anchor points computed explicitly corresponding three regimes. interior occurs ttoucht) where note rm/r invariant scaling ellipsoid global factor reﬂecting role ﬁxed centers. anchor covariance matrix also compute covariance matrix anchor points. matrix diagonal principal directions ellipsoid eigenvalues interesting compare well known measure eﬀective dimension covariance matrix participation ratio given spectrum eigenvalues covariance matrix notation deﬁne generalized participation conventional ratio participation ratio uses whereas uses scaling regime scaling regime radii small radius dimension equivalent gaussian geometry seen invariant scaling radii expected. interesting compare gaussian geometric parameters statistics induced uniform measure surface ellipsoid. case covariance matrix eigenvalues contrast gaussian geometry eigenvalues covariance matrix corresponding expression squared radius result non-uniform induced measure surface ellipse anchor geometry even gaussian limit. beyond scaling regime high dimensional ellipsoids become touching manifolds since ttouch capacity small eqs. given finally have figure bimodal ellipsoids. ellipsoidal radii classiﬁcation capacity function scaling factor full mean ﬁeld theory capacity approximation capacity given equivalent ball simulation capacity averaged repetitions measured dichotomies repetition. manifold radius relative fraction manifolds support dimension diﬀerent values small manifolds interior touching manifolds touching regime fraction fully supporting manifolds predicted properties shown function overall scale fig. shows numerical simulations capacity full mean ﬁeld solution well spherical high dimensional approximation calculations good agreement showing accuracy mean ﬁeld theory spherical approximation. seen system scaling regime regime manifold dimension constant equals predicted parquestion whether mean ﬁeld theory still valid cases. investigate scenario computing capacity ellipsoids containing realistic distribution radii. taken examples class images imagenet dataset analyzed spectrum representations images last layer deep convolutional network googlenet computed radii shown fig. yield value order explore properties manifolds scaled radii overall factor analysis. decay distribution radii gaussian dimension ellipsoid much smaller implying small manifolds eﬀectively dimensional geometry dominated small number radii. increases becomes larger solution leaves scaling regime resulting rapid increase rapid falloﬀ capacity shown fig. finally approaching lower bound capacity expected. agreement numerical simulations mean ﬁeld estimates capacity illustrates relevance theory realistic data manifolds full rank. figure ellipsoids radii computed realistic image data. spectrum taken readout layer googlenet class imagenet images radii scaled factor classiﬁcation capacity function full mean ﬁeld theory capacity approximation capacity ball theory ellipsoids simulation capacity averaged repetitions measured random dichotomies repetition. manifold dimension function manifold radius relative scaling factor rm/r function family ellipsoids represent manifolds smooth strictly convex. hand types manifolds whose convex hulls strictly convex. section consider ddimensional ellipsoids. prototypical convex polytopes formed convex hulls ﬁnite numbers points d-dimensional ellipsoid parameterized radii {ri} speciﬁed convex manifold centered consists convex polytope ﬁnite number verspecify tices principal axes ellipsoids. simplicity consider case balls radii equal concentrate cases balls high-dimensional; case balls brieﬂy described analytical expression capacity complex presence contributions types supports address important aspects high dimensional solution below. ticipation ratio manifold radius linear expected ratio close unity indicating scaling regime system dominated largest radii. eﬀective margin larger unity system becomes increasingly aﬀected full aﬃne dimensionality ellipsoid seen marked increase dimension well corresponding decrease fig. shows distributions support scaling regime dimension interior touching regimes probability close fully supporting regime negligible. increases beyond scaling regime interior probability decreases solution almost exclusively touching regime. high values fully supporting solution gains substantial probability. note capacity decreases approximately value substantial fraction solutions fully supporting. case touching ellipsoids small angle margin plane. assumed manifold aﬃne subspace dimension ﬁnite limit large ambient dimension realistic data likely data manifolds technically full rank i.e. raising words ˜st) vertex polytope corresponding component largest magnitude fig. components i.i.d. gaussian random variables large maximum component tmax concentrated around hence much smaller result consistent fact gaussian mean width d-dimensional ball scales √log since points norm eﬀective margin given order unity scaling regime. regime capacity given simple relation high-dimensional balls radius small scaling regime contributing solution touching solution increases solutions values occur support face convex polytope dimension increases probability distribution solution shifts larger values. finally large regimes dominate fully supporting probability partially supporting probability illustrate behavior balls radius aﬃne dimension fig. shows linear classiﬁcation capacity function manifold approaches capacity isolated points numerical simulations demonstrate despite diﬀerent geometry capacity polytope similar ball radius dimension scaling regime much smaller despite fact polytope equal. small various faces eventually interior polytope contribute anchor geometry. scaling regime terms support structures scaling regime manifolds either interior touching. intermediate sizes support dimension peaked intermediate value ﬁnally large manifolds polytope manifolds nearly fully supporting. figure separability balls. linear classiﬁcation capacity balls function radius solution spherical approximation full numerical simulations. illustration ball. manifold radius relative actual radius manifold dimension function small limit approximately large close showing solution orthogonal manifolds sizes large. distribution support dimensions manifolds either interior touching support dimension peaked distribution manifolds close fully supporting. many neuroscience experiments measure responses neuronal populations continuously varying stimulus small number degrees freedom. prototypical example response neurons visual cortical areas orientation object. population neurons respond object identity well continuous physical variation result smooth manifolds parameterized single variable denoted describing continuous curves since general neural responses linear curve spans linear dimension. smooth curve convex endowed complex non-smooth convex hull. thus interesting consider consequences theory separability smooth non-convex curves. simplest example considered case corresponds periodic angular variable orientation image call resulting non-convex curve ring manifold. model neuronal responses smooth periodic functions parameterized decomposing neuronal responses fourier modes. here object siuµ represents mean population response object. diﬀerent components correspond diﬀerent fourier components anfigure linear classiﬁcation d-dimensional ring manir. classiﬁcation capacity folds uniform function test samples mean ﬁeld theory spherical approximation numerical simulations. illustration ring manifold. manifold dimension shows large limit showing orthogonality solution. manifold radius relative scaling factor function fact becomes small implies manifolds fully supporting hyperplane showing small radius structure. manifold dimension grows aﬃne dimension small scaling regime. distribution support dimensions. manifolds either interior touching support dimension peaked distribution truncated fully supporting support dimensions dimensions however manifolds larger share common trends. size manifold increases capacity geometry vary smoothly exhibiting smooth cross-over high capacity radius dimension capacity large radius dimenexamples demonstrate many cases size smaller crossover value manifold dimensionality substantially smaller expected naive second order statistics highlighting saliency parameters preferred orientation angles corresponding neurons assumed statistical assumptions analysis assume diﬀerent manifolds randomly positioned oriented respect others. ring manifold model implies mean responses independent random gaussian vectors also preferred orientation angles uncorrelated. deﬁnition vectors obey norn. thus object ring manifold closed non-intersecting smooth curve residing surface d-dimensional sphere radius simplest case ring manifold equivalent circle dimensions. however larger manifold convex convex hull composed faces varying dimensions. fig. investigate geometrical properties manifolds relevant classiﬁcation function overall scale factor simplicity chosen striking feature small dimension scaling regime scaling roughly logarithmic dependence similar ball polytopes. then increases increases dramatically similarity ring manifold convex polytope also seen support dimension manifolds. support faces dimension seen implying presence partially supporting solutions. interestingly excluded indicating maximal face dimension convex hull points face convex hull point resides subspace spanned pair fourier harmonics. ring manifolds closely related trigonometric moment curve whose convex hull geometrical properties extensively studied conclusion smoothness convex hulls becomes apparent distinct patterns support assumed number manifolds positive labels approximately equal number manifolds negative labels. section consider case classes unbalanced number positively-labeled manifolds less negatively-labeled manifolds special case problem classiﬁcation manifolds heterogenous statistics manifolds diﬀerent geometries label statistics. begin addressing capacity mixtures manifolds focus sparsely labeled manifolds. theory manifold classiﬁcation readily extended heterogeneous ensemble manifolds consisting distinct classes. replica theory shape manifolds appear free energy term mixture statistics combined free energy given simply averaging individual free energy terms class recall free energy term determines capacity shape giving individual inverse critical load inverse capacity heterogeneous mixture then average fractional proportions diﬀerent manifold classes. remarkably simple generic theoretical result enables analyzing diverse manifold classiﬁcation problems consisting mixtures manifolds varying dimensions shapes sizes. adequate classes diﬀer geometry independent assigned labels. general classes diﬀer label statistics geometry correlated labels. instance positively labelled manifolds consist geometry negatively labelled manifolds diﬀerent geometry. structural diﬀerences classes aﬀect capacity linear classiﬁcation? linear classiﬁer take advantage correlations adding non-zero bias. previously assumed optimal separating hyperplane passes origin; reasonable classes statistically same. however statistical diﬀerences label assignments classes deﬁne sparsity parameter fraction positively-labeled manifolds corresponds balanced labels. theory classiﬁcation ﬁnite random points known sparse labels drastically increase capacity section investigate sparsity manifold labels improves manifold classiﬁcation capacity. separating hyperplane constrained origin distribution inputs symmetric around origin labeling immaterial capacity. thus eﬀect sparse labels closely tied non-zero bias. thus consider inequality constraints form deﬁne bias-dependent capacity general manifolds label sparsity margin bias next observe bias acts positive contribution margin positively-labeled population negative contribution negatively-labeled population. thus dependence expressed classiﬁcation capacity zero bias manifolds. note similar mixtures manifolds. actual capacity sparse labels given optimizing expression respect i.e. following consider simplicity eﬀect sparsity zero margin importantly large eﬀect manifold geometry sparsely labeled manifolds much larger non-sparse labels. non-sparse labels capacity ranges sparse manifolds upper bound much larger. indeed small-sized manifolds expected capacity increases upon decreasing similar uncorrelated points pof|log tential increase capacity sparse labels however strongly constrained manifold size since manifolds large solution orthogonal thus manifold directions geometry manifolds plays important role controlling eﬀect sparse labels capacity. aspects already seen case sparsely labeled balls summarize main results general manifolds. sparsity size complex interplay between label sparsity manifold size. analysis yields three qualitatively diﬀerent regimes gaussian radius manifolds small i.e. extent manifolds noticeable dimension high. similar previous analysis high dimensional manifolds sparse capacity equivalent capacity sparsely labeled random points eﬀective margin given maxbα gardner theory. noted equation noticeable eﬀect moderate sparsity. negligible eﬀect since bias large dominates margin. moderate sizes case equivalence capacity points breaks down. remarkably capacity general manifolds substantial size well approximated equivalent balls sparsity dimension radius equal gaussian dimension radius manifolds namely surprisingly unlike nonsparse approximation equivalence general manifold balls valid high dimensional manifolds sparse limit spherical approximation restricted large another interesting result relevant statistics given gaussian geometry rgand even small. reason small bias large. case positively labeled manifolds large positive margin fully supporting giving contribution inverse capacity regardless detailed geometry. hand negatively labeled manifolds large negative margin implying separating plane small fraction touching support. fully supporting conﬁgurations negligible probability hence overall geometry well approximated gaussian quantities scaling relationship sparsity size analysis capacity balls sparse labels shows retains simple form depends scaled sparsity reason scaling follows. labels sparse dominant contribution inverse capacity comes minority class capacity ball large hand optimal value depends balance contributions classes scales linearly needs overcome local ﬁelds spheres. thus combining yields general sparsely labeled manifolds scaled sparsity rather yield smoother cross-over small regime. similarly deﬁne optimal scaled qualitatively function bias roughly proportional proportionality constant depends extreme limit note suﬃciently small gain capacity sparsity occurs even large manifolds long large regime finally suﬃciently large increases order smaller capacity small value depends detailed geometry manifold. particular capacity manifold approaches demonstrate remarkable predictions present fig. capacity three classes sparsely labeled manifolds balls ellipsoids ring manifolds. cases show results numerical simulations capacity full mean ﬁeld solution spherical approximation across several orders magnitude sparsity size function scaled sparsity example good agreement three calculations range furthermore drop increasing similar cases except overall vertical shift diﬀerent similar eﬀect dimension balls regime moderate radii results diﬀerent fall universal curve depends predicted theory. small capacity deviates scaling dominated alone similar sparsely labeled points. curves deviate spherical approximation. true capacity rapidly decreases limit saturates figure manifold conﬁgurations geometries classiﬁcation ellipsoids sparse labels analyzed separately terms majority minority classes. radii ellipsoids histogram support dimensions moderate sparsity minority majority manifolds. histogram support dimensions high sparsity minority majority manifolds. manifold dimension function varied minority manifold radius relative scaling factor function minority majority manifolds. summary developed statistical mechanical theory linear classiﬁcation inputs organized perceptual manifolds points manifold share label. notion perceptual manifolds critical variety contexts computational neuroscience modeling signal processing. theory restricted manifolds smooth regular geometries; applies compact subset d-dimensional aﬃne subspace. thus theory applicable manifolds arising variation neuronal responses continuously varying physical variable sampled arising experimental measurements limited number stimuli. theory describes capacity linear classiﬁer separate dichotomy general manifolds universal mean ﬁeld equations. equations solved analytically simple geometries complex geometries developed iterative algorithms solve self-consistent equations. algorithms eﬃcient converge fast involve solving variables sinfigure classiﬁcation balls sparse labels capacity balls function mean ﬁeld theory approximation interpolating eqs. classiﬁcation general manifolds sparse labels capacity ellipsoids ﬁrst components equal remaining components varied numerical simulations mean ﬁeld theory spherical approximation. capacity ring manifolds gaussian fall-oﬀ spectrum fig. shows capacity ring manifolds whose fourier components gaussian fall-oﬀ i.e. spherical approximation. finally note choice parameters signiﬁcantly diﬀerent simply average radius. thus agreement theory illustrates important role gaussian geometry sparse case. discussed above sparse regimes large bias alters geometry classes diﬀerent ways. illustrate important aspect show fig. eﬀect sparsity bias geometry ellipsoids studied fig. show evolution majority minority classes increases. note despite fact shape manifolds manifold anchor geometry depends class membership sparsity levels measures depend margin. small minority class seen fig. i.e. minority class manifolds close fully supporting large positive margin. also seen distributions support dimension shown fig.. hand majority class logd manifolds mostly interior regime. increases geometrical statistics classes become similar. seen fig.- majority minority classes converge zero margin value manifold rather invoking simulations full system manifolds embedded applications statistical mechanical theory perceptron learning long provided basis understanding performance fundamental limitations single layer neural architectures kernel extensions. however previous theory considered ﬁnite number random points underlying geometric structure could explain performance linear classiﬁers large possibly inﬁnite number inputs organized distinct manifolds variability changes physical parameters objects. theory presented work explain capacity limitations linear classiﬁcation general manifolds. theory important understanding sensory neural systems perform invariant perceptual discrimination recognition tasks realistic stimuli. furthermore beyond estimating classiﬁcation capacity theory provides theoretically based geometric measures assessing quality neural representations perceptual manifolds. variety ways deﬁning geometry manifolds. geometric measures unique determine ability linearly separate manifold theory shows. theory focuses linear classiﬁcation. however broad implications non-linear systems particular deep networks. first models sensory discrimination recognition biological artiﬁcial deep architectures model readouts networks linear classiﬁers operating sensory layers. thus manifold classiﬁcation capacity geometry applied understand performance deep network. furthermore computational advantage multiple intermediate layers assessed comparing performance hypothetical linear classiﬁer operating layers. addition changes quality representations across deep layers assessed comparing changes manifold’s geometries across layers. indeed previous discussions sensory processing deep networks hypothesized neural object representations become increasingly untangled signal propagates along sensory hierarchy. however concrete measure untangling provided. geometric measures derived theory used quantify degree entanglement tracking perceptual manifolds nonlinearly reformatted propagate multiple layers neural network eventually allow linear classiﬁcation layer. notably statistical population-based nature geometric measures renders ideally suited comparison between layers diﬀerent sizes nonlinearities well diﬀerent deep networks artiﬁcial networks biological ones. lastly theory suggest algorithms building deep networks instance imposing successive reduction manifold dimensions radii part unsupervised learning strategy. discussed domain visual object classiﬁcation recognition received immense attention recent years. however would like emphasize theory applied modeling neural sensory processing tasks modalities. instance used provide insight olfactory system performs discrimination recognition odor identity presence orders magnitude variations odor concentrations. neuronal responses sensory signals static vary time. theory applied explain brain correctly decodes stimulus identity despite temporal nonstationarity. applications require extensions present theory. important ones include present work assumed correlations directions aﬃne subspaces diﬀerent manifolds uncorrelated. realistic situations expect correlations manifold geometries mainly types center-center correlations. correlations harmful linear separability another correlated variability directions aﬃne subspaces correlated centers. positive correlations latter form beneﬁcial separability. extreme case manifolds share common aﬃne subspace rank union subspaces dtot rather dtot solution weight vector need null space smaller subspace. work needed extend present theory incorporate general correlations. generalization performance studied separability manifolds known geometries. many realistic problems information readily available samples reﬂecting natural variability input patterns provided. samples used estimate underlying manifold model and/or train classiﬁer based upon ﬁnite training set. generalization error describes well classiﬁer trained ﬁnite number samples would perform test points drawn manifolds would important extend theory calculate expected generalization error achieved maximum margin solution trained point cloud manifolds function size training geometry underlying full manifolds. unrealizable classiﬁcation throughout present work assumed manifolds separable linear classiﬁer. realistic problems κ√n. however distance depends scale input vectors correct scaling since margin thermodynamic limit adopted normalization correct scaling margin evaluation solution volume following gardner’s replica framework ﬁrst consider volume solution space deﬁne signed projections direction vector solution weight then separability constraints hence volume heaviside step function. support function deﬁned volume deﬁned depends quenched random variables well known order obtain typical behavior thermodynamic limit need average carry using replica trick limn→ natural refers average need evaluate load capacity linear separation i.e. alternatively neural noise cause manifolds unbounded extent tails distribution overlapping separable zero error. several ways handle issue supervised learning problems. possibility nonlinearly unrealizable inputs higher dimensional feature space multi-layer network nonlinear kernel function classiﬁcation performed zero error. design multilayer networks could facilitated using manifold processing principles uncovered theory. another possibility introduce optimization problem allowing small training error example using complementary slack variables procedures raise interesting theoretical challenges including understanding geometry manifolds change undergo nonlinear transformations well investigating statistical mechanics performance linear classiﬁer manifolds slack variables conclusion believe application theory corollary extensions precipitate novel insights perceptual systems biological artiﬁcial eﬃciently code process complex sensory information. would like thank cohen ryan adams leslie valiant david dicarlo doris tsao yoram burak helpful discussions. work partially supported gatsby charitable foundation swartz foundation simons foundation human frontier science program also acknowledges support national science foundation army research laboratory oﬃce naval research force oﬃce scientiﬁc research department transportation. section outline derivation mean ﬁeld replica theory summarized eqs. deﬁne capacity linear classiﬁcation manifolds maximal load high probability solution exists given points manifolds assume components drawn independently gaussian disn tribution zero mean variance binary labels randomly assigned manifold equal probabilities. consider thermodynamic limit ﬁnite. note geometric margin deﬁned distance solution hyperplane given yµw·xµ note √qti represents quenched random component randomness thermal component variability within solution space. order parameter calculated limit overcapacity solutions become unity volume shrinks zero. convenient deﬁne study limit limit leading order finally note mean squared ’annealed’ variability ﬁelds entropy solutions vanishes capacity limit thus quantity equation represents annealed variability times remains ﬁnite limit evaluate capacity strictly convex manifolds starting expression general manifolds strictly convex manifold point line segment connecting points belongs interior thus boundary manifold contain edges ﬂats spanning dimension except entire manifold therefore exactly contributions inverse capacity. obeys ttoucht) tfst) integrand contributes t·˜st )−t+κ) tfst) manifold fully embedded. case integrand reduces summary capacity convex manifolds written ellipsoids support function computed explicitly follows. vector zero support function minimized vector occurs boundary ellipsoid i.e. obeys equality constraint evaluate sivi respect diﬀerentiate lagrange multiplier enforcing constraint yielding dimensional vector ellipsoid principal radii denote ◦refers pointwise product vectors viri. given vector determined analytic solution used derive explicit expressions diﬀerent regimes follows. interior regime interior regime resulting zero contribution inverse capacity. anchor point given following boundary point ellipse given regime holds obeying inequality ttoucht) yielding form intuition beyond factor clear stemming fact distance point margin plane scales norm hence margin entering capacity factor norm. stated above implies ﬁnite capacity scaling regime hand implies regime contribution capacity given touching regime holds ttouch vanishes anchor point point boundary ellipsoid antiparallel substituting value yields fully supporting regime implying center well entire ellipsoid fully supporting margin solution. case anchor point antiparallel interior point contribution capacity linear separation large size limit manifolds reduces linear separation random d-dimensional aﬃne subspaces. separating subspaces must fully embedded margin plane otherwise would intersect violate classiﬁcation constraints. however large size manifolds approach limit subtle. analyze limit note large condition i.e. o˜s−). small implies aﬃne basis vectors except center direction either exactly almost orthogonal solution weight vector. since −t+v follows almost antiparallel gaussian vector hence elucidate manifold support structure note ﬁrst ttouch −||t||||˜s|| hence fractional volume interior regime negligible statistics dominated embedded regimes. fact fully embedded transition given tembed fractional volume fully embedded regime contribution inverse capacity there remaining summed probability touching partially embedded regimes therefore regimes combining contributions obtain large sizes fully supporting balls touching balls almost parallel margin planes hence capacity reaches lower bound. finally large manifold limit discussed appendix average gaussian evaluating involves calculations self consistent statistics anchor points. calculation simpliﬁed high dimension. particular reduces hence approximately constant independent deriving approximations used selfaveraging summations involving intrinsic coordinates. full dependence longitudinal gaussian remain. thus rmand fact substituted denoting averaging would yield complicated expressions eq.dd. reason replace average quantities following potential dependence anchor radius dimension however inspecting note scenarios small order case manifold radius small contribution small neglected. argument case geometry replaced gaussian geometry depend second scenario order case order contribution negligible. optimal bias given ∂αball/∂b analyze equations various size regimes assuming small balls small radius capacity points unless dimensionality high. thus large note aﬀect capacity scaled sparsity. capacity proportional realistic regime small capacity decreases roughly proportionality constant depends shown examples fig. finally suﬃciently large order smaller. case second term dominates contributes yielding capacity saturates shown fig. noted however large approximations don’t hold anymore. noted optimal bias diverges hence presence order induced margin noticeable moderate small large analyze equations limit small large assume suﬃciently small optimal bias large. contribution minority class inverse capacity deriving last equation used second integrals substantial value case integral combining results yields following simple expression inverse capacity poole subhaneil lahiri maithreyi raghu jascha sohl-dickstein surya ganguli. exponential expressivity deep neural networks transient chaos. advances neural information processing systems pages marc aurelio ranzato huang y-lan boureau yann lecun. unsupervised learning invariant feature hierarchies applications object recognition. computer vision pattern recognition cvpr’. ieee conference pages ieee yoshua bengio. learning deep architectures foundations trends machine learning charles cadieu hong daniel yamins nicolas pinto diego ardila ethan solomon najib majaj james dicarlo. deep neural networks rival representation primate cortex core visual object recognition. plos comput biol thomas cover. geometrical statistical properties systems linear inequalities applications pattern recognition. ieee transactions electronic computers deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference pages ieee christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition pages joshua tenenbaum silva john langford. global geometric framework nonlinear dimensionality reduction. science shun-ichi amari naotake fujita shigeru shinomoto. four types learning curves. neural computation", "year": "2017"}