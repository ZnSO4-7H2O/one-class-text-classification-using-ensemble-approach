{"title": "Variational auto-encoding of protein sequences", "tag": "q-bio", "abstract": " Proteins are responsible for the most diverse set of functions in biology. The ability to extract information from protein sequences and to predict the effects of mutations is extremely valuable in many domains of biology and medicine. However the mapping between protein sequence and function is complex and poorly understood. Here we present an embedding of natural protein sequences using a Variational Auto-Encoder and use it to predict how mutations affect protein function. We use this unsupervised approach to cluster natural variants and learn interactions between sets of positions within a protein. This approach generally performs better than baseline methods that consider no interactions within sequences, and in some cases better than the state-of-the-art approaches that use the inverse-Potts model. This generative model can be used to computationally guide exploration of protein sequence space and to better inform rational and automatic protein design. ", "text": "proteins responsible diverse functions biology. ability extract information protein sequences predict effects mutations extremely valuable many domains biology medicine. however mapping protein sequence function complex poorly understood. present embedding natural protein sequences using variational auto-encoder predict mutations affect protein function. unsupervised approach cluster natural variants learn interactions sets positions within protein. approach generally performs better baseline methods consider interactions within sequences cases better state-of-the-art approaches inverse-potts model. generative model used computationally guide exploration protein sequence space better inform rational automatic protein design. protein engineering increasing importance modern therapeutics. designing novel proteins perform particular function challenging number functional proteins compared possible protein sequences miniscule. renders naive experimental search desirable variants intractable. hence computational heuristic narrow experimental search space extremely valuable. variety energy-based models protein folding used past decades recent advances machine learning particularly domain generative models opened avenues computational protein design. rich databases protein sequences document functional proteins found living organisms provide ample training data. majority datasets lack labels however prompts unsupervised learning approach. sequences arise closely related living organisms reasonable assume functional given sparse unstructured discrete space protein sequences exist prudent anchor search functional sequences known protein desired functionality. starting sequence interest search public databases sequence variants related ∗program evolutionary dynamics department organismic evolutionary biology †wyss institute correspondence directed §department genetics ¶department mathematics interested using data unsupervised manner train models inform protein function. hope models good candidate sequences function similarly better already observed. generative models appealing properties purpose trained sequence data alone produce candidate sequences similar present dataset exactly same. variational auto-encoders type unsupervised generative models reconstruct input compressed continuous latent domain. traditional auto-encoders neural networks reconstruct input imperfectly. vaes incorporate variational bayesian methods impose lower bound probability input allowing probabilistic interpretation results. protein sequence length lives dimensional space possible values. number sequences within possibilities perform particular function small. hope achieve compress large space lower dimensional continuous embedding latent variables explain differences protein sequences. protein design purposes traverse space functional proteins. additionally would hope compression would teach properties affect protein function. past years generative graphical models used sequence alignments predict protein structure function models learn correlations amino acids different positions approximate effects changing amino-acid angiven position. successful applications methods used potts models core modeling approach. models incorporate independent pairwise interactions along sequence. technical details explained application large data recently published methods show harnessing correlations pairs amino acids different positions provides signiﬁcant power protein folding function prediction. recently variational-auto encoders used continuous representation chemical compounds allowed optimization process chemical design additionally variational inference graphical models shown hold promise predicting protein structure show vaes also hold promise protein design. method model needs learn joint probability latent variables observed data. learn good distribution latent variables generate data points like haven’t observed dataset sampling sampling points computing observed data requires compute evidence term data point good model would maximize probability data. however direct computation integral intractable. instead approximated using variational inference. speciﬁcally approximate using evidence lower bound formula family normal distributions approximating kullback-leibler divergence. vaes learn parameters distributions simultaneously gradient descent. language neural networks speciﬁes encoder speciﬁes decoder. maximizing lower bound evidence gradient ascent approximation maximum likelihood data. notably also standard assumption prior build generative model produce sequences like dataset high probability generate novel similar sequences evaluate likelihood sequences model hasn’t seen before. matrix representing one-hot encoding sequence interest number amino-acids number positions considered protein alignment. identical dimensions probability weight matrix generated feeding network sequence. generated multiple ways simplest procedure compute reconstructing sequence represented alternatively computed average reconstruction multiple neighboring sequences reconstruction wild-type sequence. found approaches result similar performance function prediction. validate model feeding network sequences single double mutants calculate probability. compare rank correlation probability experimental ﬁtness measurements. neither test sequences ﬁtness measurements passed network training. report outcomes training model using procedure described protein families ﬁtness measurements publicly available following architecture vae. encoder decoder network three dense layers exponential linear units encoder decoder include dropout layer. architecture selected grid search hyper-parameters including number dense hidden layers number units layer inclusion/exclusion dropout batch normalization layers hidden layer. ﬁnal decoder layer uses sigmoid neurons. keras implement model train model using adam optimizer empirically networks dense layers trained faster performed comparably better convolutional layers. protein function prediction latent variables. lower dimensional representation visualization latent variables. pruning latent variables slightly weakens predictive power provides easily interpretable representations latent space. results summarized three main observations probability estimates calculated network correlate well protein functions measured experiments embedding separates variants minimum edit-distance clusters latent space used learns pairwise higher-level interactions loci protein show vae’s viable approach predict sequence functionality unlabeled sequences overall performs better baseline datasets tested suggesting captures relevant information protein structure. datasets selected presumed large sufﬁciently diverse training because used previous approaches aimed predict protein function. expect figure summary results. comparison predictions ﬁtness measurements double mutations reference sequence line shows hypothetical perfect rank correlation. comparison model’s prediction ability baseline pairwise models size dataset provided reference. projection training data latent space. square latent coordinates reference sequence. points colored k-means clustering sequences showing branches star-like latent structure correspond close-by sequences. conﬁrmed fact experimental data single double mutants fall close reference example change input sequence represented one-hot matrix corresponds wild-type yellow mutant sequences separately network difference reconstruction matrices pmut shown bottom panel. bottom single mutation results updates probability many locations sequence thus least pairwise higher-order interactions captured. wild-type sequence denoted dark spots mutation marked proteins small size relative length natural diversity less suitable approach. line expectation model performs better inverse potts approach pabp largest size relative length. observations indicate models generate candidate sequences high likelihood performing particular function comparable sequences training set. unlike inverse potts model latent layer provides continuous representation protein sequence. argued chemical molecules continuous representation protein used together gradient-based optimization achieve desirable property. show fig. latent space encodes phylogenetic data possibly features protein. ability continuously traverse latent space provided approach yield opportunities informed protein design qualitatively different present-day methods. study serves proof-of-concept utility vaes representing protein families well ability predict effects mutations. work improved certain dimensions. despite longer training times expect recurrent convolutional architectures outperform model hence exhaustive search architectures would prudent. predicted effects pairwise higher order interactions also validated projecting onto protein tertiary structures. additionally method could adjusted sample weights standard approaches however found empirically reweighing consistently help performance across datasets. would like thank pierce ogden surojit biswas gleb kuznetsov jeffery gerold helpful comments. would also like thank debora marks john ingraham adam riesselman feedback project independently pursued similar research objective", "year": "2017"}