{"title": "Efficient Algorithms for Searching the Minimum Information Partition in  Integrated Information Theory", "tag": "q-bio", "abstract": " The ability to integrate information in the brain is considered to be an essential property for cognition and consciousness. Integrated Information Theory (IIT) hypothesizes that the amount of integrated information ($\\Phi$) in the brain is related to the level of consciousness. IIT proposes that to quantify information integration in a system as a whole, integrated information should be measured across the partition of the system at which information loss caused by partitioning is minimized, called the Minimum Information Partition (MIP). The computational cost for exhaustively searching for the MIP grows exponentially with system size, making it difficult to apply IIT to real neural data. It has been previously shown that if a measure of $\\Phi$ satisfies a mathematical property, submodularity, the MIP can be found in a polynomial order by an optimization algorithm. However, although the first version of $\\Phi$ is submodular, the later versions are not. In this study, we empirically explore to what extent the algorithm can be applied to the non-submodular measures of $\\Phi$ by evaluating the accuracy of the algorithm in simulated data and real neural data. We find that the algorithm identifies the MIP in a nearly perfect manner even for the non-submodular measures. Our results show that the algorithm allows us to measure $\\Phi$ in large systems within a practical amount of time. ", "text": "abstract ability integrate information brain considered essential property cognition consciousness. integrated information theory hypothesizes amount integrated information brain related level consciousness. proposes quantify information integration system whole integrated information measured across partition system information loss caused partitioning minimized called minimum information partition computational cost exhaustively searching grows exponentially system size making difﬁcult apply real neural previously shown measure satisﬁes mathematical property data. submodularity found polynomial order optimization algorithm. however although ﬁrst version submodular later versions not. study empirically explore extent algorithm applied non-submodular measures evaluating accuracy algorithm simulated data real neural data. algorithm identiﬁes nearly perfect manner even non-submodular measures. results show algorithm allows measure large systems within practical amount time. keywords integrated information theory; integrated information; minimum information partition; submodularity; queyranne’s algorithm; consciousness. brain receives various information external world. integrating information essential property cognition consciousness fact phenomenologically consciousness uniﬁed. example object cannot experience shape independently color. cannot experience left half visual ﬁeld independently right half. integrated information theory consciousness considers uniﬁcation consciousness realized ability brain integrate information brain internal mechanisms integrate information shape color object information right left visual ﬁeld therefore visual experiences uniﬁed. proposes quantify degree information integration information theoretic measure integrated information hypothesizes integrated information related level consciousness. although hypothesis indirectly supported experiments showed breakdown effective connectivity brain loss consciousness studies directly quantiﬁed integrated information real neural data computational difﬁculties described below. conceptually integrated information quantiﬁes degree interaction parts equivalently amount information loss caused splitting system parts proposes integrated information quantiﬁed least interdependent parts quantiﬁes information integration system whole. example system consists independent subsystems subsystems least interdependent parts. case integrated information information loss system partitioned independent subsystems. critical partition system called minimum information partition information minimally lost equivalently integrated information minimized. general searching requires exponentially large amount computational time number partitions exponentially grows arithmetic growth system size computational difﬁculty hinders application experimental data despite potential importance consciousness research even broader ﬁelds neuroscience. present study exploit mathematical concept called submodularity resolve combinatorial explosion ﬁnding mip. submodularity important concept functions analogous convexity continuous functions. known exponentially large computational cost minimizing objective function reduced polynomial order objective function satisﬁes submodularity. previously hidaka oizumi showed computational cost ﬁnding reduced utilizing queyranne’s submodular optimization algorithm used mutual information measure integrated information satisﬁes submodularity. measure integrated information used ﬁrst version based mutual information. thus consider mutual information practical approximation measure integrated information queyranne’s algorithm utilized ﬁnding mip. however practical measures integrated information later versions submodular. paper extend applicability submodular optimization non-submodular measures integrated information. speciﬁcally consider three measures integrated information; mutual information stochastic interaction geometric integrated information mutual information strictly submodular others not. oizumi previously showed close relationship among three measures relationship speculate queyranne’s algorithm might work well non-submodular measures. here empirically explore extent queyranne’s algorithm applied non-submodular measures integrated information evaluating accuracy algorithm simulated data real neural data. queyranne’s algorithm identiﬁes nearly perfect manner even non-submodular measures. results show queyranne’s algorithm utilized even non-submodular measures integrated information makes possible practically compute integrated information across real neural data multi-unit recordings used ecog typically consist around channels. although originally proposed understanding consciousness utilized analyze system irrespective consciousness biological networks multi-agent systems oscillator networks. therefore work would beneﬁcial consciousness studies also research ﬁelds involving complex networks random variables. paper organized follows. ﬁrst explain three measures integrated information closely related uniﬁed theoretical framework order relation among three measures; next compare partition found queyranne’s algorithm found exhaustive search randomly generated small networks also evaluate performance queyranne’s algorithm larger networks since exhaustive search intractable compare queyranne’s algorithm different optimization algorithm called replica exchange markov chain monte carlo method finally evaluate performance queyranne’s algorithm ecog data recorded monkeys investigate applicability algorithm real neural data. consider stochastic dynamical system consisting elements. represent past present states system respectively. case neural system variable signals multi-unit recordings ecog fmri etc. conceptually integrated information designed quantify degree spatio-temporal interactions subsystems. previously proposed measures integrated information generally expressed kullback-leibler divergence actual probability distribution disconnected probability distribution interactions subsystems removed kullback-leibler divergence measures difference probability distributions interpreted information loss used approximate thus integrated information interpreted information loss caused removing interactions. minimum taken best approximation satisfying constraint interactions subsystems removed arrows indicate inﬂuences across different time points lines without arrowheads indicate inﬂuences elements time. below show three different measures integrated information derived different probability distributions past present states i-th subsystem respectively. model interactions subsystems removed i.e. subsystems totally independent case corresponding measure integrated information given represents joint entropy. measure called total correlation multi information special case number subsystems measure simply equivalent mutual information subsystems measure integrated information used ﬁrst version based mutual information identical mutual information critical difference measures based perturbation considered study based observation. perturbational approach used evaluating probability distributions attempts quantify actual causation perturbing system possible states perturbational approach requires full knowledge physical mechanisms system i.e. system behaves response possible perturbations. measure deﬁned based observational probability distribution estimated empirical data. since empirical application method consider perturbational approach study. partitions transition probability past present whole system product transition probability subsystem. corresponds removing causal inﬂuences case corresponding measure integrated information given indicates conditional entropy. measure proposed practical measure integrated information barrett seth following measure proposed second version measure also independently derived measure complexity means present state subsystem depends past state corresponds removing causal inﬂuences subsystems retaining equal-time interactions constraint equivalent markov condition section provide mathematical deﬁnition minimum information partition then formulate search optimization problem function. partition divides system least interdependent subsystems information loss caused removing interactions among subsystems minimized. information loss quantiﬁed measure integrated information. thus πmip deﬁned partition integrated information minimized partitions. general universal partitions including bi-partitions tri-partitions study however focus bi-partitions simplicity computational time. bi-partition whole system divided subset complement since bi-partition uniquely determined specifying subset integrated information considered function finding equivalent ﬁnding subset smip achieves minimum integrated information since number bi-partitions system n-elements exhaustive search large system intractable. however formulating search optimization function above take advantage discrete optimization technique reduce computational costs polynomial order described next section. submodularity important concept functions analogue convexity continuous functions objective functions submodular efﬁcient algorithms available solving optimization problems. particular symmetric submodular functions function called symmetric integrated information computed bi-partition symmetric function speciﬁes bi-partition. function symmetric submodular minimum function queyranne’s algorithm function calls previous study queyranne’s algorithm utilized used measure integrated information shown previously submodular however measures integrated information submodular. study apply queyranne’s algorithm non-submodular functions objective functions submodular queyranne’s algorithm necessarily mip. evaluate accurately queyranne’s algorithm used non-submodular measures integrated information. order relation among three measures integrated information inequality graphically understood fig. connections removed larger corresponding integrated information measures causal inﬂuences subsystems measures equal-time interactions present states well causal inﬂuences subsystems measures interactions subsystems. thus closer relationship implies would behave similarly submodular measure does. thus surmise queyranne’s algorithm would work accurately show subsection indeed case. however difference rather small queyranne’s algorithm works almost perfectly measures evaluate accuracy queyranne’s algorithm compare partition found queyranne’s algorithm found exhaustive search number elements small enough however large cannot know exhaustive search unfeasible. evaluate performance queyranne’s algorithm large system compare different method replica exchange markov chain monte carlo method remcmc also known parallel tempering method draw samples probability distributions. remcmc improved version mcmc methods. here brieﬂy explain search problem represented problem drawing samples probability distribution. details remcmc method given appendix parameter called inverse temperature. probability higher/lower smaller/larger. gives highest probability deﬁnition. draw samples distribution selectively scan subsets integrated information efﬁciently compared randomly exploring partitions independent value integrated information. simple mcmc methods like metropolis method draw samples single value often suffer problem slow convergence. sample sequence trapped local minimum sample distribution takes time converge target distribution. remcmc aims overcoming problem drawing samples parallel distributions multiple values continually exchanging sampled sequences neighboring ﬁrst evaluated performance queyranne’s algorithm simulated networks. throughout simulations below consider case variable obeys gaussian distribution ease computation. shown appendix measures integrated information analytically computed. note although computed principle even distribution gaussian practically hard compute large systems computation involves summation possible speciﬁcally consider ﬁrst order autoregressive model present states past states system connectivity matrix gaussian noise. stationary distribution model considered. stationary distribution gaussian distribution. covariance matrix consists covariance cross-covariance computed solving following equation ﬁrst evaluated computation time queyranne’s algorithm compared exhaustive search number elements changed. connectivity matrices randomly generated. element connection matrix sampled normal distribution mean variance ./n. covariance gaussian noise generated wishart distribution covariance degrees freedom corresponded amount noise identity matrix. wishart distribution standard distribution symmetric positive-semideﬁnite matrices typically distribution circles solid lines indicate computational time queyranne’s algorithm approximate curves black triangles black dashed lines indicate computational time exhaustive search approximate curves used generate covariance matrices inverse covariance matrices. practical details example number elements changed computation times measured machine intel xeon .ghz. calculations implemented matlab. figure shows results φsi. circles indicate computational time queyranne’s algorithm solid line contrast black triangles indicate exhaustive search black dashed line means computational time queyranne’s algorithm increases polynomial order exhaustive search exponentially increases example queyanne’s algorithm takes exhaustive search takes sec. practice impossible compute even supercomputer. similarly shown fig. used queyranne’s algorithm roughly takes exhaustive search takes note reason order computational time queyrannes algorithm higher multi-dimensional optimization needed compute evaluated accuracy queyranne’s algorithm comparing partition found queyranne’s algorithm found exhaustive search. used measures integrated information. considered different architectures connectivity matrix models. ﬁrst random matrix element randomly sampled normal distribution mean variance ./n. block matrix consisting sub-matrices aij. element diagonal sub-matrices drawn normal distribution mean variance ./n. off-diagonal sub-matrices zero matrices. covariance gaussian noise model generated wishart distribution parameter number elements randomly generated connectivity matrices setting evaluated performance using following four measures. following measures averaged trials. correct rate correct rate rate correctly ﬁnding mip. rank rank rank partition found queyranne’s algorithm among possible partitions. rank based values computed partition. partition gives lowest rank highest rank equal number possible bi-partitions error ratio error ratio deviation value integrated information computed across partition found queyranne’s algorithm computed across normalized mean error computed possible partitions. error ratio deﬁned φmip amount integrated information computed across computed across partition found queyranne’s algorithm mean amounts integrated information computed across possible partitions respectively. correlation partition found queyranne’s algorithm found exhaustive search. represent bi-partition n-elements n-dimensional vector indicates subgroups. absolute value correlation vector given given partition found queyranne’s algorithm computed results summarized table table shows that used queyranne’s algorithm perfectly found mips trials even though strictly submodular. similarly used queyranne’s algorithm almost perfectly found mips. correct rate normal models block structured models. additionally even algorithm missed rank partition found algorithm averaged rank trials block structured models. also error ratio error trials around average error ratios small. appendix plots values integrated information partitions. thus miss trials would affect evaluation amount integrated information practice. however terms partitions partitions found queyranne’s algorithm error trials markedly different mips. block structured model partition split system halves. contrast partitions found queyranne’s algorithm one-vs-all partitions. summary queyranne’s algorithm perfectly worked φsi. regards although queyranne’s algorithm almost perfectly evaluated amount integrated information need treat partitions found algorithm carefully. slight difference performance explained order relation closer strictly submodular function consider queyranne’s algorithm worked better evaluated performance queyranne’s algorithm large systems exhaustive search impossible. compared replica exchange markov chain monte carlo method applied algorithms models generated similarly previous section. number elements respectively. reason difference requires much heavier computation randomly generated connectivity matrices setting. compared algorithms terms amount integrated information number evaluations remcmc convergence criterion satisﬁed. appendix details convergence criterion. results shown tables winning percentage indicates fraction trials algorithm terms amount integrated information partition found algorithm. partitions found algorithms exactly matched trials. consider algorithms probably found mips following three reasons. first well known remcmc minima sufﬁciently long time many applications second algorithms different unlikely incorrectly identiﬁed partitions mips. third queyranne’s algorithm successfully ﬁnds mips smaller systems shown previous section. fact suggests queyranne’s algorithm worked well also larger systems. note case half-and-half partition block structured model half-and-half partition. conﬁrmed partitions found queyanne’s algorithm remcmc half-and-half partition trials. thus block structured case certain true mips successfully found algorithms. also evaluated number evaluations algorithms computational processes. simulations computational process queyranne’s algorithm ended much faster convergence remcmc. queyranne’s algorithm ends ﬁxed number evaluations depending contrast number evaluations convergence remcmc depends many factors network models initial conditions pseudo random number sequences. thus time convergence varies among different trials. note retrospectively examining sequence monte carlo search solutions turned found earlier points monte carlo searches queyranne’s algorithm however impossible stop remcmc algorithm points solutions found tell whether points reach solution algorithm enough amount time. finally ensure applicability queyranne’s algorithm real neural data similarly evaluated performance electrocorticogram data recorded macaque monkey. dataset available open database neurotycho.org hundred twenty-eight channel ecog electrodes implanted left hemisphere. electrodes placed intervals covering frontal parietal temporal occipital lobes medial frontal parietal walls. signals sampled rate down-sampled analysis. monkey chibi awake eyes covered eye-mask restrain visual responses. remove line noise artifacts performed bipolar re-referencing nearest neighbor electrode pairs. number re-referenced electrodes total. ﬁrst simulation evaluated accuracy. extracted -minute length signals electrodes. -minute sequence consists sec. samples. then randomly selected electrodes times. approximated probability distribution signals multivariate gaussian distributions. covariance matrices computed time window time step applied algorithms randomly selected sets electrodes measured accuracy similarly subsection results summarized table queyranne’s algorithm worked perfectly next compared queyranne’s algorithm remcmc. applied algorithms re-referenced signals evaluated performance terms amount integrated information number evaluations subsection segmented non-overlapping sequences minute each computed covariance matrices time step measured average performance sets. here used requires heavy computations dimensional systems. results shown table partitions selected algorithms matched sequences. terms amount computation queyranne’s algorithm ended much faster convergence remcmc. study proposed efﬁcient algorithm searching minimum information partition integrated information theory computational time exhaustive search grows exponentially arithmetic growth system size obstacle applying experimental data. showed using submodular optimization algorithm called queyranne’s algorithm computational time reduced stochastic interaction geometric integrated information respectively. measures integrated information non-submodular thus theoretically guaranteed queyranne’s algorithm mip. empirically evaluated accuracy algorithm comparing exhaustive search simulated data ecog data recorded monkeys. found queyranne’s algorithm worked perfectly almost perfectly also tested performance queyranne’s algorithm larger systems exhaustive search intractable comparing replica exchange markov chain monte carlo method found partitions found algorithms perfectly matched suggests algorithms likely found mips. terms computational time number evaluations taken queyranne’s algorithm much smaller taken remcmc convergence. results indicate queyranne’s algorithm utilized effectively estimate even non-submodular measures integrated information. although concept originally proposed understanding consciousness utilized general network analysis irrespective consciousness. thus method searching proposed study beneﬁcial consciousness studies research ﬁelds. here discuss pros cons queyranne’s algorithm comparison remcmc. since partitions found algorithms perfectly matched experiments equally good terms accuracy. regards computational time queyranne’s algorithm ended much faster convergence remcmc. thus queyranne’s algorithm would better choice rather large systems note retrospectively examine sampling sequence remcmc remcmc found partitions much earlier convergence estimated mips change later parts sampling process. thus could introduce heuristic criterion determine stop sampling based time course estimated mips remcmc could stopped earlier convergence. however setting heuristic criterion non-trivial problem. queyranne’s algorithm ends within ﬁxed number function calls regardless properties data. system size much larger queyranne’s algorithm computationally demanding time complexity practically work. case remcmc would work better above-mentioned heuristics introduced stop algorithm earlier convergence. alternative interesting approach approximately ﬁnding graph-based algorithm proposed toker sommer method reduce search space candidate partitions selected spectral clustering method based correlation. calculated candidate partitions best partition selected. difference method whether search method fully based values integrated information not. method uses quantities searching method uses graph theoretic measure signiﬁcantly differ cases. would interesting future work compare method graph-theoretic methods combine methods develop better search algorithms. study considered three different measures integrated information these submodular measures not. described section clear order relation among closer submodular function relation implies queyranne’s algorithm would work better found actually case experiments error trials whereas miss trials φsi. practical measures note major differences among three measures. quantify. shown fig. measures causal interactions units across different time points. contrast also measure equal time interactions well causal interactions. best follows original concept sense measures causal interactions. needs acknowledge theoretical difference whenever applying measures order correctly interpret obtained results. difference computational costs. computational costs almost much larger requires multi-dimensional optimization. thus practical analysis large systems. case used instead care taken theoretical difference. although study focused bi-partitions queyranne’s algorithm extended higher-order partitions however algorithm becomes computationally demanding higher-order partitions computational complexity algorithm k-partitions main reason focused bi-partitions. another reason established fairly compare partitions different proposed integrated information normalized minimum entropy partitioned subsystems normalized note integrated information normalized always found bi-partitions integrated information becomes larger system partitioned subsystems. whether integrated information normalized integrated information normalized still open questions. study normalization used appropriate entropy negative continuous random variables. additionally regardless whether random variables continuous discrete normalization signiﬁcantly affects submodularity measures integrated information. example normalization proposed even submodular measure integrated information longer satisﬁes submodularity. thus queyranne’s algorithm work well normalized. although resolved major computational difﬁculties additional issue still remains. searching intermediate step identifying informational core called complex. complex subnetwork integrated information maximized hypothesized locus consciousness iit. identifying complex also represented discrete optimization problem requires exponentially large computational costs. queyranne’s algorithm cannot applied search complex cannot formulate important limitation study showed nearly perfect performance queyranne’s algorithm limited simulated data real neural data. general cannot tell whether queyranne’s algorithm works well data beforehand. real data analysis recommend procedure applied. first section accuracy checked comparing exhaustive search small randomly selected subsets. next works well performance checked comparing remcmc relatively large subsets section queyranne’s algorithm works better equally well remcmc reasonable queyranne’s algorithm analysis. applying procedure expect queyranne’s algorithm could utilized efﬁciently wide range time series data. acknowledgments thank shohei hidaka japan advanced institute science technology providing queyranne’s algorithm codes. work partially supported crest grant number jpmjcre japan. author contributions j.k. m.o. conceived designed experiments; j.k. performed experiments; j.k. m.o. analyzed data; j.k. r.k. m.o. wrote paper. conﬂicts interest authors declare conﬂict interest. founding sponsors role design study; collection analyses interpretation data; writing manuscript decision publish results. integrated information theory minimum information partition mcmc markov chain monte carlo remcmc replica exchange markov chain monte carlo ecog electrocorticogram autoregressive correct rate rank error ratio corr correlation monte carlo step describe analytical formula three measures integrated information multi information stochastic interaction geometric integrated information probability distribution gaussian. details theoretical background normalizing factor covariance matrix note assume mean gaussian distribution zero without loss generality mean value affect values integrated information. covariance matrix given obtain value need value requires solving eqs. thus calculation requires solving multi-dimensional equations. malab codes computation available replica exchange markov chain monte carlo method originally proposed investigate physical systems rapidly utilized applications including combinatorial optimization problems detailed history remcmc example ﬁrst brieﬂy explain search problem dealt metropolis method. then improvement metropolis method introduce remcmc effectively search global minimum avoiding trapped around local minimum. next describe convergence criterion mcmc sampling. finally present parameter settings experiments. iterating steps sufﬁcient time sample distribution converges probability distribution given steps sampling referred monte carlo step number elements. element attempted added removed average. depending value behavior sample sequence changes. small probability distribution given close uniform distribution subsets sampled nearly independently value large candidate likely accepted integrated information decreases. sample sequence easily falls local minimum cannot explore many subsets. thus smaller larger advantage disadvantage smaller better exploring around many subsets larger better ﬁnding minimum. metropolis method need appropriate value taking account trade-off generally difﬁcult. overcome difﬁculty setting inverse temperature remcmc samples distributions multiple values parallel sampled sequences exchanged nearby values exchange sampled sequences high inverse temperatures escape local minima explore many subsets. exchange neighboring inverse temperatures given number samples drawn subsets neighboring inverse temperatures swapped according following probability probability indicates integrated information higher inverse temperature larger lower inverse temperature subsets always swapped; otherwise swapped probability maximize efﬁciency remcmc important appropriately multiple inverse temperatures. neighboring temperatures apart acceptance ratio exchange becomes small. remcmc reduced separately simulating distributions previous study recommended different temperatures without exchange. keep average ratio higher every temperature pair. time highest/lowest inverse temperatures high/low enough sample sequence highest inverse temperature reach tips minima lowest search around many subsets. satisfy constraints sufﬁcient number inverse temperatures accommodated inverse temperatures optimized equalize average acceptance ratio exchanges temperature pairs details temperature setting described below. inverse temperatures initially follows. first subset randomly selected then randomly chosen element added eliminated subset absolute value change amount integrated information taken. using absolute values highest lowest inverse temperatures determined bisection method respective averages acceptance ratio match predeﬁned values. intermediate inverse temperatures geometric progression difference amount integrated information candidate subset current subset stored difference positive then using stored values inverse temperatures highest lowest inverse temperatures determined bisection method average acceptance ratio inverse temperatures approximately equalize expected values acceptance ratio exchange temperature pairs expected value represented probabilities mean variance represented functions temperature functions given interpolating sample mean variance. study functions estimated using regression sample mean variance highly variable. mean variance temperature computed every update means variances regressed temperature using continuous piecewise linear function t-axis anchor points current temperatures. anchor points interpolated using piecewise cubic hermite interpolating polynomials. then roughly equalize expected values acceptance ratio exchange temperature pairs minimize following cost function varying temperatures commonly used mcmc convergence criteria potential scale reduction factor proposed gelman rubin modiﬁed brooks gelman criterion multiple mcmc sequences run. converge statistics sequences must same. assessed comparing between-sequence variance within-sequence variance random variable calculating psrf ˆrc. large suggests sequences converge yet. close diagnose converged. study sequence inverse temperature former latter halves applied criterion half sequences. temperatures predeﬁned threshold regarded sequences converged. number inverse temperatures ﬁxed throughout experiments. highest/lowest inverse temperatures averages acceptance ratio become respectively. exchange process done every mcss. update inverse temperatures performed every mcss initial mcss. threshold computing discarded ﬁrst mcss burn-in period started computing mcss. show examples distributions values experiments subsection figures plots block-structured models respectively. fig. computed partition found queyranne’s algorithm perfectly matched mips. fig. computed partition found queyeranne’s algorithm match mips trials deviations small. figure values block-structured models plots represent distribution partitions. solid line indicates mip. green circles indicate partitions found queyranne’s algorithm.", "year": "2017"}