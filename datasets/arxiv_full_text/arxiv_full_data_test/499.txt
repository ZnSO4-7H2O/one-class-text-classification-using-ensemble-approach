{"title": "A deep generative model for gene expression profiles from single-cell  RNA sequencing", "tag": "q-bio", "abstract": " We propose a probabilistic model for interpreting gene expression levels that are observed through single-cell RNA sequencing. In the model, each cell has a low-dimensional latent representation. Additional latent variables account for technical effects that may erroneously set some observations of gene expression levels to zero. Conditional distributions are specified by neural networks, giving the proposed model enough flexibility to fit the data well. We use variational inference and stochastic optimization to approximate the posterior distribution. The inference procedure scales to over one million cells, whereas competing algorithms do not. Even for smaller datasets, for several tasks, the proposed procedure outperforms state-of-the-art methods like ZIFA and ZINB-WaVE. We also extend our framework to account for batch effects and other confounding factors, and propose a Bayesian hypothesis test for differential expression that outperforms DESeq2. ", "text": "single-cell sequencing revolutionary technology allows studying fundamental biological questions previously reach allows ﬁrst time reveal cell’s identity characterize molecular circuitry unbiased data-driven way. product scrna-seq experiment data matrix entry approximates number transcripts gene cell careful computational analysis allows deriving data exciting insights diverse biomedical ﬁelds typical observe thousands gene products cell many transcripts observed infrequently technical reasons related method sequencing particularly prone high variance. additionally limited transcript capture efﬁciency inherent rna-seq protocols entries typically zero-inﬂated often little prior knowledge single-cell heterogeneity generating reasonably general assumption generated low-dimensional manifold cellular states therefore numerous dimensionality reduction techniques proposed interpreting technique shortcomings however. based linear models data though basis assuming linearity. optimized batch algorithms preventing scaling beyond thousands cells however sequencing millions cells becoming possible best performing method date particularly complicated train involving numerous subroutines alternating minimization. recent articles apply neural networks without architecture based biology propose single-cell variational inference probabilistic inference procedure based fully generative model. scvi conditional distributions speciﬁed neural networks encode complex nonlinear relationships learned large datasets. scvi explicitly models technical effects disentangle low-dimensional vector represents cells’ underlying states. describe generative model inference procedure compare scvi alternative methods three benchmarks tasks finally introduce bayesian hypothesis testing procedure leverages scvi differentially expressed genes source code based tensorflow publicly available https//github.com/ yoseflab/scvi. accounts stochasticity gene expressed cell constant optional covariates passed account batch effects like normalization remove unwanted variation latent representation. latent variable conditional distribution zero-inﬂated negative binomial— distribution known effectively model kinetics stochastic gene expression entries replaced zeros neural networks dropout regularization batch normalization. network fully connected-layers nodes each. activation functions relu exponential linear. weights layers shared posterior distribution combines prior knowledge information acquired data cannot directly apply bayes rule determine posterior denominator intractable. instead variational inference approximate posterior variational distribution gaussian diagonal covariance matrix. variational distribution’s mean covariance given encoder network applied encoder network optionally given constant covariates wish discourage encoding batch effects unwanted variations. variational lower bound optimize variational lower bound write analytically marginalizing discrete random variables yng. variational lower bound continuous end-to-end differentiable. maximize variational lower bound using stochastic backpropagation. assess performance scvi three benchmarks tasks generalizing heldout data imputating zeroed data recovering known clusters throughout compare scvi factor analysis well state-of-the-art methods zifa zinb-wave scvi factor analysis scale larger benchmark datasets—a advantage relative zifa zinb-wave. zifa zinb-wave based batch optimization algorithms. runtimes iteration numerical optimization routines scale linearly number samples linearly number genes—both potentially large. cells methods requires minutes computation. cells methods memory machine ram. scvi trains entire million cell dataset less hours single using off-the-shelf neural network software. task dataset contains million brain cells genomics sampled variable genes. method learn mapping -dimensional latent space reconstruction training then assess marginal likelihood held-out data conditioned latent representation learned held-out data model. table shows scvi best compresses held-out data even smallest dataset. scvi’s lead methods grows dataset size grows. table absolute errors imputing zeroed entries mean cross entropy predicting entries zeroed-out entries scores based dataset brain cells. magic predict dropout probabilities. sample dataset zero entries random conditioned expected transcript abundance according probabilities zifa model mimic technical effects zero entries real data. introduced zeros synthetically know entry’s true value entry zero technical effect true expression level nearly zero. also compare task state-of-the-art method magic based diffusion cell k-nearest neighbors graph report results table assess models compare clusters cells known types latent space. task make slight modiﬁcation model treat unknown parameter estimate rather latent variable distribution. procedure maximizes mutual information ﬁrst dataset contains mouse cortex cells gold-standard labels seven distinct cell types. cell type corresponds cluster recover. sample variable genes report silhouette mouse cortex dataset table second dataset contains peripheral blood mononuclear cells sampled genes biologically meaningful clusters software seurat dataset scone select important factors unwanted variation incorporated downstream models. factors generally include batches meta-data sequencing depth quality controls cell. case scone selected strategy consists scaling depth regressing compare scvi without covariates without normalization table show better remove variation yielding high silhouette score means would consistent tighter clustering latent space. signiﬁcant application generative model main interest ﬁeld clustering procedure identifying gene differentially expressed cell-types. model relies bayesian statistics thus beneﬁt uncertainty evaluation provide hypothesis testing framework differential expression. cells ﬁxed gene. take want test following gamma latent variable generative model mean gene expression conditioned non-dropout event. posterior hypotheses approximated variational distribution measures uni-dimensional low-dimensional naive monte-carlo compute integrals. bayes factor test. pbmc dataset seurat-based cell classiﬁcation understand differential expression captured testing method compared tradition deseq deﬁned reference publicly-available bulk array expression proﬁling data human cells dendritic cells baseline vaccination test association gene’s expression biological class deﬁning -sided t-test p-value gene. deﬁning threshold curve ambiguous prefer look reproducibility microarray experiment family tests used scrna-seq sequencing experiment. quantify this model relationship signiﬁcance ranks using irreproducible discovery rate model matched rank lists report correlation score reproducible components figure", "year": "2017"}