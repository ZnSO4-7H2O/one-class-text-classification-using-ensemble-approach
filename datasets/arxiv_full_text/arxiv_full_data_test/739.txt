{"title": "Dendritic error backpropagation in deep cortical microcircuits", "tag": "q-bio", "abstract": " Animal behaviour depends on learning to associate sensory stimuli with the desired motor command. Understanding how the brain orchestrates the necessary synaptic modifications across different brain areas has remained a longstanding puzzle. Here, we introduce a multi-area neuronal network model in which synaptic plasticity continuously adapts the network towards a global desired output. In this model synaptic learning is driven by a local dendritic prediction error that arises from a failure to predict the top-down input given the bottom-up activities. Such errors occur at apical dendrites of pyramidal neurons where both long-range excitatory feedback and local inhibitory predictions are integrated. When local inhibition fails to match excitatory feedback an error occurs which triggers plasticity at bottom-up synapses at basal dendrites of the same pyramidal neurons. We demonstrate the learning capabilities of the model in a number of tasks and show that it approximates the classical error backpropagation algorithm. Finally, complementing this cortical circuit with a disinhibitory mechanism enables attention-like stimulus denoising and generation. Our framework makes several experimental predictions on the function of dendritic integration and cortical microcircuits, is consistent with recent observations of cross-area learning, and suggests a biological implementation of deep learning. ", "text": "animal behaviour depends learning associate sensory stimuli desired motor command. understanding brain orchestrates necessary synaptic modiﬁcations across different brain areas remained longstanding puzzle. here introduce multi-area neuronal network model synaptic plasticity continuously adapts network towards global desired output. model synaptic learning driven local dendritic prediction error arises failure predict top-down input given bottom-up activities. errors occur apical dendrites pyramidal neurons long-range excitatory feedback local inhibitory predictions integrated. local inhibition fails match excitatory feedback error occurs triggers plasticity bottom-up synapses basal dendrites pyramidal neurons. demonstrate learning capabilities model number tasks show approximates classical error backpropagation algorithm. finally complementing cortical circuit disinhibitory mechanism enables attention-like stimulus denoising generation. framework makes several experimental predictions function dendritic integration cortical microcircuits consistent recent observations cross-area learning suggests biological implementation deep learning. figure dendritic cortical circuit learns predict self-generated top-down input. illustration multi-area network architecture. network consists input area intermediate areas output area hidden area consists microcircuit pyramidal cells lateral inhibitory interneurons pyramidal cells consist three compartments basal compartment top-down input converges somatic compartment integrates basal apical voltage. interneurons receive input lateral pyramidal cells onto basal dendrites same-area pyramidal cells. pre-learning developmental stage network learns predict cancel top-down feedback given randomly generated inputs. pyramidal-to-interneuron synapses changed stage according predictive synaptic plasticity rules example voltage traces randomly chosen downstream neuron pyramidal cell apical compartment development three consecutively presented input patterns. learning lateral synapses onto interneurons converged self-generated top-down signals predicted network self-predicting state. concrete network hidden area pyramidal neurons particular network dimensions impact ability network produce results. note desired targets presented output area network solely driven random inputs. lateral inhibition cancels top-down input. interneurons learn match next-area pyramidal neuron activity input weights adapt concurrently learning interneuron-to-pyramidal synapses silences apical compartment pyramidal neurons pyramidal neurons remain active general effect lateral microcircuit learns predict cancel expected top-down input every random pattern figure deviations self-predictions encode backpropagating errors used learning bottom-up synapses. novel associative signal presented output area prediction error apical compartments pyramidal neurons upstream area generated. error appears apical voltage deﬂection propagates soma modulates somatic ﬁring rate. bottom-up synapses basal dendrites learn predict somatic ﬁring rate elements directly involved encoding error modifying bottom-up synapses highlighted microcircuit. activity traces microcircuit associative signal learned. learning associative signal presented output area somatic voltage output neurons changes accordingly originates mismatch top-down feedback cancellation given lateral interneurons learning associative signal plasticity bottom-up synapses leads near-exact prediction previously unexplained associative signal uassoc consequently distal dendrite longer shows voltage deﬂection results top-down lateral cancellation inputs magnitude opposite signs network fully reproduces associative signal learning gradually explains away backpropagated activity. interneurons learn predict effectively cancel backpropagated activity lateral weights pyramidal-to-interneurons apical compartment eventually silenced even though pyramidal neurons remain active vertical blue dashed line represents moment associative signal presented ﬁrst time. figure multi-area network learns solve nonlinear associative task online continuous time without phases. starting self-predicting network state fully-connected pyramidal neuron network learns approximate nonlinear function stream input-output pattern pairs. neuronal synaptic weight dynamics evolve continuously without interruption. example ﬁring rates randomly chosen output neuron desired target imposed associative input together voltage apical compartment hidden neuron input rate sensory neuron learning apical dendrite hidden neuron shows errors response three consecutive input patterns disappear successful learning presentation novel output target produces deviations baseline apical compartment visible initial eventually leads reduction error output area henceforth return baseline apical voltage hidden area below. error curves full model shallow learner comparison backpropagation errors occurs output weights adapted. figure learning classify real-world structured stimuli multi-area network. network pyramidal neurons learns recognize classify handwritten digits mnist data set. subset connections shown enhance clarity. competitive accuracy achieved standard mnist testing dataset network comparison performance shallow learner standard artiﬁcial neural network trained backprop also shown. figure top-down synapses adapted simultaneously drive bottom-up learning input construction denoising. classiﬁcation performance network exposed mnist images plastic top-down synapses learns predict lower-area activities. top-down forward weights co-adapt without pauses phases. driving network topto-bottom recreates class-speciﬁc image examples input area. top-down connections tuned encode simple inverse visual model. inverse model yields image denoising achieved reconstructing corrupted inputs hidden area activities. network also successfully learns classify images. inverse reconstruction losses original images hidden neuron activities. top-down synapses connecting hidden pyramidal neurons back input area learn reconstruct pixel arrangements given hidden neuron activities; synapses originating output area learn predict hidden area activities given current class label estimate. example consists randomly drawn input pattern teacher network weights input pattern entries sampled uniform distribution choose soft rectifying nonlinearity neuronal transfer function simpliﬁed dynamics approximates full recurrent network relaxation deterministic setting approximation improving top-down dendritic coupling decreased", "year": "2017"}