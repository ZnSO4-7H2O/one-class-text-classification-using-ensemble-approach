{"title": "Chaos-guided Input Structuring for Improved Learning in Recurrent Neural  Networks", "tag": "q-bio", "abstract": " Anatomical studies demonstrate that brain reformats input information to generate reliable responses for performing computations. However, it remains unclear how neural circuits encode complex spatio-temporal patterns. We show that neural dynamics are strongly influenced by the phase alignment between the input and the spontaneous chaotic activity. Input structuring along the dominant chaotic projections causes the chaotic trajectories to become stable channels (or attractors), hence, improving the computational capability of a recurrent network. Using mean field analysis, we derive the impact of input structuring on the overall stability of attractors formed. Our results indicate that input alignment determines the extent of intrinsic noise suppression and hence, alters the attractor state stability, thereby controlling the network's inference ability. ", "text": "anatomical studies demonstrate brain reformats input information generate reliable responses performing computations. however remains unclear neural circuits encode complex spatio-temporal patterns. show neural dynamics strongly inﬂuenced phase alignment input spontaneous chaotic activity. input structuring along dominant chaotic projections causes chaotic trajectories become stable channels hence improving computational capability recurrent network. using mean ﬁeld analysis derive impact input structuring overall stability attractors formed. results indicate input alignment determines extent intrinsic noise suppression hence alters attractor state stability thereby controlling network’s inference ability. introduction brain actively untangles input sensory data behaviorally relevant dimensions enables organism perform recognition effortlessly spite variations–. instance visual data object translation rotation lighting changes forth cause complex nonlinear changes original input space. however brain still extracts high-level behaviorally relevant constructs varying input conditions recognizes objects accurately. remains unknown brain accomplishes untangling. here introduce concept chaos-guided input structuring reservoir computing network provides avenue untangle stimuli input space improve ability stimulus entrain neural dynamics. speciﬁcally show complex dynamics arising recurrent structure randomly connected reservoir– used extract explicit phase relationship input stimulus spontaneous chaotic neuronal response. then aligning input phase along dominant projections determining intrinsic chaotic activity causes random chaotic ﬂuctuations trajectories network become locally stable channels dynamic attractor states that turn improve its’ inference capability. fact using mean ﬁeld analysis derive effect introducing varying phase association input network’s spontaneous activity. results demonstrate successful formation stable attractors strongly determined input alignment. model description describe effect chaos guided input structuring standard ﬁring-rate based reservoir model interconnected neurons. speciﬁcally neuron network described activation variable ...n satisfying represents ﬁring rate neuron characterized nonlinear response function tanh neuron time constant. represents sparse recurrent weight matrix chosen randomly independently gaussian distribution mean variance g/pcn synaptic gain parameter connection probability units. output unit reads activity network connectivity matrix wout initial values drawn gaussian distribution mean variance readout weights trained using recursive least square algorithm–. input weight matrix winput drawn gaussian distribution zero mean unit variance. external input oscillatory sinusoidal signal icos amplitude frequency unit here phase factor chosen randomly independently uniform distribution ensures spatial pattern input correlated recurrent connectivity initially. input alignment analysis figure cartoon depicting angle subspace deﬁned ﬁrst chaotic activity input driven activity projections reservoir activity space vectors trajectories reservoir neurons across different trials driven input rotated θrotate. relationship input temporal phase orientation driven activity respect chaotic subspace varying input amplitude frequency. reservoir framework stimulated brief sinusoidal input trained generate timed output response shown cartoon showing different angles chaotic subspace along inputs aligned. euclidean distance trajectories inputs plotted different orientation chaotic subspace. projections reservoir activity space vectors corresponding inputs different alignment conditions. subspace alignment first question subspace input driven activity aligned respect subspace spontaneous activity recurrent network. using principal component analysis observed input-driven trajectory converges uniform shape becoming circular increasing input amplitude. utilize concept principal angles introduced visualize relationship chaotic input driven subspace. speciﬁcally subspaces dimension deﬁned unit principal component vectors ...d fig. schematically represents angle circular input driven network activity irregular spontaneous chaotic activity. here θchaos refers subspace deﬁned ﬁrst principal components intrinsic chaotic activity evident rotating circular orbit θrotate align along chaotic trajectory projection. observe aligning inputs directions account maximal variance chaotic spontaneous activity facilitates intrinsic noise suppression relatively input amplitudes thereby allowing network produce stable trajectories. instance instead using random phase input icos visualize network activity shown fig. even lower amplitude observe uniform circular orbit network activity characteristic reduction intrinsic noise input sensitization. fact even input turned neural units yield stable synchronized trajectories minimal variation across different trials comparison random phase input driven network shows effectiveness subspace alignment intrinsic noise suppression. addition working input-amplitude regimes offers additional advantage higher network dimensionality turn improves overall discriminative ability network. note previous work shown spatial structure analysis input phase corresponds subspace rotation driven activity toward spontaneous chaotic activity. observe temporal phase input contributes neuronal activity recurrent network. fig. illustrates correlation wherein input phase determines orientation input-driven circular orbit respect dominant subspace intrinsic chaotic activity. given input frequency input phase aligns driven activity along chaotic activity resulting θrotate varying input amplitude interesting observation frequency input modiﬁes orientation evoked response yields different input phases θchaos θdriven aligned also observe subspace alignment extremely sensitive toward input phase certain regions abrupt jumps non-smooth correlation. non-linear behavior consequence recurrent connectivity overall shapes complex interaction driving input intrinsic dynamics. correlation yields several important implications network modeling experiments utilize behavior subspace alignment. consequently experiments given θrotate corresponding input phase approximately aligns input preferred direction. impact input structuring discriminative capability next describe implication input alignment along chaotic projections overall learning ability network. first trained recurrent network output units generate timed response shown fig. distinct brief sinusoidal inputs used stimulate recurrent network. network trajectories produced mapped output units using training here network expected produce timed output dynamics readout unit response input respectively. network reliable generates consistent response readout units across repeated presentations inputs testing across different trials. simple experiment utilizes fact neural dynamics recurrent network implicitly encode timing fundamental processing generation complex spatio-temporal patterns. note cases multiple inputs values inputs zero except timing window input brieﬂy turned given trial. since inputs experiment amplitude frequency dynamics circular orbit describing network activity input-driven state almost similar giving rise principal angle input subspace. discriminate output responses inputs apparent inputs aligned different directions. obvious choice align input along different principal angles deﬁning chaotic spontaneous activity note ∠pc.pc denotes angle calculated using eqn. another approach align along ∠pc.pc θchaos along ∠pc.pc θchaos◦ shown fig. analyze latter detail involves input phase rotation subspace makes easier formal theoretical analysis. characterize discriminative performance network evaluated euclidean distances ﬁring rate activity network corresponding inter-/intra-input trajectories response different inputs slightly varied version input random number input phase aligns along θchaos inter-/intra-input trajectory distances plotted fig. scenarios-with without input alignment. desirable larger inter-trajectory distance small intra-trajectory distance network easily distinguishes inputs able reproduce required output response even particular input slightly perturbed. observe aligning inputs direction parallel perpendicular dominant projections increases inter-trajectory distance compared non-aligned case decreasing intra-input trajectory separation. ascertains fact subspace alignment reduces intrinsic ﬂuctuations within network thereby enhancing prediction capability. note without input alignment intrinsic ﬂuctuations cannot overcome low-amplitude inputs hence fair comparison obtain stable readout-trainable trajectory non-aligned case higher input amplitude hypothesize intrinsic noise suppression occurs input subspace alignment along dominant projections causes chaotic trajectories along different directions become locally stable channels attractor states. attractors behave potential wells toward network activity converges different inputs. thus successful formation stable distinctive attractors different inputs strongly inﬂuenced orientation along inputs aligned. consequence hypothesis depending upon orientation input respect dominant chaotic activity extent noise suppression vary particular trajectory eventually alter stability attractor states. test this rotated monitored intra-trajectory distance. note anti-phase correlated chaotic subspace. fig. corresponding phase difference corresponds stable attractor since intra-distance former lower. contrast fig. corresponding phase difference turns stable note phase difference refers phase difference inputs chaotic subspace subspace alignment using analysis yields phase chaotic subspace yields phase. addition trajectory distance visualizing network activity space also shows inﬂuence input orientation toward formation distinct attractor states. since aligned subspace deﬁned ∠pc.pc projection circular orbit onto input aligned scenarios comparable. however third dimension marks difference input projections. fact progress network activity time evolves follows completely different cycle input aligned scenarios. change overall rotation cycle anti-clockwise clockwise viewed indication toward altering attractor state stability. hand non-aligned case yields incoherent random trajectory representative intrinsic noise. order coherent activity suppress noise further need increase input amplitude shown mean field analysis explain results analytically mean-ﬁeld methods developed evaluate properties random network models limit quantity mean field theory average autocorrelation function characterizes interaction within network denotes time average. main idea replace network interaction term eqn. gaussian noise incorporates averaged temporal phase relationship reservoir neurons input induced input subspace alignment temporal correlation calculated self-consistently self-consistence ﬁrst second moment must match moments network interaction term. thus mean recurrent synaptic matrix calculating second moment identity jwkl jδkl/n obtain combining result noise-interaction based network equation yields eqn. resembles newtonian motion equation classical particle moving inﬂuence force given right hand side equation. force depends that turn depends input subspace alignment directs initial position particle analogy evident analyzing overall potential energy function particle equivalent visualizing different attractor states formed network response particular input stimulus. thus formulated expression correlation function using taylor series expansion allows derive force hence dynamics network various input alignment conditions. non-linear ﬁring rate function tanh expanded taylor series small values i.e. denotes small increment beyond note satisﬁes criterion operate networks chaotic regime. also overall network statistics change expressed gain factor ﬁring-rate function instead overall synaptic strength. using tanh express eqn. /gacos express eqn. gacos/ parameter deﬁned terms note supplementary section provides detailed derivation eqn. comments assumptions initial conditions. note eqn. approximate version eqn. depicts network activity manner newtonian motion independent intrinsic time taking account inﬂuence input alignment. express potential network driven force equivalent right hand side eqn. solve eqn. initial conditions monitor change force potential different values first examine attractor state formation input stimulus visualizing potential expressions force potential become fig. shows evolution potential energy varies different since external input network dynamics chaotic results formation potential wells equally stable. network activity thus converge wells depending upon initial state starting point. supports observation network input yields chaotic activity incoherent irregular trajectory every trial. nonzero force equation dependent since cos. different values solved numerically plotted potential evolution shown fig. potential well attractive left end. validates fact intrinsic ﬂuctuations suppressed presence input. left attractor becomes stable. changing shows potential well right becomes stable. result conﬁrms input subspace alignment respect initial chaotic state inﬂuences overall stability convergence capability recurrent network. fact stability corresponding different attractor states arises qualiﬁes earlier hypothesis input orientation respect chaotic subspace alters attractor state stability corroborating result fig. finally illustrate effectiveness input alignment complex motor pattern generation task reliable generation learnt handwritten patterns multiple trials even presence perturbations. detailed analysis results shown supplementary. note solved eqn. setting initial boundary value conditions iterating different reached steady state solution. changing conditions result completely values nevertheless observe similar evolution potential well change attractor state stability fig. furthermore calculations denote functional relationship subspace alignment input phase eventually affects attractor state stability. future examine real-time evaluation its’ impact analytical studies. finally constraint derive potential energy functions show altering attractor state expect results valid large well since eqn. still remains unchanged. conclusion models cortical networks often diverse plasticity mechanisms effective tuning recurrent connections suppress intrinsic chaos show input alignment alone produces stable repeatable trajectories even presence variable internal neuronal dynamics dynamical computations. combining input alignment recurrent synaptic plasticity mechanism enable learning stable correlated network activity output zero input projections chaotic spontaneous activity onto vectors visualization chaotic trajectory subspace composed dominant vectors account signiﬁcant variance network activity trajectories reservoir neurons across different trials. panel non-chaotic input driven activity input amplitude effective dimensionality network different input amplitudes g=.. layer) resistant external perturbation large extent. furthermore since input subspace alignment allows operate networks amplitude maintaining stable network activity provides additional advantage higher dimensionality. network higher dimensionality offers larger number disassociated principal chaotic projections along different inputs aligned. thus classiﬁcation task wherein network discriminate different inputs notion untangling chaos-guided input structuring thus serve foundation building robust recurrent networks improved inference ability. investigation required examine orientations speciﬁcally improve discrimination capability network impact given alignment stability readout dynamics around output target. summary analyses present suggest input alignment chaotic subspace large impact network dynamics eventually determines stability attractor state. fact control network’s convergence toward different stable attractor channels voyage neural state space regulating input orientation. indicates that besides synaptic strength variance critical quantity might modiﬁed modulatory plasticity mechanisms controlling neural circuit dynamics input stimulus alignment. acknowledgments p.p. k.r. supported part center brain-inspired computing sponsored center semiconductor research corporation intel corporation vannevar bush fellowship. supplementary material examining network activity examine structure recurrent network’s representations visualize compare neural trajectories response varying inputs using principal component analysis network state given time instant described point n-dimensional space coordinates corresponding ﬁring rates neuronal units. time network activity traverses trajectory n-dimensional space outline subspace trajectory lies. conduct diagonalize equal-time cross-correlation matrix ﬁring rates units angle brackets denote time average denotes ﬁring rate activity neuron. eigenvalues matrix indicate contribution different principal components toward ﬂuctuations/total variance spontaneous activity network. fig. shows impact varying input amplitude spontaneous chaotic activity network. network completely chaotic evident highly variable projections network activity onto different principal components shown fig. .generally leading account network’s chaotic activity. visualizing network activity space composed dominant principal components shows random irregular trajectory characteristic chaos fact plotting trajectories neuron time evolves) recurrent units network shows diverging incoherent activity across different trials also representative intrinsic chaos. addition projections network activity onto components smaller variances ﬂuctuate rapidly irregularly corroborates fact leading deﬁne network’s spontaneous chaotic activity. driving recurrent network sinusoidal input high amplitude sensitizes network toward input thereby suppressing intrinsic chaotic ﬂuctuations. projections network activity relatively periodic. noteworthy observation trajectories recurrent units become stable consistent across different presentations input pattern increasing amplitude. readout layer appended recurrent network easily trained stable trajectories particular task. thus input amplitude determines network’s encoding trajectories turn its’ inference ability. fact chaotic intrinsic activity completely suppressed larger inputs. however preferred input dominance drastically declines discriminative ability network justiﬁed dimensionality measurements. effective dimensionality reservoir calculated provides measure effective number describing network’s activity given input stimulus condition. fig. illustrates effective dimensionality decreases increasing input amplitude different values. hence critical input drive strong enough inﬂuence network activity overriding intrinsic chaotic dynamics enable network operate edge chaos. note higher fig. yields larger dimensionality richer chaotic activity. simulations fig. input shown starting thus observe trajectories recurrent units chaotic input turned although network returns spontaneous chaotic ﬂuctuations input turned observe network trajectories stable non-chaotic coherence previous ﬁndings from visualization network activity dominant space input-driven trajectory converges uniform shape becoming circular higher input amplitude informs orbit describing network activity input-driven state consists circle two-dimensional subspace full n-dimensional hyperspace neuronal activities. note simulations supplementary conducted similar parameters mentioned manuscript i.e. averaging eqn. still retain term unlike general calculations gets washed since ﬁxed relationship input recurrent activity. here another approximation i.e. cos.cos ﬁnally eqn. force equation derived potential energy. note approximate equations derived respect single input driving entire network. multiple inputs corresponding alignment inputs along different projections result potential well roughly interpreted linear combination observed input gin. linear combination follow similar evolution proﬁle shown fig. main manuscript. thus change intra-trajectory distance varying alignment inputs chaotic subspace shown fig. main manuscript justiﬁed given analysis. figure handwriting patterns generated across test trials response absence external noise variation performance different noise amplitude shown output pattern different input alignment scenarios. chaos represents error value obtained chaos generation phase difference chaotic subspace respectively. handwriting generation elucidate effectiveness input alignment complex pattern generation trained recurrent network generate handwritten words chaos neuron response different inputs obtaining principal angle chaotic spontaneous activity aligned input corresponding chaos along ∠pc.pc using optimal input phase then monitored output activity different orientation input corresponding neuron respect chaotic subspace. output units trained using trace original target locations handwritten patterns time instant. fig. shows handwritten patterns generated network across test trials scenario inputs aligned chaotic subspace. observe similar robust patterns generated phase well. notable feature input alignment chaotic trajectories become locally stable channels function dynamic attractor states. visualized stable synchronized trajectories observed different neural units fig. main manuscript however external perturbation induce chaos reservoir overwhelm stable patterns activity. test susceptibility dynamic attractor states formed input structuring external perturbation introduced random gaussian noise onto trained model along standard intrinsic chaos-aligned inputs testing. injection noise alters external current received neuronal units noise amplitude denotes neural unit reservoir rand random gaussian distribution). fig. shows mean squared error actual output produced different time instants averaged across test trials) network varying levels noise. increases observe steady increase error value implying degradation prediction capability network. however moderate noise network exhibits high robustness negligible degradation prediction capability words. interestingly phase difference neuron stable chaos increased reproducibility across different trials even noise contrast phase chaos less sensitive noise hand phase alignment chaotic subspace observe network sensitive even toward slight perturbation implies attractor states formed case unstable. corroborates fact extent noise suppression hence attractor state stability varies based upon input alignment. fig. shows handwritten pattern generated test trial different phase alignment aligned along principal angle deﬁning spontaneous chaotic activity network. noteworthy mention neural trajectories recurrent units corresponding cases stable. fact observe case trajectories neurons responding corresponds output chaos become slightly divergent incoherent beyond contrast trajectories units responding word neuron synergized coherent throughout time period simulation. indicates network activity neuron converges stable attractor state chaos. result network robust reproducing neuron even presence external perturbation phase difference case exactly opposite stability phenomena chaos converging stable attractor. figure generation handwriting patterns corresponding brief sinusoidal inputs different phase difference inputs chaotic subspace. shows patterns generated absence external perturbation test trial shows trajectories recurrent units reservoir across test trials corresponding output pattern shows patterns generated presence external perturbation test trial. references dicarlo zoccolan rust brain solve visual object recognition? neuron thorpe fize marlot speed processing human visual system. nature dicarlo untangling invariant object recognition. trends cognitive sciences rajan abbott eigenvalue spectra random matrices neural networks. physical review letters kadmon sompolinsky transition chaos random neuronal networks. physical review stern sompolinsky abbott dynamics random neural networks bistable units. physical review", "year": "2017"}