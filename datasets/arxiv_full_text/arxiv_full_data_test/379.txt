{"title": "Representation Learning using Event-based STDP", "tag": "q-bio", "abstract": " Although representation learning methods developed within the framework of traditional neural networks are relatively mature, developing a spiking representation model remains a challenging problem. This paper proposes an event-based method to train a feedforward spiking neural network (SNN) layer for extracting visual features. The method introduces a novel spike-timing-dependent plasticity (STDP) learning rule and a threshold adjustment rule both derived from a vector quantization-like objective function subject to a sparsity constraint. The STDP rule is obtained by the gradient of a vector quantization criterion that is converted to spike-based, spatio-temporally local update rules in a spiking network of leaky, integrate-and-fire (LIF) neurons. Independence and sparsity of the model are achieved by the threshold adjustment rule and by a softmax function implementing inhibition in the representation layer consisting of WTA-thresholded spiking neurons. Together, these mechanisms implement a form of spike-based, competitive learning. Two sets of experiments are performed on the MNIST and natural image datasets. The results demonstrate a sparse spiking visual representation model with low reconstruction loss comparable with state-of-the-art visual coding approaches, yet our rule is local in both time and space, thus biologically plausible and hardware friendly. ", "text": "although representation learning methods developed within framework traditional neural networks relatively mature developing spiking representation model remains challenging problem. paper proposes event-based method train feedforward spiking neural network layer extracting visual features. method introduces novel spike-timingdependent plasticity learning rule threshold adjustment rule derived vector quantization-like objective function subject sparsity constraint. stdp rule obtained gradient vector quantization criterion converted spike-based spatio-temporally local update rules spiking network leaky integrate-and-ﬁre neurons. independence sparsity model achieved threshold adjustment rule softmax function implementing inhibition representation layer consisting wtathresholded spiking neurons. together mechanisms implement form spike-based competitive learning. sets experiments performed mnist natural image datasets. results demonstrate sparse spiking visual representation model reconstruction loss comparable state-of-the-art visual coding approaches rule local time space thus biologically plausible hardware friendly. unsupervised learning approaches using neural networks frequently used extract features visual inputs single layer networks using distributed representations autoencoder networks oﬀered eﬀective representation platforms. however robust high level eﬃcient representation obtained networks brain still fully understood understanding brain’s functionality representation learning accomplished studying spike activity bio-inspired spiking neural networks snns provide biologically plausible architecture high computational power eﬃcient neural implementation main challenge develop spiking representation learning model encodes input spike trains uncorrelated sparse output spike trains using spatio-temporally local learning rules. study seek develop representation learning network spiking neurons address challenge. contribution determines novel spatio-temporally local learning rules embedded single layer code independent features visual stimuli received spike trains. synaptic weights proposed model adjusted based novel spike-timingdependent plasticity rule achieves spatio-temporal locality. nonlinear hebbian learning played role development uniﬁed unsupervised learning approach represent receptive ﬁelds f¨oldi´ak inﬂuenced barlow early designers sparse weakly distributed representations redundancy. f¨oldi´ak’s model introduced three learning rules work concert achieve representations. zylberberg showed f¨oldi´ak’s plasticity rules spiking platform could derived constraints reconstructive accuracy sparsity decorrelation. furthermore acquired receptive ﬁelds representation cells model qualitatively matched primate visual cortex. representation kernels determining synaptic weight sets successfully utilized recent study spiking convolutional neural network extract primary visual features mnist dataset. additionally learning rules used information locally available relevant synapse. although sailnet utilized spiking neurons representation layer plasticity rules spatially local learning rules temporally local. sailnet plasticity rules spike counts accumulated duration stimulus presentation interval. since sailnet rules spike times question training spiking representation network using spatio-temporally local spikebased approach like spike-timing-dependent plasticity needs neural spike times remains unresolved. later work extends excitatory inhibitory neurons learning rules still temporal windows varying duration estimate spike rates rather timing spike events. work seeks develop learning rule matches performance remains local time space. another line research based cost functions olshausen field bell sejnowski showed constraints reconstructive ﬁdelity sparseness applied natural images could account many qualitative receptive ﬁeld properties primary visual cortex works agnostic possible learning mechanisms used visual cortex achieve representations. following rehn sommer developed sparse-set coding network minimizes number active neurons instead average activity measure. later olshausen introduced l-norm minimization criterion embedded highly overcomplete neural framework. although models oﬀer great insight might computed receptive ﬁelds acquired oﬀer insight details learning rules used achieve representations. early works proposed learning mechanism explain emergence orientation selectivity visual cortex malsburg bienenstock state-of-the-art model masquelier model blends strong biological detail signal processing analysis simulation establish proof-of-concept demonstration original hubel wiesel feedforward model orientation selectivity. feature model relevant present paper stdp account acquisition. stdp popular learning rule snns synaptic weights adapted according relative prepostsynaptic spike times diﬀerent variations stdp shown successful visual feature extraction layer-wise training snns similar vein burbank also proposed stdp-based autoencoder. autoencoder uses mirrored pair hebbian anti-hebbian stdp rules. goal account emergence symmetric physically separate connections encoding weights decoding weights another component playing role representing uncorrelated visual features bioinspired pertains inhibition circuits embedded within layer. instance savin developed independent component analysis computation within using stdp synaptic scaling independent neural activities representation layer controlled lateral inhibition. lateral inhibition established winner-take-all neural circuit maintain independence sparsity neural representation layer. recent work combined layer unsupervised stdp explicit layer non-learning inhibitory neurons. inhibitory neurons impose discipline. representations tested handwritten mnist dataset shown eﬀective recognition digits. acquired representations tended resemble mnist prototypes although reconstructive properties directly studied. also studied spiking network stochastic neurons performs mnist classiﬁcation acquires mnist prototype representations. architecture -layer network hidden layer uses soft implement inhibition. since functional need introduce explicit inhibitory layer learning work uses softmax function achieve inhibition. work standard softmax adapted spiking network. acquired representations trained mnist dataset acquires representations resembling v-like receptive ﬁelds contrast mnist prototypes research described above. works related spike-based clustering vector quantization evolving snns acquire representations recruitment learning paradigm neurons recruited participate representation pattern desnn framework online pattern suﬃciently similar already represented pattern representations merged form cluster. later work uses number bio-plausible mechanisms including spiking neurons rank-order coding variant stdp dynamic synapses present research proposes event-based stdp-type rules embedded single layer spatial feature coding. speciﬁcally paper proposes novel stdp-based representation learning method spirit learning rules local time space implement approximation clustering-based vector quantization using controlling sparseness independence visual codes. local time means information modify synapse recent within couple membrane time constant postsynaptic spike triggers stdp. local space mean information used modify synaptic weight principle available presynaptic terminal postsynaptic cell membrane. derivation uses continuous-time formulation takes limit length stimulus presentation interval tends time step. leads stdp-type learning rules although diﬀer classic rules found sense rules resulting visual coding model novel. independence sparsity also maintained implicit inhibition threshold adjustment rule implementing circuit. f¨oldi´ak developed feedforward network anti-hebbian interconnections visual feature extraction. hebbian rule model shown inspired oja’s learning rule extracts largest principal component input sequence where weight associated synapse connecting input neuron representation unit input linear output respectively. repeated trials term yjxi increases weight input output correlated. maintains learning stability respect binary units appropriate assumption made f¨oldi´ak modiﬁed previous feedforward network incorporating non-linear threshold units representation yj). weight change rules deﬁned eqs. based input output correlation. another interpretation explained terms vector quantization weights connected output neuron represent particular clusters weight change also aﬀected output neuron activation paper utilize vector quantization concept deﬁne objective function. objective function adapted develop spiking visual representation model equipped temporally local learning rule still maintaining sparsity independence. motivation event-based stdp-type learning rules. requires learning temporally local speciﬁcally using spike times prepostsynaptic neurons. proposed model adopts constrained optimization approach develop learning rules synaptically local. spiking representation model single layer shown derive plasticity rules operate stimulus presentation interval take limit tends local time step derive event-based rules. case linear unit objective function minimized shown below. uses vector quantization criterion regularizer prefers small weight values. linear output activation excitatory synaptic weight respectively. ﬁrst component shows vector quantization criterion scaled output neuron’s activity scales weight update rule according neuron’s response input pattern second component also scaled output neuron’s activity control weight decay criterion assume input output values converted spike counts hyperparameter controls model’s relative preference smaller weights. objective function emphasizes vector quantization criterion. contrast vector quantization component eliminated minimum objective function obtained wji’s response stimulus presentation subset spiking neurons representation layer activated code input. represent stimuli uncorrelated codes neurons activated independently sparsely. representation layer demands neural implementation. criterion achieved soft constraint where shows binary state unit presentation interval unit ﬁres least once. also ﬁring status neuron controlled threshold therefore constraint addressed threshold adjustment rule. where lagrange multiplier. minimizing ﬁrst component results coding module represents input feature vector cluster data synaptic weights. minimizing second component supports sparsity independence representation ﬁnally winner-take-all network exactly neuron ﬁres upon stimulus presentation. matter accomplished adapting however information needed temporally local. denotes rescaled pixel intensity represent input spike train. re-encode pixel intensity spike train uniformly distributed spikes rate according normalized pixel intensity range maximum number spikes interval additionally positive value denoting neuron’s activation response stimulus presentation available synapse wji. value reexpressed representing output spike train neuron spike trains formulated dirac functions shown either given sets presynaptic postsynaptic spike times. coding spike trains respectively propose local stdp learning rule following when coded spike trains synaptic change continuous time given shows ﬁring status neuron time speciﬁes presence presynaptic spike emitted neuron time interval experiments synaptic weight changed postsynaptic spike occurs finally learning rule formulated follows weight change related presynaptic spike times received postsynaptic neurons. scenario resembles spike-timing-dependent plasticity stdp rule current synaptic weight aﬀects magnitude weight change. instance second adaptation rule threshold learning rule. used implement learning rule adjusting threshold threshold learning rule shown provides independent sparse feature representation. threshold neurons representation layer. network architecture shown fig. consisting neurons input representation layers respectively. stimuli converted spike trains layers. given time step neuron representation layer allowed ﬁring criterion met. ﬁring criterion records neuron’s score winners-take-all competition. score time step given entire incoming weights representation layer given network softmax value governs time stdp occurs. wtascore neuron greater adaptive threshold stdp triggered spike emitted. softmax phenomologically implements mutual inhibition among representation neurons develop winners-take-all circuit representation layer. neurons representation layer purely excitatory explicit lateral inhibition implicitly implemented softmax. softmax inhibition imposed within representation layer network implements form competitive learning virtue stdp triggered ﬁring postsynaptic neurons. neurons competition allowed learn. stated earlier rule temporally non-local shows weight change interval. contrast stdp rule temporally local applying weight change time step postsynaptic neuron ﬁres. make comparable other consider time interval postsynaptic spike speciﬁcally break interval subintervals whose boundaries determined event postsynaptic spike. suﬃcient analyze arbitrary subinterval. therefore time simpliﬁes where ﬁring probability presynaptic neuron also generated presynaptic spike trains using normalized pixel intensities range diﬀerent random lags. thus probability value normalized pixel intensity ﬁring rate. therefore matches weight change shown shows proposed stdp rule consistent non-local rule. additionally stdp weight change unbiased estimation non-local learning rule. short time period proposed learning rule also unbiased estimation hebbian rule f¨oldi´ak threshold adaptation following threshold learning rule written where learning rate. number neurons representation layer ﬁring rule adjusts threshold neuron ﬁres response stimulus. criterion provides framework extract independent features sparse representation. used softmax-based neurons representation layer initial threshold value θinit following range where number neurons representation layer. upper-bound allows neuron active initial training steps capture visual features hand initial threshold enough stop high synchronization beginning according minimum number neurons used experiments initial threshold report reconstruction loss correlation measure root mean square error normalized original reconstructed patches shown eqs. respectively. patch stands spike rates calculate sparsity average activity breadth tuning measures. average activity speciﬁes density spikes released neurons representation layer time steps given breadth tuning measure introduced rolls tovee speciﬁes density neural layer activity calculated ratio mean standard deviation spike frequencies representation layer upon presenting stimulus. breadth tuning measures neural selectivity sparse code distribution concentrates near zero heavy tail neural layer neurons activity distribution uniformly spread breadth tuning greater contrast sparse code neurons distribution peaked zero breadth tuning less experiments using mnist natural image datasets evaluate proposed local representation learning rules embedded single-layer snn. datasets intensities gray-scale images normalized fall range yielding possible spike rates generate uniformly distributed spike trains input layer learning rates stdp learning threshold adjustment respectively. number experiments diﬀerent learning rates found changing range change model’s performance signiﬁcantly. additionally threshold adjustment rule modulated current threshold value chose smaller learning rate avoid possible threshold instability. variations network architecture determine under-complete overcomplete representations. trained ﬁlters iterations network neurons representation layer shown fig. training iterations kernels start becoming selective speciﬁc visual patterns ﬁlters shown image tend orientation selective extract diﬀerent visual features. loss values reach near optimal uniformly plateau. reason zero experiments. additionally fig. shows maximum minimum synaptic weights training respectively predicted three performance measures section used assess model. reconstructed images reconstruction loss sparsity. reconstructed images randomly selected digits acquired neurons representation layer shown fig. reconstructed maps show high quality images comparable original images. reconstruction loss measures snns ﬁlters appear figs. snns show lowest reconstruction loss training. sparsity measures reported average sparsity breadth tuning shown fig. sparsity measures also show fig. shows trained representation ﬁlters snns neurons representation layer. instance except ﬁlters marked dotted circles ﬁlters correlation other. visual assessments fig. shows four natural images reconstructed maps. performance proposed model terms reconstruction loss sparsity measures natural images shown maximum-minimum synaptic weights. reconstructed images based overlapped non-overlapped patches. overlapped patches selected windows sliding image stride figure model performance trends mnist training iterations terms correlation-based reconstruction loss reconstruction loss average sparsity breadth tuning. model’s performance training. evaluation measures trained visual representation model kernels. error bars show standard error mean. proposed spiking representation learning method shows better performance traditional k-means clustering restricted boltzman machine introducing local learning time space. implemented methods traditional quantization-like representation learning examples using training/testing images. k-means approaches applied normalized pixel intensities image patches thus methods temporally local. table shows comparison terms reconstruction loss model outperforms k-means methods except cases shows slightly better performance. fig. shows trained ﬁlters obtained k-means rbms model based mnist natural image patches. k-means similar model detects diﬀerent visual orientations mnist natural image patches ﬁlters highly correlated. perform well mnist dataset successfully learned table compares results spike-based representation learning models. correlation-based reconstruction loss mnist natural images shows improvement existing spiking autoencoder using mirrored stdp proposed burbank sparse representation introduced king modiﬁed version sailnet algorithm reported reconstruction loss around calculated based spike rates normalized unit standard deviation model compared favorably model zrms=.. paper derived novel stdp-based representation learning method embedded evaluated acquired representations experiments establish method’s initial viability. derived rules extremely simple evaluated reconstruction loss extremely low. simplicity rules makes attractive hardware implementation. like objective function regularization sparsity constraint. learning rules included spatio-temporally local stdp-type weight adaptation threshold adjustment rule. stdp rule equilibrium showed probabilistic interpretation synaptic weights scaled regularizer hyperparameter. addition threshold adaptation rule wtathresholded neurons representation layer implemented inhibition represent sparse independent visual features. softmax standard implement winners-take-all circuit implement mutual inhibition without using explicit inhibitory neurons representation layer experimental results showed high performance proposed model comparison spiking non-spiking approaches. model almost outperformed traditional kmeans models representation learning training orientation selective kernels. also method showed better performance state-of-the-art spiking representation learning approaches used plement stdp based representation learning threshold adjustment rule spiking platforms. spike-based platform spatio-temporally local learning rules lead main diﬀerence study well-known traditional representation learning methods introduced literature. existing spiking representation learning methods literature suﬀer limitations violating dale’s synapses change sign performance terms reconstruction loss non-spiking input signals study proposed stdp learning rule updates synaptic weights falling within range architecture consists excitatory neurons implicit inhibition occurring representation layer. implicit inhibition analogous separate inhibitory layer balancing neural activities representation neural layer dale’s maintained. furthermore proposed implements spiking neurons input representation layers neurons communicate temporal spike trains. best knowledge approach high performance representation learning model implemented snns. several studies literature developing snns equipped bio-inspired stdp unsupervised feature extraction single multi-layer spike-based architectures. recent works utilized features classify mnist digits. although networks introduce novel spiking network architectures feature representation oﬀer pure representation learning approach reconstruction loss. although proposed spiking representation learning successful reconstruction limitation spike rate presynaptic neurons higher biological spiking neurons. future work seeks reduce spike rate biologically plausible. using presynaptic neurons presenting mutual exclusive intensity bands would starting point. additionally matter future work determine well acquired representations stdp algorithm perform pattern recognition context. also tested future work whether acquired representations stackable aﬀord ability multilayer stdp-based learning.", "year": "2017"}