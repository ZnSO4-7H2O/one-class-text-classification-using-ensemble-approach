{"title": "Autoregressive Point-Processes as Latent State-Space Models: a  Moment-Closure Approach to Fluctuations and Autocorrelations", "tag": "q-bio", "abstract": " Modeling and interpreting spike train data is a task of central importance in computational neuroscience, with significant translational implications. Two popular classes of data-driven models for this task are autoregressive Point Process Generalized Linear models (PPGLM) and latent State-Space models (SSM) with point-process observations. In this letter, we derive a mathematical connection between these two classes of models. By introducing an auxiliary history process, we represent exactly a PPGLM in terms of a latent, infinite dimensional dynamical system, which can then be mapped onto an SSM by basis function projections and moment closure. This representation provides a new perspective on widely used methods for modeling spike data, and also suggests novel algorithmic approaches to fitting such models. We illustrate our results on a phasic bursting neuron model, showing that our proposed approach provides an accurate and efficient way to capture neural dynamics. ", "text": "autoregressive point-processes latent state-space models moment-closure approach fluctuations autocorrelations michael rule guido sanguinetti institute adaptive neural computation school informatics university edinburgh crichton edinburgh modeling interpreting spike train data task central importance computational neuroscience signicant translational implications. popular classes data-driven models task autoregressive point process generalized linear models latent state-space models point-process observations. leer derive mathematical connection classes models. introducing auxiliary history process represent exactly ppglm terms latent innite dimensional dynamical system mapped onto basis function projections moment closure. representation provides perspective widely used methods modeling spike data also suggests novel algorithmic approaches models. illustrate results phasic bursting neuron model showing proposed approach provides accurate efcient capture neural dynamics. connecting single-neuron spiking collective dynamics emerge neural populations remains central challenge systems neuroscience. well representing major barrier understanding fundamental neural function challenge recently acquired saliency rapid improvements technologies measure neural population activity vitro vivo unprecedented temporal spatial resolution technologies hold immense promise elucidating normal neural functioning aetiology many diseases high dimensionality complexity pose formidable statistical challenges. response needs recent years seen considerable eorts develop strategies extracting modeling information large-scale spiking neural recordings. successful strategies emerged last decade latent state-space models autoregressive point-process generalized linear models latent state-space models describe neural spiking arising unobserved latent dynamics auxiliary intensity model internal external factors contributing dynamics mathematically models generally take form process intensity obeys evolution equations. representation therefore recasts analysis spike trains within well-established line research statistical signal processing leveraging classical tools recent developments models used variety tasks describing population spiking activity motor system however models certainly lead biological insights latent state-space models remain phenomenological recurrent spiking activity implement latent state-space dynamics autoregressive ppglm models treat spiking events neurons point events arising latent inhomogeneous poisson process models generalized linear model regression used observed spiking events extrinsic variables like stimuli motor output intrinsic spiking history ppglm models especially useful statistical tests sources variability neural spiking benet simple procedure solved convex optimization. however require careful regularization avoid instability fail generalize outside regimes trained importantly ppglm models suer confounds unobserved sources neural variability especially apparent recorded neural population small subsample population latent state-space models accurate decoding applications leer establish mathematical connection autoregressive ppglm models ssms based dimensional low-order approximation exact innite-dimensional representation ppglm. unlike previous work explored mean-eld limits gaussian momentclosure capture excitatory eects uctuations process autocorrelations. convert auto-history eects spiking nonlinear dynamics low-dimensional latent state space. converts autoregressive point-process latent-variable process spikes viewed poisson events driven latent states. connection well interesting right also provides valuable cross-fertilization opportunity approaches. example issue runaway self-excitation ppglms emerges divergence moment closure ordinary dierential equations leading practical insights obtaining stabilized state-space analogue autoregressive ppglm. illustrate approach case study phasic bursting izhikevich neuron model considered weber pillow showing approach achieves high accuracy mean capture remarkably well uctuations process. start recapitulating basic notations denitions ppglms ssms. provide detailed derivation mathematical connection frameworks highlighting approximations make process. nally illustrate performance method application case study. point-process generalized linear models point process subset dimension zero higher-dimensional space purposes consider time domain equivalently consider realization figure phasic bursting neuron emulated autoregressive ppglm model. trained spiking output izhikevich neuron model parameters training data sampled using poisson noise stimulation current ltered alpha-function synapse time constant testing stimuli generated sampling poisson noise low-pass log-gaussian process. series points time point delta distribution event time. associate time locally constant counting process counts cumulative number events time process thought representing spike train output neuron cumulative process provides clearer notation derivations follow restrict aention described underlying intensity function simplest case event counts times occur poisson distribution mean rate given integral time window. called link function matrix operator projecting extrinsic covariates dimensionality point-process mean bias parameter history lter function. inputs bias denoted single time-dependent input function take link function natural logarithm. re-writing making time-dependence implicit unambiguous hydÏ„ explore generalized linear models autoregressive ppglms emulate various ring behaviors real neurons example phasic bursting neurons exhibit complex autohistory dependence fast slow timescales. dependence process intrinsic history confers additional slow dynamics. latent state-space point-process models alternative strategy capturing slow dynamics neural spike trains postulate slow latent dynamical system responsible bursting post-burst derivative standard wiener process reecting uctuations. functions describe respectively deterministic evolution stochastic uctuations observation model. case example poisson linear dynamical system latent dynamics linear gaussian noise reects inputs latent state-space. latent state-space models point-processes investigated detail mature inference approaches available estimate states parameters data however models typically phenomenological lacking clear physiological interpretation latent dynamics. importantly point-process history typically another extrinsic covariate eects poisson uctuations either neglected handled mean-eld limit. obscures dynamical role population spiking history uctuations autoregressive ppglm. remainder paper illustrate history dependence ppglm models implicitly denes latent-state space model moments process history. auxiliary history process ppglm nite) history dependence makes autoregressive ppglms nonmarkovian dynamical systems however crucial insight that since dependence history linear re-interpret history dependence linear lter time approximate eect conditional intensity using low-dimensional linear dynamical system. formalize this introduce auxiliary history process indicates events inserted history process derivative respect time converts autoregressive ppglm stationary markovian process augmented nite dimensional) state space history lter introduced equation formulation history process still point-process. however interaction between history intensity mediated entirely projection averages process history. capture relevant inuences process history suces capture eects poisson variability averaged projection. continuous approximation limit events frequent poisson process approximated wiener process mean variance equal instantaneous point-process intensity derivations follow omit explicit notation time-dependence unambiguous approximation holds averaging population weakly-coupled neurons averaging slow-timescales single neuron. applying approximation driving noise term evolution equation auxiliary history process obtain continuous innitedimensional approximation ppglm history process instantaneous rate spde analytically intractable exponential link function however possible derive nite) coupled moment equations process; close equations seing cumulants order greater zero eectively enforcing gaussianity history process equation process mean follows introduced uctuis expectation incorporates second-order eects ations time correlations mediated history lter. note time-evolution moment depends covariance time derivative covariance deterministic stochastic contributions. overall deterministic contribution derivative covariance wrien figure basis projection history process yields linear lter approximating original history basis elements. history dependence autoregressive point-process models typically regularized using nite history basis. right convert history basis elements linear dynamical system projects innite-dimensional delay-line onto low-dimensional basis. resulting linear system response functions approximate history basis. note however ringing introduced approximation. history process innite dimensional. make inference simulation practical represents continuous history lter nite collection basis functions common choice cosine basis example weber pillow parameters select base oset functions log-time respectively osets integer multiples basis projection moves innite dimensional history nite state space dened projection normalized volume preserved every time i.e. history basis features treated poisson random variables. practice history extend innite time basis functions omied. continuous history lter replaced silico dierentiation dirac delta operators implemented matrices representing discrete derivative point mass timestep respectively. basis projection yields low-dimensional linear operators dening dynamical system. resulting process basis projections integrate extended time-window. intensity approximately constant time window basis-projected history variables poisson variables rate variance projections virtue integrating longer timescales approximated gaussian. fluctuations gaussian point process well approximated gaussian projections history process. case approximate poisson analogously moment-closure innite-dimensional system derive gaussian moment-closure low-dimensional basis-projected system. equations evolution mean second moment nite basis projection equations reminiscent classical neural mass neural models unlike neural models however moment equations arise population averages rather directly considering expected behavior stochastic process describing neural spike train worth reecting analogy limitations representation. spiking events dramatic all-or-nothing events cannot approximated continuous stochastic process accordingly would expect nite-dimensional moment closure system fail capture rapid uctuations. however slow timescales gaussian approximation accurate even single neuron. contrast neural interpretation averages large population time instant average extended time window arrive approximation slow timescales pictorial description relationship proposed moment closure approach ppglms ssms neural models summarized figure case study izhikevich neuron model consider eectiveness approach case study ppglm emulation izhikevich neuron model considered weber pillow compare accuracy gaussian moment closure mean approach figure illustrates moment-closure phasic-bursting figure moment-closure autoregressive ppglms combines aspects three modeling approaches. log-linear autoregressive ppglm framework dependence history extrinsic covariates process mediated linear lters combined predict instantaneous log-intensity process. latent state-space models learn hidden dynamical system driven extrinsic covariates spiking outputs. models using expectation-maximization learned dynamics descriptive. moment-closure recasts autoregressive ppglm models state-space models latent dynamical state space physical interpretation moments process history. compare neural mass neural models dene dynamics state-space physical interpretation moments neural population activity. izhikevich neuron emulated ppglm averaging history process slow-timescales autoregressive point-process captured gaussian moment-closure. unlike mean-eld model considers largepopulation limit weakly-coupled neurons moment-closure able capture inuence poisson variability dynamics. additionally mean-eld considers single path process history whereas gaussian moment-closure provides approximation distribution paths uctuations autocorrelations taken account. benet moment-closure system sensitive combined eects self-excitation poisson uctuations captures example selfexcitation burst using second-order second moment terms. reveals another benet moment-closure approach runaway self-excitation detected moment-closure divergence mean second moment terms. self-excitation however introduces numerical challenges. typically post-spike lters ppglm models confer large rapid negativefeedback fast timescales order model absolute refractoriness spike trains. combination large negative feedback fast timescales emergent dynamics slow timescales make state-space moment equations small time-steps must taken capture refractory eects even slow-timescales relevant model. note second-order approximation moment equations less exhibits reduced runaway self-excitation highlights major benet moment closure approach numerical issues prove dicult intractable original representation readily addressed state-space model. importantly basis-projects moment-closure system ordinary dierential equation form reminiscent nonlinear continuous-time kalman-bucy ltering moment-closure state-space equations allow tools reasoning stability ordinary dierential equations applied ppglms. figure moment closure captures slow timescales mean fast timescales variance. moment-closure tracks low-frequency trends spiking log-rate bursts spiking captured increase variance compare mean-eld cannot account uctuation eects time correlations over-estimates mean log-intensity bursts. model trained phasic bursting neuron using shued square pulse stimuli amplitude. improve robustness log-ornsteinuhlenbeck noise added training stimulus mean steady-state variance time constant leer introduced mathematical connection ppglms ssms provides explicit constructive procedure neural spike train data. autoregressive point-processes state-space models combined always manner treats latent state-space extrinsic driver neural activity. importantly generative dynamical eects poisson uctuations process autocorrelations directly addressed previous approaches. additionally although ppglm models condition population spiking history training conditioning addresses single sample-path process history reect recurrent dynamical model spiking outputs uctuations lead emergence collective dynamics. moment-closure approach outlined used incorporate auto-history eects latent state-space model conditioning process history replaced bayesian ltering update updates moments history process time-step. results highlight capacity ppglm models implicitly learn hidden causes neural ring autoregressive history lter. example collective network mode induce spiking rhythmicity detected point-process history lter even isolated neurons exhibit oscillation. history dependence autoregressive ppglms denes latent variable process captures process auto-history eects inuence unobserved hidden modes inputs. moments population spiking history identied latent variables explaining population spiking. interpretation replaces pairwise coupling neural population ppglm formulation coupling single neurons shared latent-variable history process. identication latent states moments population history opens interpretation connecting ppglms latent state-space models neural models rich area research theoretical neuroscience suggests conditions neural-eld models interpreted latent variable models using modern techniques latent state-space models. conversely connection illustrates latent-state space models viewed modeling hidden causes spiking also capturing statistical moments population relevant neural dynamics neural-eld sense. precise convergence moment closure ppglm neural model remains beer explored mathematically. convergence moment closure approximations studied extensively area stochastic chemical reactions indeed approach partly inspired recent work chemical reaction-diusion systems pairwise interactions pointwise agents space replaced coupling single agents statistical eld. contrast chemical reaction systems however model self-interactions point-process time capture also eects uctuations system. models. first autoregressive time-dependencies converted low-dimensional system ordinary dierential equations re-interpreting ppglm dynamical latent-state space model similar form phenomenological latentdynamics models population neural models. second moment closure equations open strategies estimating ppglm models. major challenge ppglm models large populations challenges estimating large number pairwise interactions. work suggests dierent avenue toward estimating large models low-dimensional latent-variable stochastic process suitable nonlinearity poisson noise interpreted process governing moments ppglm model. allows extensive methodological advancements toward identifying low-dimensional statespace models applied autoregressive point-processes. could especially useful systems spiking output uctuations therein substantially inuences population dynamics. model accurately captures dynamics moment-closure equations outlined allow process moments estimated along model likelihood using bayesian ltering. addition ltering distribution paths process history ltering also average models thus implicitly capture uctuation eects model uncertainty. however remains subject future work apply momentclosure approach inference. methods particle ltering useful situations latent state-space distribution highly nongaussian. acknowledgments authors acknowledge support engineering physical sciences research council grant ep/l/ large scale spatio-temporal point processes novel machine learning methodologies application neural multi-electrode arrays appendix linear noise approximation stabilized moment closure simpler alternative moment-closure linear noise approximation uses deterministic mean obtained limit large weakly-coupled population eect uctuations mean negligible. basis-projected system deterministic mean contrast moment-closure equations capture approximate distribution paths history process correct eects uctuations mean-rate. however practice moment equations challenging integrate. self-excitation typically stabilized rapid negative-feedback short timescales exponential nonlinearity lead runaway self-excitation. modied version approximates variance correction second order incorporates variance corrections improved stability system mean-eld used deterministic evolution variance uctuation eects mean rate approximated second-order equations heuristic practice provide state-space analogue autoregressive ppglm stable.", "year": "2018"}