{"title": "Floating Forests: Quantitative Validation of Citizen Science Data  Generated From Consensus Classifications", "tag": "q-bio", "abstract": " Large-scale research endeavors can be hindered by logistical constraints limiting the amount of available data. For example, global ecological questions require a global dataset, and traditional sampling protocols are often too inefficient for a small research team to collect an adequate amount of data. Citizen science offers an alternative by crowdsourcing data collection. Despite growing popularity, the community has been slow to embrace it largely due to concerns about quality of data collected by citizen scientists. Using the citizen science project Floating Forests (http://floatingforests.org), we show that consensus classifications made by citizen scientists produce data that is of comparable quality to expert generated classifications. Floating Forests is a web-based project in which citizen scientists view satellite photographs of coastlines and trace the borders of kelp patches. Since launch in 2014, over 7,000 citizen scientists have classified over 750,000 images of kelp forests largely in California and Tasmania. Images are classified by 15 users. We generated consensus classifications by overlaying all citizen classifications and assessed accuracy by comparing to expert classifications. Matthews correlation coefficient (MCC) was calculated for each threshold (1-15), and the threshold with the highest MCC was considered optimal. We showed that optimal user threshold was 4.2 with an MCC of 0.400 (0.023 SE) for Landsats 5 and 7, and a MCC of 0.639 (0.246 SE) for Landsat 8. These results suggest that citizen science data derived from consensus classifications are of comparable accuracy to expert classifications. Citizen science projects should implement methods such as consensus classification in conjunction with a quantitative comparison to expert generated classifications to avoid concerns about data quality. ", "text": "large-scale research endeavors hindered logistical constraints limiting amount available data. example global ecological questions require global dataset traditional sampling protocols often inefficient small research team collect adequate amount data. citizen science offers alternative crowdsourcing data collection. despite growing popularity community slow embrace largely concerns quality data collected citizen scientists. using citizen science project floating forests show consensus classifications made citizen scientists produce data comparable quality expert generated classifications. floating forests web-based project citizen scientists view satellite photographs coastlines trace borders kelp patches. since launch citizen scientists classified images kelp forests largely california tasmania. images classified users. generated consensus classifications overlaying citizen classifications assessed accuracy comparing expert classifications. matthews correlation coefficient calculated threshold threshold highest considered optimal. showed optimal user threshold landsats landsat results suggest citizen science data derived consensus classifications comparable accuracy expert classifications. citizen science projects implement methods consensus classification conjunction quantitative comparison expert generated classifications avoid concerns data quality. much scientific community slow embrace citizen science despite potential massively increase scale research done. scientific community turns towards global issues climate change research bottlenecked requisite amount data tackle questions simply cannot collected traditional small research teams citizen science offers efficient method collect dataset adequate size tackle large-scale questions crowdsourcing tasks would otherwise prohibitively time consuming citizen science also provides rare valuable opportunity collaboration researchers members public particular import ecological research outcome study could lead management decisions implications general public disconnect exists general public understanding scientific process lessened participating citizen science projects interacting researchers personal basis however useful need assured citizen science generating data sufficient quality produce meaningful results. problem pernicious scientific circles leading professional scientists look askance citizen science projects present simple validation method utilizing consensus multiple citizen scientists generate high quality data. demonstrate remote sensing data giant kelp produce data comparable accuracy expert scientists. citizen science concept; examples astronomy ornithology late century recently several citizen science ornithological studies become household names including audubon christmas bird count cornell’s projectfeederwatch nestwatch ebird usgs’s breeding bird survey citizen science approach collaborative field data collection bled fields ecology north american butterfly count began covers united states canada mexico last decade power reach internet explosion citizen science activity fields well galaxy extremely successful example online citizen science resulted publications biomedical field also found success projects foldit protein folding gamified list makes tiny fraction larger citizen science body least active citizen science projects running last assessment despite potential powerful research tool scientific community hesitant embrace citizen science. reluctance largely lack rigorous validation standards results many datasets unknown quality general attitude distrust citizen scientists’ data despite concerns several ways overcome question data quality citizen science. perhaps conventional simply ensure large sample size; data typically means precision estimated population parameters despite increases variance data major obstacle citizen science projects often decision engage citizen science made size required dataset still even large dataset scientists often skeptical without means quality control. large datasets guarantee accurate data ensure data quality data collected citizens must compared data collected experts. beyond simply validating dataset comparisons invaluable developing comprehensive sampling regimes lessons learned validation pilot dataset applied definition volunteer eligibility sampling protocols turn ensuring higher accuracy main body work many projects particularly web-based requirements volunteer eligibility. studies rely post comparisons citizen scientist expert classifications ensure data quality. example cornell’s projectfeederwatch citizen scientists contribute individual bird observations annual basis. cornell developed semi-automated system anomalous observations flagged reviewed experts re-integrated dataset method allows integration quality-control protocols directly data generation pipeline contributes efficiently processing large amounts citizen science data without compromising quality. developing methods ensure data quality without restricting volunteer eligibility requirements priority web-based citizen science becomes increasingly popular consensus classification leverages agreement multiple citizen scientists improve quality data provided citizen scientists unknown varied backgrounds stands contrast methods seek rely individuals quality data foundation consensus classification lies redundant processing samples multiple volunteers. results multiple complete sets classifications aggregated suit researcher’s needs. advantage consensus classification data quality preserved even number individual citizen scientists inaccurate allows projects many demographics search volunteers without concerns previous experience need judge abilities individual citizen scientists. consensus approach proved popular particularly online citizen science. demonstrate efficacy using citizen science project detect thirty years change world’s kelp forests floating forests floating forests uses consensus classifications crowdsource detection giant kelp landsat satellite images effort establish global picture kelp distribution health last years traditional field sampling approaches labor intensive attain requisite global coverage obvious reasons cannot performed retroactively. existing techniques measuring kelp canopy cover landsat satellite images require hundreds human work-hours classify even relatively small geographic region. show citizen science utilizing consensus classifications classify kelp landsat images large scale comparable accuracy expert classifications. full size landsat scenes converted small jpeg subsets presented citizen scientists floating forests classification interface. region used noaa’s world vector shoreline dataset identify path/rows contained coastline. available images path/row downloaded usgs landsat archives. landsat image converted atmosphere reflectance using scene specific bias gain values earth-sun distance solar zenith angle. split landsat scene images equal size along grid image subset displayed short-wave infrared band near infrared band green band blue. high near infrared reflectance kelp canopy caused stand bright green band combination. evaluated accuracy consensus classification using floating forests citizen science website requirements participate. users viewed brief tutorial oriented website provided training identify classify kelp patches first visit. additionally field guide accessible time contained entries image features commonly confusing. showed randomly selected preprocessed landsat images tasked tracing borders visible kelp patches using free-form selection tool also asked indicate presence clouds image. problems classification users encouraged image would cross-post talk forum could interact directly researchers. launched project august imagery california tasmania taken december project relaunched different landsat imagery processing pipeline; data floating forests considered here. first version platform hosted users contributed classifications. users could also flag images confusion regarding classification researchers address issues online talk forum. data landsat used analysis validates citizen scientist classifications central southern california. figure classification interface floating forests. points interest include following classification window. additional image information including geographic coordinates landsat metadata link view image google maps. additional user flags indicate image retired image contains clouds. field guide containing examples potentially confusing features users could encounter. floating forests ensured data quality partially consensus classifications. images classified least four users. user detected kelp image retained system classified total users. however user detected kelp image retired removed image pool. additionally image flagged user image dropped pool calibration data california obtained previous work estimate kelp biomass patch borders landsat photographs short kelp estimates derived relationship kelp detected satellite images aerial surveys; detailed methods. assess user threshold produced optimum classifications assess overall quality classifications created consensus dataset overlaying user classifications image. pixel receives score corresponding number unique kelp classifications received. score user threshold image compared consensus expert classifications using matthews correlation coefficient user threshold highest considered optimal user threshold image. provides method assess performance binary classifier utilizing confusion matrix comparison produce following score equation above refer true positive true negative respectively false positive false negative respectively. floating forests true positives pixels correctly classified kelp true negatives pixels correctly classified kelp. false positives pixels classified kelp reality false negatives pixels contained kelp classified. ranges completely wrong classifier representing classifier good coin toss perfect classifier. also interpreted terms strength akin pearson correlation. used opposed methods evaluate classifiers works well cases prevalence class number kelp pixels comparison total number pixels image scenario methods often produce biased paucity kelp relative types pixels images. answer questions optimal threshold overall accuracy used polynomial regression assess relationship optimal user threshold subject said optimal mcc. satellite sensor season acquisition potentially effect user classification accuracy included predictors optimal well allowing interact user threshold. assess image retirement rules used calibration data summarize number expert classified kelp pixels image. retirement rules effective must drop images little kelp images contain incomplete glitched satellite data. rejected images contain non-zero amount kelp visually inspected confirm accurately flagged images. along accuracy possible citizen scientists likely overunder-classify kelp. assess whether users biased calculated density difference false negative false positive indicating under-classification. bias would indicated mean density significantly departs zero. table sample output image demonstrating confusion matrix change based user threshold. table user kelp pixels calibration kelp pixels total kelp pixels refer number pixels selected users number kelp pixels calibration data total number pixels image respectively. original image viewed https//static.zooniverse.org/www.floatingforests.org/subjects/efdbd.jpg. landsat image courtesy u.s. geological survey. analysis optimal found peaked relationship user threshold optimal satellite identity affected modify relationship user threshold determined optimal user threshold landsats landsat shows majority rejected images contain little kelp. visually inspected images significant amount expert classified kelp still rejected. inspection showed correctly rejected bad/glitched images figure model output displaying correllation user threshold satellite landsat differ optimal user threshold landsat optimal user threshold significantly higher effect season either optimal user threshold figure distribution number pixels containing kelp image expert classifications among subjects retired user flags. images pixel count retired kelp flags whereas images higher pixel counts retired image flags. subject akpimr typical images retired classification workflow image user flag. landsat- image courtesy u.s. geological survey. analysis shows data derived citizen science using consensus classifications used confidently comparable accuracy expert classifications. found optimal consensus threshold users produced average landsats landsat reference would indicate classifier correct time further citizen scientist results unbiased. implies accurate result obtained relatively level consensus among volunteers landsat producing accurate classifications older satellites. fact appears differ landsat missions unexpected landsat produces higher quality images. consensus approach yields enormous dividends terms worries individual citizen scientists. accuracy single citizen scientist high given final classifications determined consensus. approach naturally eliminates outliers example allowed highly accurate definition kelp patch borders figure floating forest image presented users. note green patches kelp offshore. heatmap user consensus thresholds. landsat- image courtesy u.s. geological survey. retirement rules effective eliminating unwanted images. figure shows majority rejected images contain little kelp. breakdown also shows number rejected images appear contain kelp. partial glitched images received image flag retired along non-kelp images partial images contained full neighboring landsat scenes thus represent missing data. consensus approach provides efficient method avoid problem evaluating citizen scientist expertise. attempts verify improve citizen science data suggested weighting participants contributions based factors length participation prior accuracy effective inefficient requires significant overhead part researcher. using system consensus classifications vast majority data collected utilized. consensus classification always best choice. classifier weighting required citizen scientists responsible complicated tasks. consensus classification require large number citizen scientists evaluate thing. many programs might still impractical. example reef check bio-blitz style citizen science projects typically unable muster number volunteers required generate consensus possible combining consensus classification classifier weighting provide higher accuracy either method independently last results alleviate doubts effectiveness training slowed acceptance citizen science previous work suggests citizen scientists higher accuracy accompanied professionals results demonstrate citizen scientists create high quality data despite small amount training remote communications experts. supervised data compare scores optimal thresholds provide confirmation citizen classifications accurate relative expert classifications even absence hands-on expert guidance. move unprecedented period environmental change critical consider questions global scale. questions often necessitate datasets derived long term environmental monitoring efforts prohibitive small research teams citizen science provides rewarding approach crowdsource data collection engaging volunteers. despite concerns regarding data quality publication results derived citizen science data increased substantially last years shown citizen science data collected consensus classifications adequate quality rigorous scientific analyses. confidence data quality utmost importance citizen science embraced scientific community. consensus classifications part increasingly comprehensive toolkit ease quality concerns increase trust citizen scientists data. authors would like thank umass boston psychology department stats snack invaluable advice nceas kelp climate change working group. authors would also like recognize lter keen temperate reef base making floating forests today. thanks nasa project -csesp- providing funding support work.", "year": "2018"}