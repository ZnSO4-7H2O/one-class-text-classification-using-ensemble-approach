{"title": "Importance sampling for partially observed temporal epidemic models", "tag": "q-bio", "abstract": " We present an importance sampling algorithm that can produce realisations of Markovian epidemic models that exactly match observations, taken to be the number of a single event type over a period of time. The importance sampling can be used to construct an efficient particle filter that targets the states of a system and hence estimate the likelihood to perform Bayesian parameter inference. When used in a particle marginal Metropolis Hastings scheme, the importance sampling provides a large speed-up in terms of the effective sample size per unit of computational time, compared to simple bootstrap sampling. The algorithm is general, with minimal restrictions, and we show how it can be applied to any discrete-state continuous-time Markov chain where we wish to exactly match the number of a single event type over a period of time. ", "text": "abstract present importance sampling algorithm produce realisations markovian epidemic models exactly match observations taken number single event type period time. importance sampling used construct eﬃcient particle ﬁlter targets states system hence estimate likelihood perform bayesian inference. used particle marginal metropolis hastings scheme importance sampling provides large speed-up terms eﬀective sample size unit computational time compared simple bootstrap sampling. algorithm general minimal restrictions show applied continuous-time markov chain wish exactly match number single event type period time. many epidemic models naturally described continuous-time markov chains capture discrete nature individuals important considering smaller populations well random nature underlying events. bayesian inference using models diﬃcult because apart state space small transition density models cannot evaluated point-wise. thus many modern methods performing inference using models rely simulating underlying model—sampling transition density instead evaluating it—which typically quite simple black school mathematical sciences university adelaide adelaide australia. acems school mathematical sciences university adelaide adelaide australia. e-mail andrew.blackadelaide.edu.au method particle marginal metropolis hastings understood basic metropolis-hastings algorithm targeting parameter posterior model likelihood replaced unbiased estimate particle ﬁlter form sequential monte carlo popularity pmmh stems targeting exact posterior distribution parameters model despite estimate. challenge implementing method mixing chain depends strongly variance likelihood estimate turn depends performance particle ﬁlter. simulations discrete-state model normally done using stochastic simulation algorithm example bootstrap ﬁlter problem approach observed events rare state space large number particles needed estimate state hence marginal likelihood step becomes prohibitively large. well known problem accurate observations worse ﬁlter performs. epidemic models observe component state exactly—for example number infection recovery events interval time —the cost producing simulations match data becomes high. mitigated extent assuming adding noise observations essentially increasing likelihood particle matching whether reasonable modelling decision inevitably extra variance parameter estimates. better approach importance sampling generate realisations process exactly match observations. importance sampling works changing rules process evolves make rare event probable bias corrected calculation likelihood. paper present simulation algorithm implements importance sampling produce realisations complex stochastic epidemic models match observations exactly. builds substantially earlier work mckinley modifying extending basic ideas mckinley resulting algorithm easily applied quite complex models suﬀer numerical instabilities inherent original algorithm. begin describing basic importance sampling idea simplest one-dimensional model. show generalised number complex multi-dimensional epidemic models data observations component state typically number certain transition event. finally illustrate importance sampling particle ﬁlter perform inference number outbreak time series using model accounts diﬀerent infectious phases. resulting posterior distributions compared results using alive particle ﬁlter uses sampling. version using importance sampling achieves large speed-up eﬀective sample size unit computation time using number particles speed-up increases size data grows. matlab code provided methods part epistruct project. ﬁrst introduce basic importance sampling idea continuous-time markov chain using simple example. consider simple model decay process collection objects initially decay independently rate deﬁne number decay events time hence number objects left rate event therefore assume interval time without loss generality take observe events. wish calculate monte carlo estimate likelihood observation corresponds step sequential monte carlo routine likelihood observation written hence generate times increment variables keep repeating this next generated time greater observation window algorithm stops. realisation process speciﬁed initial state times events occur. sampling transition density illustrated figure using realisations. observation example none particles match this hence weight would zero estimate likelihood zero. fig. illustration bootstrap sampling transition density importance sampling circles indicate particles initially; blue circles show particles propagation day. grey plot shows true transition density. importance sampling makes particles observed state particles diﬀerent weights note panel estimate likelihood none particles match observation hence particles would assigned weight insight developed mckinley possible design simulation algorithms observation likelihood i.e. realisations state consistent observation. reduces known order statistics generated sorting uniform random numbers employing dedicated algorithm times generated remains calculate weight realisation. importance density particular realisation follows considering joint distribution uniform random variables. original process—i.e. events occur rate given —the time next event distributed exponentially term front expression probability events interval recursive nature expression means simple evaluate iteratively—this seen clearly code provided—and exploited later simulation algorithms. practice work avoid numerical issues. figure shows realisations produced using importance sampling simulations example before. construction particles observed ﬁnal state diﬀerent weights. take basic idea developed previous section—that simulate realisations match observations exactly—and show applied estimate states likelihood two-dimensional susceptible–infected–recovered epidemic model observe single component state. this begin develop idea importance sampling algorithm considered suitably modiﬁed version original process conditioned event times initially randomly generated. marily terms numbers events occur infection recovery thus denote number infection recovery events occurred time hence state system speciﬁed vector assume observations system correspond number infection events interval time recovery events observed; hence know number infected recovered individuals system. deﬁning observation likelihood matrix known stoichiometric matrix assumed start completely susceptible population size stoichiometric matrix encodes event changes numbers example infection event decreases number increases number rates events infection recovery rate parameters respectively herein assumed ﬁxed. reason work primarily terms event counts rather population numbers event counts ever increase leads naturally relation given system starts particular state time realisation generated using basic versions this; ‘direct’ version time next event exponentially distributed rate parameter event happens time chosen randomly according probabilities ai/a equivalent performing simulation instead draw times next event chosen smallest time. known next reaction method however generated realisation process speciﬁed initial condition list times index event occurs times equal times random number times recovery events. basic idea realisation produced ﬁrst randomly generating times infection events interval generating recovery times times consistent model data. recovery times generated simulating modiﬁed version model conditioned initially-generated infection times. consistency model means must true; consistency data means specify times infection events ﬁrst step must always possible requires call forced events. next generate recovery times produce valid realisation; running modiﬁed version model interval time conditioned times this ﬁrst note speciﬁed times exact number infection events ﬁrst step rate infection events must zero observation period. also cannot another recovery event next infection event hence rate event must zero. modiﬁed process therefore rates events type respectively b+b. seem wasteful specify process redundancy signiﬁcantly simpliﬁes general version algorithm exposition. removed—and algorithm made slightly quicker—by simply re-labelling events discussed later. starting modiﬁed process simulated rates time next forced event given current time algorithm proposes time next event drawn exponential distribution note next step algorithm implement next forced infection event time algorithm reached time last forced infection event procedure carries potentially adding recovery times means events occur observation window simulation terminates. thus iteration three things happen either recovery proposed time pre-calculated infection time observation window reached. procedure generate realisation number infection events matches observation also consistent. remains calculate probability densities realisation modiﬁed process original process calculate weight realisation. first contribution weight initially-generated infection times before given rest weight contributions easily calculated iteratively algorithm proceeds. first note transition density event original process extend algorithm developed generate realisations seir model match observations exactly. adds additional complication maintaining consistency particular realisation. seir model similar model additional latent class individual infected infectious. model deﬁned terms three events infection latent progression recovery conciseness refer events respectively counts number events occurred time relation number event population numbers simulation algorithm model similar model. ﬁrst step times event generated second step times events generated conditional event times. model assumed observations ﬁrst event chain model observe second event introduces additional complexity. model require consistency that emphasized zero zero event must occur next event practice means certain points simulation event also need forced maintain consistency realisation—how done discussed later. depending observations made also wish condition process ﬁnal size outbreak. total number event observed course outbreak implies also. simulation proceeds stages follows. ﬁrst stage times event generated uniform distribution observation interval sorted. before stored simple vector stored stack denoted times added stack reverse order along event indices earliest forced event stack. move stack algorithm need force events course simulation. point time event index respectively call next forced event. example step shown figure second stage procedure simulates modiﬁed markov chain interval generate times events. algorithm proceeds similar manner model proposing times next event accepting rejecting based time next forced event. additional step occurs beginning iteration algorithm checks index next forced event state system. idea illustrated example figure state system state consistent next forced event. thus time proposed derived modiﬁed process speciﬁed detail later. example next forced event implemented instead proposed updated. event next iteration hence state inconsistent next forced event least event must occur within interval model consistent time time event generated truncated exponential distribution interval rate i.e. current rate event time event index pushed stack. step represented pictorially figure whenever additional event forced modiﬁed rate event zero event implemented. contribution importance weight extra forced event second rate always zero number times events already generated. third rate zero single infected exposed individual stop disease going extinct. ﬁrst rate equal zero i.e. event next forced event. ﬁrst rate also zero number events equals observed ﬁnal size thus realisations would correct total number infections. situations case next forced event implemented next step algorithm. modiﬁed rates calculated time proposed compared time next forced event described model. additional step required proposed time accepted particular event occurs randomly chosen proportion rate bi/b ssa. hence proposed time accepted event chosen importance weight updated fig. illustration algorithm seir model initialisation times event generated added reverse order stack point time event index respectively. ﬁrst step state system consistent next forced event. hence time proposed shown light blue. example next forced event implemented popped stack. state inconsistent. thus algorithm forces event interval time shown light green. stack addition note added stack still state consistent next forced event. hence next step algorithm propose time next event steps algorithm returns start checks state system next forced event. continues last detection event point constraints modiﬁed process much simpler algorithm stop fade disease forced events. ﬁrst proposed time goes beyond observation window algorithm stops ﬁnal weight contribution step importance sampling described limitation perform well parameters. example model eﬀectively becomes model individual leaves exposed class almost immediately entering case times infection events observed events become highly correlated importance sampling process longer good approximation true process. case importance sampling actually increase variance estimate likelihood comparison naive approach. restrict range parameters algorithm used. used within pmmh routine enforced setting appropriate priors metropolis hastings part algorithm. point give summary importance sampling algorithm. assume model event types observe number type interval algorithm follows steps model dependent hence discussed detail next sections. initialisation initial condition generate times observed events uniform distribution interval sort stack reverse order initial importance weight forced events implemented simulation continues t+t′ step longer required conditioning modiﬁed process step turn simpler. algorithm terminates ﬁnal contribution weight signposted steps depend model structure. model step required observe ﬁrst event chain. seir model steps detailed section next section describe steps carried complex model potentially requires forcing chains events maintain consistency state given next forced event. section give example algorithm applied model complex structure requires decision tree deciding force events maintain consistency realisation. second part incorporate particle ﬁlter compare standard approach literature. model structure illustrated figure splits infectious class stages modelling pre-symptomatic symptomatic phase also includes possibility asymptomatic individuals contribute towards overall force infection paper refer seiar model. assume observe event interval interval corresponds individuals becoming symptomatic. also assume ﬁnal size observation made later observations. final size refers total number detections course outbreak total number individuals infected. fig. model individuals pre-symptomatic stage also includes asymptomatic individuals contribute force infection. individuals observed ﬁrst enter class. steps algorithm diﬀerent already presented. step complicated previous models situations algorithm needs force chain extra events state consistent time current next forced event. example least event event must occur interval another complication arises step also need monitor number event occur later time-steps—as would occur particle ﬁlter—there still enough susceptible individuals left algorithm match ﬁnal size observation. diﬀerent ways implementing step adopt rule current state system inconsistent next forced event force ﬁrst event system consistent. returning example above algorithm would ﬁrst force event interval consistent current state. algorithm reached time state inconsistent ﬁxed forcing event rule adopted state system made consistent forcing single event thus algorithm needs keep track next forced event rather keeping track chains. following this times extra forced events importance weights remain simple calculate detailed previous section algorithm proceeds steps modiﬁcations. rule also means logic events forced given type next forced event current state represented decision tree readily deduced. decision tree computing type forced event step algorithm model shown figure implemented conditional statements reached indicates current state system consistent next forced event type computed index along time added stack described section another example simpler decision tree given supplementary material. step modiﬁed rates model rates zero either events already forced stack. rates modiﬁed allow many either events ﬁnal size data available. finally rates also modiﬁed disease cannot prematurely fadeout. modiﬁed rates calculated steps algorithm section section seiar model perform inference example time series using pmmh algorithm. particle ﬁlter uses importance sampling described previous sections resulting posteriors compared obtained using alive particle ﬁlter alive ﬁlter works time step simulating number realisations using certain number matches obtained. clearly computationally expensive gives unbiased estimate likelihood. assumed frequency-dependent transmission hence scaled deﬁne proportion transmission pre-symptomatic individuals hence proportions attributable presymptomatic individuals performed inference number synthetically generated outbreak time series increasingly large populations. parameters used generate data similar inﬂuenza populations could represent example outbreaks aboard ships. ﬁxed initial condition used simplicity replicated inference routines. major outbreaks chosen ﬁnal number detections outbreak summarised table time series plotted supplementary material. algorithms coded metropolis hastings part algorithm used simple random walk proposal pilot carried determine appropriate covariance matrix this used four time series. implementation alive ﬁlter follow drovandi mccutchan maximum number trials time step ﬁlter terminates returns zero likelihood. introduces error algorithm stops becoming stuck proposed parameters mean observation rare event; three smaller datasets increased largest using smaller value resulted large error tails posterior parameter likelihood small estimate trials time step. alive ﬁlter coded iteration terminates soon becomes inconsistent observation constraints results optimal performance. example realisation matches observation weight realisation thus estimate likelihood time step ﬁlters. particle ﬁlter using importance sampling re-sampled particles time step; advantage found re-sampling less frequently. assume informative priors lower bounds respectively. done without extra information either data observations parameters unidentiﬁable. parameters assume uninformative priors respectively. resulting marginal posterior distributions shown figure eﬀective sample sizes second time given table complete times statistics given supplementary material. firstly excellent agreement alive ﬁlter importance sampling. data sets grow size versions slow increasing number particles used increasing number events simulated. speed importance sampling ﬁlter alive ﬁlter quantiﬁed terms second time increases size datasets also increases. attempt made tune number particles either algorithm beyond attaining reasonable performance likely slightly better results could obtained both. slight deviation posteriors parameter datasets result error introduced small value alive ﬁlter fig. marginal posterior distributions performing inference using particle ﬁlter importance sampling using alive ﬁlter true values parameters used generate data marked grey lines. table inference statistics population size total number detected cases. eﬀective sample size second computing time given along speed alive ﬁlter. parameters slightly diﬀerent follow pattern. values running times given supplementary material. shown importance sampling used produce weighted realisations model exactly matches data number given event interval time. visualise basic exact matching algorithm ends continuous spectrum potential importance sampling schemes. essentially blind sampling transition density without regard observed data—a simple form rejection sampling. conversely exact-matching algorithm guided state ensure realisation consistent observations. importance sampling used construct particle ﬁlter pmmh providing large speed-up terms unit time compared bootstrap sampling using alive ﬁlter. importance sampling builds work mckinley diﬀers major respects. firstly algorithms mckinley tailored explicitly seir models recovery events observed. contrast algorithm presented general applied continuous-time markov chain single event observed. diﬀerence current algorithm forces extra events maintain consistency realisation. older algorithm detected extra event forced event possible algorithm chooses event proportion relative rates two. implicitly incorporates consistency requirements prone numerical instabilities many events occur interval eﬀect reducing size interval next forced event without actually making state consistent. hence algorithm attempts correct putting events smaller smaller time intervals point errors ﬂoating point arithmetic arise. contrast current algorithm explicitly forces particular events speciﬁc order automatically update current time original algorithm does. eﬀects; ﬁrstly events simulated intervening time periods reduce need force events later secondly consistency ensured without undue forcing less likely produce realisations deviate expected behaviour would assigned weight. knock eﬀect numerical instabilities reduced need force events small intervals reduced. still numerical instabilities arise algorithm. generating truncated exponential random variables small intervals inherent instabilities exponentiating required calculating cdf. source error could circumvented instead generating times uniform distribution size interval threshold value another workaround simply weight realisations become inconsistent zero. egregious realisation likely small weight anyway unlikely result error overall o’neill roberts contrast pmmh marginalises missing data calculation likelihood data-augmented mcmc infers missing data part overall markov chain. event times known likelihood trivial write conditional distributions parameters often derived allowing eﬃcient gibbs sampling. greatest strength data-augmented approach ﬂexibility; non-markov models handled easily markov ones along potentially large amounts heterogeneity population spreading process downside strong dependence missing data parameters means mixing become slow convergence become issue amount missing data increases contrast particle ﬁlters marginalises missing data estimating likelihood. means mcmc scheme targeting parameter posterior much simpler easy tune. another important diﬀerence data-augmented mcmc pmmh deal increasing amounts data collected independent outbreaks. aspect data-augmented mcmc essentially serial algorithm. thus performing inference many independent outbreaks becomes challenging reasons mentioned above; state space becomes large convergence becomes problem. parallel chains convergence issue useful. contrast marginalise missing data particle ﬁlters easily parallelised hence take advantage modern computing hardware. particle ﬁlter easily accomplished running number independent ﬁlters separate cores averaging results obtain estimate likelihood lower variance approach described takes parallelism even parameter particles particular iteration updated independently. main drawback pmmh mixing main chain depends strongly variance likelihood estimate. trade decreasing variance log-likelihood increasing number particles hence computational expense. thus higher variance estimate resulting worse mixing oﬀset reduced computational expense hence samples posterior. idealised models optimal performance achieved tuning variance particular range turn maximises eﬀective sample size unit computational time paper attempted tune either particle ﬁlters employed herein likely better performance obtained both. recently algorithms proposed ameliorate tuning issues sequential monte carlo target parameter posterior well states system. importance sampling developed paper easily used approaches. primary weakness importance sampling presented paper although algorithm general black box. implementation depends model structure rates events well event observed constraints. details edge cases important algorithm correct testing always simple working rare events. example edge case calculating likelihood last point time series simulation depend whether disease allowed fade forced events implemented. clear models observe ﬁrst event longer chain much easier handle observe later events multiple another weakness importance sampling scheme breaks rate observed event become large. example seir model model eﬀectively becomes model individual leaves exposed class almost immediately entering case times infection events observed events become highly correlated importance sampling process longer good approximation true process. limitation also applies algorithms presented mckinley problem essentially stems attempting model selection time parameter inference. avoid running pmmh algorithm simply priors disallow large rates meaning seir model cannot become model. example section lower bound parameter i.e. latent period must least half day. natural trade-oﬀ speed method generality problems applied alive ﬁlter restrictions parameters much slower. provided matlab code models presented paper code somewhat unoptimised keep simple follow. particle ﬁlters used inference example coded gives order magnitude improvement speed. redundancy speciﬁed algorithms. example modiﬁed rates observed event always zero. done readability easily factored performance gains. done simply relabelling events observed event last. example seiar model would relabel then vector propensities event rate zero always hence never iterated generating next event type. also possible re-factor algorithm remove need stack hold forced event times would allow optimisation speciﬁc states. optimisation comes expense generality clarity presented here. importance sampling algorithm described paper state dependent alter underlying parameters model. schemes rare event simulation based altering parameters process create matches cross-entropy methods guide this. similar ideas could implemented here computational expense would probably outweigh beneﬁt. exact-matching algorithm easily extended situations observations noisy rather exact. instead number events sampled observation density simulation algorithm value. noisy observations increase performance bootstrap particle ﬁlter using sampling allows particle match larger states. gain would seen using algorithm presented paper number observed events exactly realisation generated. note seeiir model described supplementary material could written using binomial observation process instead observed unobserved events would still beneﬁt using importance sampling produce realisations likely match observations. paper assumed observation single event type natural modelling epidemics necessarily systems. ecology lokta-volterra model often assumes observations population numbers case range numbers events could give rise observation. principle similar ideas presented construct realisations match types observations. diﬃculty arises generating times forced events ordered correctly also calculating order statistics. approaches currently investigation. acknowledgements research supported decra fellowship also acknowledges support centre excellence mathematical statistical frontiers australian government nhmrc centre research excellence policy relevant infectious diseases simulation mathematical modelling supercomputing resources provided phoenix service university adelaide. would also like thank joshua ross james walker comments earlier draft manuscript.", "year": "2018"}