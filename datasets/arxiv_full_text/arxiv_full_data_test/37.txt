{"title": "Bits from Biology for Computational Intelligence", "tag": "q-bio", "abstract": " Computational intelligence is broadly defined as biologically-inspired computing. Usually, inspiration is drawn from neural systems. This article shows how to analyze neural systems using information theory to obtain constraints that help identify the algorithms run by such systems and the information they represent. Algorithms and representations identified information-theoretically may then guide the design of biologically inspired computing systems (BICS). The material covered includes the necessary introduction to information theory and the estimation of information theoretic quantities from neural data. We then show how to analyze the information encoded in a system about its environment, and also discuss recent methodological developments on the question of how much information each agent carries about the environment either uniquely, or redundantly or synergistically together with others. Last, we introduce the framework of local information dynamics, where information processing is decomposed into component processes of information storage, transfer, and modification -- locally in space and time. We close by discussing example applications of these measures to neural data and other complex systems. ", "text": "unit brain imaging center goethe university frankfurt main germany csiro digital productivity flagship marsﬁeld australia department non-linear dynamics planck institute dynamics self-organization g¨ottingen germany bernstein center computational neuroscience g¨ottingen germany correspondence* michael wibral unit brain imaging center goethe university heinrich-hoffmann strasse frankfurt main germany wibralem.uni-frankfurt.de intelligence broadly deﬁned biologically-inspired computing. usually inspiration drawn neural systems. article shows analyze neural systems using information theory obtain constraints help identify algorithms systems information represent. algorithms representations identiﬁed information-theoretically guide design biologically inspired computing systems material covered includes necessary introduction information theory estimation information theoretic quantities neural data. show analyze information encoded system environment also discuss recent methodological developments question much information agent carries environment either uniquely redundantly synergistically together others. last introduce framework local information dynamics information processing decomposed component processes information storage transfer modiﬁcation computational intelligence broadly deﬁned biologically-inspired computing. often must deal ill-posed problems ﬁeld draws inspiration natural information processing systems cannot afford luxury dismiss problem happens cross path ‘ill-posed’. instead natural systems evolved algorithms approximately solve problems relevant them algorithms adapted often limited resources yield ‘good enough’ solutions. algorithms serve inspiration artiﬁcial information processing systems solve similar problems tight constraints computational power data availability time. inspiration copy incorporate much biological detail possible artiﬁcial system hope also copy emergent information processing biological system. however already small errors copying parameters system compromise success. therefore useful derive inspiration also abstract directly linked information processing carried biological system. gain insight information processing without caring biological implementation? formal language quantitatively describe dissect information processing system provided information theory. particular question exploit fact information theory care nature variables enter computation information processing. thus principle possible treat relevant aspects biological computation biologically inspired computing systems natural framework. here ﬁrst review information theoretic preliminaries. systematically present analyze biological computing systems especially neural systems using methods information theory discuss information theoretic results inspire artiﬁcial computing systems. close brief review studies information theoretic point view served goal. section introduce necessary terminology notation deﬁne basic information theoretic quantities later analyses build experts information theory proceed immediately section discusses information theory neuroscience. terminology notation analyze neural systems biologically-inspired computing systems alike show analysis inspire design other establish common terminology. neural systems bics common property composed various smaller parts interact. parts called agents general also refer neurons brain areas appropriate. collection agents referred system. deﬁne agent system produces observed time series sampled time intervals simplicity choose index measurements {...n} time series understood realization random process random processes collection random variables sorted integer index speciﬁc time described possible outcomes associated probabilities pxt. since probabilities outcome change nonstationary random processes indicate probabilities belong subscript pxt. physical agent conceptualized random process composed collection produce realizations according probability distributions pxt. referring agent notation generalized overview complete notation found table estimation probability distributions stationary non-stationary random processes general probability distributions unknown. since knowledge probability distributions essential computing information theoretic measure probability distributions estimated observed realizations possible form replication processes wish analyze. replications probabilities estimated example counting relative frequencies density estimation agent system random process random variable whenever necessary index detailed stationary processes index omitted. realization random variable speciﬁc outcome random variable probability speciﬁc outcome possible outcomes cyclostationary process cyclostationary random variable stationary process stationary random variable state space representation state space representation superscript minush serves reminder assumed interaction delay processes physical true interaction delay processes random variables referring stimuli responses joint variable entropy conditional entropy information content conditional information content mutual information conditional mutual information note colon used separate random variables compute local mutual information local conditional mutual information comma used separate random variables semicolon used separate sets random variables general probability obtain j-th outcome time estimated replications processes time point i.e. ensemble physical replications systems question. replications often obtained bics multiple simulation runs even physical replications systems question small and/or simple. complex physically embodied bics neural systems generating sufﬁcient number replications process often impossible. therefore either resorts repetitions parts process time generation cyclostationary processes even assumes stationarity. three possibilities discussed following. general repetitions time. random process repeated time probability obtain value estimated observations made sufﬁciently large time points know design experiment process repeated itself. know cyclostationary case. cyclostationarity understood speciﬁc form repeating parts random process repetitions occur regular intervals cyclostationary processes assume t+nt times probability distribution stationary case probability distribution estimated entire measured realizations thus drop subscript index indicating speciﬁc i.e. process stationary also stationarity irrelevant basic information theory based deﬁnitions deﬁne necessary basic information theoretic quantities. focus often neglected local information theoretic quantities become important later start shannon information content realization occupy fraction original probability space. obtaining therefore interpreted informing full realization lies fraction space. thus reduction uncertainty information gained must function ensure subsequent realizations indepenent yield additive amounts information take logarithm ratio obtain shannon information content measures information provided single realization information content speciﬁc realization given already know outcome another variable necessarily independent called conditional information content conditional entropy described various perspectives average amount information making observation already made observation terms uncertainties average remaining uncertainty observed. also information directly obtained conditional entropy used derive amount information directly shared variables mutual information variables total average information variable minus average information variable obtained variable hence mutual information deﬁned measures mutual information averages. although average values used often localized counterparts perfectly valid inspect local values ‘localizability’ fact requirement shannon fano postulated proper information theoretic measures growing trend neuroscience theory distributed computation return local values. measures mutual information localized forms listed following. take expected values local measures obtain mutual conditional mutual information. measures called local allow quantify mutual conditional mutual information single realizations. note however probabilities involved equations global sense representative possible outcomes. words valid probability distribution estimated irrespective whether interested average local information measures. also note local local conditional negative unlike averaged forms occurs local measurement variable misinformative variable i.e. realization lowers probability initial probability means observer expected less observing before occurred nevertheless. therefore misinformative estimating information theoretic quantities data advance speciﬁc information theoretic analyses neural data must stressed estimation information theoretic measures ﬁnite data difﬁcult task. naive estimation probabilities empirically observed frequencies followed plugging probabilities deﬁnitions almost inevitably leads serious bias problems situation improved degree using binless density estimators however ususally statistical testing surrogate data empirical control data necessary judge whether non-zero value measure indicates effect bias signal representation state space reconstruction random processes analyze agents computing system usually memory. means form process longer independent depend variables past. setting proper description process requires look present past jointly. general dependence form smallest collection variables jointly make conditionally independent i.e. sufﬁcient collection past variables also called delay embedding vector always reconstructed scalar observations dimensional deterministic systems shown takens unfortunately real world systems high-dimensional dynamics rather lowdimensional deterministic. systems obvious delay embedding similar taken’s approach would yield desired results. fact many systems require inﬁnite number past random variables scalar observable high-dimensional stochastic process accessible nevertheless behavior scalar observables systems approximated well ﬁnite collection past variables practical purposes words systems approximated well ﬁnite order one-dimensional markov-process according note without proper state space reconstruction information theoretic analyses almost inevitably miscount information random process. indeed importance state space reconstruction cannot overstated; example failure reconstruct states properly lead false positive ﬁndings reversed directions information transfer shown imperfect state space reconstruction also cause failure transfer entropy analysis demonstrated shown impede otherwise clear identiﬁcation coherent moving structures cellular automata information transfer entities useful organize understanding neural computing systems three major levels originally proposed david marr level information theory provides insights level task neural system bics trying solve information processing problem neural system tries solve. problems could example detection edges objects visual scene maintaining information object object longer visual scene. important note questions task level typically revolve around entities direct meaning e.g. objects speciﬁc object properties used stimulus categories operationally deﬁned states concepts attention working memory. example analysis carried purely level investigation whether person behaves optimal bayesian observer algorithmic level entities quantities task level represented neural system system operates representations using algorithms. example neural system represent either absolute luminance changes luminance visual input. algorithm operating either representations example identify object input causing luminance pattern either brute force comparison luminance patterns ever seen alternatively transform luminance representation ﬁltering etc. inferring object targeted comparisons. implementation level representations algorithms implemented neural systems. descriptions level given terms relationship various biophysical properties neural system components e.g. membrane currents voltages morphology neurons spike rates chemical gradients etc. typical study level might example reproducing observed physical behavior neural circuits separation levels understanding served resolve important debates neuroscience also growing awareness speciﬁc shortcoming classic view results obtained careful study levels constrain possibilities level example task winning game tic-tac-toe reached brute force strategy realized mechanical computer alternatively task solved ﬂexible rule realized biological brains young children missing relationships marr’s levels ﬁlled information theory section show link task level implementation level computing various forms mutual information variables levels. mutual informations decomposed contributions agent multi-agent system well information carried jointly. covered section section local information measures link neural activity implementation level components information processing algorithmic level information storage transfer. done agent time step thereby yields sort information theoretic footprint algorithm space time. clear analyses yield footprint– identify algorithm itself. nevertheless footprint useful constraint identifying algorithms neural systems various possible algorithms solve problem clearly differ respect footprint. section covers current attempts deﬁne concept information modiﬁcation. close short review example applications information theoretic analyses neural data describe relate marr’s levels. introduced above information theory serve bridge task level deal properties stimulus task bear direct meaning implementation level recorded physical indices neural activity action potentials. mutual information derivatives thereof answer questions neural systems like these much observer speciﬁc neural response i.e. receiving brain area change beliefs identity stimulus initial belief posterior belief receiving neural response empirical answers questions bear important implications design bics. example encoding enviroment bics maybe modeled neural system successfully lives environment. following paragraphs show answer questions using information theory. neural responses carry information stimuli question easily answered computing mutual information stimulus identity neural responses. despite deceptive simplicity computing mutual information informative neural codes. description constitutes stimulus response rely consider relevant features. example presenting pictures fruit stimulus could compute mutual information neural responses stimuli described versus green fruit described apples versus pears. resulting mutual information differ descriptions stimulus allowing neural system partitions stimuli. likewise could extract features neural responses time ﬁrst spike relative spike times comparing mutual information features allows identify feature carrying information. feature potentially also read internally stages neural system. however investigating individual stimulus response features also keep mind several stimulus response features might considered jointly could carry synergistic information receiving neural response question natural setting bayesian brain theories since question addresses quantity associated speciﬁc response decompose overall mutual information stimulus variable response variable speciﬁc information terms. question difference probability distributions receiving naturally expressed terms kullbackleibler divergence resulting measure called speciﬁc surprise simply indicates incoming response either update beliefs immediately follows cannot additive subsequent responses ﬁrst leads update beliefs second leads revert update i.e. isp. loosely speaking series surprises belief updates necessarily lead better estimate. fact largely overlooked early applications measure neuroscience pointed deweese meister caution therefore necessary interpreting results literature obtained using particular decomposition mutual information. speciﬁc neural response particularly informative unknown stimulus certain stimuli? question asks much knowledge worth terms uncertainty reduction i.e. information gain. contrast question update beliefs above whether update increases reduces uncertainty question naturally expressed terms conditional entropies comparing uncertainty response pir. however individual contributions necessarily positive. response lead probability distribution entropy high entropy accepting ‘negative information’ terms makes measure additive subsequent responses stimulus leads responses informative stimulus itself? words stimulus reliably associated responses relatively unique stimulus know occurrence speciﬁc stimulus response unambiguously. stimuli encoded well system sense lead responses informative downstream observer. type question response considered informative strongly reduces uncertainty stimulus i.e. large informative responses given stimulus average responses stimulus elicits probabilities resulting measure issi called stimulus speciﬁc information pissi meaning issi another valid decomposition mutual information. response speciﬁc information terms composed stimulus speciﬁc information negative stimulus speciﬁc information used investigate stimuli encoded well neurons speciﬁc tuning curve; demonstrated speciﬁc stimuli encoded best changed noise level responses results kind figure stimulus speciﬁc surprise stimulus speciﬁc information orientation tuned model neuron different noise regimes. tuning curve mean ﬁring rate standard deviation versus stimulus orientation repeated clarity. stimulus speciﬁc information issi maximal regions high slope tuning curve noise case; high noise case issi maximal peak tuning curve. corresponding values stimulus speciﬁc surprise relevant conditional probability distributions. figure reproduced creative commons attribution license. immediately visible equations central quantities treatment depend strongly choice stimulus example chooses study human visual systems visual stimuli infrared spectrum likely small analysis futile stimuli correctly point human visual system care code infrared stimuli). hence characterizing neural code properly hinges large extent appropriate choice stimuli. respect safe assume move artiﬁcial stimuli natural ones alter view neural codes future. similar argument holds response features selected analysis. feature dropped measured distort information measures above. even happen dropped feature exact spike time variable seems carry mutual information stimulus variable considered alone i.e. still synergistic information recovered looking response variables jointly example would possible principle neither spike time spike rate carry mutual information stimulus variable considered individually i.e. still considered jointly informative problem omitted response features almost inevitable neuroscience full sampling parts neural systems typically impossible work sub-sampled data. considering subset variables dramatically alter apparent dependency structure neural system example). therefore effects subsampling always kept mind interpreting results studies neural coding. neural systems information often encoded ensembles agents evidenced success various ’brain reading’ decoding techniques applied multivariate neural data knowing information ensemble distributed agents inform designer bics strategies distribute relevant information problem available agents. strategies determine properties like coding capacity system well reliability. example reliable strategy would represent information multiple agents making information redundant. contrast maximizing capacity would require taking account full combinatorial possibilities states agents making coding synergistic. here investigate basic ensemble agents introduce concepts redundant synergistic unique information note encoding larger ensembles still ﬁeld active research. speciﬁcally consider ensemble neurons responses stimulation stimuli answer following questions interestingly questions answered using standard tools information theory mutual information. fact answers questions i.e. quantiﬁcation unique redundant synergistic information need mathematical concepts shown below. present details would like illustrate questions thought experiment three visual neurons recorded simultaneously stimulated four stimuli neurons almost identical receptive ﬁelds third collinear spatially displaced receptive ﬁeld neurons stimulated following stimuli contain anything receptive ﬁelds three neurons neurons stay inactive; small preferred orientation neurons similar small receptive ﬁeld neuron instead long covering receptive ﬁelds example. figure redundant synergistic neural coding. receptive ﬁelds three neurons four stimuli. circuit synergistic coding. responses neurons determine response neuron xor-function. hidden circuit open circles denote excitatory neurons ﬁlled circles inhibitory neurons. numbers circles activation thresholds signed numbers connecting arrows synaptic weights. make things easy encode responses three neurons binary form simply indicating response response window assume stimuli presented equal probability entropy stimulus obviously none information terms larger bits. also neuron shows activity half cases yielding entropy responses neuron. responses three neurons fully specify stimulus therefore mutual information individual neuron’s response stimulus compute this remember number equiprobable outcomes drops half observing single neuron gives hence neuron provides information stimulus considered individually. already here something curious although neuron stimulus together bits. reason ‘vanishing bit’ considering responses pairs neurons especially pair information joint variables formed pairs neurons? ﬁrst look neurons responses stimulus identical. neurons provides information stimulus. even look jointly still information carried responses redundant. this consider cannot decide stimuli result also decide stimuli observing combinations responses occur here. neurons exactly information stimulus measure redundant information yield full case understand concept synergy next consider output responses example neurons transform responses network implements mathematical function downstream neuron output xor-network gets activated small screen neither stimulus long investigate mutual information case individual mutual informations neuron downstream neuron zero however mutual information introduce mathematical framework partial information decomposition formalizes intuition examples consider decomposition mutual information right hand side input variables left hand side variable output variable i.e. general decomposition mutual information unique redundant synergistic information make sense total information variable decomposable unique information term redundant shared information variables similarly total information variables decomposable unique information terms redundant shared information variables synergistic complementary information obtained considering jointly figure shows called partial information decomposition sees redundant unique synergistic information cannot obtained simply subtracting classical mutual information terms. however given either measure redundancy synergy unique information parts decomposition computed. hence classic information theory insufﬁcient partial information decomposition deﬁnition either unique redundant synergistic information based choice axioms needed. minimal requirement axioms measures satisfying them comply intuitive notion unique redundant synergistic information clear extreme cases examples above. original axioms proposed functional deﬁnition redundant information comprises three axioms authors seem agree figure graphical depiction principle behind deﬁnition unique information bertschinger ﬁgure meant guide structure original work consulted rigourous treatment topic. three axioms also lead global positivity i.e. said above axioms uncontroversial although authors restrict input variables detailed axioms however sufﬁcient uniquely deﬁne measure either redundant unique synergistic information. therefore various additional axioms assumptions proposed compatible exemplarily discuss recent choice assumption bertschinger deﬁne measure unique information fact equivalent another formulation proposed grifﬁth koch reasons selecting particular assumption time writing comes richest derived theorems appealing link game theory utility functions thus measures success agent bics. note outset measures deﬁned input variables output details restriction rauh basic idea deﬁnition bertschinger colleagues comes game theory states someone access input variable unique information output variable must able prove variable information available other. prove this alice design output variable someone else access input variable average loose bet. intermediate steps leads assumption unique information depends marginal probability distributions exact full distribution words unique information change space probability conditional mutual joint distribution instead note conditional mutual information change minimum measure unique information stated above knowing three parts enough compute among notable properties measures deﬁned fact found convex optimization three measures explicitly shown positive. moreover measures bounds deﬁnitions synergy redundancy unique ﬁeld information decomposition seen rapid development since initial study williams beer however major questions remain unresolved far. importantly deﬁnitions acceptable properties apply case decomposing mutual information contributions input variables. structure decomposition inputs active area research moment. analyzing neural coding goal functions domain-independent way. analysis neural coding strategies presented relies priori knowledge task level features encoded neural responses implementation level. knowledge information theory help link levels. somewhat similar situation cryptography consider code ‘cracked’ obtain human-readable plain text message i.e. move implementation level task level however happens plain text language never heard case would potentially crack code without ever realizing plain text still meaning sitution neuroscience bears resemblance example last respects first neurons direct access properties outside world rather receive nothing input spike trains. ever learn process must come structure input spike trains. second researchers probe system beyond early sensory motor areas little knowledge actually encoded neurons deeper inside system. result proper instead relying descriptions outside world take point view information processing neuron nothing transformation input spike trains output spike trains. information theory link implementation algorithmic level retrieving ‘footprint’ information processing carried neural circuit. approach builds general agreement neural systems perform least kind information processing. information processing decomposed component processes information storage information transfer information modiﬁcation. decomposition kind already formulated turing recently formalized lizier information storage quantiﬁes information contained past state variable process used process next relatively abstract deﬁnition means least part past information future process potentially transformed. hence information storage naturally quantiﬁed mutual information past future process. information transfer quantiﬁes information contained past state variables xt−u source process used predict information future variable target process context past state variables target process based turing’s general decomposition information processing lizier colleagues recently proposed information theoretic framework quantify distributed computations terms three component processes locally i.e. part system time step framework called local information dynamics successfully applied unravel computation swarms following present global local measures information transfer storage modiﬁcation beginning well established measures information transfer ending highly dynamic ﬁeld information modiﬁcation. conditional mutual information process time xt−u past state-rvs processes respectively. delay variable xt−u indicates past state source taken time steps past account potential physical interaction delay processes. parameter need chosen recently proven bivariate systems estimator maximized parameter equal true delay information transfer relationship allows estimate true interaction delay data simply scanning assumed delay functional linked wiener-granger type causality precisely systems jointly gaussian variables transfer entropy equivalent linear granger causality references therein). however whether assumption jointly gaussian variables appropriate neural setting must checked carefully case fact source signals found non-gaussian transfer entropy estimation probability distributions entering known computed directly. however cases probability distributions derived data. probabilities estimated naively data couting estimates used compute information theoretic quantities transfer entropy speak plug estimator. indeed plug estiamtors used past come serious bias problems therefore newer approaches estimation rely direct estimation entropies decomposed estimators still suffer bias problems lesser degree therefore restrict presentation approaches. proceed estimate reconstruct states processes approach state reconstruction time delay embedding uses past variables xt−nτ spaced time interval number variables optimal spacing determined using established criteria realizations states variables represented vectors form subscript reminder past states target constructed conditioning optimal sense taking active information storage target correctly account condition instead self prediction would optimal transfer entropy would overestimated. entropies estimated efﬁciently nearest-neighbor techniques. techniques exploit fact distances neighboring data points given embedding space inversely related local probability density higher local probability density around observed data point closer next neighbors. since next neighbor estimators data efﬁcient allow estimate entropies high-dimensional spaces limited real data. unfortunately problematic estimate simply applying naive nearest-neighbor estimator entropy kozachenko-leonenko estimator separately terms appearing equation reason dimensionality state spaces involved equation differ largely across terms creating bias problems. overcome kraskov-st¨ogbauer-grassberger estimator ﬁxes number neighbors highest dimensional space projecting resulting distances lower dimensional spaces range look additional neighbors adapting technique formula suggested estimator angle brackets indicate averaging time denotes digamma function stationary systems ensemble replications non-stationary ones number nearest neighbors used estimation. refers number neighbors within hypercube deﬁnes search range around state vector. described above size hypercube marginal spaces deﬁned based distance k-th nearest neighbor highest dimensional space. algorithmic level. describes computation algorithmic level level physical dynamical system. optimal inference causal interactions although used purpose past. fundamental reason information transfer relies causal interactions causal interactions necessarily lead nonzero information transfer instead causal interactions serve active information storage alone force systems identical synchronization information transfer becomes effectively zero. might summarized stating transfer entropy limited effects causal interaction source target process unpredictable given past target process alone. sense seen quantifying causal interactions currently communication aspect distributed computation. therefore measures predictive algorithmic information transfer. simple thought experiment serve illustrate point plays unknown record chain causal interactions serve transfer information music record brain. causal interactions happen record’s grooves needle magnetic transducer system behind needle conversion pressure modulations neural signals cochlea ﬁnally activate cortex. situation undeniably information transfer information read source record given moment known target process i.e. neural activity cochlea. however information transfer ceases record crack making needle skip repeat certain part music. obviously information transferred certain mild conditions equivalent information transfer all. interestingly analysis sound cochlear activity yield result repetitive sound leads repetitive neural activity neural activity thus predictable it’s past condition vanishing neural ‘noise’ leaving room prediction improvement sound source signal. hence obtain zero correct result conceptual point view. remarkably time chain causal interactions remains practically unchanged. therefore causal model able data original situation problem data situation cracked record well. again conceptually correct result time causal point view. difference analysis information transfer computational sense causality analysis based interventions demonstrated convincingly recent study lizier prokopenko authors also demonstrated analysis information transfer yield better insight analysis causal interactions computation system understood. difference causality information transfer also reﬂected fact single causal structure support diverse pattern information transfer pattern information transfer realized different causal structures shown battaglia local information transfer transfer entropy formally conditional mutual information obtain corresponding local conditional mutual information equation quantity called local transfer entropy realizations processes time reads said earlier section basic information theory local information measures eliminate need appropriate estimation probability distributions involved. hence non-stationary process distributions still estimated ensemble approach time point involved e.g. physical replications system enforcing cyclostationarity design experiment. analysis local transfer entropy applied great success study cellular automata conﬁrm conjecture certain coherent spatio-temporal structures traveling network indeed main carriers information transfer similarly local transfer entropy identiﬁed coherent propagating wave structures ﬂocks information cascades indicated impending synchronization amongst coupled oscillators common problems solutions typical problems estimation encompass ﬁnite sample bias presence non-stationarities data need multivariate analyses. recent years problems addressed least isolation summarized below finite sample bias overcome statistical testing using surrogate data observed realizations reassigned process temporal order underlying information transfer destroyed reassignment conserve many data features single process realizations possible. already explained section basic information theory above non-stationary random processes principle require necessary estimates probabilities equation based physical replications systems question. impossible experimenter design experiment processes repeated time. cyclostationary data available estimated using ensemble methods described implemented trentool toolbox restricted presentation transfer entropy estimation case interacting random processes i.e. bivariate analysis. setting realistic neuroscience deals large networks interacting processes case various complications arise analysis performed bivariate manner. example process could transfer information different delays δz→x δz→y processes case pairwise analysis transfer entropy yield apparent information transfer process receives information shorter delay receives longer delay similar problem arises information transferred ﬁrst process case bivariate analysis also indicate information transfer moreover sources transfer information purely synergistically i.e. transfer entropy source alone target zero considering jointly reveals information transfer. however even small networks random processes joint state space variables xt−u become intractably large estimation perspective. moreover problem ﬁnding information transfers network either single sources variables target synergistic transfer collections source variables target combinatorial problem therefore typically solved reasonable time. therefore faes lizier rubinov stramaglia suggested analyze information transfer network iteratively selecting information sources target iteration either based magnitude apparent information transfer signiﬁcance next iteration already selected information sources added conditioning next search information sources started. approach stramaglia colleagues particular conditional mutual information terms computed level series expansion following suggestion bettencourt allows efﬁcient computation series truncate early search proceed next level. importantly approaches also consider synergistic information transfer source variable target. example variable transferring information purely synergistically maybe included next iteration given variables transfers information already again cryptography serve example here. encrypted message received discernible information transfer encrypted message plain text without key. information transfer alone plain text. encrypted message combined relation combination encrypted message side plain text side revealed. conditioning however currently explicit indication approaches faes lizier rubinov whether multivariate information transfer sources target fact synergistic; addition redundant links included. contrast redundant synergistic multiplets variables transferring information target identiﬁed approach stramaglia looking sign contribution multiplet. unfortunately also possibility cancellation types multivariate information present. present explicit measures active information storage comments serve avoid misunderstanding. since analyze neural activity here measures active information storage concerned information stored activity rather synaptic properties example. laid above storage conceptualized mutual information past future states neural activity. clear much information storage information contained future states neural activity general. hand future states rich information bear relation past states i.e. unpredictable information storage low. hence large information storage occurs activity rich information time predictable. thus information storage gives deﬁne predictability process independent prediction error information storage quantiﬁes much future information process predicted past whereas prediction error measures much information predicted. quantiﬁed information measures i.e. bits error predicted information total amount information random variable process. importantly measures lead quite different views predictability process. total information vary considerably process predictable unpredictable information thus vary almost independently. important design bics predictive coding strategies. turning explicit deﬁnition measures information storage worth considering temporal extent ’past’ ’future’ states interested globally predictive information excess entropy mutual information semi-inﬁnite past semi-inﬁnite future process time point contrast interested information currently used next step process mutual information semi-inﬁnite past next step process active information storage greater interest. measures deﬁned next paragraphs. k→∞i limit span semi-inﬁnite past future respectively. general mutual information equation evaluated multiple realizations process. stationary process however timedependent equation rewritten average time points computed single realization process least principle process quantify information even nonstationary processes investigate local active storage values given corresponding probability distributions properly obtained ensemble realizations interpretation information storage measure algorithmic level laid information storage measure amount information process predictable past. quantiﬁes example well activity brain area predicted another area e.g. learning statistics. hence questions information storage arise naturally asking generation predictions brain e.g. predictive coding. measures local active information storage local transfer entropy introduced preceding section fruitfully combined pairing storage transfer values point time agent. resulting space termed local information dynamics state space used investigate computational capabilities cellular automata pairing pair sources targets time point here suggest concept used disentangle various neural processing strategies. speciﬁcally suggest pair local active information storage inputs target outgoing local information transfers target targets agent time point resulting point used answer important question whether aggregate outgoing information transfer agent high either predictable surprising input. former information processing function amounts sort ﬁltering passing reliable information would linked something reliable represented activity. latter information processing function form prediction error encoding high outgoing information transfer triggered surprising unpredictable information received note type analysis recordings least triplets connected agents necessary. pose considerable challenge experimental neuroscience extremely valuable disentangle information processing goal functions various cortical layers example. type analysis also valuable understand information processing evolved bics systems availability data triplets agents problem. figure various information processing regimes information state space. lais local active information storage input outgoing local transfer entropy. represents values agent time step. langton described information modiﬁcation interaction transmitted and/or stored information results modiﬁcation other. attempts deﬁne information modiﬁcation rigorously implemented basic idea. first attempts deﬁning quantitative measure information modiﬁcation resulted heuristic measure termed local separable information local active information storage pairwise local transfer entropies target taken vxt\\xt− {zt− zt−g} indicating past state variables processes zt−i transfer information target variable note history target explicitly part set. index reminder past state variables taken account i.e. shown above local measures entering negative mis-informative future target. eventually overall separable information might also negative indicating neither pairwise information transfers history could explain information contained target’s future. interpreted modiﬁcation either stored transferred information. ﬁrst attempt provided valuable insights systems like elementary cellular automata ultimately heuristic. rigorous approach look decomposition local information realization random variable shed light issue part information modiﬁcation. view overall information future target process explained looking sources information history target jointly least genuinely stochastic part target shown lizier contrast cannot decompose information pairwise mutual information terms only. described following remainder exhausting pairwise terms synergistic information information sources motivated suggestion deﬁne information modiﬁcation based synergy differences decomposition considering variables jointly pairwise terms consider series subsets formed variables zt−i transfer information target except variables target’s history. bold typeface zt−i reminder work state space representation necessary. following \\xt− derivation lizier create series subsets {zt− zt−g−} i.e. g-th subset contains ﬁrst sources. decompose collective transfer entropy source variables series conditional mutual information terms incrementally increasing condition compare equation difference potentially mis-informative equation fully accounted information equation lies conditioning local transfer entropies. means context source variables provide neglected synergies redundancies properly accounted for. importantly results equations identical information provided either redundantly synergistically sources zt−g. observation lizier colleagues propose rigorously deﬁned measure information modiﬁcation based synergistic part information transfer source variables zt−g targets history target deﬁnition information modiﬁcation several highly desirable properties. however relies suitable deﬁnition synergy currently available case source variables currently considerable debate deﬁne part mutual information synergistically provided larger source variables question best measure information modiﬁcation maybe considered open. here present recent applications neural data estimation strategies pdfs. both estimation done using java information dynamics toolkit state space reconstruction performed trentool ﬁrst study investigated magnetoencephalographic source signals patients autism spectrum disorder reported reduction hippocampus patients compared healthy controls study strategy obtaining estimate baseline data guarantee stationarity data. results study align well predictive coding theories references therein). signiﬁcance study current context lies figure patients compared controls. investigated source locations whisker plot lais source signiﬁcant differences patients controls found. modiﬁed creative commons attribution license second study analyzed lais voltage sensitive imaging data visual cortex. study found lais baseline onset visual stimulus negative lais directly stimulus onset sustained increases lais whole stimulation period despite changing signal amplitude study available data pooled baseline well stimulation periods also across recording sites pooling across time unusual reasonable insofar neurons also deal nonstationarities arise measure neurally accessible lais reﬂect this. pooling across sites study motivated argument neural pools seen pixels capable dynamic transitions brain area. thus pixels treated physical replications estimation pdf. evaluation strategy study applicable nonstationary data delivers results strongly depend data included. future application therefore needs informed precise estimates time scales neurons sample input statistics. recurrent neural networks consist reservoir nodes artiﬁcial neurons connected recurrent network structure typically structure constructed random output neurons connections trained perform given task. approach becoming increasingly popular non-linear time-series modeling robotic applications intrinsic plasticity based techniques known assist performance rnns general although method still outperformed memory capacity tasks example implementation certain changes network structure address issue dasgupta on-line rule adapt leak-rate neuron based internal state. leak-rate reduced certain threshold increased above. technique shown improve performance delayed memory tasks benchmark tests embodied wheeled hexapod robots. dasgupta describe effect technique speeding slowing dynamics reservoir based time-scale input signal. terms marr’s levels also view intervention algorithmic level directly adjusting level information storage system order affect higher-level computational goal enhanced performance memory capacity tasks. particularly interesting note connection information storage features across different levels here. conjectured brain operate self-organized critical state recent evidence demonstrates human brain least close criticality albeit slightly sub-critical prompts question advantages would delivered operating critical state. dynamical systems perspective suggest balance stability perturbation spreading regime gives rise scale-free correlations emergent structures associate computation natural systems. information dynamics perspective suggest critical regime represents balance capabilities information storage several studies upheld interpretation maximised balanced information processing properties near critical regime. study random boolean networks shown optimal balance near critical point echoed ﬁndings recurrent neural networks maximisation transfer entropy ising model maximization entropy neural models recordings marr’s perspective algorithmic level optimal balance information processing operations yields emergent scale-free structures associated critical regime implementation level. reﬂects ties marr’s levels described section theoretical ﬁndings computational properties critical point great relevance neuroscience aforementioned importance criticality ﬁeld. cellular automata discrete dynamical systems array cells synchronously update value function ﬁxed number spatial neighbours cells using uniform rule classic complex system where despite simplicity emergent structures arise. include gliders coherent structures moving regular background domains. gliders interactions formed basis analysis cellular automata canonical examples nature-inspired distributed information processing particular gliders conjectured transmit information across static gliders store information collisions interactions process information computing macro-scale dynamics local transfer entropy active information storage separable information applied produce spatiotemporal local information dynamics proﬁles series experiments results experiments conﬁrmed long-held conjectures gliders dominant information transfer entities blinkers background domains dominant information storage components glider/particle collisions dominant information modiﬁcation events. results crucial demonstrating alignment figure local information dynamics rule φpar. local information dynamics rule φpar values displayed time steps displayed cells starting initial random state. notice short initial transient occurs emergent structures arise. spatiotemporal information dynamics plots history length units bits. have local active information storage local apparent transfer entropy cell left local complete transfer entropy cell left qualitative understanding emergent information processing complex systems ability quantify information processing measures. insights could gained using local information measures studying averages alone tells nothing presence spatiotemporal structures. purposes crucial step extension analysis rule evolved perform density classiﬁcation task outlined since interpret marr’s levels spatiotemporal proﬁles local information dynamics sample density classiﬁcation rule shown figure reproduced using demofrontiersbitsfrombiology.m script demos/octave/cellularautomata demonstration distributed java information dynamics toolkit example classiﬁcation density initial state clear goal computation algorithmic level local information dynamics analysis allowed direct identiﬁcation roles emergent structures arising short initial transient figure example analysis revealed markers regions identiﬁed local majorities regions identiﬁed storing information figure analysis also quantiﬁes role several glider types communicating presence local majorities strength majorities figure role glider collisions resolving competing local majorities. swarming ﬂocking refers collective behaviour exhibited movement group animals including emergence patterns structures cascades perturbations travelling wave-like manner splitting reforming groups group avoidance obstacles. behaviour thought provide biological advantages e.g. protection predators. realistic simulation swarm behaviour generated using three simple rules individuals swarm based separation alignment cohesion others wang analysed local information storage transfer dynamics exhibited patterns motion swarm model based time-series headings speeds individual. importantly analysis quantitatively revealed coherent cascades motion swarm waves large coherent information transfer bikhchandani information cascades analagous gliders viewed using marr’s levels similar algorithmic role carrying information coherently efﬁciently across swarm implementation information simply relative heading speed individuals. goal computation swarm depends current environment avoid predators efﬁciently transport whole group nesting food sites. lizier inverted usual transfer entropy applying ﬁrst time ﬁtness function evolution adaptive behaviour example guided self-organisation experiment utilised snakebot snake-like robot separately controlled modules along body whose individual actuation evolved genetic programming maximise transfer entropy adjacent modules. actual motion snake emerged interaction modules environment. approach result particularly fastmoving snake result coherent travelling information waves along snake revealed local transfer entropy. coherent information waves akin gliders cascades swarms suggesting waves emerge resonant mode evolution information ﬂow. robust optimal coherent communication long distances simple construct evolutionary steps. again marr’s levels identify goal computation transfer information snake’s modules algorithmic level coherent waves carry information efﬁciently along snake’s whole body implementation simply attempted actuation modules joints interaction neural systems perform acts information processing form distributed computation many complex computations emergent information processing capabilities remain mysterious date. information theory help advance understanding ways. hand neural information processing decomposed component processes information storage transfer modiﬁcation using information theoretic tools. allows derive constraints possible algorithms served observed neural dynamics. hand representations algorithms operate guessed analyzing mutual information humanunderstandable descriptions relevant concepts quantities experiments indices neural activity. helps identify parts real world neural systems care for. however care must taken asking questions neural codes question neurons code jointly solved completely date. taken together knowledge representations possible algorithms describes operational principles neural systems marr’s algorithmic level hint solutions solving ill-deﬁned real world problems biologically inspired computing systems face constrained resources. funding supported loewe-grant neuronale koordination forschungsschwerpunkt frankfurt. received ﬁnancial support german ministry education research bernstein center computational neuroscience g¨ottingen grant gqb. battaglia function follows dynamics state-dependency directed functional inﬂuences wibral vicente lizier eds. directed information measures neuroscience understanding complex systems besserve scholkopf logothetis panzeri causal relationships frequency bands extracellular signals visual cortex revealed information theoretic analysis. comput neurosci besserve schlkopf logothetis panzeri causal relationships frequency bands extracellular signals visual cortex revealed information theoretic analysis. comput neurosci carandini circuits behavior bridge far? neurosci ceguerra lizier zomaya information storage transfer synchronization process locally-connected networks ieee symposium artiﬁcial life couzin james croft krause social organization information transfer schooling ﬁshes brown laland krause eds. fish cognition behavior fish aquatic resources faes marinazzo montalto nollo lag-speciﬁc transfer entropy tool assess cardiovascular cardiorespiratory information transfer. ieee trans biomed doi./tbme.. faes nollo bivariate nonlinear prediction quantify strength complex dynamical interactions short-term cardiovascular variability. biol comput faes nollo porta information-based detection nonlinear granger causality multivariate processes nonuniform embedding technique. phys stat nonlin soft matter phys faes porta conditional entropy-based evaluation information dynamics physiological systems directed information measures neuroscience doi. /---- garofalo nieus massobrio martinoia evaluation performance information theory-based methods cross-correlation estimate functional connectivity cortical networks. plos gomez lizier schaum wollstadt gr¨utzner uhlhaas reduced predictable information brain signals autism spectrum disorder frontiers neuroinformatics grifﬁth koch quantifying synergistic mutual information prokopenko guided self-organization inception volume emergence complexity computation hadjipapas hillebrand holliday singh barnes assessing interactions linear nonlinear neuronal sources using beamformers proof concept. clin neurophysiol hansen heiland lumsdaine litke beggs extending transfer entropy improves identiﬁcation effective connectivity spiking cortical network model. plos kawasaki mori kobata kitajo transcranial magnetic stimulationinduced global propagation transient phase resetting associated directional information ﬂow. front neurosci doi./fnhum.. lizier measuring dynamics information processing local scale time space wibral vicente lizier eds. directed information measures neuroscience understanding complex systems lizier heinzle horstmann haynes j.-d. prokopenko multivariate information-theoretic measures reveal directed information structure task relevant changes fmri connectivity. comput neurosci lizier prokopenko zomaya information dynamics phase transitions random boolean networks bullock noble watson bedau eds. proceedings eleventh international conference simulation synthesis living systems winchester lizier prokopenko zomaya framework local information dynamics distributed computation complex systems prokopenko guided selforganization inception volume emergence complexity computation lizier rubinov multivariate construction effective computational networks observational data planck preprint planck institute mathematics sciences marinazzo gosseries boly ledoux rosanova massimini directed information transfer scalp electroencephalographic recordings insights disorders consciousness. clin neurosci doi./ marinazzo pellicoro stramaglia information transfer brain insights uniﬁed approach wibral vicente lizier eds. directed information measures neuroscience understanding complex systems orlandi stetter soriano geisel battaglia transfer entropy reconstruction labeling neuronal connections simulated calcium imaging. plos doi. /journal.pone. porta faes bari marchi bassani nollo effect complexity causality cardiovascular control comparison model-based model-free approaches plos doi./journal.pone. priesemann valderrama wibral quyen neuronal avalanches differ wakefulness deep sleep–evidence intracranial depth recordings humans. plos comput biol priesemann wibral valderrama pr¨opper quyen geisel spike avalanches vivo suggest driven slightly subcritical brain state frontiers systems neuroscience pmid reynolds flocks herds schools distributed behavioral model siggraph proceedings annual conference computer graphics interactive techniques volume volume rowan neymotin lytton electrostimulation reduce synaptic scaling driven progression alzheimer’s disease frontiers computational neuroscience doi./ fncom.. sabesan good tsakalis spanias treiman iasemidis information application epileptogenic focus localization intracranial eeg. ieee trans neural syst rehabil schreiber measuring information transfer phys lett shannon weaver mathematical theory communication shew plenz functional beneﬁts criticality cortex. neuroscientist smirnov spurious causalities transfer entropy physical review staniek lehnertz symbolic transfer entropy inferring directionality biosignals. takens detecting strange attractors turbulence rand l.-s. young eds. dynamical systems turbulence warwick volume lecture notes mathematics chapter thivierge j.-p. scale-free economical features functional connectivity neuronal networks. phys stat nonlin soft matter phys doihttp//dx.doi.org/./ physreve.. untergehrer jordan kochs schneider fronto-parietal connectivity non-static phenomenon characteristic changes unconsciousness. plos doi./journal.pone. vakorin mii? krakovska mcintosh empirical theoretical aspects generation transfer information neuromagnetic source network. front syst neurosci mierlo papadopoulou carrette boon vandenberghe vonck functional brain connectivity epilepsy seizure prediction epileptogenic focus localization progress neurobiology doi./j.pneurobio... varon montalto jansen lagae marinazzo faes interictal cardiorespiratory variability temporal lobe absence epilepsy childhood proc. conference european study group cardiovascular osciliations accepted wibral lizier v¨ogler priesemann galuske local active information storage tool understand distributed neural information processing frontiers neuroinformatics wibral rahm rieder lindner vicente kaiser transfer entropy magnetoencephalographic data quantifying information cortical cerebellar networks. prog biophys biol wibral turi linden kaiser bledowski decomposition working memory-related scalp erps crossvalidation fmri-constrained source analysis ica. psychophysiol wibral vicente lindner transfer entropy neuroscience wibral vicente lizier eds. directed information measures neuroscience understanding complex systems", "year": "2014"}