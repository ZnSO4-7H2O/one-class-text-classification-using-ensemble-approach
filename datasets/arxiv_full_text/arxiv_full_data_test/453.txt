{"title": "Quantifying multivariate redundancy with maximum entropy decompositions  of mutual information", "tag": "q-bio", "abstract": " Williams and Beer (2010) proposed a nonnegative mutual information decomposition, based on the construction of redundancy lattices, which allows separating the information that a set of variables contains about a target variable into nonnegative components interpretable as the unique information of some variables not provided by others as well as redundant and synergistic components. However, the definition of multivariate measures of redundancy that comply with nonnegativity and conform to certain axioms that capture conceptually desirable properties of redundancy has proven to be elusive. We here present a procedure to determine nonnegative multivariate redundancy measures, within the maximum entropy framework. In particular, we generalize existing bivariate maximum entropy measures of redundancy and unique information, defining measures of the redundant information that a group of variables has about a target, and of the unique redundant information that a group of variables has about a target that is not redundant with information from another group. The two key ingredients for this approach are: First, the identification of a type of constraints on entropy maximization that allows isolating components of redundancy and unique redundancy by mirroring them to synergy components. Second, the construction of rooted tree-based decompositions of the mutual information, which conform to the axioms of the redundancy lattice by the local implementation at each tree node of binary unfoldings of the information using hierarchically related maximum entropy constraints. Altogether, the proposed measures quantify the different multivariate redundancy contributions of a nonnegative mutual information decomposition consistent with the redundancy lattice. ", "text": "department neurobiology harvard medical school boston neural computation laboratory center neuroscience cognitive systemsunitn istituto italiano tecnologia rovereto italy.∗ williams beer proposed nonnegative mutual information decomposition based construction redundancy lattices allows separating information variables contains target variable nonnegative components interpretable unique information variables provided others well redundant synergistic components. however deﬁnition multivariate measures redundancy comply nonnegativity conform certain axioms capture conceptually desirable properties redundancy proven elusive. present procedure determine nonnegative multivariate redundancy measures within maximum entropy framework. particular generalize existing bivariate maximum entropy measures redundancy unique information deﬁning measures redundant information group variables target unique redundant information group variables target redundant information another group. ingredients approach first identiﬁcation type constraints entropy maximization allows isolating components redundancy unique redundancy mirroring synergy components. second construction rooted tree-based decompositions mutual information conform axioms redundancy lattice local implementation tree node binary unfoldings information using hierarchically related maximum entropy constraints. altogether proposed measures quantify different multivariate redundancy contributions nonnegative mutual information decomposition consistent redundancy lattice. understanding distributed among components multivariate system information external variable reciprocal information parts help characterize infer underlying mechanisms function system. objective motivated introduction different methods break components joint entropy variables break contributions variables mutual information target variable methods many applications study complex systems biological domain genes networks neural coding well social domain collective behaviour decision agents study artiﬁcial agents particular consider target variables anset variables information want study. important question determine information distributed refers much information redundant across variables alternatively obtained synergistically joint observation variables amount redundant synergistic information implications example assess robust representation complex decode information reduce dimensionality preserving information several decompositions proposed address question breaking total information based role correlations variables explained generally separating inﬂuence dependencies different orders using maximum entropy models models synergy associated information retrieved considering high-order moments joint distribution variables. conversely redundancy traditionally quantiﬁed measure called interaction information co-information however measure cannot separate effect redundancy synergy predominant related sign measure. framework jointly quantify synergy redundancy seminal work introduced approach decompose mutual information variables target nonnegative contributions differentiate synergy redundancy. simplest bivariate formulation mutual information decomposed four terms redundancy component variables terms corresponding information unique respectively information obtained variables alone alone synergy term corresponding information unique joint source respect variables alone. decomposition separates redundancy synergy consistently leads express measure co-information difference between redundancy synergy terms. framework generally allows building type decomposition multivariate variables linchpin ingredients deﬁnition general measure redundancy fulﬁlls axioms capture abstract notion redundancy construction redundancy lattice reﬂects partial ordering different redundancy terms results axioms different elements framework gathered different degrees consensus. separation mutual information nonnegative components differentiate redundancy synergy adopted many others deﬁnition measures redundancy still topic ongoing research. argued original redundancy measure quantiﬁes common amounts information qualitatively common information sources share target condition ensure qualitative redundancy captured axiom named identity axiom formulated measures fulﬁll axiom proposed underpin decomposition alternative proposals take basic component derive terms decomposition anmeasure redundancy measure synergy unique information moreover subsequent studies pointed candidate properties fulﬁlled currently consensus properties imposed whether preeminent measure redundancy several respond complementary irreducible notions. speciﬁc systems gaussian systems univariate targets shown several proposed measures actually equivalent hand proposals depart substantially original framework either adopting principles considering existence negative components associated misinformation implementing decompositions joint entropy instead mutual information difﬁculties decompose mutual information redundant synergistic components substantially aggravated multivariate case. measures synergy easily generalized multivariate case particular within maximum entropy framework redundancy measures proposed deﬁned bivariate case allow negative components work introduced equivalent procedures construct multivariate redundancy measures either exploiting connections between lattices formed different number variables exploiting dual connection redundancy lattices information loss lattices synergy naturally deﬁned. however procedures general framework guarantee construction nonnegativity multivariate redundancy measures assessed separately speciﬁc measure. furthermore showed counterexample involving trivariate system deterministic dependencies target primary sources nonnegativity ensured terms redundancy lattice imposing identity axiom. work generally studied effect deterministic target-source dependencies. indicated general negative terms originate dependencies measures used build decomposition comply redundancy axioms information identity criterion subsumes identity axiom generalizes multivariate case. criterion assumes different pieces information target speciﬁcally associated different source variables. remains open question whether lack nonnegative decomposition signature certain target-source dependencies meaningful nonnegative decompositions could obtained replacing identity criterion. work propose procedure determine multivariate redundancy measures within maximum entropy framework thus extending bivariate approach focus maximum entropy approach motivated preeminent role bivariate case provides bounds actual redundancy unique information synergy terms reasonable assumptions shared measures particular showed that assumed bivariate nonnegative decomposition exists redundancy determined bivariate distributions target source maximum entropy measures provide lower bound actual synergy redundancy terms upper bound actual unique information. furthermore assumptions above bivariate distributions compatible potentially synergy maximum entropy decomposition retrieves bounds exact actual terms. show preeminent role maximum entropy decomposition also holds multivariate case analogous assumptions. construct maximum entropy multivariate mutual information decompositions deﬁne measures redundancy variables also unique redundant information variables redundant another variables. formulation within maximum entropy framework allows obtaining close-form general deﬁnitions multivariate redundancy measures contrast ensures nonnegativity measures construction imposing hierarchical constraints entropy maximization. ingredient deﬁne measures identiﬁcation type entropy maximization constraint using co-information measures allows separating components multivariate redundancy. terms mutual information decomposition nonnegative constraints used deﬁne different measures consistent across measures multivariate redundancy measures implement decomposition breaks redundancy contributions mutual information agreement redundancy axioms particular measures related forming rooted tree decomposition different redundancy terms locally binary unfolded using hierarchically related maximum entropy constraints. oppositely systems target source dependencies allow nonnegative mutual information decomposition compatible maximum entropy approach show hierarchy constraints lost. case measures still calculated nonnegatively deﬁned consistency mutual information decomposition longer hold interpretability quantifying different redundancy contributions impaired. section revise principles mutual information decomposition bivariate maximum entropy measures section revisit measures make apparent generalized multivariate case. particular identify entropy maximization constraints co-information allow isolating redundancy components. section develop general multivariate measures. indicate preeminent role maximum entropy decomposition holds also multivariate case start trivariate case present general development multivariate redundancy decompositions provide general formulas effect constraints co-information conditional co-information derive multivariate measures redundancy unique redundancy. show measures implement nonnegative rooted tree decomposition mutual information local binary unfoldings information tree node using hierarchically related maximum entropy constraints. section show presence negative terms impairs consistency decomposition breaking hierarchy constraints. seminal work introduced approach decompose mutual information nonnegative contributions. consider ﬁrst bivariate case target variable source variables work showed mutual information variable expressed similarly term refers redundancy component variables obtained either knowing separately. terms quantify component information unique respectively information obtained variables alone cannot obtained alone. furthermore joint information expressed refers synergistic information variables unique joint source respect variables alone. therefore given standard information-theoretic chain rule equalities analogously conditioning variable removes redundant component information adds synergistic component resulting conditional information unique synergistic terms. note decomposition redundancy synergy component exist simultaneously. fact showed measure co-information previously used quantify synergy redundancy deﬁned i+i−i assignment equals difference redundancy synergy terms generally deﬁned decompositions mutual information target multivariate variables ingredient deﬁnition general measure redundancy construction redundancy lattice. summarize elements approach relevant development rooted tree decompositions. decompose information deﬁned source subset variables collection sources. introduced measure redundancy quantify collection redundancy between sources composing collection constructed redundancy lattice reﬂects relation redundancies different collections. generically refer redundancy collection furthermore following concise notation example instead writing {}{} collection composed source containing variable source containing write save curly brackets indicate sets variables instead separate sources. argued measure redundancy comply following axioms monotonicity property allows introducing partial ordering collections reﬂected redundancy lattice. self-redundancy connects lattice decomposition mutual information lattice collection formed single source including variables furthermore number collections included lattice restricted refers collections lower equal partial ordering hence reachable descending lattice decomposition total mutual information results applying collection decompositions eqs. particular cases partial information measures obtained inverting applying m¨obius inversion bivariate case partial information measures identiﬁed redundant unique synergistic terms respectively. oppositely multivariate case partial information measures quantify general contributions result mixture notions represent general part redundancy unique collection respect others. show idea separating unique common parts redundancy core tree decompositions. work argued original measure redundancy suited quantiﬁes common amounts information introduced axiom necessary condition qualitatively quantify redundancy axiom motivated proposal several alternative measures redundancy measures focus maximum entropy-based redundancy measure associated mutual information decomposition develop rooted tree decomposition multivariate extension. motivated preeminent role bivariate maximum entropy decomposition relation underlying actual decomposition show also holds multivariate case. bivariate decomposition complies identity axiom also generalization introduced distributions preserve bivariate marginals involving also includes original distribution assumed unique information redundancy invariant within synergy depends speciﬁc form trivariate joint distributions. synergy determined difference information figure bivariate trivariate redundancy lattices lattices reﬂect partial ordering deﬁned labels indicate mapping trivariate bivariate lattice. particular nodes label trivariate lattice accumulated corresponding node bivariate lattice. p−{∅} nonempty subsets nonempty sources formed domain reﬂects symmetry axiom distinguish order sources. collections deﬁned partial ordering relation construct lattice collections source source subset source. partial ordering relation reﬂexive transitive antisymmetric. fact consistency redundancy measures partial ordering collections represents stronger condition monotonicity axiom. monotonicity axiom considers cases obtained adding sources partial ordering comprises also removal variables sources redundancy lattices case bivariate trivariate shown figure indicating correspondence three axioms partial ordering relations reﬂected lattices also underpin multivariate redundancy measures proposed work. mutual information decomposition constructed implicitly deﬁning partial information measures associated node redundancy latabbreviate minimization constraint. formulation terms maximum entropy possible preserved according maximum entropy interpretation synergy nonnegatively deﬁned. using relations eqs. components decomposition derived proved measures compose nonnegative decomposition mutual information. nonnegativity synergistic unique information measures apparent expressions above redundancy checked showing least distribution within co-information nonnegative. maximum entropy measures impose certain bounds underlying actual terms redundancy synergy unique information estimate particular assumptions actual decomposition exists actual synergy nonnegative actual redundancy determined bivariate distributions target source indicates actual terms. bounds become equalities distribution within case maximum entropy measures retrieve exactly actual decomposition interpreted following indicates conditional mutual information decomposes unique synergistic component. since distributions constrained maintain joint distribution minimization gets synergy selecting distributions allows getting unique information equal conditional information. since given mutual information variable redundancy unique component redundancy retrieved mutual information minus conditional. show especial relation actual decomposition maximum entropy estimation also holds multivariate case. figure rooted trees associated decomposition mutual information variables target tree formed adding variable variable shaded elliptical area indicates unfolding information already present previous adding variable information component redundant variable component unique respect graphical representation effect constraint imposing null co-information preserving marginal distributions variables target. arrow indicates redundancy mirrored otherwise minimized synergistic component permute order addition variables. adjunct lattice rooted trees. lattice structure bivariate redundancy lattice edge reﬂects existence edge least trees. ﬁrst step towards deﬁning multivariate redundancy decompositions ﬁrst revisit bivariate maximum entropy decompositions interpreting associated rooted trees. purpose consider start access variables variable accordingly information variables establish part redundant unique. adding unfolds components indicated shaded ellipse figure information split component redundant component unique respect furthermore information separated components unique information respect redundant unique information respect unique information respect also unique respect unique information respect tion respectively. accordingly guarantee terms decomposition nonnegative locally check binary unfoldings nonnegative components. works start reexpressing measures eqs. synergy expressed before comparison information original distribution minimum information unique informations expressed difference informations within equivalence previous expressions holds marginal informations preserved within redundancy formulated comparison informations hierarchical minimizations hence nonnegativity directly apparent contrast particular introduce equivalence checked using deﬁnition co-information equality resulting nonnegativity guaranteed minimizations within sets figure provides intuition redundancy quantiﬁed mentioned above tends eliminate synergy component original distribution. conversely redundancy constant within therefore imposing co-information vanishes since equal redundancy minus synergy enforces select distributions synergy component eliminated mirrors redundancy component. variables case pairs resulting unfolding interchanged. allows associating trees common adjunct lattice equivalent deﬁned edge lattice indicates edge present least trees. compare maximum entropy decomposition approaches based maximum entropy construction synergistic term equivalent usual maximum entropy decompositions marginal distributions certain order preserved. however strategy cannot identify redundancy terms embedded information kept maintaining bivariate marginals involving variable type constraint coinformation allows gaining access redundancy terms mirroring synergistic component otherwise would minimized given constraint preserve high-order marginals distribution. generalize multivariate case procedure construct rooted tree maximum entropy decompositions. ﬁrst indicate preeminent role bivariate maximum entropy decompositions also holds multivariate case then gain intuition examine detail trivariate case section introduce general multivariate redundancy measures describe multivariate decompositions. described section bivariate maximum entropy decomposition retrieves actual decomposition fulﬁlls certain properties. relation extended multivariate case. enunciate result provide proof appendix consider following assumption actual decomposition assumption consider distribution distributions family ∆s.s.....sn preserves marginal distributions allow constructing decompositions mutual information consistent partial ordering redundancies relations mutual information redundancy measures terms nonnegative. furthermore ∆s.s.....sn terms involve synergy decompositions invariant within family. lattice invariant according assumption. oppositely terms invariant involve distribution preserved within family. however assumption also regards bivariate decomposition related trivariate terms trivariate lattice also invariant invariance terms bivariate lattices. lemma consider target primary sources distribution consider family distributions ∆s.s.....sn preserving marginal distributions primary source. assume actual decomposition mutual information exists distribution within family conforming assumption then ∆s.s.....sn actual synergistic terms primary sources vanish redundancy measures maximum entropy decomposition retrieve corresponding redundancy terms actual decomposition. conversely maximum entropy redundancy measures retrieve corresponding actual redundancy terms ∆s.s.....sn actual synergistic terms vanish. proof. appendix lemma extends lemma however focuses relation redundancy unique redundancy terms maximum entropy actual decompositions because rooted tree decompositions decompose synergistic contributions. given sufﬁcient condition stated lemma retrieve actual terms maximum entropy measures also adopt following assumption assumption constrained minimization mutual information cancels terms actual decomposition whose cancellation compatible minimization constraints. second assumption considers within family distributions minimize distribution terms actual decomposition affected constraints zero. example assume that minimization within family distributions synergistic terms cannot reached descending nodes corresponding primary sources canceled. combination assumptions allows analyzing effect minimization constraints examining redundancy lattice hence derive general expressions effect constraints independently speciﬁc properties certain distribution accordingly rest section derive results section examine maximum entropy redundancy measures affected break figure rooted trees associated decomposition mutual information variables target updating tree resulted adding added subsequently. like figure elliptical shaded areas indicate unfolding previous node. representation effect constraint unconditional co-information arrow indicates emergence redundancy synergistic component accumulated otherwise minimized representation effect constraint conditional co-information arrow indicates emergence synergistic component redundancy unique respect dashed circles indicate effect conditioning removing redundancy components contained information target. revisiting maximum entropy decomposition introduced type minimization constraint namely imposing cancellation co-information enforces emergence redundancy synergy terms otherwise canceled. form coinformation constraints suggests approach generalized multivariate case. however multivariate case different redundancy contributions isolated retrieved combining different constraints co-information measures. examine trivariate case. figure shows tree formed sequence additions updated again redundancy component unfolded terms. conversely synergistic components accumulated term later discuss implications asymmetric updating redundancy synergy. focus identifying terms resulting unfolding checking nonnegativity. first redundancy unfolds redundancy three redundancy unique respect seen direct consequence monotonicity axiom. second information unique respect unfolded self-redundancy unique respect redundancy unique respect happens i+i. seen consequence monotonicity property unique redundancies analogous redundancies. introduce constraints enforce emergence different parts redundancy mirroring synergy ﬁrst consider constraint used bivariate case bivariate case constraint imposes terms bivariate lattice want constraint leads emergence explicitly considers variable involved. important unfolding need constraints used including different number variables tree hierarchically related. therefore want take account trivariate lattice given interested redundancy think quantify redundancy also explicitly considering could deﬁnition analogous minimizing preserving also marginal could minimize within ∆..c respectively. since difference minimized informations matters would matter common ground imposed constraint common minimizations preservation marginals target. however case effect changes depending whether marginal distributions preserved also appreciate this write constraint explicitly according figure constraint fulﬁlled minimizing mutual information within ∆..c. following redundancy unique information terms invariant within hence also ∆..c subsumed means terms ﬁxed equal ones original distribution. furthermore also constant corresponds unique information variable respect variables however ﬁxed individual values not. quantiﬁes redundancy unique respect redundancy separately. accordingly value importantly appears alone summed furthermore term involving synergy lower partial ordering node means that fulﬁll constraint increase much possible decreases keeping constant) order balance minimize since modifying keeping constant effect attainable minimum within ∆... freedom adjusting causes redundancy enforced constraint emerge synergistic terms ∆+∆+ part redundancy cannot balanced emerge. note problem considering constraint within family within reason minimized include thus constraint mirrors redundancy already makes emerge. conversely term part minimized thus mirroring redundancy make emerge. therefore make sure constraint enforces mirroring redundancy synergistic terms within family need extra constraint cancels cancelation enforced extra constraint minimizing bivariate redundancy deﬁned analogously equal terms except constant within thus minimum attained vanishes. particular minimum calculated general depending co-information constraints used would need cancel unique redundancy variable others node also thus impose also analogous constraints terms. simplicity refer constraints reexpress bivariate reconsider separate different redundancy terms terms composing purpose conditional coinformation constraints like rationale conditioning removes components redundancy shared conditioning variable furthermore usually conditioning also creates synergistic contributions assumptions fulﬁlled minimization preserving marginals contributions except degree required fulﬁll co-information constraints. accordingly case imposing since conditioning eliminates mirrored combining unconditional conditional co-information constraints isolate redundancy contribution. measures trivariate decomposition displayed table hierarchically related constraints ensures nonnegativity. like bivariate case checked measures total mutual information make apparent hierarchical structure constraints also impose imposing explain detail introducing general multivariate case effect imposing intuitively understood comparing figure figure figure enforce mirror components figure enforce mirror them. enforcing emerge something already enforced emerge effect. given also extra effect like combined unconditional co-information since constrain prevents cancelation terms i.jk. furthermore although apparent expression table symmetric variables assumptions fulﬁlled hence effect constraints analyzed based redundancy lattice structure. symmetry holds constraints could formulated permutation three variables long permutation consistent minimized mutual informations compared. indeed reexpressed implementing explicitly co-informations constraints leads symmetric expression check trivariate measures implement unfolding terms bivariate tree. check unfolding combining measures table need take account constraint conditional co-information subsumed constraint unconditional argued above. check unfolding table measures trivariate rooted tree decomposition. minimization constraint i.j.k preserves marginals variable target. constrain co-information conditional co-information respectively. indicates constraints cancel unique redundancy variable others last equality given deﬁnition conditional co-information i+i− applying eqs. compare bivariate trivariate measures check straightforwardly unfolding fulﬁlled. like bivariate case different trees built permuting order variables added. figure compare trees. although unfolding different measures derived remain symmetries present constraints mutual informations minimized. accordingly associate rooted trees deﬁned permutations variables adjunct redundancy lattice reﬂects partial ordering redundancies considered adjunct lattice edge edge present least trees means child term unfolded parent. indeed adjunct lattice corresponds sublattice studied generalizations bivariate measures derived following another approach based connection lattices different order inverting proposed checked fact measures shown table equal ones shown table here particular trivariate redundancy expressed explicit symmetric form. however opposed complete trivariate lattice rooted ﬁrst step generalize multivariate case constraints introduced deﬁning family distributions multivariate case impose that given variables none unique redundancy combination others. enforce this minimize impose mini right-hand side expression analogous general indicate constraint preserving bivariate marginals target variable before indicate constraints variables types constraints jointly accompany co-information constraints latter produce mirroring redundancy synergistic components. accordingly merge unless speciﬁcally want refer one. figure association rooted trees adjunct lattice. a-b) alternative trees constructed alternative orders addition variables respectively. adjunct lattice associated trees deﬁned permutation additions three variables. edge lattice indicates edge least present trees. trees synergistic component accumulated root tree. collections formed sources containing single variables unfolded. implements decomposition redundancy primary sources decomposition separately quantify redundancy between synergistic contributions address general rooted trees decompose redundancy local binary unfoldings hierarchically related maximum entropy constraints. consider target consisting variables primary sources. simplicity refer primary source single variable. given construct rooted trees permuting order variables added. trees associated deﬁned group permutations variables indexes represents sequence additions tree. adjunct lattice deﬁned edge between nodes edge nodes least tree. consider calculate redundancy terms associated adjunct lattice. purpose need understand general effect co-information constraints. above assumed constraints analyzed according structure redundancy lattice. section examine effect constraints changes stands constraint variables terms adjunct lattice. αij;z indicates nodes reached descending adjunct lattice formed variables node associated collection αij;z i.j. note collections containing sources formed single variables primary sources appear lattice collection corresponding root αijk;z indicates union nodes reached descending least nodes i.j.k effect mirror synergy redundancy terms unique respect variables included argued trivariate case redundancy mirrored redundancy conditioned assumptions minimization constrained preserve marginals gets synergistic components would created otherwise conditioning. consider general combination coinformation constraints determines redundancy components mirrored synergy. consider variables variables within subsets containing variables exclusive redundancy term mirrored least constraint enforces emerge. corollary adding second constraint superset leaves invariant minimized mutual information because assumptions indicated terms comprised conditioning superset enforces emergence subset already emerged redundancy terms. indicates hierarchical structure constraints plays important role decomposition implementation. another special case useful that exclusive subsets also nonoverlapping variables redundancy examined αijk;z indicates descending nodes inﬁmum nodes αijk;z indicates nodes reached descending ﬁrst node intersection descending paths αijk;z means constraints leads emergence redundancy also redundant variables unique respect except terms corresponding joint redundancy variables unique respect using type constraints deﬁne multivariate measures redundancy variables also unique redundancy variables respect another set. suppose target variable variables explicitly consider variables system accessible distinguish subset comprising variables observed interest. consider subset unique redundancy respect another exclusive subset determined. deﬁne union select pair variables reference. unique redundancy between variables respect variables adjunct lattice collection adjunct lattice constituted single variable different source. type constraints deﬁned eqs. respectively. comprises terms associated collections contain variables none distinguished indicate calculate unique redundancy subset value depend variables similarly take substitute minimized mutual informations compared without altering measure mirrored redundancies determine value. furthermore given measure invariant variables selected reference hence fulﬁlls symmetry axiom. moreover redundancy special case unique redundancy empty. special case constraints vanish since variable presents co-information constraints lead emergence desired group redundancy contributions. however unique. since group redundancy components quantiﬁed depends difference minimizations terms mirrored minimizations change long difference same. given alternatively calculate redundancy variable compare ways redundancies estimated compare minimization mirroring terms descending variable minimization mirroring terms descending variable furthermore variables together. leads isolate terms descending variable variables together. given compare minimization mirroring terms descending variable furthermore together minimization mirroring terms descending variable furthertogether. since terms containing emerge minimizations measure invariant selected. comparison leads finally eqs. deﬁne redundancies unique redundancies least variables applicable self-redundancy unique information single variable respect variables. clear since co-information constraints used require least select variables self-redundancy variable taken mutual information construction fulﬁlls self-redundancy axiom. unique information respect deﬁned pointed redundancy measures fulﬁll self-redundancy axiom construction symmetry axiom assumptions effect constraints eqs. determined structure redundancy lattice. show monotonicity axiom also complied examine local binary unfoldings information rooted tree. identity axiom discussed appendix measures deﬁned deﬁnition nonnegative. eqs. nonnegativity guaranteed left minimizations compared subjected extra constraint full information compared nonetheless guarantee measures implement decomposition total mutual information. show case need update tree incorporating variable root node pending update synergy unique information respectively previously existing nodes unfolded unfolding preserves information original term. indeed since variable contributes added unique information given long information preserved subsequent unfoldings overall redundancy terms remains information marginal distributions whole variables synergistic components accumulated root node thus equal total mutual information. show information preserved unfolding constraints hierarchically related. consider exclusive subsets observed variables consider calculated term corresponding unique redundancy between variables respect variables examine adding variable done updating tree part original unique redundancy also redundant part ﬁrst equality comes deﬁnition separate components adding subtracting minimized information. ﬁrst term corresponds unique redundancy respect seen considering matter constraint present left minimization. second term unique redundancy respect seen considering corresponds conditioning addition. apart showing unfolding occurs also shows redundancy measures fulﬁll monotonicity axiom. also show unfolding works unique information terms variable deﬁned case consider unfolding adding variable proceed inverse showing redundant information unique respect unique information respect equal unique information respect iterative application unfoldings guarantees nonnegative decomposition redundancy. therefore together root tree captures synergistic contributions decomposition total mutual information implemented. note since synergistic components accumulated root term decomposition orthogonal decompositions based hierarchical separation higher-order moments measures multivariate redundancy unique redundancy deﬁned based general effect minimization constraints derived assumptions assumptions allow analyzing constraints examining redundancy lattice. without them could establish general expressions effect constraints minimizations would depend particular properties distribution. assumptions hold effect constraints generally differs eqs. preservation information unfoldings lost eqs. fulﬁlled. case consistency measures implementing mutual information decomposition lost. examine detail measures affected nonnegativity assumed hold. understand effect existence negative terms interpretability redundancy measures focus trivariate case. ﬁrst consider constraints type using added deﬁne family distributions ∆..cmin based argued that allowed higher zero minimization within family ∆..c results emergence synergistic terms part balanced constraint added cancel redundancy emerges synergistic terms. however cancellation relies nonnegativity could minimized cancelling rendering negative then cancel coinformation synergistic terms would need balance also |∆|. accordingly equality would longer hold. measure would still nonnegative construction would quantify contrast measure therefore existence negative term would reﬂected lack consistency. negative terms also affect conditional co-information constraints hierarchy. consider constraint canceling conditional co-information like imposed measures table imposed includes synergistic terms conditioning however discussed section usually conditioning creates synergistic contributions assumptions fulﬁlled minimization preserving marginals gets except degree required fulﬁll co-information constraints. terms within ﬁrst brackets except vanish thus fulﬁll constraint term balanced however synergistic terms within ﬁrst brackets could rendered negative balance constraint would fulﬁlled minimized since terms would contribute negatively accordingly co-information constraint would longer lead emergence furthermore negative synergistic terms exist hierarchical relation constraints involving subsets conditioning variables longer hold. this compare constraints given lattice structure redundancy includes includes argued above synergistic terms negative minimize mutual information fulﬁlling conditional co-information constraint render negative synergistic terms added redundancy terms comparing former negative synergistic terms need balance latter negative component contributed synergistic terms fulﬁll co-information constraint would bigger means would minimized. accordingly contrast even constraint would constrain minimization loss hierarchical relations constraints stated impairs binary unfolding redundancy measures because pointed obtained assuming that given lack constraint alter left minimization. like lack consistency checked namely examining match measures associated binary unfolding comparing results minimizations explicitly constraints alter minimization according effect constraints described section relies validity assumptions hence also interpretation measures eqs. actually quantifying redundancies unique redundancies. measures nonnegatively deﬁned construction inquantiﬁcation redundancy components information variables target proven elusive multivariate systems. although mutual information decomposition nonnegative redundant unique synergistic components fruitful conceptual framework broad ramiﬁcations study information multivariate systems identiﬁcation suited measure multivariate redundancy subject ongoing research. particular indicated original measure proposed quantiﬁes common amounts information. subsequent proposals either focus bivariate case require nonnegativity focus characterizing synergistic components information proposed measures maximum entropy measures preeminent role bivariate case because under certain assumptions provide bounds match actual terms decomposition motivated especial role generalized maximum entropy approach proposing deﬁnitions redundant information group variables target unique redundant information group variables target redundant information another group. quantities embedded rooted tree decompositions mutual information based local unfolding redundancy components variable added tree. shown redundancy component decomposed component also redundant variable component unique redundancy respect unfolding implemented hierarchically related maximum entropy constraints guarantees nonnegativity terms decomposition. section revisited bivariate maximum entropy measures redundant unique synergistic information showed measures reexpressed implementing binary unfoldings mutual information conditional mutual information. particular identiﬁed entropy maximization constraint allows quantifying redundancy. constraint enforces cancellation co-information -which shown quantify difference redundancy synergypreserving marginal distributions variable target. effect mirroring redundancy otherwise minimized synergistic component. generalize approach multivariate systems showed especial role maximum entropy measures connection actual decomposition also holds multivariate case assumptions analogous ones comprise assumption nonnegative decomposition exists considered ﬁrst trivariate case showed isolate speciﬁc components redundancy constraints conditional co-informations. section presented general development multivariate redundancy decompositions. provided general formulas effect constraints co-information conditional co-information assumptions link maximum entropy actual decomposition. derived multivariate measures redundancy unique redundancy showed implement nonnegative rooted tree decomposition mutual information. section examined interpretability measures affected nonnegativity terms decomposition hold showed case measures implement mutual information decomposition relations different co-information constraints change. approach selection co-information constraints used isolate different redundancy components relies redundancy axioms partial ordering redundancy terms introduced however rooted tree decompositions separate components redundancy unique redundancy primary sources break contributions involve unique redundancy sources comprising several primary sources furthermore synergistic components accumulated root term tree decompositions orthogonal decompositions hierarchically separating high-order moments regarding future extensions case deserves special attention application rooted tree decompositions study dynamic dependencies multivariate systems work applied original measures decompose particular conditional mutual information namely transfer entropy quantiﬁes information transfer dynamical processes. decomposition allows separating state-independent state-dependent components information transfer also identifying information transferred speciﬁc variable breaktransfer entropy could alternatively implemented using maximum entropy measures. generally open question whether methods characterize synergy redundancy combined interventional approach suited quantify causal effects multivariate measures redundancy useful many domains data analysis like model selection independent component analysis example relevance concrete ﬁeld consider several applications systems computational neuroscience. characterization redundancies sensory stimuli neural responses fundamental step towards understanding sensory neural representations processing. regarding redundancies stimuli long-standing hypothesis brain adapts statistics natural stimuli optimize sensory processing context redundancies seen terms joint entropy multivariate stimuli argued efﬁcient coding must also take account goal sensory representation accordingly example predictive coding redundancies assessed relation target variable associated goal. regarding redundancies neural responses decomposition redundancy terms help identify neural representations distributed different neural features different spatial temporal scales well understand sensory behavioural information combined neural responses furthermore particular application information transfer measures analysis information ﬂows brain areas characterize dynamic functional connectivity also beneﬁt multivariate redundancy measures determine degree functional integration segregation neural dynamics discussed above possibility systematically characterize redundancy multivariate systems expected applications many domains comprising study biological social systems. work proposed multivariate redundancy measures within maximum entropy framework generalizing bivariate decomposition given especial link maximum entropy actual decompositions expect measures useful practice many multivariate systems. future research required extend multivariate case efﬁcient algorithms developed estimate maximum entropy decomposition bivariate case however also indicated interpretation multivariate maximum entropy redundancy measures impaired presence negative terms check consistency decomposition. pointed maximum entropy approach assumes certain criterion identify pieces information based target-sources variables associations incompatible ensuring nonnegativity presence deterministic target-source dependencies. remains open question whether lack nonnegative decomposition signature certain systems meaningful nonnegative decomposition could obtained different criterion information identity prove lema purpose start extending multivariate case results synergy bounded maximum entropy estimator stated bivariate case lema lemma consider target primary sources distribution consider family distributions ∆s.s.....sn preserving marginal distributions primary source. assume actual decomposition mutual information exists assumption within family ∆s.s.....sn synergy nonnegative nonsynergistic component invariant distributions. distributions within family nonnegative synergistic term invariant nonsynergistic term considering distribution minimizes leads proof start proving ∆s.s.....sn synergistic terms vanish redundancy maximum entropy measures retrieve corresponding redundancy terms actual decomposition. provide ﬁrst detailed proof trivariate case building results bivariate one. synergistic terms vanish trivariate lattice least distribution also vanish bivariate lattice related trivariate accordingly based lema maximum entropy actual decompositions coincide associated bivariate decompositions. since rooted tree decompositions synergistic terms accumulated root node focus trivariate sublattice corresponding adjunct lattice figure instead considering complete trivariate lattice figure sublattice equal complete trivariate lattice redundancy terms equal. given mapping terms bivariate trivariate lattices equality actual maximum entropy decompositions bivariate lattices results following equalities terms sublattice given means using equality establish equality rest redundancy terms furthermore since nonsynergistic terms assumed invariant within family equalities hold distributions within family. proof multivariate case proceeds analogously trivariate case using induction. given lemma holds case primary sources show also holds sources. particular like general equalities single terms actual maximum entropy decompositions lead equalities sums terms given binary unfolding rooted trees term terms adding variable. furthermore equality valid generically primary sources. trivariate case results therefore combination general equations analogous eqs. leads equality maximum entropy actual decompositions general multivariate case least distribution lemma. equality maximum entropy actual terms holds redundancy unique redundancy terms equal means also deﬁnition distribution within family vanishes namely minimal information. sures. particular subcase examine reexpressed maximum entropy redundancy measure conforms identity axiom concerns bivariate systems target copy sources. generally case deterministic target-sources dependencies interesting role causing negative terms decomposition particular consider effect coinformation constraints existence source variables also part target. consider conditional co-information deﬁnition coinformation subset variables overlapping equality applied iteratively several overlaps subsumes equals systems equals mutual information possible fulﬁll constraints form used calculate multivariate redundancy measures. occurs constant within family minimization performed. example consider target includes copy sources variables contained subcase corresponds identity axiom. given since family preserves includes least sources constant within family given original bivariate deﬁnition redundancy leads conforms identity axiom. however directly using alternative deﬁnition bivariate redundancy co-information constraint canfulﬁlled nonzero. around limitation co-information constraints cannot fulﬁlled target-sources deterministic dependencies calculation maximum entropy redundancy measures preceded separation stochastic deterministic components redundancy terms analyzed addressed reﬁnement deﬁnitions since although theoretically relevant case deterministic target-sources dependencies narrow scope practice. brieﬂy deﬁnitions multivariate redundancy measures applied stochastic components redundancy terms deterministic components calculated separately. phys. rev. entropy williams beer arxiv.v marko ieee commun. schreiber phys. rev. lett. beer williams cognitive science polani advances complex systems lizier prokopenko eur. phys. chicharro ledberg plos burnham anderson model selection multimodel inference practical information-theoretic approach", "year": "2017"}