{"title": "Evaluation of Parallel Tempering to Accelerate Bayesian Parameter  Estimation in Systems Biology", "tag": "q-bio", "abstract": " Models of biological systems often have many unknown parameters that must be determined in order for model behavior to match experimental observations. Commonly-used methods for parameter estimation that return point estimates of the best-fit parameters are insufficient when models are high dimensional and under-constrained. As a result, Bayesian methods, which treat model parameters as random variables and attempt to estimate their probability distributions given data, have become popular in systems biology. Bayesian parameter estimation often relies on Markov Chain Monte Carlo (MCMC) methods to sample model parameter distributions, but the slow convergence of MCMC sampling can be a major bottleneck. One approach to improving performance is parallel tempering (PT), a physics-based method that uses swapping between multiple Markov chains run in parallel at different temperatures to accelerate sampling. The temperature of a Markov chain determines the probability of accepting an unfavorable move, so swapping with higher temperatures chains enables the sampling chain to escape from local minima. In this work we compared the MCMC performance of PT and the commonly-used Metropolis-Hastings (MH) algorithm on six biological models of varying complexity. We found that for simpler models PT accelerated convergence and sampling, and that for more complex models, PT often converged in cases MH became trapped in non-optimal local minima. We also developed a freely-available MATLAB package for Bayesian parameter estimation called PTempEst (http://github.com/RuleWorld/ptempest), which is closely integrated with the popular BioNetGen software for rule-based modeling of biological systems. ", "text": "abstract—models biological systems often many unknown parameters must determined order model behavior match experimental observations. commonly-used methods parameter estimation return point estimates best-ﬁt parameters insufﬁcient models high dimensional under-constrained. result bayesian methods treat model parameters random variables attempt estimate probability distributions given data become popular systems biology. bayesian parameter estimation often relies markov chain monte carlo methods sample model parameter distributions slow convergence mcmc sampling major bottleneck. approach improving performance parallel tempering physics-based method uses swapping multiple markov chains parallel different temperatures accelerate sampling. temperature markov chain determines probability accepting unfavorable move swapping higher temperatures chains enables sampling chain escape local minima. work compared mcmc performance commonly-used metropolishastings algorithm biological models varying complexity. found simpler models accelerated convergence sampling complex models often converged cases became trapped non-optimal local minima. also developed freely-available matlab package bayesian parameter estimation called ptempest closely integrated popular bionetgen software rule-based modeling biological systems. mathematical computational models gaining widespread tools summarize understanding biological systems make novel predictions tested experimentally requires model correctly parameterized. parameter estimation process inferring model parameters experimental data typically involves deﬁning cost function quantiﬁes discrepancy model output data performing search parameterizations minimize cost many commonly-used methods ﬁnding parameter sets minimize model cost. broadly divided gradient-based gradient-free methods. gradient-based methods local optimization methods iteratively gradient cost function compute search direction step length followed updating parameters checking convergence popular gradient-based methods systems biology include gradient descent newton’s method gaussnewton algorithm levenberg-marquardt algorithm however methods fail global minimum landscapes discontinuous multimodal frequently case large biological models many parameters independent data points constrain model gradient-free methods advantage landscape need smooth local search methods nelder-mead simplex become inefﬁcient highdimensional problems gradient-free global optimization methods genetic algorithms particle swarm optimization effective ﬁnding optimal solutions high-dimensional spaces however combination high-dimensional parameter spaces limited amount data available typical biological experiments often means multiple parameter combinations equivalently describe experimental data referred parameter identiﬁability problem parameters non-identiﬁable single parameter insufﬁcient describe feasible space parameters associated model. bayesian methods solve problem naturally attempting estimate probability distribution model parameters given experimental data allows simultaneous determination best-ﬁt parameters parameter sensitivities also providing framework introduce prior information modeler parameters. bayesian methods include likelihood-based approaches markov chain monte carlo methods likelihood-free approaches approximate bayesian computation mcmc commonly used systems biology slow convergence often major bottleneck standard sampling algorithms metropolis-hastings development modular rule-based software model construction simulation allows construction combined increasing availability singlecell data motivates need accelerated methods bayesian parameter estimation. parallel tempering physics-based mcmc method efﬁciently samples probability distribution accelerate convergence conventional mcmc methods method widely used molecular dynamics simulations sample conformational space biomolecules less common systems biology here describe algorithmic elements method provide software implementation evaluate performance series biological models increasing complexity. remainder paper organized follows sec. describe algorithms well abc-smc methods used software abcsysbio later comparison. also include brief description ptempest software bayesian parameter estimation. sec. present series examples increasing complexity test performance relative regards quality convergence speed sampling efﬁciency. include comparison abc-sysbio show application using bayesian methods laplace priors achieve model reduction. finally sec. discuss main ﬁndings limitations areas future work. bayesian parameter estimation methods infer posterior distribution describes uncertainty parameter values remains even data known probability observing parameter given data given bayes’ rule conditional probability given described likelihood model independent probability often referred prior distribution model parameters. distribution represents prior beliefs model parameters used restrict parameters range values even limit number nonzero parameters discussed below. prior distributions parameters estimated. uniform priors common choice little known parameters except upper lower limits. priors also introduced simplify model reducing parameters zero process called regularization. example lasso regularization penalizes absolute values parameters ridge regression penalizes squared parameter values proposal function deﬁne probability distribution next parameter sample given current set. common choice normal distribution centered current value userspeciﬁed variance determines effective step size. ptempest uses single adaptive step-size determine change parameters mcmc implementations permit different step sizes govern changes different directions parameter space algorithm metropolishastings algorithm popular mcmc methods assume symmetric proposal function i.e. probability moving parameter equals moving algorithm sample follows parallel tempering differences between existence temperature parameter scales effective shallowness energy landscape. several markov chains constructed parallel different markov chain value samples true energy landscape higher temperature chains lower values sample shallower landscapes acceptance probability −β∆e higher temperature chains accept given min. however varying parameters construction chain violates assumption symmetric proposal function advisable burn-in phase prior sampling. implementation work present ptempest matlab-based tool parameter estimation using integrated rule-based modeling software bionetgen models speciﬁed bionetgen language exported models called matlab functions ptempest. bionetgen commands writemfile writemexfile used export models matlab’s m-ﬁle format uses matlab’s built-in integrators matlab mex-ﬁle encodes model invokes cvode library usually much efﬁcient experience. additional compatibility models imported bionetgen system biology markup language user write cost function matlab. bayesian parameter estimation capabilities ptempest complement another tool performing parameter estimation rule-based models bionetfit ptempest uses adaptive step sizes temperatures. user provides following hyper-parameters control sampling initial step size initial temperature adaptation intervals target acceptance probabilities steps swaps. given intervals step acceptance probabilities swap acceptance probabilities calculated step sizes chain temperatures adjusted bring step swap acceptance probabilities closer target values respectively. example step acceptance rates high step size increased vice versa. similarly swap acceptance rates high chain temperatures increased vice versa. although considerable number hyperparameters associated method found default values provided ptempest generally work well practice. rejection simplest algorithm rejection algorithm involves repeatedly sampling parameter vector prior distribution simulating model sampled parameters calculating discrepancy simulated data ysim experimental data yexpt. discrepancy threshold accepted member posterior distribution; otherwise discarded another drawn. process continues number samples reaches speciﬁed number resulting approximation approximate bayesian computation-sequential monte carlo rejection algorithm suffer acceptance rates abc-smc algorithm uses tolerance schedule decrease sequentially constructs approximate posterior distributions increasing accuracy eventually converge true posterior distribution abc-smc generate results shown sec. iii-b analyses models synthetic data generated using ﬁxed parameter values. comparison presented sec. iii-b used synthetic data additional noise provided abc-sysbio example ﬁles. models containing parameters algorithms global minimum compared performance using convergence time sampling efﬁciency. convergence time deﬁned number mcmc steps energy drops speciﬁed threshold determined empirically convergence time based number mcmc steps lowest temperature chain. uniform priors data simulated without noise negative likelihood approaches zero chain converges global minimum. sampling efﬁciency deﬁned ratio range posterior distribution range prior distribution either model parameter known uniformly distributed added control parameter contribute model output therefore uniformly distributed. complex models always obtain parameter sets data. case compare algorithms terms negative likelihood best parameter sets. case uniform priors directly corresponds minimum energy attained markov chain. compare disparate algorithms terms total amount computational resource used allowed perform speciﬁed number model integrations. number model integrations number mcmc steps number mcmc steps times number chains parallel. algorithms rejection sampling number model integrations equals total number parameter sets evaluated generate desired number samples. demonstrate bayesian methods identify constrained parameter relationships even individual parameters unidentiﬁable. michaelis-menten model describes enzyme substrate kinetics using following scheme michaelis constant. product trajectory constrains kcat individual forward backward rates unidentiﬁable. generated synthetic product trajectory quality produced comparable however average required mcmc steps reach convergence required thus even though step needs times many model integrations total number model integrations smaller consistent observation made chains length efﬁcient single-chain monte carlo search length also higher sampling efﬁciency compared would expect non-identiﬁability posterior distributions uniform across prior ratio identiﬁable constrained parameter constrained distribution centered distributions shown figures fig. parameter estimation michaelis-menten model. distribution minimum energy values obtained example ﬁtted ensemble obtained synthetic data using distribution convergence times energy threshold sampling efﬁciency parameters repeats using estimated posterior distributions model parameters. x-axis limits uniform prior boundaries. scatter plots sampled parameter sets pair model parameters. axis limits reﬂect prior boundaries. section compare efﬁciency abc-smc parameter estimation simple model mrna self-regulation abc-sysbio software distributed example ﬁles estimate parameters model assuming uniform priors using abc-smc algorithm. model parameters ﬁxed quality deﬁned euclidean distance ﬁtted trajectory data. abc-smc algorithm extended default -step tolerance schedule provided abc-sysbio -step schedule ensemble size abc-smc times found fig. parameter estimation model mrna self regulation. reaction network diagram mrna self regulation model quality ﬁnal ensemble obtained abc-sysbio plots show distribution euclidean distances members ﬁtted ensembles synthetic data. example typical ﬁtted ensemble obtained abcsysbio. black lines show synthetic data colored lines show ﬁtted trajectories. refers number mrna molecules. distribution convergence times energy threshold takes average ~-fold fewer steps reach convergence. comparison sampling efﬁciency compared using control parameter described above. quality produced mcmcbased algorithms substantially higher abc-smc takes fewer steps reach convergence higher sampling efﬁciency given number model integrations section demonstrate mcmc approaches perform model reduction coupling parameter estimation regularization. lasso regularization penalizes l-norm parameter vector minimizing cost function parameter estimation. performs variable selection ﬁnding minimum number non-zero parameters required data lasso penalty equivalent assuming laplace prior parameters width prior inversely related regularization parameter governs strength penalty. here present example using bayesian lasso model reduction compare problem. fig. model reduction lasso. reaction network diagram negative feedback model. core model used obtain synthetic data blue extraneous elements tuning regularization parameter i.e. width laplace prior w.r.t negative likelihood ﬁtted ensembles examples ﬁtted ensembles corresponding different regularization strengths. error bars show synthetic data. solid lines show simulated ﬁts. posterior distributions lasso parameters show extraneous parameters peaking lines indicate true parameter values posterior distributions obtained without lasso. boxes indicate extraneous parameters. distribution convergence times energy threshold distributions sampling efﬁciency across repeats. core model negative feedback regulation three processes simulated synthetic trajectory species using value three rate constants three extraneous process added model subset reactions reaction scheme required data. constructed likelihood function assuming gaussian error assumed laplace priors width model parameters regularization parameter needs tuned. high values i.e. wide priors impact likelihood achieve much variable selection. conversely values parameters cost degrading likelihood. here tested range values. mcmc steps times obtain distribution negative log-likelihood values figure shows examples ﬁtted ensembles obtained different regularization strengths. chose smallest value signiﬁcantly increase negative also considered three species negative feedback oscillator tyson evaluate difﬁcult case ﬁtting model complex dynamics multiple species. generated synthetic data three model species conditions three species undergo sustained oscillations. model parameters sampled log-space uniform priors units wide centered true values. likelihood function t-distribution error. generated random initial parameter sets starting chains chains mcmc steps. figure shows examples chains converging different minima. shows example convergence high energy. case calcium signaling chains outperforms chains turn outperforms algorithm ﬁnding global minimum interestingly data generated sufﬁciently constrain frequency oscillations exhibited model parameter sets corresponding different frequencies data. figure shows posterior distributions model parameters corresponding shown middle panel figure obtained using chains. ﬁrst parameter shows clear peaks centered true value. separating parameter sets corresponding peaks shows correspond speciﬁc differences oscillation frequencies part ﬁtted ensemble reinforcing need bayesian methods problems. finally apply substantially larger model parameters rule-based model regulation growth factor signaling generates species reactions. generated synthetic data micromolar concentration phosphorylated receptors observable combines time courses model species constructed likelihood function assuming gaussian error. parameters sampled log-space uniform prior interval starting point obtain markov chains mcmc steps using four chains. figure shows chain convergence different minima figure shows consistently ﬁnds good study shown even relatively simple biochemical models signiﬁcant beneﬁts using terms convergence speed sampling efﬁciency. complex models found given ﬁxed budget mcmc steps often fails global minimum whereas consistently fig. parameter estimation model calcium signaling. examples convergence local minimum global minimum error bars show synthetic data. solid lines show simulated ﬁts. right column shows energy chains corresponding left. distributions minimum energy repeats. posterior distributions model parameters obtained regularization show extraneous parameters peaking essential parameters well deﬁned distributions peak close true values without regularization extraneous parameters take non-zero values make parameters unidentiﬁable converges faster sampling efﬁciencies calculated mcmc steps comparable models considered relatively small number parameters achieve convergence readily. models parameters complex dynamics convergence becomes difﬁcult achieve. example consider four-species model calcium oscillations free parameters model describes dynamics subunits g-protein active free cytosolic calcium calcium endoplasmic reticulum. generated synthetic data free cytosolic calcium constructed likelihood function assuming gaussian error. free parameters sampled log-space uniform priors units wide centered true values. generated random initial parameter sets starting point sampled using chains chains fraction chains converged global minimum mcmc steps. figure shows example chain converged local minimum high energy another chain converged global minimum. distributions minimum energy chains obtained algorithm show found better turn better returned highly variable results frequently reach global minimum. fig. parameter estimation growth factor signaling model. examples convergence different minima. error bars show synthetic data. solid lines show simulated ﬁts. right column shows energy chains corresponding shown left. distributions minimum energy obtained chains repeats. attempted compare wall clock times different algorithms. instead performance metric used number mcmc steps number model integrations required independent implementation. practice reported sec. performed typical workstation computer times ranging minutes smallest model hours largest model however found despite requiring number model integrations processor step single-chain sampling signiﬁcantly faster step terms wall clock time four chains. preliminary tests showed differences likely arise requirement current implementation chain complete ﬁxed number steps swap attempted. parallel efﬁciency decreases trajectories different processors take different amounts time complete. found difference wall clock time decreased chains temperature algorithmic beneﬁts. chains different temperatures high temperature chains tend sample parameter space broadly results greater variability model integration time causes slow synchronization requirement. plan investigate asynchronous swapping chains order alleviate problem. another limitation current work comparisons made using speciﬁc choices hyperparameters control algorithm control step sizes moves temperatures. adjustment result improvements sampling efﬁciency convergence rates. would also like investigate effect using different proposal functions hessian-guided mcmc well different likelihood models. fig. parameter estimation negative feedback oscillator. examples convergence different minima. error bars show synthetic data. solid lines show simulated ﬁts. three colors correspond three different model species. right column shows energy chains corresponding shown left. distributions minimum energy repeats. posterior distributions corresponding middle panel simulated corresponding three peaks posterior distribution ﬁrst parameter shown succeeds. also showed example bayesian parameter estimation effectively perform model reduction introduction regularizing prior. methods constitute popular class alternative methods bayesian parameter estimation cases likelihood models expensive available found outperformed parameter sampling relatively simple model. direct performance comparison supports previous observation likelihood-based methods preferable likelihood-free methods likelihood models feasible compute models. efﬁciency mcmc methods differential evolution adaptive metropolis delayed rejection adaptive metropolis would interesting investigate whether parallel tempering could fruitfully combined approaches. finally presented used point molecular simulation literature moderately parallel algorithm uses handful chains. remains seen whether using much larger number chains would retain advantages sampling simultaneously multiple temperatures result acceleration. authors would like thank cihan kaya technical assistance helpful discussions. work funded grant r-gm recl nigms-funded national center multiscale modeling biological systems bhatnagar bogdanov mossel. computational complexity estimating mcmc convergence time. approximation randomization combinatorial optimization. algorithms techniques. lecture notes computer science springer berlin heidelberg eydgahi chen muhlich vitkup tsitsiklis sorger. properties cell death models calibrated compared using bayesian approaches. molecular systems biology gutenkunst waterfall casey brown myers sethna. universally sloppy parameter sensitivities systems biology models. plos computational biology haario laine mira saksman. dram efﬁcient hindmarsh brown grant serban shumaker woodward. sundials suite nonlinear differential/algebraic equation solvers. transactions mathematical software hucka finney sauro bolouri doyle kitano arkin bornstein bray cornish-bowden systems biology markup language medium representation exchange biochemical network models. bioinformatics identiﬁcation optimal drug combinations targeting cellular networks integrating phospho-proteomics computational network analysis. cancer research liepe kirk filippi toni barnes stumpf. framework parameter estimation model selection experimental data systems biology using approximate bayesian computation. nature protocols lukens depasse rosenfeld ghedin mochan brown grefenstette burke swigon clermont. large-scale immuno-epidemiological simulation inﬂuenza epidemics. public health malkin sheehan mathew federspiel redl clermont. neutrophil phenotype model extracorporeal treatment sepsis. plos computational biology metropolis rosenbluth rosenbluth teller teller. equation state calculations fast computing machines. journal chemical physics thomas chylek colvin sirimulla clayton hlavacek posner. bionetfit ﬁtting tool compatible bionetgen nfsim distributed computing environments. bioinformatics toni welch strelkowa ipsen stumpf. approximate bayesian computation scheme parameter inference model selection dynamical systems. journal royal society interface", "year": "2018"}