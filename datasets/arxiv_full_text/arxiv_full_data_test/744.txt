{"title": "Transferable neural networks for enhanced sampling of protein dynamics", "tag": "q-bio", "abstract": " Variational auto-encoder frameworks have demonstrated success in reducing complex nonlinear dynamics in molecular simulation to a single non-linear embedding. In this work, we illustrate how this non-linear latent embedding can be used as a collective variable for enhanced sampling, and present a simple modification that allows us to rapidly perform sampling in multiple related systems. We first demonstrate our method is able to describe the effects of force field changes in capped alanine dipeptide after learning a model using AMBER99. We further provide a simple extension to variational dynamics encoders that allows the model to be trained in a more efficient manner on larger systems by encoding the outputs of a linear transformation using time-structure based independent component analysis (tICA). Using this technique, we show how such a model trained for one protein, the WW domain, can efficiently be transferred to perform enhanced sampling on a related mutant protein, the GTT mutation. This method shows promise for its ability to rapidly sample related systems using a single transferable collective variable and is generally applicable to sets of related simulations, enabling us to probe the effects of variation in increasingly large systems of biophysical interest. ", "text": "mohammad sultan hannah wayment-steele vijay pande† department chemistry stanford university campus drive stanford california usa. †pandestanford.edu variational auto-encoder frameworks demonstrated success reducing complex nonlinear dynamics molecular simulation single non-linear embedding. work illustrate non-linear latent embedding used collective variable enhanced sampling present simple modification allows rapidly perform sampling multiple related systems. first demonstrate method able describe effects force field changes capped alanine dipeptide learning model using amber. provide simple extension variational dynamics encoders allows model trained efficient manner larger systems encoding outputs linear transformation using time-structure based independent component analysis using technique show model trained protein domain efficiently transferred perform enhanced sampling related mutant protein mutation. method shows promise ability rapidly sample related systems using single transferable collective variable generally applicable sets related simulations enabling probe effects variation increasingly large systems biophysical interest. efficient sampling protein dynamics remains unsolved problem computational biophysics. even advances hardware custom chips algorithms molecular dynamics simulation studies limited understanding atomistic dynamics protein system time. however predictive guiding experiments require methods capable describing effects perturbations system. perturbation broadly defined could mutation protein sequence post-translational modification ionic concentration solvent type protonation state chemical potential better understanding simulation parameters change force field instance would like predict simulation perturbations affect protein dynamics instance characterizing protein’s folded state stabilized intermediate trapped. several analytical methods developed combine information simulation multiple conditions able make predictions system different thermodynamic states however also much information gained original simulation leveraged accelerate simulations related systems. phase space visited perturbed system mostly unchanged original system predicting changes cheaper running analyzing simulations scratch information phase space already known regions explored timescales shorter enhanced sampling runs. kinetics states equilibrium populations states changed perturbation enhanced sampling promising method rapidly sample mutant systems. enhanced sampling slow coordinate conserved system conditions able give unbiased exploration along faster coordinates accelerating simulation entire phase space across conditions. type study system complementary post-simulation analysis methods combine information multiple states. enhanced sampling methods prior information system accelerate simulation. metadynamics– commonly-used enhanced sampling method focus work time-dependent gaussians deposited along user-selected collective variables biases system away regions phase space already visited. however selection critical meaningfully sampling system. choice poor even simplest cases leads hysteresis timescales required convergence approach even exceed unbiased sampling timescales. address problem choice recently showed time-structure based independent component analysis relatively recent advance markov state model field– yields excellent linear non-linear collective variables enhanced sampling metadynamics methods. tica many variants attempt linearly approximate dominant eigenfunctions markovian transfer operator. also showed tica modes also called tics well approximated data regime even transition observed unbiased training simulation. argued using eigenfunctions transfer operator represent natural basis enhanced sampling since approximate slowest dynamical modes. biasing metadynamics tica mode still represents userselected intended slowly-decorrelating thereby hopefully maximizing exploration phase space. traditional tica analyses produce linear models limit descriptive capabilities. contrast non-linear tica methods employ popular kernel trick greatly increases ability approximate transfer operator. however kernel methods require user select distance metric appropriate kernel computational efficiency kernel methods additionally require identification appropriate landmark locations parameters. empirically observed poor choice landmarks parameters lead slow convergence sampling conformation space relative free-energies. introduce non-linearity without drawbacks kernel tica turn towards deep neural networks dimensionality reduction. recent development time-lagged extensions variational auto-encoders namely variational dynamics encoder time-lagged auto-encoder made flexibility deep neural networks available dimensionality reduction molecular simulation. vdes taes variants traditional auto-encoders unsupervised machine learning algorithms learn encodings input data trying reconstruct high dimensional data low-dimensional encoded value. contrast traditional method frameworks attempt reconstruct future dynamics based upon current encoded value. work show latent coordinate model used perform enhanced sampling. single latent layer encodes information regarding slow modes system. model trained protein also transferred perform enhanced sampling closely related mutants previous papers attempted traditional auto-encoders enhanced sampling methods suffer three flaws. immediately clear model learns. example traditional autoencoder similar principal component analysis might incorrectly identify fast floppy movement important reconstruction. black-box nature neural network makes understanding model learned difficult. previous work protein saliency maps tica-vde extension discussed section significantly improves understanding model learning also atomic coordinates accelerated. possible auto-encoder-based models given information dynamics learn representation artificially adds barriers kinetically similar states. example consider fictitious landscape basins state pass state state possible traditional auto-encoder could states non-continuous integers perfectly good dimensionality reduction coordinate sampling metadynamics particle basin need overcome additional repulsive forces jumps final basin contrast vde’s loss function designed reproduce time-lagged dynamics naturally continuous integers. none methods show networks might transferred unseen mutants. present work show simple sequence alignment used transfer networks mutant proteins. given time latent space used enhanced sampling protein sequence proof concept train learn dynamics alanine dipeptide coordinates enhanced sampling. obtained previously generated solvated alanine dipeptide trajectory trajectory ambersb forcefield model contained transitions along slower coordinate. rarity transition makes thermodynamic analysis difficult enhanced sampling allows obtain converged statistics. first create latent coordinate separates major alanine states defined ramachandran plot generated model using training data used latent coordinate trained model enhanced sampling original training data obtain converged sampling transferred charmm efficiently sample alanine’s landscape across multiple mutant ffs. model used sin-cosine transform backbone dihedrals inputs neural network. encoder network four input values fully connected layers hidden nodes swish non-linearity middle. layer compressed single encoding used enhanced sampling calculations. numerically architecture represented integers indicate number input hidden layer latent nodes respectively. decoder network singular latent node expanded using architecture encoder network reverse passing latent node variational noise layer trained model using dual time-lagged reconstruction auto-correlation losses suggested previously. trained model lag-time epochs. training performed using adam optimizer initial learning rate model built using pytorch training took less minutes platform. trained model’s results shown figure a-b. seen figure latent coordinate learns highly non-linear transformation alanine dipeptide simulation separating major states along single coordinate. contrast slowest tica solution primarily describes movement along slower coordinate. tica coordinate used sampling additional coordinate needed distinguish basins along alternatively simulations utilizing enhanced sampling first tica coordinate would need long enough simulations naturally equilibrate along faster coordinate. contrast single coordinate distinguishes major alanine basins forms collective variable simulations. perform enhanced sampling simulations alanine using latent coordinate translated fitted pytorch network custom plumed expressions performed simulations using openmm. core encoding simply nonlinear combination input dihedral features optimized using dual loss function mentioned above. collective variable transferred sampling along coordinate accelerated using variety cv-based enhanced sampling methods metadynamics adaptive bias force sampling umbrella sampling etc. results well-tempered metadynamics simulations shown figure c-d. supporting information table contains simulation parameters though empirically found range parameter values gave similar results. simulations observed fast diffusion along coordinate less obtained multiple transitions along slower coordinate. lastly vde-metadynamics simulations reweighted full phase space using mbar last-bias reweighting timeindependent estimator method allowing arbitrary projections along observables figure alanine dipeptide efficiently sampled using vde-metadynamics. projection training simulation ref. along slowest tica coordinate latent coordinate. coordinate captures major alanine basins single coordinate. projection alanine trajectory function simulation time shows rare transitions making difficult compute thermodynamic quantities. projection wt-metadynamics results amber charmm forcefields using transferable network. vde-metadynamics simulations re-weighted full phase space using mbar projected onto ramachandran plot using existing libraries. despite success enhanced sampling framework demonstrated above training directly input features unlikely efficiently scalable large systems many features. instance commonly-used contact featurization protein length results features. small proteins tractable; however protein residues already distance features. neural network architectures uncommon require number units hidden layer larger number input features order capture nonlinear effects. thus even first hidden layer would need perform order hundreds thousands float multiples calculations need performed every single time step thereby reducing simulation speed. given scaling problem could tractably larger system without limiting featurization schemes hand-selecting features? address scaling issue demonstrate transforming original feature input space projections tica space prior encoding alleviates scaling problem tica seeks embed protein configurations along slowest de-correlating coordinates. project data along selected number slowest tica modes also called tics rest modes exchange timescale smaller length enhanced simulations thereby ensuring convergence. collective variable non-linear combination current frame’s projection onto training dataset’s slowest orthogonal tica collective modes thus single hidden node encodes information slow linear tica modes interest. methodology still requires user pick appropriate feature space note multiple feature schemes used simultaneously appropriate scaling term feature space also potentially optimized using simulation. several advantages using tica ability explicitly model slowest modes system. tica pre-processing step additionally allows train networks much faster since hidden node sizes significantly lowered. tica modes necessary accurately capture slow subspace system since enhanced molecular simulation naturally equilibrate across remaining faster modes. reduced network dimensionality also means model actually used efficient collective coordinate without excessively slowing simulations. another advantage using tica protein dynamics coupled i.e. moving along requires change along second naturally include coupling. also allows understand network accelerating since know tics represent atomic scale. added advantage limit simulations certain regions phase-space prevent sampling irrelevant high free energy regions simply excluding tics represent movement region. methods sample multiple collective modes parallel proposed tica-vde samples multiple collective modes avoiding setting series parallel bias hamiltonian replica exchange simulations accompanying murky parameter selection. figure modeling creates transferable coordinate sampling mutants. d.e. shaw simulation data projected slowest tica modes show presence folded unfolded mis-folded states similar previous works. compress slow modes onto single coordinate using model. gold star indicates starting point simulations. model transferable mutant protein. x-axis tracks simulation time y-axis tracks projection along latent coordinate inset shows predicted folded state similar folded state metadynamics simulations clustered reweighted using mbar project tica coordinates. kcal/mol defined using highest populated state. protein images generated using rest used ipython notebook msmexplorer. proof concept recaptured effects mutation domain folding landscape. obtained domain folding trajectories shaw group performed using anton machine. computed closest heavy atom alpha carbon contact distances residues least apart sequence training simulation total features. mean-centered scaled unit variance. used sparse-tica algorithm performs tica calculation tunable penalty model complexity allowing create tica model reduced number features tic. penalty tunable increasing penalty reduces number features results fewer features needed drive system enhanced sampling too-sparse model leads hidden orthogonal modes since wait discarded degrees freedom respond bias. simulations selected model retained contact distance features across tics. thus every integration time step sparsetica saves contact distance calculations. similarly previous work others tica analysis indicated slowest modes exchange timescale longer corresponded folding process offpathway register shift mis-folding process modeling also showed could approximate tica solutions little long trajectories indicating methodology likely scale data-poor regimes well. previous work linear coordinate able distinguish folded unfolded mis-folded states thereby previously requiring multiple coordinates simultaneously enhanced connected hamiltonian replica exchange. simply perform enhanced sampling orthogonal modes trained model simulation data projected onto sparse tics. tica-transformed data network encoder architecture decoder mirrored encoder architecture except additional 𝜆-layer. model trained epochs using adam optimizer initial learning rate entire pipeline transferred plumed enhanced sampling well-tempered metadynamics. shown figure latent coordinate able transform linear tica modes highly non-linear function goes folded unfolded register shifted misfolded states. without preprocessing transformation unable fully distinguish folding process misfolded state thus latent coordinate trained tica-transformed data used transferable collective variable metadynamics simulations. simulations previously described exact parameters metadynamics simulations given table started walkers mutant initial coordinates walker approximately first discarded equilibration. obtained aggregate sampling worth noting simulations completed days commodity gpus. also optimize metadynamics parameters believe sampling significantly accelerated either selecting better parameters initial gaussian height gaussian drop rate bias factor coupling structural reservoir homology modeled states. results simulations shown figure b-c. mutant several walkers naturally folded topology without providing additional information folded states. moreover walkers sampled mis-folded state indicating ability single non-linear coordinate separate major basins domain. lastly also clustered metadynamics simulation states using minibatch kmeans algorithm obtained macrostate populations reweighting mbar. projecting onto either tica coordinates latent coordinate showed folded state slightly stable compared fip. paper piana also showed triple mutation stabilized folded state less kcal/mol. however inherent robustness issues metadynamics simulations combined limited methods post-error analysis metadynamics simulations make difficult make precise predictions. however repeat metadynamics simulations different parameters obtained qualitatively similar results aggregate sampling mutant. believe might possible push aggregate simulation time even further optimization outside scope paper. iscussion conclusions work shown extension traditional auto-encoders namely variational dynamics encoder provide excellent single collective variable enhanced sampling protein dynamics. simple systems inherently dimensional like alanine dipeptide learn dynamics directly degrees freedom. benefit approach direct connection original degrees freedom. however larger systems natural employ dimensionality reduction scheme creates another layer interpretation allows efficient learning. therefore also demonstrate pre-processing training trajectories using tica algorithm allows efficient training produce simpler collective variables directly push system along relevant slow modes. flexibility scalability method larger systems allows single collective variable rapidly perform enhanced sampling related protein mutants. purpose paper create single transferable collective variable using deep neural networks several extensions possible framework. possibility would create end-to-end training procedure instance using convolutional neural network constrains dimensions every hidden layer tica transformation becomes unnecessary without sacrificing simulation speed. method used compress multiple orthogonal processes single collective variable readily imagine topologically complex free energy landscapes make sense compressed single case method could also used higher-dimensional latent variable sampled using bias-exchange parallel bias metadynamics methods mentioned earlier. another trivial extension would tica algorithm optimization protocols feature selection tool choses features network. could potentially make better models still small enough efficiently sampled. engineering perspective methodology writing custom plumed scripts almost certainty inefficient would likely benefit ability embed networks directly engine. enhanced sampling scheme also presents opportunity engineer network architectures better suited molecular simulations. example dynamics better represented fatter networks nodes relatively layers deeper networks layers fewer nodes? difference performance different non-linear transformations better describe networks learning atomic scale? tests recommend shaw datasets standard benchmark knowledge mutant domains contain least slow modes able efficiently sample remains challenging. furthermore several computational experimental studies predict observable difference mutants. current manuscript previous works draw inspiration transfer learning commonly-used method machine learning model trained dataset shows utility related dataset. similarly believe neural networks models built using datasets transferred mutant simulations enhance dynamics potentially allowing predictive. many biophysical parameters determine protein conformation would useful able rapidly vary characterize simulation. method could enable performing sets simulations protein different mutations post-translational modifications ionic concentrations protonation states solvents forth. addition transfer collective variable information model might useful setting metadynamics parameters even accelerating convergence coupling structural reservoirs. ultimately known transfer learning fail efficient sampling related systems. moffet argue answer likely system dependent. worth noting defined observable quantity molecular simulation collective variable guaranteed converge conditioned enough sampling. however poorly-chosen could require sampling converge brute force therefore similar previous work recommend caution arbitrarily transferring networks. believe auto-encoder frameworks models offer promising path forward enhancing dynamics transferring models within related systems ultimately allowing probing larger complex biophysical systems simulation making simulations predictive experimental data. acknowledges support nsf-mcb-. hkws acknowledges support gfrp. work used xstream computational resource supported national science foundation major research instrumentation program authors would like acknowledge keri mckiernan suggestion tica preprocessing step network. authors would like thank d.e. shaw deseres graciously providing folding trajectories. authors also thank brooke husic critical reading manuscript. software data availability code needed regenerate models plumed input files presented paper available freely online www.github.com/msultan/vde_metadynamics. code licensed license. generated trajectories available upon request. eastman swails chodera mcgibbon zhao beauchamp wang simmonett harrigan stern wiewiora brooks pande openmm rapid development high performance algorithms molecular dynamics. plos comput. biol. bowman pande introduction markov state models application long timescale molecular simulation; vol. sultan pande transfer learning markov models leads efficient sampling related systems. phys. chem. acs.jpcb.b. sultan denny unwalla lovering pande millisecond dynamics reveal kinome-wide conformational plasticity within kinase domain. sci. rep. laio gervasio metadynamics method simulate rare events reconstruct free energy biophysics chemistry material science. reports prog. phys. pfaendtner bonomi efficient sampling high-dimensional free-energy landscapes parallel bias metadynamics. chem. theory comput. dama rose voth transition-tempered metadynamics promising tool studying permeation drug-like molecules membranes. chem. theory comput. fischer joseph-mccarthy kuchnir kuczera mattos michnick nguyen prodhom reiher roux schlenkrich smith stote straub watanabe wiórkiewicz-kuczera karplus all-atom empirical potential molecular modeling dynamics studies proteins. optimization additive charmm all-atom protein force field targeting improved sampling backbone side-chain dihedral angles. chem. theory comput. ramachandran zoph searching activation functions. kingma adam method stochastic optimization. paszke chanan gross yang antiga devito automatic shaw chao eastwood gagliardo grossman ierardi kolossváry klepeis layman mcleavey deneroff moraes mueller priest shan spengler theobald towles wang dror kuskin larson salmon young batson bowers anton special-purpose machine molecular dynamics simulation. proceedings annual international symposium computer architecture isca press york york vol. mohammad sultan hannah wayment-steele vijay pande† department chemistry stanford university campus drive stanford california usa. †pandestanford.edu parameter gaussian height gaussian width bias factor gaussian drop rate feature space normalized features true sim. save rate sim. temp table parameters used -metadynamics simulations alanine dipeptide across different figure mbar reweighted projection simulation onto latent space shows folded state becomes slightly stable upon mutation. similar results obtained simply summing hills using plumed estimate deposited bias. cases kcal/mol lowest value ensemble. parameter gaussian height gaussian width bias factor gaussian drop rate sim. save rate sim. temp number walkers walker read stride table parameters used replication study -metadynamics simulations domains. case wanted effects adding figure results replication study using different metadynamics parameters give similar result. integrated bias showing folded state stable fips. b-c) similar results obtained coarse graining frames states using mbar re-weight dynamics. figure comparison misfolded state folded state reveals presensce register shift first beta sheets. example residue colored salmon hydrogen bonding residue green light green/teal register shifted state. figure time dependence test convergence tica solution timescales function trajectory length. trajectory limited somewhere final length recomputed sparse tica model.", "year": "2018"}