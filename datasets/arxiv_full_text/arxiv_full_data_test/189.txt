{"title": "Spatio-temporal Dynamics of Intrinsic Networks in Functional Magnetic  Imaging Data Using Recurrent Neural Networks", "tag": "q-bio", "abstract": " We introduce a novel recurrent neural network (RNN) approach to account for temporal dynamics and dependencies in brain networks observed via functional magnetic resonance imaging (fMRI). Our approach directly parameterizes temporal dynamics through recurrent connections, which can be used to formulate blind source separation with a conditional (rather than marginal) independence assumption, which we call RNN-ICA. This formulation enables us to visualize the temporal dynamics of both first order (activity) and second order (directed connectivity) information in brain networks that are widely studied in a static sense, but not well-characterized dynamically. RNN-ICA predicts dynamics directly from the recurrent states of the RNN in both task and resting state fMRI. Our results show both task-related and group-differentiating directed connectivity. ", "text": "introduce novel recurrent neural network approach account temporal dynamics dependencies brain networks observed functional magnetic resonance imaging approach directly parameterizes temporal dynamics recurrent connections used formulate blind source separation conditional independence assumption call rnn-ica. formulation enables visualize temporal dynamics ﬁrst order second order information brain networks widely studied static sense well-characterized dynamically. rnn-ica predicts dynamics directly recurrent states task resting state fmri. results show task-related group-differentiating directed connectivity. functional magnetic resonance imaging blood oxygenation-level dependent signal provides powerful tool studying temporally coherent patterns brain intrinsic networks functional connectivity important outcomes fmri studies illuminate understanding healthy diseased brain function deep nonlinear approaches fmri exist tools available widely used generative models shallow linear structure. models typically shared parameterization structure learn common model across subjects refactor data constrained space provides straightforward analysis allows efﬁcient effective learning algorithms. popular methods independent component analysis begins hypothesis data mixture maximally independent sources. trainable many relatively simple optimization routines maximize non-gaussianity minimize mutual information however popular linear methods separating order-agnostic time multivariate signal time step treated independent identically distributed model degeneracy time convenient learning; assumption data explicit lack temporal dependence necessarily marginalizes dynamics must extrapolated post-hoc analysis. addition commonly used fmri studies uses parameterization across subjects allows either temporal spatial variability consequence optimized represent variation shape also representing variation time courses. encourage exaggerate time course statistics signiﬁcant variability shape size primarily accounted time courses. despite drawbacks beneﬁts using separating independent sources fmri data strongly evident numerous studies extent become dominant approach separating analyzing connectivity order overcome shortcomings temporal dynamics subject/temporal variability without abandoning fundamental strengths extend model sequences using recurrent neural networks resulting model call rnn-ica naturally represents temporal dynamics sequential objective easily trainable using back-propogation gradient descent. formalize problem source separation temporal dependencies formulate solution terms maximum likelihood estimation recurrent model parameterizes conditionally independent distribution problem generally understood inference unobserved latent conﬁgurations time-series observations. convenient assume sources stochastic random variables well-understood interpretable noise gaussian logistic variables independence constraints. representable directed graphical model time choice a-priori model structure relationship latent variables observations consequences model capacity inference complexity. directed graphical models often require complex approximate inference introduces variance learning. rather solving general problem equation assume generating function noiseless source sequences dimensionality data source signal composed conditionally independent components density parameterized recurrent neural network show learning objective closely resembles noiseless independent component analysis assuming generation noiseless preserves dimensionality reduce variance would otherwise hinder learning high-dimensional low-sample size data fmri. hypothesizes observed data linear mixture independent sources stnmmm {stnm} sources columns mixing matrix constrains sources maximally independent. framework presupposes speciﬁc deﬁnition component independence algorithms widely used fmri typically fall primary families kurtosis-based methods infomax although algorithms providing ﬂexible density estimation infomax algorithm model parameterized unmixing matrix w·xn. context fmri infomax objective seeks minimize mutual information subjects times. shown equivalent assuming prior density {stnm} m-dimensional vector. sources drawn logistic distribution shown infomax equivalent maximum likelihood estimation log-likelihood objective empirical density transformed generating example sequences done applying inverse unmixing matrix ordered sources. however cannot simply sample model generate samples observed figure basic recurrent units recurrent connections input connections output connections recurrent connections rolled out. sequence modeling generation. data attempt would simply generate unordered data true sequences. sources constrained marginally independent time; explicitly model dynamics training shufﬂed observed sequences regularly produce source structure. numerous graphical models methods designed model sequences including hidden markov models sequential monte carlo hmms popular simple generative directed graphical models time tractable inference learning traditional approach modeling language. however hmms place high burden hidden states encode enough long-range dynamics model entire sequences. recurrent neural networks hand capacity encode long-range dependencies deterministic hidden units. used conjunction objective resulting algorithm novel show much powerful approach blind source separation based conditional independence assumption. recurrent neural networks type neural network cyclic connections seen widespread success neural machine translation sequence-to-sequence learning sequence generation numerous settings. computing internal state across sequence index rnns apply parameters step. gives model properties translational symmetry directed dependence across time desirable expect directed dependence update rules across sequence. addition makes relatively memory-efﬁcient parameters used across sequence dimension. deterministic recurrent states recurrent connections take current observation hidden state input output next recurrent state. output connections take recurrent states input step output parameters conditional distribution. note model parameters recurrent parameters used every time step unique across sequence index square matrix recurrent weights input weights bias term. mappings various variables model need shallow deep neural networks model complex recurrent transitions. parameterizations gating types memory functions long short-term memory gated recurrent units used better model longer sequences also widely used. typically loss computed mini-batches instead entire dataset efﬁciency randomizing mini-batches training epoch. marginal density learned ﬁtting average marginal across time either parameters target distribution directly training neural network predict hidden state generates methods rnn-ica rnns already shown capable predicting signal bold fmri data though usually supervised setting. unsupervised framework sequence modeling easily extended incorporate infomax objective. deﬁne linear transformation observation source conﬁguration wxtn deﬁne high-kurtosis factorized source distribution pstn time step fmri sequence apply transformation fmri time series log-likelihood function whole sequence re-parameterized jacobian transformation source distribution pstn parameters determined recurrent states htn. high-kurtosis distribution desirable ensure independence sources reasonable choice outputs time step mean scale logistic distribution figure illustrates network structure time steps well forward back-propagated signal algorithm demonstrates training procedure rnn-ica. model network parameters weight un-mixing matrix subjects times. treatment assumes weight matrix square necessary ensure tractable determinant jacobian inverse. fmri data high dimensional reduce dimensionality must resort sort dimensionality reduction preprocessing. widely used dimensionality reduction studies fmri principle component analysis used reduce data match selected number sources stn. note rnns deeper architectures successful generative tasks rnn-ica could beneﬁt deeper architecture capable inferring complex relationships data. however fmri data often composed number training samples found necessary demonstrate ability rnn-ica learn meaningful sources simple architecture. leave architectural improvements rnn-ica future research. figure ica. preprocessed fmri images transformed dimensionality-reduced using pre-trained pca. components passed square matrix every subject time-point. components also passed input compute hidden states help previous state. states used compute likelihood next source time. source time series computed well likelihoods loss back-propagated network training. ﬁrst apply rnn-ica synthetic data simulated evaluate model performance subsequently real functional magnetic imaging data. fmri analyses typically falls categories task-based resting state analysis. task experiments typically involve subjects exposed time-series stimulus task-speciﬁc components extrapolated. case rnn-ica reveal task-related directed connectivity spatial variability addition usual task-relatedness activity ica. resting-state data often used conﬁrm presence distinct functional states brain. chose dataset resting state experiment also simultaneous electroencephalography ground-state subject neurobiological states could derived. rnn-ica able correspondence predicted activation deﬁned model changes state. result provide means prevent false positives negatives interpreting resting state network inter-group differences owing different sleep stages present examined cohorts. experiments simulated data test model generated synthetic data using simtb toolbox framework developed assess dynamics functional connectivity described figure lehmann total subjects corresponding groups subjects simulated healthy simulated schizophrenia patients generated. time courses generated simhc simsz subject initialize unmixing matrix initialize recurrent input weights bias initialize output weights {xtn}n repeat draw random samples sequences transform component sequence unmixing matrix initialize ﬁrst hidden state discussed section compute parameters initial probability distribution update hidden state conditional in-sequence compute parameters conditional probability time pstn compute negative likelihood perform gradient descent parameters constraint states transition probability matrix group dictates state transitions derived data prior work real data initial state probabilities also derived work. sequence time points seconds generated. total subjects generated ﬁrst group used training remaining samples used testing model. parameters hemodynamic response model used simulate data also varied subject introduce heterogeneity. known initial state subject transition probability matrix governs transitions ensured ground truth state transition vector rnn-ica model trained subject training data epochs model parameters similar subsequent sections. resultant sources source distributions predicted hidden unit activations subject correlated subject’s ground truth state vector. trained model test data correlations computed model outputs state vectors. computed group differences correlation distributions simhc simsz groups summarized training test cases figure results show rnn-ica generalized group differences well test setting represented hidden state activations scaling factor. task experiments demonstrate properties strengths model apply method task fmri data. data used work comprised task-related scans healthy participants subjects diagnosed schizophrenia gave written informed hartford hospital yale approved consent institute living compensated participation. participants scanned auditory oddball task involving detection infrequent target sound within series standard novel sounds. detailed information regarding participant demographics task details provided swanson scans acquired olin neuropsychiatry research center institute living/hartford hospital siemens allegra dedicated head scanner equipped mt/m gradients standard quadrature head coil. functional scans acquired trans-axially using gradient-echo echo-planar-imaging following parameters repeat time echo time ﬁeld view acquisition matrix angle voxel size slice thickness slices ascending acquisition. dummy scans acquired beginning allow longitudinal equilibrium paradigm automatically triggered start scanner. ﬁnal dataset consisted volumes subject. data underwent standard pre-processing steps using software package subject scans masked global mean image voxel variance normalized. voxel timecourses detrended using th-degree polynomial repeated subjects. applied complete dataset without whitening ﬁrst components kept reduce data. finally component mean removed entered model. figure simhc versus simsz group differences correlation hidden unit activations component scale factors ground truth state vectors train test subjects. shown values. note hidden units component scale factors track group differences train test cases although lower strength test cases. dashed black line corresponds false discovery rate threshold rnns data segmented windowed data shufﬂed arranged random batches. loading matrix subject comprised time courses length segmented equal-length windowed slices using window size stride number components roughly corresponds number found studies time steps equivalent seconds shown provides good trade-off terms capturing dynamics overly sensitive noise ﬁnal dataset comprised volumes subjects time courses each. randomly shufﬂed epoch batches volumes random subjects time points. used simple recurrent hidden units recurrent parameterization equation anticipate needing model long range dependencies necessitate gated models initial hidden state -layer feed forward network softplus imposed unmixing matrix additional regularization decay rate model trained using rmsprop algorithm learning rate epochs. figure selected spatial maps rnn-ica without spatial corrections. maps ﬁltered hand omitting grey matter ventricle motion artifact features. source mean-predicted uncertainty target novel stimulus time courses shown normalized respective variance offset easier visualization. thresholded standard deviations grouped according spatial maps sign ﬂipped along respective time courses ensure distribution back-reconstructed voxels positive skew. truncated labels found visual inspection afni package correspond mifg middle frontal gyrus mefg medial frontal gyrus smefg superior medial frontal gyrus inferior frontal gyrus morbg middle orbital gyrus inferior parietal lobule superior parietal lobule inferior occipital gyrus middle occipital gyrus superior occipital gyrus inferior temporal gyrus superior temporal gyrus supramarginal pocg postcentral gyrus precg precentral gyrus paracl paracentral mcing middle cingulate acing anterior cingulate pcing posterior cingulate angular gyrus basal ganglia supplementary motor area fusiform gyrus cerebellar vermis calcarine gyrus frontoparietal default-mode network parag parahippocampal gyrus lingg lingual gyrus white matter white matter precun. precuneus thal. thalamus vis. visual temp. temporal cere. cerebellum cun. cuneus puta. putamen cing. cingulate caud. caudate pari. parietal front. frontal insula vent. ventricle. figure functional network connectivity matrix essentially temporal cross correlation matrix case averaged across subjects. grouping found using multi-level community algorithm blondel ordering axes according groups. labels indicate regions interest majority positive negative voxel values respectively. figure shows spatial maps back-reconstructed. spatial maps ﬁltered original omitting white matter ventricle motion artifact features. spatial maps along respective time-courses sign-ﬂipped ensure back-reconstructed distribution voxels positive skew. maps highly analogous typically found linear though combined positive/negative features map. figure shows functional network connectivity matrix components grouped according multi-level community algorithm blondel using symmetric temporal cross-correlation matrix. subject component performed multiple linear regression sources predicted means predicted scale-factor subject target novel stimulus. table shows p-values -sample t-test beta values across subjects components values many components show similar task-relatedness across source time courses predicted means notably temporal gyrus features parietal lobule default mode network addition shows strongest task-relatedness scale factor. table p-values -sample t-test beta-values target novel stimulus sources predicted means predicted scale-factor σtn. beta-values found subject component using multiple regression target novel stimulus t-tests performed component subjects. among mostsigniﬁcant task-related components target stimulus include middle temporal gyrus default mode network parietal lobule. legend label names found caption figure label name specify sign figure p-values speciﬁes sign corresponding t-value. derivatives tractable means differentiable functions w.r.t input xt−. derivatives interpreted measure directed connectivity components time represent predicted change future component given change previous component. full jacobian provides directed connectivity source pairs time simplify analysis looked next-time terms connection averaged across time subjects sign removed covariance σ¯νi standard deviation across components indexed grouping done constructing undirected graph using pearson coefﬁcients clustering vertices using communitybased hierarchical algorithm above. example directed connectivity graph spatial maps given figure next-step jacobian terms used time-courses multiple-regression target novel stimulus signiﬁcance tested using one-sample t-test time courses two-sample t-test across groups. resulting task-related directed connectivity represented figure targets novels example graph spatial maps presented figure group-differentiating relationships given figure example graph spatial maps given figure resting state experiments evaluated model resting state data show rnn-ica viable model demonstrate properties network correspond wake/sleep states. resting state functional data collected subjects minutes siemens trio scanner subjects transitioned wakefulness sleep stage data approved ethics committee goethe university. simultaneous acquired facilitating sleep staging aasm criteria resulting hypnogram subject sleep stae). discarded ﬁrst time points account equilibration effects. performing rigid body realignment slice-timing correction subject data warped space using spm. voxel time courses despiked using afni. regressed voxel time courses respect head motion parameters mean white matter signals. next averaged time subjects. features figure graphical representation next-time jacobian terms ∂µit grouped multi-level community algorithm blondel using pearson correlation coefﬁcient deﬁne undirected graph corresponding rois provided right complete legend found figure grouping done constructing undirected graph using pearson coefﬁcients clustering vertices using standard community-based hierarchical algorithm. used model training procedure task data analysis previous section. subjects subjects used training subjects left testing. examined correspondence hidden recurrent units trained model subject hypnogram well mean scale predictive source distribution hypnogram. similar tests model outputs left test cases. activity several hidden recurrent units trained model predictive wakefulness across subjects hidden unit activity stays extremes awake state exhibiting higher standard deviation activity tends towards zero lower standard deviation subject transitions wakefulness sleep. one-way anova absolute mean standard deviation hidden unit activity hypnogram state shows signiﬁcant group differences mean standard deviation subsequent posthoc t-tests reveal signiﬁcant reductions wakefulness light sleep state deeper sleep stages states also states standard deviations p-values correcting multiple comparisons). addition scaling factor tended correlate well changes state figure graphical representation target novel task-signiﬁcant next-time jacobian terms target stimulus directed connectivity thresholded novel directed connectivity thresholded target novel graphs thresholded different values cleaner graphical representations. legend nodes figure measures correlation smoothed derivative hypnogram. figure shows correlation coefﬁcients hidden units subject hypnogram state component scale factors subject hypnogram vector. several hidden states show consistent correlation hypnograms indicating encoding subject sleep state. similarly component scale factors also encode sleep states. surprisingly however source time courses means not. finally component scale factors correlate somewhat consistently changes state across subjects. indicates model encoding changes state terms uncertainty. discussion conclusion summary work demonstrate recurrent neural networks used separate conditionally independent sources analogous independent component analysis beneﬁts modeling temporal dynamics recurrent parameters. results show approach effective modeling task-related resting-state functional magnetic imaging data. using approach able separate similar components additional beneﬁt directly analyzing temporal dynamics recurrent parameters. notably addition ﬁnding similar maps task-relatedness able derive directed temporal connectivity task-related group-differential derived directly parameters rnn. addition resting state data found hidden unit activity corresponded well wake/sleep states uncertainty factor consistent changes state learned completely unsupervised way. related work method introduces deep nonlinear computations time maximum likelihood estimation independent component analysis without sacriﬁcing simplicity linear relationships source observation. equivalent learning objective infomax widely used fmri studies sources drawn factorized logistic distribution model learns linear transformation data sources unmixing matrix source dynamics encoded deep nonlinear transformation recurrent structure represented rnn. alternative nonlinear parameterizations figure graphical representation target novel group-differentiating next-time jacobian terms target stimulus directed connectivity thresholded legend nodes figure shows inﬂuence components across time different stimulus present vary across groups. transformation exist deep neural networks shown work fmri data approaches allow deep nonlinear static spatial maps compatible learning objective. temporal used group like spatial capture temporal dynamics summaries onetwo-stage preprocessing step. temporal summaries captured analyzed however learned part end-to-end learning objective. overall strengths rnn-ica compared methods dynamics directly learned model parameters allows richer higher-order temporal analyses showed previous section. recurrent neural networks typically incorporate latent variables requires expensive inference. versions incorporate stochastic latent variables exist trainable variational methods working approaches sequential data exist however require complex inference introduces variance learning make training fmri data challenging. method instead incorporates concepts noiseless reduces inference inverse generative transformation. consequence temporal analyses relatively simple relying tractable computation jacobian component conditional densities given activations. figure select hidden unit time courses corresponding hypnogram subjects training data test subjects used training. hidden states track subject neurobiological state sleep stages) using fmri activity. bold lines median ﬁltered activation time courses. future work rnn-ica model provides unique mode analysis previously unavailable fmri research. results encouraging able task-related group-differentiating directed connectivity however broader potential approach unexplored. belief method expand neuroscience research involves temporal data leading signiﬁcant conclusions. finally uncertainty factor resting state experiments indicate novel application imaging data rnn-ica change-of-state detection. model employed simple intended take advantage this. quite possible modiﬁcations could produce model reliably predicts change-of-state fmri data.", "year": "2016"}