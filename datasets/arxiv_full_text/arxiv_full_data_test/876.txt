{"title": "From Neuronal Models to Neuronal Dynamics and Image Processing", "tag": "q-bio", "abstract": " This paper is an introduction to the membrane potential equation for neurons. Its properties are described, as well as sample applications. Networks of these equations can be used for modeling neuronal systems, which also process images and video sequences, respectively. Specifically, (i) a dynamic retina is proposed (based on a reaction-diffusion system), which predicts afterimages and simple visual illusions, (ii) a system for texture segregation (texture elements are understood as even-symmetric contrast features), and (iii) a network for detecting object approaches (inspired by the locust visual system). ", "text": "neurons make contacts synapses presynaptic neuron sends output synapse axon postsynaptic neuron receives dendritic tree neurons produce output form virtually binary events classes neurons nevertheless generate spikes show continuous responses. dendrites classically considered passive cables whose function transmit input signals soma picture neuron integrates input signals generates response exceeds threshold. therefore neuron site computations take place information stored across network synaptic weights connectionist point view neuronal functioning inspired neuronal networks learning algorithms error backpropagation recent deep-learning architectures recent evidence however suggest dendrites excitable structures rather passive cables perform sophisticated computations suggest even single neurons carry complex computations previously thought. naturally modeller choose reasonable level abstraction. good model necessarily incorporates possible details many details make diﬃcult identify important mechanisms. level detail related research question wish answer also computational resources example interest understanding neurophysiological details small circuit neurons ∗first published chapter bookbiologically inspired computer vision fundamentals applications. gabriel cristobal laurent perrinet matthias keil isbn ---- wiley probably would choose hodgkin-huxley model neuron also include detailed models dendrites axons synaptic dynamic learning. disadvantage hodgkin-huxely model high computational complexity. requires ﬂoating point operations simulation time therefore want simulate large network many neurons omit dendrites axons simpler model integrate-andﬁre neuron model izhikevich model izhikevich model oﬀers rich spiking dynamics full hodgkin-huxley model computational complexity similar integrate-and-ﬁre neuron model simpliﬁcations made simulating psychophysical data solving image processing tasks. example spiking mechanisms often necessary then compute continuous response neuron’s state variable thresholding sigmoidal function chapter approaches neural modeling computational neuroscience respectively tutorial-like fashion. means basic concepts explained simple examples rather providing in-depth review speciﬁc topic. proceed ﬁrst introducing simple neuron model derived considering neuron electrical circuit. membrane potential equation augmented spiking mechanism known leaky integrate-and-ﬁre model. leaky means model forgets exponentially fast past inputs. integrate means sums inputs excitatory inhibitory finally refers spiking mechanism. membrane potential equation subsequently applied three image processing tasks. section presents reaction-diﬀusion-based model retina accounts simple visual illusions afterimages. section describes model segregating texture features inspired computations primary visual cortex finally section introduces model collisionsensitive neuron locust visual system last section discusses examples image processing methods also could explain brain processes corresponding information. order derive simple powerful neuron model imagine neuron’s cell membrane small sphere. membrane bilayer lipids thick isolates extracellular space cell’s interior thus forms barrier diﬀerent species ionic concentrations extracellular ﬂuid cytoplasm same would probably dead. neuronal signalling relies presence ionic gradients. cells rest half brain’s energy budget consumed moving outside cell inward types neurons also pump outside pumping mechanisms compensate ions leak cell membrane respective reverse directions driven electrochemical gradients. rest neuron therefore maintains dynamic equilibrium. ready model neuron electrical circuit capacitance connected parallel resistance battery. battery sets cell’s resting potential vrest. particular cells negative resting potential typically vrest −mv. resting potential value membrane voltage ionic concentrations dynamic equilibrium. course simpliﬁcation since lump together diﬀusion potentials species. simpliﬁcation comes cost however resulting neuron model able produce action potentials spikes without explicitly adding spike-generating mechanism ohm’s current leaks membrane ileak gleak leakage conductance gleak inverse membrane resistance. charge kept apart cell’s membrane capacitance cvm. whenever neuron signals distribution charges changes membrane potential thus dq/dt cdvm/dt non-zero. words current carried ions. assuming ﬁxed value change slower higher capacitance kirchhoﬀ current equivalent current conservation ileak right hand side corresponds current ﬂows excitation inhibition biologically current ﬂows occur across protein molecules embedded cell membrane. various protein types implement speciﬁc functions ionic channels enzymes pumps receptors. gates doors cell membrane highly speciﬁc particular information substances enter exit cell. strictly speaking channel embedded neuron’s cell membrane would corresponds rc-circuit equation fortunately neurons small justiﬁes assumption channels uniformly distributed potential vary across cell membrane cell said isopotential adequately described single rc-compartment. let’s assume nothing better waiting suﬃciently long time neuron reaches equilibrium. then deﬁnition constant thus remains last equation vrest absence excitation inhibition neuron resting potential. long wait equilibrium reached? move terms side equation term contains time other. technique known separation variables permits integration order convert inﬁnitesimal quantities normal variables easy vrest absence external currents time wait equilibrium reached depends higher resistance bigger capacitance longer take constant selected according initial conditions problem. example assume neuron rest start simulation vrest therefore neurons loners massively connected neurons. connection sites called synapses. synapses come ﬂavours electrical chemical. electrical synapses directly couple membrane potential neighbouring neurons. distinct networks speciﬁc neurons formed usually neurons type. examples electrically coupled neurons retinal horizontal cells cortical low-threshold-spiking interneurons cortical fast-spiking interneurons sometimes chemical electrical synapses even combine permit reliable fast signal transmission case locust lobula giant movement detector connects descending contralateral movement detector chemical synapses common junctions. cubic millimeter cortical grey matter billion chemical synapses synapses usually plastic. whether increase decrease connection strength post-synaptic neuron depends causality. pre-synaptic neuron ﬁres within post-synaptic neuron connection gets stronger contrast pre-synaptic spike arrives activation post-synaptic neuron synaptic strength decreased mechanism known spike time dependent plasticity identiﬁed hebbian learning synaptic potentiation thought triggered back-propagating calcium spikes dendrites postsynaptic neuron synaptic plasticity occur several timescales short term long term remember acronyms letter combinations like std. activation fast chemical synapses causes rapid transient voltage change post-synaptic neuron. voltage changes called postsynaptic potentials psps either inhibitory excitatory excitatory neurons depolarize target neurons whereas inhibitory neurons hyperpolarize post-synaptic targets. model synapses? psps caused temporary increase membrane conductance series so-called synaptic reversal battery synaptic input deﬁnes current right hand side equation last equation sums synaptic inputs conductance reversal potential notice whether input acts excitatory inhibitory membrane potential depends usually whether synaptic battery bigger smaller resting potential vrest. consider type excitatory inhibitory input. write solve equation? converting diﬀerential equation diﬀerence equation equation solved numerically standard integration schemes euler’s method runge-kutta cranknicolson adams-bashforth chapter details). typically model neurons integrated step size less relevant time scale neuronal signalling. simulation focusses perceptual dynamics choose bigger integration time constant well. ideal integration method stable produces solutions high accuracy computational complexity. practice course make trade-oﬀ. remember gleak called leakage conductance inverse membrane resistance. constant capacitance leakage conductance determines time constant c/gleak neuron bigger values gleak make faster smaller values cause higher degree low-pass ﬁltering input. vexc vrest vinh vrest excitatory inhibitory synaptic battery respectively. vexc vrest synaptic current inward negative convention. membrane thus gets depolarized. signature epsp. brain common type excitatory synapses release glutamate neurotransmitter diﬀuses across synaptic cleft binds glutamate-sensitive receptors post-synaptic cell membrane. consequence channels open enter cell. agonists pharmacological substances exist brain open channels well. instance agonist nmda open excitatory voltage-sensitive nmda-channels. ampa another agonist activates fast excitatory synapses. however ampa-synapses remain silent presence nmda vice versa. therefore imagine ionic channels locked doors. opening right necessary either speciﬁc neurotransmitter artiﬁcial pharmacological agonist. locks receptor sites neurotransmitter agonist binds. reversal potentials fast ampa-synapse resting potential. usually ampa-channels co-occur nmda-channels enhance computational power neuron vinh vrest membrane hyperpolarized. vinh vrest gets less sensitive depolarization accelerates return vrest synaptic input. pre-synaptic release gaba activate three subtypes receptors gabaa gabab gabac synaptic batteries also called reversal potentials? excitatory input vexc imposes upper limit means matter gexc drive neuron vexc. order understand that consider so-called driving potential vexc driving potential high neuron depolarizes fast. closer gets vexc smaller driving potential excitatory current gexc eventually approaches zero. value equilibrium? equilibrium means change left hand side equation zero. course implies excitatory inhibitory inputs vary suﬃciently slow consider constant. otherwise expressed neuron -methyl-d-aspartat α-amino--hydroxy--methyl--isoxalone propionic acid from ref. γ-aminobutryc acid gabaa ligand-gated channels permeable cl−. post-synaptic gabab heptahelical receptors coupled inwardly rectifying channels. finally gabac ligand-gated channels primarily expressed retina. time equilibrium reached depends gleak active conductances. consequence neuron receives continuously input neurons react faster neuron starts vrest specially interesting case deﬁned vinh vrest called silent shunting inhibition. silent gets evident neuron depolarized shunting inhibition decreases time constant neuron thus making faster. return resting potential accelerated excitatory inhibitory input. furthermore divisive inhibition special form shunting inhibition vrest spiking neurons however pure divisive inhibition seem exist. case shunting inhibition rather subtractive cannot gain control mechanism. networks balanced excitation inhibition choice ours’ change balance excitation inhibition eﬀect neuron’s response additive subtractive respectively. leave balance unchanged increase decrease excitation inhibition parallel multiplicative divisive eﬀect neuron’s response occur equation represents membrane potential neuron could represent diﬀerent quantities well. example could interpreted directly response probability example vrest vexc vinh accordingly latter case another possibility vinh neuronal responses positive however half-wave rectiﬁed meaning take output neuron. naturally half-wave rectiﬁcation makes sense negative values occur. absence explicit spiking neuron’s output represents ﬁring rate usually interpreted spikes second. equation depends mainly purpose simulation. synaptic input consists spikes needs mechanism convert continuous quantities. low-pass ﬁltering characteristics equation that. instance spikes necessary implementing spike-time-dependent plasticity modiﬁes synaptic strength dependent prepostsynaptic activity. purposes spikes strictly necessary figure spikes postsynaptic potentials ﬁgure shows three methods converting spike postsynaptic potential α-function tpeak represented gray curve. result low-pass ﬁltering spike shown dashed line curve sudden rise gradual decay. finally applying times low-pass ﬁlter spike results black curve. thus -pass low-pass ﬁlter approximate α-function reasonably well. ﬁgure shows psps output model neuron equation endowed spike mechanism. excitatory pale green curve) inhibitory pale curve) psps cause corresponding ﬂuctuations membrane potential soon membrane potential crosses threshold vthresh spike added afterwards membrane potential reset vreset half-wave rectiﬁed membrane potential represents neuron’s output. input model neuron random spikes generated according poisson process random spikes converted psps simple low-pass ﬁltering ﬁlter memories βexc βinh respectively weight wsyn integration method crank -nicolson step size rest parameters equation vexc vinh vrest gleak ﬁring rate pre-synaptic neurons directly input post-synaptic neurons. case equation could used. steady-state solution however less computational complexity need integrate numerically. recall using steady-state solution implicitly assumes synaptic input varies relatively slow time scale neuron reach equilibrium state moment. straightforward convert leaky integrate-and-ﬁre neuron. term leaky refers leakage conductance gleak neuron would perfect integrator gleak soon membrane potential crosses neuron’s response threshold vthresh record pulse amplitude neuron’s response. otherwise response usually deﬁned zero order account afterhyperpolarization value vreset spike. usually vreset vrest chosen. refractory period neuron time ionic pumps need reestablish original charge distributions. within absolute refractory period neuron spike spiking probability greatly reduced spiking mechanisms conceivable well. example deﬁne model neuron’s response identical ﬁring rate spike whenever vthresh. vthresh would represent spiking threshold response thus switches rate code spike code typical spike train produced latter mechanism shown ﬁgure binary events spikes converted post-synaptic potentials sharp rise smooth decay therefore usually broader spike evoked. assume spike arrives time post-synaptic neuron. time course corresponding adequately described so-called α-function weight wsyn. instead using α-function simplify matters assuming spike causes instantaneous increase gexc ginh respectively followed exponential decay. amounts adding simple diﬀerential equation model neuron time constant determines rate exponential decay wsyn synaptic weight kronecker delta function argument zero i-th spike increments wsyn. last equation low-pass ﬁlters spikes. easy-to-compute discrete version obtained converting last equation ﬁnite diﬀerence equation either forward backward diﬀerencing forward diﬀerentiation ∆t/τ integration time constant comes approximating dx/dt /∆t. backward diﬀerencing degree low-pass ﬁltering determined ﬁlter output reproduces input spike pattern ﬁlter ignores input spikes stays forever value initialized value zero ﬁltering takes place ﬁltering gets stronger increasing spikes ﬁltered equation sudden increase time followed gradual decay sudden increase stands contrast gradual increase predicted equation better approximation shape results applying low-pass ﬁltering twice shown black curve ﬁgure tantamount simulating equations synaptic input equation retina powerful computational device. transforms light intensities diﬀerent wavelengths captures cones eﬃcient representation sent brain axons retinal ganglion cells term eﬃcient refers redundancy reduction stimulus hand coding eﬃciency level ganglion cells other. decorrelation means predictable intensity levels time space suppressed responses ganglion figure test images images rows columns. upper lower grating separated small stripe called test stripe. although test stripe luminance throughout humans perceive wave-like pattern opposite brightness inducers. inducers white test stripe appears darker vice versa. inducer gratings opposite phase illusory luminance variation across test stripe weak absent. real-world image photograph luminance staircase used illustrate afterimages ﬁgure cells example digital photograph clear blue spatial redundancy select blue pixel highly probable neighbours blue pixels well coding eﬃciency linked metabolic energy consumption. energy consumption increases faster information transmission capacity organisms therefore seem evolved tradeoﬀ increasing evolutionary ﬁtness saving energy retinal ganglion cells show eﬃcient coding sense noisy energetically expensive coding symbols less used often spatial aspects visual information processing retina grossly approximated employing diﬀerence-of-gaussian model gaussians typically two-dimensional isotropic centered identical spatial coordinates. resulting model convolution kernel positive values center surrounded negative values. mathematical terms dog-kernel ﬁlter takes second derivative image. signal processing terms bandpass ﬁlter center-surround antagonism retinal ganglion cells modelled on-center ganglion cells respond center illuminated surround off-cells respond surround receives light intensity center model thus assumes symmetric onoffresponses simpliﬁcation diﬀerences biological onganglion cells include receptive ﬁeld size response kinetics nonlinearities light-dark adaptation naturally convolving image ﬁlter neither cannot account adaptation dynamical aspects retinal information processing. hand however many retinal figure simulation grating induction dynamic retina predict illusory luminance variation across test stripe here image ﬁgure assigned plot shows temporal evolution horizontal line centered test stripe shows columns ﬁxed number diﬀerent instances time time increases towards background. values interpreted brightness values darkness wave pattern adequately predicts grating induction eﬀect. image ﬁgure assigned wave-like pattern twice frequency inducer gratings moreover strongly reduced amplitude. thus dynamic retina correctly predicts greatly reduced brightness modulation across test stripe. models target explanation physiological psychophysical data suitable image processing tasks. biologically-inspired image processing means model solve image processing task time produce predictions features large consistent psychophysics biology spirit introduce simple dynamical model retinal processing re-produces interesting brightness illusions could even account afterimages. afterimage illusory percept continues stimulus physically present unfortunately author unable version model could strictly based equation instead that even simpler version based temporal low-pass ﬁlter gray level image luminance values zero number iterations denoted then figure snapshots dynamic retina on-response iterations dynamic retina image ﬁgure assigned darker gray levels indicate higher values ]+). corresponding offresponses shown darker gray levels indicate higher off-responses. luminance staircase assigned image replaced image camera man. simulates retinal saccade. consequence ghost image luminance staircase visible onoff-responses onoff-responses indistinguishable here brighter gray levels indicate higher values corresponding off-responses again brighter gray levels indicate higher values simulations performed ﬁlter memory constants diﬀusion coeﬃcient on-cell responses off-cell responses const diﬀusion coeﬃcient ~∇vt div) diﬀusion operator discretized convolution kernel center north east south west pixels. corner pixels zero. thus whereas receptive ﬁeld center pixel surround dynamically constructed diﬀusion. diﬀusion length depends ﬁlter memory constant diﬀusion coeﬃcient figure shows four test images used testing dynamic retina model. image shows visual illusion observers perceive illusory modulation luminance gratings although luminance actually constant. figure shows dynamic retina predicts wave-like activity pattern grating on-responses represent brightness offresponses represent darkness dynamic retina correctly predicts grating induction. also account absence grating induction ﬁgure corresponding simulation shown ﬁgure amplitude wave-like pattern strongly reduced frequency doubled. thus absence grating induction adequately predicted. equilibrium state dynamic retina performs contrast enhancement boundary detection respectively. illustrated onoffresponses image ﬁgure comparison ordinary dog-ﬁlter however responses dynamic retina asymmetric somewhat higher off-responses luminance step nice feature dynamic retina prediction images. illustrated computing ﬁrst responses luminance staircase replacing staircase image image camera figure shows corresponding responses immediately images swapped. although camera image assigned slightly blurred afterimage staircase still appears persistence afterimage depends luminance values higher intensities ﬁrst image lower intensities second image promote prolonged eﬀect. figure texture segregation illustration processing gray-scale image texture system. input image lena pixels superimposed numbers. output retina on-activity white black. analysis retinal image proceeds along four orientation channels. image shows intermediate result after summing across four orientations afterwards local wta-competition suppresses residual features desired leading texture representation. texture representation input image represents ﬁnal output texture system. before texture brightness white texture darkness black. networks based equation used biologically plausible image processing tasks. nevertheless speciﬁc task wishes achieve imply modiﬁcations equation example outline corresponding network segregating texture gray scale image omit many mathematical details point probably would make reading cumbersome. texture system forms part theory explaining early visual information processing essential proposal simple cells segregate visual input texture surfaces luminance gradients. idea emerges quite naturally considering symmetry scale simple cells relate features visual world simple cells small odd-symmetric receptive ﬁelds respond preferably contours caused changes material properties objects reﬂectance object surfaces delimited odd-symmetric contours. likewise even-symmetrical simple cells respond particularly well lines points call texture context. texture features often superimposed object surfaces. texture features variable rather irrelevant recognize certain object also correspond identifying feature finally simple cells coarse resolutions supposed detect shallow luminance gradients. luminance gradients pictorial depth resolving three-dimensional layout visual scene. however ignored determining material properties object surfaces. ﬁrst computational step three systems consists detecting respective features. following feature detection representations surfaces gradients texture respectively eventually build corresponding system. normal visual perception would result superimposing three representations three separate representations advantage higher-level cortical information processing circuits could selectively suppress reactivate texture and/or gradient representations addition surface representations. ﬂexibility allows diﬀerent requirements deriving material properties objects. instance surface representations directly linked perception reﬂectance object recognition respectively. contrast computation surface curvature interpretation three-dimensional scene structure relies gradients and/or texture representations identify texture features? start processing gray scale image retinal model based modiﬁcation equation input image center kernel pixel. result convolving input image surround kernel north east west south position. elsewhere zero. thus approximates second spatial derivative image. accordingly deﬁne types ganglion cell responses on-cells deﬁned respond preferably increments luminance. off-cells prefer decrements luminance. figure shows output equation half-wave rectiﬁed membrane potential max. biological ganglion cell responses saturate increasing contrast. modeled gijsi gijsi corresponds self-inhibition center surround activated stronger. mathematically gijsi constant determines fast responses saturate. simply gexc ginh equation latter case response amplitudes would diﬀerent luminance step luminance steps odd-symmetric features texture. thus want suppress them suppression easier response amplitudes equal. response even-symmetric features distinguish response patterns. black line white background produces on-off-on response central off-response ﬂanking off-responses much smaller amplitudes. analogously bright line dark background trigger off-on-off response pattern. onresponses lines vary essentially dimension. accordingly analyze along four orientations. orientation selective responses established convolving offchannel oriented gaussian kernel subtract on-channel. deﬁnes texture brightness. channel texture darkness deﬁned subtracting blurred responses responses. subsequently even-symmetric response patterns enhanced respect surface features. order boost pattern left-off multiplied central-on right-off note surface response pattern ideally ﬂanking response. dendritic trees plausible neurophysiological candidate implementing logic gate subsequent stage orientated texture responses summed across orientations leaving non-oriented responses means winnertakes-all competition adjacent texture brightness texture darkness possible suppress residual surface features hand ﬂanking responses texture features other. example response pattern generate competition since texture feature central response bigger survive competition ﬂanking responses. ﬂanking responses however not. surface response survive either dl-responses equal amplitudes. local spatial wta-competition established nonlinear diﬀusion paradigm ﬁnal output texture system computed according equation texture brightness acts excitatory texture darkness inhibitory. illustration figure video sequences showing object approaches video sequences served input equation shows couple representative frames video drives still observer. except camera shake background motion present video. actually collide observer. video frames shown bottom drives static obstacle. sequence implies background motion. observer actually collides balloon ﬂies impact. many animals show avoidance reactions response rapidly approaching objects animals visual collision detection also attracted attention engineering prospective applications example robotics driver assistant systems. widely accepted visual collision detection biology mainly based angular variables angular size approaching object angular velocity rate expansion object approaches observer constant velocity angular variables show nearly exponential increase time. biological collision avoidance stop here computes mathematical functions accordingly three principal classes collision-sensitive neurons identiﬁed classes neurons found animals diﬀerent insects birds. therefore evolution came similar computational principles shared across many species particularly well studied neuron lobula giant movement detector neuron locust visual system neuron relatively easy access. responses lgmd object approaches described so-called eta-function evidence lgmd biophysically implements logarithmic encoding converts product representing excitation inhibition. distinctive property eta-function response maximum collision would occur. time response maxfigure simulated ldmd responses ﬁgures show rectiﬁed lgmd activities computed equation lgmd activities onedimensional signals vary time responses video shown ﬁgure observer move background motion generated. ldmd responses peak collision would occur. responses video shown bottom ﬁgure observer moves resulting background motion causes spurious lgmd activity small amplitude collision. collision time indicated dashed vertical line. on-lgmd activity peaks frames collision. off-lgmd activity collision generated balloon swirling moving away observer. imum determined constant always occurs ﬁxed angular size arctan. much theory angular variables computed sequence image frames? model presented compute explicitly although output resembles eta-function. however eta-function explicitly computed either. without going ongoing debate biophysical details computations carried lgmd model rests lateral inhibition order suppress self-motion background movement ﬁrst stage model computes diﬀerence consecutive image frames diﬀusion coeﬃcient diﬀusion layers inhibitory serve attenuate background movement translatory motion caused self-movement. approaching object suﬃciently away spatial variations small. similarly translatory motion speed also generate small spatial displacements. activity propagation proceeds constant speed thus acts predictor small movement patterns. diﬀusion cannot keep spatial variations generated late phase approach. output diﬀusion layer respectively input diﬀusion layers brought feeding back activity third stage model excitatory input hence receives types inhibition first directly inhibits inhibitory input ginh. second gates excitatory input gexc activity ﬁrst stage attenuated positions means decrease feedback assures also activity grow then. largely avoided diﬀusion layers continuously accumulate activity eventually drown positions drowning otherwise would occur presence strong background motion making model essentially blind object approaches. diﬀusion layers contributes reduction drowning. ﬁfth ﬁnal stage model represents lgmd neuron spatially sums output previous stage rectiﬁed lgmd activities respectively. figure shows representative frames video sequences used input model. figure shows corresponding output computed last equation. simulated lgmd responses nice clean absence background movement presence background movement hand produces spurious lgmd activation collision occurs neurodynamical models -based image processing algorithms typically diﬀer designed respective predictions regard neuroscience. pde-based image processing algorithms derive often optimization principle result diﬀerential equations usually obtained evolves time many algorithms designed speciﬁc image processing tasks segmentation denoising usually designed according neuronal circuits. similarly usually predict psychophysical results. remarkable exceptions however exist. example color enhancement algorithm described reference based perceptually motivated energy functional includes contrast term dispersion term similar vein could retinex algorithm estimating perceived reﬂectance also casted variational framework algorithm tone mapping high dynamic range images originally motivated compressing high contrasts preserving contrasts although authors explicitly acknowledge inspiration neuroscience nevertheless striking algorithm resembles ﬁlling-in architectures filling-in proposed mechanism computing smooth representations object surfaces visual system. smooth representations means surfaces tagged perceptual value color movement direction depth lightness filling-in often modeled activity propagation within compartments deﬁned contrast boundaries recently proposed ﬁlling-in based computer vision algorithm identiﬁes regions coherent movement direction optic ﬁeld algorithm proposes also solution so-called aperture problem based corresponding computations brain’s visual system method resembles ﬁlling-in process image impainting image impainting completes missing regions propagating structure surround region. although image impainting related neurophysiological principles similar ﬁlling-in eﬀects seem exist also brain", "year": "2018"}