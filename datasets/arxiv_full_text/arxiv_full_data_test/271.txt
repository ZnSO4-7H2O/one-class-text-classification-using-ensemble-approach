{"title": "The Same Analysis Approach: Practical protection against the pitfalls of  novel neuroimaging analysis methods", "tag": "q-bio", "abstract": " Standard neuroimaging data analysis based on traditional principles of experimental design, modelling, and statistical inference is increasingly complemented by novel analysis methods, driven e.g. by machine learning methods. While these novel approaches provide new insights into neuroimaging data, they often have unexpected properties, generating a growing literature on possible pitfalls. We propose to meet this challenge by adopting a habit of systematic testing of experimental design, analysis procedures, and statistical inference. Specifically, we suggest to apply the analysis method used for experimental data also to aspects of the experimental design, simulated confounds, simulated null data, and control data. We stress the importance of keeping the analysis method the same in main and test analyses, because only this way possible confounds and unexpected properties can be reliably detected and avoided. We describe and discuss this Same Analysis Approach in detail, and demonstrate it in two worked examples using multivariate decoding. With these examples, we reveal two sources of error: A mismatch between counterbalancing (crossover designs) and cross-validation which leads to systematic below-chance accuracies, and linear decoding of a nonlinear effect, a difference in variance.  Highlights: 1. Traditional design principles can be unsuitable when combined with cross-validation; 2. This can explain both inflated accuracies and below-chance accuracies; 3. We propose the novel \"same analysis approach\" (SAA) for checking analysis pipelines; 4. The principle of SAA is to perform additional analyses using the same analysis; 5. SAA analysis should be performed on design variables, control data, and simulations ", "text": "charité universitätsmedizin berlin corporate member freie universität berlin humboldt‐ universität berlin berlin institute health bernstein center computational neuroscience berlin center advanced neuroimaging department neurology excellence cluster neurocure; berlin germany department systems neuroscience university medical center hamburg‐eppendorf martinistr. hamburg germany section learning plasticity laboratory brain cognition national institute mental health national institutes health bethesda humboldt‐universität berlin berlin school mind brain institute psychology; berlin germany technische universität dresden; volition cognitive control; dresden germany authors contributed equally work standard neuroimaging data analysis based traditional principles experimental design modelling statistical inference increasingly complemented novel analysis methods driven e.g. machine learning methods. novel approaches provide insights neuroimaging data often unexpected properties generating growing literature possible pitfalls. propose meet challenge adopting habit systematic testing experimental design analysis procedures statistical inference. specifically suggest apply analysis method used experimental data also aspects experimental design simulated confounds simulated null data control data. stress importance keeping analysis method main test analyses possible confounds unexpected properties reliably detected avoided. describe discuss analysis approach detail demonstrate worked examples using multivariate decoding. examples reveal sources error mismatch between counterbalancing cross-validation leads systematic below-chance accuracies linear decoding nonlinear effect difference variance. research practice psychology cognitive neuroscience traditionally guided principles experimental design statistical analysis much pioneered fisher purpose principles observe effects clearly possible conditions noisy limited data make reliable inferences relation between experimentally manipulated measured variables presence potentially confounding influences. methodological work established corpus design principles e.g. counterbalancing randomization statistical tests researcher normally apply without extensive checks published work provides transparency reviewers readers. cognitive neuroimaging followed lead adapted principles specific properties large highdimensional data sets leading mass-univariate glm-based data analysis main workhorse however complexity neuroimaging data development theoretical ideas neural processing motivated wealth alternative analysis approaches foremost among multivariate pattern analysis driven standard statistical approaches machine learning methods classification algorithms crossvalidation significantly extended data-analytic toolbox made larger variety possible effects neuroimaging data accessible drawback methodological plurality soundness applied methods longer judged based established corpus novel methods often prove unexpected properties. evidenced growing literature possible pitfalls pointing e.g. known ways control confounds longer work multivariate analysis accuracies binomially distributed estimated cross-validation second-level t-test provide population inference applied information-like measures even applies seemingly small extensions established methodology like extraction correlations brain leading inflated estimate cluster-level statistics threshold underlying approximation might invalid propose meet challenges validity novel analysis methods adopting habit systematic testing experimental design analysis procedures statistical inference concerning single parts whole analysis pipeline. common practice performing control analyses additionally obtained data rule confounds already seen limited form testing recommendation goes beyond that particular suggest perform analyses aspects experimental design simulated null data. crucial point test analyses preserve properties actual pipeline possible; particular performed using analysis method actual data analysis. reason call proposal analysis approach following detail analysis approach worked examples general overview. along reveal possible confounds mvpa widely known neuroimaging community mismatch counterbalanced design analysis using cross-validation unexpected ability linear classifier decode differences variance absence differences mean. believe specific examples errors presented paper general interest main highlight generally many types unexpected errors occur mismatch design analysis. thus provide general tool find errors might affect particular analysis pipeline different ways. researcher intends perform simple neuroimaging experiment test whether difference experimental conditions experiment performed four runs. conditions presented consecutive trials. common confound setup presentation order conditions always presented would unclear observed difference caused true difference whether caused difference first second trials prevent this researcher employs counterbalancing experimental condition equally often presented first second trial i.e. equally often together level potentially confounding factor trial order purpose counterbalancing prevent bias final analysis even effect trial order present conditions equally often presented trial trial systematic effect confound trial order cancel out. example data experimental trial. denote condition specific observed difference means could thus arise difference perimental conditions equal thus standard statistics either condition trial number. contrast design panel controls confound trial order using counterbalancing. even data would depend trial number example mean variance test indicate difference conditions confound control worked. experimental design leave‐one‐run‐out cross‐validated classification. panel shows partitioning data training test cross‐validation fold demonstrates systematic misclassification test data arises resulting correct predictions. systematic misclassification also occur cross‐validation folds thus intended confound control failed. note reason systematic below‐chance accuracy nei‐ ther imbalanced number training test samples conditions specific particular clas‐ sifier cross‐validation general. instead caused design–analysis mismatch counterbalanced design cross‐validation scheme ployed analysis. test whether counterbalancing works expected indeed removes confounding effect trial order calculate would happen difference experimental conditions data influenced confound trial order controlled. figure show situation neuroimaging measurements first trials second trials. whereas experimental design figure allow distinguish observed difference arises true difference condition cause always presented counterbalanced setup figure allow distinction. collecting counterbalanced measurements condition across runs yields clearly t-test would indicate significant difference conditions data values identical both. counterbalancing therefore worked intended factor trial order heavily confounded data systematic effect outcome statistical test. happens counterbalanced confounded data figure analysed cross-validated classification instead t-test? crossvalidated classification standard mvpa method estimate well classifier learn examples predict experimental condition independent data serve test statistical dependency conditions data like t-test although cross-validated classification typically applied multivariate data applied onedimensional data equally well. t-test above check whether counterbalancing potential confound trial order also prevent unexpected effects outcome cross-validated classification analysis assessing performance confounded counterbalanced data figure selecting specific cross-validation scheme i.e. separate data training test sets different folds required analysis decision. stratified leave-one-run-out cross-validation scheme example common neuroimaging contrast data different imaging runs considered approximately statistically independent. contains equally many samples class crossvalidation also balanced. confound trial order controlled counterbalancing know effect experimental condition classifier able distinguish classes. balanced setting classes cannot distinguish translates classifier assigns conditions data non-systematic fashion leading expected classification performance around cross-validated classification performed repeatedly splitting measured data samples independent training test sets inferring relation data labels training quantifying strength test set. example data individual runs held successive split data served test data quality inferred prediction typically measured classification accuracy averaging performance measure across folds yields estimate well classifier would perform completely data. final performance estimate serves measure information content significantly chance demonstrates statistical relationship experimental conditions data. hebart baker recent overview differences misunderstandings mvpa univariate analyses. instead obtained accuracy performing analysis i.e. chance. means every single data sample misclassified despite absence true effect result worse chance demonstrating example counterbalancing completely failed control confound trial order. actual experiment researcher course cannot know true effect present data. since result systematically deviates chance might even suspect true effect present might sure interpret systematic below-chance accuracy this would probably conduct analysis look source unexpected behaviour. example researcher might perform control analysis reaction times recorded together neural data. idea potentially confounding variable influence experimental control data. consequently finding effect control data would demonstrate main results could alternatively explained confound. example assume reaction times depend solely trial order neuroimaging data. example first trials participants vigilant responded faster whereas second trials attention level decreased responded slowly following common practice conventional control analyses carried using standard univariate statistics typically t-tests f-tests even mvpa employed main analysis since example reaction times depend solely trial order already know t-test indicate reaction time effect thus help explain puzzling below-chance classification accuracy. even though datasets brain responses reaction times completely equivalent control analysis reveal information confound occurred main analysis. thus control analysis failed purpose. figure mismatch main analysis control analysis. standard control analysis fails detect problem initial example. upper panel leave‐one‐ run‐out cross‐validated decoding applied counterbalanced data without true experimental effect trial order confound leads puzzling classifica‐ tion accuracy lower panel t‐test applied reaction times control data find difference thereby fails detect problem gener‐ ated mismatch counterbalanced design cross‐validated analysis. lead false sense certainty results main analysis explained confound. initial example illustrates main points. first design prin‐ ciple used experimental design comes tablished corpus paired analysis method not. researcher overlooked design principles analysis methods stand work tandem using another analysis method design–analysis mismatch. second using standard control data analysis diagnose problem failed analysis method producing mismatch main analysis control analysis. many problems arise novel analysis methods caused similar kinds mismatch design analysis different analysis steps dif‐ ferent design principles analysis statistical assumptions note focus paper specific problem outlined above could provide explanation solution point; provide exhaustive overview possible errors. instead introduce general approach diagnose problems form principled form con‐ trol analysis number errors. introduce control analysis demonstrate problem initial example. that explain origin problem provide solutions discuss generalisations especially point property renders t‐test valid cross‐validated decoding invalid employing potential solutions help test work expected. note confounding effect demonstrate example purely theoretical construct. also simply remedied increasing number subjects number runs subject; substantially creasing number data points would help. demonstrate problems like initial example hard detect seemingly experiment designed correctly using counterbalancing neutralize common confound. in-depth examination design analysis statistics might alerted researcher problem often hard determine exactly look especially novel analysis methods little practical experience. performing empirical control analyses good idea systematically fail analysis methods differ between main control analyses demonstrated above. type apply analysis method used experimental data variables experimental design. perform positive negative control analyses synthetic noise-free data sets created single variables experimental design analyse main analysis method. positive control analyses test design variables influence experimental outcome typically experimental variable indeed yield significant results; failing tests demonstrates experimental setup suitable detect effect interest. complex designs also test latent design variables describe dependencies within design. negative control analyses test design variables influence experimental outcome indeed yield significant results; failing tests indicate variable potential confound and/or confound control work. type apply analysis method used experimental data empirical control data. main analysis method applied additionally measured variables since control data provide proxy main results result indicates main analysis would respond actual data influenced confound. type apply analysis method used experimental data synthetic null data. applying analysis multiple realisations synthetic null data tests false positive rate indeed expected provides general information null distribution expected outcomes range shape. suggest start analysis simple possible example simple one-factor tests efficient detecting confounds easy high diagnostic power found useful practice. article provide framework efficientillustrate return initial example experiment four runs trials experimental condition presentation order counterbalanced across runs. neurophysiological data measured single voxel larger region interest additionally reaction times measured assumed above neurophysiological example data influenced experimental condition confounding factor presentation order\". applying leave-one-run-out cross-validated classification find accuracy rois main analysis leaving question interpret result type analysis design variables. experiment three design variables experimental condition number trial number experimental condition translated pseudo-data using assignment analysis result accuracy confirms cross-validated classification could detect effect experimental manipulation number trial number directly used pseudo-data. analysis number results expected chance level analysis trial number results correct providing strong indication trial order confound could explain observed below-chance accuracy apparently cross-validated classification susceptible confound even though counterbalancing employed counteract finally employ type apply analysis simulated random null data contain neither experimental effect confound. result different simulation performing many simulations observe classification accuracy fluctuates around average expected chance level. repeated random data different dimensionality different distributions. supplemental table details. figure analysis approach applied initial example. sign variables initial example assumed data features neural data used main analysis. data point available trial. parallel analysis test data data point available trial either generated design properties control data synthetic null data. abbreviations figure outc. outcome; exp. expected; plus/minus statistical devi‐ ation. experimental design contained additional variables could systematically gone design variables time used design variable instead measured data performed analysis val‐ checked influence variable expected. note that except type rely real data. therefore problem combination experimental design analysis method could actually detected data collected. guide solve problem initial example analyses demonstrate presentation order confound lead below‐chance classification thereby explains result main analy‐ sis. theoretical examination based results along lines figure reveals culprit mismatch counterbalanced design cross‐validated analysis particular design factor trial order counterbalanced within training test set. element might confusing context terminology analysis actually balanced sense term normally used cross‐ validation number training samples class equal partition. balanced data well balanced cross‐validation scheme note origin problem specific example indeed missing counterbalancing cross-validation fold neither analysis type dimensionality data cross-validated manova cross-validated mahalanobis distance used cross-validated distance measure suffer problem systematically estimate negative distances confusing below-chance results. possible remedy problem counterbalancing randomization i.e. randomly decide independently whether trial order employ test solution indeed works expected re-running analysis design variable trial number randomized designs. simulating many experiments find average classification accuracy indeed i.e. confound statistically controlled. looking individual outcomes however find accuracy never occurs; rather occurs randomizations thus revealed randomization seem ideal solution context. another possibility solve problem would keep design validation scheme ensures confound counterbalanced test i.e. contains equally many runs. achieved leaving runs employ test analysis solves problem. time result indeed every single experiment average above. possibilities course exhaustive. since example problem related cross-validation implemented another alternative would replace classification accuracy test statistic need cross-validation. please note example deliberately chosen small possible. demonstrated systematic negative bias will however also occur larger real datasets trial order effect data leaveone-run-out cross-validation used. negative bias extreme example easily large enough suppress real effects and/or lead confusing significant below-chance accuracies. supplemental section demonstration real empirical dataset. majority classifier yield leave-one-out cross-validation employed balanced data example simpler critically depends different numbers exemplars class already general structure ours balancing ignored splitting data training test sets. second example antilearning demonstrates datasets specific properties always yield below-chance accuracies large number classifiers independent specific design property validation scheme. third cause hinges using binomial test single cross-validated accuracy estimates yield many significant below-chance results chance results another scenario counterbalancing also unexpectedly fails control confounding factor mvpa described todd differs example individual decoding analyses calculated unit whereas single decoding analysis using units calculated example. major differences causes chance results chance results depend particular cross-validation scheme crux example. example demonstrates systematic below-chance classification accuracies caused design–analysis mismatch even occur employing basic experimental methodology. specific example above design variable trial order controlled. problem however specific controlling time sequence effects; logic applies counterbalancing variable experimental variable. general often unexpected consequences design features implemented respect full data ignored data split training test sets cross-validation. examples cases class equal number samples full data differing numbers training test cases dissolving strata assignment patients matched controls different partitions. section give non-exhaustive overview possible forms different aspects considered setting analysis. supplement section provides in-depth explanations components individual test cases section demonstrates necessary steps perform concrete empirical example below. design variables explicit design variables experimental condition level factor factorial design implicit design variables sequential number trial within repetition number stimulus. control data additionally recorded data reaction times error rates motion correction parameters eye-tracking data etc. possible acrosssubject data include gender personality scores. simulated data simulations open wide range possibilities. data generated effect specific effect confound present present combinations thereof. simplistic example data consisting come generative model attempting capture many aspects real data possible special case modified data experiment e.g. shifted trial experimental data unrelated experiment resting-state data mapping function cases test data form cannot processed same analysis. example experimental condition nominal label therefore compatible classifier expects numerical input. categorical data mapped input data several ways conditions arbitrarily assigned numerical values encoded multiple dummy variables assigned randomly chosen multivariate patterns. another case analyses intrinsically multivariate measures pattern correlation cosine distance e.g. representational similarity analyses here simple mathematical statistical models used create multivariate data similarities determined input variable. indeed high value creating different test cases variable test data different mapping functions understand analysis pipeline reacts input might encoded different expected depending complexity mapping function continuum simple design variable full-blown simulation. analysis approach applied different analysis ranges complete pipeline single parts specific combinations parts. mvpa study parts pre-processing data extraction single-trial run-wise values cross-validated classification second-level analysis statistical inference. depending range form test data inspected outcomes changes e.g. time series trial-wise values run-wise values accuracies test statistic values p-values statistical significance. whenever possible test case come defined expectation interpretations expectation fulfilled violated. depending test data expectation specific value distributional property test data fixed e.g. noise-free pseudo-data generated deterministic mapping design variable corresponding analysis result interpretation result depends single fixed value. noisy data like experimental control data outcome still fixed interpretation straightforward statistical test necessary determine whether result significant. simulation incorporating random variation simulation sufficient number times assess properties distribution outcome values e.g. mean variance number significant outcomes. latter statistical testing simulations combined looking e.g. frequency statistical test indicates significant result across simulation runs determine whether test valid given circumstances. simple implement large number tests especially using simulated data. results assessed statistical test number false positives increase number tests significance level adjusted. raises question balance sensitivity specificity possible confounds efficiently detect problems within many test outcomes. purpose suggest following measures adjusting significance level less important tests. tests separated small number important tests targeted potential confounds expected exert strong influence possibly large number less important tests performed safe side. first class sensitivity kept high second class corrected multiple comparisons. priori checks problem diagnosis. prior data collection signs problem exist analysis experimental data sensitivity lower trying find source concrete problem evident main analysis. sorting test cases influence. tests sorted according whether violation test likely imply violation another test problems targeted overlap. example test null data shows interpretation results. problem diagnosis rest simply whether statistical test gives significant result researcher judgement decide whether confound likely relevant main analysis. realistic simulations help assess practical impact confound. multiple test cases indicate potential confounds actually affect main analysis. check this correlations calculated outcomes multiple test cases outcomes main analysis. statistical test negative result mean tested variable potential confound positive result strongly indicates moreover main analysis performed different segments data e.g. brain regions time points correlations outcomes calculated segment detect location-specific confounding effects e.g. confound affect motor cortex visual cortex. helps find solutions experimental data already acquired analysis indicates problem; cases however come late phase. therefore recommend systematically different phases study main analysis phase. data collection tests control data check whether corresponding confounds present worst case diagnose problem become apparent main data analysis. section demonstrate diagnose problem real empirical data. researcher performs experiment participants press button either left right index finger response visual stimuli. left button presses frequent right button presses trials bold data recorded runs participants. identify brain regions carry information button pressed researcher applies leave-one-run-out cross-validated classification parameter estimates voxels within searchlight using linear support vector machine. time-resolved analysis finite impulse response regressors comprising two-second time bins aware imbalanced data pose problem many classification algorithms single regressors modelling left right button presses respectively time yields single parameter estimate image condition used time-resolved searchlight classification. subject-wise classification accuracy maps entered second-level t-test across subjects chance level clear expectations result analysis. first information localized mainly motor regions analysis contrasts different movement conditions. second above-chance classification possible earlier button press hemodynamic delay. results however show significant information large regions across entire brain already button press apparently something analysis went wrong. example constructed using data unpublished study rule representation. preprocessing parameter estimation second-level analysis fmri data performed searchlight classification decoding toolbox using libsvm figure results confounded corrected example fmri analyses. significant results button press classification variance confound real data button press. run-wise parameter estimates calculated using trials left trials right button presses. bottom above using left right button presses calculate run-wise estimates. displayed voxels show significant effects uncorrected. larger clusters also significant fwec-corrected; corrected analysis cluster survives fwec correction supplement figures contain combinations time bins. researcher wants diagnose suspected problem checking temporal attention sequence effects well details task. create test cases making following decisions numerical test data used input values analysis as‐is cate‐ gorical data mapped dummy variables trials expect left button press trials expect right button press). test range test data generated level single‐trial values whole analysis second‐level t‐test accuracies consid‐ ered. analysis steps range computing run‐wise parameter estimates leave‐one‐run‐out cross‐validated classification group level t‐test applied subject‐wise classification accuracies. outcomes subject‐wise accuracies p‐values second level t‐test frequency test indicates significance null data. increase sensitivity researcher sorts test cases dif‐ ferent categories labelled sanity checks design random control data supplement section provides detailed explanation including concrete steps setup analysis. –cat. design rand– onset target onset trial trial type target onset target onset trial type target –cat. ctrl data– correct time button press correct button figure results different test data. left panel distribution accuracies subjects plots medians outliers correction small dotted line marks chance level level p‐values column provides p‐values one‐sided t‐test across sub‐ jects right panel summary simulated random null data sets showing relative frequency cases result significant focusing sanity check category first researcher reassured outcome positive control analysis side confirms analysis able distinguish left right button presses difference corresponding trials. also significant effect ntrial number trials condition surprising since systematic difference conditions number trials contrast unexpectedly high number significant results random null data second-level t-test rejected null hypothesis instances instead expected therefore confirmed suspicion problem. increased false positive rate null simulations strongly suggests general aspect design property analysis procedure neither experimental variable design factor influence simulated null data. peculiar property design different number trials conditions. researcher assumed dealt applying classification single-trial data run-wise parameter estimates enough? researcher checks hypothesis modifying analysis equally many trials used calculate estimates left right button presses run. indeed correction number significant results instances null data drops consistent false positive rate test case remains significant positive control using variable interest result corrected analysis fmri data confirms apparent confound removed; significant effect present time effects time located motor sensory regions expected. supplemental figures d.-d. supplement section show time-resolved results combinations button presses side illustrating problem indeed caused imbalance trials e.g. differences power; choosing number left right trials always solves problem. described above help diagnose problem quickly check whether approach resolve likely work. reveal cause left researcher. conclude example briefly explain problem arose. experimental setup reasoning researcher following classifiers known sensitive imbalanced training data therefore classification applied run‐wise parameter estimates essential‐ averages across trials. linear classifiers sensitive linear differences class‐specific data distributions i.e. differences class means. effect trial‐wise data classes comes distribution averaging fewer trials change mean. mistake argument difference number trials condition change mean change variance run‐ wise estimates. contrary common assumption linear classifiers differences mean also differences variance achieve above‐ chance classification behaviour limited specific types linear classifiers; applies even classifiers utilizing means data nearest centroid classifiers linear discriminant analysis gen‐ erally linear classification based variance parameter estimates come differences estimability regressors note successful linear classification based differences vari‐ ance confound classifier reveals difference truth‐ fully exists data however interpretation error interpreted showing linear difference conditions. contrast pirical example contains true confound data classes come different distributions variance difference induced analysis detailed simulation svms nearest centroid classifiers found supplemental sec‐ tion figure induction variance difference design successful variance classifi‐ cation linear classifier. original probability distribution single trial values classes averaging different numbers trials creates distributions mean different variance. example linear classifier classifies classes chance using nonlinear variance difference two. expected accuracy classifiers decision boundary different positions probability distribution nearest centroid classifier linear discriminant analysis place boundaries. expected accuracy minimum chance level otherwise chance. position boundary varies expected accuracy classifying classes differ nonlinear mean using linear classifier chance. note successful linear classification data classes differ variance confound classifier truthfully reveals difference exists data. however interpretation error interpreted showing linear difference. contrast confound example arises variance classes induced analysis detailed simulations found supplemental section main example demonstrate employed practice. however example also interesting itself. averaging before classification feature extraction multiple trials complex data analyses methods standard analysis procedures. advise test potential confounding effects null simulations here. animportant point relates inference classification analyses since linear classifiers successfully extract nonlinear information successful linear classification allow direct inference linear versus nonlinear nature representations example also demonstrates confounds arise combination analysis steps pose problems individually. detected simple simulations synthetic null data analysis employed. paper advocated systematically check experimental design data analysis methods statistical inference order cope challenges possible pitfalls novel methods neuroimaging methods sometimes fail conform researchers’ expectations intuitions. leads situations confounding influences controlled consequently spurious effects observed true effects fail identified overly optimistic pessimistic effect size estimates. propose blindly rely expectations intuitions explicitly check them applying analysis used experimental data also design variables control data simulated data. discuss number points need clarification. keep simple. main focus paper introduce design principles create efficient control tests. made number suggestions paper achieved. main suggestion keep simple; suggestions perform positive negative control analyses many simple control datasets influenced synthetic empirical variable create time-shifted datasets using variable values previous trial detect sequence effects common confounds neuroimaging. recommendations keep effective include adaptive alarm rate thresholds correlating main analysis outcomes. however believe ultimate principles efficient tests experimental paradigms. rather conceive first suggestions hope principles efficient tests emerge employing practice. employ saa. used diagnose problems already become apparent recommend continuously phases study planning piloting final analysis become aware possible problems early possible. side benefits practice encourages consider details analysis already design phase therefore tailor design questions wants ask; used power analysis helps detect simple programming errors indeed sole process setting design phase prevent programming errors first place coding scheme variable names content fresh mind programming design analysis time reducing risk confusion both. contrast time passes setting design analysis e.g. data recorded chances confuse variable names coding schemes much higher. might also facilitate design optimization believe investigation potential negative side effects necessary. examples. addition describing detailing general illustrated concrete examples. main function paper spell detail applied uncovers potential problems given data design. however also relevant reveal relevant problems mvpa initial example demonstrates classic strategy counterbalancing experimental design control confound become ineffectual combined analysis method uses cross-validation. empirical example shows differences variances yield successful linear classification specifically demonstrating inferring linear differences linear classification would invalid addition demonstrate analyses control data fail even to-be-controlled effects present different methods employed control experimental analyses. fact neither example depends dimensionality data demonstrates unexpected confounds also specifically bound multivariate analysis occur univariate analyses well. relevance. fact would detect examples well examples recent literature mvpa specific general demonstrates potential aiding detect easy-to-overlook problems. found helpful personal work looking forward seeing whether case general. finally employing analysis method especially important control analyses least addition analysis methods demonstrated fail purposes different analysis methods employed. data; data remedy. common misconception confounding effects occur small data sets data would reduce confounds. data help reducing effects nonsystematic confounds simply adding data universal solution specifically confounding effects systematically induced design examples demonstrate here. indeed empirical example already normal-sized sample confounding effect present subject subjects would even increase effect strength. holds initial example presented design would used multiple subjects test would applied group level would also stay potential confound number runs increased. idealized case noise effect classification accuracy would stay correct. real data increasing number runs importance confound depends relative effect sizes confound experimental effect. experimental effect present primary effect measure come closer chance level null distribution becomes narrower confounding effect could still significant impact. differences simulation studies. shares aspects standard simulation studies routinely used demonstrate merits pitfalls particular design analysis methods. like simulation studies demonstrate claims computation. however differs simulation studies important aspects. first avoids particular problem simulation studies choose settings important demonstrate generality. used particular experiment parameters fixed. second simulation studies typically include complex realistic simulations demonstrate operation method realistic scenarios. contrast employed perform sanity checks believe effectively done simple simulations. whether case additional principles help create useful control analyses open question believe need employ practice. thus theoretical tool demonstrate claim; empirical tool help creating better designs analyses. unit testing. inspired practice unit testing software development i.e. writing software form modules tested independently combination situation software development insofar similar neuroimaging principle validity algorithm strictly proven multitude newly produced code makes practically impossible. contrast unit-testing however test software modules e.g. functions analysis package instead design–analysis combinations specific experiments. fields. shares rationale number scientific approaches. follows logic routine positive negative controls disciplines like chemistry molecular/cell biology working full analysis pipeline tested every experimental data analysing positive negative probes alongside experimental data e.g. medicine e.g. using diluent histamine controls skin prick testing allergy diagnosis general solution. would like point although tool generally applied data analysis pipelines specialized find specific kinds problems specific kinds analyses guarantee help detect kind problem kind analysis. moreover solve problem merely points researcher possible problems resolved case-by-case basis. conclusion. hope developments neuroimaging data analysis long term lead establishment corpus particular heuristics machine learning methods backed integrated theory statistical inference however believe testing experiments provides highly efficient additional safeguard detect avoid eliminate confounds therefore help improving quality replicability experimental research. authors declare conflict interest. work supported german research foundation m.n.h. supported german ministry education research intramural research program national institute mental health feodor-lynen fellowship humboldt foundation.", "year": "2017"}