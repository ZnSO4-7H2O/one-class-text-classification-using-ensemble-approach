{"title": "Invariant recognition drives neural representations of action sequences", "tag": "q-bio", "abstract": " Recognizing the actions of others from visual stimuli is a crucial aspect of human visual perception that allows individuals to respond to social cues. Humans are able to identify similar behaviors and discriminate between distinct actions despite transformations, like changes in viewpoint or actor, that substantially alter the visual appearance of a scene. This ability to generalize across complex transformations is a hallmark of human visual intelligence. Advances in understanding motion perception at the neural level have not always translated in precise accounts of the computational principles underlying what representation our visual cortex evolved or learned to compute. Here we test the hypothesis that invariant action discrimination might fill this gap. Recently, the study of artificial systems for static object perception has produced models, CNNs, that achieve human level performance in complex discriminative tasks. Within this class of models, architectures that better support invariant object recognition also produce image representations that match those implied by human and primate neural data. However, whether these models produce representations of action sequences that support recognition across complex transformations and closely follow neural representations remains unknown. Here we show that spatiotemporal CNNs appropriately categorize video stimuli into actions, and that deliberate model modifications that improve performance on an invariant action recognition task lead to data representations that better match human neural recordings. Our results support our hypothesis that performance on invariant discrimination dictates the neural representations of actions computed by human visual cortex. ", "text": "recognizing actions others video sequences across changes viewpoint actor gait illumination hallmark human visual intelligence. large number studies highlighted areas human brain involved processing biological motion many described single neurons behave response videos human actions. however little known computational necessities shaped neural mechanisms either evolution experience. paper test hypothesis computational goal discrimination complex video stimuli according action content. show that within class spatiotemporal convolutional neural networks deliberate model modifications leading representations videos better support robust action discrimination also produce representations better match human neural data. modifying naïve model increase performance invariant recognition resulted representation similar human neural data results suggest that similarly known object recognition process performance optimization invariant discrimination constrained hierarchical st-cnn-like architecture shaped neural mechanisms underlying ability perceive actions others. humans’ ability recognize actions others crucial aspect visual perception. remarkably accuracy finely discern others largely unaffected transformations that substantially changing visual appearance given scene change semantics observe recognizing actions middle ground action primitives activities across transformations hallmark human visual intelligence proven elusive replicate artificial systems. this invariance transformations orthogonal learning task subject extensive theoretical empirical investigation artificial biological perception recognition recent studies described neural processing mechanisms underlying ability recognize action others visual stimuli. specific brain areas implicated processing biological motion perception actions. example humans primates superior temporal sulcus particularly posterior portion believed participate processing biological motion actions addition studying entire brain regions engage action recognition number studies characterized responses single neurons. preferred stimuli neurons visual areas well approximated moving edge-detection filters energy-based pooling mechanisms neurons region macaque monkeys respond selectively actions invariant changes actors viewpoint tuning curves well modeled simple snippet-matching models broadly neural computations underlying action recognition visual cortex organized hierarchical succession spatiotemporal feature detectors increasing size complexity despite precise characterization organizational regional single-unit data involved processing biological motion little information known computational tasks might relevant explaining recapitulating computations. fueled advances computer vision methods object scene categorization recent studies made progress towards linking computational outcomes neural signals quantitatively accurate models single neurons inferior temporal cortex addition studies highlighted correlation performance optimization discriminative object recognition tasks accuracy neural predictions single recording site neural representation level however results extended action perception dynamic stimuli. specific computational goals relate biological structure underlies ability recognize actions others particular representations action sequences human visual cortex learned evolved compute remains unknown. test hypothesis invariant recognition might fill gap. artificial systems action recognition compare data representations human magnetoencephalography recordings show that within spatiotemporal convolutional neural networks model class deliberate modifications result better performing models invariant action recognition tasks also lead empirical dissimilarity matrices better match obtained human neural recordings. results suggest performance optimization discriminative tasks especially require generalization across complex transformations alongside constraints imposed hierarchical organization motion processing visual cortex determined representation action sequences computed visual cortex. moreover highlighting role robustness nuisances orthogonal discrimination task results extend scope invariant recognition computational framework understanding human visual intelligence study action recognition filmed video dataset showing five actors performing five actions five different viewpoints developed four variants feedforward hierarchical models visual cortex used extract feature representations videos showing different viewpoints frontal side. subsequently trained machine learning classifier discriminate video sequences sample frames action recognition dataset consisting video clips depicting five actors performing five actions actions recorded five different viewpoints performed treadmill actors held water bottle apple hand regardless action performed order minimize low-level object/action confounds. actors centered frame background held constant regardless viewpoint. four models developed extract video representations instances spatiotemporal convolutional neural networks currently best performing artificial perception systems action recognition. architectures direct extensions convolutional neural networks used recognize objects faces static images input stimuli extend space time. stcnns hierarchical models build selectivity specific stimuli template matching operations robustness transformations pooling operations qualitatively spatiotemporal convolutional neural networks detect presence certain video segment input stimulus; detections various templates aggregated following hierarchical architecture construct video representations. nuisances reflected model output like changes position discarded pooling mechanism schematic overview class models used spatiotemporal convolutional neural networks st-cnns hierarchical feature extraction architectures. input videos layers computation output layer serves input next layer. output last layer constitutes video representation used downstream tasks. models considered consisted convolutionalpooling layers’ pairs denoted conv pool conv pool. convolutional layers performed template matching shared templates positions space time pooling layers increased robustness max-pooling operations. convolutional layers’ templates either fixed priori sampled learned. example templates first layer conv fixed depict moving gabor-like receptive fields templates second simple layer conv sampled videos containing actions filmed different viewpoints. considered basic purely convolutional model subsequently introduced modifications pooling mechanism template learning rule improve performance invariant action recognition first purely convolutional model consisted convolutional layers fixed templates interleaved pooling layers computed max-operations across contiguous regions space. particular templates first convolutional layer contained moving gabor filters templates second convolutional layer sampled action sequences collected various viewpoints. second unstructured pooling model allowed pooling layer units span random sets templates well contiguous space regions third structured pooling model allowed pooling contiguous regions space well across templates depicting action various viewpoints. orientation template discarded pooling mechanism similarly position space discarded traditional cnns fourth final model employed backpropagation gradient based optimization method learn convolutional layers’ templates iteratively maximizing performance action recognition task used model extract feature representations action sequences different viewpoints frontal side. sought evaluate four models based well could support discrimination five actions video dataset. trained machine learning classifier discriminate stimuli based action later assessed classification accuracy unseen videos. experiment trained tested classifier using model features computed videos captured viewpoint. experiment trained tested classifier using model features computed videos mismatching viewpoints introduced modifications basic st-cnn increase robustness changes d-viewpoint. qualitatively spatiotemporal convolutional neural networks detect presence certain video segment input stimulus. orientation template discarded pooling mechanism structured pooling model analogous position space discarded traditional cnn. models structured pooling template conv layer cells sampled videos containing four actors performing five actions five different viewpoints templates sampled videos specific actor performing specific action pooled together pool layer unit. models employing unstructured pooling allowed pool cells pool entire spatial extent input well across channels. models used exact templates employed models relying structured pooling matched models number templates wired pooling unit. however assignment templates pooling randomized reflect semantic structure. experiment trained tested action classifier using feature representations videos acquired viewpoint therefore investigate robustness changes viewpoint. models produced representations successfully classified videos based action depicted observed significant difference performance model end-to-end trainable model fixed template models however task considered experiment sufficient rank four types st-cnn models. trained supervised machine learning classifier discriminate videos based action content using feature representation computed spatiotemporal convolutional neural network models considered. figure shows prediction accuracy machine learning classifier trained tested using videos recorded viewpoint. classifier trained videos depicting four actors performing five actions either frontal side view. machine learning classifier accuracy assessed using unseen videos unseen actor performing five actions. generalization across changes viewpoints required feature extraction classification system. report mean standard error classification accuracy five possible choices test actor. models learned templates outperform models fixed templates significantly task. chance indicated horizontal line. horizontal lines indicate significant difference conditions based group anova bonferroni corrected paired t-test viewpoint. experiment investigated well model representations could support learning discriminate video sequences based action content across changes viewpoint. general experimental procedure identical outlined experiment except used features extracted videos acquired mismatching viewpoints training testing models considered produced representations were least minimal degree useful discriminate actions invariantly changes viewpoint unlike observed experiment possible rank models considered based performance task. expected since various models designed exhibit various degrees robustness changes viewpoint figure shows prediction accuracy machine learning classifier trained tested using videos recorded opposed viewpoints. classifier trained using videos frontal viewpoint accuracy discriminating unseen videos would established using videos recorded side viewpoint. report mean standard error classification accuracy five possible choices test actor. models learned templates resulted significantly higher accuracy task. among models fixed templates spatiotemporal convolutional neural networks employing structured pooling outperformed purely convolutional unstructured pooling models. chance indicated horizontal line. horizontal lines indicate significant difference conditions based group anova bonferroni corrected paired t-test used representational similarity analysis assess well model feature representation matched human neural data. produces measure agreement artificial models brain recordings based correlation empirical dissimilarity matrices constructed using either model representation stimuli recordings neural responses stimuli elicit used video feature representations extracted model unseen stimuli construct model dissimilarity matrices magnetoencephalograpy sensor recordings neural activity elicited stimuli compute neural dissimilarity matrix finally constructed dissimilarity matrix using action categorical oracle. case dissimilarity used feature representations extracted four spatiotemporal convolutional neural network models videos depicting five actors performing five actions different viewpoints frontal side. moreover obtained magnetoencephalography recordings human subjects’ brain activity watching videos used recordings proxy neural representation videos. videos used construct learn models. representations video constructed empirical dissimilarity matrix using linear correlation normalized empirical dissimilarity matrices stimuli constructed video representations model purely convolutional model model unstructured pooling model model structured pooling model model learned templates model categorical oracle magnetoencephalography brain recordings. observed end-to-end trainable model produced dissimilarity structures better agreed constructed neural data models fixed templates within models fixed templates model constructed using structured pooling mechanism build invariance changes viewpoint produced representations agree better neural data models employing unstructured pooling purely convolutional models computed spearman correlation coefficient lower triangular portion dissimilarity matrix constructed artificial models considered dissimilarity matrix constructed neural data assessed uncertainty measure resampling rows columns matrices constructed. noise ceiling assessed computing individual human subjects’ dissimilarity matrix average dissimilarity matrix rest subjects. noise floor computed assessing lower portion dissimilarity matrix constructed using model representation scrambled version neural dissimilarity matrix. score reported normalized noise ceiling noise floor models learned templates agree neural data significantly better models fixed templates. among these models structured pooling outperform purely convolutional unstructured models. horizontal lines indicate significant difference conditions based group anova bonferroni corrected paired t-test shown that within spatiotemporal convolutional neural networks model class across deliberate model modifications feature representations useful discriminate actions video sequences manner robust changes viewpoint also produce empirical dissimilarity structures similar constructed using human neural data. discrimination performance simpler task require generalization across complex transformations sufficient fully rank model representations. results support hypothesis performance invariant discriminative tasks shaped neural representations actions computed visual cortex. moreover dissimilarity matrices constructed st-cnns representations match build neural data better purely categorical dissimilarity matrix. highlights importance computational task architectural constraints described previous accounts neural processing action motions build quantitatively accurate models neural data representations findings agreement reported perception objects static images single recording site whole brain level identify computational task explains recapitulates representations human action visual cortex. developed four st-cnn models using deliberate modifications improve models’ feature representations invariant action recognition. doing verified structured pooling architectures memory based learning previously described theoretically motivated applied build representations video sequences support recognition invariant complex non-affine transformations. however empirically found learning model templates using gradient based methods fully supervised action recognition task better results terms classification accuracy agreement neural recordings dissimilarity structures constructed neural recordings model representations. relatively abstract comparison provides guidance establishing one-to-one mapping model units brain regions sub-regions therefore cannot exclude models basis biological implausibility work mitigated limitation constraining model class reflect previous accounts neural computational units mechanisms involved perception motion furthermore class models developed experiments purely feedforward however neural recordings selected acquired stimulus onset. late visual processing likely feedback signals among energy sources captured recordings. signals accounted models. provide evidence adding feedback mechanism recursion improve recognition performance correlation neural data recognizing actions others complex visual stimuli crucial aspect human perception. investigated relevance invariant action discrimination improving model representations’ agreement neural recordings showed computational principles shaping representation human action sequences human visual cortex evolved learned compute. deliberate approach model design underlined relevance supervised gradient based performance optimization methods memory based structured pooling methods modeling neural data representations. primate visual cortex could implement gradient based optimization acquire structured pooling investigated extensively biologically plausible learning algorithms irrespective precise biological mechanisms could carry performance optimization invariant discriminative tasks computational studies point relevance understanding neural representations visual scenes recognizing semantic category visual stimuli across photometric geometric complex changes sample regimes hallmark human visual intelligence. building data representations support kind robust recognition shown here obtains empirical dissimilarity structures match constructed using human neural data. wider context study perception results strengthen claim computational goal human visual cortex support invariant recognition broadening study action perception. camera keep background constant across changes viewpoint actors instructed hold apple bottle hand regardless action performing objects background would differ actions. action/actor/view filmed least subsequently original videos clips long resulting dataset video clips. video clips started random points action cycle clip contained full action cycle. authors manually identified single spatial bounding contained entire body actor cropped videos according bounding box. experiment experiment designed quantify amount action information extracted video sequences four computational models primate visual cortex. experiment tested basic action recognition. experiment particular quantified whether action information could support action recognition robustly changes viewpoint. motivating idea behind design that machine learning classifier able discriminate unseen video sequences based action content using output computational model model representation contains action information. moreover classifier able discriminate videos based action unseen viewpoints using model outputs must model representations carry action information changes viewpoint reflected model output. procedure analogous neural decoding techniques important difference output artificial model used lieu brain recordings general experimental procedure follows constructed feedforward hierarchical spatiotemporal convolutional models used extract feature representations number video sequences. trained machine learning classifier predict action label video sequence based feature representation. finally quantified performance classifier measuring prediction accuracy unseen videos. procedure outlined performed using three separate subsets action recognition dataset described previous section. particular constructing spatiotemporal convolutional model requires access video sequences depicting actions sample learn convolutional layers’ templates. subset video sequences used learn sample templates called embedding set. training testing classifier required extracting model responses number video sequences; sequences organized subsets training test set. never overlap test union training embedding set. purpose experiment assess well data representations produced four models supported non-invariant action recognition task. particular embedding used sample learn templates contained videos showing five actions five viewpoints performed four five actors. training subset embedding contained videos either frontal viewpoint side viewpoint. lastly test contained videos five actions performed fifth left-out actor performed either frontal side viewpoint. obtained five different splits choosing five actors exactly test. templates either learned sampled used model extract representations train test sets videos. averaged performance possible choices training viewpoint frontal side. report mean standard error classification accuracy across five possible choices test actor. experiment designed assess performance model producing data representations useful classify videos according action content generalization across changes viewpoint required. experiment identical experiment used exact models. however training contained videos recorded frontal viewpoint test would contain videos side viewpoint vice-versa. report mean standard deviation choice test actor average accuracy choice training viewpoint. video sequences layers computations output layer serves input next layer models direct generalizations models neural mechanisms support recognizing objects static images stimuli extend space time within layer single computational units process portion input video sequence compact space time. outputs layer’s units processed aggregated units subsequent layers construct final signature representation whole input video. sequence layers adopted alternates layers units perform template matching layers units perform pooling operations units’ receptive field sizes increases signal propagates hierarchy layers. convolutional units within layer share templates output dotproduct filter input. qualitatively models work detecting presence certain video segment input stimulus. exact position space time detection discarded pooling mechanism. specific models present consist convolutionalpooling layers’ pairs. layers denoted conv pool conv pool convolutional layers completely characterized size content stride units’ receptive fields pooling layers completely characterized operation perform pooling regions first layer conv consisted convolutional units templates size frames frames frames. convolution carried stride pixel conv filters obtained letting gabor-like receptive fields shift space frames full expression filter follows \";&'% transformed coordinates take account rotation shift direction orthogonal gabor filters considered spatial aperture filter preferred orientation chosen among possible orientations speed chosen linear grid points pixels frame lastly conv templates time modulation .abcd %=…l temporal receptive field size degrees respect vertical). template obtained letting gabor-like receptive field described shift orthogonal direction preferred orientation unit time second simple layer conv followed pool. templates case sampled randomly pool responses videos embedding set. used model conv units sizes units time units time units time stride directions. modifying pool layer purely convolutional models. specifically models pool units pooled entire spatial input temporal unit remaining scales conv channels. models employing structured pooling mechanism templates sampled videos particular actor performing particular action regardless viewpoint pooled together templates different sizes corresponding different scale channels pooled independently. resulted pool units action/actor pair receptive-field-size/scale-channel pair. intuition behind structured pooling mechanism resulting pool units respond strongly presence certain template regardless pose models employing unstructured pooling mechanism followed similar pattern however wiring simple complex cells random fixed templates models employed exact templates models’ general architecture similar used models structured unstructured pooling. specifically template learning used stacked convolution-batchnorm-maxpooling-batchnorm modules followed linear-relu-batchnorm modules final log-soft-max layer. feature extraction linear logsoftmax layers discarded. input videos resized frames like fixed-template models. first convolutional layer’s filter bank comprised filters size frames convolution applied stride directions. first max-pooling layer used pooling regions size unit time applied stride units spatial directions unit time. second convolutional layer’s filter bank made templates size units time responses computed stride units time unit spatial directions. second maxpooling layer’s units pooled full extent spatial dimensions unit time channels. lastly linear layers units respectively free bias terms. model training carried using stochastic gradient descent mini-batches videos. used gurls package train test regularized least squares gaussian-kernel classifier using features extracted training test respectively corresponding action labels. aperture gaussian kernel well regularization parameter chosen leaveone-out cross-validation procedure training set. accuracy evaluated separately class averaged classes. used group one-way anova assess significance difference performance fixed-template methods models learned templates. used paired-sample t-test brain activity human participants normal corrected normal vision recorded elekta neuromag triux magnetoencephalography scanner watched videos acquired procedure outlined above included dataset used model template sampling training. recordings data first presented original neural recording study recordings used train pattern classifier discriminate video stimuli basis neural response elicited. performance pattern classifier assessed separate recordings subjects. train/test decoding procedure repeated every individually subject non-invariant invariant case. possible used filtered recordings elicited videos mentioned above averaged across subjects averaged window centered around stimulus onset proxy neural representation video computed pairwise correlation-based dissimilarity matrix model representations videos shown human subjects meg. likewise computed empirical dissimilarity matrix computed using neural recordings. performed rounds bootstrap round randomly sampled videos original -videos sample assessed level agreement dissimilarity matrix induced model representation computed using neural data calculating spearman correlation coefficient lower triangular portions matrices. computed estimate noise ceiling neural data repeating bootstrap procedure outlined assess level agreement individual human subject average rest. selected highest possible match score across subjects across rounds bootstrap serve noise ceiling. similarly assessed chance level representational similarity score computing match model scrambled version neural data matrix. performed rounds bootstrap model selected maximum score across rounds bootstrap models serve baseline score used one-way group anova assess difference spearman correlation coefficient obtained using models employed fixed templates models learned templates. subsequently assessed significance difference model performing paired t-test samples obtained bootstrap procedure. deemed differences significant martin giese charles jennings heuihan jhuang cheston feedback work. material based upon work supported center brains minds machines funded award ccf-. gratefully acknowledge support nvidia donation tesla used research. grateful martinos imaging center neural recordings used work acquired mcgovern institute brain research supporting research.", "year": "2016"}