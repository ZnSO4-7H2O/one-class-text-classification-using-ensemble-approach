{"title": "Statistical Physics and Representations in Real and Artificial Neural  Networks", "tag": "q-bio", "abstract": " This document presents the material of two lectures on statistical physics and neural representations, delivered by one of us (R.M.) at the Fundamental Problems in Statistical Physics XIV summer school in July 2017. In a first part, we consider the neural representations of space (maps) in the hippocampus. We introduce an extension of the Hopfield model, able to store multiple spatial maps as continuous, finite-dimensional attractors. The phase diagram and dynamical properties of the model are analyzed. We then show how spatial representations can be dynamically decoded using an effective Ising model capturing the correlation structure in the neural data, and compare applications to data obtained from hippocampal multi-electrode recordings and by (sub)sampling our attractor model. In a second part, we focus on the problem of learning data representations in machine learning, in particular with artificial neural networks. We start by introducing data representations through some illustrations. We then analyze two important algorithms, Principal Component Analysis and Restricted Boltzmann Machines, with tools from statistical physics. ", "text": "document presents material lectures statistical physics neural representations delivered fundamental problems statistical physics summer school july ﬁrst part consider neural representations space hippocampus. introduce extension hopﬁeld model able store multiple spatial maps continuous ﬁnite-dimensional attractors. phase diagram dynamical properties model analyzed. show spatial representations dynamically decoded using eﬀective ising model capturing correlation structure neural data compare applications data obtained hippocampal multi-electrode recordings sampling attractor model. second part focus problem learning data representations machine learning particular artiﬁcial neural networks. start introducing data representations illustrations. analyze important algorithms principal component analysis restricted boltzmann machines tools statistical physics. context background zero-dimensional attractors hopﬁeld model associative memory place cells rodent hippocampus model memorizing d-dimensional attractors replica theory phase diagram dynamics within transitions maps decoding neural representations eﬀective ising networks inference eﬀective ising models data back model subsampling problem simulations constructing reference test sessions decoding results inferred true couplings analysis multi-electrode recordings mathematical reminder bayesian inference multivariate gaussian variables principal components minimal models interacting variables retarded-learning phase transition incorporating prior information inverse hopﬁeld problem introduction early statistical physicists proved ideas issued ﬁeld could lead substantial advances disciplines. simulated annealing versatile optimization procedure ﬁctitious sampling temperature decreased minimum cost function reached major impact applied computer science engineering attractor neural network models memories soon analytically solved spin-glass techniques emerged major conceptual tool computational neuroscience. theoretical point view became rapidly clear statistical physics oﬀered powerful framework deal problems outside physics particular computer science theoretical neuroscience involving many random heterogeneous strongly interacting components remained hard tackle far. purpose present document present applications statistical physics ideas tools understanding high-dimensional representations neural networks. brain represents processes information coming outside world central issue computational neuroscience experimental progress electrophysicological optical recordings make possible record activity populations tens thousands neural cells behaving animals opening study question unprecedented access data questions brain operation large scales concomittantly machine learning algorithms largely based artiﬁcial neural network architectures recently achieved spectacular performance variety ﬁelds image processing speech recognition/production machines produce eﬃcient representations data underlying distributions crucial question understood profound similarities seem emerge representations encountered real artiﬁcial neural networks questions raised contexts utterly hard cover recent advances diverse vivid ﬁeld task impossible lectures hours each. material gathered merely reﬂects interests presumably ignorance authors anything else. present notes focus applications statistical physics study neural representations contexts computational neuroscience machine learning. ﬁrst part motivated representation spaces i.e. multiple environments hippocampal place-cell networks. extension hopﬁeld’s attractor neural network case ﬁnitedimensional attractors introduced phase diagram dynamical properties diﬀusion within attractor transitions distinct attractors analyzed. also show eﬀective functional ising models ﬁtted hippocampal multi-electrode recordings ’neural’ data generated spatially subsampling model share common features abstract model used decode track evolution spatial representations time. second part move representations data machine learning algorithms. special emphasis aspects low-dimensional representations achieved principal component analysis compositional representations produced restricted bolztmann machines combining multiple features inferred data. cases show statistical physics helps unveil diﬀerent properties representations role essential control parameters. representation space hippocampus model context background zero-dimensional attractors hopﬁeld model associative memory statistical mechanics neuroscience apart seem ﬁrst sight. indeed brains made billions neurons connected together. many cases brain functions thought outcome collective states. makes good playground statistical mechanics. here focus particular brain function memory. hebb visionary intuition memory could correspond retrieval certain activity patterns network interconnected neurons attractor hypothesis goes follows memorized attractors network i.e. activity states stable dynamical evolution rule; hence recalling memory corresponds retrieving activity pattern; attractors stored network couplings govern network dynamics stable states; possible make arbitrary pattern attractor ’wire together neurons together’ pattern j.j. hopﬁeld proposed model based hebb’s ideas case zero-dimensional equivalently point attractors. model known hopﬁeld model strongly inspired statistical physics models used context magnetic systems ising model. consists number }i=...nµ=...p e.g. independently uniformly drawn random. synaptic couplings allow conﬁgurations attractors given hebb rule later studies e.g. incorporated possibility stochasticity response noise parameter system obeyed detailed balance gibbs distribution associated hamiltonian ’temperature’ terms biological relevance hopﬁeld model course extremely schematic. captures many fundamental robust aspects neurons network remaining large extent analytically tractable. hopﬁeld model aroused great excitement statistical mechanics community since shared many common points frustrated disordered magnetic systems. tools methods statistical physics disordered systems developed ﬁeld spin glasses could therefore used derive analytically properties hopﬁeld model ﬁrst question check whether patterns }i=...nµ=...p indeed attractive ﬁxed points dynamics answer turned positive small enough values ratio i.e. strong noise memory load. many aspects model studied reﬁned particular make biologically realistic. reader kindly referred detailed presentation literature. rather focus extension model diﬀerent kind attractors ﬁnite-dimensional attractors. place cells rodent hippocampus turn real brains speciﬁcally space represented brain experimentalists small electrodes that implanted brain awake animals able record simultaneous activity population single neurons. particular brain area called hippocampus o’keefe dostrovsky discovered existence ’place cells’ recording rodents freely moving enclosure neurons surprising property animal physically located precise region space hence name. region activity corresponding place cell figure remapping place ﬁeld recorded place cell living exploring square environments identical shapes. size environments ﬁgure reports average ﬁring rate recorded cell spatial bins; values color bar. data experiment jezek environment deﬁnes ’place ﬁeld’. area hippocampus region strong recurrent connection pyramidal cells diﬀerent place ﬁelds attached given place cell across diﬀerent environments visited rodent seem totally uncorrelated property called global ’remapping’. another hippocampal area called remapping place ﬁelds environment another generally weaker; change activity place cell characterized mainly modulation ﬁring rate phenomenon called rate remapping though global changes place ﬁelds also observed cells fig. many reasons hippocampus precisely subregion often supposed work continuous attractor neural network means attractors point conﬁgurations attractors dimensions manifold corresponds environment i.e. collection activity conﬁgurations hippocampal neural population characterizing positions environment. apart dimensionality attractors place cells share common points hopﬁeld model absence correlation attractors random remapping hebb rule biological counterparts. hence appealing extend hopﬁeld model continuous attractors. model memorizing d-dimensional attractors thus introduce model place cells onetwo-dimensional spaces extension hopﬁeld model model based binary neurons; models real-valued neural variable e.g. ﬁring rates found literature place cells modeled binary units equal neurons interact together excitatory couplings jij. moreover interact inhibitory interneurons whose eﬀect maintain total activity place cells fraction active cells also assume stochasticity response neurons controlled noise parameter assumptions come considering network states distributed according gibbs distribution associated hamiltonian restricted conﬁguration spins want store environments coupling matrix. call place ﬁeld position space place cell preferentially ﬁres. environment deﬁned random permutation neurons’ place ﬁelds models experimentally observed remapping place ﬁelds other. deﬁnition environment said stored activity patterns localized environment stable states dynamics. words conﬁgurations active neurons neighbouring place ﬁelds environment equilibrium states. make possible assume hebbian prescription couplings straightforward extension hopﬁeld synaptic matrix case quasi-continuous attractors. rule illustrated figure mathematically described follows one-dimensional case would correspond linear corridors. though experimental evidence place cells code also rich contextual information consider hereafter indiﬀerently terms \"neuron\" \"place cell\" \"spin\" analogy magnetic systems. basic version model every place cells place ﬁelds every environments. possibility silent cells figure remapping connectivity rule model illustrated three units two-dimensional environments. place ﬁeld centers units displayed respectively blue green. thick yellow lines indicate excitatory couplings cells neighbouring place ﬁelds environment. place ﬁelds overlap; here sake clarity centers place ﬁelds represented. distance place-ﬁeld centers environment instance π‘|. represents distance place ﬁelds overlap. dimension practice chosen that environment neural cell coupled fraction factor cells dimension again choose ensures total input received cell remains ﬁnite goes inﬁnity limit case exact calculations become possible replica theory phase diagram calculation study stable states network conditions stable states correspond active neurons whose corresponding place ﬁelds nearby environments. words want know parameter values hebbian synaptic matrix ensures retrieval stored maps. system study enjoys disordered frustrated interactions. start computing free energy system quantity depends priori realization random permutations map. assume that large limit free energy self-averaging particular value given realization disorder typically close average possible realizations disorder thus good approximation randomness remapping process thus hypothesis model tractable. compute average logarithm replica method ﬁrst compute moment compute ﬁrst derivative respect since interested conﬁgurations place ﬁelds active neurons spatially concentrated environments arbitrarily select environments averaging remaining permutations; details calculation found choice totally arbitrary diﬀerence environments eventually averaged out. reference environment neurons indexed order place ﬁelds allows move microscopic activity conﬁguration macroscopic activity density continuous space overbar denotes average random remappings brackets correspond average fast noise. simplicity assumed environment one-dimensional here formula easily extended higher dimensions. note model analytically tractable large limit unit inﬁnite number neighbours weakly interacts with. mean-ﬁeld approximation therefore becomes exact sense order figure phase diagram lebowitz penrose’s theory lquid/vapor transition dimension function position space boundar conditions). parameters note coexistence homogeneous bump states intermediate temperatures. location bump arbitrary. parameters density exhibits ﬂuctuation however contrary standard mean ﬁeld approaches order parameters depend space case single environment i.e. strongly reminiscent theory liquidvapor transition lebowitz penrose continuous translational symmetry spontaneously broken enough temperature i.e. liquid drop surrounded low-density vapor fig. bump freely diﬀuse describes ﬁnite-dimensional continuum ground states. case moment given equation averaged term depends conﬁgurations s··· overlap matrix entries then perform limit consider replica symmetric ansatz assumes overlaps take single value edwards-anderson parameter spin glasses characterizing site-to-site ﬂuctuations spin magnetizations ansatz generally valid high-enough temperature gibbs measure deﬁned energy landscape rough below. allows compute free energy function order parameters simple interpretations. eﬀective ﬁeld acting neuron whose place ﬁeld located reference environment terms ’signal’ contribution coming neighboring neurons retrieved gaussian noise zero mean variance coming maps here denotes load memory. four order parameters fulﬁlll following saddle-point equations obtained extremization free-energy functional figure phase diagram plane thick lines transition phases. dashed-dotted line tpm. thin dashed line phase’s longitudinal stability regions. dotted line phase’s line. storage capacity replicasymmetric clump phase. cl-sg transition load temperature loss stability clump cl-pm transition temperature actually memorized. fact environments memorized since could chosen reference environment. note value totally arbitrary positions equivalent averaging permutations. glassy phase corresponding large loads local activity hsii varies neuron neuron cluster around speciﬁc location space environments averaging remappings). phase crosstalk environments large none actually stored network activity. phase contrary phase environment memorized. ’black-out catastrophe’ already described hopﬁeld model retrieval also takes place all-or-nothing fashion. need determine solution selected functions phase lowest free energy thermodynamically favored well domains existence three phases longitudinal replicon modes transition lines them. study stability write hessian free energy study eigenvalues longitudinal replicon sectors. then transition phases line free energies phases equalize. done calculations one-dimensional case detailed ref. outcome phase diagram shown fig. displaying three phases domains plane glassy phase exists always replica-symmetry broken; expect replica symmetry breaking continuous region celebrated sherrington-kirkpatrick model longitudinal stability clump phase computed numerically shown thin dashed line fig. clump stable replicon modes except little low-t high-α region interesting feature clump phase stability domain reentrance high-α boundary. checked analytically-derived phase diagram monte carlo simulations. detailed comparison phase diagram hopﬁeld model dimensionality attractors plays role dynamics within transitions maps phase diagram informs stable states model.for moderate temperature memory load i.e. phase thermodynamically stable states activity spatially localized somewhere maps. constrain retrieved i.e. position intersecting place ﬁelds active cells. indeed inﬂuence noise bump activity move around given also jump another map. studied dynamics within maps respectively within shown formally that case single continuous attractor bump activity behaves like quasi-particle little deformation. quasi-particle undergoes pure diﬀusion diﬀusion coeﬃcient computed exactly ﬁrst principle i.e. knowledge microscopic ﬂipping rates spins monte carlo simulations. diﬀusion coeﬃcient scales imposing force spins section activity changes move bump. illustration shown fig. shown analytically mobility bump diﬀusion coeﬃcient obey stokes-einstein relation. presence multiple maps disorder presence multiple maps stored couplings creates eﬀective free-energy landscape bump activity reference environment. freen correlated space length order bump size energy barriers scale typically dimension bump therefore eﬀectively undergoes brownian motion sinai potential strongly activated diﬀusion. higher dimension diﬀusion facilitated respect case observed simulations. addition moving reference environment bump also spontaneously jump maps. fast transitions maps evoked light inputs observed jezek colleagues so-called ’teleportation experiment’ understanding transitions framework simple model provides insight mechanisms involved biological system. map-to-map transitions studied replica theory again subtle framework solutions non-uniform activities maps above) searched for. scenarios spontaneous transitions spatial representations fig. mixed state gives bumps activity maps; bumps weaker phase single reference environment. scenario preferred transitions take place special ’confusing’ positions environments maps locally resemble most. show fig. rate transitions maps computed monte carlo simulations supplemental material details. observe rate increases temperature diminishes load system size. according langer’s nucleation theory expect rate related free-energy barrier phase environment phase environment growth rate unstable mode transition state unique negative eigenvalue hessian free-energy transition state volume saddle-point subspace hence expect measures rates obtained diﬀerent system size collapse onto upon following rescaling decoding neural representations eﬀective ising networks problem decoding brain state internally represented observation neural activity natural application experiments involving simultaneous recording neural population decoding problem tackled learning statistical properties known brain states classifying observations accordingly problem deeply connected high-dimensional classiﬁcation machine learning. example provided environmental memory hippocampus. activity figure monte carlo simulation sessions memory model case environments denoted x-axis states system neurons ordered increasing order place ﬁeld centers permutations. y-axis time rounds increasing down. bump forced move rightwards external force columns system initialized localized bump activity environments respectively column test simulations composed second halves simulations reported used decoding purposes text. parameter values figure map-to-map spontaneous transitions. rate transitions computed monte carlo simulations function temperature parameters replotting data panel transformation allows estimate ratio free-energy barrier temperature recorded animal explores environments. environment associated memorized cognitive reference sessions used learn models activity associated map. turn models used decode activity recorded controlled test session i.e. decide activity associated test session environmental cues manipulated experimentalist decoding internal response change experimental conditions allows investigate induced dynamics internal representations tackle decoding problem inferring probability density function neural patterns brain state probability distributions used decode internal state given observation test session. precisely decoded maximizing log-likelihood inference framework relies deﬁnition parametric probability function whose parameters inferred solving corresponding inverse problem reference data. according maxentropy principle choice family graphical models parametric probabilistic functions. depending reference sample size and/or complexity representations invert independent model accounts diferent average activations neurons diﬀerent brain states make step include correlations neural activity deﬁning ising model state reference session. brain state collect samples neural pattern known brain state compute frequencies recorded neurons; ising model reproduces quantities average i.e. denotes average probability distribution hsii highly non-trivial computational problem reviewed section deﬁniteness hereafter consider neural conﬁguration given time binarized neural activities neurons i=...n consideration neuron silent active section details. within framework therefore decode neural representation observed neural pattern. procedure applied experimental data hippocampus showing good performance retrieving explored environment neural activity similar procedures successfully applied brain regions instance applying decoding procedure context representations space review eﬀective ising model ﬁtted data. discuss section problem inference graphical model data data deﬁned recorded conﬁgurations variables denotes value variable site conﬁguration variables real valued binary multi-categorical cases great practical interest neurons described binary variables expressing whether silent active i.e. emit spike. spiking times obtained multi-electrode recordings processed series recorded neural conﬁgurations dividing recording time small time windows ...b duration then equal neuron emits spikes time remains silent. amino acid site protein sequence take diﬀerent values. data conﬁgurations sequences familiy homologous proteins assumed share common fold biological function collected protein databases sake simplicity assume hereafter variables take binary values assume distribution conﬁgurations variables given ising model deﬁned lighten notations drop subscript hereafter. model parametrized ﬁelds couplings jij. assume diﬀerent data conﬁgurations independently drawn hence probability data reads best values ﬁelds couplings minimizing convex function arguments minimum guaranteed ﬁnite. instance minimum realized neurons never spike together problem avoided including prior also called regularization ﬁelds couplings. usual regularization schemes include adding and/or norms couplings avoid small nonzero couplings couplings large unrealistic values. another regularization scheme consists imposing small rank coupling matrix section computational problem minimization calculation partition function generally intractable involves summation conﬁguration systems. methods solve inverse ising problem bypass calculation boltzmann machine algorithm pseudo-likelihood approximations minimum probability resort approximate expressions e.g. mean ﬁeld high-temperature expansions adaptive cluster expansions ising model inferred used various tasks case neurons functional connectivity physiological eﬀective couplings depending brain state protein covariation analysis shown large couplings often coincide amino acids contact three-dimensional structure protein generate conﬁgurations monte carlo simulations. useful obtain silico data features ones training instance proteins structure function natural proteins. back model subsampling problem theoretical model spatial memory place-cell populations section shows remarkable features compatible recall brain states level population activity existence clump phase system maintained local minimum self-sustained localized activity compatible attractor-neural-network general paradigm cognitive functions represented collective states neural network. presence spontaneous transitions representation another consistent ﬂickering phenomena triggered weak inputs observed hippocampus spatial memory thought stored retrieved however theoretical results obtained limit large systems also case real brain regions electrophysiological setups permit record simultaneously much smaller number neurons. natural wonder extent small number neurons could provide information collective state whole population. hereafter describe attempt draw parallel experimental conditions multi-array recordings theoretical model environment memory attractor neural network framework. ﬁrst design monte carlo simulation mimics experiment memorized environments referred simulate single-environment reference sessions forcing activity explore local minima corresponding memorized environments system relatively large number neurons. address question small randomly selected neurons could provide enough information perform decoding procedure infer time course spatial representations neural activity. place cells topographical i.e. cells physically nearby hippocampus distant place ﬁelds recording spatially located population cells thought equivalent random subsample place-ﬁeld abstract space. decoding task ﬁnally performed test session using ising independent models learned reference sessions decoding capability tested classiﬁcation problem test session composed samples states. relationship true inferred coupling analyzed. simulations performed monte carlo steps starting initial neuronal condition localized reference environments maintain total activity constant select algorithm step active spin silent spin trial deﬁned joint spins. additional small force added make bump exhaustively explore dimensional asymmetric term energy. results left-right asymmetry monte carlo acceptance rule simulations conducted. parameters carefully chosen clump phase maintained bump thoroughly explores environment spontaneous transitions occur. words system sampled maps whole simulation; mimicks fact rodent explores single environment reference sessions deﬁned using ﬁrst half simulation test session constructed concatening second halves total total time steps. parameters used following analysis diﬀerence independent ising model shown fig. remarkable. independent model couplings zero accounts average ﬁring rates cells. shows decoding capability equal could expected fact localized bump activity represents position within retrieved moves along entire environment reference sessions. hence average activity cells close maps. independent model uses information averages decode activity therefore unable achieve useful discrimination. conversely ising model exhibits impressive performance decoding task. shown fig. time course likelihood diﬀerence allows unambiguously decode spatial representation function time. diﬀerence also clear scatter plot likelihoods test session shows well-separated pattern plane contrary independent model computation true positive rates fully justiﬁes remarkable visual diﬀerence models application inference routines simulated neural network allows investigate relationship functional couplings i.e. inferred inverse ising model real coupling strength deﬁned eqn. show fig. couplings inferred neurons functions distances palce-ﬁeld centers map. observe that couplings decay rapidly distance typical scale compatible width bump; note similar values simulations. long distances couplings independent distance equal negative value. presence many long-range inhibitory couplings clearly visible histrograms fig. natural consequence constraint level activity. magnitude coupling small distances fig. much larger ’true’ couplings model equal suggest inferred couplings eﬀective would coincide true couplings limit perfect spatial sampling figure log-likelihood diﬀerence along test session using independent model ising model montecarlo test session. ﬁrst half test session sampled environment second half environment better understand value inferred couplings compute statistical moments neurons. explained description independent-cell models neurons average activity environments also estimate easily joint probability neurons active. large-n limit true couplings vanish scale hence spins become two-by-two independent ground state hamiltonian bump centered around given position conditioned deﬁning position place-ﬁeld center cell eﬀective matrix pairwise activities threfore depends explains ising model contrary independent model map-speciﬁc eﬃciently decode representation. however eﬀective correlation neurons ising couplings thus eﬀective interactions simply related true couplings model. expect statement hold also functional couplings inferred real recordings physicological synaptic counterparts. analysis multi-electrode recordings inference routines test-session validation described directly applicable micro array recordings neural activity vivo. previously introduced record brain area activity collection stable memory states build models activties decode representations successive test session. good testing ground analysis spatial memory hippocampus. environments memorized animal relatively easy collect samples memory state letting explore corresponding environment. recent experiment conducted jezek environmental conditions abruptly changed trigger instabilities evoked spatial maps test session. decoding representation expressed test session function time investigate response fast dynamics memory state hippocampal network. ising inference method used context decode denominated retrieved test sessions unknown environmental conditions. test session recorded hippocampal containing environment-switch event shown fig. contrary decoding analysis performed subsampled theoretical model iindependent-cell model shows good performance decoding task real recordings possible reason theoretical model designed describe memory storage retrieval functions hippocampal network account anatomical context region. mammalian brain hippocampus plays role complex neural circuitry involves inputs outputs brain areas medio entorhinal cortex lateral entorhinal cortex sensory input conveyed dramatically change ﬁring properties hippocampal place cells respect external environmental conditions even point silencing neurons environments. mean neural activity could therefore carry enough information achieve useful discrimination explored environments corresponding recalled memory states identical maps therefore useless decoding model. properties remapping place cells activities diﬀerent representations extensively studied neuroscience literature diﬀerent features reported diﬀerent hippocampal sub-regions detailed analysis comparison inference methods applied hippocampal data figure relationship true couplings inferred couplings. purple historgram inferred couplings. inferred couplings corresponding truly connected neurons environment. figure log-likelihood diﬀerence along test session using ising independent decoder hippocampal data. environmental conditions abruptly changed correspondence line. representations data machine learning start deﬁnition data representation. suppose given data samples ...x n-dimensional random variable joint density data transformation deterministic transformation multidimensional vector space data another larger smaller general assumed diﬀerentiable necessarily invertible. random vector representation original random vector changing representation random variable often extremely helpful data science because allows better visualization understanding process generated data; performance machine-learning algorithm classiﬁcation clustering methods heavily depends choice representation used. although obvious given representation good clear many many representations useless trivial random variable carry information generally clear transformation vary strongly across support little use. opposite much either since properties data distribution changed. typically good data representation must helpful properties have dimensionality independence components sparse values carrying information original random vector thus transformation must depend learnt. learnt data representation often shed light data generated so-called ’features’ i.e. frequent collective modes variation data partition classes discover outliers... moreover good data representation signiﬁcantly improve performance subsequent machine learning tasks retaining useful information data sample. instance so-called deep neural network learns sequence data transformations e.g. predict label image. using non-linearities so-called pooling architectures learnt intermediate representations data become invariant e.g. w.r.t. noise shifts rotations... hence learn quicker deep neural networks brought remarkable breakthrough many areas visual speech recognition natural language processing... illustrate concepts examples great relevance applications. example dimensionality reduction important subclass data transformation dimensionality reduction transformations. aims compressing random vector typically high dimension smaller random vector dimension e.g. keeping much information possible compression motivated fact data often close subspace much lower dimension so-called ’manifold hypothesis’. indeed consider instance data constituted pictures person’s face taken many diﬀerent positions; picture made pixels. clear data small subset possible colored pictures deﬁne –dimensional vector. reason that given face varying degrees freedom small number compared hence data points manifold dimension compared generally variability data often comes small number explanatory latent factors aﬀect components would like recover them. generality good general methods learn functions turns image kind ’muscle positions’ representation. simpler dimensionality reductions nonetheless learnt extremely useful. instance dimensionality reduction obtained simple linear transformation weight rectangular matrix must trained data order retain much information possible interesting choice matrix obtained principal components analysis algorithm rows eigenvectors corresponding i’th largest eigenvalues empirical data covariance matrix hxixji−hxiihxji average computed data; choice justiﬁed section transformation mainly serve purposes. ﬁrst provide better understanding data visualizing computes dimensional-representation data; data point represented space. example compute figure mnist data samples. -dimensional representation mnist handwritten digits data set. point diﬀerent image coordinates value ﬁrst second components representation. here digits represented. visualization weight matrix image principal component vector wi.; blue pixels denote large positive values. representation obtained computing overlaps image principal component vector representation images digits mnist handrwitten digits dataset vectorized dimensional vectors fig. scatter plot shows distinct clusters corresponding digit types interesting illustration interpretation molecular dynamics simulation complex systems made many strongly interacting heterogeneous microscopic components. observing dynamics systems e.g. protein described atomic level amounts practice look thousands correlated time series. principal component analysis oﬀer low-dimensional projections time traces allows visualize collective motions underlying evolution system recent review applications biomolecules including nucleic acids proteins. second purpose dimensionality reduction overcome so-called curse dimensionality. high dimensional spaces datasets sample sparsely vector space consider instance following supervised learning problem. given training data basis grayscale images cats dogs binary labels attached want train parametric model classify whether images cats dogs. point useful think classiﬁcation task essentially interpolation problem exist mathematical function want interpolate values test images. interpolation problem would trivial input space densely sampled e.g. point would training data point distance practice impossible latter condition requires data points out-of-reach large. possible way-out ﬁrst learn data representation lower dimension e.g. using train classiﬁcation model form dimensional representation keeps relevant information nature image learning performed. popular application supervised learning ’eigenface’ face recognition algorithm. representation trained data faces applying supervised learning eigenface algorithm considered among ﬁrst successful face recognition algorithms. example extracting latent features data variability real-world data images often decomposed largely independent modes variation. instance faces diﬀerent parts diﬀerent nose ears lips... lower level description image contain edge given location angle scale diﬀerent images diﬀerent activated edges. extracting so-called ’features’ particular interest machine learning particular classiﬁcation decision function must learnt expressed easily function ’features’ pixels instance could achieve better results expressing linear function instead higher order polynomial moreover learnt representations interesting statistical properties statistical dependence modes invariance respect irrelevant perturbations data corruption noise... used denoising. notable algorithms unsupervised feature extraction independent component analysis sparse autoencoders sparse dictionary learning display fig. features learnt applied mnist digits data set. features learnt correspond individual handwritten strokes unlike principal component simple interpretation. interestingly features found sparse dictionary learning applied natural images dataset qualitatively match well receptive ﬁelds neurons visual cortex mammalians monkey feature extraction carried brain bear strong analogies machine-learning procedures. mathematical reminder bayesian inference observe data sample would like data model parametrized variable assume random variables joint distribution according deﬁnition conditional probabilities write ﬁrst equality probability data given model parameters also called likelihood model parameters given data. second term prior distribution model parameters. expression rewritten using posterior distribution parameters given observations overall probability data generated class models uner consideration. posterior distribution given bayes formula simply derived historical application bayesian inference formula laplace’s statistical proof boys girls diﬀerent birth rates. laplace access number boys girls born paris girls babies born time period. although numbers male female births diﬀerent possible know priori whether discrepancy comes statistical ﬂuctuation systematic diﬀerence birth rates. laplace assumed birth realization independent identically distributed random variable giving girl probability probability basic assumption follows binomial distribution likelihood normalization constant shown fig. easy calculate mean value standard deviation posterior distribution results mean given integral probability actually larger equal interval approximately equal extremely small value makes unlikely discrepancies large numbers female male births pure statistical ﬂuctuation. multivariate gaussian variables popular example parametric model multivariate gaussian distribution used model sets continuous random variables exhibiting correlations. hereafter assume random variables zero mean sake simplicity. given vector variables write called precision matrix symmetric positive deﬁnite matrix encodes inter-dependencies variables. oﬀ-diagonal entries interpreted couplings variables. instance means conﬁgurations likely conﬁgurations leading positive correlation given data samples compute analytically maximum likelihood estimator precision matrix inversion performed numerically long data covariance matrix full rank. however ﬁnite sampling eﬀects order thus large-dimensional data sets i.e. ratio order unity expect inference plagued errors. principal components minimal models interacting variables simplest model distribution vector random variables components normalized zero mean unit variance independent corresponds case model obtained breaking isotropy. assume exists speciﬁc direction denoted dimensional space larger variance expression principal component interpreted collective mode eiσi variance he|c|ei larger whereas would independent. variables correlate makes large variance fig. given data maximum likelihood estimation performed analytically infer principal component |ei. likelihood writes hence direction eigenvector empirical covariance matrix although inference performed easily covariance matrix expect inferred vector always statistically signiﬁcant according discussion section instance even data generated according null model empirical covariance matrix largest eigenvalue ﬁnite sampling similarly data generated according principal-component model ’small’ ﬁnite largest eigenvector empirical correlation matrix away next section report analytical results derived using random matrix theory statistical mechanics tools telling inference possible. retarded-learning phase transition ﬁrst study empirical covariance matrix spectrum data generated according null model interested particular empirical density figure distribution eigenvalues empirical covariance matrix null model inﬁnite sampling; null model ﬁnite sampling; principal component model ﬁnite sampling eigenvalues overbar denotes average realizations data samples note that large limits ﬁxed ratio expect spectrum attached random realization coincide average spectrum high probability. probability density eigenvalues computed analytically using random matrix theory tools limit case dimension number data points inﬁnity ﬁxed noise level result so-called marcenko-pastur distribution expression valid larger covariance matrix full rank also dirac peak mass distribution eigenvalues plotted fig. various values noise level good sampling wigner semi-circle recovered around diﬀerent entries correlation matrix becomes essentially uncorrelated. interestingly spectrum density quite wide small instance eigenvalues large consequence ’sampling noise’ eigenvectors screen away true principal components large. computation carried principal-component model shows existence phase transition fig. largest eigenvalue well ’bulk’ eigenvalues ﬁnite sampling largest eigenvector ﬁnite overlap he|vi principal component |ei. principal eigenvalue inside marcenko-pastur ’bulk’ eigensummary recovering principal component impossible unless examples least presented error decays monotonously; hence name retarded learning coined slightly figure retarded-learning phase transition. left panels spectrum eigenvalues empirical correlation matrix principal-component model cases weak noise strong noise right panel average squared overlap components true empirical correlation matrices function noise level figure prior potentials used learning components. nonnegative case component inferred presence prior twice bigger maximum-likelihood threshold prior potentials include regularization potential favoring large components diﬀerent context. computation generalized ﬁnite number eigenvalues associated time noise level crosses eigenvalue pops noisy bulk eigenvalues corresponding eigenvector informative principal component inferred. practical application computation serve guideline many principal components keep used dimensionality reduction. instance choose keep eigenvalues larger bulk eigenvalue incorporating prior information seen previous section that possible extract vector ﬁnite scalar product component |ei. natural question whether exploiting prior information structure component help increase threshold i.e. component less data. assume prior distribution entries component useful practice look collective excitatory mode e.g. gene expression data ﬁrst sight seems trivial vector positive product good candidate however since arbitrarily sparse guarantee he|vi actually ﬁnite large limit. recently shown maximizing condition leads estimate component positive product long fig. hence incorporating prior information non-negativity entries components allows doubling noise level. figure retarded-learning phase diagram using priors large entries ﬁxed inference component possible strength large-component prior comprised between prior weak situation similar maximum likelihood decoding. prior strong inferred vector large entries required sites carrying large entries match counterparts true component. favors sparsity. recently motivated study covariation protein families search eigenvectors residue-residue correlation matrix strong components sites contact structure considered following potential favors large components. total normal still ﬁxed unity ﬁnite number components large note cost weak components small hence potential enforce sparsity constraint contrary instead maximizing likelihood maximize full posteriori probability optimization performed analytically anymore analysis typical properties solution analyzed replica method prior shown particular small values reduce learning whereas large values impeach learning fig. inverse hopﬁeld problem also strong connection inverse ising problem section coupling matrix constrained rank typically assumption help avoid overﬁtting data therefore write interaction matrix follows here respectively numbers positive negative eigenvalues total rank note number variables interaction matrix together gibbs measure deﬁne generalized hopﬁeld model made standard attractive patterns repulsive pattern make meaning patterns explicit rewrite probability distribution generalized hopﬁeld model denotes scalar product meaning patterns transparent deﬁne favored attractive patterns disfavored repulsive patterns directions space conﬁgurations along probability increases decreases quadratically. attractive repulsive patterns inferred minimization cross-entropy deﬁned ﬁnds ﬁelds patterns minimizing lowest order given figure eigenvalue spectrum pearson correlation matrix sequences trypsin inhibitor family noise ratio hence edges marcenko-pastur spectrum approximately bottom pattern contributions log-likelihood inferred hopﬁeld model patterns corresponding eigenvalues along xaxis. most-contributing patterns attractive patterns corresponding largest eigenvalues repulsive patterns corresponding smallest eigenvalues. details. figure smallest-eigenvalue repulsive patterns obtained trypsin inhibitor family index site index amino acid index. pattern localized essentially components corresponding sites contact cysteine-cysteine bridge. importantly sites close fold distabt along protein backbone. details. associated eigenvectors procedure strongly reminiscent pca. formula shows however patterns coincide eigenvectors presence pi-dependent terms. furthermore presence λµ-dependent factor discounts patterns corresponding eigenvalues close unity. eﬀect easy understand case independent spins limit perfect sampling coincides identity matrix hence patterns couplings vanish should. general case coupled spins eigenvalues equal therefore largest smallest eigenvalues guaranteed respectively unity corresponding attractive repulsive patterns real valued. inserting expression cross-entropy obtain contribution pattern log-likelihood quantity strictly positive fig. expression helps select relevant patterns decreasing order contributions δlµ. analogy selects signal eigenvectors ones detaching spectrum marcenko-pastur distribution section however diﬀerence eigenvalues attractive patterns taken account selected patterns inverse hopﬁeld model ends spectrum. small eigenvalues much bloe give large contributions log-likelihood applications study covariations protein families repulsive patterns shown localized small number sites; much information structural constraints protein e.g. pairs amino acids contact deﬁnition motivation restricted boltzmann machine graphical model i.e. probability distribution multidimensional data similar multivariate gaussian distribution boltzmann machine distribution. constituted sets random variables visible layer -the data layerhidden layer coupled together fig. joint probability distribution visible hidden unit conﬁgurations gibbs distribution uiuµ unary potentials control marginal distributions variables weight matrix couples visible hidden layers. depending choice potentials visible hidden variables binary continuous. visible potentials general chosen based data want model; example −givi ﬁeld parameter model. hidden potentials chosen arbitrarily long sampling feasible. useful examples former expression expressed analytically terms weight matrix potentials. training consists ﬁtting marginal distribution data maximum likelihood figure architecture restricted boltzmann machines. deﬁned bidirectional bipartite graph visible layer represents data connected hidden layer supposed extract statistically meaningful features data turn condition distribution. visible units indexed hidden units indexed connections visible hidden units denoted wiµ. figure model correlations among variables. boltzmann machine approach matrix pairwise correlations variables computed data network couplings inferred reproduce correlations. restricted boltzmann machine approach observed correlations common input whose values drive conﬁgurations variables. network connection visible layer layer hidden units found maximize probability data items. rightmost column indicates magnitude hidden unit function visible conﬁguration. unlike multivariate gaussian distributions studied previous section optimal must found numerically e.g. using approximate stochastic gradient ascent likelihood stress that contradistinction boltzmann machines multivariate gaussian distributions direct couplings pairs units layer nonetheless model correlations visible variables latter indirectly correlated hidden layer. informally speaking instead explaining correlations several visible units adequate couplings interpretate collective variation driven common inputs shared visible units fig. hidden units thus represent collective modes variation data. another state observe that marginalizes hidden layer eﬀective couplings visible layer units arise. instance easy show gaussian hidden units i.e. quadratic potential list above marginal distribution visible layer case recognize pairwise eﬀective hamiltonian hopﬁeld model patterns general non-quadratic hidden-unit potentials generate eﬀective hamiltonian visible units highorder interactions. presence couplings orders produced unique connections deep eﬀects sampling dynamics rbm. sampling connection data representation algorithms best seen considering sampling scheme. since connections within layer hidden layer units conditionally independent given conﬁguration visible layer conversely; hence following gibbs sampling procedure schematized fig. ﬁrst steps seen stochastic feature extraction conﬁguration whereas last steps stochastic reconstruction features deﬁne particular data representation likely hidden layer conﬁguration given visible layer conﬁguration figure back-and-forth sampling procedure rbm. hidden conﬁgurations sampled visible conﬁgurations turn deﬁne distribution visible conﬁgurations next sampling step. figure transfer functions various hidden units potentials main text. transfer function gives likely value hidden unit function input received visible units. phenomenology trained data maximum-likelihood training completed good generative models complex multimodal distributions. following describe phenomenology various kind hidden-unit potentials trained mnist dataset grayscale images handwritten digits. image ﬂattened binarized thresholding grayscale level -dimensional binary vector. following observations done training samples drawn equilibrium distribution bernoulli relu look like real digits suggesting good data distribution fig. contrary gaussian i.e. pairwise hamiltonians data distribution well. hidden unit activated selectively presence speciﬁc feature data seen visualizing columns weight matrix fig. features strokes small part digits. weight matrix therefore essentially sparse fraction nonzero weights fig. relu hidden units data image strongly activates around hidden units whereas hidden units silent weakly activated fig. precise deﬁnition number strongly hidden units estimated learnt probability distribution rough many local maxima probability seen fig. remarkably training data sample within pixels local maximum probability. shows combinatorial nature capable generating large number conﬁgurations training. phenomenology raises several questions. first simple networks generate complex distribution large variety local minima matching original data points? secondly hidden unit potentials give good results whereas others not? lastly connect behavior hopﬁeld model corresponding case quadratic potential? statistical mechanics hopeless provide answers questions full generality given parameters ﬁtted real data. however statistical physics methods concepts allow study typical energy landscape properties drawn appropriate random ensembles. follow approach hereafter using replica method deﬁne random-rbm ensemble model relu hidden units follows also drawing fig. figure training mnist. data composed binarized images includes hidden relu. weights attached four representative hidden units averages conditioned hidden-unit conﬁgurations sampled equilibrium. black white pixels correspond respectively averages equal intermediary values indicated grey levels seen edges digits. distributions numbers strongly activated hidden units silent hidden units equilibrium. evolution weight sparsity squared weight value training time measured epochs represented square–root scale. evolution number distinct local maxima distance original sample sample local minimum obtained sampling section figure random-rbm ensemble control parameters threshold hidden relu ratio sizes hidden visible layers ﬁeld visible units sparsity weights sparse weight distribution previously introduced study parallel storage multiple sparse items hopﬁeld model important understand magnitude hidden-unit activations given visible layer conﬁguration cases encountered respectively magnetization number feature-encoding hidden units mean squared activity hidden units response functions i.e. derivatives mean activity respectively hidden visible non-sparse weights depending values control parameters system show following qualitative diﬀerent behaviors found hopﬁeld model fig. ferromagnetic phase hidden conﬁgurations dominate. visible conﬁgurations strong overlap feature likely choice arbitrary ’basins’ visible conﬁgurations. phases i.e. strong overlap several features exist thermodynamically stable unfavorable free energies increase phase transition occurs frustration system. assume instance system ferromagnetic phase. input recevied visible unit strong contribution strongly magnetized unit weak inputs hidden units. ratio increases numerous weak noisy contributions unique strong signal contribution systems enters glassy phase. transition takes place well deﬁned value depends small intermediate qualitative behavior emerges compositional phase visible conﬁgurations strong overlap features fig. observed trained real data fig. random generate combinatorial diversity low-energy visible conﬁgurations corresponding diﬀerent choices subset strongly activated hidden units. phase found limit appropriate values threshold ﬁeld validation data outcomes statistical physics analysis that compositional phase number strongly activated hidden units scales inverse degree sparsity precisely determined minimzing free energy random-rbm model. minimum free energy found compositional phase contrary ferromagnetic phase prediction tested trained real data e.g. mnist fig. value training without regularization found fig. however higher figure average number active hidden units degree sparsity weights trained mnist data. values ratio exponent regularization term reported ﬁgure. ﬁgure also shows theoretical curve obtained random another realistic statistical ensemble random weights degree sparsity ﬂuctuates visible sites dashed lines show standard deviations away mean value ﬁnite-size ﬂuctuations details. changing value obtain training higher sparsities. figure shows theoretical scaling ‘∗/p well reproduced decade variation addition product good agreement theoretical prediction acknowledgements. work beneﬁted ﬁnancial support human frontier science program rgp/ project. monasson rosay crosstalk transitions multiple spatial maps attractor neural network model hippocampus collective motion activity physical review. statistical nonlinear soft matter physics vol. posani cocco jezek monasson functional connectivity models decoding spatial representations hippocampal recordings journal computational neuroscience vol. mackay information theory inference learning algorithms. cambridge university press tavoni ferrari battaglia cocco monasson functional coupling networks inferred prefrontal cortex activity show experience-related eﬀective plasticity network neuroscience ./netn_a_ tavoni cocco monasson neural assemblies revealed inferred connectivity-based models prefrontal cortex recordings journal computational neuroscience vol. mcnaughton o’keefe barnes stereotrode technique simultaneous isolation several single units central nervous system multiple unit records journal neuroscience methods vol. finn coggill eberhardt eddy mistry mitchell potter punta qureshi sangrador-vegas pfam protein families database towards sustainable future nucleic acids research vol. morcos pagnani lunt bertolino marks sander zecchina onuchic weigt direct-coupling analysis residue coevolution captures native contacts across many protein families proceedings national academy sciences vol. balakrishnan kamisetty carbonell s.-i. langmead learning generative models protein fold families proteins structure function bioinformatics vol. monasson rosay crosstalk transitions multiple spatial maps attractor neural network model hippocampus collective motion activity physical review vol. muellerstein loccisano firestine evanseck principal components analysis review application molecular dynamics data annual reports computational chemistry vol. zylberberg murphy deweese sparse coding model synaptically local plasticity spiking neurons account diverse shapes simple cell receptive ﬁelds plos computational biology vol. cocco monasson weigt from principal component direct coupling analysis coevolution proteins low-eigenvalue modes needed structure prediction plos computational biology vol. smolensky information processing dynamical systems foundations harmony theory parallel distributed processing explorations microstructure cognition vol. foundations press agliari annibale barra coolen tantari immune networks multitasking capabilities near saturation journal physics mathematical theoretical vol.", "year": "2017"}