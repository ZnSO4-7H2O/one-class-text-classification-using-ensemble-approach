{"title": "Algorithmically probable mutations reproduce aspects of evolution such  as convergence rate, genetic memory, and modularity", "tag": "q-bio", "abstract": " Natural selection explains how life has evolved over millions of years from more primitive forms. The speed at which this happens, however, has sometimes defied formal explanations when based on random (uniformly distributed) mutations. Here we investigate the application of a simplicity bias based on a natural but algorithmic distribution of mutations (no recombination) in various examples, particularly binary matrices in order to compare evolutionary convergence rates. Results both on synthetic and on small biological examples indicate an accelerated rate when mutations are not statistical uniform but \\textit{algorithmic uniform}. We show that algorithmic distributions can evolve modularity and genetic memory by preservation of structures when they first occur sometimes leading to an accelerated production of diversity but also population extinctions, possibly explaining naturally occurring phenomena such as diversity explosions (e.g. the Cambrian) and massive extinctions (e.g. the End Triassic) whose causes are currently a cause for debate. The natural approach introduced here appears to be a better approximation to biological evolution than models based exclusively upon random uniform mutations, and it also approaches a formal version of open-ended evolution based on previous formal results. These results validate some suggestions in the direction that computation may be an equally important driver of evolution. We also show that inducing the method on problems of optimization, such as genetic algorithms, has the potential to accelerate convergence of artificial evolutionary algorithms. ", "text": "natural selection explains life evolved millions years primitive forms. speed happens however sometimes deﬁed formal explanations based random mutations. investigate application simplicity bias based natural algorithmic distribution mutations various examples particularly binary matrices order compare evolutionary convergence rates. results synthetic small biological examples indicate accelerated rate mutations statistical uniform algorithmic uniform. show algorithmic distributions evolve modularity genetic memory preservation structures ﬁrst occur sometimes leading accelerated production diversity also population extinctions possibly explaining naturally occurring phenomena diversity explosions massive extinctions whose causes currently cause debate. natural approach introduced appears better approximation biological evolution models based exclusively upon random uniform mutations also approaches formal version open-ended evolution based previous formal results. results validate suggestions direction computation equally important driver evolution. also show inducing method problems optimization genetic algorithms potential accelerate convergence artiﬁcial evolutionary algorithms. keywords. mutation; artiﬁcial life; artiﬁcial evolution; biological evolution; algorithmic complexity; algorithmic information; solomonoﬀ induction; universal distribution; levin’s semi-measure central modern synthesis general evolutionary theory understanding evolution gradual explained small genetic changes populations time genetic variation populations arise chance mutation small changes leading major evolutionary changes time. interest connection possible links theory biological evolution theory information place role randomness process provides variety necessary allow organisms change adapt time. hand known sources non-uniform random mutations example function environment gender plants animals conditions same mutations traditionally considered uniformly distributed across coding noncoding regions. non-coding regions subject diﬀerent mutation rates throughout genome subject less selective pressure coding regions. so-called microsatellites repetitive segments mostly non-coding mutation rate increases function number repetitions. however beyond physical properties probability given nucleotide mutating also depends weaker stronger chemo thermodynamic bonds departures non-uniformity less well understood seem result process rather related driven direct physical chemical interactions. hand random mutation implies evidence directing force artiﬁcial genetic algorithms mutation traditionally uniform even strategies subject continuous investigation introduced function example time data size. recently suggested deeply informational computational nature biological organisms makes amenable studied considered computer programs following random walks software space space possible—and valid— computer programs. here numerically test hypothesis explore consequences vis-`a-vis understanding biological aspects life natural evolution natural selection well applications optimization problems areas evolutionary programming. context metabiology programme gregory chaitin founder theory algorithmic information introduced theoretical computational model evolves ‘organisms’ relative environment considerably faster classical random mutation theoretically sound ideas tested advancements needed actual implementation. follow experimental approach heavily based theory chaitin helped found. apply ideas evolution operating software space synthetic biological examples even investigation needed work represents ﬁrst step towards testing advancing sound algorithmic framework biological evolution. signiﬁcantly faster exponential time process would take random mutations uniform distribution applied. speed-up obtained drawing mutations according universal distribution distribution results operation computer programs explain detail following section. previous result shown chaitin’s model exhibits open-ended evolution according formal deﬁnition deﬁned accordance general intuition decidable system computable dynamics achieve computational deﬁnition. introduce system that following universal distribution optimally approaches oee. core approach concept algorithmic probability introduced solomonoﬀ levin chaitin denoted algorithmic probability binary string formally deﬁned algorithmic probability connects algorithmic likelihood intrinsic algorithmic less algorithmically complex frequently produced running random computer program descriptive algorithmic complexity mainstream practice consideration application mutation mutations happen according uniform distribution based e.g. length genomic sequence independent ﬁtness function. show things equal without making considerations genetic operations results indicate operation random mutation based algorithmic probability universal distribution makes ‘organisms’ converge faster interesting phenomenological implications modularity. evidently claim completely independent ﬁtness function. ﬁtness function assigns example higher ﬁtness organisms whose description maximizes algorithmic randomness application mutations based algorithmic probability universal distribution fail optimally would pushing exactly opposite direction. show long ﬁtness function maximizes non-algorithmic random structure—as would expected organisms living structured world mutations based universal distribution converge faster. illustrate diﬀerence other classical probability producing ﬁrst digits mathematical constant binary chance e.g. randomly typing typewriter exponentially unlikely function number digits produced. however random sense short description generate arbitrary number digits formula algorithmic likelihood generated random program much higher classical probability. probability producing short computer program encoding short mathematical formula likely typing digits one. probability based generating computer programs rather generating objects computer programs geenrate called algorithmic probability. π-generating formula thus written computer program bits probability occurring chance divergent probability given classical probability. chaitin’s evolutionary model successful mutation deﬁned computable function chosen according probabilities stated universal distribution changes current state system better approximation constant order able simulate system would need compute universal distribution ﬁtness function. however universal distribution ﬁtness function system require solution halting problem uncomputable. nevertheless itself solution approximated proposing model that best knowledge ﬁrst computable approximation chaitin’s proposal. justify ﬁrst concession similar fashion chaitin assume interactions mechanics natural world computable probability decidable event occurring given universal distribution. third necessity algorithmic probability object uncomputable upcoming section show shannon’s entropy good purposes. finally note given universal distribution ﬁxed input probability mutation inverse proportion descriptive complexity output constant error. words highly probable mutation reduce information content input improbable increase information content. therefore last concession yields adequate approximation since information mutation reduce descriptive complexity input increase meaningful way. chaitin’s evolution model faster regular random models despite targeting highly random object thanks fact positive mutations algorithmic information complexity hence high probability stochastically chosen universal distribution. universally algorithmic complexity positive mutations relies fact that assuming oracle also implying constant algorithmic complexity contrast expected model sensitive respect algorithmic complexity target matrix obtaining high speed-up structured target matrices decreases algorithmic complexity target grows. however change behaviour remains congruent main argument metabiology assertion that contrary regular random mutations algorithmic probability driven evolution tends produce structured novelty faster rate hope prove upcoming experiments. shown compared think shannon’s entropy alone less accurate approximation algorithmic complexity object therefore expect entropy-induced speed-up consistently outperformed target matrix moves away algorithmic randomness thus structure. furthermore random matrices expected balanced number anticipated performance single entropy nearly identical uniform distribution unstructured matrices. block entropy entropy computed submatrices rather single bits probability repeated blocks inverse proportion size blocks smaller sizes approximate single entropy yielding similar results uniform distribution. results support assumptions claims. positive integer called extinction threshold real number called convergence parameter non-deterministic evolution dynamic number steps mutations took produce unable better ﬁtness given time ﬁnds speciﬁcally function receives individual returns evolved individual time speciﬁed improves upon value ﬁtness function time took unable reached convergence value. experimental setup problem instance experiment phase state binary matrices sizes ﬁtness function deﬁned hamming distance target matrix convergence parameter words evolution converges produce target matrix guided hamming distance deﬁned number diﬀerent bits input matrix target matrix. stated setup chosen since allows easily deﬁne control descriptive complexity ﬁtness function controlling target matrix therefore also control complexity evolutionary system itself. important note setup seen generalization problem initial state binary initial gene target matrix target gene; obtain hamming distance obtained gene equality. universal distribution non-computable approximate approximating algorithmic complexity means block decomposition method based coding theorem method evolution instance computed iterating dynamic. start deﬁning possible mutations within ﬁxed number bits input matrix. words given input matrix possible mutations single instance deﬁned normalization factors respective probabilities implementation purposes used minor variation entropy probability distribution used compared bdm. probability distributions possible mutations using entropy built using heuristics possible mutation probability obtaining mutation deﬁned either ﬁrst deﬁnition assigns linearly higher probability mutations lower entropy. second deﬁnition consistent rest experiments. constant arbitrary small value included avoid undeﬁned probabilities. presented experiments probability distribution computed number steps then using random number generator proceed stochastically draw matrix sated probability distributions evaluate ﬁtness function adding number steps. resultant matrix show improvement ﬁtness draw another matrix another number steps stopping process obtain matrix better ﬁtness reach extinction threshold. either replace drawn matrix leave produce complete evolution sequence iterate stated process either convergence extinction reached. stated before choose replace evaluated matrix possible mutations instance chose keep track evaluated matrices instance complete. done order keep open possibility dynamic ﬁtness functions future experiments. case evolution time deﬁned number steps took initial matrix reach equality target matrix. computing evolution dynamics diﬀerent probability distribution schemes denote uniform strategy strategy strategy respectively. uniform distribution distribution algorithmic probability estimation distribution shannon entropy. using strategies uniform distribution block shannon’s entropy blocks size denoted entropy single bits denoted variants divide respectively. strategies repeated -bit shifts results obtained summarized table lays strategy used experiment number shifts/mutations allowed average number steps took reach convergence well standard error sample mean. diﬀerences number steps required reach convergence statistically signiﬁcant validating assertion that random matrices entropy evolution much diﬀerent uniform evolution. algorithmic complexity network makes sense unlabelled version general cases. showed theoretically numerically approximations algorithmic complexity adjacency matrices labelled graphs good approximation algorithmic complexity unlabelled graphs. means consider adjacency matrix network good representation network disregarding graph isomorphisms. highly structured matrix adjacency matrix star nodes. matrix expected entropy unable capture structure results obtained accorded expectations. results shown table results entropy unable show statistically signiﬁcant speedcompared uniform distribution. next sections show obtained statistically signiﬁcant speed-up using approximation algorithmic probability distributions expected manages better capture algorithmic structures matrix rather distribution bits entropy measures. based previous experiments conclude entropy thus good approximation omit rest article. sets separated initial target matrices. proceeded evolve initial matrix corresponding target following strategies uniform within -bit -bit shifts generated high complexity evolving system using uniform strategy tends faster using highly random matrices. secondly although increasing number possible shifts seems ﬁrst glance small change setup impact results number extinctions gone methods uniform strategy bdm. means evolutions rise threshold right evolution needed number shifts removing repeated matrices help signiﬁcant avoid extinction since much larger given values discussed chosen extinction threshold number shifts matrices allowing possible mutations stage strategies requiring evolutionary steps evolutionary stage shown consume high amount computational resources. matrices overall number steps needed reach extinction often signiﬁcantly higher strategy. behaviour cannot explained analysis done uniform distribution predicts sharp drop observed blue curve. analyzing matrices drawn failed mutations found matrices common highly regular structures. call structures persistent structures. formally regular structures deﬁned follows computable position small number codiﬁcation without contents call persistent regular structure degree probability choosing mutation subsequence previous inequality consequence fact possible mutations ﬁnite small number them smaller algorithmic complexity mutations contain otherwise contradict existence words relatively complexity structures contain tend also algorithmic complexity hence higher probability chosen. finally shown section expect number mutations persistent structures increase factorial order number possible mutations polynomial order respect size matrices compose state space. previous section established strategy yields negative speed-up targeting randomly generated matrices expected high algorithmic information content unstructured. however stated section behaviour within expectations. next section show instances positive speed-up including cases previously entropy failed show statistically signiﬁcant speed-up outperformed bdm. evolving randomly generated matrices three stated matrices report found varying degrees positive speed-up correspond respective descriptive complexities approximated values. complete graph along empty graph graph lowest approximated descriptive complexity value expected best speed-up quotient case. complete graph star intuitively seems less complex graphs draw. however value higher grid accordingly speed-up obtained lower. results shown figure figure positive speed-up quotient consistently found within -bit shifts without replacements. instance negative speed-up shift replacements grid negative speed-up complete graph shifts. however important almost instances negative speed-up statistically signiﬁcant high extinction rate diﬀerence averages lower standard errors mean. exception grid -bit shift extinctions strategy. complete tables presented appendix. cause extinctions found grid call maladaptive persistent structures occur signiﬁcantly higher rate distribution. also results suggest strategy avoid problem adding memory evolution. case replace matrices already drawn possible mutations. believe change contradictory stated goals since another behaviour universal distribution dooms populations certain mutations extinction evolution must strategies eliminate mutations fast population. argument also implies extinction faster universal distribution regular random evolution persistent maladaptive mutation present seen form mutation memory. requirement potential explain evolutionary phenomena cambrian explosion well mass extinctions positively structured mutation developed algorithmic mutations keep applies negatively structured mutations. also explain recurring structures found natural world. degradation structure still possible relatively slow. words evolution remember positive negative mutations structured. explored various cases found several conditions negative positive speed-up present following experiment oﬀer broader view distribution speed-up instances functions algorithmic complexity. target matrix ﬁrst target matrix generated random initial matrices evolved population convergence reached using stated strategies uniform without replacements. saved number steps took evolutions reach convergence computed average speed-up quotient target matrix. stated process repeated second target matrix generating random matrices target matrices conserve computational resources. experiment results figure average number steps required reach convergence lower using distribution matrices algorithmic complexity diﬀerence drops along complexity matrices never crosses extinction threshold. suggests symmetry diagonal enough guarantee degree structure captured bdm. important report found extinction case combining data obtained previous sequence random matrices used section approximate positive speed-up distribution. given nature data approximation given curves representing expected evolution time random initial matrix function algorithmic information complexity figure speed-up quotient deﬁned took reach convergence uniform strategy sbdm ‘ext.’ diﬀerence ebdm factor number extinctions obtained universal distribution respectively. case extinction sample used compute average number steps. line designates speed-up threshold line positive speed-up negative speed-up. blue line represents cubic regression data points. ﬁrst result figure conﬁrmation expected unlike uniform strategy strategy highly sensitive algorithmic information content target matrix. words makes diﬀerence uniform probability mutation space whether solution structured algorithmic probability driven mutation naturally converge faster structured solutions. results obtained expand upon theoretical development presented section possible mutations grows instances persistent structures slow-down itself. behaviour evident given that increase dimension matrices obtain wider within intersection point curves expected value corresponds expected algorithmic complexity randomly generated matrices. however also increase number structured matrices ultimately producing richer interesting evolution space. figure positive speed-up instances coral curve computed cubic linear regression evolution times strategy teal line cubic approximation evolution times uniform strategy. black line expected value randomly chosen matrix. large data reﬂects fact hard structured objects. main hypotheses algorithmic probability better model explaining biological diversity important explore whether naturally occurring structures likely produced strategy uniform strategy equivalent showing evolving faster. binary target matrix shown figure bits. ﬁrst experiment generated random matrices evolved using -bit shift mutations uniform distributions without repetitions. matrix right intersection point inferred cubic models shown figure therefore predict slow-down. results obtained shown table results show obtained slow-down without extinctions. however mentioned above target matrix relatively high result consistent previous experiments. however strategy improved. evolutionary network tensor dimension nodes networks edges drawn evolves weight corresponding number times network evolved fig. shows subnetwork full network evolutionary strategy randomly generated networks biological erbb signalling network target. call forward mutations mutations target network backward mutations mutations away target network evolutionary paths induced forward mutations. forward mutations neighbourhood target strategy follow. uniform distribution following network forward mutations equal probability lower bdm-based disconnected graph meaning produced case convergent evolution even reaching target. previously mentioned proposition main causes slow-down distribution maladaptive persistent structures. structures negatively impact evolution speed factorial order relative size state space. direct reduce size possible mutations reduce size matrices evolving. however reduce number interesting objects evolve towards too. another accomplish objective using heuristic rely localized mutations. force mutation take place submatrix input matrix. adjacent submatrices compose input matrix overlap force mutation computing probability distribution matrices contain bit-shift chosen place. call method local method. important note that within -bit shifts space total possible mutations remains compared uniform strategies. furthermore behaviour uniform strategy would remain unchanged extra step applied using uniform distribution. repeated experiment shown table addition local strategy random initial matrices. results shown table results obtained local obtains statistically signiﬁcant speed-up compared uniform. potential explanation failed obtain speed-up network strategy that approximation model depends ﬁnding global algorithmic structures sample based substructure might enough information underlying structures hypothesize govern natural world allow scientiﬁc models predictions. however biology evolves modular systems genes cells turn build building blocks proteins tissues. therefore local algorithmic mutation better model. good place recall local devised natural solution problem presented maladaptive persistent structures global algorithmic mutation. also means type modularity evolved given provides evolutionary advantage results demonstrate. compatible biological phenomenon non-point mutations contrast point mutations aﬀect single nucleotide. example microsatellites mutations lead gain loss entire repeated unit sometimes several repeats simultaneously. explore relationship local within context global structures next section. current setup optimal experimentation biological local structured matrices computational resources required build probability distribution instance grows quadratic order relative matrix size though computational resources needed real world graphs k-ary trees evolving -star graph star-to-path graph dynamic transition artiﬁcially created project dynamical networks families directed labelled graphs evolve time using deterministic algorithm display interesting graph-theoretic entropy-fooling properties evolution dynamics graphs fully deterministic expected signiﬁcantly faster evolutionary strategies uniform probability local bdm. evolution input deﬁned adjacency matrix corresponding target adjacency matrix si+. order normalize matrix size deﬁned networks always containing number nodes followed results obtained starting random graphs diﬀerent evolution paths stage shown figure important note that since graphs directed matrices used non-symmetrical. results local consistently outperformed uniform probability evolution strategy faster signiﬁcant margin. results expected conﬁrm hypothesis uniform evolution cannot detect underlying algorithmic cause evolution inducing faster overall evolution. local detect local regularities good enough outrun uniform evolution cases. however algorithmic regularities global local slower bdm. target matrix expected random matrix conﬁrming theoretical expectations. obtained speed-up considered stated quotient suﬃcient close rich evolution space expect diﬀerence signiﬁcant rough estimate human genome potentially store megabytes data biggest matrix used experiments hand classical mechanics establishes random events apparent fundamental. means mutations truly random result interacting deterministic systems distribute diﬀerently random. distribution representing causal determinism suggested algorithmic probability universal distribution theoretical stability changes formalism description language relevancy even non-turing universal models computation also proven able explain bias towards simplicity. hand mathematical mechanisms biological information mendelian inheritance darwin’s evolution discovery digital nature genetic code together mechanistic nature mechanisms translation transcription inter cellular processes suggests strong algorithmic basis underlying fundamental biological processes. taking next consequence ideas indicate evolution natural selection diﬀerent thus regarded studied evolving programs software space suggested chaitin ﬁndings demonstrate computation thus powerful driver evolution better explain aspects life. eﬀectively algorithmic probability reduces space possible mutations. abandoning uniform distribution assumption questions ranging apparition sudden major stages evolution emergence ‘subroutines’ form modular persistent structures need evolving memory carrying information organized modules drive evolution selection explained. algorithmic distribution emerges naturally interaction deterministic systems words simulating conditions algorithmic/procedural world reason believe requires greater real-world computation required assumption uniform distribution given highly parallel computing nature physical laws. universal distribution thus considered natural even natural uniform distribution. interplay evolvability organisms persistence structures also explains opposed phenomena recurrent explosions diversity mass extinctions phenomena occurred history life earth satisfactorily explained uniform mutation assumption. results suggest extinction intrinsic mechanism biological evolution. summary taking informational computational aspects life based modern synthesis ultimate natural consequences present approach based weak assumptions deterministic dynamic systems oﬀers novel framework algorithmic evolution within study biological artiﬁcial evolution. complexity invariant—up constant value— respect choice reference universal turing machine. finding exact value lower semi-computable problem. means general eﬀective method given string upper bounds estimated. among computable methods used upper bound coding theorem method block decomposition method relies upon approximating algorithmic probability object running every possible machine large small turing machines generating empirical probability distribution produced strings counting number small turing machines produce string halt. algorithm decided small number turing machines decide runs exponential time therefore approximations small strings feasible. however computation needs done populate lookup table allows application linear time. test speed algorithmic evolution recursive dynamic networks generated algorithmic graphs diﬀerent graph deﬁned also algorithmic complexity. needed graphs evolved time algorithmic complexity fashion algorithmic complexity graphs. graphs canonically labelled using positive natural numbers maximizing number nodes consecutive numbers rule applied lowest highest number transformation complete. details theoretical numerical application matrices graphs provided tables contain full statistical information speed-up obtained simple graphs ‘complete’ ‘star ‘grid conceived project. provided guidance data proposed experiments. analyzed data. wrote code. wrote paper. authors gave ﬁnal approval publication. corresponding author. acknowledge ﬁnancial support mexican science technology council posgrado ciencia ingenier´ıa computati´on unam research grant sep-conacyt. acknowledges support swedish research council (veten-", "year": "2017"}