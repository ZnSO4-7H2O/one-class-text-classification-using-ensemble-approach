{"title": "First-spike based visual categorization using reward-modulated STDP", "tag": "q-bio", "abstract": " Reinforcement learning (RL) has recently regained popularity, with major achievements such as beating the European game of Go champion. Here, for the first time, we show that RL can be used efficiently to train a spiking neural network (SNN) to perform object recognition in natural images without using an external classifier. We used a feedforward convolutional SNN and a temporal coding scheme where the most strongly activated neurons fire first, while less activated ones fire later, or not at all. In the highest layers, each neuron was assigned to an object category, and it was assumed that the stimulus category was the category of the first neuron to fire. If this assumption was correct, the neuron was rewarded, i.e. spike-timing-dependent plasticity (STDP) was applied, which reinforced the neuron's selectivity. Otherwise, anti-STDP was applied, which encouraged the neuron to learn something else. As demonstrated on various image datasets (Caltech, ETH-80, and NORB), this reward modulated STDP (R-STDP) approach extracted particularly discriminative visual features, whereas classic unsupervised STDP extracts any feature that consistently repeats. As a result, R-STDP outperformed STDP on these datasets. Furthermore, R-STDP is suitable for online learning, and can adapt to drastic changes such as label permutations. Finally, it is worth mentioning that both feature extraction and classification were done with spikes, using at most one spike per neuron. Thus the network is hardware friendly and energy efficient. ", "text": "mozafari kheradpisheh masquelier nowzari-dalini ganjtabesh first-spike-based visual categorization using reward-modulated stdp. ieee transactions reinforcement learning recently regained popularity major achievements beating european game champion. here ﬁrst time show used eﬃciently train spiking neural network perform object recognition natural images withusing external classiﬁer. used feedforward convolutional temporal coding scheme strongly activated neurons ﬁrst less activated ones later all. highest layers neuron assigned object category assumed stimulus category category ﬁrst neuron ﬁre. assumption correct neuron rewarded i.e. spiketiming-dependent plasticity applied reinforced neuron’s selectivity. otherwise anti-stdp applied encouraged neuron learn something else. demonstrated various image datasets reward modulated stdp approach extracted particularly discriminative visual features whereas classic unsupervised stdp extracts feature consistently repeats. result r-stdp outperformed stdp datasets. furthermore r-stdp suitable online learning adapt drastic changes label permutations. finally worth mentioning feature extraction classiﬁcation done spikes using spike neuron. thus network hardware friendly energy eﬃcient. keywords spiking neural networks reinforcement learning reward-modulated stdp visual object recognition temporal coding firstspike based categorization. neurons brain connected synapses strengthened weakened time. neural mechanisms behind long-term synaptic plasticity crucial learning investigation many years. spiketiming-dependent plasticity unsupervised form synaptic plasticity observed different brain areas particular visual cortex stdp works considering time diﬀerence prepostsynaptic spikes. according rule presynaptic neuron ﬁres earlier postsynaptic synapse strengthened studies shown stdp results coincidence detectors neuron gets selective frequent input spike pattern leading action potential whenever pattern presented stdp works well ﬁnding statistically frequent features however unsupervised learning algorithm faces diﬃculties detecting rare diagnostic features important functionalities decision-making. several studies suggest brain’s reward system plays vital role decision-making forming behaviors. also known reinforcement learning learner encouraged repeat rewarding behaviors avoid leading punishments found dopamine neuromodulator important chemical substances involved reward system release proportional expected future reward also shown dopamine well neuromodulators inﬂuences synaptic plasticity changing polarity adjusting time window stdp well-studied ideas model role reward system modulate even reverse weight change determined stdp called reward-modulated stdp rstdp stores trace synapses eligible stdp applies modulated weight changes time receiving modulatory signal; reward punishment izhikevich proposed r-stdp rule solve distal reward problem reward immediately received. solved problem using decaying eligibility trace recent activities considered important. showed model solve classical instrumental conditionings year farries fairhall employed r-stdp train neurons generating particular spike patterns. measured diﬀerence output target spike trains compute value reward. also florian showed r-stdp able solve task either rate temporal input coding learning target ﬁring rate. year later legenstein investigated conditions r-stdp achieves desired learning eﬀect. demonstrated advantages r-stdp theoretical analysis well practical applications biofeedbacks two-class isolated spoken digit recognition task. vasilaki examined idea r-stdp problems continuous space. showed model able solve morris water maze quite fast standard policy gradient rule failed. investigating capabilities r-stdp continued research fr´emaux conditions successful learning theoretically discussed. showed prediction expected reward necessary r-stdp learn multiple tasks simultaneously. studying mechanism brain gathered attentions recent years researchers solve practical tasks reward-modulated synaptic plasticity visual object recognition sophisticated task humans expert. task requires feature extraction done brain’s visual cortex decision-making category object higher brain areas involved. spiking neural networks widely used computational object recognition models. terms network architecture several models shallow deep recurrent fully connected convolutional structures rate-based coding others temporal coding various kinds learning techniques also applied snns backpropagation tempotron supervised techniques unsupervised stdp stdp-variants although stdp-enabled networks provide biological plausible means visual feature extraction need external readout e.g. support vector machines classify input stimuli. additionally stdp tends extract frequent features necessarily suitable desired task. research present hierarchical equipped r-stdp solve visual object recognition natural images without using external classiﬁer. instead classspeciﬁc neurons reinforced early possible target stimulus presented network. thus input stimuli classiﬁed solely based ﬁrst-spike latencies fast biologically plausible way. r-stdp enables network task-speciﬁc diagnostic features therefore decreases computational cost ﬁnal recognition system. network based masquelier thorpes model four layers. ﬁrst layer network converts input image spike latencies based saliency oriented edges. spike train goes local pooling operation second layer. third layer network includes several grids integrate-and-ﬁre neurons combine received information oriented edges extract complex features. trainable layer network employs r-stdp synaptic plasticity. signal modulation synaptic plasticity provided fourth layer decision network made. network uses earliest spike emitted neurons third layer make decision withusing external classiﬁer. decision correct global reward signal generated. besides order increase computational eﬃciency cell network allowed spike image. motivation spike neuron computational eﬃciency also biological realism decision-making without classiﬁers spike neuron turns proposed method well-suited candidate hardware implementation. performed experiments illustrate abilities r-stdp. showed network employing r-stdp ﬁnds informative features using fewer computational resources stdp. also showed r-stdp change behavior neuron needed encouraging unlearn learned before. thus reusing computational resource longer useful. moreover evaluated proposed network object recognition natural images using three diﬀerent benchmarks caltech face/motorbike eth- norb results experiments demonstrate advantage employing r-stdp stdp ﬁnding task-speciﬁc discriminative features. network reached performances caltech face/motorbike eth- norb datasets. rest paper organized follows precise description proposed network provided section then section results experiments presented. finally section proposed network discussed diﬀerent points view possible future works highlighted. section ﬁrst describe structure proposed network functionality layer. explain r-stdp neurons achieve reinforced selectivity speciﬁc group input stimuli. finally give detailed description classiﬁcation strategy used evaluate network’s performance. ﬁrst layer network simple layer whose cells detect oriented edges input image. cells emit spike latency inversely proportional saliency edge. complex layer introduces degrees position invariance applying local pooling operation. neuron propagates earliest spike input window. figure overall structure proposed network four retinotopically organized layers. ﬁrst layer extracts oriented edges input image applying gabor ﬁlters. local max-pooling operation applied cells subsequent layer gain degrees position invariance. here spikes propagated latencies inversely proportional maximum values. spikes inputs neurons layer equipped r-stdp learning rule. neurons encouraged/punished learn/unlearn complex features. activity neurons used neurons decision-making. neurons associated class labels decision made based neuron earliest spike. feature maps grid size containing dummy neurons propagate spikes. using intensityto-latency encoding scheme obtained feature maps converted spike latencies inversely proportional saliency edges. words salient edge earlier corresponding spike propagated. second complex layer network decision-making layer. neuron layer assigned category performs global pooling operation neurons particular grid. using rank-order decoding scheme neuron ﬁres ﬁrst indicates network’s decision input image. according decision made network reward/punishment signal generated drives synaptic plasticity neurons. goal layer extract oriented edges gray scaled input image turn spike latencies. input image convolved gabor ﬁlters four diﬀerent orientations. thus layer includes four feature maps representing saliency edges particular preferred orientation. made integrate-and-ﬁre neurons. neuron layer detects complex feature receives inputs neurons generates spike membrane potential reaches threshold. synaptic plasticity learning rule based three factors pre-synaptic spike time postsynaptic spike time reward/punishment signal. kind synaptic plasticity provides ability control behavior neurons terms selectivity input patterns. neurons particular grid which emits spike immediately receiving earliest input spike. pooling operation decreases redundancy layer shrinks number required neurons consequently increases computational eﬃciency. also adds local invariance position oriented edges. inhibition mechanisms employed help network propagate salient information. neuron located position grid ﬁres neurons position grids prevented ﬁring latencies nearby neurons grid increased factor relative mutual euclidean distance. experiments inhibition done distances pixel inhibition factors respectively. neurons synaptic weights feature detecting updated based order prepost-synaptic spikes well reward/punishment signal signal derived activity next layer indicates network’s decision. besides initial weights synapses randomly generated mean standard deviation note choosing small midrange values mean results inactive thus untrained neurons. moreover large values variance increase impact network’s initial state. accordingly high mean value small variance suitable choice mentioned before activity neurons indicates decision network. divide neurons several groups assign group particular category input stimuli. then network’s decision category input stimulus assumed whose group propagates earliest spike among groups. ﬁrst. network receives reward decision matches correct category input stimulus. none neurons reward/punishment signal generated thus weight-change applied. moreover neuron early minimum index selected. propose reinforcement learning mechanism update pre-synaptic weights neurons. here magnitude weight change modulated reward/punishment signal received according correctness/incorrectness network’s decision. also applied one-winnertakes-all learning competition among neurons earliest spike winner updates synaptic weights. note neuron determining network’s decision. refer postpre-synaptic cells respectively ∆wij amount weight change synapse connecting neup scale magnirons tude weight change. furthermore specify direction weight change here learning rule take account exact spike time diﬀerence uses inﬁnite time window. according learning rule punishment signal reverses polarity stdp words swaps long-term-depression long-term-potentiation done conduct eﬀect aversion encourage neuron learn something else. reinforcement learning problems chance trapped local optima overﬁtting acquiring maximum possible reward training examples. order help network exploring possible solutions general cover seen unseen examples apply additional mechanisms training phase. techniques used object recognition tasks. relatively high beginning training phase training trials ratio correctly classiﬁed samples misclassiﬁed ones increases. case high rate misclassiﬁcation network receives punishment signals rapidly weakens synaptic weights generates dead highly selective neurons cover small number inputs. similarly rate correct classiﬁcation gets higher rate reward acquisition increases well. case network prefers exclude misclassiﬁed samples getting selective correct ones remain silent others. either cases overﬁtting happens unbalanced impact reward punishment. tackle problem multiply adjustment factor amount weight modiﬁcation impact correct incorrect training samples balanced trials. assume network sees training samples training iteration nhit nmiss denote number samples classiﬁed correctly incorrectly last training iteration respectively. number training samples then weight changes current training trial modiﬁed follows here employ dropout technique causes neuron temporary turned probability pdrop. technique gives rise overall involvement rate neurons turn increases chance ﬁnding discriminative features also decreases rate blind ﬁrings mentioned before activity last layer particularly earliest spike layer information network uses make ﬁnal decision input stimuli. need external classiﬁers increase biological plausibility network time. then network uses equation classify input stimuli. training phase network’s decision compared label stimulus reward signal generated decision matches label. reinforcement learning scenario goal learner maximize expected value reward acquisition. case since network sees training samples number features suﬃcient correctly classify almost training samples. issue appears cause severe overﬁtting face complex problems network prefers leave neurons untrained. neurons decrease rate network testing samples blindly almost stimuli. object recognition tasks make comparison model r-stdp uses stdp. ﬁrst train network using stdp network extract features unsupervised manner. next compute three kinds feature vectors length layer extracting feature vectors training testing sets k-nearest neighbors support vector machine classiﬁers used evaluate performance network. moreover learning strategy stdp formula make fair comparison values parameters models. parameters explored stdp magnitudes ltd. evaluate proposed network learning strategy performed types experiments. first used series hand-made problems show superiority r-stdp stdp. second assessed proposed network several object recognition benchmarks. using stdp neuron exposed input spike patterns tends earliest repetitive sub-pattern neuron reaches threshold ﬁres tendency favor early input spikes troublesome case distinguishing spike patterns temporal diﬀerences late sections. assume several categories input stimuli possess spatial conﬁguration also identical early spikes. patterns repetitively presented group neurons synaptic plasticity governed stdp onewinner-takes-all mechanism. neurons thresholds gets selective early common part input stimuli inhibits figure temporal discrimination task. input stimuli including temporally diﬀerent sub-pattern. spikes propagated white squares order written them. synaptic weights learned neurons stdp r-stdp higher weight lighter gray level. synaptic weights used stdp-enabled neurons large receptive ﬁelds high thresholds. neurons. since early parts spatiotemporally among input stimuli chance neurons synaptic plasticity. consequently overactivity neuronal group input stimuli classiﬁes single category. also stdp-based solutions problem however ineﬃcient using computational resources. example increase size receptive ﬁelds along thresholds neurons gain opportunity receive last spikes well early ones. another possible solution many neurons locally inhibit drop one-winner-takes-all constraint. regarding initial random weights chance neurons learn parts input stimuli. spatially similar means spikes propagated similar locations inputs. illustrated fig. input grid white gray squares. white squares denote locations spike propagated. time presenting patterns network spikes propagated temporal order deﬁned numbers written squares. according ordering spikes lower numbers propagated earlier. since input stimuli artiﬁcial spike patterns need apply gabor ﬁlters thus directly layer there neuronal grids parameters shown fig. using stdp network extracted non-discriminative feature shared input stimuli. side proposed reinforcement learning mechanism guided neurons extract features whose temporal order appearance thing leading successful pattern discrimination. repeated experiment times using diﬀerent random initial weights. results showed network succeeded times chance stdp discriminative features. increased threshold similar input stimuli parameter values except swapped target input stimuli training iterations shown fig. beginning simulation desired behavior neurons belonging ﬁrst grid respond ﬁrst stimulus earlier second grid vice versa. iterations convergence fulﬁlled swapped target stimuli. stage since neurons exclusively sensitive previous target stimuli began generate false alarms. consequently network receiving high rates punishments around iterations turn swapped network received punishments previously weakened synapses stronger therefore sensitivity diminished while neurons regained possibility learning something new. iteration neurons found target stimulus again converged discriminative features summary r-stdp enables neurons unlearn learned far. ability results neurons ﬂexible behavior able learn rewarding behavior changing environments. ability also helps neurons forget escape local optima order learn something earns reward. applying stdp scenario work since diﬀerence task task unsupervised point view. mentioned earlier brain reward system plays important role emergence particular behavior. part paper demonstrate r-stdp’s capability readjusting neurons’ behavior online manner. section performance network categorization natural images evaluated. begin description datasets used experiments. then show network beneﬁts reinforcement learning mechanism extract features natural images followed comparing r-stdp stdp object recognition tasks. finally illustrate figure neurons ﬂexible behavior. target stimuli neuronal grid. task target stimuli task swapped. flexibility network changing environment. plots rows represent changes synaptic weights. plot bottom illustrates changes rate receiving reward punishment convergence synaptic used three well-known object recognition benchmarks evaluate performance proposed network. ﬁrst easiest caltech face/motorbike mainly used demonstration purposes. next used evaluate proposed network eth- small norb. datasets contain images objects diﬀerent view points make task harder criminative features spatially temporally. here show r-stdp encourages neurons become selective particular category natural images. trained examined network images categories face motorbike caltech dataset. experiment neuronal grids category reinforced ﬁrstspike competition response images target categories. therefore desired behavior network neurons ﬁrst grids selective face category grids selective motorbikes. fig. illustrates behavior network training iterations. since early iterations contained rapid changes plotted wider. early iterations strong synaptic weights figure training network caltech face/motorbike dataset. evolution four diﬀerent features extracted network. black plots correspond face motorbike neurons respectively. rate neurons category. gray ﬁlled curves depict percentage times face neurons emit earliest spike response target stimulus. notice curves motorbike neurons mirrored vertically sake better illustration rates testing indicated dot-patterns. trajectory changes learning rate respect number correct incorrect categorizations. dropout probability resulted unstable network whose neurons responded random input stimuli. chaotic behavior easily spotted early iterations middle plot network continues training iterations reward/punishment signals made neurons selective target categories. shown fig. iterations quite robust selectivity appeared training samples testing samples elongated iterations. quick convergence training samples fact network relatively fast ﬁnding features successfully discriminate seen samples primary features need converge applicable testing samples requires even iterations adaptive learning rates. moreover learning rate shown proposed network successfully classiﬁed faces motorbikes high accuracy. here examined performance proposed network eth- norb datasets challenging performance network tested entire testing training iteration network receives training samples random order. eth- dataset conﬁgured network extract features category resulted ﬁeld neuron layer covered whole input image. here nine instances category presented network training samples remaining employed test phase. performing training testing iterations best testing performance network reported. structure. examine stdp performance used support vector machines linear kernel knns according results accuracy achieved network maximum potentials used feature vectors classiﬁer knn. considering proposed network classiﬁes input patterns solely based ﬁrst-spike information r-stdp deﬁnitely outperforms stdp. table provides details comparison made r-stdp stdp. looking confusion matrices found r-stdp stdp agree confusing categories horse. however thanks reinforcement learning r-stdp decreased confusion error also provided balanced error distribution. experiment also performed norb dataset. again neuronal grids categories whose neurons able entire incoming stimuli. proposed network r-stdp reached perforples whereas stdp achieved most. reviewing confusion matrices methods found networks encountered diﬃculties mostly distinguishing four-leg animals humans well cars trucks before r-stdp resulted balanced error distribution. additionally compared proposed network convolutional neural networks although proposed network able beat pre-trained deep cnns comparing shallow similar network structure input would fair point. repeated object categorization experiments using shallow implemented keras neural networks tensorﬂow backend. shown table proposed network successfully outperformed supervised eth- norb datasets. overﬁtting common issues supervised reinforcement learning scenarios. problem even worse emergence deep learning algorithms. many studies focused developing techniques increase generalization power learning algorithms. mechanism shown promising empirical results deep neural networks dropout technique technique temporary reduces complexity network suppressing activity speciﬁc number neurons. reduction neuronal resources forces network generalize order reduce prediction error. figure impact dropout adaptive learning rate techniques. plot left demonstrates result eth- dataset. plots solid lines illustrate performance network diﬀerent dropout probabilities adaptive learning rate proposed network exception shown tendencies overﬁt training samples examinations. therefore adopted dropout technique experiments. also found steady learning rate increase chance overﬁtting. thus made dynamic learning rates respect performance network impact mentioned mechanisms categorization test samples. clear adaptive learning rate mechanism applied network achieved higher performances also shown dropout probability must chosen according complexity dataset well network. since norb dataset contains complex samples eth- tends overﬁtting training samples. consequence needs dropout rate overcome issue. magnitude tendency even clearer steady learning rates used. diﬀerently faster convergence rate along complexity samples induce overﬁtting turn needs dropout rate. mammals fast accurate visual object recognition. visual cortex processes incoming data hierarchical manner complexity neuronal preference gradually increased. hierarchical processing provides robust invariant object recognition computational modeling mammalian visual cortex investigation many years. developing biologically plausible model enables scientists examine hypotheses cost also provides human-like vision artiﬁcially intelligent machines show impact aforementioned mechanisms repeated object recognition experiments diﬀerent dropout probabilities steady learning rates. fig. simultaneously shows cortex. despite promising results obtain dcnns biologically plausible using supervised learning rules. addition employ rate-based encoding scheme energy resource consuming. angroup studies trying spiking neurons along unsupervised stdp learning rule models biologically plausible cannot beat dcnns terms accuracy. theory spiking neural networks computational power dcnns however harder control complex dynamics high dimensional space eﬀective parameter. furthermore since trained unsupervised manner classiﬁcation step done external classiﬁer statistical methods. here solved object recognition task using hierarchical equipped reinforcement learning rule called r-stdp several studies showing brain uses solve problem decision-making therefore suitable choice training classspeciﬁc neurons able decide class input image. therefore step further developing biologically plausible model able perform visual categorization totally own. proposed network functions temporal domain information encoded spike times. input image ﬁrst convolved oriented gabor ﬁlters spike train generated based latency-to-intensity coding scheme. resulting spikes propagated toward feature extraction layer. using r-stdp proposed network successfully found task-speciﬁc diagnostic features using neurons pre-assigned class labels. words neuron assigned class priori desired behavior respond early instances belonging speciﬁed class. decrease computational cost even more neurons forced input image latency spike considered measure stimulus preference. therefore neuron ﬁred earlier others would received preferred stimulus. measure experiments compared r-stdp stdp diﬀerent aspects. showed rstdp save computational resources. clariﬁed hand-designed discrimination task order spikes discriminative feature. r-stdp solved problem using minimal number neurons synapses threshold whereas stdp needed neurons synapses higher thresholds. drawback stdp fact tends statistically frequent features necessarily diagnostic ones. consequence needs either neurons synapses ensure diagnostic features eventually found. hand since r-stdp informs neurons outcomes function better using minimal resources. demonstrated advantages rstdp ﬁnding diagnostic features investigated well combined hierarchical solving visual feature extraction object categorization biologically plausible manner. evaluated proposed network similar network uses stdp well structure three datasets natural images caltech face/motorbike eth- norb. last contain images objects diﬀerent viewpoints made task harder. compared performances obtained networks found r-stdp strongly outperforms stdp structure. even interesting point proposed network achieved superiority decisions solely based ﬁrst-spikes case others even powerful classiﬁers like svms error back-propagation help. tuned thresholds compensate blind unsupervised feature extraction achieve better performances again conclude r-stdp helps network eﬃciently consuming computational resources. network interesting neuromorphic engineering since biologically plausible hardware-friendly. although hardware implementation eﬃciency scope current paper believe proposed network implemented hardware energy-eﬃcient manner several reasons. firstly snns hardware friendly classic artiﬁcial neural networks energyconsuming multiply-accumulator units replaced energy-eﬃcient accumulator units. reason studies training deep convolutional snns converting dcnns dcsnns well restricted dcnns gained interests recent years. secondly hardwares eventdriven approaches considering spikes events. energy consumption increases number spikes. thus allowing spike neuron proposed model eﬃcient possible. finally proposed learning rule suitable online on-chip learning error backpropagation deep networks updating weights based high-precision gradients brings diﬃculties hardware implementation. date could works possessing aforementioned features. mention closest attempts gardner tried classify poisson-distributed spike trains readout neuron equipped r-stdp. although method working cannot applied natural images time-based encoding target labeling. another related work huerta nowotny work authors designed model mechanism occurs mushroom body. applied mechanism pool randomly connected neurons readout neurons classify handwritten digits. work diﬀerent several aspects. first used hierarchical structure based mammalian visual cortex used randomly connected neurons. second used r-stdp learning rule whereas employed probabilistic approach synaptic plasticity. third input network natural images using intensity-to-latency encoding used binary encoding threshold artiﬁcial images. although results proposed network signiﬁcantly better network employing stdp external classiﬁers still competitive state-of-the-art deep learning approaches. limitations current method using trainable layer. besides receptive ﬁeld neurons last layer large enough cover informative portion image. result network cannot resist high rates variations object unless using number neurons. extending number layers current network directions future research. going deeper seems improve performance providing gradual simple complex feature extraction. however deeper structure needs parameter tuning suitable multi-layer synaptic plasticity rule. recent studies also shown combining deep networks lead outstanding results kinds animals diﬀerent viewpoints able relating category animal diﬀerent animals reason co-occur. extract features frontal proﬁle face cannot generate association putting general category face. hand reinforcement signal using learning rules like r-stdp neurons able extract diagnostic features also learn relative connections categories create super-categories. authors would like thank jean-pierre jaﬀr´ezou proofreading manuscript well amirreza yousefzadeh bernab´e linaresbarranco providing useful hardware-related information. research received funding european research council european union’s framework program /erc grant agreement iran national science foundation insf institute research fundamental sciences tehran iran.", "year": "2017"}