{"title": "Linking connectivity, dynamics and computations in low-rank recurrent  neural networks", "tag": "q-bio", "abstract": " Large scale neural recordings have established that the transformation of sensory stimuli into motor outputs relies on low-dimensional dynamics at the population level, while individual neurons exhibit complex selectivity. Understanding how low-dimensional computations on mixed, distributed representations emerge from the structure of the recurrent connectivity and inputs to cortical networks is a major challenge. Here, we study a class of recurrent network models in which the connectivity is a sum of a random part and a minimal, low-dimensional structure. We show that, in such networks, the dynamics are low dimensional and can be directly inferred from connectivity using a geometrical approach. We exploit this understanding to determine minimal connectivity required to implement specific computations, and find that the dynamical range and computational capacity quickly increase with the dimensionality of the connectivity structure. This framework produces testable experimental predictions for the relationship between connectivity, low-dimensional dynamics and computational features of recorded neurons. ", "text": "large scale neural recordings established transformation sensory stimuli motor outputs relies low-dimensional dynamics population level individual neurons exhibit complex selectivity. understanding low-dimensional computations mixed distributed representations emerge structure recurrent connectivity inputs cortical networks major challenge. here study class recurrent network models connectivity random part minimal low-dimensional structure. show that networks dynamics dimensional directly inferred connectivity using geometrical approach. exploit understanding determine minimal connectivity required implement speciﬁc computations dynamical range computational capacity quickly increase dimensionality connectivity structure. framework produces testable experimental predictions relationship connectivity low-dimensional dynamics computational features recorded neurons. understanding relationship synaptic connectivity neural activity behavior central endeavor neuroscience. networks neurons encode incoming stimuli terms electrical activity transform information decisions motor actions synaptic interactions thus implementing computations underly behavior. reaching simple mechanistic grasp relation connectivity activity behavior however highly challenging. cortical networks believed constitute fundamental computational units mammalian brain consist thousands neurons highly inter-connected recurrent synapses. even able experimentally record activity every neuron strength synapse behaving animal understanding causal relationships quantities would remain daunting challenge appropriate conceptual framework currently lacking simpliﬁed computational models neural networks provide testbed developing framework. computational models trained artiﬁcial neural networks strengths synapses activity neurons known understanding relation connectivity dynamics input-output computations achieved speciﬁc cases ben-yishai wang popular best-studied classes network models based fully random recurrent connectivity networks display internally generated irregular activity closely resembles spontaneous cortical patterns recorded in-vivo however randomly connected recurrent networks display stereotyped responses external inputs implement limited range inputoutput computations spontaneous dynamics typically high dimensional implement elaborate computations low-dimensional dynamics classical network models rely instead highly structured connectivity every neuron belongs distinct cluster selective feature task amit brunel litwin-kumar doiron actual cortical connectivity appears neither fully random fully structured activity individual neurons displays similar mixture stereotypy disorder take observations account implement general-purpose computations large variety functional approaches developed training recurrent networks designing appropriate connectivity matrices uniﬁed conceptual picture connectivity determines dynamics computations however currently missing remarkably albeit developed independently motivated diﬀerent goals several functional approaches designing connectivity appear reached similar solutions implemented computations determine every single entry connectivity matrix instead rely speciﬁc type minimal low-dimensional structure mathematical terms obtained connectivity matrices rank. classical hopﬁeld networks rank-one term added connectivity matrix every item memorized terms ﬁxes single dimension i.e. row/column combination connectivity matrix. echo-state force learning similarly within neural engineering framework computations implemented feedback loops readout units bulk network. feedback loop mathematically equivalent adding rank-one component ﬁxing single row/column combination otherwise random connectivity matrix. predictive spiking theory requirement information represented eﬃciently leads connectivity matrix similar low-rank form. taken together results studies suggest minimal low-rank structure added random recurrent connectivity provide general unifying framework implementing computations recurrent networks. based observation study class recurrent networks connectivity structured low-rank part random part. show networks spontaneous stimulus-evoked activity low-dimensional predicted geometrical relationship small number high-dimensional vectors represent connectivity structure feed-forward inputs. understanding relationship connectivity network dynamics allows directly design minimal low-rank connectivity structures implement speciﬁc computations. focus four tasks increasing complexity starting basic binary discrimination ending context-dependent evidence integration dynamical repertoire network increases quickly dimensionality connectivity structure rank-two connectivity structures already suﬃcient implement complex context-dependent tasks task illustrate relationship connectivity low-dimensional dynamics performed computation. particular framework naturally captures ubiquitous observation single-neuron responses highly heterogeneous mixed dimensionality dynamics underlying computations increases task complexity crucially task framework produces experimentally testable predictions directly relate connectivity dominant dimensions dynamics computational features individual neurons. studied class models call low-rank recurrent networks. networks connectivity matrix given uncontrolled random matrix structured controlled matrix structured matrix rank i.e. consisted small number independent rows columns entries assumed weak considered moreover ﬁxed known uncorrelated random part considered unknown except statistics classical models networks consisted ﬁring rate units sigmoid input-output transfer function connect previous literature introduce methods underlie results start describing spontaneous dynamics network unit-rank structure turn response external inputs core results exploit demonstrate low-rank networks implement four tasks increasing complexity. one-dimensional spontaneous activity networks unit-rank structure started simplest possible type low-dimensional connectivity matrix unit-rank matrix speciﬁed n-dimensional vectors {mi} {nj} fully determine entries. every column matrix multiple vector every multiple vector individual entries given call respectively rightleft-connectivity vectors consider arbitrary ﬁxed uncorrelated random part connectivity. show spontaneous network dynamics directly understood geometrical arrangement vectors absence structured connectivity dynamics determined strength random connectivity activity absence inputs decays zero displays strong chaotic ﬂuctuations ﬁrst understand interplay ﬁxed low-rank part random part connectivity shapes spontaneous activity network. analysis network dynamics relies eﬀective statistical description mathematically derived network large low-dimensional part connectivity weak assumptions activity unit figure spontaneous activity random networks unit-rank connectivity structure. recurrent network model whose connectivity matrix consists random structured unitrank component. left dynamical regimes network activity function structure connectivity strength random strength gray areas bistable activity; chaotic activity. side panels samples dynamics ﬁnite networks simulations c-d. activity statistics random strength increased structure strength ﬁxed activity along vector quantiﬁed blue lines theoretical prediction stationary dynamics. activity variance random connectivity. blue pink lines static heterogeneity temporal variance quantiﬁes chaotic activity. dots simulations ﬁnite-size networks. methods details. described terms mean variance total input receives. dynamical equations quantities derived extending classical dynamical mean-ﬁeld theory theory eﬀectively leads low-dimensional description network dynamics terms equations couple macroscopic quantities. full details analysis provided methods; here focus main results. averaged diﬀerent realizations random component connectivity depends implicitly overlap therefore quantiﬁes degree structure along vector activity network. equilibrium activity neuron correlated corresponding component vector implies structure present. overlap macroscopic quantity describing network dynamics theory provides equations specifying dependence network parameters. represents network activity point n−dimensional state-space every dimension corresponds activity single unit shows structured part connectivity induces one-dimensional organization spontaneous activity along vector one-dimensional organization however emerges overlap vanish. activity network organized along vector quantiﬁes projection activity onto vector non-vanishing values require mjnj/n directly quantiﬁes strength structure connectivity. connectivity structure strength activity structure strength therefore directly related highly non-linear manner. connectivity structure weak network exhibits homogeneous unstructured activity corresponding connectivity structure strong structured heterogeneous activity emerges activity network equilibrium organized dimension along vector random connectivity induces additional heterogeneity along remaining directions. note that symmetry speciﬁc input-output function heterogeneous equilibrium state exists conﬁguration opposite sign equilibrium state network activity bistable random part connectivity disrupts organization activity induced connectivity structure diﬀerent eﬀects. ﬁrst eﬀect random strength increased given realization random part connectivity total input unit deviate strongly expected mean consequence activity along directions orthogonal increases resulting noisy input individual neurons smoothens gain non-linearity. eﬀectively leads reduction overall structure activity quantiﬁed second distinct eﬀect increasing random strength eventually leads chaotic activity purely random networks. depending strength structured connectivity diﬀerent types chaotic dynamics emerge. disorder connectivity much stronger structure overlap zero result mean activity units vanishes dynamics consist unstructured n−dimensional temporal ﬂuctuations classical chaotic state fully random networks contrast strengths random structured connectivity comparable structured type chaotic activity emerges mean activity diﬀerent units organized dimension along direction shown activity diﬀerent units ﬂuctuates time structured static activity situation system bistable states opposite signs always exist. phase diagram fig. summarizes diﬀerent types spontaneous dynamics emerge function strength structured random components connectivity matrix. altogether structured component connectivity favors one-dimensional organization network activity random component favors high-dimensional chaotic ﬂuctuations. particularly interesting activity emerges structure disorder comparable case dynamics show one-dimensional structure combined high-dimensional temporal ﬂuctuations give rise dynamics slow timescales two-dimensional activity response external input turn response external feed-forward input equilibrium total average input unit recurrent input feed-forward input fig. illustrates response network step input. response individual units highly heterogeneous diﬀerent units showing increasing decreasing multi-phasic responses. every unit responds diﬀerently theory predicts that level n-dimensional state space representing activity whole population trajectory activity lies average two-dimensional plane spanned right-connectivity vector vector {ii} corresponds pattern external inputs applying simulated activity dimensionality reduction technique figure external inputs generate two-dimensional activity random networks unit-rank structure. pattern external inputs represented n-dimensional vector {ii} input unit transient dynamics response step input along sample network units. left activity traces units. right projections population trajectory onto plane deﬁned right-connectivity vector input vector light trace theoretical prediction. dark traces simulations. principal components analysis average activity trajectory. bottom fraction standard deviation explained successive pcs. correlation vectors direction projections onto plane represented also fig. activity along determined geometrical arrangement vector connectivity vectors three diﬀerent cases illustrated mutually orthogonal; mutually orthogonal non-zero overlap non-zero overlap leading bistable activity absence inputs. increasing external input along suppresses stable states. continuous lines theoretical predictions. dots simulations. methods details. recent review) principal components analysis conﬁrms dominant dimensions activity indeed plane random part connectivity leads additional activity remaining directions grows quickly strength random connectivity approach therefore directly links connectivity network emerging low-dimensional dynamics shows dominant dimensions activity determined combination feed-forward inputs connectivity contribution connectivity vector two-dimensional trajectory activity quantiﬁed overlap network activity left-connectivity vector activity trajectory one-dimensional simply propagates pattern feed-forward inputs. particular case fully random networks. network response instead non-trivial two-dimensional combination input connectivity structure patterns. general value therefore organization network activity depends geometric arrangement input vector respect connectivity vectors well strength random component connectivity neural activity lies predominantly plane non-vanishing together non-trivial two-dimensional activity obtained vector non-zero component plane. qualitatively diﬀerent input-output regimes distinguished. ﬁrst obtained connectivity vectors orthogonal case overlap zero spontaneous activity network bears sign underlying connectivity structure. adding external input however reveal connectivity structure generate non-trivial two-dimensional activity input vector non-zero overlap left-connectivity vector situation vector picks component activity along feed-forward input direction leads non-zero overlap turn implies network activity component along rightconnectivity vector increasing external input along direction therefore progressively increase response along leading two-dimensional output. second qualitatively diﬀerent input-output regime obtained connectivity vectors strong enough overlap along common direction already shown fig. overlap larger unity induces bistable structured spontaneous activity along dimension adding external input along vector increases activity along also eventually suppresses bistable states. large external inputs along direction therefore reliably network state activity two-dimensional combination input direction connectivity direction lead strongly non-linear input-output transformation network initially state lies opposite branch additional eﬀect external input generally tends suppress chaotic activity present random part connectivity strong suppression occurs irrespectively speciﬁc geometrical conﬁguration input connectivity vectors therefore independently input-output regimes described above. altogether external inputs suppress chaotic bistable dynamics therefore always decrease amount variability dynamics summary external feed-forward inputs network unit-rank connectivity structure general lead two-dimensional trajectories activity. elicited trajectory depends geometrical arrangement pattern inputs respect connectivity vectors play diﬀerent roles. rightconnectivity vector determines output pattern network activity left-connectivity vector instead selects inputs give rise outputs along output structured along obtained selects recurrent inputs selects external inputs higher-rank structure leads rich dynamical repertoire focused unit-rank connectivity structure framework directly extended higher rank structure. general structured component rank written superposition independent unit-rank terms principle characterized vectors network average dynamics -dimensional subspace spanned right-connectivity vectors input vector left connectivity vectors select inputs ampliﬁed along corresponding dimension details dynamics general depend geometrical arrangement vectors among respect input pattern. number possible conﬁgurations increases quickly structure rank leading wide repertoire dynamical states includes continuous attractors sustained oscillatory activity remainder manuscript explore rank-two case. implementing simple discrimination task developed intuitive geometric understanding given unit-rank connectivity structure determines low-dimensional dynamics network reverse approach given computation figure implementing simple go-nogo discrimination task unit-rank connectivity structure. linear readout added network randomly chosen weights stimuli represented random input patterns task consists producing output response stimulus simplest unit-rank structure implements task given response sample network nogo inputs. activity traces units. projections population trajectories onto planes predicted contain dominant part dynamics. gray predicted trajectory. colored traces simulations. linear regression coeﬃcients nogo stimuli. every corresponds network unit. readout dynamics nogo stimulus. average connectivity strength function product coeﬃcients ﬁrst every corresponds pair units. generalization properties network. select stimuli build input pattern normalized mixture preferred continuous lines theoretical predictions. dots patterns gradually increase component along simulations. methods details. implemented choosing appropriately structured part connectivity. start computation underlying basic common behavioral tasks go-nogo stimulus discrimination. task animal produce speciﬁc motor output e.g. press lever lick spout response stimulus ignore another stimuli computation implemented straightforward recurrent network unit-rank connectivity structure. simple computation principle require recurrent network implementation describe illustrates transparent manner relationship connectivity dynamics computations low-rank networks leads non-trivial directly testable experimental predictions. also provides basic building block complex tasks turn next sections. model sensory stimuli random patterns external inputs network stimuli represented ﬁxed randomly-chosen n-dimensional vectors model motor response supplement network output unit produces linear readout wiφ) network activity readout weights chosen randomly form also ﬁxed n-dimensional vector task network produce output selective stimulus readout stimulus presentation needs non-zero input pattern corresponds stimulus zero input n-dimensional vectors generate appropriate unit-rank connectivity structure implement task directly determined description network dynamics. shown fig. response network input pattern general two-dimensional lies plane spanned vectors output unit therefore produce non-zero readout readout vector non-vanishing overlap either assumed uncorrelated therefore orthogonal input patterns implies connectivity vector needs non-zero overlap readout vector network produce non-trivial output. output depend amount activity along quantiﬁed overlap shown fig. overlap non-zero non-vanishing overlap input pattern. altogether implementing go-nogo task therefore requires right-connectivity vector correlated readout vector left-connectivity vector correlated stimulus choosing therefore provides simplest unit-rank connectivity implements desired computation. fig. illustrates activity corresponding network. level individual units construction stimuli elicit large heterogeneous responses display mixed selectivity predicted theory response stimulus dominantly one-dimensional organized along input direction response stimulus two-dimensional lies plane deﬁned right-connectivity vector input direction readout network corresponds projection activity onto direction non-zero response stimulus network indeed implements desired go-nogo task. framework therefore allows directly link connectivity low-dimensional dynamics computation performed network leads experimentally testable predictions. ﬁrst performing dimensionality-reduction separately responses stimuli lead larger dimensionality trajectories response stimulus. second prediction stimulus dominant directions activity depend recurrent connectivity network nogo stimulus not. speciﬁcally activity elicited stimulus dominant principal components combinations input vector right-connectivity vector therefore neurons large principal component weights expected also large weights therefore stronger mutual connections average contrast activity elicited nogo stimulus dominant principal components determined solely feed-forward input correlation dominant weights recurrent connectivity expected prediction principle directly tested experiments analogous calcium imaging behaving animals combined measurements connectivity subset recorded neurons. note setup weak structured connectivity suﬃcient implement computations expected correlations weak random part connectivity strong unit-rank connectivity structure forms fundamental scaﬀold desired input-output transform. random part connectivity adds variability around target output induce additional chaotic ﬂuctuations. summing activity individual units readout unit however averages present heterogeneity readout error decreases network size implementation therefore robust noise desirable computational properties terms generalization novel stimuli. particular extended straightforward detection category stimuli rather single stimulus detection noisy stimulus turn slightly complex task integration continuous noisy stimulus. contrast previous discrimination task stimuli completely diﬀerent consider continuum stimuli diﬀer along intensity single feature coherence random-dot kinetogram given stimulus presentation feature moreover ﬂuctuates time. therefore represent stimulus ﬁxed randomly chosen input vector encodes figure implementing noisy detection task unit-rank connectivity structure. network given noisy input along ﬁxed random pattern inputs task consists producing output average input larger threshold dynamics sample network. noisy input threshold. bottom activity traces four units diﬀerent noise realizations stimulus leading nogo output. readout dynamics stimuli. projections population trajectory onto plane deﬁned right-connectivity vector input vector left single-trial trajectories corresponding right trial-averaged trajectories nogo outputs diﬀerent values mean input stars indicate correct responses. left linear regression coeﬃcients input amplitude decision outcome. every corresponds network unit. right correlation coeﬃcients vectors input choice regression axes projection directions input choice regression axes onto plane shown detection threshold time scale eﬀective exponential ﬁlter increasing values structure strength. psychometric curve. shaded area indicates bistable region. average connectivity strength function product linear regression coeﬃcients choice variable. every corresponds pair network units. methods details. basic discrimination task central requirements unit-rank network implement task right-connectivity vector correlated readout vector left-connectivity vector correlated input pattern novel requirement present task however response needs non-linear produce output strength input along larger threshold. shown fig. non-linearity obtained leftright-connectivity vectors strong enough overlap. therefore shared component along direction orthogonal setup stimulus intensity network bistable regime activity along direction take distinct values input assuming lower state represents nogo output network initialized state beginning trial increasing stimulus intensity threshold lead sudden jump therefore non-linear detection stimulus. input amplitude ﬂuctuates noisily time whether jump occurs depends integrated estimate stimulus intensity. timescale estimate integrated determined time-constant eﬀective exponential ﬁlter describing network dynamics. unit-rank network time-constant connectivity strength i.e. overlap leftright-connectivity vectors also determines value threshold. arbitrarily large timescales obtained adjusting overlap close bifurcation value case threshold becomes arbitrarily small section structure strength threshold corresponds integration timescale order time constant individual units. fig. illustrates activity example implementation network. given trial stimulus noisy activity individual units ﬂuctuates strongly theory predicts population trajectory average lies plane deﬁned connectivity vector input pattern activity along direction picked readout value stimulus presentation determines output bistable dynamics network whether direction explored output produced depends speciﬁc noisy realization stimulus. stimuli identical average strength therefore either lead two-dimensional trajectories activity responses one-dimensional trajectories activity corresponding nogo responses probability generating output function stimulus strength follows sigmoidal psychometric curve reﬂects underlying bistability note bistability clearly apparent level individual units. particular activity individual units always saturation inputs distributed along zero-centered gaussian responses individual units strongly heterogeneous exhibit mixed selectivity stimulus strength output choice popular manner interpret activity population level targeted dimensional reduction approach input choice dimensions determined regression analyses expected theoretical analysis dimensions obtained regression closely related particular choice dimension highly correlated right-connectivity vector result plane network activity dominantly lies corresponds plane deﬁned choice input dimensions framework therefore directly links recurrent connectivity eﬀective output choice direction low-dimensional dynamics. resulting experimentally testable prediction neurons strong choice regressors stronger mutual connections context-dependent discrimination task next consider context-dependent discrimination task relevant response stimulus depends additional explicit contextual cue. speciﬁcally focus task studied saez context stimulus requires output stimulus nogo context associations reversed task direct extension basic binary discrimination task introduced fig. signiﬁcantly complex represents hallmark cognitive ﬂexibility non-linearly separable xor-like computation single-layer feed-forward network cannot solve show task implemented rank-two recurrent network direct extension unit-rank network used discrimination task fig. context-dependent task seen combination basic opposite go-nogo discriminations independently implemented unit-rank structure right-connectivity vector correlated readout left-connectivity vector correlated input combining unit-rank structures left-connectivity vectors correlated figure implementing context-dependent go-nogo discrimination task rank-two connectivity structure. fig. stimuli presented network. task consists producing output response stimulus determined contextual modeled inputs along random directions ictxa ictxb. inputs along overlap direction leftright-connectivity vectors modulate response threshold network dynamics sample network response stimulus stimulus contextual input. bottom activity units contexts readout dynamics contexts. projections average population trajectories onto planes spanned vectors network performance contexts. average connectivity strength pairs units function product regression coeﬃcients context. every corresponds pair network units. methods details. respectively leads rank-two connectivity structure serves scaﬀold present task. cues context represented additional inputs along random vectors ictxa ictxb presented full length trial inputs contextual information incorporated network. particular readout vector ﬁxed independent context crucially since readout needs produce output input stimuli right-connectivity vectors need correlated requirement implementing context-dependent discrimination contextual input eﬀectively switches irrelevant association. implement requirement rely non-linearity noisy discrimination task based overlap leftright-connectivity vectors however exploit additional property threshold non-linearity controlled additional modulatory input along overlap direction modulatory input acts eﬀective oﬀset bistability macroscopic population level stimulus given strength therefore induce transition lower upper state transition depending strength modulatory input sets threshold value. noisy discrimination task overlap chosen arbitrary direction present setting take overlaps pair leftright-connectivity vectors along direction corresponding contextual input overlap along ictxa along ictxb) contextual inputs directly modulate threshold non-linearity. ﬁnal rank-two setup described detail methods. fig. illustrates activity example resulting network implementation. contextual present beginning trial eﬀectively sets network context-dependent initial state corresponds lower bistable states. low-dimensional response network following stimulus determined initial state sustained contextual input. context present stimulus leads crossing non-linearity transition lower upper state therefore two-dimensional response plane determined generating output contrast context present threshold underlying non-linearity increased direction input presentation stimulus induce transition lower upper states leads one-dimensional trajectory orthogonal readout therefore nogo response situation totally symmetric response stimulus contextual cues fully reverse stimulus-response associations overall context-dependent discrimination relies strongly non-linear interactions stimulus contextual inputs connectivity level implemented overlaps connectivity vectors along contextual inputs. central experimentally testable prediction framework therefore that network implementing computation units strong contextual selectivity average stronger mutual connections context-dependent evidence integration task ﬁnally examine task inspired mante combines context-dependent output ﬂuctuating noisy inputs. stimuli consist superpositions diﬀerent features strengths features ﬂuctuate time given trial. mante stimuli random kinetograms features corresponded direction motion color stimuli. task consists classifying stimuli according features relevant indicated explicit contextual implemented go-nogo version task output required non-zero relevant feature stronger prescribed threshold present task therefore direct combination detection task introduced fig. context-dependent discrimination task fig. individual stimuli two-dimensional consist independently varied features task signiﬁcant additional diﬃculty every trial irrelevant feature needs ignored even stronger relevant feature context-dependent evidence integration task implemented exactly rank-two conﬁguration basic context-dependent discrimination fig. contextual gating relying non-linear mechanism fig. contextual presented throughout trial determines features two-dimensional stimulus leads non-linear dynamics along direction connectivity vectors directions share common component along readout vector readout unit picks activity along dimension. consequence depending contextual stimulus lead opposite outputs altogether context output independent values feature conversely context output therefore behaves based orthogonal readout directions readout direction unique ﬁxed output relies instead context-dependent selection relevant input feature important additional requirement present task respect basic context-dependent integration network needs perform temporal integration average temporal ﬂuctuations stimulus. illustrated fig. network dynamics response stimuli indeed exhibit slow timescale progressively integrate input. strikingly slow dynamics require additional constraints network connectivity; direct consequence rank-two connectivity structure used figure implementing context-dependent evidence accumulation task using rank-two connectivity structure. stimuli consist superposition features ﬂuctuate time around mean values ¯cb. every trial pair contextual inputs determines relevant input feature. task consists producing output average strength relevant feature larger threshold. dynamics sample network. stimulus contextual inputs. bottom activity four units contexts readout dynamics contexts. average population trajectories projected onto planes spanned vectors blue trajectories sorted according value strength stimulus averaged across stimulus network performance. probability response function input strengths bottom probability response averaged ¯cb. continuous line theoretical prediction; dots simulations. projection population activity onto plane deﬁned orthogonal components vectors comparison underlying circular attractor trajectories sorted strength relevant stimulus averaged across non-relevant one. direction projections regression axes choice context indicated gray. methods details. contextual gating speciﬁcally symmetry contexts implies sets leftrightconnectivity vectors identical overlaps without constraints connectivity symmetric conﬁguration leads emergence continuous line attractor shape two-dimensional ring plane deﬁned implementation present task symmetric overlaps four connectivity vectors include common direction along readout vector. additional constraint eliminates ring attractor stabilizes equilibrium states correspond nogo outputs. ring attractor close parameter space proximity induces slow manifold dynamics trajectories leading output slowly evolve along diﬀerent sides underlying ring depending context result directions plane correspond choice context axis found regression analysis similiar mechanism context-dependent evidence integration based line attractor previously identiﬁed reverse-engineering trained recurrent network whether underlying dynamical structure ring case line attractors contexts depended details network training protocol show mechanism based ring attractor implemented minimal network rank-two connectivity structure solutions certainly found. note rank-two network also serve alternative implementation context-independent evidence integration integration timescale threshold value fully independent contrast unit-rank implementation motivated observation variety approaches implementing computations recurrent networks rely common type connectivity structure studied class models connectivity matrix consists ﬁxed low-rank term random part. central result low-rank connectivity structure induces low-dimensional dynamics network hallmark population activity recorded behaving animals low-dimensional activity usually detected numerically using dimensional-reduction techniques showed mean-ﬁeld theory allows directly predict low-dimensional dynamics based connectivity input structure. approach simple geometrical understanding relationship connectivity dynamics enabled design minimal-connectivity implementations speciﬁc computations. particular found dynamical repertoire network increases quickly rank connectivity structure ranktwo networks already implement variety computations. study explicitly considered structures rank higher theoretical framework principle valid arbitrary rank size network. works examined dynamics networks mixture structured random connectivity ahmadian classical approach implementing computations recurrent networks endow clustered distance-dependent connectivity networks inherently display low-dimensional dynamics similar framework clustered connectivity fact special case low-rank connectivity. clustered connectivity however highly ordered neuron belongs single cluster therefore selective single task feature neurons clustered networks therefore highly specialized display pure selectivity here instead considered random low-rank structures generate activity organized along heterogeneous directions state space. consequence stimuli outputs represented random highly distributed manner individual neurons typically responsive several stimuli outputs combinations two. mixed selectivity ubiquitous property cortical neurons confers additional computational properties networks particular allowed easily extend context-dependent situation network implementation basic discrimination task. typically diﬃcult clustered purely selective networks type connectivity used study closely related classical framework hopﬁeld networks hopﬁeld networks store memory speciﬁc patterns activity creating pattern corresponding ﬁxed-point network dynamics. achieved adding unit-rank term item approach investigating capacity setup relied mean-ﬁeld theory network connectivity consists rank-one term random matrix approach clearly close adopted present study important diﬀerences. within hopﬁeld networks unit-rank terms symmetric corresponding leftright-connectivity vectors identical pattern. moreover unit-rank terms correspond diﬀerent patterns generally uncorrelated. contrast considered general case leftright-eigenvectors diﬀerent potentially correlated diﬀerent rank-one terms. importantly main focus responses external inputs input-output computations rather memorizing items. particular showed leftright-connectivity vectors play diﬀerent roles respect processing inputs left-connectivity vector implementing inputselection right-connectivity vector determining output network. study also directly related echo-state networks force learning frameworks randomly connected recurrent networks trained produce speciﬁed outputs using feedback loop readout unit network mathematically equivalent adding rank-one term random connectivity matrix basic implementation force learning train readout weights. training performed ﬁxed speciﬁed realization random connectivity ﬁnal rank-one structure correlated random part connectivity strong respect contrast results presented rely assumption low-rank structure weak independent random part. although force networks necessarily fulﬁll assumption ongoing work found approach describes well networks trained using force produce constant output note framework computations rely solely structured part connectivity ongoing work suggests random part connectivity play important role training. speciﬁc network model used identical studies based trained recurrent networks highly simpliﬁed lacks many biophysical constraints basic ones positive ﬁring rates segregation excitation inhibition interactions spikes. recent works investigated extensions abstract model used networks biophysical constraints additional work needed implement present framework networks spiking neurons. results imply novel directly testable experimental predictions relating connectivity low-dimensional dynamics computational properties individual neurons. main result dominant components low-dimensional dynamics combination feed-forward input patterns vectors specifying lowrank recurrent connectivity direct implication that low-dimensional dynamics network generated low-rank recurrent connectivity neurons large loadings dominant principal components tend mutual connections stronger average contrast low-dimensional dynamics generated recurrent interactions instead driven feed-forward inputs alone correlation principal components connectivity expected since low-dimensional dynamics based recurrent connectivity form scaﬀold computations model basic prediction extended various task-dependent properties individual neurons. instance recurrent connectivity implements evidence integration units strong choice regressors predicted mutual connections stronger average analogously recurrent connections implement context-dependent associations units strong context regressors expected share connections stronger average predictions principle directly tested experiments combine calcium imaging neural activity behaving animals measurements connectivity subset recorded neurons noted however weak structured connectivity suﬃcient implement computations expected correlations connectivity various selectivity indices weak. class recurrent networks considered based connectivity matrices consist explicit low-rank random part. seem limited class models fact arbitrary matrix approximated low-rank e.g. keeping small number dominant singular values singular vectors basic principle underlying dimensionality reduction. recurrent network arbitrary connectivity matrix therefore principle approximated low-rank recurrent network. point view theory suggests simple conjecture lowdimensional structure connectivity determines low-dimensional dynamics computational properties recurrent networks. work needed establish precise conditions low-rank network provides good computational approximation full recurrent network conjecture provides simple practically useful working hypothesis reverse-engineering trained neural networks relating connectivity dynamics computations neural recordings. grateful alexis dubreuil vincent hakim kishore kuchibhotla discussions feedback manuscript. work funded programme emergences city paris program investissements d’avenir launched french government implemented references anr--labx- anr--idex-- psl* research university. funders role study design data collection analysis decision publish preparation manuscript.", "year": "2017"}