{"title": "An Algorithmic Information Calculus for Causal Discovery and  Reprogramming Systems", "tag": "q-bio", "abstract": " We demonstrate that the algorithmic information content of a system is deeply connected to its potential dynamics, thus affording an avenue for moving systems in the information-theoretic space and controlling them in the phase space. To this end we performed experiments and validated the results on (1) a very large set of small graphs, (2) a number of larger networks with different topologies, and (3) biological networks from a widely studied and validated genetic network (e.coli) as well as on a significant number of differentiating (Th17) and differentiated human cells from high quality databases (Harvard's CellNet) with results conforming to experimentally validated biological data. Based on these results we introduce a conceptual framework, a model-based interventional calculus and a reprogrammability measure with which to steer, manipulate, and reconstruct the dynamics of non- linear dynamical systems from partial and disordered observations. The method consists in finding and applying a series of controlled interventions to a dynamical system to estimate how its algorithmic information content is affected when every one of its elements are perturbed. The approach represents an alternative to numerical simulation and statistical approaches for inferring causal mechanistic/generative models and finding first principles. We demonstrate the framework's capabilities by reconstructing the phase space of some discrete dynamical systems (cellular automata) as case study and reconstructing their generating rules. We thus advance tools for reprogramming artificial and living systems without full knowledge or access to the system's actual kinetic equations or probability distributions yielding a suite of universal and parameter-free algorithms of wide applicability ranging from causation, dimension reduction, feature selection and model generation. ", "text": "department computer science university oxford oxford science life laboratory solna sweden algorithmic nature group labores natural digital sciences paris france. biological environmental sciences engineering division computer electrical mathematical sciences engineering division king abdullah university science technology thuwal kingdom saudi arabia uthor contributions responsible general design conception. responsible data acquisition. contributed data analysis. developed methodology contributions undertook numerical experiments contributing. contributed literature-­‐based enrichment analysis. wrote article contributions correspondence addressed hector.zenilalgorithmicnaturelab.org jesper.tegnerkaust.edu.sa online algorithmic complexity calculator implements perturbation analysis method introduced paper http//complexitycalculator.com/ online animated video explains basic concepts motivations general audience https//youtu.be/ufzqptvli demonstrate algorithmic information content system deeply connected potential dynamics thus affording avenue moving systems information-­‐theoretic space controlling phase space. performed experiments validated results large small graphs number larger networks different topologies biological networks widely studied validated genetic network well significant number differentiating differentiated human cells high quality databases results conforming experimentally validated biological data. based results interventional calculus reprogrammability measure steer manipulate reconstruct dynamics non-­‐ linear dynamical systems partial disordered observations. method consists finding applying series controlled interventions dynamical system estimate algorithmic information content affected every elements perturbed. approach represents alternative numerical simulation statistical approaches inferring causal mechanistic/generative models finding first principles. demonstrate framework’s capabilities reconstructing phase space discrete dynamical systems case study reconstructing generating rules. thus advance tools reprogramming artificial living systems without full knowledge access system’s actual kinetic equations probability distributions yielding suite universal parameter-­‐free algorithms wide applicability ranging causation dimension reduction feature selection model generation. make optimal predictions behaviour dynamical complex systems fundamental problem science. remains challenge understand ultimately reprogram behaviour systems access partial knowledge incomplete noisy data. based established knowledge drawn mathematical theories computability algorithmic probability describe limits optimal characterization algorithmic inference introduce conceptual framework specific methods applications demonstrate advantage powerful calculus based change system’s algorithmic content time subject perturbations. theory algorithmic information provides definition constitutes cause real discrete dynamical systems. formally algorithmic complexity string given min{|p| program produces halts running universal turing machine input empty represented simply length description generating mechanism. object referred random thus non-­‐causal algorithmic complexity length i.e. generating mechanism print function. algorithmic complexity intrinsic randomness object generalization statistical randomness refinement concept shannon entropy depend choice probability distribution. moreover proven mathematically robust randomness lack universal computational power test every possible non-­‐random feature. also seen measure compressibility compression algorithms fact entropy rate estimators thus behave exactly like shannon entropy despite generalized estimators function takes length bits length shortest program generates lower semi-­‐computable means approximated above. proper introductions areas finite algorithmic complexity applications provided introductions algorithmic randomness found robust hypothesis available observable data. algorithmic probability establishes shows consensus several algorithmically likely solutions likely one. chief advantage algorithmic indices causal signals sequence escape entropic measures contain statistical regularities escape metric turing machine capturing every statistical also algorithmic aspect compresses produces full less information actually family probabilities depends enumeration programs reference universal turing machine optimal choices exist thanks invariance-­‐type theorems algorithmic probability sequence probability produced computer program constructed flipping coin running reference universal turing machine divided halting probability. formally lossless compression traditionally used estimate algorithmic content object algorithmic complexity sequence defined length shortest compressed file producing decompressing lossless compression approximation algorithmic complexity actual implementations lossless compression algorithms based purely upon entropy rate thus deal statistical regularities window length size hence closely related algorithmic complexity entropy itself. entropy entropy rate however sufficiently sensitive inherently invariant vis-­‐a-­‐vis object description. however constitutes true algorithmic approach numerically estimating algorithmic coding theorem formally relating seminal measures follows coding theorem method rooted relation i.e. frequency production sequence algorithmic probability. unlike computable measures shannon entropy potential identify regularities merely statistical shortest program even sequences high entropy statistical regularities random algorithmic complexity thus causal result evolving computer program. previously demonstrated exhaustive search carried small-­‐enough number turing machines halting problem known thanks busy beaver game. strategy minimize impact choice average across large different turing machines size. space n-­‐state m-­‐symbol turing machines then function assigned every finite binary sequence standard turing machine defined busy beaver problem. remark thus said semi-­‐measure probability measure reach non-­‐halting machines. using relation established coding theorem measure complexity heavily reliant upon used throughout paper therefore defined follows thus upper bound estimation algorithmic complexity. small values computable whereas larger objects estimation based informed cutoff runtime based theoretical numerical grounds asymptotically capturing halting turing machines polynomial time computationally expensive algorithmic complexity short sequences thus estimated method. approximate complexity longer sequence therefore necessary aggregate various computer programs generate string clever fashion taking advantage shannon entropy. hybrid measure thus calculates local algorithmic complexity global shannon entropy time. formally string finite sequence follows multiplicity subsequence decomposition subsequences length possible sequence remainder length multiple decomposition length parameter runs handle; overlapping parameter deal boundary conditions boundary conditions studied shown errors boundary conditions convergent vanish asymptotically lower bounded shannon entropy upper bounded algorithmic complexity thereby providing local estimations algorithmic complexity global estimations entropy. graph algorithmic probability upper bounds graph randomness shown measures robust vis-­‐à-­‐vis object description algorithmic probability algorithmic complexity constant term. adjacency matrix thus taken lossless description network invariant look algorithmic probability matrix produced chance computer program working grid algorithmic complexity graph defined follows adjacency matrix automorphism group i.e. graphs isomorphic itself. algorithmic complexity graph min{k)|a adjacency matrices aut. since computable lossless description safely write proven isomorphic graphs similar algorithmic information content. also shown numerical approximations graphs large automorphism groups similar algorithmic complexity graphs small automorphism groups high complexity thereby establishing numerical relationship algebraic complexity group symmetry algorithmic complexity approximated results conforming theoretical expectations. call approximation following algorithmic complexity approximation graph defined composed pairs element decomposition square sub-­‐arrays equal dimension multiplicity submatrix obtained using dimensional turing machines calculation introduced words often random turing machine produce adjacency matrix algorithmic complexity introduces dimension capturing causal content graph separating random-­‐looking graphs algorithmic random graphs clearly graphs maximal algorithmic-­‐random graphs. consider binary pseudo-­‐random number generator seed graph size edge density edges. every edge {e...ern/} connected node {v...vn} disconnected clearly maximally algorithmic-­‐random recursively generated seed core causal calculus estimation specific sequence events— form perturbations—that change fate function system thereby ranking perturbations system’s elements effects exert whole system. demonstrate manipulating reprogramming systems algorithmic-­‐ information space runs parallel dynamic state/attractor space network system moved along optimized paths different directions. based universal principles powerful theory induction inference sense optimal drawing recent numerical advances produce estimations introduce suite parameter-­‐free algorithms inherited property great power tackle challenge causal discovery find reveal generating mechanisms behind observations effectively control dynamics general non-­‐linear systems removed limitations classical statistical tools without knowing kinetic equations require expensive numerical simulations arbitrarily assuming non-­‐linearity requiring access probability distributions. algorithmic causal calculus based chiefly upon evaluating algorithmic information dynamical objects strings networks changing time. non-­‐random binary file containing string binary short computer program generates written terms already account string except perturbed thus account for. candidate program =print times; print; print times. clearly length binary computer program generating upper bounded |ps’| length bits computer program generating words assuming shortest computer programs single-­‐bit mutation |ps| |ps’| lengths shortest computer programs generating algorithmically incompressible. means generating mechanism shorter string generated computer program form print much shorter itself. result negating print. possible relations single perturbation either depending moving towards away respect however perturbing single cannot result less random bitwisen) used reverse random bitwise fixed small length contribute already high algorithmic complexity/randomness original random string contrary original assumption algorithmically random also illustrated fig. means perturbations algorithmically random object lower impact generating mechanism perturbations non-­‐random objects-­‐-­‐-­‐ respect generating mechanisms unperturbed states-­‐-­‐-­‐and thus effect exploited estimate infer causal content causal non-­‐causal objects. algorithmically random object change goes unnoticed perturbation lead dramatic change algorithmic content. real-­‐world cases move intermediate region determinism randomness power-­‐set cardinality |s’| length evaluation represents length possible generating mechanism accounting perturbation intervention past future evolution string dynamic object changing s’n. models trajectories advantage highlighting principles system network organized uncovering candidate mechanisms grow develop. causal calculus consists thus studying algorithmic-­‐information dynamic properties objects view constructing algorithmic-­‐information landscape identify rank elements algorithmic contribution changes exert original object moving towards away randomness. follows demonstrate insights gleaned algorithmic information landscape effectively used find unveil dynamics reprogramming capabilities systems starting reconstruction space-­‐time dynamics initial boundary conditions helping infer generating mechanism evolving system observations explore potential calculus characterize genes regulatory networks reprogram systems general including specific examples theoretical experimental focusing synthetic biological data. date alternatives applying non-­‐linear interventions complex systems phase space make strong assumptions perform simulations possible dynamical trajectories often requiring unavailable computing resources. calculus however requires much less information makes significantly less assumptions produce collection guiding causal interventions desired even rough dynamical trajectories promises wide range applications. practice estimating algorithmic complexity deterministic system characterized lower semi-­‐computable meaning approximated above. recent numerical advances—alternatives lossless compression algorithms—have however estimations based seminal concept algorithmic probability goes beyond methods lossless compression better equipped tackle causation estimation entails finding algorithmic models reproducing aggregated produce candidate generative models novel methods found wide range applications areas cognition protein folding logic circuit design mention among several others. take advantage fundamental numerical advances tackle problem related finding mechanisms underlying design control systems particular biological first explore power calculus simpler objects discrete dynamical systems cellular automata. table algorithmic information dynamics string pushed towards away randomness digit removal estimated algorithmic probability initial string consists simple segment followed random-­‐looking segment digits. resulting strings mostly extract respective segments. able neutral digit deletion maximizes preservation elements contributing algorithmic information content original string thus important features applied string removal digits. ables illustrate three kinds algorithmic shifts calculus identifying elements move object towards away randomness deliver. concept core causal algorithmic calculus applied example strings moving sophisticated objects networks. fig. demonstrates calculus applied networks shift towards away randomness strings show shifts impacts possible dynamics network demonstrated fig. illustrating connection. illustrated random simpler object network possible interventions upon little effect. however objects removed simplicity randomness richer candidate generative models able perform greater shifts effects guided perturbations. demonstrates advantages moving previous purely statistical approaches algorithmic approaches deeply related causation object sequence positive integer numbers explained assigned high causal figure statistical correlation algorithmic causation. graphs different origin require different encodings capable recursively generate graph interventions graph effect candidate algorithmic models depending algorithmic causal content. sequence cannot characterized measures based entropy classical statistics characterized algorithmic complexity possible turing machines states encode decimal counter thus many small computer programs encode highly algorithmic sequence statistical regularity computer program whose halting criterion leftmost head position turing machine respective space-­‐ time evolution whose output binary reproduce sequence decimal unary non-­‐ halting computer program computes directly followed space-­‐time diagram effectively encoding sequence generating function thus clear entropy diverge algorithmic complexity encoding simplicity therefore characterizing causal mechanistic nature found procedure based described exploited paper heart causal interventional calculus. lgorithmic probability deals challenge inductive inference obverse rigorous mathematical formalization randomness so-­‐called coding theorem formally relating complexity probability example causal graph network defined probability random computer program constructed bit–by e.g. random flips coin–outputs description e.g. adjacency matrix. upper semi-­‐computable allows algorithmic complexity approximated. among remarkable properties cannot refuted arbitrary significance level computable measure estimations asymptotically converge independently relatively small constant idea behind numerical methods estimating enumerate computer programs explain generate full part generating system representing predictive computational model observables interesting application algorithmic calculus evolving systems. deterministic dynamical system length shortest generating mechanism describing system’s state time denoted grow function specifically log. deterministic dynamic system every state calculated i.e. trivial fundamental property deterministic dynamic systems exploited find perturbations system’s state related perturbations deviations indicate non-­‐causal trajectories disconnected patches unrelated originally observed dynamic system. system completely isolated parts seem explained state system thereby appearing non-­‐deterministic patches thus exposed identified foreign system’s normal cause algorithmic perturbation analysis. formally algorithmic calculus consists estimation change algorithmic information content network intervention. define element negative mutated network without element moving towards algorithmic randomness positive neutral otherwise example maximally random network positive elements exists perturbation increase randomness network either removing node edge already random denote information spectra list non-­‐integer values quantifying information-­‐ content contribution every element signature sorted version largest smallest value information spectra. example cycle graph nodes edges information content removal leads path graph therefore information spectra unsorted values elements sorted list greatest lowest values. spectra informative perform ab-­‐initio identification example vulnerable breaking points regular s-­‐w networks whereas removal neutral elements minimizes loss information relevant description network important graph-­‐theoretic properties information spectra maximizes preservation original algorithmic information content thus represents optimal parameter-­‐free method dimensionality reduction sense minimizes loss information thus preserving important features system disregarding nature save computational resources numerical experiments part consider single perturbations only rather full power-­‐set them. figure basic concepts features nodes edges identified according information contribution network evaluating effect upon removal element moves network towards randomness gray moves network away randomness logarithmic factor only hence neutral. neutral information removal node equivalent removal edges connecting node even though individual edges negative random connection complete graphs positive removal makes generating mechanism complete graphs shorter complete graphs randomly connected. information analysis directed graph identifies changes direction. nodes edges non-­‐recursive random graph neutral element move network towards randomness element ranked according algorithmic/causal contribution information spectrum identifying contributions algorithmic model original network elements coloured move network towards randomness elements towards blue move network away randomness network blue shifted depending causal content interconnection/dependence among elements. signature graph info spectrum sorted highest lowest rank used profile classes networks. distribution values entropy versus algorithmic complexity strings length normalized maximum entropy. strings less random entropy lossless compression suggest. gaps causality discovery gain using algorithmic complexity look statistically random causally pinpointing cases random. techniques pinpoint elements breaking points evolving random graphs regular graphs according watts–strogatz model. neutral node neutral edge removal size network otherwise said elements preserve information signatures) able preserve graph-­‐theoretic properties edge betweenness degree distribution clustering coefficient thus optimal dimensionality reduction since neutral elements contribute algorithmic content system affect length underlying generating program means network recover neutral elements moment simply running system back point elements removed process identification algorithmic contributing elements allows systems ‘peeled back’ likely causal origin unveiling generating principles used handle causally steer system measures fail analogously elements added network increase maximize algorithmic information content thus approximating maximally algorithmic-­‐random graph used maximum-­‐entropy modelling purposes advantage discarding false maximal entropy instances contrast neutral elements extreme valued network elements hold drive network towards away algorithmic randomness extending current study networks constrained mostly graph-­‐theoretic statistical entropic approaches thereby adding another dimension research bservers limited access system’s generating mechanism denoted precise dynamics system. system’s perturbation observation time correspond estimation complexity based upon likelihood explaining objective attempting identify system access inspecting system observation intervals capturing possible features associate knowledge dynamics governed system e.g. odes discrete mapping cellular automaton rule beyond reach network-­‐based approaches conveniently focus relationships among system’s elements represented timeline thus serving topological projection system’s dynamics based e.g. correlation computer programs empty inputs encode dynamics changing initial conditions system time constituting true causal generating mechanism system’s timelines dynamics topology included. systems equally dependent internal kinetic dynamics. example network-­‐rewriting systems updated according replacement rules dynamics igure representations dynamical systems causal calculus introduced help reveal generating mechanism discrete dynamical system regardless different lossless representations have representations preserve information system reconstructed full. ability comes property closed deterministic systems must preserve algorithmic complexity along time evolution given generating mechanism always every time step except time index encoded bits. means deviation indicates system closed deterministic thus possibly interacting system identify interacting elements perturbing measuring deviation log. one-­‐dimensional evolving system displays information elements determining different causal regions instantaneous observation following perturbation analysis. cellular automaton random perturbations algorithmic calculus reveals rows artificially perturbed grey cells showing identified neutral last dynamic evolution indicating time direction system. fig. unlike entropy invariant different object descriptions. shown tree-­‐like representation constructed causal network algorithmic randomness near maximum entropy degree sequence contradiction given recursive nature graph zero shannon entropy rate adjacency matrix diverging expected shannon entropy. latest nodes graph depicted identified neutral nature revealing time order thereby exposing generating mechanism recursive network first principles systems random displaying inherent regular structure relatively deeper attractors thus robust face stochastic perturbations derived programmability index according algorithmic causal perturbations network elements pushing system towards away algorithmic randomness reveal qualitative changes attractor landscape absence dynamical model system. network thus programmable elements freely move network towards away randomness. formally relative programmability system defined median absolute deviation elements move towards randomness elements move away randomness absolute programmability defined max) interpolation function. cases removed reprogrammable closer less reprogrammable take combined reprogrammability norm vector ||vr|| programmability space given cartesian product indices assign values simple random systems high values systems non-­‐trivial structures thus constitute known measure sophistication tells apart random simple cases highly structured case quantifying algorithmic plasticity resilience system face causal perturbations figure reprogrammability space. illustration programmability space defined cartesian product programmability indexes dimensional vector space. relative programmability takes account sign different segments i.e. much signature segment zero immune small numerical errors boundary conditions whereas absolute programmability accounts shape signature measures variability sample robustness regards extreme values. indices contribute information programmability capabilities system network. combined version effectively weighted index programmability measures maximizes certainty measured magnitude vectors closer programmability vector fig. demonstrated algorithmic calculus help reconstruct discrete dynamical systems high accuracy disordered states even index observations correctly effectively providing mechanistic generating model backwards forwards time. late perturbations akin neutral information value representing step index dynamical). minor disagreements reconstructed order observations shown fig. come three sources expected model explain several arrangements data apply single double perturbations single perturbations limitations. example greater number simultaneous perturbations would needed correctly reconstruct deterministic order-­‐r markov systems finally algorithms approximate algorithmic information content upper bounds exact values. reconstruction dynamical systems taking consideration increasing number observations illustrates however data accurate reconstruction thereby demonstrating numerical algorithm fault reconstruction practice possible computationally feasible reliable. method scalable clever shortcuts implemented method reconstruct space-­‐time evolution discrete dynamical systems high accuracy instantaneous non-­‐ordered observations demonstrates identify causal even among random-­‐looking systems rules correlation values lower though reconstructions still qualitatively close. exploiting result later step time dynamical system elementary cellular automaton less disruptive effect perturbation respect algorithmic-­‐information original system able reconstruct cellular automata row-­‐scrambling gave time index automatic reconstruction possible generating mechanisms quantifying disruptive perturbation algorithmic information content space-­‐time evolution allows extract generating mechanism order perturbations less disruptive hypothesized generating mechanism inferred instantaneous observation. apparently simpler rules simpler hypotheses almost perfect correspondence order ranking observations systems look random others locally rule generating observed deterministic dynamical system maps derived causal deconvolution blocks demonstrated accomplished looking smallest valid transformation among consecutive observations order infer influence event outcome local state leads single future i.e. `light cones’. amounts claiming regions causally disconnected. assumption system deterministic also infer system’s rulespace thus maximum length generating rule. always start assuming smallest possible rulespace size correct rule length first causally disconnects regions manner consistent observations model rule. example infer rule behind rule would start simplest rule hypothesis assigns every black cell either black white vice versa. have -­‐> -­‐> -­‐> -­‐>. however neither cases consistent observations move consider mapping cells e.g. -­‐> -­‐> however maps onto thereby failing causally separate future states past rule first separate regions manner accords observations thus precisely rule triplets sent except properly rank observations correct time index thus sufficient condition make optimal prediction based likely generating rule/mapping-­‐-­‐-­‐by principle unnecessary multiplication assumptions. non-­‐trivial cases reconstruction formalism causal composition decomposition found figure reconstructing dynamical systems. reconstruction space-­‐time evolution dynamic systems normal space-­‐time evolution displayed left-­‐hand-­‐side; right-­‐hand-­‐side reconstructed space-­‐times scrambling finding lowest algorithmic complexity configuration among possible permutations followed spearman correlation values order. time inference linear time generation algorithmic model forwards backwards thus revealing dynamics first principles underlying dynamic systems without brute force exploration simulation. predicted later time perturbation performed less disruptive compared length hypothesized generating mechanism evolution original system. pair shows statistical values reconstructed original space-­‐time evolutions models separating system different apparent causal elements. depicted reconstruction simplest elementary cellular automata random-­‐looking steps illustrating perturbation-­‐based algorithmic calculus model generation opposite behavioural cases. accuracy reconstruction scaled improved cost greater computational resources going beyond single perturbation power-­‐set depicted reconstructions random-­‐looking cellular automata single double-­‐row-­‐knockout perturbation analysis. errors inherited decomposition method look like ‘shadows’ explained numerical deviations boundary conditions estimation bdm. variations magnitude found effect different systems different qualitative behaviour simpler less different effects deleterious perturbations different times. mathematical standpoint every non-­‐random network exists generative program certain size contrast network shorter description itself generative causal program defined algorithmically random. generative program number possible halting states driven into determine number attractors dynamical system. network internal dynamics topological kinetic details encoded full lossless description thus handled algorithmic causal calculus introduced here. observations time result factors access generative program deconvolution measured elements contributing underlying system’s dynamics impossible usually keep partial account system’s dynamic output figure connecting algorithmic complexity dynamical systems numerically pushing complete graph away randomness edge deletion produces complete graphs theoretically expected. pushing network towards randomness however produces graphs approaching edge density also theoretically expected. pushing random graph towards simplicity reveals structured subgraphs contained original random thereby revealing structure randomness single silico perturbations. distributions number attractors possible -­‐node boolean networks. small difference significant number attractors small graphs tightly bounded. numerical calculation change number attractors simple directed complete graphs scale-­‐free networks converted boolean networks scale-­‐free networks like regular networks resilient face perturbations. algorithmic calculus enables identification system’s causal core facilitates assessment causal contribution system’s elements evaluated whether calculus could serve guide reprogramming system represented network corresponding qualitative shifts attractor landscape associated system’s behaviour even absence access dynamical system’s equations. algorithmic content networks complete graphs nodes immune perturbations logarithmic effect leaving basins attraction number attractors graphs however structure thus predicted numerous shallow attractors. moving network away randomness thus effect number depth attractors moves away randomness. conversely networks removed randomness fewer deeper attractors moving towards randomness eventually increase number attractors decrease average depth. theoretical inferences confirmed simulation boolean networks based upon principles using e.g. complete graphs model could predictively push networks towards away randomness also emulated boolean dynamic networks different topologies predicting nature change number attractors pushing networks towards away algorithmic randomness. number attractors based simulation networks equipped random boolean functions nodes directing edges inputs outputs dynamics mounted network effect said perturbations studied respect algorithmic randomness number attractors network. results show moving moving towards away algorithmic randomness significant difference versus control experiments removing random nodes thereby establishing connection algorithmic complexity dynamical systems. consider alternative perform educated perturbations existed move network along attractor landscape guide steer behavior perform actual simulations finding `drivers’ using control theory making strong assumptions linearity. tested whether algorithmic causal calculus provide biological insight explanatory power. first applied calculus experimentally validated network e-­‐coli negatively labelled genes protect network becoming random therefore found genes provide specialization cellular network whereas positive nodes contribute processes homeostasis pinpointing elements network make prevail since removal would deprive network algorithmic content thus important properties. analyzed network controlling cell differentiation assess informative value qualitatively reconstructed attractor/differentiation landscape. roceeding undifferentiated cell state towards mature cell state calculus predicts fewer deeper attractors differentiated state fig. b-­‐d follow process naive cell differentiating cell signature. revealed information spectrum significantly different values time programmability significantly higher first time-­‐points final terminal time-­‐point. interestingly network signatures suggest information stability point nodes move network towards greater randomness. gene enrichment analysis genes classified positive negative information values comprised many genes known involved cell differentiation transcription factors stat families. finally retrieving network data cellnet reconstructed heights corresponding epigenetic waddington landscape different cell types conforming biological developmental expectation igure k-­‐medoid clustering transcription factors algorithmic-­‐information node perturbation analysis validated e.coli network according kegg ecocyc. positive genes found related homeostasis negative genes processes specialization. distribution genes according causal information value differentiation process cells. uneven distribution genes information value strengthens significance enrichment analysis heatmap normalized information values approximately half genes able move early network towards away randomness. genes turn positive differentiated stage. charting regulatory networks sketch suggested epigenetic differentiation landscape reconstructed average reprogrammability algorithmic randomness cell network. summarize prevailing paradigm system identification control broadly described aiming understand relevant features system order formulate models properties interest maximize fitting model respect properties. unbiased identification features complete problem unless additional assumptions made nature underlying data-­‐ distribution. thus despite advances computational tools fitting data—big not—to particular model issue relevant properties upon perform model maximization error minimization unresolved. since causal calculus introduced based fundamental mathematical results algorithmic information theory combination novel schemes numerical evaluations advanced model-­‐free proxy estimate qualitative shape dynamic possibilities system thus make educated assumptions beyond current statistical approaches. approach gives handle intervene steer system using powerful parameter-­‐free algorithms. results bridge concepts across disciplines connect mature mathematical theories computability algorithmic complexity dynamic systems challenge causality science. martin-­‐löf definition random sequences. inf. control solomonoff formal theory inductive inference. part inf. control levin laws information conservation aspects foundation probability theory. delahaye zenil numerical evaluation algorithmic complexity short strings glance innermost structure randomness. appl. math. comput. soler-­‐toscano zenil delahaye j.-­‐p. gauvrit calculating kolmogorov complexity output frequency distributions small turing machines. plos zenil hector; kiani narsis;tegner low-­‐algorithmic-­‐complexity entropy-­‐deceiving graphs. phys. rev. stat. nonlinear soft matter phys. gauvrit singmann soler-­‐toscano zenil algorithmic complexity psychology user-­‐friendly implementation coding theorem method. behavior research methods volume issue gauvrit zenil soler-­‐toscano delahaye brugger human behavioral complexity peaks plos comput biol chaitin length programs computing finite binary sequences. rathmanner hutter philosophical treatise universal induction. entropy kolmogorov three approaches quantitative definition information. probl. peredachi informatisii zenil soler-­‐toscano dingle correlation automorphism group size topological properties program-­‐size complexity evaluations graphs complex networks. physica cellnet. cell y.-­‐y. barabási a.-­‐l. control principles complex systems. rev. mod. phys. noël kerschen nonlinear system identification structural dynamics years progress. mech. syst. signal process. nilsson björkegren pena tegnér consistent feature selection pattern recognition polynomial time. mach. learn. res. zenil soler-­‐toscano kiani n.a. hernández-­‐orozco rueda-­‐toicen decomposition method global evaluation shannon entropy local estimations algorithmic complexity arxiv. riedel zenil cross-­‐boundary behavioural reprogrammability reveals evidence pervasive computational universality international journal unconventional computing riedel zenil rule primality minimal generating sets turing-­‐universality causal decomposition elementary cellular automata journal cellular automata yang-­‐yu albert-­‐lászló control principles complex systems rev. mod. phys. dingle camargo c.q. louis a.a. input–output maps strongly biased towards simple outputs nature communications peled vikas mishra avishy carmi computing nowhere increasing complexity ieee symposium series computational intelligence lgorithmic causality causal content dynamical system running given smallest |st| denotes size difference |st| approximation causal content causal content non-­‐causal system approximates i.e. small meaning |st| trajectory algorithmically random. causal systems i.e. complexity causal system driven evolution time logarithms base otherwise indicated. removal/knockout) element denoted original algorithmic information content without loss generalization let’s take system network dynamic system nodes links connecting nodes lgorithmic system inference generating mechanism dynamical system call neutral perturbation. perturbation thus change generating mechanism recovered st\\e st+\\e otherwise disruptive degree disruptiveness general providing means reverse system time reveal possible generating mechanism process. system reversible number generating models formulated thereby producing optimal hypotheses form generative models. nformation sensitivity derivative absolute value programmability graph programmability curve numerically calculated rate change versus elements i.e. list signatures capturing non-­‐linear effects perturbations maximal algorithmic random graph erdős -­‐rényi graph algorithmically random i.e. whose shortest possible computer description shorter number edges order randomness deficiency difference information signatures e.g. kolmogorov-­‐smirnoff distance i.e. removed network algorithmic randomization. imply directed graph transformation undirected graph directed edge directions chosen minimize number independent paths number path collisions. combined reprogrammability metric induced norm ||v_r defined euclidean distance programmability indices. metric combines relative absolute programmability indices takes equal account sign signature shape consequently minimizing impact uncertain sign estimations errors calculation algorithmic complexity attributable boundary conditions atural programmability expected theoretical programmability system network compared estimated programmability e.g. complete graph nodes edges algorithmic-­‐information contribution thus analytically derived node count thus nodes complete graph ‘slightly’ positive core causal calculus based upon change complexity system subject perturbations particularly direction magnitude difference algorithmic information content graphs e.g. removal difference estimation shared algorithmic mutual information g\\e. contribute description log|v| node count i.e. difference small function graph size thus almost bits algorithmic information element removal results loss information. contrast cannot explained alone algorithmically contained/derived therefore fundamental part description generative causal mechanism else part explained independently e.g. noise. whether noise part generating mechanism depends relative magnitude respect original causal content itself. random effect small either case richly causal small generating program noise greater impact would removing description already short description increase randomness. former result simply another complete graph smaller size latter deleted link would need described description complete graph itself. however removal node equivalent removal edges connecting node edges positive information even though individual edges negative nonlinear phenomenon call information incoherence. connecting complete graphs random node designates connecting link positive removal pushes network towards simplicity minimal description graphs shorter minimal description graphs plus description missing link random points. link also seen element connecting networks hence network networks. identification removal would thus reveal separation networks. general positive elements identify major structures generated likely generating mechanism given observation elements stand negative thereby identifying layers networks independent separable generating mechanisms even removing apparent noise signal networks richly causal. random graphs node-­‐ edge-­‐ blueshifted simple graphs complete wheel graphs edge-­‐redshifted. perturbing node recalculating spectra changes original spectrum clearly non-­‐reductionistic approach characterizing networks. methods introduced also work directed weighted graphs without loss generality. eal-­‐world networks generated physical laws recursive according classical mechanics thus left side schematic extended data figure. also contain information interacting systems captured transient state incorporates external signals pushing networks towards randomness. quantified concept proposing different programmability indices extended data figure. summarizes theoretical expectations numerical results. thermodynamic argument curve negatively skewed easy fast move regular networks towards randomness function number edges—there ways move network towards randomness description moves i.e. description edge removed edge count g—there fewer ways move random network away randomness. graph example cannot moved edge node deletion |e|. result compatible asymmetries energy landscapes moving systems towards fewer future attractors versus moving back states greater number attractors latter requiring much energy former. causal algorithmic calculus defines optimal parameter-­‐free dimension reduction algorithm minimizes information loss reducing size original object. minimal information loss sparsification method based removing neutral elements preserving information content network therefore properties used reduction minimizing loss informational feature needs described cannot compressed shorter description maximal algorithmic randomness preferential attachment algorithm maximal algorithmic randomness preferential attachment algorithm viewed reverse algorithm comparison mils. marpa seeks maximize information content graph adding edges every step. process approximates network given size largest possible algorithmic randomness also erdős-­‐rényi graph. approximation ‘maximal’ algorithmic-­‐ random graph produced reference object whose generating program smaller network better serve maximum entropy modelling. supplement section proof existence graphs maximal algorithmic-­‐random graphs boolean network consists discrete boolean variables boolean function takes inputs subset variables. conducted first experiment single-­‐node single-­‐edge deletion effects possible boolean networks size nodes boolean functions. output boolean network state numbered sequence states nodes. boolean model network represented boolean general connected network node controlled subset nodes. size controlling subset network depends connectivity pattern network example e-­‐r random graph edges equally distributed edge density change state arbitrary node initial state effect dynamics network average means basin attraction remains mostly unchanged. basin attraction size number attractors size depend network density m<<n. however simply connected complete graph nodes control nodes attractor basin attraction size modular scale-­‐free networks edges statistically equally distributed nodes control many others unlike e-­‐r random network significantly greater basin attraction sizes therefore smaller number attractors–. estimated algorithmic-­‐information contribution every node possible -­‐node graphs. estimation algorithmic-­‐information contribution considered vertices orbit automorphism group information value respect largest component according unlabelled definition algorithmic complexity unlabelled graphs thus correcting minor deviations estimations complexity boundary conditions calculation every however feasible general production thus calculation believed thereby making brute force exploration computationally intractable. however also shown estimations similar constant eigenvalue number different eigenvalues number attractors largest remaining connected component larger graphs. experiment repeated boolean functions leading results. apply uninformed perturbations move networks towards statistical randomness based algorithmic-­‐information calculus controlled fashion towards away algorithmic randomness thus taking account non-­‐statistical non-­‐linear effects system generating mechanism providing sequence causal interventions move networks systems level generating model order reveal first principles control side effects system’s manipulation every step. andom versus regular networks sensitive different ways. algorithmic-­‐ random network hard move fast along algorithmic -­‐random location changes simple regular graphs dramatic effects displaying different degrees algorithmic-­‐content networks simply directed complete graphs nodes immune perturbations leaving basins attraction number attractors principles evident systems random display inherent regular properties thus robust face random perturbations deeper attractors theory algorithmic complexity provides means find mechanistic causes likely algorithmic models helping reverse engineer partial observations dynamic systems networks. causal reconstruction method system follows remove neutral elements repeat reassigned o\\{e}. iterations reverse sequence observations o\\{e} provides indication evolution system time thereby yielding hypothesis generating mechanism producing unveiling initial condition last element iteration first reversing chaitin algorithmic information theory. wuensche espanés osses rapaport biosystems fixed-­‐points random boolean networks impact parallelism barabási albert scale-­‐free topology case. biosystems aldana boolean dynamics networks scale-­‐free topology. explored whether algorithmic content precisely information spectrum system/network influences transitions different stable states thereby effectively providing tool steer reprogram networks. observed average decrease size reachable states nodes distribution reachable states becomes clustered symmetrical graphs nodes single deletion. positive info nodes similar effect deletion network. absolute relative negative nodes similar effect whereas neutral nodes preserve distribution skewness closest original. istograms perturbation effects graphs size nodes using functions produced similar results small size graph able control graph automorphisms correct minor errors produced boundary conditions objects orbit automorphism algorithmic perturbation analysis elements orbit take perturbation every element equal min{| words effect every element same. automorphism group generated help public software experiment. larger networks however becomes computationally expensive context perturbation analysis thus shown continued calculating only. exhaustive experiment connected graphs node count deleting largest versus smallest node degree produced statistical differences expected previously suggested. relevant purposes found positive versus negative versus neutral information node/edge removal statistically different effects executed connected networks. negative information node removal interestingly similar lowest degree removal significantly different statistically control node removal. absolute relative negative information removal similar effects neutral nodes/edges kept distribution skewness closest original distribution accordance theory. negative edges number attractors significantly increased theory predicted. below provide pseudo-­‐code mils algorithm. mils allows dimensionality reduction graph deletion neutral elements thus maximizing preservation important properties object algorithmic information content invariant neutral node perturbation. graph. then ssuming estimations spectra definite fixed mils deterministic algorithm. network information value element respect mils algorithm removes first minimizes loss information choice remove either thus however hold general removal followed removal equal removal followed removal even non-­‐linear effects suggests deal cases mils deterministic simultaneous removal elements time complexity mils thus ranges original worst case nodes information value/contribution thus removed single step. therefore removal turns mils proper deterministic algorithm yields object mils object property network ultimately contributes information content information minimization preserve potential measure interest. show following section minimizing loss information maximizes preservation graph theoretic properties networks edge node betweenness clustering coefficient graph distance degree distribution finally information content itself. xperimental evaluation mils using real-­‐world networks depicted example scale free network nodes original information signature neutral edge removal preserves information signature deleting neutral edges also preserves graph theoretic properties edge betweenness clustering coefficient node degree distribution deleting graph edges thus superior several common sparsification methods element deleted lead reduction network’s algorithmic information content maximization attempt preserve algorithmic information content informative redundant properties network/system removed. mils applied well-­‐known networks used pioneering studies find loss information signatures thus non-­‐linear algorithmic information content system minimized also tested common graph-­‐theoretic measures maximally preserved. contrasts application random deletion common dimension reduction methods. marpa allows constructions maximally random graph filling blanks i.e. adding edges given graph manner randomness increases. network information value respect possible perturbations. finite bounded elements e.g. edges network find perturbations iterate apply perturbations make possible perturbations single perturbations only) maximizing complexity max{g among alternatively configuration edges maximizes algorithmic randomness maximal complexity denoted maxc. find sequential perturbations maxc maxc measure related randomness deficiency removed randomized version maxc upper bounded maxc difference always positive). fig. a-­‐c shows numerically moved regular network towards randomness notice network edge density maximal entropy high algorithmic randomness i.e. recursively generated high algorithmic-­‐random graph also because contradiction would statistically compressible thus non-­‐algorithmic random graph statistical regularity cannot also algorithmic-­‐random graph. also consider absolute maximum algorithmic-­‐random graph denoted amaxc disconnected number elements graph comprising generating graph computationally expensive exponential time complexity every step possible attachments tested evaluated small graphs computationally feasible represent approximations perfect random graphs unlike graphs cannot principle recursively generated small computer programs. intuition behind construction graph shortest computer program produce adjacency matrix graph size adjacency matrix significantly shorter. thus strict sense considered perfect graph. every time larger graph therefore addition edges needed computer program generates grows proportionally size adjacency matrix systems whose internal kinetics fully determine system’s behaviour i.e. attractor structure hopfield networks boltzmann machines independent fixed topology networks however dependent topology geometry boolean networks governed topological internal kinetic properties encoded connectivity node assigned boolean function node. observation system necessarily partial snapshot system’s trajectory phase space reveals certain aspects generating cause without loss generality causal calculus introduced either combination order produce algorithmic models causal generating mechanisms approaching producing focus causal calculus introduced readily incorporate moving phase space without essential modification. included examples using discrete dynamic systems cellular automata show calculus utilized. elements move system towards away randomness conversely positive negative elements like defined application networks. cellular automaton defined rule computing value position configuration based values cells finite neighborhood surrounding given position. commonly evolves square grid lattice cells updated according finite local rules synchronously applied parallel. snapshot time symbols cells called configuration. snapshot space time called evolution. symbols cellular automaton finite configuration configuration finite number symbols differs distinguished state denoted sequence symbols stack configurations configuration obtained preceding applying updating rule called evolution. formally positive integers configuration initial configuration function also called global rule local rule determining values cell neighborhood range radius cellular automaton number cells taken consideration left right central cell rule determines value next cell cells update states synchronously. cells extreme must connected indicates local state dependency cellular automata updates every row. depicted elementary cellular automaton rule generates typical -­‐dimensional cone simplest initial condition rule thus seen -­‐bit computer program represented rule icon representing function function determining local global dynamics perturbation simple evolution rule leads increase complexity rule longer description rule would needed incorporate random perturbation introduced thus every rule information negative except random rows whose deletion would bring rule simplest description unlike rest dynamic system last step evolution dynamic system information neutral remove complexity removal neutral elements reverses system’s unfolding evolution original cause rule derived reversing sequence neutral elements every step effectively peeling back dynamic system single instant sequence observations clustering consecutive rows evolution elementary cellular automata found later perturbations time neutral thus conforming theoretical expectation taking sample representative also clearly case proceeded reverse engineer rule finding lowest complexity configuration disordered observations show found correct times thus generating powerful method reverse engineer find design principles generating mechanism evolving systems. running sequence forward also make predictions phase space configuration dynamic evolving system. fig. shows predicted point phase space diverge actual position system phase space thus providing good estimations evolution system backward forward. paper choose work level convenient simplifying reasons followed network-­‐based approaches unlike possible approaches theory methods hold general non-­‐linear dynamical systems static evolving networks. working only assume lossless descriptions observations full descriptions even unknown. date alternatives applying non-­‐linear interventions complex systems phase space actually calculate dynamical properties system often assumed little knowledge else assumed linear fixed states requiring computationally intractable simulations. calculus however requires much less information make educated causal interventions prove extremely useful powerful. ntropy-­‐deceiving graphs introduced method building family recursive graphs denoted ‘zk’ property recursively constructed thus algorithmic complexity uninformed observer would appear statistically random thus maximal entropy. graphs proven maximal entropy lossless descriptions minimal entropy lossless descriptions exactly object thereby demonstrating entropy fails unequivocally unambiguously characterizing graph independent particular feature interest reflected choice natural probability distributions. natural probability distribution object given uniform distribution suggested object dimension alphabet size. example graph losslessly described adjacency matrix face information natural distribution probability space matrices dimensions binary alphabet. however losslessly described degree sequence information provided natural distribution given probability space sequences length alphabet size |{s}| denotes number n-­‐ary different symbols natural distribution thus less informative state observer knowledge source nature object denote ‘zk’ graph constructed follows clearly supporting nodes always latest added iteration. perturbing elements network last elements break generating program thus elements move network towards randomness whereas removing latest nodes little impact moves network back time originating program remaining needing reach state before. thus inspecting elements contribute make network slightly simpler reverse network time thereby revealing generating mechanism shown entropy highly observer dependent even face full accuracy access lossless object descriptions. specific complexity-­‐deceiving graphs entropy produces disparate values object described different ways even descriptions reconstruct exactly same same object. drawback shannon entropy ultimately related dependence distribution serious often overlooked objects strings graphs. object graph shown changing descriptions change values divergent contradictory values produced. means needs choose description interest apply definition entropy-­‐-­‐ adjacency matrix network degree sequence-­‐-­‐but soon choice made entropy becomes trivial counting function feature-­‐-­‐ feature-­‐-­‐of interest. case example adjacency matrix network entropy becomes function edge density degree sequence entropy becomes function sequence normality. entropy thus trivially replaced functions without loss cannot used profile object independent arbitrary feature interest. measures introduced robust measures complexity independent object description based upon mathematical theory randomness algorithmic probability sensitive enough deal causality provide framework causal interventional calculus. regulation network escherichia coli. incompressibility method. soc. ind. appl. math. doi./s-­‐-­‐-­‐ computational abilities biophysics hopfield vis. pattern recognit. -­‐coli transcription factor network ontology enrichment analysis estimated information node values highly curated coli transcriptional network info values clustered clusters using partitioning around k-­‐medoids optimum average silhouette width. gene clusters tested enrichment biological functions according gene ontology kegg ecocyc databases using topgo weight algorithm hypergeometric enrichment test kegg ecocyc. values correlate degree distribution compression shannon entropy. numerical results suggest positive information genes e-­‐coli related homeostatic processes negative info genes related processes specialization agreement idea cellular development unfolding process core functions algorithmically developed first specialized functions enabling training-­‐free parameter-­‐free gene profiling targeting. extended figures show measures fell short producing statistically significant groups gene ontology analysis also provide details clusters found elements comprising them. nformation spectral enrichment analysis differentiation applied method dataset differentiation t-­‐helper cells. cells major subsets t-­‐helper cells addition comprise several sub-­‐ types treg cells. subsets differentiate common naïve cell precursor cell type based environmental signals classified certain lineage-­‐ defining markers. cells necessary protect host fungal infections time involved pathogenesis several autoimmune diseases hence processes driving differentiation great interest scientific community. gene ontology analysis taking experimentally known genes involved process differentiation naïve shown precisely genes distributed non-­‐uniformly different ways along time points suggesting algorithmic perturbation analysis succeeds identifying genes nformation spectral analysis information spectral analysis used reconstructed regulatory network functional perturbation transcriptional data corresponding differentiation. data divided three time windows hours hours hours referred earlynet intermediatenet finalnet respectively. interested investigating whether genes strongly negative positive information values would include genes known crucial thelper cell differentiation and/or novel putative regulators whether genes would according predictions change information content throughout differentiation process. noted general genes classified positive negative information values covered several genes known involved cell differentiation transcription factors stat families genes assigned regulating modules present along spread information values enrichment extreme positive values. however genes extreme information values identified original study suggesting method identify additional regulators analyzing genes present networks determining evolution time noted genes chemokines/chemokine receptors switching negative values earlynet positive values finalnet. gene group switching positive earlynet negative intermediatenet back positive finalnet many transcription factors stat family represented. extreme information values assigned many members family transcription factors comprises well-­‐known regulators thelper differentiation including th-­‐inhibiting roles appears lists intermediatenet finalnet three genes assigned negative information values finalnet namely stat tcfeb trim suggesting removing might enhance profile. networks clustered using k-­‐means algorithm clusters network list genes changed negative information values earlynet toward positive information values finalnet contained several genes involved helper cell subset differentiation function example hifa foxo ikzf ilra ilst. conversely list genes highest information values earlynet overlapping lowest information values finalnet restricted number contained general transcription factors rela jun. noted general genes classified negative positive information values comprised many genes known involved cell differentiation transcription factors stat families chemokine receptors cytokines cytokine receptors. particularly evident networks analyzing genes negative information values network change towards positive information values network found common elements lists contain several genes involved helper cell subset differentiation function example hifa foxo ikzf ifng ilra ilst cxcl cxcr cxcr. interestingly list genes positive information values network negative information values network much restricted number overlap contained highly interesting genes. network mostly transcription factors including several irfs stats well runx smad known important cell differentiation. genes negative information values network stat tcfeb trim tempting speculate over-­‐activation might able reprogram differentiated cells another lineage. indeed stat well-­‐known factor il-­‐ response induction. notably network viewed transition state genes assigned positive information values belonged family regulators thelper differentiation including th-­‐inhibiting roles appears said list. nrichment analysis assess extent informational spectral analysis identifies genes relevant differentiation process cells perform enrichment analysis based literature survey. collected landmark papers literature paper list genes extracted attempt select genes text identified relevant differentiation. script calculates intersections sets genes greater number intersections given higher weight relevant literature data represented network diagram co-­‐occurrence analysis highlighted genes commonly identified across several studies. enrichment analysis revealed positive negative information elements distributed equally thus indicating information values distributed chance three time steps changed time according theoretical biological expectations. early stages naïve cell strong sets genes handles steer network towards away randomness larger component negative elements indicate signals either activating cells perturbing cells among stable naïve cells original program. cells activated fewer negative genes present distribution skew positive patch towards neutral elements pinpoint evolving genes cell activation differentiation final step cells longer negative elements indicating program reached steady state cells fully differentiated remaining elements either positive closer neutral. cellnet network biology-­‐based computational platform assesses fidelity cellular engineering claims generate hypotheses improving cell derivations. merged networks tissue type single larger entity. result networks networks following homo sapiens cell types b-­‐cell colon endothelial fibroblast heart hspc kidney liver lung macrophage muscleskel neuron ovary skin tcell following vertex count applied causal calculus reprogrammability measures waddington landscape derived location complexity programmability quadrants according theoretical expectation. according fig. differentiated cells tend closer non-­‐differentiated ones tend farther away start state randomness shallow attractors sensitive perturbations move direction-­‐-­‐towards creating functions represented structures moving away randomness. exactly found calculating plotting cellnet networks cell lines homo sapiens. cells cellnet networks organized shape theoretical sketch describing thermodynamic-­‐like behaviour agreement biological stage expectation placing stem cells order closer randomness high reprogrammability conforming theoretical expectation greatest number possible shallow attractors network able move away randomness followed blood-­‐related cells known highly programmable adaptable followed bulk differentiated cells first second quadrants. distribution programmability cells represented networks cellnet fits naturally expected programmability yosef dynamic regulatory network controlling cell differentiation. nature kuchroo defining functional states cells. fresearch induction molecular signature pathogenic cells. nat. immunol. hong xing tyson mathematical model reciprocal differentiation helper cells induced regulatory cells. plos comput. biol. tuomela identification early gene expression changes human cell differentiation. blood e-­‐ ciofani validated regulatory network cell specification. cell ghoreschi generation pathogenic cells absence tgf-­‐β signalling. nature carneiro chaouiya thieffry diversity plasticity cell types predicted regulatory network modelling. gaublomme single-­‐cell genomics unveils critical regulators cell pathogenicity. cell doi./j.cell... yamane paul differentiation effector cell populations annu. rev. immunol. cahan cellnet network biology applied stem cell engineering. cell xtended figure algorithmic complexity adds additional dimension complementary different notion entropy performing network analysis. unlike statistical mechanical approaches shannon entropy algorithmic complexity improves entropy assigning lower entropy thus higher causal content objects appear statistically simple also algorithmically simple virtue short generating mechanism capable reproducing causal content network. without additional dimension causal non-­‐causal networks collapsed typical bernoulli distribution. indeed random-­‐looking network maximal shannon entropy recursively generated short algorithm entropy would misclassify random. additional dimension introduce study dynamic systems particular networks together methods tools thus better tackling problem revealing first principles discovering causal mechanisms dynamic evolving systems. dimension account types structures properties sensitive directions computable statistical measures would indeed erdös-­‐rényi graph example recursive recursivity meaning actually pseudo-­‐random properties random graph algorithmic-­‐random. distinction science evolving systems random-­‐looking governed rules otherwise concealed apparent noise. extended figure entropy easily fooled. preferential attachment algorithm creating networks growing density showing entropy calculated adjacency matrices capturing graph density assigning dense b-­‐a graphs higher entropy erdös-­‐rényi graphs. result reproduced replicates using node graphs replicates/graphs experiment repeated approximately times. main figc shows another graph created recursively suggests divergent values entropy object different descriptions suggesting different probability distributions. different robust approach characterizing networks systems thus needed able tell cases apart moving algorithmic mechanics/calculus introduced thus improving traditional techniques draw heavily upon statistical mechanics. xtended figure thermodynamic-­‐like effect based programmability measure sophistication moving random networks edge removal significantly difficult moving simple networks towards randomness. random graphs elements used move slowly towards simplicity. contrast greater number elements move simple network faster towards randomness. relationship described reprogrammability rate simple random graphs size induces thermodynamic-­‐like asymmetry based algorithmic probability reprogrammability. graph highest algorithmic randomness elements element removals thus cannot easily moved towards greater randomness. reprogrammability landscape thus also expected related dynamical space landscape controlled effects phase space according complexity reprogrammability indices system simple connected graphs fewer attractors random graphs size. found reported main text s.i. moving connected networks towards randomness tends increase number attractors providing insights epigenetic waddington landscape tool move systems networks hitherto impossible induce perform optimal ways actual simulation. conversely moving connected networks away randomness tend reduce number attractors xtended figure network venn diagram genes occurring major papers literature covering investigations cells papers cover majority genes associated cells. linked genes figure genes found common papers. black lines show number genes found common papers genes used main figure gene enrichment analysis differentiation network. xtended figure evaluating mils using benchmark networks common literature regards ability compared state-­‐of-­‐the-­‐art network dimensionality reduction methods preserve clustering coefficient original networks removing network edges similarly mils preserved edge betweenness degree distribution information signatures better methods random edge/node deletion lowest degree node deletion. expected properties network part description. mils thus minimizes loss information maximizing preservation properties original networks. xtended figure histograms showing preservation degree distributions mils benchmark dimensionality-­‐reduction algorithm based graph spectra maximizes preservation graph eigenvalues removing edges colour green represents overlapping areas graph method. graphs used benchmarking graphs literature. extended figure qualitative reconstruction representing binary vector produces dimensional phase space runtime sample representative ecas. hamming distance binary vectors used calculate behaviour moving particle indicating state applying procedure hypothesized generating mechanism identified causal calculus find moving average predicted particle qualitatively moves similar fashion original order among lines corresponds original one. ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●● ●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●● ●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● .e-­‐ glycolysis gluconeogenesis .e-­‐ fructose mannose metabolism .e-­‐ bacterial chemotaxis .e-­‐ two-­‐component system .e-­‐ pyruvate metabolism .e-­‐ pentose phosphate pathway .e-­‐ phosphotransferase system methane metabolism .e-­‐ biosynthesis secondary metabolites .e-­‐ microbial metabolism environments peptidoglycan biosynthesis metabolic pathways sulfur relay system dioxin degradation xylene degradation phenylalanine metabolism lysine biosynthesis purine metabolism carbon pool folate ecocyc pathway superpathway glycolysis entner-­‐doudoroff sugar alcohols degradation superpathway hexitol degradation glycolysis glycolysis gluconeogenesis gluconeogenesis sugar derivatives degradation secondary metabolites degradation superpathway glycolysis pyruvate dehydrogenase glyoxylate bypass cycle glycolysis generation precursor metabolites energy sedoheptulose bisphosphate bypass entner-­‐duodoroff pathways entner-­‐doudoroff pathway cpxar two-­‐component signal transduction system signal transduction pathways methylphosphonate degradation phosphorus compounds metabolism methylphosphonate degradation pyrimidine nucleobases degradation uracil degradation uracil degradation peptidoglycan containing) peptidoglycan biosynthesis cell wall biosynthesis putrescine degradation -­‐phenylpropionate -­‐propionate degradation proline cytochrome oxidase electron transfer udp-­‐n-­‐acetylmuramoyl-­‐pentapeptide biosynthesis udp-­‐n-­‐acetylmuramoyl-­‐pentapeptide biosynthesis -­‐oxopentenoate degradation putrescine degradation pyrimidine nucleotides degradation superpathway ornithine degradation purine nucleotides novo biosynthesis superpathway purine nucleotides novo biosynthesis extended figure three clusters identified entropy proved less sensitive clustering elements axis. non-­‐ baseline nodes enriched transcription factors. ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● extended figure gene ontology over-­‐represented categories tested topgo weight method using shannon entropy. significant groups found enrichment analysis. ●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● extended figure clusters identified using compress above-­‐ baseline nodes enriched transcription factors. significant groups found enrichment analysis. extended figure unlike graph-­‐theoretic measures described single composed functions graph-­‐theoretic measures found correlate measures correlate lossless compression shannon entropy. control experiments attempts produce statistically significant clusters graph-­‐theoretic measures lossless compression shannon entropy failed tested gene ontology databases. zenil hector; kiani narsis;tegner low-­‐algorithmic-­‐complexity entropy-­‐deceiving graphs. phys. rev. stat. nonlinear soft matter phys. yosef dynamic regulatory network controlling cell differentiation. nature induction molecular signature pathogenic cells. nat. immunol. hong xing tyson mathematical model reciprocal differentiation helper cells induced regulatory cells. plos comput. biol. tuomela identification early gene expression changes human cell differentiation. blood e-­‐ ciofani validated regulatory network cell specification. cell ghoreschi generation pathogenic cells absence tgf-­‐β signalling. nature carneiro chaouiya thieffry diversity plasticity cell types predicted regulatory network modelling. gaublomme single-­‐cell genomics unveils critical regulators cell pathogenicity. cell doi./j.cell... yamane paul differentiation effector cell populations annu. rev. immunol. building blocks complex networks. science zenil kiani tegnér quantifying loss information network-­‐based dimensionality reduction techniques. <http//arxiv.org/abs/.> soler-­‐toscano zenil delahaye j.-­‐p. gauvrit calculating kolmogorov complexity output frequency distributions small turing machines. plos", "year": "2017"}