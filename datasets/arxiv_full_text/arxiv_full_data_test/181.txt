{"title": "A random version of principal component analysis in data clustering", "tag": "q-bio", "abstract": " Principal component analysis (PCA) is a widespread technique for data analysis that relies on the covariance-correlation matrix of the analyzed data. However to properly work with high-dimensional data, PCA poses severe mathematical constraints on the minimum number of different replicates or samples that must be included in the analysis. Here we show that a modified algorithm works not only on well dimensioned datasets, but also on degenerated ones. ", "text": "principal component analysis widespread technique data analysis relies covariance-correlation matrix analyzed data. however properly work high-dimensional data poses severe mathematical constraints minimum number different replicates samples must included analysis. show modified algorithm works well dimensioned datasets also degenerated ones. science today surrounded large amounts data. produced techniques instruments able measure huge number variables large number samples deposited increasing number online databases grow exponentially also modern numerical simulations produce large high-dimensional outputs. challenge growing size data concerns sciences field seen spectacular growth probably life sciences advancement genomics proteomics high-throughput technologies produced overwhelming amount data often freely available researchers. beside large number samples data also high-dimensional means sample instance typical dataset contains large number degree freedom. high-dimensionality makes visualization exploration samples datasets difficult. overcome limitations series techniques developed help researchers visualization exploration mining large data. retaining important information successful principal component analysis reinvented several times developed modern form pearson hotelling. works recalled methods section important note that classical implementation relies covariance matrix analyzed data. actually point often overlooked end-users stressed number samples needed accurately estimate covariance/correlation matrix system containing degree freedom larger otherwise covariance/correlation matrix full spurious correlations well rank deficient mathematical point view number samples less however show important functioning method particular covariance/correlation matrix rather symmetry characterizes type matrices. developed method order calculate protein crystallographic structures. proteins structurally dynamically complex objects. molecular dynamics actually level accuracy permits predict experimentally observables nowadays standard tool dynamical characterization proteins. analysis trajectories widespread high-dimensional large number different molecular conformations constitute output experiment ideal dataset pca. hand number protein structures reported protein data bank collectively large structures single protein. although possible find dozens even hundreds versions single protein number available structures incomparably smaller number degree freedom protein. used analysis thousands conformations obtained simulations classical implementation used analysis experimental structures number different conformations reported allow accurate calculation covariance matrix. perform structural analysis similar choose analyze human serum albumin available structures pdb. abundant protein plasma monomeric multi-domain molecule. non-glycosylated all-α protein chain globular heart-shaped conformation consisting three homologous domains domain composed subdomains important transport protein different binding sites able accommodate number chemically different ligands. represents main carrier fatty acids depot carrier exogenous compounds thus affecting pharmacokinetics many drugs. methods selection criteria). structural alignment α-carbon atom cartesian coordinates extracted arranged data matrix represented single structure. thus data matrix composed rows columns clearly degenerated dataset impossible obtain true correlation matrix multivariate system degree freedom using samples. calculate correlation matrix best rank deficient approximation true large number false correlations must expected. true that using careful error handling program also using algorithms estimate principal components without ever computing covariance matrix generally possible calculate first principal components classical calculable dataset. axes describe greatest variance data instead orthogonal linear transformation data could useful exploratory data analysis. relax request correlation-covariance matrix needed transformation possible important clustering tool particular matrix instead matrix belonging particular symmetry class. bases hypothesis rooted fact good models covariance matrices protein configurations obtained class symmetric random matrices. moreover consequences johnson-lindenstrauss lemma fact pearson original view important subspace axes such furnish justification. thus applied albumin dataset variant algorithm square symmetric random matrix used instead correlation-covariance refer algorithm random component analysis detailed algorithm described methods section easily customizable implementation reported supplementary information section. results analysis reported figure easily appreciated inspecting figure leads well defined clusters structures interesting cluster contains molecules bound fatty acid structures without fatty acid. cluster figure random component analysis structures. figure reports random component analysis structures contained dataset described text. structures bound fatty acids reported solid circles whereas structures without bound fatty acids reported void circles. algorithm clearly permits differentiate clusters structures dataset discriminant presence absence respectively bound fatty acids. similar cluster structures obtained random component analysis calculations carried dataset reproducible similar obtained different protocols worth noting large number structural functional works lead conclusion structures possibly related presence fatty acids discernible protein. analysis permits further clearly demonstrates discriminant structural switch whole dataset presence absence bound fatty acid. characterized larger number degree freedom respect number samples analyzed) tested also well sized datasets. retrieved machine learning repository results classical iris dataset reported figure stated least inferior clustering purposes results reproducible reported supplementary figure similar results obtained wine dataset reported supplementary figure figure iris dataset. principal component analysis random component analysis iris dataset reported. iris dataset simple classical benchmark clustering algorithms. dataset contains entries species iris virginica iris setosa iris versicolor algorithms easily differentiate iris setosa cluster whereas species partially discriminated algorithms class. full random component analysis carried iris dataset shows similar clustering results reported supplementary information. algorithm proposed communication easy implement conceptually simple numerically robust. another example useful application random matrix theory whose pervasiveness even evident large number fields. work demonstrates important clustering efficiency exact form covariance-correlation matrix instead simply symmetry algorithm. fact good informative clustering achieved random projection nowadays emerging concept that beside practical applications could reaching implications also conceptual point view. finally work suggests excessive confidence correlations large covariance avoided simple random matrix could well surrogate cluster generation. order build suitably large dataset structures searched protein data bank albumin structures constraints specie single protein type structure resolution better. initial screening nc-terminal residues often present deposited structure order include largest possible number structures complete possible ones starting ending excluded database. finally structures containing number α-carbon atoms different also excluded. final dataset contained structures reported supplementary table obtain dataset matrix form files loaded α-carbon atom coordinates extracted written text file described structure script. curly brackets text file eliminated scripting obtain data matrix readable file format numerical analysis software. using standard numerical routines orthonormal transformation matrix superscript means transposition diagonal matrix whose elements eigenvalues. empirical matrix projected onto eigenvectors give principal components. normal distributed random square matrix. algorithm could conceived version classical relaxed constraints respect matrix used calculating orthonormal reference system matrix symmetry preserved. algorithms implemented python language ipython notebook. numpy numerical software library used part scipy software package. pandas matplotlib packages used import iris wine datasets obtain graphical outputs respectively implementation algorithms reported supplementary information python format. versions algorithm reported first requires dataset dimension dummy correlation matrix arguments second requires arguments dataset random matrix used calculation orthogonal projection system. last function could useful would save particularly interesting matrix analysis. files easily customizable; provided software requires seconds download analysis proposed datasets intel core machine xeon equipped workstation running ubuntu lts. large datasets could require minutes analyzed. algorithm performs random projection preferable carry multiple runs small percentage cases algorithm projection separates samples different clusters drawback simple implementation algorithm described.", "year": "2016"}