{"title": "Deep Learning for Genomics: A Concise Overview", "tag": "q-bio", "abstract": " Advancements in genomic research such as high-throughput sequencing techniques have driven modern genomic studies into \"big data\" disciplines. This data explosion is constantly challenging conventional methods used in genomics. In parallel with the urgent demand for robust algorithms, deep learning has succeeded in a variety of fields such as vision, speech, and text processing. Yet genomics entails unique challenges to deep learning since we are expecting from deep learning a superhuman intelligence that explores beyond our knowledge to interpret the genome. A powerful deep learning model should rely on insightful utilization of task-specific knowledge. In this paper, we briefly discuss the strengths of different deep learning models from a genomic perspective so as to fit each particular task with a proper deep architecture, and remark on practical considerations of developing modern deep learning architectures for genomics. We also provide a concise review of deep learning applications in various aspects of genomic research, as well as pointing out potential opportunities and obstacles for future genomics applications. ", "text": "data explosion driven advancements genomic research high-throughput sequencing techniques constantly challenging conventional methods used genomics. parallel urgent demand robust algorithms deep learning succeeded variety ﬁelds vision speech text processing. genomics entails unique challenges deep learning since expecting deep learning superhuman intelligence explores beyond knowledge interpret genome. powerful deep learning model rely insightful utilization task-speciﬁc knowledge. paper brieﬂy discuss strengths diﬀerent deep learning models genomic perspective particular task proper deep architecture remark practical considerations developing modern deep learning architectures genomics. also provide concise review deep learning applications various aspects genomic research well pointing current challenges potential research directions future genomics applications. even since watson ﬁrst interpreted molecules physical medium carrying genetic information human beings striving gather biological data decipher biological processes guided genetic information. time human genome project launched drafted information typical human genome many genome projects including fantom encode roadmap epigenomics also launched succession. collaborative eﬀorts made abundance data available thus allowed global perspective genome diﬀerent species leading prosperity genomic research. genomic research aims understand genomes diﬀerent species. studies roles assumed multiple genetic factors interact surrounding environment diﬀerent conditions. contrast genetics deals limited number speciﬁc genes genomics takes global view involves entirety genes possessed organism example study homo sapiens involves searching approximately billion units containing protein-coding genes genes cis-regulatory elements long-range regulatory element transposable elements addition genomics becoming increasingly data-intensive advancement genomic research cost-eﬀective next generation sequencing technology produces entire readout organism. high-throughput technology made available sequencing centers cataloged omicsmaps nearly every continent vast trove information generated genomic research provides potential exhaustive resource scientiﬁc study statistical methods. statistical methods used identify diﬀerent types genomic elements exons introns promoters enhancers positioned nucleosomes splice sites untranslated region etc. addition recognizing patterns sequences models take genetic genomic information input build systems help understand biological mechanisms underlying genes. large variety data types available chromatin accessibility assays genomic assays transcription factor binding chip-seq data gene expression proﬁles histone modiﬁcations data available portals like dbgap name few. combination various data bring deeper insights genes help researchers locate information interests. hand development deep learning methods granted computational power resolve complex research questions success already demonstrated revolutionizing achievements ﬁeld artiﬁcial intelligence e.g. image recognition object detection audio recognition natural language processing etc. boom deep learning supported successive introduction variety deep architectures including autoencoders variants multilayer perceptron restricted boltzmann machines deep belief networks convolutional neural networks recurrent neural networks long short-term memory recent appearing architectures introduced later article. strong ﬂexibility high accuracy deep learning methods guarantee sweeping superiority existing methods classical tasks. intersection deep learning methods genomic research lead profound understanding genomics beneﬁt multiple ﬁelds including precision medicine pharmacy even agriculture etc. take medicine example medical research applications gene therapies molecular diagnostics personalized medicine could revolutionized tailoring high-performance computing methods analyzing available genomic datasets. also process developing drugs takes long period usually costly. save time cost general approach taken pharmaceutical companies match candidate protein identiﬁed researchers known drug molecules. beneﬁts indicate necessity utilizing powerful specially designed deep learning methods foster development genomics industry. article aims oﬀer concise overview rest article organized following ﬁrst brieﬂy introduce genomic study powered deep learning characterized deep learning architectures section additional discussions oﬀered section discuss details deep learning methods help genomic study diﬀerent application areas section followed summarization current challenges potential research directions section finally conclusions drawn section various deep learning algorithms advantages resolve particular types problem genomic applications. example cnns famous capturing features image classiﬁcation tasks widely adopted automatically learn local global characterization genomic data. rnns succeed speech recognition problems skillful handling sequence data thus mostly used deal sequence. autoencoders popular pre-training models denoising pre-processing input data. designing deep learning models researchers could take advantages merits eﬃciently extract reliable features reasonably model biological process. section review details type deep architectures focusing advantages beneﬁt speciﬁc genomic research questions. article cover standard introduction deep learning methods readers visit classical textbooks concise tutorials necessary. convolutional neural networks successful deep learning models image processing owing outstanding capacity analyze spatial information. early applications cnns genomics relied fundamental building blocks convolutional neural networks computer vision extract features. zeng described adaptation cnns ﬁeld computer vision genomics accomplished comprehending window genome sequence image. highlight convolutional neural networks dexterity automatically performing adaptive feature extraction training process. instance cnns applied discover meaningful recurring patterns small variances genomic sequence motifs. makes cnns suitable motif identiﬁcation therefore binding classiﬁcation recently cnns shown take lead among current algorithms solving several sequence-based problems. alipanahi successfully applied convolutional neural networks model sequence speciﬁcity protein binding. zhou troyanskaya developed conventional threelayer model predict genomic sequence eﬀects non-coding variants. kelley adopted similar architecture study functional activities sequence. though mulitple research demenstrated superiority cnns existing methods inappropriate structure design would still result even poorer performance conventional models zeng therefore lies center researchers master optimize ability cnns skillfully match architecture particular given task. achieve this researchers indepth understanding architectures well take considerations biological background. zeng developed parameterized convolutional neural network conduct systematic exploration cnns classiﬁcation tasks motif discovery motif occupancy. performed hyper-parameter search using mainly examined performance nine variants cnns concluded convolutional neural networks necessary deep motif discovery task long structure appropriately designed. applying convolutional neural networks genomic since deep learning models always over-parameterized simply changing network depth would account much improvement model performance. researchers attention particular techniques used cnns kernel size number feature design pooling convolution kernels choice window size input sequences etc. include prior genomic information possible. known capability processing streams data recurrent neural networks raised surge interest owning impressive results shown challenging sequential prediction problems natural language processing language translation speech recognition etc. rnns outperform cnns deep neural networks sequential data able model ordering dependence sequences memorizing long-range information network loops. speciﬁc rnns scan input sequences sequentially feed previous hidden layer current input segement model input ﬁnal output implicitly integrate current previous sequence information. besides schuster paliwal proposed bidirectional recurrent neural networks scenarios past future inputs matters. cyclic structure makes seemingly shallow long-time prediction actually deep unrolled time. resolve problem vanishing gradient rendered this hochreiter schmidhuber substituted hidden units rnns long shortterm memory units truncate gradient propagation appropriately. introduced gated recurrent units similar propose. genomics data typically sequential often considered languages biological nature. recurrent models thus applicable many scenarios. example built lstm-based neural machine translation converts task protein function prediction language translation problem understanding protein sequences language gene ontology terms. boˇza developed deepnano base calling quang proposed danq quantify function non-coding sønderby devised convolutional lstm networks predict protein subcellular localization protein sequences etc. another recent proposed seq-to-seq able variable-length input sequence another sequence ﬁxed-size prediction also promising genomic research. instance busia applied idea seq-to-seq learning model protein secondary structure prediction condition previous predicted labels. autoencoders conventionally used pre-processing tools initialize network weights extended stacked autoencoders denoising autoencoders contractive autoencoders etc. proved successful feature extraction able learn compact representation input encode-decode procedure. example gupta applied stacked denoising autoencoders gene clustering tasks. extracted features data forcing learned representation resistant partial corruption input. examples found section besides autoencoders also used dimension reduction gene expression e.g. applying autoencoders aware better reconstruction accuracy necessarily lead model improvement variational autoencoders though named autoencoders rather developed approximate-inference method model latent variables. based structure autoencoders kingma welling added stochasticity encoded units added penalty term encouraging latent variables produce valid decoding. vaes deal problems data corresponding latent representation thus useful genomic data among complex interdependencies. example rampasek goldenberg presented two-step vae-based models drug response prediction ﬁrst predicts postpre-treatment state unsupervised manner extends ﬁnal semi-supervised prediction. model based data genomics drug sensitivity cancer cancer cell line encyclopedia applications found greene greene etc. deep learning constantly showing successes genomics researchers expecting deep learning higher accuracy simply outperforming statistical machine learning methods. vast majority work nowadays approached genomic problems advanced models beyond classic deep architectures employing hybrid models. review examples recent appearing deep architectures skillfully modifying combining classical deep learning models. emergent architectures natural designs modiﬁed classic deep learning models. researchers began leverage genomic intuitions particular problem advanced suitable model. residue contact prediction. consists stack neural networks topological structures identical stack. level stacked network regarded distinct contact predictor trained supervised matter reﬁne predictions previous level hence addressing typical problem vanishing gradients deep architectures. spatial features deep spatio-temporal architecture refer original model inputs temporal features gradually altered progress upper layers. angermueller took advantage sub-models fusion module predict methylation states. sub-models take diﬀerent inputs thus focus disparate purposes. module accounts correlations sites within across cells module detects informative sequence patterns fusion module integrate higher-level features derived low-level modules make predictions. instead subtle modiﬁcations combinations works focused depth trying improve model performance designing even deeper architectures. wang developed ultra-deep neural network consists deep residual neural networks predict protein contacts sequence amino acids. residual nets model particular function. series convolutional transformations designed extracting sequential features output converted matrix operation similar outer product merged pairwise features together second residual network consists series convolutional transformations. combination disparate residual nets makes possible novel approach integrate sequential features pairwise features model. fact type deep neural networks strength inspires researchers develop hybrid architectures could well utilize potentials multiple deep learning architectures. danq hybrid convolutional recurrent deep neural network predicting function non-coding directly sequence alone. sequence input one-hot representation four bases simple convolutional neural network purpose scanning motif sites. motivated fact motifs determined extent spatial arrangements frequencies combinations sequences purported motifs learned feed blstm. similar convolutional-recurrent design discussed lanchantin demonstrated understand three deep architectures convolutional recurrent convolutional-recurrent networks veriﬁed validity features generated automatically model visualization techniques. argued cnn-rnn architecture outperforms alone based experimental results transcription factor binding site classiﬁcation task. feature visualization achieved deep gdashboard indicated cnn-rnn architecture able model motifs well dependencies among them. sønderby added convolutional layer data lstm input address problem protein sorting subcellular localization. total three types models proposed compared paper vanilla lstm lstm attention model used hidden layer ensemble vanilla lstms. achieved higher accuracy previous benchmark models predicting subcellular location proteins sequences human-engineered features involved. almagro armenteros proposed bybrid integration blstm attention mechanism fully connected layer protein subcellular localization prediction; four modules designed speciﬁc purpose. hybird models increasingly favored recent research e.g. singh applications deep learning genomic problems fully proven power. although pragmatism deep learning surprisingly successful method suﬀers lacking physical transparency well interpreted better assist understanding genomic problems. auspicious genomic research researchers done lots work visualize interpret deep learning models. besides also constructive take additional considerations beyond choice deep learning architectures. section review visualization techniques bring insights deep learning architectures remarks model design might conductive realworld applications. people expecting deep netwroks success predicting results also identifying meaningful sequence signals giving insights problems solved. interpretability model appears crucial comes application. however technology deep learning exploded prediction accuracy also complexity well. connections among network units convoluted information widespread network thus perplexing captured people carrying eﬀorts remedy pitfall since prediction accuracy alone guarantee deep architectures better choice traditional statistical machine learning methods application. gave insights function intermediate features mapping hidden layers back input deconvolution technique described paper. simonyan linearly approximate network ﬁrst-order taylor expansion obtained saliency maps convnet projecting back dense layers network. people also searched understanding genes deep networks. denas taylor managed pass model knowledge back input space inverse activation function biologically-meaningful patterns highlighted. lanchantin adopted saliency maps measure nucleotide importance. work provided series visualization techniques detect motifs sequence patterns deep learning models went discuss features extracted cnns rnns. similarly alipanahi visualized sequence speciﬁcities determined deepbind mutation maps indicate eﬀect variations bound sequences. note works conducted appropriately classic models need additional techniques visualize features e.g. p¨arnamaa parts trained -layer prediction protein subcellular localization microscopy images easily interpreted model features diﬀerent layers. work took promising steps direction uncovering mystery deep neural networks. since people long aware necessity model interpretation recent works deep learning applications usually proposed proper visualization strategies aligned model e.g. singh mikolov sønderby riesselman concept transfer learning naturally motivated human intelligence people apply knowledge already acquired address newly encountered problems. transfer learning framework allows deep learning adapt previously-trained model exploit relevant problem eﬀectively successfully applied ﬁelds language processing audio-visual recogniation readers could surveys transfer learning yang weiss addition multitask learning approach inductively share knowledge among multiple tasks. learning related tasks parallel using shared architectures learned single task auxiliary related. overview multitask learning especially focuses neural networks found ruder widmer r¨atsch brieﬂy discussed multitask learning biological perspective. early adaptation transfer learning genomics based machine learing methods svms recent works also involved deep learning. example zhang developed model analyze gene expression images automatic controlled vocabulary term annotation. pre-trained model imagenet extract general features diﬀerent scales ﬁne-tuned model multitask learning capture term-speciﬁc discriminative information. developed iterative pedla predict enhancers across multiple human cells tissues. ﬁrst pre-trained pedla data derived cell type/tissue unsupervised manner iteratively ﬁne-tuned model subsequent cell type/tissue supervisedly using trained model previous cell type/tissue initialization. cohn transferred deep parameters networks trained diﬀerent species/datasets enhancer identiﬁcation. feng adopted cnn-based multi-task learning setting borrow information across cell lines predict cell-speciﬁc binding tf-cell line combinations small portion available chip-seq data. able predict cell types models trained unsupervisedly chip-seq data available took right step direction developing domain transfer model across cell types. proposed semi-supervised multi-task framework protein-protein interaction predictions. applied classiﬁer trained supervisedly perform auxiliary task leverages partially labeled examples. loss auxiliary task loss tasks jointly optimized. wang worked problem introducing multi-task convolutional network models representation learning. zhou troyanskaya incorporated multitask approach noncoding-variant eﬀects prediction chromatin jointly learn across diverse chromatin factors. idea sharing information learnt across multiple related tasks sub-tasks could extend power limited data especially genomic data costly obtain. transfer learning also adopted tackle classimbalanced problem also since usually time-consuming train tune deep learning model transfer learning might appears encouraging well applied systematical structure eﬃcient modeling particular types genomic problems. current technology made available data multi-platform multi-view inputs heterogeneous feature sets multi-view deep learning appears encouraging direction future deep learning research exploits information across datasets capturing high level associations prediction clustering well handling incomplete data. readers visit survey multi-view methods interested. many applications approach problem diﬀerent types data computer visions audio video data available genomics area data various types assimilated naturally. example abundant types genomic data tumor samples made available state-of-the-art high-throughput sequencing technologies therefore natural think leveraging multi-view information genomics achieve better prediction single view. gligorijevi´c prˇzulj reviewed methods multi-view biological data integration instructive considerations. multi-view learning achieved example concatenating features ensemble methods multi-modal learning name few. previously mentioned ultra-deep neural network case point adopted convolutional neural networks respectively sequential features spatial features. liang proposed multi-modal integrate gene expression methylation mirna drug response data cluster cancer patients deﬁne cancer subtypes. stacked gaussian restricted boltzmann machines trained contrastive divergence diﬀerennt modalities integrated stacking hidden layers common features eﬀused inherent features derived multiple single modalities. examples found shen zhang etc. gene expression highly regulated process genetic instructions converted functional products proteins molecules also respond changing environment accordingly. namely genes encode protein synthesis selfregulate functions cell adjusting amount type proteins produce review research applied deep learning analyze gene expression regulated. increasing number genome-wide gene expression assays diﬀerent species become available public databases e.g. connectivity project launched create reference collection gene expression proﬁles used identify functionally connected molecules databases greatly facilitated computational models biological interpretation data. time recent works suggested better performance obtained deep learning models gene expression data; urda used deep learning approach outperform lasso analyzing rna-seq gene expression proﬁles data. empirical results early works applied principal component analysis gene expression data capture cluster structure showed mathematical tool eﬀective enough allow complicated biological considerations also since reliability cross-experiment datasets limited technical noise unmatched experiment conditions researchers considering denosing enhancement available data instead directly ﬁnding principal components. denoising autoencoders came hand since merely retain information data also generalize meaningful important properties input distribution across input samples. even shallow denoising autoencoders proven eﬀective extracting biological insights. danaee adopted stacked denoising autoencoders detect functional features breast cancer gene expression proﬁles data. presented unsupervised approach eﬀectively applied capture biological principles breast cancer data. adage open-source project extracting relevant patterns large-scale gene expression datasets. improved adage successfully extract clinical molecular features. build better signatures consistent biological pathways enhance model robustness developed ensemble adage integrate stable signatures across models. three similar works experimented pseudomonas aeruginosa gene expression data. addition gupta demonstrated eﬃcacy using enhanced data multi-layer denoising autoencoders cluster yeast expression microarrays known modules representing cell cycle processes. motivated hierarchical organization yeast transcriptomic machinery chen adopted four-layered autoencoder network layer accounting speciﬁc biological process gene expression. work also introduced sparsity autoencoders. edges denoising autoencoders principle component analysis independent component analysis clearly illustrated aforementioned works. works moved variational inference autoencoders assumed skillful capture internal dependencies among data. greene trained vae-based models reveal underlying patterns pathways gene expression compared three architectures dimensionality reduction techniques including aforementioned adage dincer introduced deepproﬁle framework featuring extract latent variables predictive acute myeloid leukemia expression data. shariﬁ-noghabi proposed deep genomic signature pair vaes trained unlabelled labelled data separately expression data predicting metastasis. another thread utilizing deep learning characterize gene expression describe pairwise relationship. wang showed seen eﬀective replacement frequently used pearson correlation applied pair genes therefore built multi-task consider information semantics interaction genes together extract higher level representations gene pairs classiﬁcation task extended shared-parameter networks deep learning approaches gene expression prediction outperformed existing algorithms. example chen presented three-layer feed-forward neural network gene expressions prediction selected landmark genes achieved better performance linear regression. model d-gex multi-task setting tested types expression data microarrays rna-seqs. showed deep model based sdas outperformed lasso random forests prediction gene expression quantiﬁcations genotypes. making predictions gene sequences deep learning models shown fruitful identifying context-speciﬁc roles local dna-sequence elements further inferred regulatory rules used predict expression patterns successful prediction usually rely much proper utilization biological knowledge. therefore could eﬃcient pre-analyze contextual information sequences directly making prediction. deep learning models could refer early machine learning works applies bayesian networks predict gene expression based learned motifs applications powerful deep learning algorithm paled biological restrictions. therefore instead using sequence information combing epigenetic data model might explanatory power model. example correlation histon modiﬁcations gene regulation suggested experimentally cain dong weng already studied machine learning works singh presented deepchrome uniﬁed discriminative framework stacking achieved average binary classiﬁcation task predicts high gene expression level. input seperated bins discover combinatorial interactions among diﬀerent histone modiﬁcation signals. learned region representation classiﬁer maps gene expression levels. addition singh visualized high-order combinatorial make model interpretable. examples epigenetic information utilized gene expression prediction tasks include methylation mirna chromatin features etc. generative models also adopted ability capture high-order latent correlations. example explore hypothetical gene expression proﬁles various types molecular genetic perturbation greene trained cancer genome atlas pan-cancer rna-seq data capture biologically-relevant features. another previous work evaluates vaes diﬀerent architectures provided comparison among vaes non-negative matrix factorization aforementioned adgae gene expression regulation cellular process controls expression level gene products high low. increases versatility organism allow react towards adapt surrounding environment. underlying interdependencies behind sequences limit ﬂexibility conventional methods deep networks could model over-representation sequence information potentials allow regulatory motifs identiﬁed according target sequences. eﬃcient gene expression regulation organism transcriptional level occurs early stage gene regulation. enhancers promoters well characterized types functional elements regions non-coding belong cis-regulatory elements readers visit wasserman sandelin review early approaches identiﬁcation cres. promoters locate near transcription start sites genes thereby initiate transcription particular genes. conventional algorithms still perform poorly promoter prediction prediction always accompanied high false positive rate compensate sensitivity usually achieved cost speciﬁcity render methods accurate enough applications. initial work horton kanehisa applied neural networks predict coli promoter sites provided comparison neural networks versus statistical methods. matis also applied neural networks promoter recognition albeit assisted rules gene context information predicted grail. early works deep learning models noticeable enough demonstrate clear edge weight matrix matching methods. recent study umarov solovyev used three layers well demonstrated superiority conventional methods promoter recognition distant organism.their trained model implemented application called cnnprom. latest cnn-based model enhancer prediction applied transfer learning setting diﬀerent species/datasets another highlight work lies design adversarial training data. pedla developed algorithmic framework enhancer prediction based deep learning. able directly learn heterogeneous class-imbalanced data enhancer predictor generalized across multiple cell types/tissues. model embedded mechanism handle class-imbalanced problem prior probability class directly approximated training data. pedla ﬁrst trained types data cells extended iterative scheme manages generalize predictor across various cell types/tissues. pedla also compared outperformed typical methods predicting enhancers. adopted cnns surpass previous sequence-based methods task identifying enhancers background genomic sequences. compared diﬀerent designs cnns concluded eﬀectiveness max-pooling batch normalization improving classiﬁcation accuracy also pointed simply increasing depth deep architectures useful inappropriately designed. ﬁnal model ﬁne-tuned encode cell type-speciﬁc enhancer datasets model trained fantom permissive enhancer dataset applying transfer learning. yang showed possibility predicting enhancers sequence alone presentation biren hybrid rnn. demonstrating possibility seems room improve biren techniques enables deep learning heterogenity data since biren still exhibits weaker predictive performance comparison methods consider cell-type-/tissuespeciﬁc enhancer markers explicitly. deep feature selection attempt took introduce sparsity deep architectures. conventionally sparseness achieved adding regularization term took novel approach automatically select active subset features input level reduce feature dimension. implemented additional sparse one-to-one linear layer input data input layer main model. widely applicable diﬀerent deep architectures. example demonstrated based dnns based pointed back-propagation perform well deep networks people resort stacked contractive autoencoder based models pre-trained layer-wisely greedy ﬁne-tuned back-propagation. author developed open source package illustrated superiority elastic random forest identiﬁcation enhancers promoters. implemented supervised deep learning package named decres feed-forward neural network based genome-wide detection regulatory regions. enhancer-promoter interaction predictions always based non-sequence features functional genomic signals. singh proposed ﬁrst deep learning approach infer enhancer-promoter interactions genome-wide sequencebased features well locations putative enhancers promoters speciﬁc cell type. model demonstrated superior deepfinder based machine learning hybrid model consists parts. ﬁrst part accounts diﬀerences underlying features could learned enhancers promoters thus treats enhancers promoters separately input branches branch one-layer followed rectiﬁed linear unit activation layer. second part lstm responsible identifying informative combinations extracted subsequence features. work provided insights long-range gene regulation determined sequences. last point want highlight part class-imbalanced datasets common problem enhancer promoter identiﬁcation steps taken resolve problem discussed later section e.g. singh splicing refers editing pre-messenger produce mature messenger translated protein. process eﬀectively diversity protein isoforms. predicting splicing code aims understand splicing regulate manifest functional changes proteins crucial understanding diﬀerent ways proteins produced. initial machine learning attempts included naive bayes model two-layer bayesian neural network utilized thousand sequence-based features. early applic ations neural networks regulatory genomics simply replaced classical machine learning approach deep model. example xiong adopted fully connected feed-forward neural network trained exon skipping events genome predict splicing regulation mrna sequence. applied model analyze half million mrna splicing code human genome discovered many disease-causing candidates thousands known disease-causing mutations successfully identiﬁed. case high performance mainly results proper data source rather descriptive model design. yoon presented dbn-based approach capable dealing classimbalanced data predict splice sites also identify non-canonical splice sites. also proposed training method called boosted contrastive divergence categorical gradients showed experiments ability improve prediction performance shorten runtime compared contrastive divergence methods. many cases happens phenomenon alternative splicing. single gene might coding multiple unique proteins varying exon composition mrna splicing process. post-transcriptional regulatory mechanism aﬀects gene expression contributes proteomic diversity leung developed model containing three hidden layers predict alternative splicing patterns individual tissues well across-tissue diﬀerences.the hidden variables model designed include cellular context information extract genomic features. initial works adapt deep learning splicing prediction. recent work based previously developed models design integrative deep learning models alternative splicing. viewed previous work baseline original dataset developed models integrating additional types experimental data proposed target function. modtranscription factors refer proteins bind promoters enhancers sequence rna-binding proteins name suggested crucial regulatory elements biological processes. current high-throughput sequencing techniques selecting candidate binding targets certain restricted eﬃciency high cost ching researchers seeking computational approaches binding sites prediction sequences initially utilized consensus sequences alternative position weight matrices later machine learning methods using kmer features surpassed previous generative models. many existing deep learning methods approach transcription factor binding site prediction tasks convolutional kernels. alipanahi showed successful using models large scale problem tfbs tasks. chen combined advantage representation learning explicity reproducing kernel hilbert space introduce convolutional kernel networks predict transcription factor binding site interpretibility. zeng conducted systematic analysis architectures predicting sequence binding sites based large transcription factor datasets. lanchantin explored cnns rnns combination task tfbs comprehensive discussion visualization techniques. admittedly cnns well capture sequential spatial features sequences recurrent networks well bidirectional recurrent networks useful accounting motifs directions sequence. motivated symmetry double-stranded means identical patterns appear strand reverse complement shrikumar proposed traditional convolution-based model shares parameters forward reverse-complement versions sequences shown robust vivo tfbs prediction tasks using chromatin chip-seq data. novel work tailors conventional neural network consider motifs bidirectional characterizations. addition convolutional neural networks proved powerful long appropriately designed according speciﬁc problem approaches deal diﬀerent feature extraction multiple data sources. cross-source data usually shares common knowledge higher abstraction level beyond basic observation thus need integrated model. zhang proposed multi-modal deep belief network capable automatic extraction structural features sequences; ﬁrst successfully introduce tertiary structural features sequences improve prediction rna-binding proteins interaction sites. another multi-modal deep learning model purpose developed shen model consists dbns cnns integrate lower-level representations extracted diﬀerent data sources. zhang based gapped k-mers frequency vectors extract informative features. gkm-fvs normalization taken input multi-layer perceptron model trained standard error back-propagation algorithm mini-batch stochastic gradient descent. taking advantages gapped k-mer methods deep learning gkm-dnn achieved overall better performance compared gkm-svm. feng proposed cnn-based model utilizes domain adaptation methods discussed detailed section predict cell types models trained unsupervisedly chip-seq data available. shortcomings previous approaches predicting functional activities sequences insuﬃcient utilization positional information. though ghandi upgraded k-mer method introducing alternative gapped k-mers method improvement remarkable since sequence still simply represented vectors k-mer counts without considering position segment sequence. though position-speciﬁc sequence kernels exist sequence much higher dimension space thus eﬃcient enough contrast conventional methods deep learning methods cnns naturally account positional relationships sequence signals computational eﬃcient. kelley presented open-source cnn-based package trained genomics data cell types remarkably improved prediction functional activities sequences. basset enables researchers perform single sequencing assay annotate mutations genome present chromatin accessibility learned time. zhou troyanskaya contributed another open-source deep convolutional network predicting genomic sequence functional roles non-coding variants histone modiﬁcations tfbs accessibility sequences high nucleotide resolution. eﬀects mutations usually predicted site independent pairwise models approaches suﬃciently model higher-order dependencies. riesselman took generative approach tract mutation eﬀects beyond pairwise biologically-motivated beyasian deep latent networks. introduced latent variables depend visualized model parameters illustrate structural proximity amino acid correlations captured deepsequence. subcellular localization predict subcellular compartment protein resides cell biological sequence. order interact other proteins need least temporarily inhabit physically adjacent compartments therefore knowledge protein location sheds light protein might function well proteins might interact previous methods rely support vector machines involve hand-generated features. example shatkay integrated diﬀerent sequence text-based features pierleoni developed hierarchy binary svms. meinken reported previous tools covered machine learning approaches subcellular localization. early deep learning works shifted svms neural networks emanuelsson hawkins bod´en mooney based n-to- neural network develop subcellular localization predictor sønderby adopted lstm predict protein subcellular locations sequence information high accuracy. enhanced model adding convolutional ﬁlters lstm motif extractor introducing attention mechanism forces lstm focus particular segments protein. validity convolutional ﬁlters attention mechanisms visualized experiments. almagro armenteros proposed similar integrative hybrid model deeploc consisting four modules including blstm attention scheme fully connected dense layer. high-throughput microscopy images rich source biological data remain better exploited. important utilization microscopy images automatic detection cellular compartment. p¨arnamaa parts devised eleven-layer deep model ﬂuorescent protein subcellular localization classiﬁcation yeast cells eight convolutional layers succeeded three fully connected layers. internal outputs model visualized interpreted perspective image characteristics. author concluded low-level network functions basic image feature extractor higher layers account separating localization classes. proteins usually share structural similarities proteins among common evolutionary origin classiﬁcation protein structure tracked back aiming comprehend process protein folding protein structure evolution grouping proteins structural functional categories also facilitate understanding increasing number newly sequenced genome. early methods similarity measures mostly rely sequence properties fasta blast psi-blast upgraded leveraging proﬁles derived multiple sequence alignments position-speciﬁc scoring matrices addition sequences discriminative models like example cang adopted topological approach utilizing persistent homology extract features classiﬁcation protein domains superfamilies. top-performing deep learning works also rely protein homology detection review) deduce structure function protein amino acid sequence. hochreiter suggested model-based approach uses lstm homology detection. model makes similarity measures blosum matrices priori ﬁxed instead suitably learned lstm regard speciﬁc classiﬁcation task. conducted similar work protein remote homology detection showed improvement using blstm instead lstm drawback homology based approaches fold recognition lack direct relationship protein sequence fold since current methods substantially rely fold known template protein classify fold proteins therefore proposed deep fold classiﬁcation directly protein sequences. also works base available gene function annotation vocabularies perform protein classiﬁcation similar motivation biovec designed deep learning method compute distributed representation biological sequences general genomic applications protein family classiﬁcation. sequence embedded high-dimension vector biovec classiﬁcation protein families reduced simple classiﬁcation task. protein secondary structure refers form local segments proteins informative studying protein structure function well evolution. protein traditionally subdivided states alternatively ﬁne-grained states dssp algorithm evaluate model performance aforementioned -state -state prediction accuracy always calculated represents percentage correctly predicted secondary conformation amino acid residues. alternative measure -state prediction segment overlap score resonable goal prediction suggested rost accuracy deep learning became popular protein prediction machine learning approaches including probabilistic graphical models hidden markov models svms widely adopted. nascent neural networks earliest applications developed shallow feed-forward network predicts protein homology amino acid sequences works prediction adopted similar slightly enhanced neural networks qian sejnowski conducted inﬂuential works -state prediction reaching accuracy based fully connected neural networks develop cascaded architecture taking input window sequences orthogonal encoding. signiﬁcant progress -state prediction accuracy neural networks improved rost sander claimed marginal inﬂuence free parameters model rost sander accredited improvement leveraging evolutionary information encoded input proﬁles derived multiple alignments. riis krogh achieved practically identical performance structured neural network. designed speciﬁc networks class according biological knowledge output prediction made ﬁltering ensemble averaging. based pssm generated psi-blast jones used -stage neural network obtain average score around popular deep learning methods bidirectional recurrent neural networks also widely applied protein prediction emergent deep architectures protein prediction widely explored prior knowledge various features available. faraggi proposed iterative six-step model neural network step follows similar structure designed speciﬁc purpose. spencer trained deep belief network model additional hidden layer constructed facilitate unsuperivsed layer-by-layer initialization restricted boltzmann machine designed cascaded model leverages extracts multi-scale local contextual features diﬀerent kernel size added brnn accounting long-range dependencies amino acid sequences capture global contextual features. wang took large step improving accuracy extending conditional neural ﬁelds include convolutional designs. deepcnf able capture sequence-structure relationships protein label correlation among adjacent residues. also achieved accuracy around outperforming accuracy obtained supervised generative stochastic network busia explored model performance -stated prediction simple feed-forward networks adaptation recent architectures modiﬁed convolution operators diﬀerent scales residual connections successful models computer vision suit protein prediction task also highlighted diﬀerences compared vision tasks. opposed above-mentioned deepcnf included interdependencies between labels adjacent residues conditional random field busia condition current prediction previous predicted labels sequence-to-sequence modeling. prediction protein tertiary structure proven crucial huamn’s understanding protein functions applied instance drug designs however experimental methods determining protein structures x-ray crystallography costly sometimes impractical. though number experimentally solved protein structures included protein data bank keeps growing account small proportion currently sequenced proteins thus potentially practical approach number known protein sequences number found protein structures computational modeling. essential challenges protein structure prediction include sampling ranking protein structural models quality assessment predict absolute relative quality protein models native structure available rank them. previous research conducted based machine learning models. recent deep learning-based work achieved substantial improvement replacing svms previous work dnns. opposed existing methods rely energy scoring functions nguyen based solely geometry propose sparse stacked autoencoder classiﬁer utilizes contact map. another research adopted deep belief network protein structure prediction. model could used evaluate quality protein decoy. local quality assessment remains substantially improved compared global prediction introduced three models based stacked denoising autoencoders benchmark deep learning methods assessing quality individual protein models. protein contact binary matrix denoting spatial closeness residues folded protein structure. predicting residue-residue contact thus curcial protein structure prediction early studied shallow neural networks recent works proceeded deeper networks. lena stacked together multiple standard three-layer feedforward network sharing topology taking consideration spatial temporal features predict protein residueresidue contact. wang also developed ultra-deep model predict protein contacts amino acids sequence. model consists deep residual neural networks process features separately subsequently order consider sequential pairwise features whole model. zhang schreiber contributed open-source multi-modal model hi-c contact prediction. zhang ﬁrst interpolated low-resolution hi-c matrix size high trained model predict highlowresolution matrix. ﬁnal output recombined entire hi-c interaction matrix. schreiber predicted hi-c contacts high resolution nucleotide sequences dnasei assay signal data. model consists arms processing type data independently. learned feature maps concatenated combination genomic distance dense layers. discussion successes applications deep learning genomics proceed discuss current challenges. deep learning models usually over-parametrized performance conditional models appropriately designed according problem. multiple worthwhile considerations techniques involving model architectures feature extraction data limitation etc. help deep learning models better approach genomics. brieﬂy discuss current challenges deserve attention several potential research directions might shed light future development deep learning applications genomic research. inevitable challenge transferring success deep learning conventional vision text data genomics raised nature genomic data unavailability true labels lack knowledge genetic process imbalanced case control samples rarity certain disease heterogeneity data expensiveness large-scale data collection. large-scale biological data gathered assorted sources usually inherently classimbalanced. take epigenetic datasets example nature much fewer methylated regions sites non-dmr sites also common enhancer prediction problem number non-enhancer classes overwhelmingly exceeds enhancer classes dataimbalance issue also encountered machine learning methods ensemble methods appear powerful applied undersampling method together majority vote address imbalanced data distribution inherent gene expression image annotation tasks. deep learning approaches al-stouhi reddy based boosting propose instance-transfer model reduce class-imbalanced inﬂuence also improve performance leveraging data auxiliary domain. addition resorting ensemble approaches researchers manage resolve class-imbalanced problem model parameters training process. instance used embedded mechanism utilizing prior probability class directly estimated training data compensate imbalance classes. yoon presented method called boosted contrastive divergence categorical gradients training rbms class imbalanced prediction splice junctions. singh performed data augmentation slightly shifting positive promoter enhancer within window since true label sensitive minimal changes. also designed training procedure accordingly avoid high false positive rate resulting augmented dataset. intuitively integrating diverse types data discriminating features lead predictive power models. example trained model nine types data identify enhancers including chromatin accessibility cofactors histone modiﬁcations transcription methylation sequence signatures evolutionary conservation islands occupancy tfbss resulting better model performance terms multiple metrics compared existing popular methods. predicted single-cell methylation states disparate sub-networks designed accordingly sites sequences. pays manage utilize data multiple views; though merging information various data sources challenge models could well integrate them eﬀort might provide information great chance. discussions encompassing diverse data sources refer multi-view learning section data genomic applications involving medical clinical heterogeneous population subgroups regional environments. problems integrating diﬀerent types data underlying interdependencies among heterogeneous data. covariates sometimes confounding render model prediction inaccurate. genome-wide association study example populationbased confounders individual relatednesses produce spurious correlation among snps trait interest. existing statistical methods estimate confounders performing causal inference. methods based linear regression linear mixed model others wang tried upgrade tested biological variable selection prediction tasks. though lmm-based models favored researchers mathematically suﬃcient power pales facing multiple nonlinear confounding correlations. assumed guassian noise might overshadows true underlying causals also fails literally model variable correlations. seemingly reliable approach generative modeling e.g. tran blei louizos based variational inference present implicit causal models encoding complex nonlinear causal relationships consideration latent confounders. tran blei optimized model iteratively estimate confounders snps simulation study suggested signiﬁcant improvement. methodology perspective several deep learning methods designed exclusively confounder correction domain adversarial learning select-additive learning confounder ﬁltering re-used identiﬁcation confounder presented. deep learning performs automatic feature extraction saves great eﬀorts choosing hand-engineered features torng altman also discussed superiority automatically generated features manually selected features. however practice unfortunately time-consuming directly learning features genomic sequences complex interdependences long range interactions taken consideration. researchers might still resort task-speciﬁc feature extraction automatic feature detection could strongly facilitate model skillfully designed. techniques borrowed mathematics great potentials interpret complex biological structures behind data otherwise hinder generalization deep learning. example topology promising choice untangle geometric complexity underlying biomolecular structure protein homology detection widely applied protein classiﬁcation problems deepmethyl developed deep learning software using features derived genome topology sequence patterns. based stacked denoising autoencoders applied predict methylation states dinucleotides. cang introduced element speciﬁc persistent homology convolutional neural networks predict protein-ligand binding aﬃnities protein stability changes upon mutation including globular protein mutation impacts membrane protein mutations impacts. conceptual analogy fact humans communicate languages biological organisms convey information within cells information encoded biological sequences. understand language life asgari mofrad designed biovec unsupervised data-driven feature representation method embeds trigram biological sequence -dimensional vector characterizes biophysical biochemical properties sequences. biovec trained variant adapted wordvec typical method natural language processing. furhter utilized shallow two-layer neural networks compute representation variable-length k-mers sequences consistent across diﬀerent lengths. contrast representation biovec individual kmers kimothi based docvec algorithm extension wordvec proposed distributed representation complete protein sequence successfully applied protein classiﬁcation following settings asgari mofrad types feature representation potential facilitate work genomics. genomics challenging application area deep learning encounters unique challenges compared ﬁelds vision audio text processing since limited abilities interpret genomic information expect deep learning superhuman intelligence explores beyond knowledge. deep learning undoubtedly auspicious direction constantly rejuvenated moved forward genomic research recent years. discussed review recent breakthroughs deep learning applications genomics surpassed many previous state-of-the-art computational methods regard predictive performance though slightly behind traditional statistical inferences terms interpretation. cuurent applications however brought watershed revolution genomic research. predictive performances problems reach expectation real-world applications neither interpretations abstruse models elucidate insightful knowledge. plethora deep learning methods constantly proposed awaits artful applications genomics. careful selection data sources features appropriate design model structures deep learning driven towards bright direction produces accurate interpretable prediction. need bear mind numerous challenges beyond simply improving predictive accuracy seek essential advancements revolutions deep learning genomics.", "year": "2018"}