{"title": "An Unsupervised Homogenization Pipeline for Clustering Similar Patients  using Electronic Health Record Data", "tag": "q-bio", "abstract": " Electronic health records (EHR) contain a large variety of information on the clinical history of patients such as vital signs, demographics, diagnostic codes and imaging data. The enormous potential for discovery in this rich dataset is hampered by its complexity and heterogeneity.  We present the first study to assess unsupervised homogenization pipelines designed for EHR clustering. To identify the optimal pipeline, we tested accuracy on simulated data with varying amounts of redundancy, heterogeneity, and missingness. We identified two optimal pipelines: 1) Multiple Imputation by Chained Equations (MICE) combined with Local Linear Embedding; and 2) MICE, Z-scoring, and Deep Autoencoders. ", "text": "homogeneous non-redundant complete data. however data heterogeneous redundant incomplete noisy additionally human errors system biases also contribute measurement errors data. thus fully utilize reliably detect disease subtypes clustering techniques must paired pre-processing techniques normalize reduce complexity data. clustering pipeline including pre-processing steps previously proposed validated. paper assess propose optimal clustering pipeline robust nuisances data. pipeline consists imputation normalization feature reduction clustering. multiple commonly used techniques evaluated step best performing pipeline selected. since accuracy clusters real applications cannot measured lack ground truth assessed accuracy using simulated data ground truth could easily deﬁned. best knowledge ﬁrst study propose validate unsupervised homogenization pipeline clustering. simulated patient encounters sample generator mimics redundancy heterogeneity data. deﬁned rows patient encounters columns measurements taken patient designed three clusters samples cluster observed dimensionality effective dimensionality sample generator drew independent samples multivariate normal distribution form matrix nn×. then separated clusters shifting samples time. ﬁrst samples stayed origin next shifted forming last shifted abstract— electronic health records contain large variety information clinical history patients vital signs demographics diagnostic codes imaging data. enormous potential discovery rich dataset hampered complexity heterogeneity. present ﬁrst study assess unsupervised homogenization pipelines designed clustering. identify optimal pipeline tested accuracy simulated data varying amounts redundancy heterogeneity missingness. identiﬁed optimal pipelines multiple imputation chained equations combined local linear embedding; mice z-scoring deep autoencoders. health trajectory patients. diagnosis also helps predict treatments highest likelihood improving patients health. granular diagnosis speciﬁc precise medicine become. wealth medical data gathered patients digitally available electronic health record support highly granular diagnoses. unfortunately current clinical paradigm human physician wading vast data cannot deliver promise precision medicine. fortunately advances machine learning harnessed sift rich dataset extract useful information facilitate human decisions. popular application phenotyping cluster analysis. previous studies shown clustering algorithms potential classify patients similar phenotypes based data contained medical record. example using unbiased hierarchical cluster analysis penalized modelbased clustering shah identiﬁed phenotypes patients diagnosed heart failure preserved ejection fraction. upon identiﬁcation granular homogeneous clusters outcomes attempted therapies within cluster linked together predict likely outcomes resulting choosing particular therapies. heuristically resulted good performance pipelines. baseline used identify best performing pair normalization feature reduction methods used rest experiments. simulated four scenarios testing pipeline robustness various levels severity. experiments swept simulation parameter keeping others constant. measured adjusted rand-score computes similarity measure results sets labels counting pairs assigned different clusters predicted true clusterings adjusting random chance. table describes experimental setup default parameters. every experiment times extract mean standard deviation performance. effect size manipulated effect size varying distance cluster centers dimensions calculate number overlapped samples counting number samples beyond distance bivariate standard normal distribution. then triangular setting number overlapped samples would times calculated amount. conducting monte carlo simulation convert effect sizes percentage overlapped samples interpreted lowerbound error cluster assignment. redundant features assessed robustness number redundant features present dataset increasing dimensionality keeping groundtruth dimensionality simulated projection matrices generated features. uninformative/noisy features data contain information useful determining clusters similar patients. assessed effects including noninformative variables appending random continuous random binary variables. baseline experiment revealed performance clustering pipeline heavily depended choice normalization feature reduction method .dae dnae paired best z-scoring scores isomap performed best whitening spectral embedding obtained best performance scaling used. used optimal pairs conduct remainder experiments. enforced heterogeneity quantizing half variables chosen random scaling continuous feature random factor finally added gaussian noise every element data matrix mimic measurement errors. imputation tested median imputation median value valid samples complete missing values; k-nearest neighbors average value k-nearest samples used; multiple imputation chained equations missing values predicted based regression models complete samples. normalization continuous variables tested zscore every variable zero mean unit variance; minmax normalizes range; whitening feature space linearly projected inter-feature covariance identity matrix. feature reduction propose deep autoencoders denoising autoencoders feature reduction. autoencoders trained reconstruct input encoding decoding networks. dnae case noise added encoded units enforce robustness measurement noise. designed network architechture hyperparameter search layers hidden encoding units. network least number encoding units achieves reconstruction error less preferred. encoding vectors represent data compressed continuous vector suitable clustering technique. comparison evaluated methods local global neighbor algorithms well afﬁnity matrix algorithms spectral embedding multidimensional scaling table baseline results identifying best scaling feature reduction method. entries show average score standard deviation scores across repetitions. missingness shown fig. levels missingness signiﬁcantly impaired clustering performance pipeline conﬁgurations among three imputation methods mice resulted best performance feature reduction methods except isomap marginally better median imputation consistently worst performance. effect size expected performance conﬁgurations increased effect size overall three performing feature reduction methods mds. exhibited best performance across feature reduction methods marginally better e.g. p-value paired t-test effect size features essentially immune large amounts redundant features dnae appeared similarly immune levels performance sharply decreased greater features. conversely spectral embedding beneﬁted higher numbers redundant features performed best methods redundant features. isomap performed poorly levels uninformative/noisy features shown fig. methods except dnae isomap immune large amounts uninformative variables. dnae robust uninformative variables continuous binary uninformative variables. isomap tolerate even minimum number uninformative variables. following robustness experiments identiﬁed best performing feature reduction methods overall. compare methods performed subsequent experiments allowed interactions varying effect sizes missingness noise. overall matched outperformed dae. effect size noise experiment large amounts uninformative variables medium effect sizes favored lle. effect size missingness experiment showed signiﬁcantly better performance medium effect sizes missingness difference large effect sizes medium missingness. performances commonly used techniques. found pipelines outperform alternatives mice imputation feature reduction; mice imputation z-score normalization feature reduction. pipelines robust missingness uninformative noise large numbers redundant features performs slightly better smaller effect size. ﬁrst study present unsupervised homogenization pipeline designed clustering. normalization data heterogeneous containing categorical continuous variables different scales. normalization recommended reduce variance among variables. previous studies normalized variables range however shown table best normalization method closely related feature reduction method. example dnae z-score normalization results best performing pipelines normalization necessary lle. reasonable since unlike distancebased algorithms neighbor-based algorithms eliminate need estimate distance objects. imputation given wide array measurements obtained patients missing data common impossible every patient every possible test measurement. physicians evaluate cost-beneﬁt test request particular test result informative diagnosis treatment. evaluated spectrum imputation techniques could induce different levels artiﬁcial similarity. simulation results favored mice feature reduction methods except isomap. consistent studies mice also shown good performance life-history/ehr datasets previous studies main assumptions mice non-missing values predictive missing ones data missing-at-random. data satisﬁes redundancy assumption example height known good predictors aortic root diameter white note mice sensitive departures missing-at-random assumption. however assumptions relaxed long dataset contains enough complete samples build reliable predictive models. theoretically data likely follow missing random missing random mechanism likely reason missing values however true pattern missingness likely inﬂuenced mnar. hence mice still applied given ponents data clustering patients. mice imputation features zscore normalization show good clustering results. marginally outperformed several direct comparisons computational efﬁciency evaluating observations based largescale data provides important advantage. future studies required evaluate compare pipelines real clinical scenarios large-scale data. guan jiang zheng chen zhong unsupervised learning technique identiﬁes bronchiectasis phenotypes distinct clinical characteristics int. journal tuberculosis lung disease vol. shah katz selvaraj burke yancy gheorghiade bonow c.-c. huang phenomapping novel classiﬁcation heart failure preserved ejection fraction circulation vol. katz aguilar selvaraj martinez beussink-nelson k.-y. peng irvin tiwari phenomapping identiﬁcation hypertensive patients myocardial substrate heart failure preserved ejection fraction journal cardiovascular trans. research jain data clustering years beyond k-means pattern hinton salakhutdinov reducing dimensionality data neural networks science vol. vincent larochelle bengio p.-a. manzagol extracting composing robust features denoising autoencoders proceedings international conference machine learning. roweis saul nonlinear dimensionality reduction locally linear embedding science vol. tenenbaum silva langford global geometric framework nonlinear dimensionality reduction science vol. penone davidson shoemaker marco rondinini brooks young graham costa imputation missing data life-history trait datasets approach performs best? methods ecology evolution vol. fig. interaction experiments. gray areas denote neither method scored white areas denote signiﬁcant difference score means. colored areas denote signiﬁcant differences dae. feature reduction contains many redundant pieces information. example body mass index easily computed height weight. thus necessary reduce redundancy extract effective features high dimensional dataset. simulation results show among different feature reduction methods pipelines show highest accuracy. moreover outperforms medium effect size high uninformative noise. suggests might better detecting granular phenotypes overlapped samples additionally another beneﬁt using normalization input data needed discussed above. however compared computationally efﬁcient log) denotes number neighbors lle. network trained weights applied dataset minimal computation computes sorts distances neighbors. thus considering large-scale nature data might better choice used make predictions future patients. recent studies deep auto-encoders demonstrated ability identify meaningful representations data miotto ﬁrst proposed deep autoencoders data called representation deep patient demonstrated utility assessing probability patients developing various diseases showing improvement classiﬁcation scores patients different diseases. similarly beaulieu-jones reported improved classiﬁcation scores amyotrophic lateral sclerosis diagnosis clinical trials using patients promising results demonstrate potential proposed pipeline utilize data identify granular disease phenotypes ultimately facilitate precise diagnoses risk prediction treatment strategies. moreover previous studies shown promise ﬁrst study validate design entire pipeline clustering. devereux simone arnett best boerwinkle howard kitzman mosley weder normal limits relation body size gender twodimensional echocardiographic aortic root dimensions persons≥ years american journal cardiology vol.", "year": "2017"}