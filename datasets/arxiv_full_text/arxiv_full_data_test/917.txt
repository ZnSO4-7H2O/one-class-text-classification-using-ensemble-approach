{"title": "Reaching Optimized Parameter Set, Protein Secondary Structure Prediction  Using Neural Network", "tag": "q-bio", "abstract": " We propose an optimized parameter set for protein secondary structure prediction using three layer feed forward back propagation neural network. The methodology uses four parameters viz. encoding scheme, window size, number of neurons in the hidden layer and type of learning algorithm. The input layer of the network consists of neurons changing from 3 to 19, corresponding to different window sizes. The hidden layer chooses a natural number from 1 to 20 as the number of neurons. The output layer consists of three neurons, each corresponding to known secondary structural classes viz. alpha helix, beta strands and coils respectively. It also uses eight different learning algorithms and nine encoding schemes. Exhaustive experiments were performed using non-homologues dataset. The experimental results were compared using performance measures like Q3, sensitivity, specificity, Mathew correlation coefficient and accuracy. The paper also discusses the process of obtaining a stabilized cluster of 2530 records from a collection of 11340 records. The graphs of these stabilized clusters of records with respect to accuracy are concave, convergence is monotonic increasing and rate of convergence is uniform. The paper gives BLOSUM62 as the encoding scheme, 19 as the window size, 19 as the number of neurons in the hidden layer and One- Step Secant as the learning algorithm with the highest accuracy of 78%. These parameter values are proposed as the optimized parameter set for the three layer feed forward back propagation neural network for the protein secondary structure predictionv ", "text": "abstract propose optimized parameter protein secondary structure prediction using three layer feed forward back propagation neural network. methodology uses four parameters viz. encoding scheme window size number neurons hidden layer type learning algorithm. input layer network consists neurons changing corresponding different window sizes. hidden layer chooses natural number number neurons. output layer consists three neurons corresponding known secondary structural classes viz. helix β-strands coil/turns respectively. also uses eight different learning algorithms nine encoding schemes. exhaustive experiments performed using non-homologues dataset. experimental results compared using performance measures like sensitivity specificity mathew correlation coefficient accuracy. paper also discusses process obtaining stabilized cluster records collection records. graphs stabilized clusters records respect accuracy concave convergence monotonic increasing rate convergence uniform. paper gives blosum encoding scheme window size number neurons hidden layer onestep secant learning algorithm highest accuracy parameter values proposed optimized parameter three layer feed forward back propagation neural network protein secondary structure prediction. paper deals prediction protein secondary structure described four sections. remaining part section deals different online databases different techniques secondary structure prediction multilayer feed forward neural network. section describes materials methods used work. section discusses experimental results analysis. section gives conclusion. protein data obtained experimental approaches like xray nuclear magnetic resonance electron microscopy data stored different databases based characteristics. databases range simple sequence repositories curated databases. simple sequence repositories store data proteins made simple building blocks called amino acids consist carbon atom primary amino group carboxylic acid group side chain atom attached shown figure numerous amino acids nature proteinogenic. proteins organised four different structural levels primary secondary tertiary quaternary structures. primary structure refers amino acid sequence protein. provides foundation types structures. secondary structure refers arrangement connections within amino acid groups form local structures. –helix β-strands coil/turns examples these. tertiary structure three dimensional folding secondary structures polypeptide chain. quaternary structure formed interactions several keywords multi layer feed forward network learning algorithm proteins hidden neuron encoding scheme performance measures secondary structure prediction. finding proteins make organism understanding functions foundation molecular biology tertiary structure proteins derive properties well function organism. secondary structure prediction secondary structure predicted primary sequences essential intermediate step prediction structures. predict different secondary structure proteins primary sequences. computational methods used achieve secondary structure predictions include artificial neural networks support vector machines statistical methods nearest neighbor methods computational techniques overcome difficulties faced biochemical biological approaches protein secondary structure prediction. these artificial neural network often used method. review literature computational techniques secondary structure prediction using neural network indicates multilayer feed forward neural networks preferred effective tool multilayer feed forward neural network consists input layer output layer least hidden layer. layers interconnected shown figure number units known neurons layer depends upon problem study. unit input layer supplies signal every unit first hidden layer. output transformed transformation function passed units next hidden layer units output layer depending upon number hidden layers thus connected units form network. connection units weight attached amount change network determined according error correction-learning algorithm. network trained create input-output model correct mapping unseen inputs outputs predicted successfully many approaches multi layer feed forward neural networks. however multilayer feed forward back propagation networks efficient ones back propagation learning technique supervised learning technique units different layers undergo passes viz. forward pass backward pass. forward pass synaptic weights fixed signal given unit input layer propagated layer layer little manual intervention creation records curated databases store annotated sequence data. various types online protein databases details shown table repository protein amino acid sequences consist name/description taxonomic data citation information curated database protein sequence alignments. contains protein sequence motif determination searches describe sequence motif definitions protein domains families functional patterns compendium protein fingerprints. fingerprint group conserved motifs used characterise protein family. contains multiplealignment conserved regions protein families. repository protein families domain contains structure data determined x-ray crystallography repository protein domain families despite growing number protein sequences secondary tertiary structures unveiled. uniprotkb/trembl repository protein sequences currently around sequence entries protein data bank registers structure proteins biochemical biological point view shortage methodology proposed predict secondary structure protein shown figure different stages starting collection data calculation performance measures discussed following sections. data used study oldest dataset used protein secondary structure prediction. scheme created rost sander consists sequences average sequence length residues. dataset collected supplementary data files previous research study. besides also obtained online databases pdb. data collected structured rows protein name primary secondary structures. primary structure sequence amino acids represented letter code. secondary structure proteins represented three structural classes namely α−helices β−strands coil/turns rest represented dash used cuff barton sample data used shown table data encoding proposed convert amino acids represented single letter code numerical equivalent. facilitate data used neural network framework. different encoding schemes used work orthogonal hydrophobicity blosum hybrid viz. orthogonal+hydrophobicityblosum hydrophobicity orthogonal blosum hydrophobicity orthogonal pam. schemes offers matrix representation given primary sequence number rows corresponding length sequence number columns corresponding number different amino acids. schemes vary according entry matrix calculated. following subsections give brief discussion encoding schemes implemented work. reaches unit output layer. actual output compared expected output difference known error propagated back. backward pass synaptic weights adjusted according error correction rule often used generalized delta rule. process continued iteratively series forward backward passes network gives desired response output. process continued number input-output pairs train network. typical multi layer feed forward back propagation network single hidden layer secondary structure prediction assumes number parameters. parameters data encoding scheme window size number hidden neurons type learning algorithms. encoding scheme arranges input data format passed neural network. however input data cannot directly supplied network. structured parts achieved using sliding window protocol. assuming single hidden layer network number neurons hidden layer affects performance neural network substantially. training network important aspect neural network realized using different tried tested learning algorithms. survey literature secondary structure prediction using multi-layer feed forward back propagation neural networks shows highest accuracy obtained around small used attempts increase value performance using pre-processing strategies incorporating heuristic information using hybridization neural network computational techniques proposed work fine-tuning techniques objective find best parameter conventional setup. related works attempt predict secondary structure usually changing parameters authors best knowledge find work considers changes three parameters. method proposed work considers changes four parameters. uses nine data encoding schemes window size ranges three nineteen twenty different hidden neurons eight learning algorithms offers optimized parameter exhaustive space ***= search points. validated experimental results using five different performance measures. related works vary performance measures problems lesser search complexity. optimized parameter proposed also incorporates distinct behaviour determined five different performance measures. hydrophobicity encoding scheme suggested radzicka wolfenden uses hydrophobicity index given table amino acids. according this hydrophobicity matrix created wherein entries calculated formula given blosum substitution matrix shown table provides ‘logodds’ score possibility given pair amino acids interchanging other. blosum encoding scheme uses substitution matrix representation amino acid sequence. accordingly amino acid corresponding blosum substitution matrix identified placed representation amino acid encoded matrix. example amino acid sequence represented encoding scheme uses mutation matrix representation amino acids follows strategy used blosum encoding scheme. example amino acid sequence described replaced four encoding schemes discussed captures certain characteristics amino acids present given sequence represented matrix. order features amino acids incorporated propose hybrid encoding encoding schemes adding corresponding matrices suggested though four encoding schemes offer hybrid encoding schemes choosing combinations schemes method avoids hybrid blosum inappropriate inherent biological nature. thus addition four encoding schemes method proposes following five hybrid encoding schemes also. encoding schemes discussed give input data matrix format columns number rows corresponding length sequence. input data subdivided sliding window protocol. protocol helps designing dynamic neural network architecture number units input layer created corresponding window size. window size number amino acid centre window predicted. method uses window size ranging this given input data matrix hundred rows twenty columns chosen window size assume three input layer three units created network. first three rows matrix supplied input units. subsequently window slides amino acid collection three rows matrix supplied iteratively shown figure similarly neural network architecture number units input layer created corresponding respective window sizes. standard steepest descent learning algorithm. adopts adaptive learning rate incorporates momentum factor. learning algorithm becomes responsive fluctuations local error newton method class hill-climbing technique looks stationary point error function. necessitates creation matrix called hessian matrix whose entries second derivatives error function respect weights current positions. however calculation hessian matrix requires space time. quasi-newton method uses approximation instead actual calculation second order derivatives. work uses types quasi-newton algorithms broyden–fletcher–goldfarb–shanno algorithm one-step secant algorithm. bfgs uses approximation second order derivative suggested requires storage space converges faster conjugate gradient methods. onestep secant algorithm uses identity matrix hessian matrix previous iteration. reduces requirement storage space limitation bfgs algorithm. however requires little storage space conjugate gradient algorithms. method uses different types learning algorithms. different classes learning algorithms like conjugate gradient algorithms heuristic algorithms quasinewton algorithms used methodology. conjugate gradient algorithms class learning algorithms search process undertaken along conjugate directions. search directions periodically reset negative gradient. standard reset point occurs number iterations equal number network weights. algorithms require less storage space converge faster. good networks large number connections work uses four conjugate gradient algorithms. scale gradient conjugate back propagation conjugate gradient back propagation polak–riebre updates conjugate gradient back propagation fletcher-reeves updates conjugate gradient back propagation powell-beale restarts scaled conjugate gradient uses mechanism step size scaling avoid line search iteration. though requires iterations uses less number computations fast converging algorithm conjugate gradient back propagation fletcher-reeves updates learning algorithm uses ratio norm squared current gradient norm squared previous gradient. algorithms usually faster similar algorithms conjugate gradient back propagation polakriebre updates uses ratio inner product previous change gradient current gradient norm squared previous gradient. requires slightly larger storage fletcher-reeves conjugate gradient back propagation powellbeale restarts uses reset method proposed powell based suggested beale according this restart takes place small orthogonality left current gradient previous gradient. requires storage space polak-riebre. heuristic algorithms learning algorithms search problem specific better performance. proposed work uses resilient back propagation variable learning algorithm. resilient back propagation training algorithm uses sign magnitude derivative error function weight update update value weight increased factor derivative error function sign successive iterations decreased otherwise. remains derivative zero. variable learning rate algorithm uses separate mechanism learning rate used based confusion matrix five performance measures used estimate prediction accuracy. specificity mathew correlation coefficient accuracy. following subsections give brief discussion these. commonly used performance measures protein secondary structure prediction. refers three state overall percentages correctly predicted residues. measure defined initial search gradient current search direction current gradient constant previous search direction initial gradient current weight vector next weight vector hessian matrix matrix confusion matrix matrix representation cross-tabulates observed predicted observations depicted table used derive different performance measures software used experiments matlab version neural network toolbox version used implementation neural networks. computer used perform experiments model selection intel core cpu.ghz. operating system used microsoft windows version ram. work discusses series experiments performed three layer feed forward back propagation neural network using dataset. objective experiment find optimized parameter changing values parameters. thus records corresponding data encoding schemes window sizes numbers hidden neurons types learning algorithms obtained. these bfgs learning algorithm could generate data within stipulated time therefore records corresponding taken consideration analysis. thus overall focus find optimal predictive model based remaining records. figure shows different values obtained performance measures records. demonstrates accuracy similar trends. table shows best results obtained model based performance measures. reveals parameter best result based performance measure accuracy same. though table shows best results results also show similar trend. based figure. table chosen accuracy representative performance measure reflecting behaviour mcc. thus five different performance measures effectively reduced distinct performance measures accuracy. experimental results records show slightly divergent outcome based performance measure accuracy. shows selection performance measure accuracy bearing best parameter set. order study relation performance measures karl pearson correlation coefficient given spearman’s rank correlation coefficient shown find extent relation accuracy. table gives different values correlation coefficient accuracy respect different learning algorithms. values oscillate small range average value correlation coefficient accuracy records classified based different learning algorithms observed analysis correlation coefficient accuracy data classified based different yardsticks shows strong positive correlation performance measures accuracy. words value increases value accuracy also increases vice versa. calculating spearman’s rank correlation coefficient accuracy observations given rank ranging based respective measure. spearman’s rank correlation coefficient calculated using formula given found also shows strong positive correlation accuracy. previous findings based karl pearson’s correlation coefficient spearmen’s rank correlation coefficient shows strong definite positive correlation performance measures accuracy. thus measures interchangeable experimental results concerned. since accuracy taken representative measure conclude measures behave similar manner. glance literature shows preferred performance measure secondary structure prediction. however focuses values whereas accuracy uses values. means accuracy carries information performance measure. addition able looks effect size sample data taken randomly. experiments conducted find correlation coefficient accuracy records. subsequently sample size reduced half many records taken randomly. process continued upto sample size result shows values correlation coefficient vary small range value full observations found identify records rate convergence accuracy seems uniform looked records accuracy figure figure figure figure figure figure figure figure show trend records accuracy respectively. record accuracy greater figure showing accuracy greater included. glance figures shows monotonic strictly increasing. however last three figures viz. figure figure figure show interesting features rest. monotonic increasing concave almost uniform rate convergence. focus records monotonic concave uniform rate convergence. records contributing properties ones accuracy less records number labeled stabilized cluster records. records analyzed detail find effect parameters study. table gives percentage occurrences learning algorithm entire records stabilized cluster records. total seven learning algorithms used study observed variable learning rate algorithm appeared stabilized cluster records contribution minimum. learning algorithms collectively contribute around best performing learning algorithms though seven algorithms equal contribution entire dataset. established strong positive definite relation accuracy accuracy better positioned gauge trend effectiveness prediction study respect method proposes accuracy preferred performance measure. taking accuracy preferred performance measure best performing records identified. curve plotted figure best performing records. leftmost upper corner curve identified best performing reached blosum encoding scheme window size number neurons hidden layer learning algorithm. values parameters proposed optimized parameter three layer feed forward back propagation neural network. table gives values mean squared error best performing records. shows best parameter occupies second slot respect mse. difference best parameter best value only. validates effectiveness optimized parameter set. order study nature accuracy records arranged records increasing order respect accuracy. figure shows rate increase accuracy doesn’t seem uniform. also shows rate total neurons used hidden layer appeared maximum number times best performing records shown table also shows lower number neurons lesser number occurrences stabilized cluster records though equal contribution entire dataset. table gives percentage occurrence encoding scheme entire records stabilized cluster records. total nine encoding schemes used work observed hydrophobicity blosum hydrophobicity encoding schemes absent contribution orthogonal encoding scheme minimal. seen pam+ orthogonal hydrophobicity encoding schemes appear around records best performing encoding schemes. table illustrates percentage different window sizes appeared entire data stabilized cluster records. observed window size best performing parameter values appear records stabilized cluster. also noted window size appear window size appeared rarely records. though four works performance measure none establish relationship however proposed work uses five performance measures graphically shows accuracy interrelated behaved uniform manner. taking accuracy representation these karl’s pearson correlation coefficient spearman’s rank correlation coefficient used show strong positive correlation four measures. works given best parameter though number parameters different. proposed work offers learning algorithm blosum encoding scheme window size number neurons hidden layer optimized parameter set. values performance measures range performance measure cases. proposed work provides accuracy performance measure value. works compared small search space highest size search space proposed method works search space whose size substantially large respect complexity search spaces similar works. works compared study perform analysis records whose behavior almost best parameter. offer values parameters whose performance measure highest. proposed work goes beyond finding best parameter obtain stabilized cluster records. cluster contains records neighborhood best parameter set. parameters changed data type dataset used performance measures used relation performance measures best parameter performance measure size search space analysis neighbourhood best parameter sixteen works used comparison seven works fixed parameter sets three works parameter four works parameters works three parameters. proposed method uses four parameters look table shows works performance measure three works performance measures work uses three performance measures work uses three performance measures. performance measures used seventeen works appears nine works either independently along performance measures predominantly mcc. proposed work uses five performance measures include measures used accuracy. barker garavelli huang mcgarvey orcutt srinivasarao xiao ledley janda pfeiffer mewes tsugitaawu protein information resource .nucleic acids res. bernstein koetzle williams meyer brice rodgers kennard shimanouchi tasumi protein data bank. computerbased archival file macromolecular structures. biochem. bernstein koetzle williams meyer brice rodgers kennard shimanouchi tasumi protein data bank computer-based archival file macromolecular structures. mol. biol. paper proposes methodology secondary structure prediction proteins using three layer feed forward back propagation neural network. uses nine encoding schemes nine different window sizes changing three nineteen twenty neurons hidden layer eight different learning algorithms. performing exhaustive experiments performance measured different performance measures like specificity sensitivity mathew correlation coefficient accuracy. since variations values performance measures detailed analysis best performing records done. shows parameter consisting nineteen window size nineteen number neurons hidden layer blosum encoding scheme step secant learning algorithm gives optimal results independent type performance measure used. also gives stabilized cluster records monotonic increasing concave uniform rate convergence. agarwal baboota mendiratta design implementation algorithm predict secondary structure proteins using artificial neural networks. international journal emerging research management technology. apweiler attwood bairoch bateman birney biswas m.et interpro database integrated documentation protein families domains functional sites. nucleic acids res. bohr boht brunak cotterill lautrup norskov olsen petersen sb.. protein secondary structure homology neural networks. alpha-helices rhodopsin. febs lett. bordoloi sarma protein structure prediction using multiple artificial neural network classifier soft computing techniques vision science studies computational intelligence.; ./----_. dayhoff schwartz orcutt model evolutionary change proteins. dayhoff editors. atlas protein sequence structure. natl. biomed. res. found. washington suppl dinubhai shah protein secondary structure prediction using neural network comparative study. international journal enhanced research management computer applications. drenth faraggi zhang yang lukasz kurgan zhou spine improving protein secondary structure prediction multistep learning coupled prediction solvent accessible surface area backbone torsion angles. journal computational chemistry. fasman development prediction protein structure. fasman editor. prediction protein structure principle protein conformation. newyork plenum press; .pp.-. fletcher approach variable metric algorithms. computer journal. fumiyoshisasagawa koji tajima. prediction protein secondary structure neural network. inmitaku gotoh nitta konagaya yonezawa editors. genome informatics. japan; p.-. garnier osguthorpe robson analysis accuracy implications simple methods secondary structure globular predicting proteins. journal molecular biology. robson developments protein structure prediction using information theory. parameters consideration residue pairs. journal molecular biology.; zhang ramamohanarao martin survey machine learning methods secondary supersecondary protein structure prediction. methods molecular biology.; huang huang zhang prediction protein secondary structure using improved network architecture. protein peptlett. hunter artificial intelligence molecular jaroniec macphee bajaj mcmahon dobson griffin high resolution molecular structure peptide amyloid fibril determined magic angle spinning spectroscopy. proceedings national academy sciences usa. johal singh protein secondary structure prediction using improved support vector machine neural networks. international journal engineering computer science.; kloczkowski ting jernigan garnier combining algorithm evolutionary information protein secondary structure prediction amino acid sequence. proteins structfunct genet.; eyrich marti-renom przybylski madhusudhan narayanan o.gran~a valencia sali rost continuous automatic evaluation protein structure prediction servers. nucleic acids res. kuen-pin hsin-nan jia-ming chang tingyi sung wen-lian hsu. hyprosp hybrid protein secondary structure prediction algorithm knowledge-based acids research.; lena fariselli margara macro vassura casadio divide conquer strategies protein structure prediction. bruni editor. mathematical approaches polymer sequence analysis related problems. springer; mainmn rokach introduction knowledge discovery data mining. mainmn rokach editors. data mining knowledge discovery handbook. york springer; .p.-. incorporating knowledge multi-layer networks example proteins fogelman-soulie editors.neurocomputing algorithms architecturesand applications. springer-verlag berlin; p.-. safiurrahman mahdi zunaidhaque mamun hawlader abdullah al-mamun. protein secondary structure prediction using feed-forward neural network. journal computer information technology. configurations polypeptide chains favored orientations polypeptide around single bonds pleated sheets. proceedings national academy sciences usa.a; radzicka wolfenden comparing polarities side-chain distribution coefficients vapor phase cyclohexane -octanol aqueous solution. biochemistry.; riedmiller braun direct adaptive method faster backpropagation learning rprop algorithm. proceedings international conference neural networks fransisco u.s.a. rumelhart hinton williams learning internal representations error propagation. rumelhart mcclelland group editors. processing explorations microstructure cognition. cambridge press; salamov solovyev prediction proteins secondary structure combining nearest-neighbor algorithms multiple sequence alignments. journal molecular biology. shepherd gorse thornthon prediction location type beta-turns proteins using neural networks. protein sci. shepherd gorse thornton novel approach recognition protein architecture form sequence using fourier analysis neural networks. proteins. vieth kolinski skolnick sikorski prediction protein secondary structure neural networksencoding short long range patterns amino acid packing. actabiochim pol. jaewon. protein secondary structure prediction based neural network models support vector machines. final project departments electrical engineering stanford university zhang zhang chen kedaisetti mizianty stach kurgan critical assessment high-thoroughput standalone methods secondary structure prediction. briefings bioinformatics.;", "year": "2018"}