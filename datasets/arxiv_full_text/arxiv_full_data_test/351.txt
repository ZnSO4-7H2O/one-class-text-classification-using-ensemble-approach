{"title": "SuperSpike: Supervised learning in multi-layer spiking neural networks", "tag": "q-bio", "abstract": " A vast majority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in-vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in-silico. Here we revisit the problem of supervised learning in temporally coding multi-layer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three factor learning rule capable of training multi-layer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike-time patterns. ", "text": "keywords spiking neural networks multi-layer networks supervised learning temporal coding synaptic plasticity feedback alignment credit assignment vast majority computation brain performed spiking neural networks. despite ubiquity spiking currently lack understanding biological spiking neural circuits learn compute in-vivo well instantiate capabilities artiﬁcial spiking circuits in-silico. revisit problem supervised learning temporally coding multi-layer spiking neural networks. first using surrogate gradient approach derive superspike nonlinear voltage-based three factor learning rule capable training multi-layer networks deterministic integrateand-ﬁre neurons perform nonlinear computations spatiotemporal spike patterns. second inspired recent results feedback alignment compare performance learning rule different credit assignment strategies propagating output errors hidden units. speciﬁcally test uniform symmetric random feedback ﬁnding simpler tasks solved type feedback complex tasks require symmetric feedback. summary results open door obtaining better scientiﬁc understanding learning computation spiking neural networks advancing ability train solve nonlinear problems involving transformations different spatiotemporal spike-time patterns. neurons biological circuits form intricate networks primary mode communication occurs spikes. theoretical basis networks sculpted experience give rise emergent computations remains poorly understood. consequently building meaningful spiking models brain-like neural networks insilico largely unsolved problem. contrast ﬁeld deep learning made remarkable progress building non-spiking convolutional networks often achieve human-level performance solving difﬁcult tasks even though details artiﬁcial rate-based networks trained arguably different brain learns several studies begun draw interesting parallels internal representations formed deep neural networks recorded activity different brain regions major impediment deriving similar comparison spiking level currently lack efﬁcient ways training spiking neural networks thereby limiting applications mostly small problems fundamentally involve spatiotemporal spike time computations. instance recently groups begun train snns datasets mnist whereas previous studies used smaller artiﬁcial datasets. difﬁculty simulating training snns originates multiple factors. first time indispensable component functional form even individual stimuli associated outputs spatiotemporal spike patterns rather simple spatial activation vectors. fundamental difference necessitates different cost functions ones commonly encountered deep learning. second spiking neuron models inherently non-differentiable spike time derivative output respect synaptic weights zero times. third intrinsic self-memory spiking neurons introduced spike reset difﬁcult treat analytically. finally credit assignment hidden layers problematic reasons technically challenging efﬁcient auto-differentiation tools available event-based spiking neural network frameworks method weight updates implemented standard back-propagation error algorithm thought biologically implausible several studies multi-layer networks build notion feedback alignment recently illustrated strict requirements imposed feedback backpropagation error signals loosened substantially without large loss performance standard benchmarks like mnist studies performed using spiking networks still effectively rate-based approach given input activity vector interpreted ﬁring rate input neurons approach appealing often related directly equivalent rate-based models stationary neuronal transfer functions also largely ignores paper develop novel learning rule train multi-layer snns deterministic leaky integrate-an-ﬁre neurons tasks fundamentally involve spatiotemporal spike pattern transformations. beyond purely spatial rate-based activation vectors prevalent deep learning. study biologically plausible strategies deep credit assignment across multiple layers generalize enhanced context complex spatiotemporal spike-pattern transformations. supervised learning precisely timed spikes single neurons networks without hidden units studied extensively. pﬁster used probabilistic escape rate model deal hard nonlinearity spike. similar probabilistic approaches also used derive spike timing dependent plasticity information maximizing principles contrast that resume span deterministic approaches seen generalizations widrow-hoff rule spiking neurons. similar vein chronotron learns precisely timed output spikes minimizing victor-pupura distance given target output spike train. similarly gardner grüning albers studied convergence properties rules reduce rossum distance gradient descent. moreover memmesheimer proposed learning algorithm achieves high capacity learning long precisely timed spike trains single units recurrent networks. problem sequence learning recurrent neural networks also studied variational learning problem combining adaptive control theory heterogeneous neurons supervised learning snns without hidden units also studied classiﬁcation problems. instance maass used p-delta rule train readout layer liquid state machine. moreover tempotron derived gradient-based approach classiﬁes large numbers temporally coded spike patterns without explicitly specifying target ﬁring time. works embarked upon problem training snns hidden units process precisely timed input output spike trains porting backprop spiking domain. main analytical difﬁculty approaches arises partial spike train hidden neuron hidden weight. spikeprop sidesteps problem deﬁning differentiable expression ﬁring times instead standard gradient descent performed. original approach limited single spike neuron multiple extensions algorithm exist also improve convergence properties however caveat spike timing based methods cannot learn starting quiescent state spiking spike time ill-deﬁned. algorithms however suffer limitation. instance extension resume multiple layers proposed error signals backpropagated linearly. recently group proposed principled generalization backprop snns gardner using stochastic approach seen extension pﬁster multiple layers. similar ﬂavour fremaux gardner substitute partial derivative hidden spike trains point estimate expectation value. although theoretically stochastic approaches avoid problems arising quiescent neurons convergence slow injected noise become major impediment learning practice. instead approximating partial derivatives spike trains expectation value bohte corresponding partial derivative approximated scaled heaviside function membrane voltage. however heaviside function approach vanishing surrogate gradient sub-threshold activations limits algorithm’s applicability cases hidden units quiescent. finally sejnowski proposed another interesting approach which instead approximating partial derivatives hard spiking nonlinearity instead soft spiking threshold used design standard techniques gradient descent applicable. contrast previous works method permits train multi-layer networks deterministic neurons solve tasks involving spatiotemporal spike pattern transformations without need injecting noise even hidden units initially completely silent. achieve this approximate partial derivative hidden unit outputs product ﬁltered presynaptic spike train nonlinear function postsynaptic voltage instead postsynaptic spike train. following section explain details approach. begin consider single neuron would like emit given target spike train given stimulus. formally frame problem optimization problem want minimize rossum distance actual output spike train normalized smooth temporal convolution kernel. double exponential causal kernels throughout easily computed online could implemented electrical chemical traces neurobiology. computing gradient respect synaptic weights derivative spike train ∂si/∂wij appears. derivative problematic neuron models zero except spike times deﬁned. existing training algorithms circumvent problem either performing optimization directly membrane potential introducing noise renders likelihood spike train smooth function membrane potential. combine merits approaches replacing spike train continuous auxiliary function membrane potential. performance reasons choose negative side fast sigmoid monotonic functions increase steeply peak spiking threshold work well. auxiliary function yields replacement introduced causal membrane kernel corresponds postsynaptic potential shape captures spike dynamics reset. latter depends past output spike train dependence allow compute derivative directly constitutes ∂wij small correction provided ﬁring rates low. ﬁring rates seem physiologically plausible also easily achieved practice adding homeostatic mechanisms regularize neuronal activity levels. neglecting second term simply yields ﬁltered presynaptic activity ∂wij interpreted concentration neurotransmitters synapse. substituting approximation back gradient descent learning rule single neuron takes form introduced learning rate short notation output error signal eligibility trace λij. practice evaluate expression minibatches often per-parameter learning rate closely related rmsprop speed learning. equation corresponds superspike learning rule output neuron however redeﬁning error signal feedback signal rule hidden units well. move testing learning rule ﬁrst state noteworthy properties hebbian term combines prepostsynaptic activity multiplicative manner learning rule voltage-based nonlinear hebbian rule occurrence causal convolution acts eligibility trace solve distal reward problem error signals arriving error made three factor rule error signal plays role third factor unlike existing three-factor rules however error signal speciﬁc postsynaptic neuron important point return later. trained networks spiking neurons using supervised learning approach call superspike. approach generalizes back propagation error algorithm known multi-layer perceptron deterministic spiking neurons. partial derivative thus gradient deterministic spiking neurons zero almost everywhere make optimization problem solvable introduce non-vanishing surrogate gradient simulations temporal resolution using auryn simulation library publicly available neurons current-based synaptic input alternatively formulated integral form however simulate membrane dynamics computed voltage neuron described following differential equation value jumps amount moment spike arrival denotes dirac δ-function ﬁring times neuron action potential triggered membrane voltage neuron rises threshold value following spike voltage remains clamped rest emulate refractory period. generation spikes propagated neurons axonal delay .ms. depending task hand used different types stimuli. simulation experiments network learn exact output spike times used frozen poisson spike trains input. stimuli consisted single draw number input units poisson spike trains given duration. spike trains repeated loop associated target spike train consistently aligned repeats frozen poisson inputs. benchmarking comparison reasons stimulus target spike trains shown paper publicly available part supervised spiking benchmark suite classiﬁcation experiments used sets different stimuli. individual stimuli drawn random neuronal ﬁring time offsets common stimulus onset time. stimulus order chosen randomly randomly varying inter-stimulus-intervals. main ingredients supervised learning rule spiking neurons summarized equation describing synaptic weight changes. also eluded above learning rule interpreted nonlinear hebbian three factor rule. nonlinear hebbian term detects coincidences presynaptic activity postsynaptic depolarization. spatiotemporal coincidences single synapse stored transiently temporal convolution causal kernel step interpreted synaptic eligibility trace neurobiology could instance implemented calcium transient related signaling cascade importantly algorithm causal sense necessary quantities computed online without need propagate error signals backwards time. fact superspike interpreted implementation real-time recurrent learning spiking neural networks. model complexity neural feedback learning absorbed per-neuron signal unclear error feedback signaled individual neurons biology explored different strategies explained detail below. practical reasons integrate ﬁnite temporal intervals updating weights. full learning rule written follows addition neuronal dynamics described previous section evaluation thus coarsely grouped follows evaluation presynaptic traces evaluation hebbian coincidence computation synaptic eligibility traces iii) double exponential ﬁlter temporal convolution expression presynaptic traces evaluated efﬁciently online exponential ﬁltering twice. speciﬁcally explicitly integrate single exponential trace hebbian coincidence detection synaptic eligibility traces evaluate hebbian term evaluate surrogate partial derivative every time step. efﬁciency reasons partial derivative negative half +|x| require costly evaluation exponential fast sigmoid functions every. speciﬁcally compute neuronal ﬁring threshold unless mentioned otherwise. compute outer product delayed presynaptic traces surrogate partial derivatives every time step. delay chosen offsets axonal delay spikes acquire forward propagation. presynaptic traces decay zero quickly absence spikes approximate exactly zero numerical value drops machine precision allows speed computation outer product skipping presynaptic indices computation. implement synaptic eligibility trace given temporal ﬁlter ﬁlter values hebbian product term exponential ﬁlters like case presynaptic traces above. important note however traces need computed synapse makes algorithm scale number neurons. makes obvious target future optimizations algorithm. biologically complexity could implemented naturally simply synaptic spines electrical ionic compartments concentration transient calcium messengers decays short timescales. superspike function properly important transients long enough temporally overlap causally related error signal formally duration transient model given ﬁlter kernel shape used compute rossum distance. used double-exponentially ﬁltered kernel shape model kernels possible. distinguish types error signals output error signals feedback signals output error signals directly tied output units certain target signal exists. details depend underlying cost function trying optimize. feedback signals hand derived output error signals sending back hidden units. study used slightly different classes output error signals three different types feedback. level output errors distinguish cases learn precisely timed output spikes. cases output error signals exactly given output unit unless stated otherwise chose normalized unity. seen expression error signal vanishes target output spike train exactly match temporal precision simulation. cost function values computed online root mean square moving average time constant. simulations wanted classify input spike patterns rather generate precisely timed output patterns introduced slack computation error signal. instance illustrated figure gave instantaneous negative error feedback described serr erroneous additional spike serr however since task want network learn precisely timed output spikes gave positive feedback signal smiss miss trial i.e. stimulus failed evoke output spike window opportunity investigated different credit assignment strategies hidden units. hidden layer units received three types feedback distinguish symmetric random uniform feedback. symmetric feedback signals wkiek downstream error signals using actual feed-forward weights wik. note contrast backprop non-local information downstream activation functions appear expression closely related notion straight-through estimators motivated recent results feedback alignment random feedback signals computed random bkiek random coefﬁcients drawn normal distribution zero mean unit variance. conﬁguration could implemented instance individual neurons sensing differential neuromodulator release heterogeneous population modulatory neurons. finally case uniform feedback weighting corresponding closest single global update weights time continuous time series corresponding product error/feedback signal synaptic eligibility traces directly added synaptic weights ﬁrst integrated separate variable chunks speciﬁcally computed time step. stimuli exceeding duration thus seen continuous time analogue mini batch optimization. chose order half second good compromise computational cost performance synaptic updates added simulation time steps minimize periodic alignment update step stimulus. interval weights updated according rijmij parameter learning rate rij. addition that enforced constraint individual weights remain interval updating weights variables reset zero. facilitate ﬁnding right learning rate speed training times simulations implement per-parameter learning rate heuristic. compute perparameter learning rate addition integrated another auxiliary quantity ij). ensures slow decay max. however rmsprop computes moving exponential average found introducing function rendered training stable simultaneously yielding excellent convergence times. call slightly modiﬁed version rmaxprop finally parameter determined grid search values experiments random feedback added heterosynaptic regularization term learning rule hidden layer weights avoid pathologically high ﬁring rates. experiments full learning rule introduced regularization strength parameter made regularization term explicitly dependent square error signal ensure would zero cases task solved perfectly. moreover used fourth power exponential synaptic trace evolved according following differential equation test whether equation could used train single neuron emit predeﬁned target spike pattern simulated single neuron received spike trains inputs. target spike train chosen equidistant spikes interval inputs drawn poisson spike trains repeated every initialized weights regime output neuron showed sub-threshold dynamics spike previous methods starting quiescent state would require introduction noise generate spiking would turn retard speed precise output spike times could learned. finally weight updates computed evaluating integral ﬁxed interval scaling resulting value learning rate trials corresponding simulated time output neuron learned produce desired output spike train however fewer trials could generate good approximations target spike train figure superspike learns precisely timed output spikes single output neuron. snapshot initial network activity. bottom panel spike raster input activity. middle panel membrane potential output neuron ﬁring threshold target spikes shown black points. panel error signal zero error indicated reference dotted line. superspike learning. spike timing plot showing temporal evolution per-trial ﬁring times learning curves trials well mean training. figure schematic illustration superspike learning network hidden layer. spikes generated lower input layer propagated hidden layer middle output layer top. temporal evolution dynamical quantities involved updating single synaptic weight input hidden layer unit. brevity suppressed neuron indices variables. input spikes associated post-synaptic potentials membrane voltage hidden unit downstream spikes generated hidden layer output unit finally error signal computed output spike train. modulates learning output weights propagated back hidden layer units feedback weights. note error signal strictly causal. product presynaptic activity nonlinear function ﬁltered time giving rise synaptic eligibility trace biological scenario could instance manifested calcium transient synaptic spine. finally temporal coincidence error signal determines sign magnitude plastic weight changes established rule efﬁciently transform complex spatiotemporal input spike patterns precisely timed output spike trains network without hidden units next investigated well rule would perform multilayer networks. form equation suggests straight forward extension hidden layers analogy backprop. namely learning rule hidden units modiﬁcation becomes complicated function depends weights future activity downstream neurons. however non-locality space time presents serious problems terms biological plausibility technical feasibility. technically computation requires either backpropagation time kernel computation relevant quantities online case rtrl. explore latter approach since speciﬁc choice temporal kernels allows compute relevant dynamic quantities error signals online approach error signals distributed directly feedback matrix hidden layer units speciﬁcally means output error signals neither propagated actual soft spiking nonlinearity. idea closely related notion straight-through estimators machine learning investigated different conﬁgurations feedback matrix either symmetric case backprop random motivated recent results feedback alignment uniform corresponding closest single global third factor distributed neurons akin diffuse neuromodulatory signal. ﬁrst sought replicate task shown figure addition hidden layer composed neurons. initially tested learning random feedback. feedback weights drawn zero mean unit variance gaussian value remained ﬁxed entire simulation. synaptic feedforward weights also initialized randomly level neither hidden units output unit ﬁred single spike response input spike trains used training network hidden units started spikes response input. similarly output neuron started intermittent intervals closely resembling target spike train continued training task total lead reﬁnement output spike train differentiated ﬁring patterns subset hidden units although restrict synaptic connectivity obey dale’s principle present example random feedback hidden neurons positive feedback connections ended excitatory whereas neurons negative feedback weights generally turned inhibitory training. dynamics direct manifestation feedback alignment aspect random feedback learning example shown figure strictly require inhibitory neurons hidden layer many cases neurons negative feedback remained quiescent activity levels learning figure superspike learning different types feedback allows train multi-layer networks. network activity initial trial reference time bottom panel shows membrane potential traces four hidden units. membrane potential output unit shown middle. dashed line output neuron ﬁring threshold. points correspond target ﬁring times plot shows error signal output layer. hidden units receive input spikes shown fig. training. hidden units started respond repeating input spiking pattern ones positive feedback weights whereas hidden neurons receive negative feedback connections output layer respond mostly offset repeating stimulus. learning curves networks trained random feedback connections. gray lines correspond single trials black line average. dashed line average network hidden layer units. network symmetric feedback connections. uniform feedback connections. network completely failed solve task. cases feedback connections happened initialized negative value eventuality could made unlikely however increasing number hidden units that striking differences performance replaced random feedback connections symmetric uniform feedback weights figure network trained solve non-linearly separable classiﬁcation problem noisy input neurons. sketch network layout output units four hidden units. snapshot network activity training. four input patterns non-linearly separable classes presented random order stimulus periods input neurons spike randomly background ﬁring rate. learning curves trials different random initializations network without hidden layer cannot solve task. average trials given black line. network without hidden units receive random feedback training. symmetric feedback. uniform feedback connections. previous task simple enough solving require hidden layer. therefore investigated whether superspike could also learn solve tasks cannot solved network without hidden units. constructed spiking exclusive-or task four different spiking input patterns separated classes. example used input units although effective dimension problem construction. speciﬁcally picked three nonoverlapping sets input neurons associated ﬁxed random ﬁring times window. part patterns served time reference. sets combined yield four input patterns problem. moreover added second readout neuron corresponding respective target classes input patterns given random order short bouts spiking activity random inter-trial-intervals input neurons ﬁring stochastically allow ﬁnite propagation time network relaxed requirement precise temporal spiking instead required output neurons spike within narrow window opportunity aligned outlasted stimulus output error signal zero unless correct output neuron failed within window. case error signal corresponding correct output elicited window. time incorrect spike triggered immediate negative feedback. trained network comparing different types feedback. network random feedback quickly learned solve task perfect accuracy whereas network without hidden units unable solve task perhaps surprisingly networks symmetric feedback connections also learned task quickly overall learning curves stereotyped less noisy whereas networks uniform feedback performed worse average overall results illustrate temporally coding spiking multi-layer networks trained solve tasks cannot solved networks without hidden layers. moreover results show random feedback beneﬁcial uniform feedback cases. tasks considered simple enough could solved three layer networks zero error types feedback signals. hypothesized observed indifference type feedback could task simple. test whether picture would change challenging task studied network output neurons learn second-long complex spatiotemporal output pattern cyclically repeating frozen poisson noise speciﬁcally trained three layer input output different numbers hidden neurons within training symmetric feedback connections network hidden units could learn emit output spike pattern visually matched target ﬁring pattern successful learning hidden unit activity irregular intermediate ﬁring rates close exponential inter-spike-interval distribution however target pattern learned perfectly evidenced number spurious spikes non-vanishing rossum cost task simulation random feedback yielded substantially worse performance output pattern became close impossible recognize visually expected results uniform feedback worst hence option considered following. notably random feedback case performs worse network trained without hidden layer since observed abnormally high ﬁring rates hidden layer neurons networks trained random feedback wondered whether performance could improved addition heterosynaptic weight decay acts activity regularizer addition heterosynaptic weight decay term notably improved learning performance increased figure learning complex spatiotemporal spike pattern transformations. spike raster target ﬁring pattern output neurons. whole ﬁring pattern duration schematic illustration network architecture. spike raster target ﬁring pattern reference. snapshot network activity network symmetric feedback superspike learning. bottom panel spike raster repeating frozen poisson input spikes. middle panel spike raster hidden unit spiking activity. panel spike raster output spiking activity. black arrow denotes point time superspike learning switched freezes spiking activity fully deterministic network. histograms different ﬁring statistics hidden layer activity learning. distribution ﬁring rates. middle inter-spike-interval distribution semi-log axes. bottom distribution coefﬁcient variation distribution. figure learning spatiotemporal spike patterns. learning curves networks symmetric feedback hidden layer respectively. learning activated dashed line whereas learning rate reduced factor dotted line. minimum cost convergence different feedback strategies varying numbers hidden units. dashed line performance network without hidden units. minimum cost convergence symmetric feedback ﬁxed number dashed line performance network without hidden units. partial learning rule without voltage nonlinearity. fixed hidden learning hidden units disabled. spike raster snapshots output activity learning symmetric feedback like unregularized random feedback. like additional heterosynaptic regularization like without voltage nonlinearity. visual similarity output patterns however even modiﬁed learning rule achieve comparable performance levels symmetric-feedback network. importantly hidden layer sizes tested random feedback networks even achieve performance levels networks without hidden layer whereas symmetric feedback networks surprisingly networks wider hidden layers performed superior networks fewer hidden units networks random feedback performed consistently worse counterparts trained symmetric feedback finally trained network using symmetric feedback learning rule disabled nonlinear voltage dependence setting corresponding term output pattern degraded results seem conﬁrm intuition challenging tasks nonlinearity learning rule ﬁring rate regularization non-random feedback seem become important achieving good performance type spatiotemporal spike pattern transformation tasks considered here. paper derived three factor learning rule train deterministic multi-layer snns neurons. moreover assessed impact different types feedback credit assignment strategies hidden units notably symmetric random uniform. contrast previous work used deterministic surrogate gradient approach instead commonly used stochastic gradient approximations. combining rule ideas straight-through estimators feedback alignment could efﬁciently train study precisely timed spiking dynamics multi-layer networks deterministic neurons without relying introduction extraneous unavoidable noise present stochastic models noise generally impedes ability learn precise spatiotemporal spike-pattern transformations. weight update equation superspike constitutes voltage-based nonlinear hebbian three factor rule individual synaptic eligibility traces. aspects direct biological interpretations. instance nonlinear voltage dependence reported ubiquitously numerous studies hebbian long-term plasticity induction hippocampus cortex also window temporal coincidence detection model good agreement stdp moreover time course eligibility traces could interpreted local calcium transient synaptic spine level. finally multiplicative coupling error signal eligibility trace could arise neuromodulators however instead global feedback signal work highlights necessity higher dimensional neuromodulatory electrical feedback signal learning potentially knowledge feedforward pathway. biological exploration intelligent neuromodulation well extensions approach deeper recurrent snns left intriguing directions future work. authors would like thank subhy lahiri poole helpful discussions. supported snsf wellcome trust. supported burroughs wellcome sloan mcknight simons james mcdonnell foundations ofﬁce naval research.", "year": "2017"}