{"title": "Visual gesture variability between talkers in continuous visual speech", "tag": "eess", "abstract": " Recent adoption of deep learning methods to the field of machine lipreading research gives us two options to pursue to improve system performance. Either, we develop end-to-end systems holistically or, we experiment to further our understanding of the visual speech signal. The latter option is more difficult but this knowledge would enable researchers to both improve systems and apply the new knowledge to other domains such as speech therapy. One challenge in lipreading systems is the correct labeling of the classifiers. These labels map an estimated function between visemes on the lips and the phonemes uttered. Here we ask if such maps are speaker-dependent? Prior work investigated isolated word recognition from speaker-dependent (SD) visemes, we extend this to continuous speech. Benchmarked against SD results, and the isolated words performance, we test with RMAV dataset speakers and observe that with continuous speech, the trajectory between visemes has a greater negative effect on the speaker differentiation. ", "text": "recent adoption deep learning methods ﬁeld machine lipreading research gives options pursue improve system performance. either develop endto-end systems holistically experiment understanding visual speech signal. latter option difﬁcult knowledge would enable researchers improve systems apply knowledge domains speech therapy. challenge lipreading systems correct labeling classiﬁers. labels estimated function visemes lips phonemes uttered. maps speaker-dependent? prior work investigated isolated word recognition speaker-dependent visemes extend continuous speech. benchmarked results isolated words performance test rmav dataset speakers observe continuous speech trajectory visemes greater negative effect speaker differentiation. machine lip-reading dilemma evidence suggests identify individuals unique visual speech information conventional systems pursue ideal scenario able lip-read speaker. recent work deep learning classiﬁers large datasets promise achieve end-to-end systems taking road risk failing understand visual speech signal. whilst behavoural studies human perception visual speech fully utilise computers fully undertake investigations. machine lip-reading attempts interpret words spoken visual representation sounds they’re uttered. means levels ‘translation’ within process visual gestures phonemes phonemes words. previous literature reports lip-reading performance based different units three stages makes comparing performance difﬁcult. report word error rate others viseme error rate others accuracy units. however without known function determine phoneme viseme translates words pronounced alternative phonemes argument speaker-dependent function mapping visual gestures phonemes idea ﬁrst presented latest algorithm deriving speaker-dependent maps copyright document resides authors. distributed unchanged freely print electronic forms. phoneme confusions presented need understand speakerdependent maps whilst speaker-generic phoneme-to-viseme desirable concept viseme alphabet every speaker daunting know adapting trained classiﬁer speaker another similar speaker dataefﬁcient learning model speakers turn achieves less accuracy. therefore determine different speaker-dependent visemes lipreading continuous speech? prior attempt answering question used isolated word dataset similar methodology test effects maps suggest simple metric compare maps. phoneme clustering approach produce series speaker-dependent maps. intention behind design viseme derivation algorithm utilise unique gestures speaker improve reading previous viseme sets phoneme labelled classiﬁers series maps made following toolkit build classiﬁers visemes map. hmms ﬂat-started hcompv re-estimated times forced alignment seventh re-estimate. classify using hvite output results hresults. follow design decisions three state hmms associated ﬁve-component gaussian mixture state bigram word network built hbuild hlstats classiﬁcation measured word correctness report word correctness rather viseme correctness. lip-reading understanding speech i.e. interpretation words rather matching single viseme. also normalises training sample bias across viseme sets. number visemes varies speaker training sample volumes consistent number words spoken beep pronunciation dictionary used throughout experiments british english equation word correctness total number words number deletion errors substitution errors. counted comparing classiﬁer recognition outputs ground truth. rmav dataset corpus speakers seven male female reciting sentences selected resource management corpus database vocabulary size approximately words recorded full-frontal view head motion restrained lighting constant. means speaker trained using visual speech data speaker tested using speaker e.g. designates testing constructed speaker using training data speaker testing speaker accuracte speaker-dependent lip-reading exists however independent speakers training test sets accuracy signiﬁcantly falls ﬁrst tests based phoneme confusions speakers. therefore tested tests maps derived using speakers confusions test speaker. time substitute symbol place list speaker numbers meaning ‘not including speaker tests maps follows speaker dependent lip-reading benchmark baseline performance speaker-dependent results training test data speaker. mimics previous work conventional systems small datasets. maps speakers benchmark scores red. speakers speaker signiﬁcantly negatively affected using generalised multi-speaker visemes including test speakers phoneme confusions doesn’t improve quantiﬁes lip-reading dependency speaker identity dependent speakers compared. exception speaker shows insigniﬁcant decrease visemes. future improvement possibility deriving multi-speaker visemes based upon sets visually similar speakers. challenge remains knowing speakers grouped together. consistent neuroscience learning adage human brain learns recognise familiar generalise similar adapt novel also figure test isolated words maps signiﬁcantly reduced lipreading. small dataset conclusive reassuring continuous speech results similar. different speaker-dependent results figures plotted results y-axis benchmark. here trained test speaker effects unit selection. figure three maps signiﬁcantly reduce speaker contrast speaker signiﬁcantly reducing maps maps signiﬁcantly improve classiﬁcation speaker suggests speakers identity important good classiﬁcation used. individuals simply easier lip-read similarities certain speakers learned speaker adaptable lip-read visually-similar speakers. figure speaker particularly robust visual unit selection classiﬁer labels. conversely speaker’s signiﬁcantly affected visemes. variability previously considered speakers dependent good visual classiﬁers others much. again number visual classiﬁers varies speaker identity. rank speaker viseme weighting effect tests table increases within scores outside scores decreases weights negative. observation table speaker make overall improvement classifying speakers positive values total crucially speaker visemes make improvement classiﬁcation. speaker improves speakers others show negative effect reinforces assertion visual speech unique speakers evidence exceptions. future measuring similarity viseme maps individual speaker would help adaptation speaker another less training data. figures show achieved labelled ds&d tests. reassuring speakers signiﬁcantly deteriorate classiﬁcation rates speaker used train classiﬁer test speaker consistent tests. example leftmost side figure test speaker speaker speaker-dependent maps speakers used build sets classiﬁers. tested speaker maps models speakers show signiﬁcant reduction word correctness. eight speakers within standard error. figure similar trend speaker showing variation three speakers. lip-read speaker actually signiﬁcant improvement using model speaker less signiﬁcant improvements speakers however whilst signs towards speaker-independent lip-reading common trend overlap continuous speech speakers natural variation attributed speaker identity linguistics restrictions. lack training test speaker still signiﬁcantly less accurate speaker dependent tests continuous speech signiﬁcantly better difference experienced isolated words. table lists differences speakers datasets mean difference isolated words continuous speech cw%. furthermore isolated words performance attained speaker-independent tests shown cases worse guessing. whilst poorest maps might signiﬁcantly better guessing regardless test speakers. maps visemes whereas maps range six. conclude high risk over-generalising ms/si map. suggest speaker-dependency varies also contribution viseme within affects word classiﬁcation performance idea also shown results present certain visemes visemes improve speaker-independent lip-reading. shown exceptions selection signiﬁcant classiﬁers trained non-test speakers detrimental. gives hope visually similar speakers speaker independent lip-reading possible. furthermore continuous speech shown speaker dependent phonemeto-viseme maps signiﬁcantly improve lipreading isolated words. attribute co-articulation effects phoneme recognition confusions turn inﬂuences speaker-dependent maps linguistic context information. supported evidence conventional lipreading systems show strength language models lipreading accuracy. finally evidence speaker independence even unique trajectories visemes individual speakers likely achievable. need understanding inﬂuence language visual gestures. helen bear richard harvey barry-john theobald yuxuan lan. resolution limits visual speech recognition. image processing ieee international conference pages ieee helen bear gari owen richard harvey barry-john theobald. observations computer lip-reading moving dream reality. spie security defence pages g–g. international society optics photonics ./.. helen bear stephen richard harvey. speaker independent machine reading speaker dependent viseme classiﬁers. joint international conference facial analysis animation audio-visual speech processing pages isca cetingul yemez erzin tekalp. discriminative analysis motion features speaker identiﬁcation speech-reading. ieee transactions image processing issn ./tip. stephen richard harvey yuxuan jacob newman barry theobald. challenge multispeaker lip-reading. proceedings international conference auditory-visual speech processing pages timothy hazen kate saenko chia-hao james glass. segment-based audio-visual speech recognizer data collection development initial experiments. proceedings international conference multimodal interfaces icmi pages york acm. isbn ---. ./.. http//doi.acm.org/./. patrick lucey gerasimos potamianos sridha sridharan. visual speech recognition across multiple views. visual speech reognition segmentation mapping sarah taylor moshe mahler barry-john theobald iain matthews. dynamic proceedings siggraph/eurographics units visual speech. conference computer animation pages eurographics association kwanchiva thangthai richard harvey stephen barry-john theobald. improving lip-reading performance robust audiovisual speech recognition using dnns. avsp pages wong chõng phooi seng li-minn siew chin chew king hann lim. multi-purpose audio-visual unmc-vier database multiple variabilities. pattern recognition letters issn http//dx.doi.org/./j.patrec....", "year": "2017"}