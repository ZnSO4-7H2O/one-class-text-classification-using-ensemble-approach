{"title": "Exploring Architectures, Data and Units For Streaming End-to-End Speech  Recognition with RNN-Transducer", "tag": "eess", "abstract": " We investigate training end-to-end speech recognition models with the recurrent neural network transducer (RNN-T): a streaming, all-neural, sequence-to-sequence architecture which jointly learns acoustic and language model components from transcribed acoustic data. We explore various model architectures and demonstrate how the model can be improved further if additional text or pronunciation data are available. The model consists of an `encoder', which is initialized from a connectionist temporal classification-based (CTC) acoustic model, and a `decoder' which is partially initialized from a recurrent neural network language model trained on text data alone. The entire neural network is trained with the RNN-T loss and directly outputs the recognized transcript as a sequence of graphemes, thus performing end-to-end speech recognition. We find that performance can be improved further through the use of sub-word units (`wordpieces') which capture longer context and significantly reduce substitution errors. The best RNN-T system, a twelve-layer LSTM encoder with a two-layer LSTM decoder trained with 30,000 wordpieces as output targets achieves a word error rate of 8.5\\% on voice-search and 5.2\\% on voice-dictation tasks and is comparable to a state-of-the-art baseline at 8.3\\% on voice-search and 5.4\\% voice-dictation. ", "text": "acoustic model predicts likelihood acoustic input speech utterance given phoneme sequence conditional models directly predict likelihood typically replaced scaled likelihood obtained dividing posterior prior socalled hybrid models deep recurrent neural networks long short-term memory cells recently shown ideal task pronunciation model typically built pronunciation dictionaries curated expert human linguists back-off grapheme-to-phoneme model dictionary words. finally n-gram model trained text data used language model recently considerable interest training end-to-end models directly output word transcripts given input audio. thus models much simpler conventional systems single neural network used directly recognize utterances without requiring separately-trained acoustic pronunciation language model components. particular class architecures known sequence-to-sequence models particularly suited end-to-end include encoder network corresponds acoustic model conventional system decoder network corresponds language model. drawback typical encoder-decoder type architectures entire input sequence encoded output sequence decoded thus models cannot used real-time streaming speech recognition. several streaming encoder-decoder architectures proposed previously including neural transducer recurrent neural aligner investigate training end-to-end speech recognition models recurrent neural network transducer streaming all-neural sequence-to-sequence architecture jointly learns acoustic language model components transcribed acoustic data. explore various model architectures demonstrate model improved additional text pronunciation data available. model consists ‘encoder’ initialized connectionist temporal classiﬁcation-based acoustic model ‘decoder’ partially initialized recurrent neural network language model trained text data alone. entire neural network trained rnn-t loss directly outputs recognized transcript sequence graphemes thus performing end-to-end speech recognition. performance improved further sub-word units capture longer context signiﬁcantly reduce substitution errors. best rnn-t system twelve-layer lstm encoder two-layer lstm decoder trained wordpieces output targets achieves word error rate voice-search voice-dictation tasks comparable state-of-the-art baseline voicesearch voice-dictation. current state-of-the-art automatic speech recognition systems break problem three main sub-problems acoustic pronunciation language modeling. speech recognition involves determining likely word sequence given acoustic input sequence represents number frames utterance recurrent neural network transducer particular architectures allow output decoded soon ﬁrst input encoded without introducing additional latency incurred processing entire utterance once. work consider streaming recognition architectures speciﬁcally rnn-t model. despite recent work end-to-end conventional systems still remain state-of-the-art terms word error rate performance. example previous work evaluated number end-to-end models including attention-based models rnn-t trained hours transcribed training data; although end-to-end approaches found comparable state-of-the-art context-dependent phone-based baseline dictation test sets models found signiﬁcantly worse baseline voice-search test sets. end-to-end systems typically trained using transcribed acoustic data sets relatively expensive generate thus much smaller text-only data sets used train traditional speech recognizer. deﬁciency end-to-end systems appears language modeling capacity large text-only data utilized end-to-end systems. work explore particular sequence-to-sequence architecure rnn-t show text pronunciation data included improve end-to-end performance. another contribution work investigate wordpieces explored previously context machine translation sub-word unit end-to-end speech recognition. paper organized follows section describe rnn-t used streaming recognition. section describes rnn-t trained including units architectures pre-training parts model. experimental setup including baseline system detailed section section compares word error rate performance various rnn-t models baseline show relative improvement. techniques introduced work mostly improve language modeling rnn-t section shows select examples improved recognition. concluding summary acknowledgements section section rnn-t proposed graves extension connectionist temporal classiﬁcation approach sequence labeling tasks alignment input sequence output targets unknown. accomplished formulation introducing special label called blank label models probability outputting label corresponding given input frame. widely used previous works train end-toend models however major limitation fig. rnn-t model. model consists encoder network maps input acoustic frames higher-level representation prediction joint network together correspond decoder network. decoder conditioned history previous predictions. assumption model outputs given frame independent previous output labels yj|x rnn-t model depicted figure consists encoder prediction network joint network; described rnn-t model compared encoder-decoder architectures listen attend spell view combination prediction network joint network decoder. encoder converts input acoustic frame higher-level representation henc analogous ctc-based standard speech recognizer. thus output encoder network henc conditioned sequence previous acoustic frames rnn-t removes conditional independence assumption introducing prediction network explicitly conditioned history previous non-blank targets predicted model. speciﬁcally prediction network receives input last non-blank label produce output hdec finally joint network feed-forward network combines outputs prediction network encoder produce logits followed softmax layer produce distribution next output symbol step inference rnn-t model next acoustic frame previously predicted label model produces next output label probabilities predicted label non-blank prediction network updated label input generate next output label probabilities conversely blank label predicted next acoustic frame used update encoder retain prediction network output resulting rnn-t stream recognition results alternating updating encoder prediction network based predicted label blank non-blank. inference terminated blank output last frame inference likely label sequence computed using beam search described minor alteration found make algorithm less computationally intensive without degrading performance skip summation preﬁxes pref unless multiple hypotheses identical. note unlike streaming encoder-decoder architectures prediction network conditioned encoder output. allows pre-training decoder language model text-only data described section investigate graphemes sub-words output lexical units rnn-t models. graphemes letters digits special symbols space symbol space symbol used segmenting recognized grapheme sequences word sequences. state-of-the-art large vocabulary speech recognition sysinference tems recognize millions different words rnn-t many output labels would impractically slow. therefore subword units wordpieces described train statistical wordpiece model word counts obtained text data segmenting word individually subwords. additional space symbol included subword units. example segmentation sentence tortoise hare <tor> <to> <ise> <space> <and> <space> <the> <space> <ha> <re>. wordpieces shown beneﬁt end-to-end recognition since offer balance longer context graphemes tunable number labels. since wordpiece model based word frequencies common words appear single label. vocabulary generated wordpieces includes words like ‘mall’ ‘remember’ ‘doctor’ vocabulary wordpieces also includes less common encoder networks rnn-t models experimented deep lstm networks decoder networks used stack layer lstm network feed-forward layer softmax layer. addition training models random initialization parameters explored variations initializing encoder decoder network parameters pre-trained models. previously shown initializing rnn-t encoder parameters model trained loss beneﬁcial phoneme recognition task experimented initializing encoder networks models trained loss initializing lstm layer parameters prediction networks lstm language models trained text data. initialization encoder prediction network weights separate pre-trained models entire rnn-t model weights trained rnn-t objective. show example architecture rnn-t wordpiece model figure ﬁgure also shows pre-trained lstm acoustic model lstm language model architectures used initialize encoder prediction network weights. dotted arrows indicate pre-trained layers used initialize speciﬁc layers rnn-t model. encoder networks rnn-t models pre-trained loss using phonemes graphemes wordpieces output units. investigate encoder architectures multitask training using hierarchical-ctc various ’hierarchies’ losses various depths encoder network. hierarchical-ctc encoder networks trained multiple simultaneous losses beneﬁcial grapheme recognition pre-training losses additional weights associated generating softmax probabilities discarded. wordpiece models longer duration graphemes employ additional ’time-convolution’ encoder network reduce sequence length encoded activations similar pyramidal sequence length reduction models used ﬁlters covering non-overlapping consecutive activation vectors thus reducing single activation vector. lstm layers decoder networks pre-trained language model using graphemes wordpieces lexical units. input network label segmented sentence represented one-hot vector. target network next label sequence model trained cross-entropy loss. weights softmax output layer discarded pre-training lstm network weights used partially initialize rnn-t prediction network. wordpiece language models embed labels smaller dimension. embedding weights also used initialize rnn-t wordpiece models. fig. various stages training wordpiece rnn-t. encoder network pre-trained hierarchical-ctc network simultaneously predicting phonemes graphemes wordpieces lstm layers respectively. time convolutional layer reduces encoder time sequence length factor three. decoder network trained lstm langauge model predicting wordpieces optimized cross-entropy loss. finally rnn-t network weights initialized pre-trained models indicated dashed lines entire network optimized using rnn-t loss. compare rnn-t end-to-end recognizer conventional system consisting separate acoustic pronunciation language models. acoustic model trained lstm predicts context-dependent phonemes ﬁrst ﬁne-tuned sequence discriminative training described improved word-level edit-based minimum bayes risk proposed recently shannon acoustic models trained million hand-transcribed anonymized utterances extracted google english voice trafﬁc corresponds hours training data. include voice-search well voice-dictation utterances. -dimensional ﬁlterbank energy features computed every stacked every single -dimensional acoustic feature vector. achieve noise robustness acoustic training data distorted described pronunciation model dictionary containing hundreds thousands human expert transcribed english word pronunciations. additional word pronunciations learned audio data using pronunciation learning techniques out-ofdictionary words model trained using transcribed word pronunciations. -gram language model trained text sentence dataset includes untranscribed anonymized speech logs million sentences voice-search voice-dictation queries anonymized typed logs including tens billion sentences google search various sources. language model pruned -million n-grams target vocabulary million various sources text data re-weighted using interpolation optimal word error rate performance. single-pass decoding conventional wfst carried generate recognition transcripts. rnn-t trained data baseline. encoder network pre-trained acoustic transcribed data baseline acoustic model pronunciation model used generate phoneme transcriptions acoustic data. rnn-t decoder pre-trained text data lstm language model roughly half billion sentences text data sampled according count data source interpolation weight rnn-t models trained lstm networks tensorﬂow toolkit asynchronous stochastic gradient descent. models evaluated using rnn-t beam search algorithm beam grapheme models wordpiece models temperature softmax. word error rate regrapheme based rnn-t trained scratch acoustic data -layer lstm encoder cells -layer lstm decoder cells. ﬁnal unit feed-forward layer softmax layer output grapheme label probabilities. compare model model identical architecture encoder pre-trained. pre-training helpful improving .%→.% voice-search .%→.% voice-dictation. model deeper -layer encoder also trained multi-ctc loss depth depth losses optimized grapheme targets. found training -layer models without multi-loss setup unstable acknowledge addressed recent advancements training deeper recurrent models tested part work. deeper -layer encoder improves .%→.% voice-search .%→.% voice-dictation. incorporate knowledge phonemes speciﬁcally pronunciation dictionary data train -layer encoder hierarchical-ctc phoneme target depth grapheme target depth network forced model phonemes exposed pronunciation variants labels word different pronunciations approach address including pronunciations words occur acoustic training data leave future work. pronunciation data improves .%→.% voice-search little improvement voice-dictation. unlike voice-search voice-dictation test comprised mostly common words conjecture sufﬁcient learn pronunciations words acoustic data alone thus beneﬁt additional human transcribed pronunciations. next include text data pre-train -layer lstm cells language model grapheme targets. model trained word perplexity held-out longer improves table shows word preplexity sizes various language models trained. addition text data improves .%→.% voice-search .%→.% voice-dictation. explore modeling wordpieces wordpieces instead graphemes make several changes architecture. wordpiece encoder network layer lstm cells each trained hierarchicalctc phoneme targets depth graphemes depth wordpieces depth since wordpieces longer units include time convolution depth reducing sequence length factor time convolution affect drastically reduces training inference time times fewer encoder features need processed decoder network. wordpiece language models trained similar graphemes since numbers labels much larger additional input embedding size used wordpiece models. wordpiece language models perform much better terms word perplexity rnn-t initialized also signiﬁcant improvements best endto-end rnn-t wordpieces achieves voice-search voice-dictation state-of-the-art baseline speech recognition system. observe large part improvements described work reduction substitution errors. using wordpieces instead graphemes results absolute word error rate improvement ﬁxing substitution errors. inclusion pronunciation text data improve voice-search word error rate absolute respectively improvements word substitution errors. many corrected substitution errors seem improved language modeling words sound similar different meaning given text context. selected examples include improvements proper nouns ‘barbara stanwick’ recognized grapheme model ﬁxed using wordpieces correct name ‘barbara stanwyck’. similar improvements found including pronunciation data ‘sequoia casino’ ‘sycuan casino’ ‘where there’ ‘where xur’ also including text data ‘soldier boy’ ‘soulja boy’ ‘lorenzo llamas’ ‘lorenzo lamas’. also wordpieces capture longer range language context graphemes improvements like ‘tortoise hair’ ‘tortoise hare’. train end-to-end speech recognition models using rnn-t predicts graphemes wordpieces thus directly outputs transcript audio. pre-training rnn-t encoder results relative improvement using deeper -layer encoder instead -layer encoder improves relative. incorporate pronunciation data using pre-training hierarchical-ctc loss includes phoneme targets improves voice-search relative little impact voice-dictation task. include text-only data pre-train recurrent network decoder lstm language models resulting overall relative table word error performance voice-search dictation tasks various rnn-t trained graphemes wordpieces various architectures pre-training. also shown model types training data included acoustic pronunciation text. baseline state-of-the-art conventional speech recognition system separate acoustic pronunciation language models trained available data. parameters baseline system include million weights acoustic model network million word pronunciation dictionary million n-grams language model. improvement. train wordpiece rnn-ts wordpieces targets signiﬁcantly outperform grapheme-based rnn-ts. comparison baseline speech recognizer individual acoustic pronunciation language models state-of-the-art wers voice-search voice-dictation. wordpiece rnn-t achieving wers voicesearch voice-dictation demonstrate single end-to-end neural model capable state-of-the-art streaming speech recognition. authors would like thank colleagues franc¸oise beaufays alex graves leif johnson helpful research discussions mike schuster help wordpiece models. vinyals heigold senior mcdermott monga sequence discriminative distributed training long short-term memory recurrent neural networks interspeech abadi agarwal barham brevdo chen citro corrado davis dean devin tensorﬂow large-scale machine learning heterogeneous distributed systems corr vol. abs/. graves a.-r. mohamed hinton speech recognition deep recurrent neural networks international conference machine learning representation learning workshop graves fern´andez gomez schmidhuber connectionist temporal classiﬁcation labeling unsegmented sequence data recurrent neural networks proc. international conference machine learning eyben w¨ollmer schuller graves from speech letters-using novel neural network architecture grapheme based workshop automatic speech recognition understanding ieee", "year": "2018"}