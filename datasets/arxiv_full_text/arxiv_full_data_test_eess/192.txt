{"title": "Over the Air Deep Learning Based Radio Signal Classification", "tag": "eess", "abstract": " We conduct an in depth study on the performance of deep learning based radio signal classification for radio communications signals. We consider a rigorous baseline method using higher order moments and strong boosted gradient tree classification and compare performance between the two approaches across a range of configurations and channel impairments. We consider the effects of carrier frequency offset, symbol rate, and multi-path fading in simulation and conduct over-the-air measurement of radio classification performance in the lab using software radios and compare performance and training strategies for both. Finally we conclude with a discussion of remaining problems, and design considerations for using such techniques. ", "text": "greatly increased capacity feature learning directly high dimensional input data based high level supervised objectives found capacity learning large neural network models high numbers free parameters. made possible combination strong regularization techniques greatly improved methods stochastic gradient descent cost high performance graphics card processing power combining neural network architecture innovations convolutional neural networks rectiﬁed linear units alexnet many techniques used together realize increase several orders magnitude practical model size parameter count target dataset task complexity made feature learning directly imagery state art. point trend relentless towards replacement rigid simpliﬁed analytic features models approximate models much accurate high degrees freedom models derived data using end-to-end feature learning. trend demonstrated vision text processing voice widely applied fully realized radio time series data sets recently. showed methods readily applied simulated radio time series sample data order classify emitter types excellent performance obtaining equivalent accuracies several times sensitive existing best practice methods using feature based classiﬁers higher order moments. work provide extensive dataset additional radio signal types realistic simulation wireless propagation environment measurement dataset methods signal classiﬁcation drastically outperform initially introduced depth analysis many practical engineering design system parameters impacting performance accuracy radio signal classiﬁer. statistical modulation features digital modulation techniques higher order statistics cyclo-stationary moments among widely used features compactly sense detect signals strong periodic components created structure abstract—we conduct depth study performance deep learning based radio signal classiﬁcation radio communications signals. consider rigorous baseline method using higher order moments strong boosted gradient tree classiﬁcation compare performance approaches across range conﬁgurations channel impairments. consider effects carrier frequency offset symbol rate multi-path fading simulation conduct over-the-air measurement radio classiﬁcation performance using software radios compare performance training strategies both. finally conclude discussion remaining problems design considerations using techniques. rapidly understanding labeling radio spectrum autonomous enabler spectrum interference monitoring radio fault detection dynamic spectrum access opportunistic mesh networking numerous regulatory defense applications. boiling complex high-data rate ﬂood information precise accurate labels acted conveyed compactly critical component today numerous radio sensing communications systems. many years radio signal classiﬁcation modulation recognition accomplished carefully handcrafting specialized feature extractors speciﬁc signal types properties deriving compact decision bounds using either analytically derived decision boundaries statistical learned boundaries within low-dimensional feature spaces. past years seen rapid disruption occurring based improved neural network architectures algorithms optimization techniques collectively known deep learning recently replaced machine learning state computer vision voice natural language processing; ﬁelds feature engineering pre-processing critically important topics allowing cleverly designed feature extractors transforms extract pertinent information manageable reduced dimension representation labels decisions could readily learned tools like support vector machines decision trees. among widely used front-end features scale-invariant feature transform words mel-frequency cepstral coefﬁcients others widely relied carrier frequency offset carrier phase frequency offset disparate local oscillators motion symbol rate offset symbol clock offset time dilation disparate clock sources motion. delay spread non-impulsive delay spread delayed reﬂection diffraction diffusion emissions multiple paths. thermal noise additive white-noise impairment effects compactly modeled well present form wireless propagation medium. numerous additional propagation effects also modeled synthetically beyond scope exploration here. relies today optimize large parametric neural network models. since alexnet techniques described section numerous architectural advances within computer vision leading signiﬁcant performance improvements. however core approach remains largely unchanged. neural networks comprised series layers layer input output using parametric dense matrix operations followed non-linearities. expressed simply follows weights dimension bias dimension applied element-wise peroutput activation functions). back propagation loss gradients used iteratively update network weights epoch within network validation loss longer decreasing. adam optimizer whose form roughly follows conventional expression below except complex time varying expression learning rate beyond scope work. carrier symbol timing symbol structure certain modulations. incorporating precise knowledge structure expected values peaks auto-correlation function spectral correlation function surfaces used successfully provide robust classiﬁcation signals unknown purely random data. analog modulation symbol timing produce artifacts statistical features useful performing signal classiﬁcation. homs derive number higher order cumulantss shown effective discriminators many modulation types hocs computed combinatorially using homs expression varying slightly; show example expression hom. additionally consider number analog features capture statistical behaviors useful include mean standard deviation kurtosis normalized centered amplitude instantaneous frequency absolute normalized instantaneous frequency several others shown useful prior work. decision criterion mapping baseline features class label number compact machine learning analytic decision processes used. probabilistically derived decision trees expert modulation features among ﬁrst used ﬁeld many years decision processes also trained directly datasets represented feature space. popular methods include support vector machines decision trees neural networks ensembling methods combine collections classiﬁers improve performance. among ensembling methods boosting bagging gradient tree boosting particular xgboost proven extremely effective implementation gradient tree boosting used successfully winners numerous kaggle data science competitions work xgboost approach feature classiﬁer outperforms single decision tree method evaluated consistently case reduce ﬁtting training data regularization used. batch normalization regularization convolutional layers alpha dropout regularization fully connected layers. detail descriptions additional layers used including softmax max-pooling beyond scope work described fully generate datasets investigation building upon improved version tools described different analog digital modulators used cover wide range single carrier modulation schemes. consider several different propagation scenarios context work ﬁrst several simulated wireless channels generated model shown ﬁgure second consider transmission channel clean signals shown ﬁgures synthetic channel impairments. digital signals shaped root-raised cosine pulse shaping ﬁlter range roll-off values example synthetic data sets independently draw random value variables shown table results uncorrelated random channel initialization example. figure illustrates several random values channel impulse response envelope different delay spreads relating different levels multi-path fading increasingly difﬁcult rayleigh fading environments. figure illustrate examples training using simulated channel relatively information density commonly seen impaired environments. signals represent relatively simple classiﬁcation task high cases somewhat comparable canonical mnist digits. second introduce difﬁcult dataset contains modulations. include number high order modulations used real world high-snr low-fading channel environments impulsive satellite links however apply impairments beyond would expect scenario consider relatively short-time observation windows classiﬁcation number samples short time classiﬁcation hard problem since decision processes wait acquire data increase certainty. case many real world systems dealing short observations short signal bursts environment. effects examples would expect able achieve anywhere near classiﬁcation rates full dataset making good benchmark comparison future research comparison. speciﬁc modulations considered within normal classes bpsk qpsk am-ssb-sc am-dsb-sc gmsk oqpsk difﬁcult classes bpsk qpsk apsk apsk apsk apsk am-ssb-wc am-ssb-sc am-dsb-wc am-dsb-sc gmsk oqpsk additional simulating wireless channel impairments also implement test-bed modulate transmit signals using universal software radio peripheral software deﬁned radio second receive transmissions relatively benign indoor wireless channel band. radios analog devices radio frequency integrated circuit radio front-end provides frequency stability around parts million off-tune signal around avoid signal impairment associated direct conversion store signals base-band received test emissions stored unmodiﬁed along ground truth labels modulation emitter. baseline method leverages list higher order moments aggregate signal behavior statistics given table compute statistics sample example translate example feature space real values associated statistic example. representation reduced dimension example making classiﬁcation task much simpler also discarding vast majority data. ensemble model gradient boosted trees classify modulations features outperforms single decision tree support vector machine signiﬁcantly task. followed fully connected layers classiﬁers used computer vision problems. question structure networks explored several basic design principals networks introduced following approach generally straight forward construct cnns good performance. adapt architecture principals improving upon similar networks represents simple design approach readily trained deployed effectively accomplish many small radio signal classiﬁcation tasks. signiﬁcant note here features samples radio signal example normalized unit variance. perform expert feature extraction pre-processing radio signal instead allowing network learn timenetwork algorithms architectures improved since alexnet made effective training deeper networks using wider layers possible leading improved performance. original work employ small convolutional neural network several layers improve prior state art. however computer vision space idea deep residual networks become increasingly effective deep residual network shown ﬁgure notion skip bypass connections used heavily allowing features operate multiple scales depths network. signiﬁcant improvements computer vision performance also used effectively time-series audio data residual networks time-series radio classiﬁcation investigated seen train fewer epochs provide signiﬁcant performance improvements terms classiﬁcation accuracy. revisit problem modulation recognition modiﬁed residual network obtain improved performance compared dataset. basic residual unit stack residual units shown ﬁgure network architecture best architecture shown table also employ self-normalizing neural networks fully connected region network employing scaled exponential linear unit activation function meanresponse scaled initialization alpha dropout provides slight improvement conventional relu performance. based approach radio signal classiﬁcation must carefully considered designing solution. section explore several common design parameters impact classiﬁcation accuracy including radio propagation effects model size/depth data sizes observation size signal modulation type. ﬁrst compare performance lower difﬁculty dataset lower order modulation types. training dataset million example samples long obtain excellent performance high resnet cnn. resnet achieves roughly higher sensitivity equivalent classiﬁcation accuracy baseline high maximum classiﬁcation accuracy rate achieved resnet network achieves baseline method achieves accuracy. lower snrs performance resnet networks virtually identical highsnr performance improves considerably using resnet obtaining almost perfect classiﬁcation accuracy. samples examples residual stacks. here residual network provides best performance high snrs difﬁcult dataset margin improved sensitivity equivalent classiﬁcation accuracy. real world scenario wireless signals impaired number effects. awgn widely used simulation modeling effects described present almost universally. interesting inspect well learned classiﬁers perform impairments compare rate degradation impairments traditional approaches signal classiﬁcation. ﬁgure plot performance residual network based classiﬁer considered impairment model. includes awgn σclk minor offset σclk moderate offset several fading models ranging fading models moderate offset assumed well. interestingly plot resnet performance improves offset rather degrading. additional offset results spinning dilated versions original signal appears positive regularizing effect learning process provides quite noticeable improvement performance. high performance ranges around best case worst case. ﬁgure show degradation baseline classiﬁer impairments. case offset never helps performance instead degrades offset fading effects best case high method obtains accuracy worst case degrades around accuracy. remainder paper consider much harder task class high order modulations containing higher information rates much easily confused classes multiple high order psks apsks qams. signal classiﬁcation additive white gaussian noise canonical problem explored many years communications literature. simple starting point condition analytic feature extractors generally perform best ﬁgure compare performance resnet network baseline method full dataset show signiﬁcant gains large architectures deep residual networks larger dataset begin quite beneﬁt additional depth. likely signiﬁcantly larger number examples classes used. ﬁgure show increasing validation accuracy deep residual networks introduce residual stack units within network architecture performance steadily increases depth case diminishing returns approach around layers. considering primitive layers within network resnet layers trainable parameters layers trainable parameters. results shown ﬁgure show performance classiﬁer individual modulation types. detection performance modulation type varies drastically signal noise ratio signals lower information rates vastly different structure analog modulations much readily identiﬁed high-order modulations require higher snrs robust performance never reach perfect classiﬁcation rates. however modulation types reach rates accuracy around snr. ﬁgure show confusion matrix classiﬁer across classes awgn validation examples greater equal zero. largest sources error high order phase shift keying high order quadrature amplitude modulation well modes suppressed-carrier largely expected short time observations noisy observations high order extremely difﬁcult tell apart approach. many real world systems unsynchronized doppler frequency offset nearly performance advantage resnet approach baseline accuracy increase high snr. section models trained using comparison. model size signiﬁcant impact ability large neural network models accurately represent complex features. computer vision convolutional layer based models imagenet dataset started around layers deep modern state networks imagenet often layers deep recently even layers. initial investigations deeper networks dataset often enormous impact quality model learned. consider inﬂuence number example signals training well timelength individual example number samples ﬁgure show performance resulting model changes based total number training examples used. dataset size dramatic impact model training high classiﬁcation accuracy near random examples improves doubling around results illustrate sufﬁcient training data critical performance. largest case million examples training single state nvidia graphics processing unit takes around hours reach stopping point making signiﬁcant experimentation dataset sizes cumbersome. signiﬁcant improvement going examples indicating point diminishing returns number examples around conﬁguration. either examples obtain roughly test accuracy high snr. class-confusion matrix best figure shows model performance varies window size number time-samples example used single classiﬁcation. obtain approximately accuracy improvement doubling input size signiﬁcant diminishing returns reach cnns scale well size need additional scaling strategies thereafter larger input windows simply memory requirements training time requirements dataset requirements. generate examples modulation dataset using usrp setup described above. using partition training test directly train resnet classiﬁcation. nvidia around hours obtain test accuracy dataset examples roughly snr. also consider signal classiﬁcation transfer learning problem model trained synthetic data evaluated and/or ﬁne-tuned data. full model training take hours high typically requires large dataset effective transfer learning convenient alternative leveraging existing models updating smaller computational platforms target datasets. consider transfer learning freeze network parameter weights layers except last several fully connected layers network updating. commonly done today computer vision models common start using pre-trained model weights imagenet similar datasets perform transfer learning using another dataset classes. case many lowlevel features work well different classes datasets need change tuning. case consider several cases start models trained simulated wireless impairment models using residual networks evaluate examples. accuracies initial models synthetic data shown ﬁgure ranged hard -class dataset. evaluating performance models data without model updates obtain classiﬁcation accuracies ﬁne-tuning last layers models data using transfer learning recover approximately additional accuracy. validation accuracies shown process ﬁgure resnet update epochs dense layers examples take roughly seconds titan card execute instead full seconds card epoch updating model weights. accuracy evaluating data obtaining accuracy primary confusion cases prior training seem dealing suppress non-suppressed carrier analog signals well high order apsk modes. seems like perhaps best suited among models match data. small impairments present data radios used extremely stable oscillators present short example lengths radios essentially right next other providing clean impulsive direct path reﬂections surrounding room likely signiﬁcantly attenuated comparison making near impulsive channel. training harsher impairments seemed degrade performance data signiﬁcantly. suspect evaluate performance model increasingly harsh real world scenarios transfer learning favor synthetic models similarly impaired closely match real wireless conditions important class systems train either directly target signal environments good impairment simulations well suited models derived. possible mitigation include domain-matched attention mechanisms radio transformer network network architecture improve generalization varying wireless propagation conditions. work extended prior work using deep convolutional neural networks radio signal classiﬁcation heavily tuning deep residual networks task. also conducted much thorough performance evaluations type classiﬁer performs wide range design parameters channel impairment conditions training dataset parameters. residual network approach achieves state modulation classiﬁcation performance difﬁcult signal database synthetically performance. architectures still hold signiﬁcant potential radio transformer networks recurrent units approaches still need adapted domain tuned quantitatively benchmarked dataset future. works explored degree generally sufﬁcient hyper-parameter optimization meaningful. shown that contrary prior work deep networks provide signiﬁcant performance gains time-series radio signals need deep feature hierarchies apparent residual networks highly effective build structures traditional cnns struggle achieve performance make effective deep networks. also shown simulated channel effects especially moderate impairments improve effect transfer learning signal evaluation performance topic require signiﬁcant future investigation optimize synthetic impairment distributions used training. methods continue show enormous promise improving radio signal identiﬁcation sensitivity accuracy especially short-time observations. shown deep networks increasingly effective leveraging deep residual architectures shown synthetically trained deep networks effectively transferred datasets loss around accuracy directly trained effectively data enough training data available. large well labeled datasets often difﬁcult obtain tasks today channel models difﬁcult match real-world deployment conditions quantiﬁed real need training systems helped quantify performance impact still much learn best curate datasets training regimes class systems. however demonstrated work approach provides roughly performance high datasets equivalent synthetic datasets major step towards real world use. demonstrated transfer learning effective able achieve equivalent performance direct training large datasets using transfer learning. simulation methods become better ability match synthetic datasets real world data distributions improves close transfer learning become increasingly important tool real data capture labeling difﬁcult. performance trades shown work help shed light parameters data generation training hopefully helping increase understanding focus future efforts optimization systems. nandi azzouz algorithms automatic modulation recognition communication signals ieee transactions communications vol. friedman greedy function approximation gradient boosting machine annals statistics cioni colavolpe mignone modenini morello ricciulli ugolini zanettini transmission parameters optimization receiver architectures dvb-sx systems international journal satellite communications networking vol. goodfellow bengio courville deep learning. press oshea corgan clancy convolutional radio modulation recognition networks international conference engineering applications neural networks springer spooner mody chuang petersen modulation recognition using second-and higher-order cyclostationarity dynamic spectrum access networks ieee international symposium ieee fehske gaeddert reed approach signal classiﬁcation using spectral correlation neural networks frontiers dynamic spectrum access networks dyspan first ieee international symposium ieee srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting. journal machine learning research vol. zhang delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation proceedings ieee international conference computer vision ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning szegedy sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolutions proceedings ieee conference computer vision pattern recognition abdelmutalab assaleh el-tarhuni automatic modulation classiﬁcation based high order cumulants hierarchical polynomial classiﬁers physical communication vol.", "year": "2017"}