{"title": "Sample-level CNN Architectures for Music Auto-tagging Using Raw  Waveforms", "tag": "eess", "abstract": " Recent work has shown that the end-to-end approach using convolutional neural network (CNN) is effective in various types of machine learning tasks. For audio signals, the approach takes raw waveforms as input using an 1-D convolution layer. In this paper, we improve the 1-D CNN architecture for music auto-tagging by adopting building blocks from state-of-the-art image classification models, ResNets and SENets, and adding multi-level feature aggregation to it. We compare different combinations of the modules in building CNN architectures. The results show that they achieve significant improvements over previous state-of-the-art models on the MagnaTagATune dataset and comparable results on Million Song Dataset. Furthermore, we analyze and visualize our model to show how the 1-D CNN operates. ", "text": "recent work shown end-to-end approach using convolutional neural network effective various types machine learning tasks. audio signals approach takes waveforms input using convolution layer. paper improve architecture music auto-tagging adopting building blocks state-of-the-art image classiﬁcation models resnets senets adding multi-level feature aggregation compare different combinations modules building architectures. results show achieve signiﬁcant improvements previous state-of-the-art models magnatagatune dataset comparable results million song dataset. furthermore analyze visualize model show operates. time-frequency representations based short-time fourier transform often scaled log-like frequency melspectrogram common choice input majority state-of-the-art music classiﬁcation algorithms -dimentional input represents acoustically meaningful patterns well requires parameters window size/type size different optimal settings depending type input signals. order overcome problem efforts directly waveforms input particularly convolutional neural networks based models show promising results models used large ﬁlters expecting replace fourier transform. recently addressed problem using small ﬁlters successfully applied music autotagging task. inspired well-known uses small size ﬁlters sample-level model conﬁgured take waveforms input ﬁlters small granularity. number techniques improve performances cnns appeared recently image domain. introduced resnets includes skip connections enables deep effectively trained makes gradient propagation ﬂuent using skip connections could successfully train -layer resnet proposed senets includes building block called squeeze-and-excitation unlike recent approaches block concentrates channel-wise information spatial. block adaptively recalibrates feature maps using channel-wise operation. techniques developed ﬁeld computer vision fully adopted music classiﬁcation tasks. although approaches readily apply audio domain used representations input used large ﬁlters ﬁrst convolutional layer hand methods concerned overall architecture model rather designing ﬁne-grained building block speciﬁcally multi-level feature aggregation combines several hidden layer representations ﬁnal prediction signiﬁcantly improved performance music auto-tagging taking different levels abstractions labels account. paper explore building blocks advanced architectures resnets senets based sample-level music auto-tagging. also observe multi-level feature aggregation affects performance. results show achieve signiﬁcant improvements previous state-of-the-art models magnatagatune dataset comparable results million song dataset. furthermore analyze visualize model built blocks show operates. results show input signals processed different manner depending level layers. fig. proposed architecture music auto-tagging. models consist strided convolutional layer blocks fully-connected layers. outputs last three blocks concatenated used input last layers. output dimensions block denoted inside convolutional building blocks evaluate. utilize block senets increase representational power basic block. shown figure simply attached block basic block. block recalibrates feature maps basic block operations. squeeze operation aggregates global temporal information channel-wise statistics using global average pooling. operation reduces temporal dimensionality averaging outputs channel. excitation operation adaptively recalibrates feature maps channel using channel-wise statistics squeeze operation simple gating mechanism. gating mechanism consists fully-connected layers compute nonlinear interactions among channels. finally original outputs basic block rescaled channel-wise multiplication feature sigmoid activation second layer. inspired skip connections resnets modiﬁed basic block adding skip connection shown figure res-n denotes block uses convolutional layers two. speciﬁcally res- block additional layers denoted dotted line figure res- block skip connection only. block uses convolutional layers dropout layer convolutions avoid overﬁtting. technique ﬁrstly introduced wideresnets nated delivered layers. concatenation temporal dimensions outputs reduced global pooling. unlike concatenation occurs training average pooling whole audio clip followed global pooling included. evaluated proposed architectures datasets magnatagatune dataset million song dataset annotated last.fm tags split ﬁltered datasets following previous work used frequent tags. songs trimmed seconds long resampled needed. song divided segments samples. evaluate performance music autotagging multi-class multi-label classiﬁcation task computed area receiver operating characteristic curve computed average across tags. evaluation average predictions across segments. networks trained using nesterov momentum mini-batch size initial learning rate decayed factor validation loss plateau. none regularizations used msd. dropout layer inserted last layer mtat. building blocks evaluated either without multi-level feature aggregation. since training takes much time longer mtat explored architectures mainly mtat trained best models msd. code models built tensorflow keras available link. model multi-scaled features end-to-end transfer learning persistent time-frequency timbre crnn multi-level multi-scale samplecnn multi-features samplecnn rese table summarizes evaluation results compared architectures mtat dataset. show block effective res-n blocks increasing performance basic block cases. res-n block adding skip connection basic block actually decreases performance. combination res- improves slightly more. however training time rese- times longer basic block whereas block times longer. thus training prediction time models important model preferred rese-. effect multi-level aggregation valid majority models. obtained best results table using multi-level aggregation. table compares previous state-of-the-art models music auto-tagging best models block rese block multi-level aggregation. mtat dataset best models outperform previous results. best comparable second-tier. groundwork understanding cnns operate analyze sigmoid activations excitations blocks different levels graphically quantitatively. section observe blocks recalibrate channels depending level exist. blocks used analysis model using multi-level analysis chose three tags classical metal dance similar shown table figure shows average sigmoid activations blocks songs three tags. different levels activations indicate blocks process input audio differently depending music. every block figure ﬁres different patterns activations speciﬁc channel. trend strongest ﬁrst block weakest block becomes stronger last block trend somewhat different observed image domain exclusiveness average excitation input different labels monotonically increasing along layers. speciﬁcally ﬁrst block ﬁres high activations classical ones dance even lower ones metal majority channels. hand activations last block vary depending tags. example activations metal high channels others makes activations noisy even though sorted. interpret result follows. ﬁrst block normalizes loudness audios block ﬁres high activations classical music tend small volume activations metal music tend large volume. also middle block processes common features among similar levels activations. finally noisy exclusiveness last block indicates effectively discriminate music different tags. assure exclusiveness trend measuring standard deviations activations across tags every level. figure shows higher standard deviation block responses song differently according tag. result shows standard deviation highest ﬁrst block drops stays block increases gradually last block. four lower blocks except bottom tend handle general features whereas four upper blocks tend progressively discriminative features. proposed convolutional building blocks based previous work sample-level resnets senets. rese block combination three models showed best performance. also multi-level feature aggregation showed improvements majority building blocks. experiments obtained stateof-the-art performance mtat dataset high-ranked results msd. addition analyzed activations excitation model understand effect. analysis could observe blocks process nonsimilar songs exclusively different levels model process songs different manner. jongpil juhan multi-level multiscale feature aggregation using pretrained convolutional neural networks music auto-tagging ieee signal processing letters vol. a¨aron oord sander dieleman benjamin schrauwen transfer learning supervised pretraining audio-based music classiﬁcation international society music information retrieval conference keunwoo choi gy¨orgy fazekas mark sandler automatic tagging using deep convolutional neural networks international society music information retrieval conference jongpil jiyoung park keunhyoung luke juhan sample-level deep convolutional neural networks music auto-tagging using wavesound music computing conference forms shawn hershey sourish chaudhuri daniel ellis jort gemmeke jansen channing moore manoj plakal devin platt saurous bryan seybold malcolm slaney weiss kevin wilson architectures large-scale audio classiﬁcation icassp. ieee jongpil juhan multi-level multiscale feature aggregation using sample-level deep convolutional neural networks music classiﬁcation machine learning music discovery workshop international conference machine learning jeff donahue yangqing oriol vinyals judy hoffman ning zhang eric tzeng trevor darrell decaf deep convolutional activation feature generic visual recognition international conference machine learning edith kris west michael mandel mert stephen downie evaluation algorithms using games case music tagging international society music information retrieval conference thierry bertin-mahieux daniel ellis brian whitthe million song dataset paul lamere international society music information retrieval conference vol. jordi pons olga slizovskaia rong gong emilia g´omez xavier serra timbre analysis music audio signals convolutional neural networks arxiv preprint arxiv.", "year": "2017"}