{"title": "Omnidirectional CNN for Visual Place Recognition and Navigation", "tag": "eess", "abstract": " $ $Visual place recognition is challenging, especially when only a few place exemplars are given. To mitigate the challenge, we consider place recognition method using omnidirectional cameras and propose a novel Omnidirectional Convolutional Neural Network (O-CNN) to handle severe camera pose variation. Given a visual input, the task of the O-CNN is not to retrieve the matched place exemplar, but to retrieve the closest place exemplar and estimate the relative distance between the input and the closest place. With the ability to estimate relative distance, a heuristic policy is proposed to navigate a robot to the retrieved closest place. Note that the network is designed to take advantage of the omnidirectional view by incorporating circular padding and rotation invariance. To train a powerful O-CNN, we build a virtual world for training on a large scale. We also propose a continuous lifted structured feature embedding loss to learn the concept of distance efficiently. Finally, our experimental results confirm that our method achieves state-of-the-art accuracy and speed with both the virtual world and real-world datasets. ", "text": "fig. overview. visual place recognition system takes omnidirectional visual input robot then retrieve closest place exemplar using o-cnn. o-cnn used help navigate robot closest place. main technical contribution lies learn robust distance function. take advantage modern omnidirectional camera conﬁguration deep learning techniques model. hand omnidirectional cameras become widely available consumer market. literature shown omnidirectional camera improve quality mapping localization hand cnnbased methods proposed estimate camera pose recognize place information etc. combining ideas propose novel omnidirectional convolutional neural network trained completely virtual environments. virtual world offers label-free agent-safe experimental environments. o-cnn three important design elements. firstly apply circular padding image feature spaces reﬂect fact omnidirectional images true image boundary. secondly propose novel roll branching approach conquer rotational variation captured omnidirectional images. finally modify lifted structured feature embedding offer model concept distance environments call continuous lifted structured feature embedding. show experimental results method effective efﬁcient indoor place recognition several strong baselines virtual world real-world data. finally combining method proposed abstract— visual place recognition challenging especially place exemplars given. mitigate challenge consider place recognition method using omnidirectional cameras propose novel omnidirectional convolutional neural network handle severe camera pose variation. given visual input task o-cnn retrieve matched place exemplar retrieve closest place exemplar estimate relative distance input closest place. ability estimate relative distance heuristic policy proposed navigate robot retrieved closest place. note network designed take advantage omnidirectional view incorporating circular padding rotation invariance. train powerful ocnn build virtual world training large scale. also propose continuous lifted structured feature embedding loss learn concept distance efﬁciently. finally experimental results conﬁrm method achieves stateof-the-art accuracy speed virtual world real-world datasets. visual place recognition critical task robot navigation. instance delivering robot needs recognize place order deliver goods speciﬁc destination. many place exemplars given main challenge place recognition efﬁciently retrieve matched exemplars hand place exemplars given many robot visual inputs away exemplars map. order localize robot onto main challenge becomes navigate closest place exemplar given signiﬁcantly different camera poses visual inputs exemplars. although learning-based methods proposed improve robustness retrieving exemplars large camera pose variation typically address navigate closest place exemplar. paper consider robot given place exemplars current location unknown away exemplars. goal retrieve closest place exemplar navigate closest place exemplar propose learn function estimate distance visual input place exemplar captured different places. using function estimate closest place robots current location. propose heuristic policy take actions reduce made following contributions ﬁrst using novel model designed omnidirectional visual input address indoor place recognition problems o-cnn consists three unique designs improve performance constructed indoor virtual world real world dataset research use. ﬁnal experimental results conﬁrm proposed method achieves state-of-the-art accuracy speed virtual world real-world datasets. types visual descriptors place recognition local feature descriptors global feature descriptors. local feature descriptors describe image noticeable parts. methods include sift surf recently used orbslam system etc. hand global descriptors describe whole image using histogram principle component analysis corners edges colors etc. methods extracting local feature descriptors uniformly whole image considered kind global descriptor well convolution neural networks recently used learned global feature extractors. several pieces research demonstrated ability cnns handle appearance variation changing environment camera pose variation visual place recognition navigation highly related ﬁelds place recognition systems usually contain maps serve record visited places different level abstraction and/or metric information. simple mapping framework pure image retrieval methods hierarchical vocabulary trees adopted schindler improve efﬁciency scale large. place recognition methods directly regress -dof camera pose observation obtain absolute position methods typically assume place exemplars abundant nicely cover space visual inputs. increase efﬁciency matching process methods topological maps record relative positions visited locations method system matches current location visited locations needs consider places near current location topological based methods enhanced incorporating metric information distance orientation different nodes topological included metric information ranges sparse landmark given topological robot navigation abstractly done following edges connecting nodes map. furthermore robot navigation precise dense metric data. map-less navigation methods also common methods tackled obstacle avoidance visual observation. relevant works focus using visual observation navigation tasks scenario mapped map-less navigation. given retrieved closest place need navigate closest place without information. once closest place reached apply map-based navigation approaches. deep metric learning plays important role work mapping appearance difference spatial metric distance robot’s visual input exemplars map. previous works present algorithms learn representations estimate difference data samples present method learn hierarchical nonlinear transformations project face pairs feature subspace. hoffer train triplet network learn representations using distance comparisons. song propose algorithm training batches neural network lifted structured feature embedding. inspired previous works propose continuous lifted structure feature embedding learns representations robustly estimate real-world relative distances pair omnidirectional visual inputs. main contribution o-cnn extracts deep features form omnidirectional images visual place recognition. section ﬁrst deﬁne visual place recognition system. then o-cnn described. finally heuristic navigation policy introduced show effectiveness o-cnn-based system. given consisting place exemplar images visual place recognition system designed retrieve closest place exemplar respect robot help robot navigate closest place. encode exemplar images robot visual input using whole-image descriptor/feature extracted ocnn. descriptor robot input compared stored descriptors exemplars computing feature distance. minimal feature distance indicates closest place exemplar terms real-world location. addition robot getting closer place exemplar distance descriptor current visual input descriptor place exemplar decrease. information used robot’s navigation. fig. network architecture. o-cnn circular padding convolution operation googlenet. feature extraction perform roll branching make architecture robust purely perspective rotation compute lifted structure embedding loss training. illustration circular convolution operation. take omnidirectional image example actually perform operation feature maps every convolution layer. illustration roll branching roll branching shifted feature map. input o-cnn omnidirectional image output o-cnn feature number discretized rotations e.g. discretized rotation cover degree rotation dimension feature vector part image speciﬁed rotation. introduce three components improve performance method. deal omnidirectional images last loss function adapted lifted structure metric learning training stage omnidirectional image paired room label position label rotation label data paired based explained equation illustration whole architecture fig. circular padding efﬁciently using information omnidirectional image circular padding padding method speciﬁcally designed omnidirectional images order eliminate loss information adopting padding methods zero padding. circular padding horizontal axis left image rightmost part image vice versa; vertical axis upper left image upper right vice versa. apply circular padding every convolution layer since padding feature space brings slightly better performance experiments. illustration circular padding found fig. roll branching omnidirectional camera rotational invariance deal conditions omnidirectional images taken location different viewing angles relative positions feature space close other take discretized horizontal rotations consideration. given input feature shape roll branching layer outputs shifted left dimension. inference given extracted features image deﬁne rolling metric distance feature distance {||zl mink ˆrij relative rotation estimate ˆrij mink ˆrij. training relative rotation ground truth computed using rotation labels loss positive examples computed using masked gaussian distribution centered equation i.e. images positive pair rotated orientation computing loss. illustration roll branching found fig. continuous lifted structure feature embedding mapping feature difference space difference task subtle critical difference conventional metric learning. conventional metric learning aims inputs feature space similarity inputs estimated using distance similarity deﬁned discrete labels e.g. kitchen toilet. task poses challenging scenario need rank difference continuous position labels. example given ﬁxed target features smaller distance terms real-world position target closer target feature feature space. continuous lifted structured loss adapted powerful deep metric learning lifted loss jijori equation original lifted loss mines negative pairs respect examples positive pairs. contracts positive pair time pulls apart negative pairs. traditional metric learning deﬁnition positive pairs ﬁxed e.g. images taken bathroom always form positive pair whereas continuous lifted loss positiveness pair pair negative pair pseudonegative pair distance feature branch gaussian distribution centered rij. pair different room room label. labels pair deﬁne position distance exemplars physical world. ﬁrst randomly choose pair points room label positive pair pseudo-negative pair consists point positive pair points room position distance pseudo-negative pair larger positive dij}. time pair i.e. positive pair sampled contracted negative pseudo-negative pairs pulled apart once. positive pairs deﬁned using position distance results non-staticness pseudo-negative pairs obtained depending positive pairs pairs larger position distances accordingly picked times construct pairs larger position distances pulled apart feature space times smaller position distances. finally correspondence feature difference space difference established. local gradient-based heuristic policy heuristic navigation method leverages properties o-cnn features metric structure feature space corresponding real-world distances. relative rotation. given closest place exemplar current robot visual input initially policy orients robot angle target using relative rotation estimate rij. policy searches neighborhood area visiting -by- grid centered original position. visited location feature distance computed indicates close robot target forming potential ﬁeld lower potential closer target. call aforementioned procedure local searching process. subsequently robot goes along direction descending gradient potential ﬁeld heading location minimal feature distance. policy alternates local searching process heading location minimal feature distance robot incrementally gets closer target. note fig. typical examples virtual world framework. part shows simulated normal ﬁeld view camera used training data baselines. bottom part shows simulated omnidirectional camera. bottom-left image indicates location robot virtual environment. cameras integrated robot. show three typical examples virtual-world dataset. example real-world dataset. lifted structured embedding used googlenet feature extractor. training maximum training iterations weights initialized learning rate network pretrained place batch size margin parameter equirectangular omnidirectional images resized data pre-processing. implement codes tensorflow -dof camera pose estimation -scene used difference goal lack support omnidirectional data. omnidirectional datasets eynsham dataset fabmap outdoor omnidirectional datasets importantly don’t contain robot pose labels. reasons collected indoor omnidirectional dataset. collected demo scenes useful packages online asset store built another scenes based purchased assets. within total scenes designed emulate home scenarios ofﬁce scenarios shown table training evaluating proposed methods split scenes home ofﬁce scenarios training home ofﬁce scenarios testing set. average room counts scene average node counts room shown table note make virtual worlds consistent other ﬁxed height ceiling ﬂoor meters scene. furthermore make sure room different scenes least area fig. illustration data collection system. fig. shows typical examples virtual world scenarios. collecting training data place recognition manually picked recorded locations every single scene place exemplars. manually-picked place exemplar randomly sample locations around place exemplar place exemplar thus making place exemplar times larger. different original lifted structured embedding continuous lifted structured embedding requires continuous labels. result instead sampling close locations around selected locations pick additional locations close picked locations form place exemplars speciﬁcally o-cnn training. baseline methods require normal field-of-view images training instead omnidirectional images. order provide fair comparison location placed omnidirectional camera scene take omnidirectional image train model normal ﬁeld-of-view camera scene take four normal ﬁeld view images taken rotating degrees order train baseline methods orientation variations. four normal ﬁeld view images taken location used simulate output cubic projection omnidirectional image. mentioned section testing requires building retrieving closest exemplar. build hired student control robot room test virtual world scene. locations recorded mapping stage. collect test data retrieving hired another student randomly pick valid locations scene. average number nodes room mapping stage average number nodes room localization stage. training data collection place exemplar recorded omnidirectional image test method four normal ﬁeld-of-view images test baseline methods. typical examples virtual world data fig. real-world data collection utilized luna camera capture omnidirectional images different indoor scenes. indoor scenes home scenario ofﬁce scenario multiple rooms. data collection mimic robot traverses scene take photos. size outputted omnidirectional image place exemplars visual inputs house scene place exemplars visual inputs scene. make dataset available research purpose. clarify evaluation usage ground truth position place exemplar visual inputs real-world dataset obtained result running sfm. statistics collected dataset shown table table typical examples real-world data fig. section show experiments virtual world dataset real-world dataset. show effectiveness place recognition methods apply three metrics described below. experiments allow method predict closest place visual input. initially deﬁne error-tolerance consider predicted closest place correct within meters ground truth closest place. zero prediction correct ground truth closest place. recall-tolerance. recall deﬁned percentage correct prediction among visual inputs. designed metric show performance model different error tolerances recall-distance. ﬁxed error tolerance meters. then calculated recall visual inputs speciﬁc range distance away ground truth closest place. particular recall deﬁned percentage correct predictions among visual inputs meters away ground truth closest place. local navigation time step. show navigation ability different methods report average number steps method takes navigate random place closet place. ablation study o-cnn simple combination googlenet lifted structure embedding o-cnn added circular padding make network fully utilize information provided omnidirectional images. o-cnn lcr. added roll branching component make network invariant perspective changes introduced rotations location. o-cnn clcr. version continuous lifted structure feature embedding substitute lifted embedding keep circular padding roll branching components. compared approach three baselines. image retrieval methods chosen share similar goal ours. methods chosen similarities learning based method. made minor changes adapt baselines evaluation metric. disloc. disloc state-of-the-art traditional sift-based method using bag-of-word paradigm hamming embedding image retrieval. improve disloc applying geometric burstiness proposed re-rank photos clustering places based geo-tags. adapt disloc task built image vocabulary nfov images. worth mentioning fairly compare methods four nfov visual inputs taken single location it’s common equirectangular image converted cubic map. abandoned upper lower images cube took remaining four images nfov visual inputs. disloc variant returned four target node predictions based four current visual inputs applied mode estimation merge results. also disloc unable evaluated local navigation time step metric image representation concept distance. netvlad. netvlad cnn-based model. method incorporates encoding approach called vlad differentiable pooling layer mimics vlad encoding used replace last pooling layer typical model. recall idea vlad match descriptor closest cluster vocabulary. descriptor store differences descriptors belongs cluster centroid cluster. problem setting retrain netvlad omnidirectional images directly takes omnidirectional visual input obtain retrieved image mapping stage set. compare method baseline following evaluation metrics described v-a. posenet. posenet cnn-based camera pose estimation method. directly regresses camera pose given current visual input. end-to-end trained posenet exemplars follows instruction original paper. disloc four current visual inputs posenet. assigning camera pose closest node applying mode estimation obtain ﬁnal closest exemplar prediction result. show experiments virtual world datasets described section turns efﬁciency inference speed query image o-cnn netvlad disloc average. report performance measure below. recall-tolerance. according left panel fig. showed method better performance deep learning based methods especially tolerance small. moreover method outperforms disloc error tolerance lower meters takes much less inference time indicates method robust practical application. method achieves best average recall rate across different error-tolerance average recall baselines disloc netvlad posenet respectively. hand ablation study o-cnn clcr reaches best performance. additional ablation studies provided better illustration. shown plot continuous lifted feature embedding roll branching layer circular padding bring different degree improvements task. recall-distance. according left panel fig. method outperforms baselines exemplars larger meters away visual inputs. note posenet performing extremely bad. posenet conduct experiments indoor scene achieved reasonable performance. however indoor scene much smaller room scenes dataset. moreover ablation study o-cnn clcr reaches best performance consistently. hence designed components important. note show additional ablation study recall-tolerance space limit. local navigation time step. different features used heuristic policy navigate robot. compare ocnn netvlad disloc compared since signiﬁcantly slower inference. note heuristic policy netvlad omits information relative rotation derived method. episode navigation considered success robot reaches target node tolerance steps. table average method outperforms netvlad success rate speed. fig. recall-tolerance result virtual world. left plot report result compared baseline error tolerances meter. right plot provide result ablation study. continuous lifted loss roll branching layer bring considerable improvement task. table results local navigation virtual world. success rate means average successful trials avg. steps means average actions taken reach target. present experimental results real-world data. real world experiments tested o-cnn disloc superior performance virtual world. limitation collecting large-scale training data real world directly o-cnn pretrained virtual world test real world also leave local navigation time metric real world future work. recall-tolerance. according left panel fig. method still outperforms disloc real world error tolerance low. method achieves better average recall across error tolerance disloc recall-distance. according right panel fig. method also better disloc distance range larger furthermore averaging across distance range method’s mean recall also larger disloc’s paper main contributions present novel omnidirectional convolutional neural network improve visual place recognition heuristic policy navigate robot closest place map. propose virtual world framework containing lots indoor scenes. experiments show o-cnn outperforms previous learning-based methods netvlad posenet non-learning-based method disloc. however limitations proposed method retrieve closest exemplar computationally inefﬁcient generalize o-cnn features various scenarios. mitigate problems future works apply algorithm real-world robot navigation. acknowledgement. thank institute information science academia sinica most---f-- supports. thank tsu-ching hsiao construction virtual environments. also thank chingju cheng shun-chang zhong chung-chih huang fang-i hsiao ching-mao chen li-yeou wang reﬁning virtual scenes. eric brachmann alexander krull sebastian nowozin jamie shotton frank michel stefan gumhold carsten rother. dsacdifferentiable ransac camera localization. cvpr zetao chen obadiah adam jacobson michael milford. convolutional neural network-based place recognition. acra sumit chopra raia hadsell yann lecun. learning similarity metric discriminatively application face veriﬁcation. cvpr christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. cvpr iwan ulrich illah nourbakhsh. appearance-based place recogni yuke roozbeh mottaghi eric kolve joseph abhinav gupta fei-fei farhadi. target-driven visual navigation indoor scenes using deep reinforcement learning. icra", "year": "2018"}