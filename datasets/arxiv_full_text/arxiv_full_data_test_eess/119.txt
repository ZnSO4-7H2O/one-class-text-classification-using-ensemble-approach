{"title": "Performance Evaluation of Channel Decoding With Deep Neural Networks", "tag": "eess", "abstract": " With the demand of high data rate and low latency in fifth generation (5G), deep neural network decoder (NND) has become a promising candidate due to its capability of one-shot decoding and parallel computing. In this paper, three types of NND, i.e., multi-layer perceptron (MLP), convolution neural network (CNN) and recurrent neural network (RNN), are proposed with the same parameter magnitude. The performance of these deep neural networks are evaluated through extensive simulation. Numerical results show that RNN has the best decoding performance, yet at the price of the highest computational overhead. Moreover, we find there exists a saturation length for each type of neural network, which is caused by their restricted learning abilities. ", "text": "abstract—with demand high data rate latency ﬁfth generation deep neural network decoder become promising candidate capability one-shot decoding parallel computing. paper three types i.e. multi-layer perceptron convolution neural network recurrent neural network proposed parameter magnitude. performance deep neural networks evaluated extensive simulation. numerical results show best decoding performance price highest computational overhead. moreover exists saturation length type neural network caused restricted learning abilities. recently long-term evolution also known ﬁfth generation widely rolled many countries. doubt accommodate rapid increase user data system capacity. intuitively higher transmission rate requires lower decoding latency. however conventional decoding algorithms suffer high decoding complexity involves number iterative calculations. such designing high-speed low-latency decoders become emerging issue coped with. recent advances deep learning provide direction tackle problem. deep learning applied computer vision natural language processing autonomous vehicles many areas. remarkable results verify good performance. inspired this general decoding problem viewed form classiﬁcation typical application deep learning. brief deep neural network uses cascade multiple layers nonlinear processing units extract transform features contained encoding structure noise characteristic. compared conventional iterative decoding deep neural network decoder calculates estimator passing layer pre-trained neural network referred one-shot decoding. provides foundation low-latency implementations. addition high-speed demand easily satisﬁed utilizing current deep learning platforms tensorﬂow generally support parallel computing exploit powerful hardwares like graphical processing units sages tanner graph comparable decoding performance achieved less iterations traditional belief propagation decoders. weights obtained training deep learning partially compensates effect small cycles tanner graph. considering decoding contains many multiplications proposed lightweight neural offset min-sum decoding algorithm multiplication simple hardware implementation. found structured codes indeed easier learn random codes addressed challenge deep learning based decoders difﬁcult train long codes. consequently proposed divide polar coding graph sub-blocks decoder sub-codeword trained separately. although combination channel decoding deep neural network studied works important problems fully investigated. first type deep neural network suitable nnd. second length codeword affects performance. paper three types build upon multi-layer perceptron convolution neural network recurrent neural network proposed parameter magnitude. compare performance among three deep neural networks numerical simulation best decoding performance price highest computational overhead. also length codeword inﬂuences ﬁtting deep neural network inferred exists saturation length type deep neural network caused restricted learning abilities. rest paper organized follows. section system framework proposed structures provided. section shows numerical results provides comparisons among trained without noise. section concludes paper. order better present design ﬁrst describe system framework nnd. speciﬁcally training process introduced detail reason parameters like training ratio codebook explained. finally describe proposed structures respectively. however important factors consider design. ratio codebook training phase denote randomly pick entire training phase codewords received testing phase seen training phase thus process neural network like recording reading learning. evaluate generalization ability means able estimate unseen codewords information bits randomly picked covers entire factor signal noise ratio training samples denote actual decoding phase unknown timevarying performance greatly depends training samples. adopt proposed method setting training deﬁne performance metric called normalized validation error follows such measures good trained particular compared decoding range different snrs inferred less indicates better performance nnd. says always optimal explained extreme cases architecture illustrated fig. transmitter assume length information bits then encoded binary codeword length channel encoder codeword mapped symbol vector binary phase shift keying modulation. assumed bpsk symbols transmitted additive white gaussian noise paper. represents symbol vector. estimated information bits decoded structure substantially affects performance. notational convenience following paper denote refer possible possible generally decoding algorithm optimal function satisﬁes maximum posteriori criterion obviously hope reach performance decoding possible. supervised learning method construction neural network needs phases training phase testing phase respectively. training phase number training samples used correct weights biases neural network minimizing loss function function obtained phase. then testing phase actual decoding phase estimate information bits received symbol vector call one-shot decoding. design training phase neural network greatly inﬂuences decoding performance need solve problems. first generate training samples. second type loss function chosen. generating training samples train network need received vector true information bits such general sample generating process described like this information bits randomly picked received vector obtained performing channel encoding bpsk mapping simulated channel noise. class feedforward artiﬁcial neural network fully connection layers consists least three layers nodes. node neuron uses nonlinear activation function gives learning ability. shown nonlinear activation functions theoretically approximate continuous function bounded region arbitrarily closely number neurons large enough. paper proposed architecture described fig. employ three hidden layers nodes input layer output layer doubt. also class feedforward artiﬁcial neural networks successfully applied analyzing visual imagery. hidden layers either convolutional pooling emulated response individual neuron visual stimuli convolution operation signiﬁcantly reduces number parameters allowing network deeper fewer parameters. such excellent performance image characteristic extraction motivates combine work. considering usually used image modiﬁcations needed applying nnd. fig. shows modify layer’s input vector instead image. also employ general structure without high-level tricks like batch normalization presented detailed parameter setting listed table class artiﬁcial neural network connections units form directed cycle. allows exhibit dynamic temporal behavior. unlike feedforward neural networks internal memory process arbitrary sequences inputs. makes applicable tasks unsegmented connected handwriting recognition speech recognition inspired remarkable performance time series task hope also achieve good performance nnd. notably general suffers serious vanishing gradient problem described people usually adopt long short-term memory practice. presented lstm contains three gates called forget gate input gate output gate respectively control information cost function loss function measure difference actual output expected output actual output close expected output loss incremented slightly whereas large errors result large loss. work employ mean squared error loss function deﬁned although commonly used cost functions neural network focus compare performance different types deep neural network inﬂuence cost function type identical thus choose simple easy understanding. section speciﬁc designs described. well known parameters neural network large expression learning ability neural network usually strong. considering compare learning ability different deep neural networks keep total parameter number neural network approximately same avoid performance difference deep neural networks comes difference parameter number. such construct relative simple general structure reﬂects learning ability encoding structure. second part investigates performance trained noise involves simultaneous learning encoding structure noise characteristic. additionally system performance measured error rate based testing samples performance decoding compared benchmark. fig. investigates achieved without noise function number training epochs ranging respect different training ratio codebook. fig. decreases gradually number training epoch increasing ﬁnally reaches steady value represents convergence deep neural network. three deep neural networks observed lower leads higher drops denotes neural networks learned complete encoding structure. phenomenon explained overﬁtting neural network i.e. neural network relationship input output well value even prevent backpropagated errors vanishing exploding. such take lstm representative proposed architecture described fig. output dimension lstm cell i.e. vectors lstm cell employed time step. section performances different length compared. throughout experiments polar code rate codeword length training ratio codebook take different points training mentioned before best results least chosen testing phase. setting general parameters neural network called hyperparameters choose relatively reasonable satisfying shown table trials. notably tensorﬂow experimental platform source code available reproducible research. state underﬁtting appears signs underﬁtting case based results above summary exists saturation length type deep neural network caused restricted learning abilities. codeword length saturation length complete encoding structure noise characteristic well-learned entire codebook trained i.e. suffer problem overﬁtting approaches saturation length performances different training ratio close tends underﬁtting training ratio increasing. exceeds saturation length thoroughly underﬁtting matter proposed neural network architectures paper conclude saturation lengths despite learning ability little stronger saturation length investigate computational time shown fig. mentioned before keep parameter magnitude neural network however actual computational time still different speciﬁc structures. training similar case studied fig. performance getting worse compared case reason size codebook exponentially increase growth thus induces complex relationship input output goes beyond learning ability current neural network make tend underﬁtting. however satisﬁed drops represents able generalize fraction codebook entire codebook set. difference compared drops indicates learning ability little stronger mlp. notably still state overﬁtting since performance remains case part research case noise involves simultaneous learning encoding structure noise characteristic performances versus testing eb/n different codeword length investigated. fig. studies case observed achieve performance although overﬁtting similarly fig. studies case four curves different training ratio close certain compared curve indicated tend underﬁtting although little better mlp. however still keeps overﬁtting achieve performance fig. studies case conclude chen seff kornhauser xiao deep driving learning affordance direct perception autonomous driving proceedings ieee international conference computer vision shelke apte novel multi-feature multi-classiﬁer scheme unconstrained handwritten devanagari character recognition international conference frontiers handwriting recognition ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning hochreiter bengio frasconi schmidhuber gradient recurrent nets difﬁculty learning long-term dependencies field guide dynamical recurrent networks bergstra bardenet bengio k´egl algorithms hyper-parameter optimization advances neural information processing systems testing phase computational time much higher computational time little higher mlp. such conclude performances close although better decoding performance higher computational time small degree achieve best decoding performance price highest computational time. paper propose three types build upon rnn. compare performance among three deep neural networks experiment best decoding performance price highest computational time better decoding performance higher computational time small degree. length codeword inﬂuences ﬁtting deep neural network i.e. overﬁtting underﬁtting. inferred exists saturation length type neural network caused restricted learning abilities. reminding proposed structures relatively simple general focus future work design complex structures improve saturation lengths demand longer codewords example increasing neuron nodes adding convolution layers", "year": "2017"}