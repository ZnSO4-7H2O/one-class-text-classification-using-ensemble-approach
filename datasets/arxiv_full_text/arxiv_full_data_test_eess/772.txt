{"title": "Approximate Method of Variational Bayesian Matrix  Factorization/Completion with Sparse Prior", "tag": "eess", "abstract": " We derive analytical expression of matrix factorization/completion solution by variational Bayes method, under the assumption that observed matrix is originally the product of low-rank dense and sparse matrices with additive noise. We assume the prior of sparse matrix is Laplace distribution by taking matrix sparsity into consideration. Then we use several approximations for derivation of matrix factorization/completion solution. By our solution, we also numerically evaluate the performance of sparse matrix reconstruction in matrix factorization, and completion of missing matrix element in matrix completion. ", "text": "abstract. derive analytical expression matrix factorization/completion solution variational bayes method assumption observed matrix originally product low-rank dense sparse matrices additive noise. assume prior sparse matrix laplace distribution taking matrix sparsity consideration. several approximations derivation matrix factorization/completion solution. solution also numerically evaluate performance sparse matrix reconstruction matrix factorization completion missing matrix element matrix completion. matrix factorization problem factorizing single matrix low-rank matrices. matrix completion also problem matrix must infer unobserved matrix elements using low-rank assumption like solving mf/mc problem deal various data analysis problems like principal component analysis collaborative ﬁltering. article study problem mf/mc additive noise. many algorithms proposed used mf/mc make bayesian inference approach based statistical physics. solving problem bayesian formulation posterior probability complicated function intractable general. standard approaches cope diﬃculty markov chain monte carlo method already used another approach message passing equivalently cavity formulation found applicable many problem settings mentioned there. variational bayes method manage above-mentioned diﬃculty analytically. consider kullback-leibler divergence true posterior trial function attempt trial function minimize preceding work already used assumed multivariate gaussian prior zero mean vector diagonal covariance matrix. assumptions low-rank matrices sparse whose assumption supposed deal dictionary learning dictionary learning several algorithms also proposed performance dictionary determination discussed statistical mechanical method sparse factorized matrix laplace prior convenience analysis. goal function minimizes divergence true posterior trial function itself. however must several approximations reasons follows. first cannot perform multiple integrals evaluation moments trial functions without approximations. second need simplify result avoiding intractability numerical experiment. them obtain analytical expression divergence minimum relations ﬁrst second moments trial functions factorized matrices. regard analytical result iterative algorithm mf/mc evaluate performance mf/mc numerical experiment. result validates assumption laplace prior current problem. paper organized follows. give deﬁnition model bayesian formulation mf/mc section brieﬂy review method elucidate apply mf/mc section several approximations introduced section analytical result summarized section show result numerical experiment algorithm based analytical result section section devoted summary future perspective. throughout article describe matrix vector boldface letter. element matrix vector denoted regular letter subscript subscript represents position matrix vector takes covariance matrix diagonal element simplicity later convenience numerical experiment. contrast assume matrix sparser laplace distribution zero mean variance sparse prior. summarize simplify symbol multiple integrals single. however solution posterior maximization generally diﬃcult ﬁnd. diﬃculty equivalent fact estimation denominator partition function statistical physics hard calculate analytically computationally. avoid diﬃculty attempt estimate factorized matrices analytically method divergence minimization variational calculus divergence probability distributions deﬁned denominator functional corresponds variational free energy physics. convenience assume trial function statistically independent respect matrices substitution equations diﬃcult perform multiple integrals. therefore mean ﬁeld approximation namely elements mutually statistically independent. integrate variables concerned evaluation means even mean ﬁeld approximation still problem calculation integral laplace prior. laplace prior need separate integration range positive negative matrix element yields summation terms integration. principle write exact result integration. however ﬁnal expression exponential number terms mentioned leads diﬃculty inference factorized matrices past work using gaussian prior full covariance matrix taken consideration variance oﬀ-diagonal element covariance matrix zero. stated case gaussian priors determine form trial functions minimizing divergence multivariate gaussians. write expression minimal divergence explicitly. using this shown oﬀ-diagonal elements ignored discussion divergence minimization without loss generality. hand laplace prior case cannot determine form trial functions. hence present diagonality covariance matrix approximation follow analysis gaussian case. nonzero oﬀ-diagonal covariance matrix elements introduced analysis much complicated leave future problem. furthermore formulation ﬁrst second moments neglect higher also approximation. approximations mean variance calculated solving four equations conjunction evaluation variables l.h.s. infer factorized matrices estimation ¯alh ¯bhm. seems diﬃcult simpler analytical expression further therefore resort numerical experiment using expressions explained section equations readily solved numerically analytically discussion given discussion summarized follows. mentioned gaussian prior case shown minimum solution divergence diagonal covariance matrix therefore consider diagonal next applying singular value decomposition matrix product express matrices left/right eigenvectors product using results equations ﬁnally reduced algebraic equations solved analytically. consequence variables represented singular values left/right eigenvectors. furthermore argument guarantee result describes global minimum divergence. hand laplace prior several approximations ﬁnal equations nonlinear. therefore guarantee global minimum clear. solving equations numerically evaluate mf/mc performance analytical result. then based result conduct numerical experiment using synthetic data. numerical experiment regard equations iterative algorithm ¯alh ¯bhm determined alternately. convergence ¯alh ¯bhm original matrices inferred values. compute observed matrix initialize elements drawing randomly. initial matrices denoted respectively. initialize covariance matrices identity. counter iteration step t-th step substitute elements r.h.s. equations respectively.) compute ¯alh ¯bhm l.h.s. whose results regarded respectively. similarly compute l.h.s. convergence observe diﬀerence original sparse matrix reconstructed computing errors solution degeneracy simultaneous permutation columns rows best minimizing errors permutation chosen degeneracy solution subsection first evaluate errb i.e. normalized mean squared error original matrix elements. also evaluate errbabs namely normalized mean squared error taking absolute values matrix elements. quantity suitable discussion original matrix reconstruction solution another degeneracy sign inversion column paired mentioned later. means matrix taking absolute value matrix elements next performance sparse element identiﬁcation evaluate errbsp normalized mean squared error sparse elements original matrix satisfying finally correctness solution evaluate errab namely normalized error whole elements matrix product addition problem evaluate error missing elements denoted errabmc introducing auxiliary variables computational cost iteration experiment mainly estimation ¯alh ¯bhm. notice computation inverse matrices necessary iteration. costs dominant deal case small i.e. low-rank problem. first whole range value variation errab small. words frobenius norm matrix product iteration original matrix product kept small constant varied. small errab guarantees correctness solution experiment. next value errb almost gaussian prior even optimize hand regarding errbabs errbsp method outperforms gaussian prior choose appropriate particular errbsp laplace prior much smaller gaussian means identify zero elements original observed matrix easily laplace prior. expect k-dependent terms corrections laplace prior signiﬁcant contribution sparse matrix reconstruction appropriate smaller best region k-dependent denominators become zero negative make mean variance diverge ill-deﬁned. verify this also observe dependence renormalization factor denominator. easily becomes zero around value minimizing errbsp. implies best sparse matrix reconstruction correlation zero point figure example sparse matrix reconstruction. bottom original reconstructed laplace prior reconstructed laplace prior reconstructed gaussian prior symmetry respect line suppose numbers zero elements original reconstructed almost arbitrary zero elements randomly distributed whole reconstructed values directly observe matrix elements given observation matrix result shown ﬁgure original reconstructed laplace prior after/before permutation rows sign adjustment gaussian prior comparison reconstructed laplace prior permutation rows sign adjustment almost original. comparing matrices recall solution degeneracy. general problem obtain another solution operating rotation matrix even sparse matrix product invariant simultaneous permutation columns corresponding rows sign inversion column paired ﬁgure obtain similar matrix original exchange rows reverse signs rows. therefore taking degeneracy consideration conclude original sparse matrix reconstructed practically example. contrast reconstructed gaussian prior original also supports validity analysis. ﬁgure elements chosen zero. performance measured errabmc parameter varied. result cannot signiﬁcant diﬀerence errors laplace/gaussian priors. although prior aﬀects sparse matrix reconstruction make large contribution performance expected likelihood play central role prior less contributes. similarly present guess another sparse prior like bernoulli-gaussian make signiﬁcant change result however attempted analysis another prior problem left future study. analytically obtained solution mf/mc method laplace prior ﬁrst order approximation expansion. regard solution iterative algorithm conducted numerical experiment. result found region reconstruction sparse matrix works better gaussian prior. also veriﬁed method shows better performance sparser matrix. remain problems unresolved. first ﬁnal equations nonlinear cannot simpler equations moments unlike gaussian prior. nonlinearity convergence condition algorithm clear. optimal giving minimum error also problem. problem empirically found best near zero point factor evaluation help determination appropriate computational cost numerical experiment also problem. reduction cost must eﬃcient computational method. furthermore diﬃcult guarantee globality divergence minimum unlike gaussian prior. article focus analytical expression solution mf/mc numerical methods many algorithms showing good performance proposed mf/mc. compare performance mf/mc others future work. addition mf/mc theoretical works based statistical mechanical method thermodynamical limit often used. establish relation method works. present hope analysis help understanding mf/mc problem especially dictionary learning. generalization analysis another situation also future problem. first non-negative signiﬁcant application elements matrices non-negative. focused real-valued matrix problem however possible generalization analysis non-negative case reveal address non-negative problem appropriately bayesian method. second bernoulli-gaussian prior also often used describing sparsity. prior must choose appropriate mixing ratio delta function zero gaussian. however apply formulation bernoulli-gaussian prior must probabilities delta function gaussian yields exponential number factors calculation mean covariance matrix. therefore another approximation must used deal prior. start relation work sake clarity. assumes gaussian distribution zero mean covariance matrix diagonal elements uniform distribution solution obtained factor serves partition function evaluation moments. renormalization factor laplace prior must renormalize probability distribution approximation zero point describes boundary wellill-deﬁned regions renormalized probability. vector deﬁned h′hh γh′m αh′hmbhm bm\\h given removing h-th element similarly matrix σbm\\h given removing h-th column ˆσbm. matrix bm\\h) namely expression given schur complement ˆσbm. expression obtained matrix deﬁned replacing h-th column vector considering schur complement conjunction identity", "year": "2018"}