{"title": "XNORBIN: A 95 TOp/s/W Hardware Accelerator for Binary Convolutional  Neural Networks", "tag": "eess", "abstract": " Deploying state-of-the-art CNNs requires power-hungry processors and off-chip memory. This precludes the implementation of CNNs in low-power embedded systems. Recent research shows CNNs sustain extreme quantization, binarizing their weights and intermediate feature maps, thereby saving 8-32\\x memory and collapsing energy-intensive sum-of-products into XNOR-and-popcount operations.  We present XNORBIN, an accelerator for binary CNNs with computation tightly coupled to memory for aggressive data reuse. Implemented in UMC 65nm technology XNORBIN achieves an energy efficiency of 95 TOp/s/W and an area efficiency of 2.0 TOp/s/MGE at 0.8 V. ", "text": "deploying state-of-the-art cnns requires power-hungry processors off-chip memory. precludes implementation cnns low-power embedded systems. recent research shows cnns sustain extreme quantization binarizing weights intermediate feature maps thereby saving memory collapsing energy-intensive sum-of-products xnor-and-popcount operations. present xnorbin accelerator binary cnns computation tightly coupled memory aggressive data reuse. implemented technology xnorbin achieves energy efﬁciency top/s/w area efﬁciency top/s/mge computing forward pass state-of-the-art image classiﬁcation requires gop/frame access weights networks reach low-power embedded systems typically w-level embedded platforms workstations powerful gpus. hardware accelerators essential push cnns mw-range low-power platforms state-of-the-art energy efﬁciency negligible accuracy loss achieved using binary weight networks maximum energy efﬁciency imperative store repeatedly accessed data on-chip limit off-chip communication. puts stringent constraints cnns device limiting applications crippling accuracy. binary neural networks bipolar binarization model weights feature maps reducing overall memory size bandwidth constraints simplifying costly sum-of-products computation mere xnor-and-popcount operations keeping acceptable accuracy penalty relative full-precision networks also thanks iterative improvement multiple convolutions work present xnorbin hardware accelerator targeting fully-binary cnns address memory computation energy challenges. operations bnns d-convolutions multiple binary input feature maps binary ﬁlter kernel sets resulting multiple integer-valued feature maps. convolutions formulated many parallel xnor-and-popcount operations potential intensive data reuse. activation function optional batch normalization collapsed re-binarization pre-computed per-output feature threshold value applied on-the-ﬂy. optional pooling operation enabled re-binarization. xnorbin complete sense implements common operations supports wide range hyperparameters. furthermore provide tool automatic mapping trained bnns pytorch deep learning framework control stream xnorbin. top-level overview chip architecture provided fig. xnorbin accelerator sequentially processes layer bnn. data support kernel sizes processing core xnorbin composed cluster bpus every includes xnor_sum units. units calculate xnor-and-popcount result vectors containing values feature maps speciﬁc pixel. outputs xnor_sum units added-up computing output value convolution image cycle. next higher level hierarchy results bpus added produce output value convolution cycle-by-cycle convolution window slides horizontally image. resulting integer value forwarded controller includes near-memory compute unit accumulates partial results means read-add-write operation since feature maps processed tiles ﬁnal accumulation partial results unit also performs thresholding/re-binarization operation binary results written back memory also handles packing words. data re-use/buffering xnorbin comes three levels memory data buffering hierarchy. highest level main memory stores feature maps partial sums convolutions. memory divided sram blocks serves data source serves data sink switching roles layer changes. memories sized store worst-case layer non-trivial network thus avoid tremendous energy cost pushing intermediate results off-chip. additionally parameter buffer stores weights binarization threshold conﬁguration parameters sized target network used cache external ﬂash memory storing parameters. integrating accelerator camera sensor would allow completely eliminate off-chip communication. next lower level hierarchy banks used buffer rows input feature maps relieve pressure sram. since banks need rotated shifting convolution window down connected cluster crossbar. crossbar connects working memory inside bpus controlled shift registers input feature maps ﬁlter weights. shifted moving convolution window forward. data words csrs accessible parallel steered xnor_sum units. scalability xnorbin limited convolutions. conﬁgured handle smaller ﬁlter sizes however size bnn’s largest pair input output feature maps main memory furthermore convolution layer ﬁlter size larger would need split smaller convolutions number parallel working bpus xnor_sum units number banks size csrs thereby introducing large overhead. limitations depth network streaming network parameters external ﬂash memory. tested design running binary alexnet model comes pre-trained imagenet dataset. throughput energy consumption layer shown fig. results implicitly bit-true—there implementation loss going ﬁxed-point representation since intermediate results integer-valued binary. design implemented technology. evaluations typical-case corner yielded throughput gop/s gop/s energy efﬁciency top/s/w top/s/w respectively. system consumes mw.v memory crossbar bpus. performance physical characteristics presented fig. implementation parameters memory sizes chosen support models size binary alexnet. compare energy efﬁciency xnorbin state-of-the-art accelerators fig. best knowledge ﬁrst hardware accelerator binary neural networks. closest comparison point fpga-based finn results higher energy consumption running bnns. strongest competitor binary-weight accelerator strongly limited energy requiring energy operation xnorbin. thanks binarization neural networks memory footprint intermediate results well ﬁlter weights could reduced making xnorbin capable intermediate results simple realistic binary alexnet on-chip memory mere total accelerator size furthermore computational complexity decreases signiﬁcantly full-precision multiplier-accumulate units replaced xnor pop-count operations. beneﬁts—smaller compute logic keeping intermediate results on-chip reduced model size—xnorbin outperforms overall energy efﬁciency existing accelerators canziani paszke culurciello analysis deep neural network models practical applications corr vol. abs/. andri cavigelli rossi benini yodann architecture ultralow power binary-weight acceleration ieee transactions", "year": "2018"}