{"title": "Spectral feature mapping with mimic loss for robust speech recognition", "tag": "eess", "abstract": " For the task of speech enhancement, local learning objectives are agnostic to phonetic structures helpful for speech recognition. We propose to add a global criterion to ensure de-noised speech is useful for downstream tasks like ASR. We first train a spectral classifier on clean speech to predict senone labels. Then, the spectral classifier is joined with our speech enhancer as a noisy speech recognizer. This model is taught to imitate the output of the spectral classifier alone on clean speech. This \\textit{mimic loss} is combined with the traditional local criterion to train the speech enhancer to produce de-noised speech. Feeding the de-noised speech to an off-the-shelf Kaldi training recipe for the CHiME-2 corpus shows significant improvements in WER. ", "text": "task speech enhancement local learning objectives agnostic phonetic structures helpful speech recognition. propose global criterion ensure de-noised speech useful downstream tasks like asr. ﬁrst train spectral classiﬁer clean speech predict senone labels. then spectral classiﬁer joined speech enhancer noisy speech recognizer. model taught imitate output spectral classiﬁer alone clean speech. mimic loss combined traditional local criterion train speech enhancer produce de-noised speech. feeding de-noised speech offthe-shelf kaldi training recipe chime- corpus shows signiﬁcant improvements wer. adversarial networks fundamentally gains achieved injection auxiliary proxy learning objectives traditional pipelines. auxiliary objectives exploit adversarial relationship neural networks nash equilibrium generating sensible data optimal behavior generator effect reﬁning distribution generated data closer desirable outcome relative system trained without auxiliary objective e.g. sharper realistic generated images. automatic speech recognition shown tremendous progress years recognizing clean speech. however traditional dnn-hmm systems still suffer performance degradation presence acoustic interference additive noise room reverberation. strategies building noise-robust speech recognizer include using noise-invariant features augmented data bulkier acoustic models like lstms cnns sophisticated language models groups however looked systems train speech enhancement model used different tasks. speech enhancement front-end refers performanceboosting denoising technique attached standard automatic speech recognition model. deep learning models speech enhancement attempt estimate ideal ratio mask removing noise speech signal others utilize spectral mapping signal domain feature domain directly predict features. recent work computer vision seen notable success addressing problem poor resolution modiﬁed input data using framework broadly referred generative fig. speech enhancement system trained three steps. spectral classiﬁer trained predict senone labels cross-entropy criterion spectral mapper pre-trained noisy speech clean speech using criterion spectral mapper trained using joint loss clean speech outputs classiﬁer clean speech gray models frozen weights. signals. however local learning objective. student-teacher networks used improve quality noisy speech recognition model uses mimic loss instead student-teacher learning means speech enhancer jointly trained particular acoustic model. speech enhancer could used pre-processor system another similar dataset. modularity strength mimic loss. spectral mapping improves performance speech recognizer learning mapping noisy spectral patterns clean features. train dnn-based spectral mapper feature denoising. previous work shown dnn-based spectral mapper takes noisy spectrogram input predict clean ﬁlterbank features yields good results chime- noisy reverberant dataset. speciﬁcally ﬁrst divide input time-domain signals frames frame shift apply short time fourier transform compute spectral magnitudes time frame. signal frame contains samples -point fourier transform compute magnitudes forming dimensional magnitude vector. noisy spectral component frequency time slice augmented input deltas double deltas well frame window leading dimensionality similarly deﬁne clean spectral slice time performance. however initial experiments gans conditioned noisy speech inputs discriminator network trained distinguish real fake clean/noisy input pairs exposed well-known mode collapse problem endemic gans results failed improve upon simple baselines noisy-to-clean speech mappings trained loss. attempts also failed improve upon baseline speculate difference performance experiments visual domain relatively non-local structure speech signal frequency axis well smoothness speech features compared images. work described paper motivated observations over-simple outputs gans seemed stuck mode collapse orbits. hypothesize training objective provide stronger feedback simple real fake determination better able guide parameters denoising network towards producing output behaves like actual speech. resulting system retains none properties generative trained adversarially insight auxiliary task improve denoising process drawn body work. auxiliary global objective local criterion behavioral loss classiﬁer trained clean speech. call additional objective mimic loss. first train senone classiﬁer using clean speech input spectral mapper network using parallel noisy clean speech frame pairs. next freeze weights acoustic model join pre-trained spectral mapper pass noisy speech frames train spectral mapper joint objective i.e. weighted traditional ﬁdelity loss mimic loss. mimic loss then loss respect softmax outputs classiﬁer corresponding clean speech frame. ﬁgure graphical depiction model. technique using model teach another proposed caruna task model compression. work introduce student-teacher learning separate teacher student models trained task. mimic loss hand used train student model different task teacher model. deal noise many based methods proposed improve robustness systems. acoustic modeling using convolutional neural networks long short term memory networks resulted improvement performance. lstms also successfully used speech enhancement front-ends spectral classiﬁer similar traditional acoustic model trained classify stacked clean spectral pattern corresponding senone class train classiﬁer using cross entropy criterion; critically classiﬁer trained freeze weights model appropriate behavior clean speech. system monaural. experiments simply average signals left right ear. gmm-hmm system built using kaldi toolkit clean utterances wsj-k senone state frame corresponding noisy-reverberant utterances. initial clean alignments obtained performing forced alignment clean utterances. reﬁne initial clean alignments trained dnn-based acoustic model using ﬁlterbank features clean utterances re-generate clean alignments. clean alignments used labels training acoustic models study. note dnn-hmm hybrid system built clean utterances powerful recognizer. achieves clean test wsj-k dataset. order determine effectiveness additional criterion train model using denoised features off-the-shelf kaldi recipe. dnn-hmm hybrid system trained using clean wsj-k alignments generated using method stated above. hidden layers sigmoid neurons layer softmax output layer. splicing context size ﬁlter-bank features ﬁxed frames minibatch-size that train smbr sequence training achieve better performance. regenerate lattices after ﬁrst iteration train iterations. pronunciation dictionary ofﬁcial closevocabulary tri-gram language model experiments. spectral classiﬁer network multilayer feed-forward network classiﬁes clean speech frames senone labels. layers neurons leaky relu activations batch normalization layers. training spectral classiﬁer clean speech apply softmax topmost layer cross entropy criterion teach network produce senones. table experimental results chime test set; word error rate dnn-hmm hybrid system trained using cross-entropy criterion. smbr error rate sequential minimum bayes risk training. training speech enhancer found using mimic loss enough allow model converge. speculate task predicting senones different task predicting clean speech error signal drive output speech enhancer actually look like speech. combining ﬁdelity mimic losses joint loss allows enhancer better imitate behavior classiﬁer clean speech keeping projection noisy speech closer clean speech. evaluate effectiveness proposed method track chime- challenge mediumvocabulary task word recognition reverberant noisy environments without speaker movements. task three types data provided based wall street journal vocabulary read speech corpus clean reverberant reverberant+noisy. clean utterances extracted database. reverberant utterances created convolving clean speech binaural room impulse responses corresponding frontal position family living room. real-world non-stationary noise background recorded room mixed reverberant utterances form reverberant+noisy set. noise excerpts selected signal-to-noise ratio ranges among without scaling. multi-condition training development test sets reverberant+noisy contain utterances respectively utterances clean reverberation noise different conditions. table experimental results chime test broken across different snrs. mimic loss based presoftmax units. bold indicates best performing system evaluation subset. spectral mapper composed simple two-layer feedforward network neurons layer. fact model simple means used lowresource situations fast use. note mimic loss applied improve results kind speech enhancer. regularize network batch normalization dropout rate every layer. also relu activations layer. classiﬁer mapper written using tensorflow. table outputs recognizer softmax provide better target noisy speech recognizer suggested even though setup quite different theirs. difference performance prepost-softmax targets mismatch target domain loss criterion; ongoing work suggests cross-entropy mimic loss post-softmax targets performs similarly pre-softmax. furthermore information loss softmax normalization broaden allowable spectral mappings harming generalization. suggests helpful noisy recognizer targets clean recognizer also learn behave clean recognizer. also demonstrate fact training noisy speech recognizer using hard targets rather soft targets mimic loss. caused joint loss spectral mapper diverge providing evidence noisy speech recognizer must learn mimic behavior targets clean speech recognizer. finally table break results different snrs gains consistent signal-to-noise levels. context show table performance system relative published results ﬁeld. betterperforming models list sophisticated models front-end speech enhancement acoustic modeling well noise-invariant features off-the-shelf kaldi recipe using dnn-hmm speech recognition well simple layer feed-forward network spectral mapping. again mimic loss theoretically used improve results front-end system. proposed speech enhancement criterion called mimic loss used produce speech useful downstream tasks. mimic loss comes comparing outputs frozen clean speech recognizer softmax applied clean enhanced speech. conﬁguration allows speech enhancement output used clean speech downstream task. mimic loss spectral mapper learns produce detailed speech data retaining features ﬁdelity loss alone fails model. mimicking behavior pre-softmax layer classiﬁer superior mimicking output senone posterior estimates; general lower error rates show features helpful downstream tasks. future work propose extend work matching every layer phone recognizer rather inputs outputs. could also variety models speech enhancement demonstrate effectiveness mimic loss. another avenue evaluate output system multiple domains determine effectiveness approach learning domain-invariant representations speech. finally train sophisticated acoustic model rather using off-the-shelf kaldi recipe. material based upon work supported national science foundation grant iis-. also thank ohio supercomputer center providing computational resources. yoshioka delcroix ogawa kinoshita fujimoto fabian espi higuchi chime- system advances speech enhancement recognition mobile multi-microphone devices automatic speech recognition understanding ieee workshop ieee narayanan wang improving robustness deep neural network acoustic models speech separation joint adaptive training ieee/acm transactions audio speech language processing vol. wang wang woods merks zhang learning spectral mapping speech dereverberation denoising ieee transactions audio speech language processing vol. bagchi mandel wang plummer fosler-lussier combining spectral feature mapping multi-channel model-based source separation noise-robust automatic speech recognition automatic speech recognition understanding ieee workshop ieee chen watanabe erdogan hershey integration speech enhancement recognition using long-short term memory recurrent neural network proc. interspeech weninger erdogan watanabe vincent roux hershey schuller speech enhancement lstm recurrent neural networks application noise-robust international conference latent variable analysis signal separation. springer watanabe hori roux hershey student-teacher network learning enhanced features acoustics speech signal processing ieee international conference ieee vincent barker watanabe roux nesta matassoni second chime speech separation recognition challenge datasets tasks baselines acoustics speech signal processing ieee international conference ieee povey ghoshal boulianne burget glembek goel hannemann motlicek qian schwarz kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. ieee signal processing society number epfl-conf.", "year": "2018"}