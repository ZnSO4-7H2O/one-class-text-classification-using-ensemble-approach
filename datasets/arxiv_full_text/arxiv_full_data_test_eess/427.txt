{"title": "Position-Constrained Stochastic Inference for Cooperative Indoor  Localization", "tag": "eess", "abstract": " We address the problem of distributed cooperative localization in wireless networks, i.e. nodes without prior position knowledge (agents) wish to determine their own positions. In non-cooperative approaches, positioning is only based on information from reference nodes with known positions (anchors). However, in cooperative positioning, information from other agents is considered as well. Cooperative positioning requires encoding of the uncertainty of agents' positions. To cope with that demand, we employ stochastic inference for localization which inherently considers the position uncertainty of agents. However, stochastic inference comes at the expense of high costs in terms of computation and information exchange. To relax the requirements of inference algorithms, we propose the framework of position-constrained stochastic inference, in which we first confine the positions of nodes to feasible sets. We use convex polygons to impose constraints on the possible positions of agents. By doing so, we enable inference algorithms to concentrate on important regions of the sample space rather than the entire sample space. We show through simulations that increased localization accuracy, reduced computational complexity, and quicker convergence can be achieved when compared to a state-of-the-art non-constrained inference algorithm. ", "text": "case belief assigns certain probability state random variable contrast problems latent random variables cooperative localization continuous. variants belief propagation fig. schematic approach based distance estimates ˆzi→j determine constrained proposal distributions using poa. subsequently obtain estimates positions agents employing novel proposal distribution. sample space based distance estimates ˆzi→j. constraints determine proposal distributions secondly harness constraints order ease inference problem obtain position estimates ˆxj. fig. depicts high-level overview approach. employing constrained proposal distribution increases sample space. later paper show true location node guaranteed reside inside determined polygon \u0001i→j however unlikely case negative ranging error corresponding distance estimate discarded proposed denotes incoming message shorthand notation φij. operation called message ﬁltering. note integral cannot solved closed form nonlinear and/or arbitrary instead employ particle representation approximate resulting message i.e. continuous message approximated denotes particle representation continuous function using particles. context cooperative localization message ﬁltering reduces directly drawing samples equal weight hence message ﬁltering shows linear complexity number samples. consequently complexity ˆσij estimated using kernel density estimator. consider least squares cross validation estimator multiplying kernel density estimates possible closed form. however requires suitable proposal distribution obtain particle representation assign proper weight sample. weight accounts every iteration based current belief. since belief given particle representation mmse estimate obtained computing centroid particle cloud i.e. theorem support marginal posteriori distribution pxj|ˆz compact convex supports factors joint posteriori distribution px|ˆz convex least factor compact. recall assume every agent obtained range estimates ˆzi→j w.r.t. nodes s→j. ranging performed using time-of-ﬂight estimates ultra-wideband radios. discussed section ii-a errors corrupt range estimates non-negative every factor compact convex. start factors observation model regardless shape density ensures factor observation model convex compact i.e. long distance estimation error non-negative support marginal guaranteed compact convex. instance anchor true position agent. true true distance overestimated agent must inside disk radius ˆzi→j center probability one. consider prior densities. typically assume agents prior knowledge position i.e. prior distributions uniform plane agents want localize. readily seen supports emphasize support marginals outer-approximated conditions theorem met. geometric shape outer-approximating objects adjusted problem hand. thus veriﬁed theorem whether since problem generally hard solve proposed polygon outer-approximation algorithms attempts attain optimal polygon general guarantee optimal solution achieved. visualize operations. iterations agent ﬁrst receives polygons neighbors operation called polygon scaling. resulting polygons denoted intersection scaled polygons iteration intersection multiple convex polygons determined efﬁciently. procedure anchors considered. priors information agents also considered. focus description former case. denote anchors communication range then measurements ˆzi→j considered ﬁrst iteration also recall anchors perfect position information. position conﬁned polygon rather single point positive ranging errors position agent somewhere inside disk radius ˆzi→j center zi→j resides origin. done efﬁciently polar coordinates. then transform resulting vertices cartesian coordinates shift vertices polygon obtain polygon outer-approximation position anchor detail begin generating points around origin uniform angular spacing resulting output list yields intersection subject clipping polygon. note number vertices resulting polygon vertices example polygon intersection depicted fig. pseudo-code polygon intersection given algorithm polygons considered. note increasing number polygon edges tightens outer-approximation. inside polygon shifted ˆzi→j direction outward pointing normal vector respective edge. recall convex polygons halfspaces shifted ˆzi→j toward outward normal vector. obtain halfspaces manipulating right-hand side inequalities since point corresponding hyperplane obtain shifted halfspace shifting vertices polygon determine intersections adjacent hyperplanes i.e. hyperplane compute intersection hyperplane. resulting points constitute vertices scaled polygon algorithm shows pseudo-code polygon scaling. illustrative example shown fig. fig. polygon scaling resulting polygon agent ﬁrst iteration scaled version depicted. original polygon given halfspaces outward normal vector scale original polygon halfspace shifted direction normal vector. intersections adjacent halfspaces yield vertices scaled polygon. iterations polygon support outer-approximation every agent obtained polygon outer-approximates support marginal posteriori distribution pxj|z. hence choose polygons determine supports proposal distributions draw samples uniformly polygon. thus proposal distribution given pling. draw samples uniformly rectangle comprises polygon ples drawn horizontal vertical direction independently i.e. edges rectangle aligned horizontal vertical axis. sample fig. novel proposal distribution samples drawn according novel proposal distribution proposal distribution conﬁne region draw samples relevant region close fig. average polygon/ellipse area versus number iterations becomes evident iterations polygon/ellipse area reduce signiﬁcantly. moreover small number polygon edges sufﬁcient achieve tight polygons. probability. agent said outage localization error exceeds error threshold eth. compute localization error according estimated location node taken minimum mean square error estimate belief. increasing number vertices adds additional area reduction. thus restrict polygons vertices analysis. addition largest area reduction achieved iteration. reduction polygon area computation time table depicts average computation time agent. break accumulated computation time time required compute polygons tpoly time required achieve convergence respective localization algorithm tconv i.e. tpoly tconv. latter time convergence observations previous discussion. terms computation time shows lowest cost followed polygon processing scale linear number edges i.e. terms computation demanding part algorithm polygon intersection sutherland-hodgman algorithm scales quadratic number edges since intersects pairs polygons intersection |s→j| polygons requires execute sutherland-hodgman algorithm |s→j| times. recall number edges intersected polygons depends input polygons cannot generalized. thus cannot quantify number operations required intersect |s→j| polygons general. observed number edges intersecting polygons typically less number initial edges hence over-estimates number computations. assumption polygon intersection scales according considering obvious much costly terms computation polygon outer-approximation. observation makes intuitively sense. consider becomes sufﬁciently large order outperform number samples grows larger gains achieved compared parametric approach. large samples sizes signiﬁcant gains achieved regimes small large error. larger samples consider factorized posteriori distribution goal conﬁne support marginal posteriori distributions pxj|z feasible sets. marginal posteriori distribution formally deﬁned proof. consider simpler case support determined closure subset i.e. suppg) product either zero factors zero are. thus contribution supp) suppg) {supp)\\a holds also contribution supp) i.e. {supp)\\a also consider support product densities suppg) supp) supp). integrating entire domain constrain support i.e. supp) suppg) supp) supp). readily shown also valid multiple integration variables factors. approximation namely convexity support marginal posteriori distribution pxj|z. lemma support marginal posteriori distribution pxj|z convex factors convex support proof. support marginal posteriori distribution given intersection support factor intersection sets convex sets convex hence intersection supports convex support density convex.", "year": "2018"}