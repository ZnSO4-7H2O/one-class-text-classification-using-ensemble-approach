{"title": "Objects that Sound", "tag": "eess", "abstract": " In this paper our objectives are, first, networks that can embed audio and visual inputs into a common space that is suitable for cross-modal retrieval; and second, a network that can localize the object that sounds in an image, given the audio signal. We achieve both these objectives by training from unlabelled video using only audio-visual correspondence (AVC) as the objective function. This is a form of cross-modal self-supervision from video.  To this end, we design new network architectures that can be trained for cross-modal retrieval and localizing the sound source in an image, by using the AVC task. We make the following contributions: (i) show that audio and visual embeddings can be learnt that enable both within-mode (e.g. audio-to-audio) and between-mode retrieval; (ii) explore various architectures for the AVC task, including those for the visual stream that ingest a single image, or multiple images, or a single image and multi-frame optical flow; (iii) show that the semantic object that sounds within an image can be localized (using only the sound, no motion or flow information); and (iv) give a cautionary tale on how to avoid undesirable shortcuts in the data preparation. ", "text": "abstract. paper objectives ﬁrst networks embed audio visual inputs common space suitable cross-modal retrieval; second network localize object sounds image given audio signal. achieve objectives training unlabelled video using audio-visual correspondence objective function. form crossmodal self-supervision video. design network architectures trained cross-modal retrieval localizing sound source image using task. make following contributions show audio visual embeddings learnt enable within-mode between-mode retrieval; explore various architectures task including visual stream ingest single image multiple images single image multi-frame optical ﬂow; show semantic object sounds within image localized give cautionary tale avoid undesirable shortcuts data preparation. recent surge interest cross-modal learning images audio reason surge availability virtually unlimited training material form videos provide image stream audio stream cross-modal information used train deep networks. cross-modal learning long history computer vision principally form images text although audio text share fact sequential nature challenges using audio partner images signiﬁcantly diﬀerent using text. text much closer semantic annotation audio. text e.g. form provided caption image concepts directly available problem provide correspondence noun ‘dog’ spatial region image whereas audio obtaining semantics less direct common image classiﬁcation concept directly available signal requires something like convnet obtain paper interest cross-modal learning images audio particular unlabelled video source material employ audio-visual correspondence training objective brief given input pair video frame second audio task requires network decide whether correspondence not. labels positives negatives pairs obtained directly videos provide automatic alignment visual audio streams frame audio coming time video positives frame audio coming diﬀerent videos negatives. labels constructed directly data itself example self-supervision subclass unsupervised methods. task stimulates learnt visual audio representations discriminative distinguish matched mismatched pairs semantically meaningful. latter case network solve task learns classify semantic concepts modalities judge whether concepts correspond. recall visual network sees single frame video therefore cannot learn cheat exploiting motion information. paper propose networks enable functionalities section propose network architecture produces embeddings directly suitable cross-modal retrieval; section design network learning procedure capable localizing sound source i.e. answering basic question which object image making sound?. example shown figure trained scratch labels whatsoever using unsupervised audio-visual correspondence task throughout paper publicly available audioset dataset consists second clips youtube emphasis audio events video-level audio class labels available noisy; labels organized ontology. make dataset manageable interesting purposes ﬁlter sounds musical instruments singing tools yielding audio classes removing uninteresting classes like breathing sine wave sound eﬀect infrasound silence etc. videos challenging many poor quality audio source always visible audio stream artiﬁcially inserted video e.g. often case video compiled musical piece album cover text naming song still frame musician even completely unrelated visual motifs like landscape etc. dataset already comes public train-test split randomly split public training training validation sets proportions. ﬁnal audioset-instruments dataset contains clips train test splits respectively. re-emphasise labels whatsoever used methods since treat dataset purely collection label-less videos. labels used quantitative evaluation purposes e.g. evaluate quality unsupervised cross-modal retrieval section describe network architecture capable learning good visual audio embeddings scratch without labels. furthermore embeddings aligned order enable querying across modalities e.g. using image search related sounds. audio-visual embedding network designed explicitly facilitate cross-modal retrieval. input image second audio processed vision audio subnetworks respectively followed feature fusion whose goal determine whether image audio correspond task. architecture shown full detail figure enforce feature alignment ave-net computes correspondence score function euclidean distance normalized visual audio embeddings. information bottleneck single scalar value summarizes whether image audio correspond forces embeddings aligned. furthermore euclidean distance training crucial makes features aware distance metric therefore making amenable retrieval subnetworks produce normalized embedding modalities. euclidean distance features computed single scalar passed tiny scales shifts distance calibrate subsequent softmax. bias essentially learns threshold distance features deemed correspond. fig. convnet architectures. blocks represents single layer text providing information ﬁrst layer name optional kernel size second output feature size. convolutional layer followed batch normalization relu nonlinearity ﬁrst fully connected layer followed relu. pool layers perform pooling strides equal kernel sizes. show vision audio convnets perform initial feature extraction image audio inputs respectively. ave-net designed produce aligned vision audio embeddings information single scalar used decide whether inputs correspond euclidean distance embeddings. contrast l-net architecture combines modalities concatenation couple fully connected layers produce corresponds classiﬁcation scores. features inadequate cross-modal retrieval aligned fusion performed concatenating features correspondence score computed fully connected layers. contrast ave-net moves fully connected layers vision audio subnetworks directly optimizes features cross-modal retrieval. unlike contrastive loss requires tuning margin hyper-parameter parameter-free explicitly computes corresponds-or-not output thus making directly comparable l-net contrastive loss would require another hyper-parameter distance threshold. wang also train network cross-modal retrieval triplet loss also contains margin hyper-parameter pretrained networks consider diﬀerent modalities fully supervised correspondence labels. concurrent work hong similar technique pretrained networks triplet loss joint embedding music video. recent work also trains networks cross-modal retrieval uses imagenet pretrained network teacher. case train entire network scratch. architectures trained audioset-instruments train-val evaluated audioset-instruments test described section implementation details given section audio-visual correspondence task ave-net achieves accuracy beating slightly l-net gets however performance ultimate goal since task used proxy learning good embeddings real test interest retrieval performance. evaluate intra-modal cross-modal retrieval audioset-instruments test dataset. single frame surrounding second audio sampled randomly test video form retrieval database. combinations image/audio query image/audio database tested e.g. audio-to-image uses audio embedding query vector search database visual embeddings answering question which image could make sound?; image-to-image uses visual embedding query vector search database. evaluation metric. performance retrieval system assessed using standard measure normalized discounted cumulative gain measures quality ranked list retrieved items normalized range signiﬁes perfect ranking items sorted non-increasing relevance-to-query order. details deﬁnition relevance refer appendix item test dataset used query average ndcg reported ﬁnal retrieval performance. recall labels noisy note extract single frame audio video therefore miss relevant event ideal ndcg highly unlikely achievable. baselines. compare l-net also trained unsupervised manner train using identical procedure training data method. l-net expected work cross-modal retrieval since representation aligned also test l-net representations aligned baseline. addition vision features extracted last hidden layer vgg- network trained fully-supervised manner imagenet evaluated well. cross-modal retrieval vgg-imagenet visual features aligned l-net audio features using strong baseline vision features fully-supervised audio features state-of-the-art note vanilla l-net produces representations yields visual descriptor. computational reasons fair comparison ave-net table cross-modal intra-modal retrieval. comparison method unsupervised supervised baselines terms average ndcg audioset-instruments test set. columns headers denote modalities query database respectively stands image audio. ave-net beats baselines convincingly. produces embeddings cca-based methods components. cases representations l-normalized found signiﬁcantly improve performance; note ave-net includes l-normalization architecture therefore re-normalization redundant. results. ndcg combinations query-database modalities shown table intra-modal retrieval avenet better baselines including slightly beating vgg-imagenet image-image trained fully supervised manner another task. interesting note network never seen same-modality pairs training trained explicitly image-image audioaudio retrieval. however intra-modal retrieval works transitivity image violin close feature space sound violin turn close images violins. note despite learning essentially information task training data l-net ave-net outperforms l-net euclidean distance aware i.e. designed trained retrieval mind. cross-modal retrieval ave-net beats baselines verifying unsupervised training eﬀective. l-net representations clearly aligned across modalities cross-modal retrieval performance level random chance. l-net features aligned form strong baseline beneﬁts directly training network alignment apparent. interesting aligning vision features trained imagenet state-of-the-art l-net audio features using performs worse methods demonstrating case unsupervised learning varied dataset suﬃcient imagenet-pretrained networks black-box feature extractors. figure shows qualitative retrieval results illustrating eﬃcacy approach. system generally retrieve relevant items database making reasonable mistakes confusing sound zither acoustic guitar. fig. cross-modal intra-modal retrieval. column shows query retrieved results. purely visualization purposes hard display sound frame video aligned sound shown instead actual sound form. sound icon lack indicates audio vision modality respectively. example last column illustrates query image audio database thus answering question which sounds plausible query image? note many audio retrieval items indeed correct despite fact corresponding frames unrelated e.g. audio blue image white text contain drums artefact noisy real-world youtube videos are. also interesting investigate whether using information multiple frames help solving task. results only evaluate modiﬁcations architecture figure handle diﬀerent visual input multiple frames optical conciseness details architectures explained appendix overall idea ave+mf input frames convert convolution layers ave+of combine information single frame frames optical using two-stream network style performance ave+mf ave+of networks task respectively compared single input image network’s however evaluated retrieval fail provide boost e.g. ave+of network achieves im-im im-aud aud-im aud-aud respectively; comparable performance vanilla ave-net uses single frame input explanation underwhelming result that case unsupervised approaches performance training objective necessarily perfect correlation quality learnt features performance task interest. speciﬁcally ave+mf ave+of could using motion information available input solve task easily exploiting lower-level information turn provides less incentive network learn good semantic embeddings. reason single frame input used experiments. preventing shortcuts. deep neural networks notorious ﬁnding subtle data shortcuts exploit order cheat thus learn solve task desired manner; example misuse chromatic aberration solve relative-position task. prevent behaviour found important carefully implement sampling negative pairs similar possible sampling positive pairs. detail positive pair generated sampling random video picking random frame video picking second audio frame mid-point. tempting generate negative pair randomly sampling diﬀerent videos picking random frame random second audio clip other. however produces slight statistical diﬀerence positive negative audio samples mid-point positives always aligned frame thus multiple seconds negatives restrictions. allows shortcut appears network able learn recognize audio samples taken multiples therefore distinguishing positives negatives. probably exploiting low-level artefacts mpeg encoding and/or audio resampling. therefore naive implementation negative pair generation network less incentive strongly learn semantically meaningful information. prevent happening audio negative pair also sampled multiples without shortcut prevention ave-net achieves artiﬁcially high accuracy task compared proper sampling safety mechanism place performance network without shortcut prevention retrieval task consistently worse. note that fairness train l-net shortcut prevention well. l-net training encounter problem performing additional data augmentation randomly misaligning audio frame second positives negatives. apply augmentation well observation important keep mind future unsupervised approaches exact alignment might required audio-visual synchronization. standard data augmentation used random cropping horizontal ﬂipping brightness saturation jittering vision random clip-level amplitude jittering audio. network trained cross-entropy loss binary classiﬁcation task whether image audio correspond using adam optimizer weight decay learning rate obtained grid search. training done using gpus parallel synchronous updates implemented tensorflow worker processes -element batch thus making eﬀective batch size better gpu); learning rate schedule style learning rate decreased every epochs. setup able fully reproduce l-net results achieving even slightly better performance probably improved learning rate schedule larger batches. system understands audio-visual world associate appearance object sound makes thus able answer where object making sound? outline architecture training procedure learning localize sounding object still operating scenario supervision neither object location level identities. make task show designing network appropriately possible learn localize sounding objects extremely challenging label-less scenario. contrast standard task goal learn single embedding entire image explains sound goal sound localization regions image explain sound regions correlated belong background. operationalize this formulate problem multiple instance learning framework namely local region-level image descriptors extracted spatial grid similarity score computed audio embedding vision descriptors. goal ﬁnding regions correlate well sound maximal similarity score used measure image-audio agreement. network trained manner task i.e. predicting whether image audio correspond. corresponding pairs method encourages region respond highly therefore localize object mismatched pairs maximal score fig. audio-visual object localization notation building blocks shared figure audio subnetwork avenet vision network instead globally pooling feature tensor thus making entire score indicating desired object makes input sound. essence audio representation forms ﬁlter looks relevant image patches similar manner attention mechanism. pool conv features keeps operating resolution. converted convolutions conv conv. feature normalization similarities visual descriptors single audio descriptor computed scalar product producing tiny convolution followed relation previous works. usually hinting object localization previous cross-modal works fall short achieving goal. harwath demonstrate localizing objects audio domain spoken text design network localization. network trained scratch internally learns object detectors never demonstrated able answer question where object making sound? unlike approach trained ability mind. rather heatmaps produced examining responses various neurons given input image. output computed completely independently sound therefore cannot answer where object making sound?. approach similarities used average pooling respectively learn object detectors without bounding annotations single visual modality setting imagenet pretrained networks image-level labels. mil-based approach also connections attention mechanisms viewed inﬁnitely hard attention note information multiple audio channels could localization setup generally requires known calibration multi-microphone unknown unconstrained youtube videos number channels changes across videos quality audio youtube varies signiﬁcantly localization methods based multi-microphone information prone noise reverberation desire system learns detect semantic concepts rather localize cheating accessing multi-microphone information. finally similar technique appears concurrent work later works also relevant. first accuracy localization network task ave-net embedding network section encouraging means switching setup cause loss accuracy ability detect semantic concepts modalities. ability network localize object sound demonstrated figure able detect wide range objects diﬀerent viewpoints scales challenging imaging conditions. detailed discussion including analysis failure cases available ﬁgure caption. expected unsupervised method necessarily case detects entire object focus speciﬁc discriminative parts interface hands piano keyboard. interacts philosophical question object making sound body piano strings keyboard ﬁngers keyboard whole human together instrument fig. making sound? localization output avol-net unseen test data; figure https//goo.gl/jvsjp more. recall network sees single frame therefore cannot cheat using motion information. pair images shows input frame localization output input frame second audio around overlaid frame note wide range detectable objects keyboards accordions drums harps guitars violins xylophones people’s mouths saxophones etc. sounding objects detected despite signiﬁcant clutter variations lighting scale viewpoint. also possible detect multiple relevant objects violins people singing orchestra. ﬁnal shows failure cases ﬁrst likely reﬂects noise training data many videos contain music sheets text overlaid music playing columns network probably detects salient parts scene columns fails detect sounding objects. impressive results figure question comes mind whether network simply detecting salient object image desired behaviour. test hypothesis provide mismatched frame audio pairs inputs interrogate network answer what would make sound? check salient objects still highlighted regardless irrelevant sound. figure shows indeed case when example drums played image violin localization empty. contrast another violin played network highlights violin. furthermore completely reject saliency hypothesis case image depicting piano ﬂute possible play ﬂute sound network pick ﬂute piano played piano highlighted image. therefore network truly learnt disentangle fig. would make sound? similarly figure avol-net localization output shown given input image frame audio. however frame audio mismatched. triplet images shows input audio input frame localization output overlaid frame. purely visualization purposes hard display sound frame video aligned sound shown instead actual sound form example ﬁrst triplet ﬂute sound illustrated image ﬂute image piano ﬂute ﬂute middle image highlighted network successfully answers question what piano-ﬂute image would make ﬂute sound? input frame ﬁxed input audio varies showing object localization depend sound therefore system detecting salient objects scene achieving original goal localizing object sounds. evaluate localization performance quantitatively clips sampled randomly validation data middle frame annotated localization instrument producing sound. compare methods predicting localization ﬁrst baseline method always predicts center image; second mode avol-net heatmap produced inputting sound clip. baseline achieves whilst avol-net achieves demonstrates avol-net simply highlighting salient object center image. failure cases mainly problems audioset dataset described section note necessary annotate data rather using standard benchmark since datasets pascal coco davis kitti contain musical instruments. also means oﬀ-the-shelf object detectors instruments available could used annotate audioset frames bounding boxes. finally figure shows localization results videos. note video frame surrounding audio processed completely independently motion information used temporal smoothing. results reiterate ability system detect object variety poses fig. making sound? visualization figure column contains frames single video taken second apart. frames processed completely independently motion information used temporal smoothing. method reliably detects sounding object across varying poses shots furthermore able switch objects making sound interleaved speech guitar guitar lesson demonstrated unsupervised audio-visual correspondence task enables appropriate network design entirely functionalities learnt cross-modal retrieval semantic based localization objects sound. ave-net shown perform cross-modal retrieval even better supervised baselines avol-net exhibits impressive object localization capabilities. potential improvements could include modifying avolnet explicit soft attention mechanism rather max-pooling used currently. owens isola mcdermott j.h. torralba adelson e.h. freeman w.t. visually indicated sounds. proc. cvpr. aytar vondrick torralba hear read deep aligned shivappa s.t. b.d. trivedi m.m. audio-visual fusion tracking multilevel iterative decoding framework experimental evaluation. ieee journal selected topics signal processing throughout paper publicly available audioset dataset contains video-level audio class labels organized ontology; recall labels used training evaluation. section describes audioset-instruments subset details needed evaluation retrieval performance. make dataset manageable interesting purposes ﬁlter sounds musical instruments singing tools i.e. videos contain least label descendant three classes according audioset ontology. yields following audio classes accordion; acoustic guitar; alto saxophone; bagpipes; banjo; bass bass drum; bass guitar; bassoon; bell; bicycle bell; bowed string instrument; brass instrument; bugle; cello; change ringing chant; child singing; chime; choir; church bell; clarinet; clavinet; cornet; cowbell; crash cymbal; cymbal; dental drill dentist’s drill; didgeridoo; double bass; drill; drum; drum kit; drum machine; drum roll; electric guitar; electric piano; electronic organ; female singing; filing flute; french horn; glockenspiel; gong; guitar; hammer; hammond organ; harmonica; harp; harpsichord; hi-hat; jackhammer; jingle bell; keyboard male singing; mallet percussion; mandolin; mantra; maraca; marimba xylophone; mellotron; musical ensemble; musical instrument; oboe; orchestra; organ; percussion; piano; pizzicato; plucked string instrument; power tool; rapping; rattle rhodes piano; rimshot; sampler; sanding; sawing; saxophone; scratching shofar; singing; singing bowl; sitar; snare drum; soprano saxophone; steel guitar slide guitar; steelpan; string section; strum; synthesizer; synthetic singing; tabla; tambourine; tapping theremin; timpani; tools; trombone; trumpet; tubular bells; tuning fork; ukulele; vibraphone; violin ﬁddle; wind chime; wind instrument woodwind instrument; wood block; yodeling; zither. described section audioset ontology taken account evaluating retrieval performance example ideal system rank ‘electric guitar‘ higher ‘drums‘ querying ‘acoustic guitar‘. standard evaluation metric scenario retrieved results varying relevance normalized discounted cumulative gain here deﬁne relevance video another. recall audioset contains video-level labels videos generally multiple labels. therefore ﬁrst deﬁne relevance individual classes followed deﬁnition video relevance. class relevance. appropriate measure distance classes organized ontology tree distance i.e. length shortest path classes. example distances ‘acoustic guitar‘ ‘acoustic guitar‘ ‘electric guitar‘ ‘drums‘ respectively. relevance class another deﬁned negative tree distance oﬀset constant make sure relevances negative. video relevance. since videos generally contain multiple labels deﬁne relevance video another maximal relevance across pairs classes videos. motivation behind using maximal relevance opposed example minimal average audioset labels provided video-level. since single frames second audio clips throughout guaranteed contain video classes using measure maximal relevance would over-penalize perfectly relevant results. example consider case video person ‘singing‘ followed ‘electric guitar‘ imagine frame second half video query. ground truth tells ‘singing‘ ‘electric guitar‘ somewhere video know frame depict. therefore retrieving video contains ‘electric guitar‘ without ‘singing‘ perfectly acceptable result. vanilla form actually nothing forcing network make distances corresponding features small non-corresponding large could equally learn anti-aligned embeddings large distance visual audio features signiﬁes high similarity. stimulate desired behaviour small distance means large similarity simply needs enforce correct sign weights tiny layer. found suﬃcient initialize layer weights correct sign enforce training. fig. ave+of vision convnet. notation building blocks shared figure vision subnetwork ave+of network two-stream network image streams processed independently conv-conv-pool blocks each followed concatenating outputs ‘channel‘ dimension passing another conv-conv block. image single frame frames spatial location contains vector horizontal vertical displacements. section main paper discusses versions ave-net multiple frames input. give details better performing network ave+of which along frame second audio ingests frames optical well network follows architecture ave-net shown figure main paper vision subnetwork replaced network shown figure vision subnetwork two-stream architecture i.e. frame streams fused concatenation followed convolutional layers. output network dimensions original vision convnet fig. cross-modal retrieval embedding dimensionality. comparison method baselines terms average ndcg audioset-instruments test set. ave-net beats baselines regardless sizes baseline embeddings. note necessarily work better increased dimensionality denoising properties dimensionality reduction. fig. cross-modal retrieval. comparison method baselines terms average ndcgk various values audioset-instruments test set. ave-net beats baselines", "year": "2017"}