{"title": "Similarity measures for vocal-based drum sample retrieval using deep  convolutional auto-encoders", "tag": "eess", "abstract": " The expressive nature of the voice provides a powerful medium for communicating sonic ideas, motivating recent research on methods for query by vocalisation. Meanwhile, deep learning methods have demonstrated state-of-the-art results for matching vocal imitations to imitated sounds, yet little is known about how well learned features represent the perceptual similarity between vocalisations and queried sounds. In this paper, we address this question using similarity ratings between vocal imitations and imitated drum sounds. We use a linear mixed effect regression model to show how features learned by convolutional auto-encoders (CAEs) perform as predictors for perceptual similarity between sounds. Our experiments show that CAEs outperform three baseline feature sets (spectrogram-based representations, MFCCs, and temporal features) at predicting the subjective similarity ratings. We also investigate how the size and shape of the encoded layer effects the predictive power of the learned features. The results show that preservation of temporal information is more important than spectral resolution for this application. ", "text": "expressive nature voice provides powerful medium communicating sonic ideas motivating recent research methods query vocalisation. meanwhile deep learning methods demonstrated state-of-the-art results matching vocal imitations imitated sounds little known well learned features represent perceptual similarity vocalisations queried sounds. paper address question using similarity ratings vocal imitations imitated drum sounds. linear mixed effect regression model show features learned convolutional auto-encoders perform predictors perceptual similarity sounds. experiments show caes outperform three baseline feature sets predicting subjective similarity ratings. also investigate size shape encoded layer effects predictive power learned features. results show preservation temporal information important spectral resolution application. searching audio samples core part electronic music making process time consuming task area future technological development task typically involves browsing lists badly labelled ﬁles relying ﬁlenames ‘big kick’ ‘hi-hat’. methods browsing sound libraries limit users’ ability efﬁciently sounds looking for. meanwhile voice provides attractive medium effectively communicating sonic ideas used express timbral tonal dynamic temporal variations moreover previous research demonstrates musicians able accurately vocalise important acoustic features musical sounds query vocalisation process searching sounds based vocalised examples desired sound. typically systems extract audio features vocalisation compared features sounds sample library initial approaches used heuristic based features morphological features describing high-level temporal evolution sounds also applied however drum sounds generally similar high-level temporal morphology types features less applicable here. recent work shown features learned using stacked autoencoders outperform heuristic descriptors mfccs tasks. saes utilise deep learning structure multiple layers learn efﬁcient representation encode input. applied scenarios supervised learning using features train classiﬁer unsupervised search based distance sounds euclidean feature space furthermore authors present system based convolutional neural networks implemented semi-siamese network structure. case convolutional layers trained learn feature representations constant-q spectrograms vocal imitations imitated sounds. followed fully connected layers match input vocalisations audio samples requiring sample sound library compared vocal query. system shows promising results matching vocal imitations imitated sounds however general case systems require efﬁcient deployable querying. using method single query dataset data samples requires forward-pass computations network signiﬁcantly demanding example compared nearest neighbour search feature vector space. whilst approaches show promising performance terms retrieving imitated sound audio samples none mentioned methods consider perceptual similarity query retrieved sounds. central evaluation approaches assumption target sound indeed sound imitated task match imitations imitated sounds accordingly. however consider case query necessarily imitation sound database investigate feature representations correlate well perceptual similarity imitation audio samples. paper evaluate performance heuristic learned features drum sounds. overview approach illustrated fig. present convolutional auto-encoders trained dataset audio samples vocalisations. used extract features vocal imitations drum sounds. feature sets evaluated using perceptual similarity ratings vocal imitations imitated drum sounds. include types features spectrogram based representation authors show correlate strongly perceptual similarity drum sounds; mfccs; temporal descriptors; encoded representations caes. compare caes differ size encoded feature tensor shape encoded layer temporal spectral dimensions. fig. overview complete work ﬂow. audio preprocessed create barkgram representations. trained used extract features test data. euclidean distance imitation imitated sound computed ﬁtted rating data lmer model. performance feature sets measured model proportion imitated sounds signiﬁcantly negative slopes rating distance. task establish audio features best correlate perceptual similarity real drum sounds vocal imitations drum sounds speciﬁcally interested heuristic descriptors perform compared learned features using caes importance temporal spectral dimensions size encoded tensors caes. limit problem drum sounds classes consider similarity imitations within-class sounds baseline methods. ﬁrst spectrogram-based measure similarity shown correlate highly perceptual similarity ratings within-class drum sounds interested well transfers application. summary similarity sounds measured euclidean distance vectorised barkgrams constructed spectrogram following parameters window; overlap; bark scale loudness scaled using terhardt’s model barkgrams time-aligned sounds length shorter zero padded length longer one. second method calculate ﬁrst mfccs sound ﬁrst second order derivatives using time window overlap. mean variance mfcc derivatives calculated sound yielding features. third method temporal features attack time temporal centroid lat/tc ratio; temporal crest factor duration. calculate deﬁnition calculated entire time domain signal maximum value divided root mean squared. basic architecture four convolution layers encoder/decoder. convolutional layer followed batch normalisation relu activation layers. avoid checker board artefacts caused deconvolution layers apply upsampling prior decoding convolutional layer. such decoding deconvolution layer upsampling layer followed convolution layer stride. vary kernel size ﬁrst last layers using ﬁxed kernels convolution layers. encoding layers kernels mirrored decoder i.e. single-channel convolution layer used output layer. activation last layer encoder ﬂattened taken feature vector given test sample. kernel size stride convolution layers varied order compare shape size encoded representation respectively. details variants model given table network designed learn broad range vocal percussion related sounds including short percussive/non-percussive pitched/unpitched sounds non-verbal vocalisations. training dataset made percussion sounds sound effects single note instrument samples. addition include vocal imitations instruments synthesisers everyday sounds vocal imitations short synthesised sounds results dataset sounds vocal imitations. sound training compute barkgrams spectrograms time window overlap using bark bins. baseline magnitudes scaled using terhardt’s model curves achieve ﬁxed size representation sounds either zero-pad truncate barkgrams frames models implemented using keras tensorﬂow training validation sets split training data training dataset contains times audio samples vocal imitations equally interested learning sound types specify split audio samples/vocal imitations batch models ﬁtted using adaptive moment estimation optimiser learning rate mean squared error loss function. early-stopping scheme improvement validation loss epochs. best model parameter setting selected analysis. drum sounds taken fxpansion core bitkit sample libraries include range acoustic electronic drum samples. vocal imitations sound recorded musicians giving imitations. recordings took place acoustically treated room centre digital music queen mary university london. perceptual similarity ratings imitations within-class drum sounds collected listeners based listening test using format based mushra protocol subjective assessment audio quality whilst mushra standard speciﬁes expert listeners recently shown listeners provide comparable results experts measuring audio quality listener presented tests. test listener presented vocal imitation within-class drum sounds listener rated similarity imitation drum sound continuous scale ‘less similar’ ‘more similar’. test pages unique random duplicates. included post-screening listeners recommended mushra standard listener reliability assessed using spearman rank correlation duplicate test pages listener. considered reliable listeners able replicate responses least duplicates i.e. large positive correlation reliable listeners giving responses tests computed kendall’s coefﬁcient concordance ranked responses imitation. mean/standard error indicating moderate strong agreement amongst reliable listeners analysis ratings indicated listeners able correctly identify imitated sound chance accuracy imitated sound rated ﬁrst second similar imitation tests. indicates although imitations often rated similar imitated sounds considerable number cases within-class sounds rated similar imitation imitated sound. highlights potential importance perceptual similarity measures tasks depending whether task identify return imitated sound return similar sound. similarity ratings used ground truth measure performance feature sets. given feature distance measured imitations respective within-class sounds giving distance values. euclidean distance keeping baseline method distances feature normalised linear mixed effect regression models ﬁtted predicting ratings distances. lmer well suited task given listeners provide ratings imitations randomly-selected imitations addition allows include dependencies ratings listener imitated sound. maximum likelihood parameters models estimated using package general model ﬁtted rating yijk dependent variable rating random intercepts listener ﬁxed effects distance imitated sound interaction term distance imitated sound. model given slope rating distance given instance random intercept given listener note model analysis showed heteroskedasticity residuals. parameter estimates therefore compared robust models major differences found. non-robust models used analysis. wald conﬁdence intervals calculated slope interaction imitated sounds upper infer slope signiﬁcantly indicates feature good predictor imitated sound question. performance feature evaluated using metrics percentage imitated sounds signiﬁcantly akaike’s information criterion gives measure model ideal feature would signiﬁcantly negative imitated sounds good rating data given model results given table encoded features caes outperform baseline feature sets. lmer model best performing feature gives ﬁtted slopes rating distance signiﬁcantly less imitated sounds lowest aic. shows feature generally good predictor perceptual similarity vocal imitations imitated sounds tested here best ﬁtting lmer model. interestingly preservation temporal resolution important spectral resolution task caes wide time narrow frequency performance improves size encoded layer decreases. indicates redundancy spectral information encoded shapes spectral dimensions adverse effect performance. similarity ratings sounds class table details caes results feature sets. caes differ kernel shape shape encoded layer results given terms lmer model percentage imitated drum sounds rating distance slope signiﬁcantly less note lower better model pitched sounds dataset indeed toms pitched). suggests reducing size encoded spectral shape work best drum sounds used here however predictions pitched sounds suffer result. finally note slopes although generally approach listener rating data inherently noisy concordance amongst listeners varies across sounds. such clearly glass ceiling performance perfect model would useful real world application lmer model. indeed perfect model desirable interested generalisability ﬁtted lmer model. paper apply convolutional auto-encoders query vocalisation drum sound retrieval. present novel evaluation using perceptual similarity ratings vocal imitations imitated drum sounds providing insight learned features perform predicting ratings. speciﬁcally compare caes differ size shape encoded layer terms spectral temporal dimensions. experiments show caes outperform sets heuristic features considerable margin. furthermore show reducing size encoded layer height increases predictive power learned features reducing width opposite effect. ﬁnding partly unexpected given drum sounds generally similar overall temporal envelope however understandable given compare within-class sounds also likely share similar spectral distributions. future work would like investigate ﬁne-grained morphological features represent temporal evolution appears important here. addition would like investigate generalisability best performing ﬁtted lmer model tasks determine model ﬁtted sounds similarity ratings performs given larger sound library might used typical music production environment. fig. slope estimates lmer model ﬁtted performing feature negative slope indicates decrease perceptual similarity increase distance i.e. sounds feature performs well. expect high spectral similarity within class. such overall energy differences time salient spectral distribution providing cues used listeners giving ratings. hypothesis supported comparing square tall caes reducing size time dimension decreases performance. however also redundancy temporal information seen comparing feature sets post-hoc analysis tested variants using smaller encoded kernel shapes found decrease performance effect also seen models performance decreases width reduced regarding baseline features temp mfcc show similarly poor performance terms indicates although learned temporal features appear important task heuristic temporal features sufﬁcient capture salient cues used listeners. beneﬁts learned features mfccs concur previous work however greater disparity performance. speciﬁc sounds used evaluation much wider range sounds used). improved performance compared baselines indicates measure somewhat transferable vocalised drum sounds although still achieving accuracy analysis lmer model best performing feature shows individual slopes drum sound observe considerable variation imitated sounds. particular note sounds upper crosses pitched mark cartwright bryan pardo vocalsketch vocally imitating audio concepts proceedings annual conference human factors computing systems seoul korea franc¸ois chollet keras mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems arxiv preprint arxiv. mark cartwright bryan pardo gautham mysore matt hoffman fast easy crowdsourced perceptual audio evaluation ieee international conference acoustics speech signal processing giantsteps semistructured conversations musicians proceedings annual conference extended abstracts human factors computing systems seoul korea guillaume lemaitre jabbari olivier houix nicolas misdariis patrick susini vocal imitations basic auditory features journal acoustical society america vol. adib mehrabi simon dixon mark sandler vocal imitation synthesised sounds varying pitch loudness spectral centroid journal acoustical society america vol. david sanchez blancas jordi janer sound retrieval voice imitation queries collaborative databases proceedings audio engineering society conference london england enrico marchetto geoffroy peeters audio features morphological description vocal imitations proc. international conference digital audio effects trondheim norway yichi zhang zhiyao duan retrieving sounds vocal imitation recognition proceedings ieee international workshop machine learning signal processing boston yichi zhang zhiyao duan imisound unsupervised system sound query vocal imitation ieee international conference acoustics speech signal processing shanghai yichi zhang zhiyao duan iminet convolutional semisiamese networks sound search vocal imitation ieee workshop applications signal processing audio acoustics york elias pampalk perfecto herrera masataka goto comieee putational models similarity drum samples transactions audio speech language processing vol.", "year": "2018"}