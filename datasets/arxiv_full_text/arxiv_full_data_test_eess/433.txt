{"title": "Piecewise Flat Embedding for Image Segmentation", "tag": "eess", "abstract": " We introduce a new multi-dimensional nonlinear embedding -- Piecewise Flat Embedding (PFE) -- for image segmentation. Based on the theory of sparse signal recovery, piecewise flat embedding with diverse channels attempts to recover a piecewise constant image representation with sparse region boundaries and sparse cluster value scattering. The resultant piecewise flat embedding exhibits interesting properties such as suppressing slowly varying signals, and offers an image representation with higher region identifiability which is desirable for image segmentation or high-level semantic analysis tasks. We formulate our embedding as a variant of the Laplacian Eigenmap embedding with an $L_{1,p} (0<p\\leq1)$ regularization term to promote sparse solutions. First, we devise a two-stage numerical algorithm based on Bregman iterations to compute $L_{1,1}$-regularized piecewise flat embeddings. We further generalize this algorithm through iterative reweighting to solve the general $L_{1,p}$-regularized problem. To demonstrate its efficacy, we integrate PFE into two existing image segmentation frameworks, segmentation based on clustering and hierarchical segmentation based on contour detection. Experiments on four major benchmark datasets, BSDS500, MSRC, Stanford Background Dataset, and PASCAL Context, show that segmentation algorithms incorporating our embedding achieve significantly improved results. ", "text": "abstract—we introduce multi-dimensional nonlinear embedding piecewise flat embedding image segmentation. based theory sparse signal recovery piecewise embedding diverse channels attempts recover piecewise constant image representation sparse region boundaries sparse cluster value scattering. resultant piecewise embedding exhibits interesting properties suppressing slowly varying signals offers image representation higher region identiﬁability desirable image segmentation high-level semantic analysis tasks. formulate embedding variant laplacian eigenmap embedding regularization term promote sparse solutions. first devise two-stage numerical algorithm based bregman iterations compute l-regularized piecewise embeddings. generalize algorithm iterative reweighting solve general lp-regularized problem. demonstrate efﬁcacy integrate existing image segmentation frameworks segmentation based clustering hierarchical segmentation based contour detection. experiments four major benchmark datasets bsds msrc stanford background dataset pascal context show segmentation algorithms incorporating embedding achieve signiﬁcantly improved results. overlapping regions covers pixels sharing common low-level high-level characteristics. supports high-level visual inference perception ﬁgure/ground analysis object discovery. supplies vital cues methods generate object proposals often serve starting point object detection. also important low-level operations object extraction image compositing photo enhancement many challenges exist image segmentation including existence textures low-frequency appearance variations objects image. textures bring high frequency variations low-frequency appearance variations caused multiple factors including spatially varying illumination shading. textures low-frequency variations could make differences distant pixels object exceed differences nearby pixels different objects increasing difﬁculty decide object boundaries solutions suppressing textures using local cues low-frequency variations harder remove would require global solutions. common practice tackle challenge embeds pixels feature space solving optimization problem usually continuous approximation balanced graph feature space pixels similar attributes pulled closer pixels dissimilar attributes stay away other. clustering fang department computer science university hong kong hong kong. liao college computer science zhejiang university hangzhou china. e-mail chwfangconnect.hku.hk; zliaozju.edu.cn; yizhouyacm.org fig. embedding segmentation results piecewise embedding given input image transforms embedding channels focuses subset characteristics original image. main highlight embedding embedding channels tend piecewise ﬂat. frameworks tested segment image using embedding results including clustering based segmentation contour driven hierarchical segmentation pixels feature space would improve segmentation performance distances space preserve intrinsic visual differences insensitive distractions caused shading illumination. nonetheless pairwise afﬁnity measured coarsely l-regularized graph embedding method noticeable smooth variations within object regions often make clustering-based segmentation algorithms produce false region boundaries effective approach removing smooth variations computes nonlinear embeddings various norms. norm adopted spectral clustering produce -dimensional embedding relatively small variations within clusters image segmentation comprehensive summarization segmentation methods given discuss types segmentation techniques closely related method. pixels image taken unordered data samples clustering algorithms k-means gaussian mixture models meanshift used image segmentation. gradient based methods form representative approach image segmentation. examples include segmentation based watershed transform owt-ucm transforms boundary probability hierarchical segmentation oriented watershed transform designed remove noises arising standard watershed algorithm ultrametric contour combines closed non-self-intersecting weighted contour information. recent developments boundary detection owt-ucm remains favorable choice image segmentation starting small superpixels inscra generates hierarchical segmentation help sequence cascaded discriminative classiﬁers. recent hierarchical segmentation algorithm inspired least effort principle models amount effort involved tracing boundaries human. graph partition another widely used approach. modeling pixel grid image markov random ﬁeld graphcut algorithms developed solve binary multi-label image segmentation efﬁciently. graph cuts solves partition problem using energy function consisting posterior term potts model. distance adopted potts model preserve discontinuity. graph based segmentation another efﬁcient method based greedily deciding existence boundaries pairs regions. spectral embedding methods project pixels feature space similar pairs pixels stay closer dissimilar pairs. classical ratio problem considered normalized forward channels spectral embedding provide better features clustering based segmentation also clearer global boundary information cheeger able lower bound second smallest eigenvalue graph laplacian. ¨uhler hein utilize uniform p-laplacian formulation deﬁne spectral embedding methods various norms prove optimal solution cheeger found thresholding second eigenvector p-laplacian also devised algorithm based gradient newton steps compute second eigenvector algorithm applied divisive clustering/segmentation. work dedicated solving second eigenvector -laplacian basis split bregman inverse power algorithms respectively. method similar p-laplacian also adopt variational objective function sparsity regularization. however method differs aspects. first seek solve lp-regularized objective function second spectral methods using plaplacian compute single embedding ressing variations within segments clusters preserving discontinuities them. context image segmentation since object boundaries occupy small percentage pixels whole image locally connected graph exist sparse subset pixel pairs whose pixels come different object regions. embedding pursue aforementioned property small subset pixel pairs maintaining sufﬁciently large distances distance remaining pixel pairs pushed zero. condition locally connected graph noisy similarity measurements embedding problem solved minimizing objective function regularized sparsity nonzero pairwise pixel differences according theory sparse signal recovery speciﬁcally attempt promote sparse solutions multiple dimensions adopting lp-regularized energy term objective function. lp-regularized embedding generalization l-regularized formulation extra space sparsity pursuit increased degree nonconvexity becomes smaller. numerical solution ﬁrst developed solve l-regularized objective function nesting existing numerical solvers orthogonally constrained embedding channels simultaneously computed local optima objective function. inspired iteratively reweighted algorithms design iteratively reweighted algorithm based numerical solution l-regularized optimization solve generic lp-regularized problem. easy integrate piecewise embedding existing image segmentation frameworks including clustering based image segmentation contour-driven hierarchical segmentation example embedding results subsequent clustering based segmentation shown figure conduct experiment four popular benchmark datasets including bsds stanford background dataset precise version -class msrc pascal context image segmentation frameworks incorporating achieve stateof-the-art performance comparing counterparts. contents version summarized follows. previous l-regularized objective function extended lp-regularized formulation still makes embedding piecewise ﬂat. iteratively reweighted numerical algorithm based nested bregman iterations devised minimize objective function. overall numerical solution becomes order magnitude faster adoption cholesky decomposition directly solving sparse least-squares problems numerical pipeline. initialization schemes weighting strategy based residual cost individual embedding channels devised piecewise embedding achieve improved performance image segmentation. segmentation algorithms integrating embedding produce state-of-the-art performance four benchmark datasets fig. sparsity edges crossing region boundaries locally connected graph. ratio boundary length whole image area top-left image n-connected graph patch visualized divided boundary shown red. among edges cross boundary. actually percentage edges depends length shape boundaries. statical observations groundtruth segmentation maps bsds provided bottom approximately linear relation distribution among segmentation maps every pixel connected others neighborhood. average respectively. further would like constrain feature embedding pixels region tightly another possible therefore achieving goal piecewise embedding. piecewise ﬂatness property matters. point view reduces intra-class variations pixel values within image segments reasons explained earlier. another point view aims factorize piecewise smooth component known intrinsic component pixel values image formation pursues feature embedding captures piecewise constant representations. section describe formulation problem numerical solution problem deﬁnition given data points would like embed d-demension space data matrix space k-th representing coordinates k-th data point space. embedding achieves sparsity piecewise property minimizing following objective function channel every time namely second eigenvector making limited recursive top-down clustering. objective function inherits orthogonality constraint generalized version normalized -laplacian namely laplacian eigenmaps enables multiple diverse embedding channels computed simultaneously. thus method conveniently integrated clustering gradient based segmentation algorithms. manifold learning cope curse dimensionality various manifold learning algorithms proposed nonlinear dimension reduction preserving structural information. algorithms roughly fall categories local global methods. local methods laplacian eigenmaps attempt preserve local structures among data points. t-sne extension generates pleasant embedding results visualization high dimensional data. gnmf another local method proposes graph regularized objective function take account reconstruction loss local pairwise variation cost. global methods isomap structure preserving embedding attempt preserve local global relationships among data points. sparsity constraints considered smce sparse embedding smce constructs neighborhood relationships selecting neighbors manifold data point solved sparse optimization problem. follows spectral method compute embedding clustering results. sparse embedding performs simultaneous dimension reduction dictionary learning learning transformation original high-dimensional space low-dimensional space preserving sparse structures. proposed attempts learn sparse global afﬁnity matrix representing every sample samples subspace. reference sample contributes reconstructing query sample higher similarity possess. graphencoder tries embed afﬁnity matrix feature space directly prototype autoencder. proposes embed different types data uniﬁed space help deep learning technologies. method differs embedding techniques sense directly minimize global objective function formulated using regularization without dictionary learning achieves high performance image segmentation task. piecewise flat embedding consider problem dividing image regions containing pixels similar low-level/high-level features. ﬁrst construct locally connected graph represents pixels represents edges connecting pixels within neighborhoods. usually small fraction edges straddle across region boundaries sparseness boundaries image plane majority pixels region. intuitions statistical observations bsds provided figure indicate region boundaries might discovered solution energy minimization problem sparsity regularization. fig. laplacian eigenmaps embedding results histograms embedding channel. channels almost piecewise constant channels piecewise smooth. adopted norm histograms results much sparser laplacian eigenmaps. u-th column ﬁrst condition indicates every embedding channel balanced using normalized criterion second condition makes different embedding channels orthogonal other improves diversity embedding channels. discretized total variation regularization frequently used computer vision tasks image smoothing motion estimation typical image restoration/smoothing objective function weighted nonlocal total variation regularization written follows represents restored/smoothed result input image. constant usually computed guidance image. ﬁrst term formulation data term uses input image soft constraint avoid trivial optimal solution. problem solved using weighted least squares lp-regularized total variation used dynamic guidance exploited help welsch’s function data term results total variation formulations typically enhanced/restored images whose channels strictly correspond channels input image. objective function utilizes similar regularization norm. however data term total variation formulations replaced although similar obvious difference norm formulation laplacian eigenmaps inherits common trait mainstream multi-dimensional embedding methods squared norm objective function. fact small difference following important implication. norm promotes sparse solutions norm not. sparse solution implies that among locally connected point pairs exists sparse subset point pairs whose distances space sufﬁciently large distances rest point pairs almost zero. sparse solution suggests points large distance space belong different clusters points small distance space belong cluster. therefore performing clustering space becomes straightforward process. hand although afﬁnity matrix attempts move points similar attributes closer space resulting pairwise distances space still follows relatively smooth distribution measurement pairwise afﬁnity coarse noisy distinction similar points dissimilar points clear results. context image segmentation sparse solution means exist sparse subset locally connected pixel pairs whose pairwise difference space yi−yj remains sufﬁciently different zero. since afﬁnity appears together large afﬁnity forces approach zero becomes nonzero afﬁnity relatively small. hand pixel pairs crossing region boundaries typically smaller afﬁnities region. therefore sparse solution capable facilitating discovery region boundaries. interior region would likely zero large afﬁnity them. implies embedding channel almost constant value inside every region hence name piecewise embedding. visual comparison l-regularized laplacian eigenmaps shown figure image represents pixelwise values embedding channel histogram image represents pixel distribution embedding channel. according results ﬁgure channels almost piecewise constant histograms much sparser laplacian eigenmaps still possess obvious nonzero gradients within image regions. orthogonality constraint formulation. adoption orthogonality constraint gives freedom deﬁne novel embedding spaces independent input image. instance dimensionality embedding space different input image embedding channels correspond channels input image. nonetheless freedom brings numerical challenges. numerical solution following ﬁrst present iterative numerical solution special case problem norm used instead general norm. numerical solution original problem norm introduced iteratively reweighted version special-case solution. solving problem challenging consists regularization orthogonality constraint existing numerical algorithms handle them. present numerical solution nesting existing solvers handle l-regularized optimization orthogonality constraints respectively. solution later accelerated two-stage strategy relaxes orthogonality constraint. subproblem step numerical solution inherits l-regularized energy term exist many numerical solutions optimization problems regularization. reasons discussed later fig. energy curves. black curve shows energy decreases regular single-stage bregman iterations dashed curves show energy evolves -stage algorithm. ﬁrst stage black curve. second stage starts iteration three dashed curves show varying convergence rates using different parameter values choose apply split bregman algorithm solve subproblem step introducing auxiliary variables split bregman algorithm solves l-regularized optimization iteratively transforming original optimization series differentiable unconstrained convex optimization problems. deﬁnition convex optimization series depends auxiliary variables passed previous iteration convergence achieved within relatively small number iterations. detailed solution l-regularized problem step enclosed appendix two-stage implementation practice following two-stage implementation obtain high-quality solution efﬁciently. stage stage implements full numerical solution nested bregman iterations. large penalty coefﬁcient orthogonality constraint used. outer loop makes different dimensions embedded data orthogonal remove redundancy among them. important avoiding naive solutions highly redundant even duplicate dimensions. however orthogonality highly non-convex constraint prevents objective function settling truly low-energy state; important absolutely necessary pursue embedding whose dimensions strictly orthogonal long much redundancy across different dimensions. therefore following iterations full numerical solution relax orthogonality constraint second stage. stage stage executes bregman iterations inner loop without performing outer loop strictly enforce orthogonality. relaxation orthogonality constraint allows objective function reach lower energy. figure shows example energy curve stage stage veriﬁes two-stage scheme reach lower energy single-stage one. residual-based channel weighting sake efﬁciency numerical procedure stage predeﬁned number iterations instead waiting independently obtain multi-channel embedding. optimization processes method -laplacian presented using energy value second smallest eigenvalue respectively. convergence rates algorithm -laplacian comparable. laplacian gives rise similar embedding results across channels despite different initializations method computes much diverse channels. average costs compute channel using -laplacian compute channels using method. solution lp-regularized objective function leverage algorithms handling simpler objective functions sophisticated objective function optimized class numerical methods based iterative reweighting previously developed. methods iteratively optimize smooth nonsmooth convex objective function dynamically weighted coefﬁcients evaluated previous iteration. instance irls solves iteratively reweighted quadratic problems solves iteratively reweighted l-regularized problems. algorithms using iterative reweighting recently summarized inspired methods develop method based iterative reweighting solve lp-regularized problem done l-regularized problem lpregularized problem also solved stages. ﬁrst stage numerical procedures solving problems completely same. result ﬁrst stage initializes second stage. second stage revise objective function step procedure solving follows residual-based weighting introduced earlier still applied individual channels obtained ﬁrst step improve segmentation performance. deﬁne result residual-based weighting second step algorithm appendix revised follows fig. comparison -laplacian left right input image; detected boundary using afﬁnity matrix computation; initialization based weighted spectral clustering section bottom left right curves energy value second smallest eigenvalue result -laplacian; result pfe. channels computed practice embedding channels visualized color channels empirical error every embedding channel falls predeﬁned tolerance thus obtained embedding channels differ respect degree convergence. moderate effect early termination devise following weighting scheme based residual cost objective function respect embedding channel v-th columns obtained stage ﬁnal embedding respectively. weighting scheme makes importance channel lower larger residual cost. comparison pfebased segmentations without residual-based channel weighting shown figure segmentation results residual-based channel weighting clearly better. comparison -laplacian inverse power method adopted compute eigenvector -laplacian second smallest eigenvalue. method enables multiple diverse embedding channels computed concurrently help orthogonality condition comparisons embedding results convergence rates shown figure afﬁnity matrix computed boundary methods. -laplacian applied multiple distinct initializations computes generalized eigenvectors locally connected graph. elements eigenvectors corresponding pixel form d-dimensional feature vector pixel. runs standard clustering k-means pixelwise feature vectors. variant spectral clustering named weighted spectral clustering reweighs i-th feature coordinates eigenvalue corresponding i-th eigenvector. revised clustering-based segmentation simply replaces eigenvectors spectral clustering channels embedding. matrix becomes d-dimensional feature vector corresponding pixel. since embedding spectral instead eigenvalues make channel-wise residual cost channel weighting scheme deﬁned afterwards still perform k-means clustering original weighted channels. since embedding piecewise pixels region tightly distributed feature space. contrast existing embedding techniques laplacian eigenmaps share property. laplacian eigenmap obtained minimizing smooth l-regularized objective function resulting eigenmaps piecewise smooth piecewise ﬂat. therefore pixels region still reasonably large distances among feature space grouped together cluster. reason large elongated regions often broken pieces segmentation results based spectral clustering. contour-driven hierarchical segmentation strategy avoiding drawback spectral clustering applied owt-ucm derives global contour information eigenmaps computed afﬁnity matrix forms segments respect contours using hierarchical agglomerative clustering. hierarchical tree structure built clustering makes ﬂexible choose segmentation granularity. make algorithm robust method also combines local edges global contours constructing contour probability map. contour-driven segmentation follows pipeline except global contours extracted channels embedding instead eigenmaps. again since embedding piecewise channels embedding almost zero gradients everywhere except region contours gradients large magnitude. contour maps extracted embedding clear clean without many spurious edges. initialization image segmentation initialization plays important role iterative algorithm converge local minimum. proposed initialization scheme based gaussian mixture models gaussian model clusters gaussians generate density maps. density maps encoded images initial channels piecewise embedding. suppose gaussians ordered linear sequence. initial channel formed summing density maps subtracting mean every pixel. example ﬁrst channel fig. examples piecewise embedding. input images located ﬁrst third rows. images second fourth rows generated visualizing embedding channels color channels. note embedding results clearly exhibit piecewise ﬂatness. comparison segmentations obtained regularization regularization shown figure regularization gives rise accurate results. parameter setting methods developed paper select parameters follows. stage maximum number outer iterations number inner iterations stage algorithm optimizing l-regularized objective function maximum number iterations lp-regularized objective function still exists double loop second stage iterative reweighting. maximum number outer iterations number inner iterations less. parameter setting given following section image segmentation. figure shows embedding results. image segmentation using piecewise flat embedding image resolution gives rise data points. computing afﬁnity value neighboring pixels locally connected graph edge weights represented sparse afﬁnity matrix piecewise embedding data points d-dimensional space computed using numerical algorithms previous section. integrated embedding results popular image segmentation frameworks. figure illustrates segmentation pipeline. fig. image segmentation pipeline. given input image method generates piecewise embedding afﬁnity matrix image two-stage optimization. channels embedding used image segmentation either clustering-based method contour-driven hierarchical method. fig. initialization. probability density maps deﬁned gaussian models pixel clusters formed spectral clustering. channel initialization represents mixed density gaussian models. i-th channel mixes density maps −i}. records mixed density ﬁrst half gaussians; divide sequence middle equal subsequences second channel records mixed density ﬁrst half gaussians subsequences. initial initialized orthogonalized version using example initialization shown figure however neither efﬁcient robust implemented using expectation maximization iterations initialized randomly. devise effective schemes hierarchical clustering weighted spectral clustering hierarchical clustering ﬁnest level superpixel representing superpixel average values agglomeratively merging clusters left. employ values compute afﬁnity matrix cluster pixels groups. ﬁnal model adopts wsc-based scheme evaluate compare performances three schemes later section. another schemes also tested show importance initialization random values; simple color combination r◦g◦b experiments conducted experiments within aforementioned frameworks segmentation clustering contour-driven hierarchical segmentation compared revised segmentation methods incorporating embedding original existing methods. clustering-based segmentation using k-means need specify cluster numbers. used following schemes performance testing ﬁxed scheme dynamic scheme. ﬁxed scheme uses number segments provided groundtruth segmentation. multiple groundtruth segmentations algorithm number segments every groundtruth segmentation take average. dynamic scheme sets number segments numbers chooses best performance. methods require multiple ncut eigenvectors method using eigenvectors respectively take highest performance. fig. comparison clustering based segmentation methods. normalized based original local boundary cues; methods adopt local boundary cues previous methods tend produce segments break semantically coherent region. results three different initialization options method shown. also incorporated piecewise embedding contour-driven hierarchical segmentation following pipeline gpb-owt-ucm revised algorithms replace eigenmaps piecewise embedding. also conducted segmentation experiments additionally replacing local contours gpb-owt-ucm computed deep boundaries multi-scale boundaries compare segmentation performance revised algorithms original le-based algorithms number existing algorithms. comparison results shown tables metrics results evaluated using four standard criteria measure boundaries segmentation covering probabilistic rand index variation information summarized plus another widely used metric measure objects parts proposed threshold values based methods. datasets experiments comparison conducted following benchmark datasets object regions used groundtruth segmentation withsemantic meaning msrc pascal context. implementation computing piecewise embedding clustering-based segmentation parameters respectively stage reduced stage neighborhood radius computed using l-regularized lp-regularized objective functions respectively. computing piecewise embedding contour-driven hierarchical segmentation parameters respectively stage reduce stage neighborhood radius notations clarity deﬁne following expression differentiate among variants piecewise embedding either denoting initialization based random numbers color combination hierarchical clustering either denoting whether residual-based channel weighting turned not; width neighborhood window; either l-regularized objective lp-regularized objective. example gopfe version meaning computed using l-regularized objective based initialization without channel weighting. swpfep means computed using lp-regularized objective residual-based channel weighting initialization. segmentation using existing methods framework clustering-based segmentation methods incorporating piecewsie embedding achieve signiﬁcantly better performance adopting normalized cuts including spectral clustering weighted spectral clustering shown tables particularly variant embedding swpfe. achieves higher corresponding method using laplacian eigenmaps table context hierarchical image segmentation embedding standalone segmentation algorithm demonstrate incorporating embedding makes existing hierarchical algorithms achieve clearly better performance. shown table integrated gpb-owt-ucm resulting algorithm ‘swpfe.+mpb’ exhibits better performance fig. precision-recall curves objects parts measure fop. left right result bsds msrc pascal context dataset respectively. methods using swpfe. consistently outperforms original existing methods. considered metrics except increased improved likewise ‘swpfe.+mcg’ delivers better performance original metrics. replacing local contours gpb-owt-ucm computed using deep boundaries resulting ‘swpfe.+db’ delivers substantially better performances counterpart achieves state-ofthe-art performance bsds contour-driven hierarchical segmentation framework. consistently favorable results delivered pfe-based methods msrc pascal context precision-recall curves various segmentation algorithms measure shown figure table performances ms-ncut mean shift taken result hoiem methods evaluated seism toolbox algorithm taking afﬁnity matrices computed local boundary tested hierarchical segmentation method. maximum number clusters normalized criteria adopted. precomputed result provided mentioned earlier objective function bears resemblance energy function utilizes weighted nonlocal total variation regularization. attempted make image segmentation directly extracting global contours scales smoothed outputs. afﬁnity measure computed fig. ablation analysis parameters ﬁnal segmentation performance experiment parameters except analyzed ﬁxed default setting. experiments conducted bsds dataset contour-driven hierarchical segmentation framework. method used solve performance reported ’wls+db’. similarly ﬁltering method also integrated image segmentation reported ’sdf+db’. however results comparable result method. addition region-oriented measures boundaryoriented measure also widely used evaluate performance image segmentation. since embedding focuses ﬂattening regions still existing local boundary cues contour-driven segmentation piecewise embedding tends substantially improve regionoriented segmentation measures achieving equivalent slightly better results initialization schemes table shows performance perturbation different initialization schemes weighted original schemes. initialization schemes random values color combination hierarchical segmentation described section original scheme i.e. without residual-based channel weighting three initialization schemes make little differences weighting scheme noticeably worse slightly better hierarchical-based initialization. example measure ’gwpfe.+db’ schemes measure ’gwpfe.+db’ ’hwpfe.+db’ ’swpfe.+db’ initializations using random values color combination derive almost performances except different measures. experiments conducted bsds dataset. results datasets exhibit similar behavior. original weighted residual-based channel weighting makes clear difference three initialization schemes shown table example compare measure ’swpfe.+db’ ’sopfe.+db’ former latter measure performance gain brought channel weighting hierarchical initializations respectively. besides experiments show weighting scheme less effect clustering-based segmentation framework. gallery segmentation results various algorithms addition categorical parameters initialization channel weighting option let’s investigate effects important continuous parameters ﬁnal segmentation performance. parameters include neighborhood size. contour-driven hierarchical segmentation framework evaluation take value performance metrics evaluation. results plotted figure controls sparsity embedding. smaller improves sparsity. however sparser embedding always gives rise better segmentation performance. evaluation best result achieved around measures except improves increases. shown step weight l-regularized energy term. balances sparsity embedding orthogonality among embedding channels. performance metrics change consistently varies. produces best result respect around however covering become better overally increases remains stable within testing range maximum variation less neighborhood size larger neighborhood allows direct interaction among pixels apart decreases sparsity boundary-crossing connections brings difﬁculty computing accurate pairwise afﬁnity. best performance bsds four measures achieved neighborhood radius around beneﬁts smaller neighborhood radius. conclusions presented multi-dimensional nonlinear embedding called piecewise embedding image segmentation. adopt lp-regularized energy term objective function promote sparse solutions. solve l-regularized objective function two-stage numerical algorithm ﬁrst developed nesting existing solvers. generalize numerical algorithm iterative reweighting solve lpregularized problem. experiments bsds msrc pascal context indicate segmentation methods incorporating embedding achieve signiﬁcantly improved results. work supported part hong kong research grants council general research funds supported part nsfc young researcher grant alibaba-zju joint institute frontier technologies. processing visual information. press pont-tuset arbelaez barron marques malik multiscale combinatorial grouping image segmentation object proposal generation ieee transactions pattern analysis machine intelligence vol. elmagarmid aref automatic image segmentation integrating color-edge extraction seeded region growing ieee transactions image processing vol. y.-w. salient region detection high-dimensional color transform local spatial support ieee transactions image processing vol. image transform edgepreserving smoothing scene-level intrinsic decomposition transactions graphics vol. august martin fowlkes malik learning detect natural image boundaries using local brightness color texture cues pattern analysis machine intelligence ieee transactions vol. malik belongie leung contour texture analysis image segmentation international journal computer vision vol. hein ¨uhler inverse power method nonlinear eigenproblems applications -spectral clustering sparse advances neural information processing systems ochs dosovitskiy brox pock iteratively reweighted algorithms nonsmooth nonconvex optimization computer vision siam journal imaging sciences vol. malisiewicz efros improving spatial support objects multiple segmentations proceedings british machine vision conference. bmva press doi./c... shotton winn rother criminisi textonboost joint appearance shape context modeling multi-class object recognition segmentation european conference computer vision. springer mottaghi chen n.-g. s.-w. fidler urtasun yuille role context object detection semantic segmentation wild proceedings ieee conference computer vision pattern recognition. chen davis hager rajamanickam algorithm cholmod supernodal sparse cholesky factorization update/downdate transactions mathematical software vol. arbelaez maire fowlkes malik from contours regions empirical evaluation ieee conference computer vision pattern recognition june doll´ar zitnick fast edge detection using structured forests pattern analysis machine intelligence ieee transactions vol. maninis pont-tuset arbel´aez gool convolutional oriented boundaries image segmentation highlevel tasks ieee transactions pattern analysis machine intelligence images least effort humans proceedings british machine vision conference xianghua eds. bmva press september .–.. available https//dx.doi.org/./c.. bregman relaxation method ﬁnding common point convex sets application solution problems convex programming ussr computational mathematics mathematical physics vol. daubechies devore fornasier ¨unt ¨urk iteratively reweighted least squares minimization sparse recovery communications pure applied mathematics vol. pont-tuset marques supervised evaluation image segmentation object proposal techniques ieee transactions pattern analysis machine intelligence vol. cour benezit spectral segmentation multiscale graph decomposition computer vision pattern recognition cvpr ieee computer society conference vol. ieee y.-c. c.-k. cheng towards efﬁcient hierarchical designs ratio partitioning computer-aided design iccad. digest technical papers. ieee international conference ieee maire arbelaez fowlkes malik using contours detect localize junctions natural images ieee conference computer vision pattern recognition june nguyen patel nasrabadi chellappa sparse embedding framework sparsity promoting dimensionality reduction proceedings european conference computer vision volume part ser. eccv’. berlin heidelberg springer-verlag available http//dx.doi.org/./---- tian chen t.-y. learning graph clustering proceedings deep representations twenty-eighth aaai conference artiﬁcial intelligence ser. aaai’. aaai press available http//dl.acm.org/citation.cfm?id=. chang tang g.-j. aggarwal huang heterogeneous network embedding deep architectures proceedings sigkdd international conference knowledge discovery data mining. choi sohn fast global image smoothing based weighted least squares ieee transactions image processing vol. werlberger pock bischof motion estimation non-local total variation regularization computer vision pattern recognition ieee conference ieee fig. comparison contour-driven hierarchical segmentation methods bsds. ’swpfe.+cob’ denotes contour-driven segmentation algorithm integrates swpfe. utilizing local boundary information cob. according majorization-minimization algorithm applied solve problem form following iterative algorithm makes energy value monotonically decrease matrix following important characteristics stays across iterations right hand side evolves; sparse symmetric positive deﬁnite hermitian diagonally dominant matrix real positive diagonal entries. needs decomposed iterations using sparse matrix solver. practice cholesky matrix decomposition factorize matrix lower triangular matrix. thus equivalent triangular systems iteration extremely efﬁcient forward backward substitution method used solve sparse linear system main reason choice splitting bregman algorithm solving l-regularized optimization. conference version mldivide function matlab performs cholesky decomposition internally without returning triangular matrix. thus unnecessarily perform cholesky decomposition every iteration solving work perform cholesky decomposition factorize matrix save resulting triangular matrix. allows solve triangular systems every iteration shown gives rise much faster algorithm. test sample images bsds dataset", "year": "2018"}