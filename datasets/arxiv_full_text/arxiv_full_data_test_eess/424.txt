{"title": "Layered Coding of Hidden Markov Sources", "tag": "eess", "abstract": " The paper studies optimal coding of hidden Markov sources (HMS), which represent a broad class of practical sources obtained through noisy acquisition processes, beside their explicit modeling use in speech processing and recognition, image understanding and sensor networks. A new fundamental source coding approach for HMS is proposed, based on tracking an estimate of the state probability distribution, and is shown to be optimal. Practical encoder and decoder schemes that leverage the main concepts are introduced. An iterative approach is developed for optimizing the system. It also focuses on a significant extension of the optimal HMS quantization paradigm. It proposes a new approach for scalable coding of HMS which accounts for all the available information while coding a given layer. Simulation results confirm that these approaches significantly reduce the reconstructed distortion and substantially outperform existing techniques. ", "text": "known parameters analyzed steady state performance. ﬁnite state quantizer general effective approach fully exploit underlying structure hms. words ﬁnite state quantizer agnostic source state although updates encoder state based observations. subtle important shortcoming motivates scheme explicitly considers source states quantization process approach pursued previous work fundamental approach exploit available information source state i.e. probability distribution states underlying markov chain. important distinction paradigm prior work speciﬁcally ﬁnite-state quantizer state probability distribution captures available information depends entire history observations. hence observation encoder decoder reﬁne estimate state probability distribution correspondingly update coding rule. fundamental approach therefore optimal. specialize practical codebook switching variant propose algorithm optimizes decision rule i.e. optimize codebook used quantization based estimate state probabilities. also focused signiﬁcant extension optimal quantization paradigm. advances internet communication technologies created extremely heterogeneous network scenario data consumption devices highly diverse decoding display capabilities accessing content networks time varying bandwidth latency. thus necessary coding transmission systems provide scalable bitstream allows decoding variety rates lower rate information streams embedded within higher rate bitstreams manner minimizes redundancy. need generate layered bitstreams wherein base layer provides coarse quality reconstruction successive layers reﬁne quality incrementally. scalable coding quality levels transmits rate decoders rate second decoder. commonly employed technique scalable coding completely neglects hidden states employs predictive coding approach dpcm base layer enhancement layer base layer reconstruction error compressed transmitted. base layer reconstruction used estimate original signal estimation error compressed enhancement layer. review scalable coding techniques audio video signals found estimation theoretically optimal scalable predictive coding technique proposed accounts information available current base layer abstract—the paper studies optimal coding hidden markov sources represent broad class practical sources obtained noisy acquisition processes beside explicit modeling speech processing recognition image understanding sensor networks. fundamental source coding approach proposed based tracking estimate state probability distribution shown optimal. practical encoder decoder schemes leverage main concepts introduced. iterative approach developed optimizing system. also focuses signiﬁcant extension optimal quantization paradigm. proposes approach scalable coding accounts available information coding given layer. simulation results conﬁrm approaches signiﬁcantly reduce reconstructed distortion substantially outperform existing techniques. ﬁnite state markov chain observed memoryless channel. random process consisting sequence observations referred hidden markov source markov chains common models information sources memory memoryless channel among simplest communication models. hmms widely used image understanding speech recognition source coding communications information theory economics robotics computer vision several disciplines. note signals modeled markov process fact captured imperfect sensors hence contaminated noise i.e. resulting sequence hms. motivated modeling capability practical sources memory consider optimal quantization hms. conventional approach design quantizers employ predictive coding techniques exploit time correlations followed standard scalar quantization residual. however direct prediction method cannot fully exploit structure source process i.e. underlying markov process. indeed even underlying process ﬁrst order markov markov prior samples needed optimally predict current sample. naive inﬁnite order prediction clearly impractical complexity. alternative predictive coding encoding ﬁnite-state quantizer ﬁnite-state quantizer ﬁnitestate machine used data compression successive source sample quantized using quantizer code book depends encoder state. current encoder state codeword obtained quantization determine next encoder state i.e. next quantizer codebook. authors optimized ﬁnite-state quantizer given past reconstructed samples generate optimal estimate enhancement layer. another work proposed novel scalable coding technique accounts available information coding given layer. base layer exploit available information employing previously proposed technique tracking state probability distribution encoder decoder using update quantizers encoding current observation. enhancement layer track state probability distribution encoder decoder using higher quality enhancement layer reconstruction better estimate enhancement layer quantizer adapted interval determined base layer quantization enable full exploitation available information. multimedia transmission networks enables wide range applications online radio highdeﬁnition teleconferencing. applications often suffering problem unreliable networking conditions leads loss data. packet loss concealment important tool solve issue. packet loss concealment objective exploit available information approximate lost packet maintaining smooth transition neighboring packets. paper propose approach state probability distribution tracking unit considers scenarios either packet loss based updates probability distribution design parameters. important feature scalable coding base layer decoded independently enhancement layers allows enhancement layer coder potential access information future base layer reconstruction given coding latency relative base layer. paper also consider means exploit future information addition current base layer prior enhancement layer information scheme complements scalable framework reﬁne enhancement layer probability distribution thereby achieve considerable performance gains non-delayed approach. finally tackle problem designing coding approach data paper. linear predictive coding tool used mostly speech processing representing spectral envelope digital signal speech compressed form. useful methods encoding good quality speech rate provides extremely accurate estimates speech parameters. transmission ﬁlter coefﬁcients directly undesirable since sensitive errors. line spectral frequencies used represent transmission channel. paper design best coding approach coefﬁcients. rest paper organized follows section overview background information. section proposed methods described. experimental results presented section concluding remarks section hidden markov model drives name deﬁning properties first assumes observation time generated based state hidden observer. secondly state sequences related markov process current state’s dependence past completely determined immediately preceding state. hmms become popular various important applications. hmms widely used image understanding speech recognition source coding communications information theory economics robotics computer vision several disciplines. note signals modeled markov processes usually captured imperfect sensors hence contaminated noise i.e. resulting sequence fact hms. special case broader family multivariate markov sources focus recent research notably context parameter estimation observation probability density function state continuous case observation probability mass function {bj} discrete case. denotes source output time initial state distributions {πi} important property state time captures past information relevant emission next source symbol. speciﬁcally implies observations time provide additional information next source symbol beyond obtained state markov chain time note know certainty state hms. based fact fundamental paradigm achieving optimal coding track state probability distribution hms. approach output encoder sent unit called state probability distribution tracking function. unit estimates state probability distribution i.e. probabilities markov chain states denoted probabilities used encoder adapter unit redesign encoder optimally next input sample. fig. shows fundamental approach optimal encoder. objective design encoder state probability distribution tracking function encoder adapter given training samples minimize average reconstruction distortion given encoding rate. ﬁrst describe tracking state probability distribution describe fundamental design approach proceed practical encoder design approach. finally describe approach lossy network condition. solution first problem evaluation problem want calculate probability observation given model i.e. efﬁcient solve problem using so-called forward backward procedure deﬁne forward variable fig. depicts notion. approach output encoder sent state probability distribution tracking function. unit estimates state probability distribution i.e. probabilities used encoder adapter unit design quantizer source output ground finally encoder encode symbols. fundamental framework requires encoder redesigned observed sample entails high complexity. therefore propose practical approximation complexity memory requirement restrict encoder adapter unit select codebook predesigned codebooks initialization. facilitate operation introduce module called ‘next quantizer selector’ decides codebook used initialization codebooks next sample based output state probability distribution tracking function. fig. shows proposed practical encoding scheme. process encoding decoding given closest representative finally using codebook closest representative initialization iteration lloyd’s algorithm codebook used current sample. audio video transmission networks enables wide range applications multimedia streaming online radio high-deﬁnition teleconferencing. applications often plagued problem unreliable networking conditions leads intermittent loss data. problem severe packet loss rate internet communications example reach clearly robustness packet loss crucial requirement. packet loss concealment forms crucial tool amongst various strategies used mitigate issue. packet loss concealment objective exploit available information shortcoming tracking state probability distribution based original source symbols decoder cannot exactly mimic encoder. avoid problem state probability distribution tracking unit must track states probability distribution based output encoder. enhancement layers additional information quantization interval lower layers along state probability distribution. thus enhancement layer encoder adapter unit combines types available information redesign encoder optimally next input sample. fig. shows proposed scalable encoder hms. objective design encoder adapter enhancement layes given training samples minimize average reconstruction distortion given encoding rates enhancements layers. ploss probability network’s packet loss reaches decoder. another words probability ploss encoded packet reach decoder gets lost. decoder calculation since decoder probability ploss would receive packet calculates state probability ˆpj)no−loss probability ploss would miss packet calculates state probability ˆpj)with−lossthe encoder’s best estimate decoder’s probability distribution would decoder side depending whether packet loss time decoder chooses scenarios calculate probability distribution time packet loss time decoder usual ﬁnds correct codebooks decodes received symbol. however packet loss time decoder conceals substituting decoded symbol weighted average class representatives weights given probability distribution states time note uniform codebook initialization enhancement layer quantizer design observation within quantization interval base layer lesser variations closer uniform distribution. however assumption true base layer. using uniform codebook initialization also reduces memory requirements encoder decoder. ﬁrst symbol assume source symbol ﬁxed state encoder decoder combination quantization interval information base layer. note generalize proposed approach number enhancement layers combining reﬁned estimate state probability distribution based observation reconstruction given layer quantization interval information lower layers. expected value using subsource statistics stationary probability state minimization possible quantizers. note bound optimistic pretends markov chain state time known exactly selecting next quantizer. ﬁrst experiment gaussian subsources mean variance mean variance moreover depicted fig. simulation coding rate varies bits symbol. compare proposed method ﬁnite state quantizer well dpcm. result shown fig. evident proposed method offers gains approximately ﬁnite state quantizer dpcm. comparison theoretical lower bound shows method leaves little room improvement performs close bound. note distortion bound makes optimistic assumption state time known used obtain best encoder time obviously real system access source state time. achievable lower bound proposed system performs fairly close bound. enhancement layer state probability distribution track similar base layer using enhanceing unit ﬁnds ment layer reconstructed observation samples till time well base layer reconstructed observation samples till time assume encoder arbitrarily uses last quantizer processes ﬁrst source symbol. rest symbols encoder ﬁrst ﬁnds based selects suitable quantizer. exact procedure employed decoder. method shows consistent gains competitors somewhat larger gains values depicted fig. note lower bound introduced earlier assumes know state time exactly. however it’s bold assumption makes looser lower bound. fair tighter lower bound could assuming know actual quantized data till time based calculate lower bound. fig. shows comparison proposed approach lower bound. could higher rate proposed approach almost identical lower bound. since real design decoder experiment gaussian subsources mean variance mean variance moreover experiment different cases case transition probability case transition probability linear predictive coding tool used mostly audio signal processing speech processing representing spectral envelope digital signal speech compressed form using information linear predictive model. useful methods encoding good quality speech rate provides extremely accurate estimates speech parameters. since frequently used transmitting spectral envelope information tolerant transmission errors. transmission ﬁlter coefﬁcients directly undesirable since sensitive errors. words small error distort whole spectrum worse small error might make prediction ﬁlter unstable. last experiment used real speech dataset. want design best coding approach coefﬁcients. using approaches explained section ﬁrst best guassian emittion modeling dataset. next using approaches explained section design best codebooks well coding approaches. layer rate bits enhancement layer rate varies bits. compare proposed method prior approach assumes simple markov model uses dpcm base layer employs quantizer designed lloyd’s algorithm encoding base layer reconstruction error enhancement layer. also compare results theoretical lower bound using switched scalar quantizer operating total rate bits. rate distortion plots different approaches lower bound shown fig. superiority proposed method evident plots substantial gains around competition. note around lower bound scalable coding penalty hierarchical structure employed source distortion pair successively reﬁnable. second experiment source used varying transition probability ﬁxed coding rate bits base enhancement layer. results experiment shown fig. demonstrate large gains decrease marginally values note calculating base enhancement layers encoder decoder impose signiﬁcant computational burden forward variables easily updated recursively sample gaussian subsources mean variance mean variance transition probabilities scalable coder base layer rate bits enhancement layer rate varies bits. i.e. using future base layer sample. measure additional gains proposed method access future base layer reconstructed sample. depicted fig. average method obtains additional gain exploiting available future base layer reconstructed samples. fig. rate-distortion plots proposed method enhancement layer access future base layer reconstructions enhancement layer without access future base layer reconstructions base layer rate enhancement layer rate ranging bits sample. fundamental source coding approach hidden markov sources proposed based tracking state probability distribution shown optimal. practical encoder decoder schemes leverage main concepts introduced consist three modules switchable quantizer codebooks state probability distribution tracking function next quantizer selector. iterative approach developed optimizing system given training set. state probability distribution tracking module estimates time instance probability source possible state given quantized observations next quantizer selector determines codebook used given state probability distribution receives input. decoder mimics encoder switches codebook. switching decisions made optimally accounting available information past obtained efﬁciently recursive computation forward variables. also proposed novel technique scalable coding hidden markov sources utilizes available information coding given layer. enhancement layer state probability distribution reﬁned sample using past enhancement layer reconstructed observations. information combined quantization interval available base layer quantizer current sample updated accordingly. decoder mimics rabiner levinson sondhi application vector quantization hidden markov models speaker-independent isolated word recognition bell system technical journal vol. available http//dx.doi.org/./j.-..tb.x juang levinson sonphi maximum likelihood estimation multivariate mixture observations markov chains. ieee transactions information theory vol. march juang rabiner segmental k-means algorithm estimating parameters hidden markov models ieee trans. accoust. speech signal processing vol. mehdi salehifar mehdi salehifar received b.sc degree electrical computer engineering university tehrantehraniran m.sc. ph.d. degrees electrical computer engineering university california santa barbara respectively. currently working electronics senior research engineer. research interests include signal processing general quantization theory information theory video/audio coding. worked electronics senior video researcher intern well qualcomm audio researcher. winner several titles national international mathematic competitions. tejaswi nanjundaswamy tejaswi nanjundaswamy received degree electronics communications engineering national institute technology karnataka india m.s. ph.d. degrees electrical computer engineering university california santa barbara respectively. currently post-doctoral researcher signal compression ucsb focuses audio/video compression processing related technologies. worked ittiam systems bangalore india senior engineer audio codecs effects development. also interned multimedia codecs division texas instruments india .dr. nanjundaswamy associate member audio engineering society student technical paper award convention. kenneth rose kenneth rose received ph.d. degree california institute technology pasadena joined department electrical computer engineering university california santa barbara currently professor. main research activities areas information theory signal processing include rate-distortion theory source source-channel coding audiovideo coding networking pattern recognition non-convex optimization. interested relations information theory estimation theory statistical physics potential impact fundamental practical problems diverse disciplines.prof. rose corecipient william bennett prize paper award ieee communications society well ieee signal processing society best paper awards.", "year": "2018"}