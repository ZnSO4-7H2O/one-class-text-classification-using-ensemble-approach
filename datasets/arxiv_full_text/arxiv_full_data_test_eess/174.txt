{"title": "Tensors, Learning, and 'Kolmogorov Extension' for Finite-alphabet Random  Vectors", "tag": "eess", "abstract": " Estimating the joint probability mass function (PMF) of a set of random variables lies at the heart of statistical learning and signal processing. Without structural assumptions, such as modeling the variables as a Markov chain, tree, or other graphical model, joint PMF estimation is often considered mission impossible - the number of unknowns grows exponentially with the number of variables. But who gives us the structural model? Is there a generic, `non-parametric' way to control joint PMF complexity without relying on a priori structural assumptions regarding the underlying probability model? Is it possible to discover the operational structure without biasing the analysis up front? What if we only observe random subsets of the variables, can we still reliably estimate the joint PMF of all? This paper shows, perhaps surprisingly, that if the joint PMF of any three variables can be estimated, then the joint PMF of all the variables can be provably recovered under relatively mild conditions. The result is reminiscent of Kolmogorov's extension theorem - consistent specification of lower-dimensional distributions induces a unique probability measure for the entire process. The difference is that for processes of limited complexity (rank of the high-dimensional PMF) it is possible to obtain complete characterization from only three-dimensional distributions. In fact not all three-dimensional PMFs are needed; and under more stringent conditions even two-dimensional will do. Exploiting multilinear algebra, this paper proves that such higher-dimensional PMF completion can be guaranteed - several pertinent identifiability results are derived. It also provides a practical and efficient algorithm to carry out the recovery task. Judiciously designed simulations and real-data experiments on movie recommendation and data classification are presented to showcase the effectiveness of the approach. ", "text": "known would straightforward task. classical example recommender systems users rate small fraction total items objective make item recommendations users according predicted ratings. joint item ratings known recommendation readily implementable based conditional expectation mode unobserved ratings given observed ratings. closely related problem top-k recommendation goal predict items user likely next. joint items known easy identify items highest individual joint conditional probability given observed user ratings. another example data classiﬁcation. joint features label known given test sample easy infer label according maximum posteriori principle. fact joint used infer features useful imputing incomplete information surveys databases. despite importance signal data analytics estimating joint often considered mission impossible general structure relationship variables assumed. true even problem size merely moderate. reason number unknown parameters exponential number variables. consider simple scenario variables taking distinct values each. number parameters need estimate case ‘naive’ approach joint estimation counting occurences joint variable realizations. practice however dealing even moderately large sets random variables probability encountering particular realization low. therefore small portion empirical distribution non-zero given reasonable amount data samples makes approach inaccurate. many applications different workarounds proposed circumvent sample complexity problem. example recommender systems instead trying estimate joint ratings popular approach based low-rank matrix completion idea users roughly clustered several types users type would rate different movies similarly. consequently user-rating matrix approximately rank used prior information infer missing ratings. classiﬁcation parsimonious function approximations employed model relationship random variables lies statistical learning signal processing. without structural assumptions modeling variables markov chain tree graphical model joint estimation often considered mission impossible number unknowns grows exponentially number variables. gives structural model? generic ‘non-parametric’ control joint complexity without relying priori structural assumptions regarding underlying probability model? possible discover operational structure without biasing analysis front? observe random subsets variables still reliably estimate joint all? paper shows perhaps surprisingly joint three variables estimated joint variables provably recovered relatively mild conditions. result reminiscent kolmogorov’s extension theorem consistent speciﬁcation lower-dimensional distributions induces unique probability measure entire process. difference processes limited complexity possible obtain complete characterization threedimensional distributions. fact three-dimensional pmfs needed; stringent conditions even twodimensional exploiting multilinear algebra paper proves higher-dimensional completion guaranteed several pertinent identiﬁability results derived. also provides practical efﬁcient algorithm carry recovery task. judiciously designed simulations real-data experiments movie recommendation data classiﬁcation presented showcase effectiveness approach. index terms—statistical learning joint estimation tensor decomposition rank elementary probability kolmogorov extension recommender systems classiﬁcation estimating joint probability mass function random variables great interest numerous applications ﬁelds machine learning data mining signal processing. many cases given partial observations and/or statistics data incomplete data marginalized lower-dimensional distributions lowerorder moments data goal estimate missing data. full joint variables interest original manuscript submitted ieee trans. signal processing november revised april accepted june supported part iis- iis-. conference version part work appeared information theory applications workshop kargas dept. univ. minnesota minneapolis sidiropoulos dept. univ. virginia charlottesville school oregon state university corvallis author e-mails kargaumn.edu nikosvirginia.edu xiao.fuoregonstate.edu variables ‘reasonably dependent’. makes sense estimation problems involving fully independent fully dependent regressors unknowns contrived middle ground interesting. also important note marginal pmfs triples reliably estimated smaller sample complexity joint variables. example user-movie ratings marginal three given variables estimated counting co-occurrences given ratings three given movies; user rate movies. contributions speciﬁc contributions follows propose novel framework joint estimation given limited possibly incomplete data samples. method based nice delicate connection canonical polyadic decomposition naive bayes model. model sometimes referred parallel factor analysis model popular analytical tool multiway linear algebra. model used model analyze tensor data signal processing machine learning found many successful applications speech separation blind cdma detection array processing spectrum sensing unmixing cognitive radio topic modeling community detection recent overview paper nevertheless never considered statistical learning tool recovering general joint work ﬁrst establish exciting connection present detailed identiﬁability analysis proposed approach. ﬁrst show that joint represented naive bayes model ﬁnite-alphabet latent variable size latent alphabet bounded function alphabet sizes observed variables. show that latent alphabet size certain threshold joint arbitrary number random variables identiﬁed three-dimensional marginal distributions. prove identiﬁability result relating joint marginal pmfs model known uniqueness even tensor rank much larger outer dimensions. addition novel formulation identiﬁability results also propose easily implementable joint recovery algorithm. identiﬁcation criterion considered coupled simplex-constrained tensor factorization problem propose efﬁcient alternating optimization-based algorithm handle deal probability simplex constraints arise estimation celebrated alternating direction method multipliers algorithm employed resulting lightweight iterations. judiciously designed simulations real experiments there works considered using model joint speciﬁc problems however works rely speciﬁc physical interpretation associated model sharply different setup employ model explain general joint without assuming physical model. fig. applications joint estimation. recommender systems given partially observed ratings user movies would like infer unobserved ratings. bottom classiﬁcation problems given medical features people would like infer person heart disease. probability function) features label. successful methods fall category support vector machines logistic regression recently kernels neural networks mentioned methods nice elegant triggered tremendous amount theoretical research practical applications successful many ways. however workarounds answered question interest ever reliably estimate joint variables given limited data? question well-motivated practice since knowledge joint indeed gold standard enables optimal estimation variety well-established criteria mean-square error minimum probability error bayes risk. knowing joint facilitate large variety applications including recommender systems classiﬁcation uniﬁed statistically optimal instead resorting often adhoc modeling tools. paper shows perhaps surprisingly joint three variables estimated joint variables provably recovered relatively mild conditions. result reminiscent kolmogorov’s extension theorem consistent speciﬁcation lower-dimensional distributions induces unique probability measure entire process. difference processes limited complexity possible obtain complete characterization three-dimensional distributions. fact threedimensional pmfs needed; stringent conditions even two-dimensional rank condition high-dimensional joint interesting random interpretation reader wonder consider recovery three-dimensional joint pmfs onetwo-dimensional pmfs. well-known recovery one-dimensional marginal pmfs possible random variables known independent. case joint equal product individual one-dimensional marginals. interestingly recovery one-dimensional marginals also possible random variables known fully dependent i.e. completely determined other. case joint recovered one-dimensional marginal unique permutation other. however complete dependence unrealistic statistical estimation learning practice. general possible recover joint one-dimensional marginals. illustration variables shown figure represented matrix ‘projections’ matrix along column directions using projector respectively case denote matrix rank independent. basic linear algebra knowing enough recovering general since equivalent solving underdetermined system linear equations variables equations. know two-dimensional marginals? given random variables obey probabilistic graphical model genie reveals model estimating highdimensional joint two-dimensional marginals possible. example shown figure know priori random variables conditionally independent given verify knowledge sufﬁcient recover however kind approach hinges knowing probabilistic graph structure. unfortunately genies hard come real life learning graph structure data challenging problem statistical learning preliminary version part work appeared journal version includes stronger identiﬁability theorems interpretations detailed analysis theorems insightful experiments number real datasets. bold lowercase uppercase letters denote vectors matrices respectively. bold underlined uppercase letters denote n-way tensors. uppercase letters denote scalar random variables outer product vectors n-way tensor elements aa··· kronecker product matrices denoted khatri-rao product matrices denoted hadamard product matrices denoted deﬁne vector obtained vertically stacking elements tensor vector. additionally diag ri×i denotes diagonal matrix elements vector diagonal. integers denoted denotes cardinality assume take discrete values joint pmfs variable triples i.e. i)’s available. identify joint {xn}n i.e. threedimensional marginals? question lies heart statistical learning. this consider classiﬁcation problem represent observed features sought label. known given speciﬁc realization features easily compute posterior probability certain employed ‘absorb’ norms columns. illustration threeway tensor shown figure cases denote n-way tensor again expression automatically assumed certain refer decomposition nonnegative factors rin×f framework heavily based low-rank tensor factorization nice identiﬁability properties. facilitate later discussion brieﬂy introduce pertinent aspects tensors section. n-way tensor ri×i×···×in data array whose elements indexed indices. two-way tensor matrix whose elements indices; i.e. denotes element matrix matrix rank denotes outer product vectors i.e. similarly n-way tensor rank admits following rank decomposition distinctive feature tensors essentially unique mild conditions even much larger continue discussion ﬁrst formally deﬁne mean essential uniqueness rank decomposition tensors. deﬁnition tensor rank nonnegative decomposition essentially unique factors unique common permutation. means exists another nonnegative decompo∈ rin×f smallest number decomposition exists. convenience notation denote decomposition. rank decomposition also called canonical polyadic decomposition parallel factor analysis note model tensor. every tensor admits rank necessarily smaller latter sharp contrast matrix case matrix case similarly n-way tensor sometimes wishes restrict columns an’s unit norm therefore tensors represented words tensor essentially unique nonnegative ambiguity column permutation column-normalized factors {an}n simply amounts permutation rank-one ‘chicken feet’ outer products fig. clearly approximation might even exist case; fortunately adding structural constraints latent factors mitigate this work interest lies revealing fundamental limits joint estimation. therefore analysis leveraging exact decomposition results e.g. lemmas however since formulated problem naturally involves nonnegative latent factors computational framework utilizes structural prior knowledge enhance performance practice. show joint admits naive bayes model representation i.e. generated latent variable model hidden variable. naive bayes model postulates hidden discrete random variable taking possible values given discrete random variables {xn}n conditionally independent. follows joint {xn}n decomposed prior distribution latent variable conditional distributions naive bayes model also referred latent class model simplest form bayesian network employed diverse applications classiﬁcation density estimation crowdsourcing name few. interesting observation naive bayes model interpreted special nonnegative polyadic decomposition. alluded exploited identifying joint lower-dimensional marginals consider element-wise representation compare column factor matrices represent conditional vector contains prior probabilities latent variable i.e. special nonnegative polyadic decomposition model restricts subtle point however maximal rank model bounded number latent states naive bayes model exceed bound. even number latent states maximal rank bound naive bayes model reducible sense exists naive bayes model fewer latent states generates joint pmf. result every joint admits naive bayes model interpretation bounded every naive bayes model reduced special model. following result. unavoidable. regarding essential uniqueness tensors consider three-way case ﬁrst. following arguably well-known uniqueness condition revealed kruskal lemma ri×f ri×f ri×f rank decomposition essentially unique. matrix equal largest integer every subset columns linearly independent. lemma implies following generic result decomposition essentially unique almost surely probability elements generated following certain absolutely continuous distribution. relaxed powerful uniqueness conditions proven recent years. lemma ri×f ri×f ri×f then rank decomposition essentially unique almost surely lemma ri×f ri×f ri×f integers largest α+β− decomposition essentially unique almost surely. condition also implies unique decomposition almost surely. many different uniqueness conditions cpd. take-home point model essentially generically unique even much larger long less maximal possible rank. example lemma large model still unique. remark mention identiﬁability results derived tensors noiseless setup. addition although results stated real factor matrices general also cover nonnegative an’s fact nonnegative orthant positive measure. follows tensor generated using random nonnegative factor matrices noiseless setup plain recover true nonnegative factors. hand practice instead considering exact tensor decomposition often low-rank tensor approximation interest limited sample size factors. best low-rank tensor generally also column scaling counter-scaling ambiguity column multiplied corresponding yellow column divided without change outer product. scaling ambiguity nonnegative column-normalized representation obviously sign ambiguity scaling estimation however cases hidden variable speciﬁc physical meaning thus connection established using speciﬁc data generative model. here emphasize even physically meaningful presumed generative model always represent arbitrary joint possibly corresponding complicated probabilistic graphical model simple naive bayes model bounded number latent states result signiﬁcant also spells latent structure probabilistic graphical model cannot identiﬁed simply assuming hidden nodes; limit number hidden node states well. remark although joint admits naive bayes representation mean representation unique. clearly needs strictly smaller upper bound guarantee uniqueness fortunately many joint pmfs encounter practice relatively low-rank tensors since random variables real world moderately dependent. leads interesting connection linear dependence/independence statistical dependence/independence. explain consider simplest case case two-way model corresponds nonnegative matrix factorization related probabilistic latent semantic indexing two-way model independence variables implies probability matrix rank-. hand variables ri×ii used matlab notation denote frontal slabs tensor additionally iin×in denotes identity matrix size vector size every frontal slab tensor synthesized adiag)at upon normalizing columns matrix columns factor role symmetry permuting modes tensor follows need columns factor exact decomposition. easily generalized four-way tensor ri×i×i×i noticing slab three-way tensor thus decomposed {an}n deﬁned satisfy based connection naive bayes model lower-dimensional marginals joint propose following steps recover complete joint three-dimensional marginals. procedure joint recovery triples estimate data; jointly factor xjkl estimate using model rank synthesize joint step individual factorization least xjkl unique joint readily identiﬁable already interesting. however show sec. identify joint even marginal tensors unique cpd. reason many marginal tensors share factors exploit come much stronger identiﬁability results. discuss theoretical results identiﬁability joint using three higher-dimensional marginals ﬁrst propose implementation proposed procedure. brevity assume estimates threedimensional marginal distributions i.e. given empirical lowdimensional marginal distributions. assumption low-rank model every empirical marginal distribution three random variables approximated follows optimization problem instance coupled tensor factorization. coupled tensor/matrix factorization usually used combining various datasets share dimensions corresponding factor matrices notice case estimates two-dimensional marginals optimization problem corresponds coupled matrix factorization. optimization fully dependent i.e. value variable exactly determines value other probability matrix fullrank. however low-rank necessarily mean variables close independent shown figure there rank probability matrix also model highly dependent random variables. practice expect random variables neither independent fully dependent interested cases rank joint lower upper bound given proposition sanity check conducted preliminary experiments real-life data. anticipated veriﬁed many joint pmfs indeed low-rank tensors practice. table shows interesting results joint three movies rating values ﬁrst estimated using data movielens project. joint factored using nonnegative model different rank values. rank modeling error terms relative observation enables approach marginal distribution subset random variables also nonnegative model. direct consequence total probability. marginalizing respect k-th random variable handling large-scale tensor decomposition details admm algorithm solving problems found appendix whole procedure listed algorithm mentioned algorithm easily modiﬁed cover cases higher-dimensional marginals pairwise marginals given thus cases omitted. section study conditions identify marginalized lower-dimensional distributions. brevity focus three-dimensional lower-dimensional distributions even though many results possible concentrate case ease exposition manuscript length considerations. similar type analysis applies different however analysis customized properly address particular cases. convey spirit possible terms identiﬁability results cannot provide exhaustive treatment individually identiﬁable combination then identiﬁable. means given three-dimensional marginal distributions generically identiﬁable assuming readily shown invoking lemma equation link naive bayes model tensor factorization discussed sec. note already condition since many cases approximately low-rank tensors practice. however since many factor-coupled xjkl’s identiﬁability condition signiﬁcantly improved. following theorems. theorem assume drawn absolutely continuous distribution joint represented using naive bayes model rank then almost surely identiﬁable pr’s theorem assume drawn absolutely continuous distribution joint represented using naive bayes model rank largest then a.s. integer identiﬁable pr’s problem challenging deserves developing sophisticated algorithms handling ﬁrst number random variables gets large large number optimization variables determined matrix large. addition probability simplex constraints impose extra computational burden. nevertheless found that carefully re-arranging terms formulated problem recast reminiscent classical alternating least squares algorithm constraints. least-squares problem respect matrix probability simplex constraints columns. optimization problem form factor role symmetry. order update solve following optimization problem problems linearly constrained quadratic programs solved optimality many standard solvers. here propose employ alternating direction method multipliers solve sub-problems ﬂexibility effectiveness rank bounds theorems nontrivial albeit maximal attainable rank cases considered. recalling higher-order tensors identiﬁable higher ranks natural question whether knowledge fourhigher-dimensional marginals enhance identiﬁability complete joint pmf. next theorem shows answer afﬁrmative. theorem assume drawn absolutely continuous distribution joint represented using naive bayes model rank assume partitioned disjoint subsets denoted four-dimensional marginals available. then joint a.s. identiﬁable conditions theorem satisﬁed much higher rank theorems shown tables ii-iii. results related four-dimensional marginals obtained following theorem checking possible partitions. caveat need many samples reliably estimate four-dimensional marginals. nevertheless theorems present section offer insights regarding choice lower-dimensional marginals work choice depends size alphabet variable number variables well amount available data samples. remark results rely lemmas concern identiﬁability generic choice parameters; i.e. parameters assumed drawn randomly jointly continuous distribution. point wonder whether realistic assumption practice. example latent model identiﬁcation problems hidden variable speciﬁc physical meaning observed variable depend state hidden variable values. consider hidden markov model denote observed variable time hidden state conditional equal thereby rendering deterministic identiﬁability condition useless. note however setting latent variable necessarily physical interpretation; convenient ‘universal’ parametrization joint pmf. therefore conditional distribution observed variable values hidden state still depend value ‘virtual’ global latent variable hence recovery joint using lower-dimensional marginals could still possible. section employ judiciously designed synthetic data simulations showcase effectiveness proposed joint recovery methods. also apply approach real-data problems classiﬁcation recommender systems demonstrate usefulness real machine learning tasks. ﬁrst evaluate proposed approach using synthetic data. consider case random variables present variable take discrete values. assume joint random variables represented naive bayes model whose latent variable take values generate matrices rin×f model conditional probabilities i.e. vector also generated latent random variable elements vector drawn independently uniform distribution zero column normalized ground-truth joint constructed following naive bayes model i.e. assume observable data two- threefour-dimensional marginals joint pmf. settings verify proposed procedure algorithm effectively recover joint modeling error joint rank. monte carlo simulations compute noiseless marginal distributions proposed approach indeed recover joint pmf. practice usually exact estimates lower-dimensional marginal distributions. next provide realistic simulations estimate marginal pmfs using sample averages observed data. fully-observed data follow generating ground-truth joint previous simulation. then drawing joint generate synthetic dataset ﬁve-dimensional data points. data generated follows data point ﬁrst draw sample according i.e. realization hidden variable data point sm]t generated drawing elements independently i.e. drawn {an}. {an}n equivalent synthesizing ﬁve-way joint tensor drawing outcome generated -dimensional data points estimate lower-dimensional marginals admm algorithm recover full joint pmf. repeat total monte carlo simulations. figure shows tensor mean relative error estimated joint different dataset sizes also include performance additional methods estimating joint pmf. given full data together ‘oracle’ observations hidden variable perform maximum likelihood estimation naive bayes parameters denoted oracle done simply frequency counting table shows mean relative errors estimating conditional pmfs joint using different types input different choices rank. consistent analysis using marginal distributions triples quadruples random variables able recover joint random variables. recovery high accuracy demonstrated; exact recovery also possible certain cases however using pairs promising. recall identiﬁability result built upon identiﬁability thirdhigher-order models. nice identiﬁability results general hold matrices explains sharp performance difference using pairs higher-dimensional marginals. pairs work however conditional probability matrices sufﬁciently sparse stringent constraints rank defer detailed discussion follow paper lack space one. real applications ground-truth joint conditional pmfs known. nevertheless evaluate method variety standard machine learning tasks observe effectiveness. subsection test proposed approach different tasks namely data classiﬁcation recommender systems. note tasks easily accomplished joint pertinent variables known thus suitable evaluating method. note rank joint tensor cannot known simulations. fortunately single discrete variable easily tuned e.g. observing validation errors machine learning. classiﬁcation task evaluate performance approach different datasets machine learning repository five selected datasets correspond binary classiﬁcation multi-class classiﬁcation. dataset represent training samples using discrete features pmf-based approach applied. split dataset data samples used training used validation testing. dataset label selected features. estimate lower-dimensional marginal distributions pairs triples quadruples variables using samples training set. then marginals estimate applying proposed approach estimating joint predict data point test corresponding label using rule. estimator label m-th observation sm]t test written count number times appear together dataset count number times takes value also include non-parametric approach empirical dimensional distribution estimate i.e. estimation performance method similar different rank values approaches oracle using threefour-dimensional marginals. addition expected recovery accuracy steadily improves size available dataset increases. hand using two-dimensional marginals performance improves reaches plateau approximately ability recover joint using pairs random variables obviously limited identiﬁability matrix factorization. missing data repeat experiment dataset entries missing. randomly hide data compute estimates twothreefour-dimensional marginals using available data. admm-based algorithm repeat monte carlo simulations. estimation performance method similar different rank values approaches oracle mle. expected observe slight decrease performance less accurate estimation lower-dimensional marginals. case nonparametric method takes account fully observed samples. lowerdimensional distributions used number samples limited three-dimensional distributions give lower relative error compared four-dimensional ones. shows cases using lower-dimensional distributions beneﬁcial higher-order ones terms parameter estimation accuracy. actually surprising since shown empirical lower-dimensional appropriate rank model. therefore dataset models different rank values choose minimizes misclassiﬁcation error validation standard machine learning practice. different classical classiﬁers matlab statistics machine learning toolbox baselines; linear kernel radial basis function naive bayes classiﬁer. classiﬁers original data encoding well one-hot encoding usually suitable discrete data report best result among two. note baseline naive bayes approach different ours baseline method assumes features independent given label assume label features independent given unknown latent variable. former strong assumption rarely satisﬁed real data assumption holds arbitrary random variables provided large enough showed proposition table shows classiﬁcation errors obtained datasets. approach outperforms naive bayes classiﬁer assumes features independent given label. several observations order. first using higher-dimensional marginals proposed approach gives better classiﬁcation results compared using lower-dimensional ones. consistent analysis sec. higher-dimensional marginals lead stronger overall identiﬁability joint pmf. datasets test using four-dimensional marginal distributions gives best classiﬁcation accuracy compared threetwo-dimensional ones. second binary classiﬁcation experiments proposed method works better comparable baselines. quite surprising since method directly optimize classiﬁcation criterion does. result suggests proposed method indeed captures essence joint distribution recovered joint utilized make inference practice. third multiclass datasets proposed method yields accuracy less methods. also makes sense value whose cardinality grows joint pmfs require samples estimate accurately. also shows interesting sample complexity-accuracy trade-off proposed method. nevertheless method still works comparably well linear supports usefulness joint estimation method. recommender systems also evaluate method task recommender systems using movielens dataset movielens dataset contains ratings -star scale half-star increments number users. order test algorithm select three different subsets full dataset round ratings next integer. initially three different categories selected. category extract user-by-rating submatrix keeping rated movies form datasets experiments. note constructed three datasets many missing values since users watched rated movies. task recommender systems recommend unwatched movies users based prediction user’s rating given available data. estimating joint movie ratings. case random variable represents movie takes values i.e. ratings. consequently joint twenty-way tensor elements. partially observed datasets used order estimate lower-dimensional marginal distributions pairs triples quadruples variables estimated compute expected value users’ ratings observe given ones observe. speciﬁcally ratings m-th user i.e. user missing rating. conditional expectation movie’s rating given baseline algorithm biased matrix factorization method commonly used method recommender systems. method essentially lowrank matrix completion modiﬁcations. additionally present results obtained global average ratings user average item average baselines predicting missing entries. dataset randomly hide ratings test ratings validation remaining dataset used training set. monte carlo simulations using approach algorithm. select parameters methods based rmse validation set. table shows performance algorithms terms rmse mean absolute error that three datasets test proposed method method output clearly lower rmses maes relative naive methods using averaging. addition proposed method slightly outperforms three datasets. note considered state-of-art method movie recommendation incorporates applicationspeciﬁc custom features user bias movie bias achieve good performance. proposed method hand uses basic probability handle task completely application-blind. suggests joint modeling proposed algorithm quite effective. last also observe accuracy improvement increase dimension marginal distributions used approach. again performance come identiﬁability gain analyzed theorem work taken fresh look fundamental problems statistical learning joint estimation. curse dimensionality naive countbased estimation mission impossible cases. popular approach historically assume plausible structural model markov chain tree probabilistic graphical model inference using model. showed different ‘non-parametric’ tensor-based approach possible features several beneﬁts. foremost among guaranteed identiﬁability high-dimensional joint low-dimensional marginals reliably estimated using counting much fewer examples even samples missing example. ability infer unique higherdimensional joint specifying lower-dimensional ones reminiscent kolmogorov extension intuitively pleasing. also proven results appear fundamental close heart probability learning every joint interpreted naive theory bayes model; probabilistic graphical models popular statistical computer science never identiﬁable simply limits number hidden nodes; needs bound number hidden states well. non-parametric approach reveal true hidden structure instead assuming alleviates risk up-front bias analysis. practical side approach appealing since lowerdimensional marginals reliably estimated limited amount partially observable data. also provided practical easily implementable algorithm based factor-coupled tensor factorization handle recovery problem. simulations judicious experiments real data shown performance proposed approach consistent analysis approaches exceeds state-of-art application-speciﬁc solutions come years intensive research satisfying. ri|s|×i|s|×i|s| used subset available information synthesize virtual single nonnegative model size i×i×i i|sk|. therefore apply identiﬁability results three-way tensors. observe sizes different modes virtual tensor depend partition variables. distinguish cases apply lemma partition variables three sets clearly according case partition variables three sets always equality satisﬁed unique decomposition min) koren bell volinsky matrix factorization techniques recommender systems computer vol. aug. jain netrapalli sanghavi low-rank matrix completion using alternating minimization proc. annu. symp. theory comput. carroll chang analysis individual differences multidimensional scaling n-way generalization eckart-young decomposition psychometrika vol. sep. harshman foundations parafac procedure models conditions explanatory multimodal factor analysis ucla working papers phonetics vol. nion mokios sidiropoulos potamianos batch adaptive parafac-based blind separation convolutive speech mixtures ieee trans. audio speech lang. process. vol. aug. sidiropoulos tranter w.-k. factor analysis framework power spectra separation multiple emitter localization ieee trans. signal process. vol. aug. sidiropoulos lathauwer huang papalexakis faloutsos tensor decomposition signal processing machine learning ieee trans. signal process. vol. jul. minors generic version lemma states decomposition essentially unique a.s. know khatri-rao product matrices full rank almost surely three-way tensor i|s| i|s| |s||s|. applying generic version lemma obtain desired result. projection operator onto convex denotes iteration index. various methods exist projecting onto probability simplex e.g. note order efﬁciently compute matrix property khatri-rao product; ak). efﬁcient algorithms also exist computation matrix matricized tensor times khatri-rao product terms similarly derive updates beutel talukdar kumar faloutsos papalexakis xing flexifact scalable ﬂexible factorization coupled tensors hadoop proc. siam int. conf. data mining jiang sidiropoulos kruskal’s permutation lemma identiﬁcation candecomp/parafac bilinear models constant modulus constraints ieee trans. signal process. vol. sep.", "year": "2017"}