{"title": "Efficient Neural Audio Synthesis", "tag": "eess", "abstract": " Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating high-quality samples. Efficient sampling for this class of models has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24kHz 16-bit audio 4x faster than real time on a GPU. Second, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds for sparsity levels beyond 96%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile CPU in real time. Finally, we propose a new generation scheme based on subscaling that folds a long sequence into a batch of shorter sequences and allows one to generate multiple samples at once. The Subscale WaveRNN produces 16 samples per step without loss of quality and offers an orthogonal method for increasing sampling efficiency. ", "text": "ages videos speech music models learn joint probability data factorizing distribution product conditional probabilities sample. structure lets models allot signiﬁcant capacity estimate conditional factor makes robust during training easy evaluate. ordering encoded structure also makes sampling process strictly serial sample generated samples depends produced accordance ordering. serial aspect sampling process make slow impractical models generate high-dimensional data like speech video. goal increase efﬁciency sampling sequential models without compromising quality. time sampling process takes product number samples target time required produce sample. latter decomposed computation time overhead layers model value grow prohibitively large following conditions large case high-ﬁdelity audio composed -bit samples second; large deep architecture wavenet large e.g. especially wide layers large number parameters; overhead high cost launching individual operation. focus text-to-speech synthesis propose methods make sampling orders magnitude faster. reduce contributions factors minimal loss quality generated output. benchmark models single-speaker north-american english text-to-speech dataset input composed predicted linguistic feature vectors sequential models achieve state-of-the-art results audio visual textual domains respect estimating data distribution generating high-quality samples. efﬁcient sampling class models however remained elusive problem. focus text-to-speech synthesis describe general techniques reducing sampling time maintaining high output quality. ﬁrst describe single-layer recurrent neural network wavernn dual softmax layer matches quality state-of-the-art wavenet model. compact form network makes possible generate -bit audio faster real time gpu. second apply weight pruning technique reduce number weights wavernn. that constant number parameters large sparse networks perform better small dense networks relationship holds sparsity levels beyond small number weights sparse wavernn makes possible sample high-ﬁdelity audio mobile real time. finally propose generation scheme based subscaling folds long sequence batch shorter sequences allows generate multiple samples once. subscale wavernn produces samples step without loss quality offers orthogonal method increasing sampling efﬁciency. output -bit waveform report negative log-likelihood reached model held-out data results comparison tests pair models rated human listeners mean opinion scores samples model. begin designing sequence model requires number operations sample. make core property recurrent neural networks single recurrent layer applied previous state deliver highly non-linear transformation context. wavernn model single-layer dual softmax layer designed efﬁciently predict -bit audio samples. wavernn units achieves scores comparable largest wavenet model signiﬁcant difference audio ﬁdelity according comparison test similarly high. wavernn achieves performance requiring matrix-vector products sequence -bit sample; simplicity exclude non-linearities minor operations count contrast wavenet residual blocks layers requiring series matrix-vector products. even overhead still represent signiﬁcant bottleneck regular implementation sampling wavernn. sidestep overhead implementing custom operations sampling process. allows wavernn generate -bit samples second nvidia corresponds real time high-ﬁdelity -bit audio. comparison best kernel wavenet model runs roughly real time platform. throughput increases batch kernels achieve samples second reducing number parameters network decreases amount computation required sampling. mind maximizing performance given amount parameters. also consider problem maximizing performance given compute budget solve approach based neuron pruning. sparsify weights wavernn using weight pruning techniques ﬁxed parameter count discover large sparse wavernns signiﬁcantly outperform small dense wavernns relationship holds high levels sparsity greater combination sparse wavernn’s high quality output small number parameters requirements memory bandwidth makes model well-suited efﬁcient implementations low-power mobile platforms figure architecture wavernn dual softmax layer. represents coarse sample represents sample. multiplication happens coarse bits simultaneously output gates evaluated coarse bits sampled. sampled gates evaluated bits sampled. implement benchmark sparse matrix-vector products nonlinearities used wavernn mobile even though amounts computation memory bandwidth respectively three orders magnitude smaller mobile benchmarks off-the-shelf mobile cpus indicate resources sufﬁcient real-time on-device audio synthesis high-quality sparse wavernn. knowledge ﬁrst sequential neural model capable real-time audio synthesis broad computing platforms including off-the-shelf mobile cpus. finally tackle contribution component equation multiple recent approaches goal making sampling sequential models parallel however models either make local independence assumptions generated samples undermining backbone sequential models require training multiple domain-speciﬁc networks specialized losses restrict overall usability models. propose generation process based subscaling. tensor scale folded sub-tensors scale l/b. sub-tensors generated order conditioned previous sub-tensors. subscaling lets generate multiple samples batch. since conditioning generation sub-tensor previous sub-tensors requires practice relatively small future horizon generation next sub-tensor start soon start generation previous sub-tensor. possible principle although necessary practice recover distant future past dependencies beyond horizon; precise cost batched sampling table results comparison tests given model wavernn-. test includes human ratings grades collapse counts different positive negative categories. distant dependencies samples current batch. subscale wavernn able produce samples step without loss audio ﬁdelity evidenced comparison tests batched sampling subscale wavernn opens many orthogonal ways increasing sampling efﬁciency. even regular tensorﬂow implementation model achieves real-time sampling speed nvidia gpu. fused variant subscale wavernn also gives sampling speed real time nvidia using slight modiﬁcation kernel wavernn-. convolutional sequence models achieve excellent performance speech synthesis architecture tends deep narrow requiring long chain layers executed sample. seek architecture provides equally expressive non-linear transformation context requires small number operations step. having hidden state maintains already compressed representation context especially suitable purpose able combine context input within single transformation. overall computation wavernn follows indicates masked matrix whereby last coarse input connected part states thus affects output coarse parts encoded scalars scaled interval matrix formed matrices computed single matrix-vector product produce contributions three gates standard sigmoid tanh non-linearities. possible architectural variant depend fully connected layer followed summation concatenation condition found version required parameters also performed centi-nats worse. split state parts predict respectively coarse bits bits -bit audio sample part feeds softmax layer corresponding bits prediction bits conditioned coarse bits. resulting dual softmax layer allows efﬁcient prediction -bit samples using small output spaces instead single large output space figure shows visually. note possible train softmax values addition requiring signiﬁcantly parameters memory compute consistently performs centi-nats worse. architecture reduces number operations needed step wavenet -bit discretized logistic mixture output proposed wavernn dual softmax. despite reduced number operations regular implementation wavernn sampling directly yield real-time faster synthesis. primary hindrance flops required sampling; rather difﬁculties twofold limits memory bandwidth time takes launch operations. regarding former wavernn state units parameters. regular implementation sampling calls wavernn operation separately sequence samples loads wavernn parameters memory registers step totalling gbytes required memory bandwidth. already third memory bandwidth available nvidia giving upper bound table benchmarks sparse wavernn mobile sampling performance executed widely available snapdragon mobile cpus. model hidden units sparsity structure sparsity. benchmarks based running equivalent computation mobile figure sparse wavernns curve number parameters. sparse wavernns structured sparsity point maximum performance high degree sparsity. points maximum performance unstructured sparse wavernns fall beyond tested range. overhead launching operation separately even larger. launching operation constant overhead microseconds step requires operations means launch overhead alone induces upper bound samples second. wavenet architecture requires operations sample launch overhead induces upper bound samples second. without considering time spent actual computation operations. practice regular implementation sampling e.g. tensorﬂow yields respectively samples second wavernn- wavenet. reduce factors implementing sampling procedure directly single persistent operation. memory bandwidth bottleneck avoided since parameters loaded registers start sampling persist registers throughprocess. possible full-precision registers sufﬁce store million half-precision parameters i.e. twice many needed wavernn-. operation launch bottleneck also avoided since entire sampling process utterance executed single operation. state size chosen speciﬁcally multi-processors. minimum numbers warps must assigned multi-processor access full register assign warp state calculation state size must multiple largest multiple available register space resulting kernel wavernn sampling orders magnitude efﬁcient regular sampling implementation reaching samples/second wavernn-. corresponding operation wavenet reaches samples/second. overhead given synchronization thousands cores takes nanoseconds synchronization instead microseconds needed operation launch. wavernn architecture dramatically reduces number required operations implementing sampling single operation eliminates much original computation overhead bottlenecks. next present technique reducing directly amount computation required operation. decreasing number hidden units reduce amount computation comes signiﬁcant loss quality instead reduce number non-zero weights network sparsifying weight matrices retaining large state size respective representation capacity. reduces since number non-zero weights directly proportional pruning scheme based weight magnitude increases sparsity training proceeds maintain binary mask specifying sparsity pattern weight matrices. beginning training weight matrices dense. every steps weights within sparsiﬁed layer sorted magnitude mask updated zeroing weights smallest magnitude. number computed fraction total number weights gradually increased target sparsity figure dependency scheme subscale wavernn. corresponds -bit sample. subscaling ﬁrst reshapes tensor sub-tensors interleaving samples. sub-tensor generated conditioned past future samples previously generated sub-tensors; past horizon unbounded whereas future horizon size tied receptive ﬁeld conditioning network. batched sampling applied. ﬁnal tensor original scale reconstituted generated sub-tensors. step weight pruning begins total number pruning steps. train total steps models. scheme practical easy integrate existing models increase training time. sparsify three gate matrices within cell separately. need encode sparsity mask manner allows efﬁcient computation. standard compressed sparse format uses amount storage encoding sparsity mask storing parameters. unlike hardware-oriented approaches viterbi pruning explore structured sparsity means reducing memory overhead. structure sparsity mask consider form non-overlapping blocks weights pruned retained together based average magnitude weights within block. blocks weights lose little performance unstructured sparsity reducing amount memory needed storing sparsity pattern required unstructured mask. besides rectangular blocks found work well also adopt blocks shape induce even lower memory bandwidth overhead. case blocks needs retrieve single activation value hidden state perform product. contrast take advantage computation memory bandwidth required sparse wavernn implement matrix-vector operations necessary sampling mobile cpu. maximize memory utilization weights stored -bit ﬂoating point converted -bit ﬂoating point used computation. activations calculations kept -bit ﬂoating point. memory overhead afforded small blocks allows sparse matrix-vector products match performance dense matrix-vector products parameter count. number sequential matrix-vector products second thus determined almost entirely number parameters network. described ways reducing sampling time high-ﬁdelity audio generation wavernn reduces sparse wavernn reduces lastly reduce contribution factor equation factor depends size utterance direct reduction size would negatively affect audio quality. instead propose method generating batch samples step instead sample ubi+s given depends samples ubk+z generation proceeds follows ﬁrst generates ﬁrst sub-tensor second sub-tensor conditioned ﬁrst third sub-tensor conditioned previous etc. subscale wavernn generates given sub-tensor conditioned future context previous sub-tensors using masked dilated relus mask applied past connections instead future ones. like multiscale scheme subscale schemes equally applicable multi-dimensional tensors. contrast multi-scale scheme subscaling makes possible generate samples single step. equation values future horizon dependencies ubi+s future samples ubk+z become overwhelmingly weak conditioning network subscale wavernn sees ﬁnite usually small number future samples previous sub-tensors. sampling sub-tensor begin immediately ﬁrst samples previous sub-tensor generated. subscale wavernn shared across sub-tensors possible batch inputs steps total batch subscale wavernn since value relatively small compared scale length even relatively large values total steps remains negligible total sampling latency. although conditioning network needs executed batch samples computing conditioning network doesn’t affect factor subscale wavernn network executed parallel chosen number future samples. increases total sampling steps even values remains negligible. batched sampling even regular implementation tensorﬂow achieves real-time speed subscale wavernn hidden state units. dropping distant future dependencies allows principle also recover almost equal number distant past dependencies. sub-tensor succeeds current sub-tensor steps behind leaves trace distant past samples. training sampling distant past samples accessed condition generation current pass analogously constant number future distant samples beyond sub-tensors previous also available additional conditioning. exact dependency scheme using subscaling batched sampling includes distant dependencies; practice however choosing larger value many cases computation time batch exam) grows sublinearly computation time ples weights reused spare computational capacity available. ability batch samples also makes possible generate across multiple processors reduction total sampling time linear number processors. previous work producing sample step sequential models required breaking local dependencies nearby samples strongly depend produced independently possibly conditioned samples. introduce general method allows trade small constant number distant past future dependencies ability generate batches samples step. tensor ﬁrst extracts sub-tensors frequency scale times smaller. sub-tensor corresponds subscale slice audio utterance sub-tensor corresponds utterance. contrast multi-scale scheme different subtensors extracted increasing scales. subscaling induces following ordering dependencies variables equivalent standard factorization joint scheme behind subscale wavernn directly generate bits step wavernn itself. take subscale wavernn model instead batching sub-tensors split hidden state wavernn parts. softmaxes bits value two. samples sub-tensors given directly wavernn input without using conditioning network. resulting fused subscale wavernn achieves small drop quality output maps well onto wavernn custom operation. compared wavernn runs real time model generates bits step requires fewer synchronizations resulting sampling speed real time. note contrast subscale wavernn fusion requires splitting hidden state audio quality drops quickly factors beyond fused subscale wavernn. text-to-speech models trained dataset hours north american english speech recorded professional speaker generation conditioned conventional linguistic features predicted pitch information. compared models synthesize audio -bit format. evaluation carried held-out test consider three performance measures negative log-likelihood groundtruth audio; generated speech utterances according subjective quality evaluation human raters; results direct comparison tests pairs models rated subjectively humans scale wavernn models trained sequences audio samples -bit full back-propagationthrough-time applied models. table reports results various sizes wavernn. larger wavernns approach performance -layer wavenet model. human rated comparison test between wavernn- wavenet indicates signiﬁcant difference quality speech produced additional comparison test wavenet table performance matrix-vector multiplies respectively gﬂops second using cores processors snapdragon dense kernels higher performance possible custom layouts dense matrix best performance could achieve standard major layout. figure illustrates core point investigation sparse models. dense wavernn model state size starting point largest could many current shelf mobile processors. second baseline model state size estimate still reach even fastest mobile platforms model would require gb/sec memory bandwidth current mobile platform provide amount. figure shows total parameter count keep corresponding sampling time also increase degree sparsity resulting size layers ﬁdelity models improves. holds high degrees sparsity state size models reaches hidden units. higher sparsity monotonically implies lower fact higher sparsity levels larger slopes. suggests given computational budget inference time much efﬁcient parameters sparsely connect larger number neurons layer. block sparsity speed blocks generally yield best blocks speed advantage. surprisingly better unstructured sparsity sparsity levels improve slowly eventually minimum around unstructured sparsity continues improve. explore even higher levels unstructured sparsity investigating extremely high levels sparsity requires starting extremely large dense layers making training computationally intensive. unstructured sparsity unsurprisingly slower inference depending quality trade offs involved using blocks might still preferred. obtain estimate sparse wavernn sampling speed benchmarked computationally heavy operations required producing audio sample used measurements derive estimate sampling speed. example sample model requires multiplications gates multiplications projection multiplications logits evaluating non-linearities. time operations estimate upper bound sampling performance. perform benchmarks snapdragon snapdragon mobile cpus widely available mobile phones. cores .ghz gﬂops/sec bandwidth shared cache gb/sec. faster .ghz cores able gﬂops/sec pull gb/sec cache. practice achievable ﬂops often much lower around gﬂops/sec gﬂops/sec respectively. numbers suggest dense sparse implementations close maximum possible performance processor comparison modern intel desktop gﬂops/sec gb/sec bandwidth cache cores. conditioning network subscale wavernn masked dilated layers convolutional kernels size convolutional channels residual channels. conditioning stages increasing dilation total future horizon blocks samples each. subscale wavernns evaluate units hidden state. recoverable distant dependencies. step corresponds signal. shown table subscale wavernn achieves equivalent baseline wavernn- shows ability subscale wavernn accurately learn distribution modiﬁed dependency scheme. also evaluate subscale wavernn generates interleaving signal khz. shown table audio ﬁdelity subscale wavernn signiﬁcantly different wavernn- transitivity wavenet remarkable audio generation sequential models extremely sensitive lost dependencies especially local ones quality result demonstrates effectiveness subscale dependency scheme preserve local dependencies high performance sequential models. ability batch computation factor yields large amount ﬂexibility. batching increase throughput single device increasing overall sampling speed. addition makes possible generate multiple devices once generated bits sent one-way online device next. setup gives principle linear speed-up sampling speed single device. single pass subscale wavernn runs real time connected rack gpus subscale wavernn principle gain equivalent linear speed-up total sampling speed times real time. subscale wavernn also combined sparse wavernn executed multi-core gaining speed-up proportional number cores available. conclusion introduced wavernn simple powerful recurrent network sequential modeling high ﬁdelity audio demonstrated high performance implementation model gpus. shown large sparse models much better quality small dense models number parameters written high performance block-sparse matrix-vector product operations demonstrate sampling time proportional parameter count. showed high ﬁdelity audio generation achievable widely available low-power mobile cpus. finally introduced subscale dependency scheme lets sequential models generate many samples step preserving output quality original model. underlying ideas methods introduce speciﬁc audio results sparse models implications inference types neural networks. oord babuschkin simonyan vinyals kavukcuoglu driessche lockhart cobo stimberg casagrande grewe noury dieleman elsen kalchbrenner graves king walters belov hassabis parallel wavenet fast high-ﬁdelity speech synthesis. corr abs/. wang skerry-ryan stanton weiss jaitly yang xiao chen bengio agiomyrgiannakis clark saurous tacotron fully end-to-end text-to-speech synthesis model. corr abs/.", "year": "2018"}