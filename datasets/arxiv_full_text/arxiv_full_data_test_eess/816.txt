{"title": "Style Tokens: Unsupervised Style Modeling, Control and Transfer in  End-to-End Speech Synthesis", "tag": "eess", "abstract": " In this work, we propose \"global style tokens\" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable \"labels\" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis. ", "text": "style modeling goal provide models capability choose speaking style appropriate given context. difﬁcult deﬁne precisely style contains rich information intention emotion inﬂuences speaker’s choice intonation ﬂow. proper stylistic rendering affects overall perception important applications audiobooks newsreaders. style modeling presents several challenges. first objective measure correct prosodic style making modeling evaluation difﬁcult. acquiring annotations large datasets costly similarly problematic since human raters often disagree. second high dynamic range expressive voices difﬁcult model. many models including recent end-to-end systems learn averaged prosodic distribution input data generating less expressive speech especially long-form phrases. furthermore often lack ability control expression speech synthesized. work attempts address issues introducing global style tokens tacotron state-of-the-art end-to-end model. gsts trained without prosodic labels uncover large range expressive styles. internal architecture produces soft interpretable labels used perform various style control transfer tasks leading signiﬁcant improvements expressive long-form synthesis. gsts directly applied noisy unlabeled found data providing path towards highly scalable robust speech synthesis. model based tacotron sequence-to-sequence model predicts spectrograms directly grapheme phoneme inputs. spectrograms converted waveforms either low-resource inversion algorithm neural vocoder wavenet point that tacotron choice vocoder affect prosody work propose global style tokens bank embeddings jointly trained within tacotron state-of-the-art end-toend speech synthesis system. embeddings trained explicit labels learn model large range acoustic expressiveness. gsts lead rich signiﬁcant results. soft interpretable labels generate used control synthesis novel ways varying speed speaking style independently text content. also used style transfer replicating speaking style single audio clip across entire long-form text corpus. trained noisy unlabeled found data gsts learn factorize noise speaker identity providing path towards highly scalable robust speech synthesis. past years seen exciting developments deep neural networks synthesize natural-sounding human speech text-to-speech models rapidly improved growing opportunity number applications audiobook narration news readers conversational assistants. neural models show potential robustly synthesize expressive long-form speech research area still infancy. deliver true human-like speech system must learn model prosody. prosody conﬂuence number phenomena speech paralinguistic information intonation stress style. work focus figure model diagram. training log-mel spectrogram training target reference encoder followed style token layer. resulting style embedding used condition tacotron text encoder states. inference feed arbitrary reference signal synthesize text speaking style. alternatively remove reference encoder directly control synthesis using learned interpretable tokens. reference encoder proposed compresses prosody variablelength audio signal ﬁxed-length vector call reference embedding. training reference signal ground-truth audio. reference embedding passed style token layer used query vector attention module. here attention used learn alignment. instead learns similarity measure reference embedding token bank randomly initialized embeddings. embeddings alternately call global style tokens gsts token embeddings shared across training sequences. attention module outputs combination weights represent contribution style token encoded reference embedding. weighted gsts call style embedding passed text encoder conditioning every timestep. directly condition text encoder certain tokens depicted right-hand side inference-mode diagram figure allows style control manipulation without reference signal. baseline gst-augmented tacotron systems architecture hyperparameters except details. phoneme inputs speed training slightly change decoder replacing cells layers -cell lstms; regularized using zoneout probability decoder outputs -channel logmel spectrogram energies frames time dilated convolution network outputs linear spectrograms. grifﬁn-lim fast waveform reconstruction. straightforward replace grifﬁn-lim wavenet vocoder improve audio ﬁdelity reference encoder made convolutional stack followed rnn. takes input log-mel spectrogram ﬁrst passed stack convolutional layers kernel stride batch normalization relu activation function. output channels convolutional layers respectively. resulting output tensor shaped back dimensions single-layer -unit unidirectional gru. last state serves reference embedding input style token layer. style token layer made bank style token embeddings attention module. unless stated otherwise experiments tokens found sufﬁcient represent small rich variety prosodic dimensions training data. match dimensionality text encoder state token embedding similarly text encoder state uses tanh activation; found applying tanh activation gsts applying attention greater token diversity. content-based tanh attention uses softmax activation output combination weights tokens; resulting weighted combination gsts used conditioning. experimented different combinations conditioning sites found replicating style embedding simply adding every text encoder state performed best. content-based attention similarity measure work trivial substitute alternatives. dotproduct attention location-based attention even combinations attention mechanisms learn different types style tokens. experiments found using multihead attention signiﬁcantly improves style transfer performance moreover effective simply increasing number tokens. using attention heads token embedding size concatenate attention outputs ﬁnal style embedding size remains same. basis vectors soft clusters i.e. style tokens. mentioned above contribution style token represented attention score replaced desired similarity measure. layer conceptually somewhat similar vq-vae encoder learns quantized representation input. also experimented replacing layer discrete vq-like lookup table layer seen comparable results yet. decomposition concept also generalized models e.g. factorized variational latent model exploits multi-scale nature speech signal explicitly formulating within factorized hierarchical graphical model. sequence-dependent priors formulated embedding table similar gsts without attention-based clustering. gsts could potentially used reduce required samples learn prior embedding. embeddings also viewed external memory stores style information extracted training data. reference signal guides memory writes training time memory reads inference time. leverage recent advances memory-augmented networks improve learning. prosody speaking style models studied decades community. however existing models require explicit labels emotion speaker codes small amount research explored automatic labeling learning still supervised requiring expensive annotations model training. autobi example aims produce tobi labels used models. however autobi still needs annotations training tobi hand-designed label system known limited performance cluster-based modeling related work. jauk example uses i-vectors acoustic features cluster training train models different partitions. methods rely complex hand-designed features however require training neutral voice model separate step. mentioned previously introduces reference embedding used work shows used transfer prosody reference signal. embedding enable interpretable style control however show section work substantially extends research several fundamental differences. first uses single frame tacotron decoder query learn tokens. thus models local variations primarily correspond gsts instead summary entire reference signal input thus able uncover local global attributes essential expressive synthesis. second contrast decoder-side conditioning design gsts allows textual input conditioned disentangled style embedding. show crucial implications style control transfer section finally gsts applied clean recordings noisy found data. discuss signiﬁcance detail section train models using hours american english audiobook data. read blizzard challenge speaker catherine byers animated emotive storytelling style. books contain expressive character voices high dynamic range challenging model. common generative models objective metrics often correlate well perception visualizations experiments below strongly encourage readers listen samples provided demo page. conditioning manner several beneﬁts. first allows examine style attributes token encodes. empirically token represent pitch intensity also variety attributes speaking rate emotion. seen figure shows sentences synthesized three different style tokens -token model. plots show curves quite different across style tokens. however contours generated token follow clear relative trend despite fact input sentences completely different. single-token conditioning also reveals tokens capture single attributes token learn represent speaking rate others learn mixture attributes reﬂect stylistic co-occurrence training data encouraging independent style attribute learning important focus ongoing work. addition providing interpretability style token conditioning also improve synthesis quality. consider problem long-form synthesis training data lots prosodic variation. many models learn generate average prosodic style problematic expressive datasets since variation characterizes collapsed. also lead undesirable side effects pitch continuously declining towards sentence. conditioning lively-sounding tokens address problems signiﬁcantly improving prosodic variation. another method controlling style token output scaling. multiplying token embedding scalar value intensiﬁes style effect. illustrated figure shows spectrograms utterances synthesized different tokens. perceptually tokens encode figure effect token scaling. left right scale tokens respectively. note model seems exhibit reverse effect negative scale never seen training. also control synthesis inference modifying attention module weights inside style token layer. since attention produces combination weights reﬁned manually yield desired interpolation. also randomly generated softmax weights sample style space. sampling diversity controlled tuning softmax temperature. style embedding added text encoder states training doesn’t need case inference mode. audio samples demonstrate allows piecewise style control morphing conditioning tokens different segments input text. style transfer active area research aims synthesize phrase prosodic style reference signal property model conditioned convex combination style tokens lends well task; inference time simply feed reference signal guide choice token combination weights. experiments -head attention. different speaking styles faster speaking rate animated speech figure shows increasing scaling factor faster speaking rate token causes gradual compression spectrogram time domain. similarly figure shows increasing scaling factor animated speech token yields commensurate increases pitch variation. style scaling effects hold even negative values despite fact model sees positive values training. figure robustness non-parallel style transfer. left right attention alignments obtained feeding three references whose text lengths characters respectively. target text length characters. figure shows spectrograms parallel transfer task text synthesize matches text reference signal. model spectrogram bottom right compared three baselines ground-truth input signal inference performed baseline tacotron model inference performed tacotron system conditions text encoder directly reference embedding. that given text input baseline tacotron model closely match prosodic style reference signal. contrast direct conditioning method results nearly time-aligned prosody transfer. model somewhere between output duration formant transitions don’t precisely match reference overall spectrotemporal envelopes perceptually gsts resemble prosodic style reference. prosodic style reference signal. chose three different reference signals task tested well model replicated style synthesizing target phrase. since long-form synthesis beneﬁt signiﬁcantly proper stylistic rendering used long target phrase. chose source phrases varying lengths figure shows alignment matrices synthesis conditioned source signal. shows -token model. model robustly generalizes three conditioning inputs evidenced good alignment plots. bottom shows -token model exhibiting behavior; include model show gsts remain robust even number tokens larger reference embedding dimensionality middle shows model direct reference embedding conditioning. attention matrices show model fails conditioned shorter source phrases since tries squeeze synthesis time interval reference. model successfully aligns conditioned longest input intelligibility poor words per-utterance embedding captures much information source hurting generalization. table subjective preference p-values audiobook synthesis tacotron baseline. shows inference conditioned different reference signal p-values given -point -point rating system. evaluate quality method scale sideby-side subjective tests non-parallel style transfer tacotron baseline. used evaluation audiobook sentences including many long phrases. generated sets output conditioning model different narrative-style reference signals unseen training. side-by-side subjective test indicated raters preferred sets synthesis tacotron baseline shown table studio-quality data economically time consuming record. internet holds vast amounts rich real-life expressive speech often noisy difﬁcult label. section demonstrate gsts used train robust models directly noisy found data without modiﬁcations. ﬁrst experiment artiﬁcially generate training sets adding noise clean speech. motivation simulate real noisy data performing controlled experiments. achieve this pass single-speaker english proprietary dataset room simulator adds varying types background noise room reverberations. training gst-augmented tacotron datasets inference ﬁrst mode described section instead providing reference signal condition model individual style token gives interpretable audible sense token learned. interestingly different noises treated styles absorbed different tokens. illustrate spectrograms tokens figure tokens clearly correspond different interference types music reverberation general background noise. importantly method reveals subset learned tokens also correspond completely clean speech. means synthesize clean speech arbitrary text input conditioning model single clean style token. demonstrate this inference using manuallyidentiﬁed clean style token evaluate output using naturalness tests. -phrase evaluation collecting ratings crowdsourced native speakers. table shows results baseline tacotron clean-token model. baseline tacotron achieves dataset clean decreases interference increases dropping score model prior knowledge speech noise blindly models statistics training resulting substantial amounts noise synthesis. noise conditions. note number tokens needs increase along percentage noise achieve result. example -token model yields clean tokens trained noise dataset noisier datasets required -token model. future work explore adapt number tokens automatically given data distribution. second experiment uses real data. dataset made audio tracks mined ofﬁcial youtube channel videos. tracks contain signiﬁcant acoustic variations including channel variation noise reverberation. endpointer segment audio tracks short clips followed model create <text audio> training pairs. despite fact model generates signiﬁcant number transcription misalignment errors perform preprocessing. ﬁnal training hours long contains speakers. without using metadata labels train baseline tacotron -token model comparison. expected baseline fails learn since multi-speaker data varied. model results presented figure shows spectrograms phrase overlaid tracks generated conditioning model randomly chosen tokens. examining trained gsts different tokens correspond different speakers. means that synthesize speciﬁc speaker’s voice simply feed audio speaker reference signal. section quantitative evaluations. finally exploit fact talks english small fraction spanish. experiment compare baseline gst-enabled noisy data models cross-lingual style transfer task. baseline train multi-speaker tacotron similar using video proxy speaker labels. conditioned spanish speaker label synthesize english phrases. system feed reference signal spanish speaker synthesize english phrases. spanish accent speaker preserved model produces completely intelligible english speech similar pitch range speaker. contrast multi-speaker tacotron output much less intelligible. evaluate result objectively compute word error rates english model synthesized speech. shown table utterances much lower multi-speaker model. results strongly corroborate gsts learn embeddings disentangled text content. though exciting early result in-depth study using prosody-preserving language transfer order. t-sne visualize style embeddings learned artiﬁcial noise datasets. figure shows embeddings learned artiﬁcial noisy dataset clearly separated classes. figure shows style embeddings randomly drawn samples containing talk data speakers. samples well separated clusters corresponding individual speaker. female male speakers linearly separable. also style embeddings features perform noise speaker classiﬁcation linear discriminative analysis. results shown table noise classiﬁcation gsts uncover true label accuracy. speaker classiﬁcation video true labels compare i-vector method standard representation used modern speaker veriﬁcation systems. task test contains speakers. trained tested short utterances gsts comparable i-vectors. encouraging result given i-vectors speciﬁcally designed speaker classiﬁcation. speculate potential applied speaker diarization. results important implications future research found data. first robustness gsts acoustic textual noise design automated data mining pipelines greatly simpliﬁed. accurate segmentation models example longer necessary build high-quality models. second style attributes emotion often difﬁcult label large-scale noisy data. using gsts weights automatically generate style annotations substantially reduce human-in-the-loop efforts. work introduced global style tokens powerful method modeling style end-to-end systems. gsts intuitive easy implement learn without explicit labels. shown that trained expressive speech data model yields interpretable embeddings used control transfer style. also demonstrated that originally conceived model speaking styles gsts general technique uncovering latent variations data. corroborated experiments unlabeled noisy found data showed still much investigated including improving learning using weights targets predict text. finally applied tacotron work believe readily used types end-to-end models. generally envision applied problem domains beneﬁt interpretability controllability robustness. example similarly employed text-to-image neural machine translation models. authors thank jansen clark zhifeng chen weiss mike schuster yonghui patrick nguyen machine hearing google brain google teams helpful discussions feedback. references arik sercan chrzanowski mike coates adam diamos gregory gibiansky andrew kang yongguo xian miller john raiman jonathan sengupta shubho deep voice real-time neural text-to-speech. icml dehak najim kenny patrick dehak r´eda dumouchel pierre ouellet pierre. front-end factor analysis speaker veriﬁcation. ieee transactions audio speech language processing wei-ning zhang glass james. unsupervised learning disentangled interpretable representations sequential data. advances neural information processing systems chanwoo misra ananya chin kean hughes thad narayanan arun sainath tara bacchiani michiel. generation large-scale simulated utterances virtual rooms train deep-neural networks far-ﬁeld speech recognition google home. proc. interspeech. isca kinnunen tomi juvela lauri alku paavo yamagishi junichi. non-parallel voice conversion using i-vector plda towards unifying speaker veriﬁcation transformation. icassp krueger david maharaj tegan kram´ar j´anos pezeshki mohammad ballas nicolas rosemary goyal anirudh bengio yoshua larochelle hugo courville aaron zoneout regularizing rnns randomly preserving hidden activations. proc. iclr nakashika toru takiguchi tetsuya minami yasuhiro nakashika toru takiguchi tetsuya minami yasuhiro. non-parallel training voice conversion using adaptive restricted boltzmann machine. ieee/acm trans. audio speech lang. proc. november ping peng kainan gibiansky andrew arik sero kannan ajay narang sharan raiman jonathan miller john. deep voice -speaker neural text-to-speech. arxiv preprint arxiv. shen jonathan pang ruoming weiss schuster mike jaitly navdeep yang zongheng chen zhifeng zhang wang yuxuan skerry-ryan natural synthesis conditioning wavenet spectrogram predictions. arxiv preprint arxiv. silverman beckman mary pitrelli john ostendorf mori wightman colin price patti pierrehumbert janet hirschberg julia. tobi standard labeling english prosody. second international conference spoken language processing skerry-ryan battenberg eric xiao ying wang yuxuan stanton daisy shor joel weiss clark saurous towards end-to-end prosody transfer expressive speech synthesis tacotron. arxiv preprint oord a¨aron dieleman sander heiga simonyan karen vinyals oriol graves alex kalchbrenner senior andrew kavukcuoglu koray. wavenet generative model audio. corr abs/. vaswani ashish shazeer noam parmar niki uszkoreit jakob jones llion gomez aidan kaiser łukasz polosukhin illia. attention need. advances neural information processing systems wang yuxuan skerry-ryan stanton daisy yonghui weiss jaitly navdeep yang zongheng xiao ying chen zhifeng bengio samy quoc agiomyrgiannakis yannis clark saurous tacotron towards end-to-end speech synthesis. proc. interspeech august https//arxiv.org/abs/.. heiga agiomyrgiannakis yannis egberts niels henderson fergus szczepaniak przemysław. fast compact high quality lstm-rnn based statistical parametric speech synthesizers mobile devices. proceedings interspeech", "year": "2018"}