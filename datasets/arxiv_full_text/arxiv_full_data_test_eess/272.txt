{"title": "Generative Sensing: Transforming Unreliable Sensor Data for Reliable  Recognition", "tag": "eess", "abstract": " This paper introduces a deep learning enabled generative sensing framework which integrates low-end sensors with computational intelligence to attain a high recognition accuracy on par with that attained with high-end sensors. The proposed generative sensing framework aims at transforming low-end, low-quality sensor data into higher quality sensor data in terms of achieved classification accuracy. The low-end data can be transformed into higher quality data of the same modality or into data of another modality. Different from existing methods for image generation, the proposed framework is based on discriminative models and targets to maximize the recognition accuracy rather than a similarity measure. This is achieved through the introduction of selective feature regeneration in a deep neural network (DNN). The proposed generative sensing will essentially transform low-quality sensor data into high-quality information for robust perception. Results are presented to illustrate the performance of the proposed framework. ", "text": "figure effect image quality predictions using alexnet model trained high-resolution images. degradation severity increases left right white additive noise gaussian blur seen that quality degradation hinder human ability classify images object class label predicted changes significantly even degradation levels. low-power low-profile sensors often limited produce low-resolution images. dodge karam showed degradations even relatively small levels cause significant drop classification performance stateof-the-art dnns typically trained using highresolution images. work dodge karam image quality aspect often overlooked designing deep learning based image classification systems. dodge karam also showed recently that although dnns perform better humans pristine high-resolution images classification performance still significantly much lower images degradations quality. fig. illustrates that humans capable recognizing reasonable accuracy objects low-resolution blurred images paper introduces deep learning enabled generative sensing framework integrates low-end sensors computational intelligence attain high recognition accuracy attained highend sensors. proposed generative sensing framework aims transforming low-end low-quality sensor data higher quality sensor data terms achieved classification accuracy. low-end data transformed higher quality data modality data another modality. different existing methods image generation proposed framework based discriminative models targets maximize recognition accuracy rather similarity measure. achieved introduction selective feature regeneration deep neural network proposed generative sensing essentially transform low-quality sensor data high-quality information robust perception. results presented illustrate performance proposed framework. recent advances machine learning coupled accessibility compact low-cost multimedia sensors visible image infrared cameras fundamentally altered humans live work interact other. however performance available sensors still significantly constrained quality/cost tradeoff sensors sensitivity deep learning algorithms variations sensor data quality sensor modality. example image quality impacted environmental factors i.e. lighting conditions specifications sensors i.e. number pixels dynamic range noise etc. lowlighting conditions visible wavelength image suffers image largely suffer dark visible wavelength image sensor denoted image sensor core part digital cameras smart phones cost image sensors aggressively scaled largely high-volume consumer market products. addition cost technical specification image sensors including number pixels color contrast dynamic range power consumption meet almost demands consumer market products. sensors hand mostly used specific needs surveillancetrackingbased applications military domain started penetrating consumer market recently sensors typically cost significantly higher image sensors produce images equivalent resolution primarily relatively volume market. generally sensors show trend higher cost better delivered performance. work develops deep learning based framework refer generative sensing enables attaining classification accuracy higher quality high-end sensor using low-quality low-end sensor. furthermore high-end sensor low-end sensor different types case proposed framework seen transforming type sensor sensor) another type sensor terms achieved classification performance. performed feature regeneration improved classification. important enabling low-power low-cost sensing platforms without compromising recognition performance. paper organized follows. section discusses related work. section describes proposed generative sensing framework. illustrative performance results presented section conclusion given section existing methods proposed image generation applications colorization including colorization artistic style transfer dataset augmentation existing methods mainly concerned generation data samples similar existing reference data samples. achieve goal optimize objective function maximizes similarity measure generated data reference data. contrast proposed generative sensing based discriminative models optimizes target-oriented objective function maximize classification accuracy rather similarity measure. presented method identify convolutional filters sensitive degradations image quality correct degraded activations filters. showed correcting fraction susceptible filter activations using small complexity convolutional filter blocks results significant performance improvement popular datasets including imagenet also showed deepcorrect model achieve classification accuracy higher fine-tuning training fraction network parameters also train faster fine-tuning. proposed generative sensing framework builds extends deepcorrect work sensors varying illumination environment acquisition characteristics addition resolution) varying modalities block diagram proposed generative sensing framework shown fig. following description term high-end sensor data refers sensor data result relatively high classification accuracy using pre-trained deep neural network data separately. փhigh փlow denote respectively feature maps resulting high-end sensor data low-end sensor data. using distance measure quantify differences colocated features փhigh փlow feature difference maps obtained. mainly interested locating feature differences result significant drop classification accuracy finding features փlow significantly different co-located features փhigh based change classification accuracy. approach locating features measuring classification accuracy drop feature փhigh replaced co-located փlow feature keeping features փhigh unchanged another approach determining significant feature differences based maximizing drop classification accuracy clusters replacing clusters features փhigh features փlow. classification-based feature difference maps thus obtained values corresponding classification accuracy drop. also thresholded produce binary significance difference tensor value denoting significant feature difference location value denoting insignificant difference. selective feature regeneration performed learning transformations applied features փlow corresponding significant values leaving features փlow unchanged. transformations learned using relatively small residual learning units adopting learning models. refer transformations generative transformation units simply generative units short. resulting generative sensing network 𝜙𝜙𝑔𝑔𝑔𝑔𝑔𝑔 consists original pre-trained augmented generative units 𝜙𝜙𝑔𝑔𝑔𝑔𝑔𝑔 obtain improved classification accuracy. generative unit takes form multilayer network trainable parameters generative transform estimated determining trainable parameters wgen generative unit minimize target-oriented loss function 𝐸𝐸�𝐖𝐖𝒈𝒈𝒈𝒈𝒈𝒈�=𝜆𝜆 𝜌𝜌�𝐖𝐖𝒈𝒈𝒈𝒈𝒈𝒈�+𝑀𝑀�ℒ𝑦𝑦𝑖𝑖) classification loss function 𝑦𝑦𝑖𝑖 target output label input 𝐱𝐱𝑖𝑖 𝜙𝜙𝑔𝑔𝑔𝑔𝑔𝑔 output refer generative sensing network regularization term regularization parameter 𝑀𝑀is total number data parameters compared order show performance proposed generative sensing framework ability generalize different tasks different input modalities different sensor sizes/resolutions adopt baseline pre-trained object recognition relatively high-quality visible-wavelength imagenet dataset varying tasks input modalities make rgb-ir scface face recognition dataset epfl rgb-nir scene recognition dataset unlike task face recognition assign face test image known subjects database goal scene recognition classify entire scene image. scface dataset primarily designed surveillance-based face recognition. consists images acquired visible well infrared spectrum subjects frontal mugshot image subject input modality addition nonfrontal mugshots. make frontal mugshot images. epfl rgb-nir scene dataset consists scene categories least images class visible near-infrared spectra simulate effect decreasing input sensor resolution blurring original images gaussian kernel size blur kernel times blur standard deviation using blur standard deviation table top- accuracy images scface dataset acquired different sensor resolutions different input modalities respectively. modality bold numbers show best accuracy resolution level. table top- accuracy images rgb-nir scene dataset acquired different sensor resolutions different input modalities respectively. modality bold numbers show best accuracy resolution level. face recognition scene recognition tasks images acquired different sensor resolutions modalities since baseline alexnet model trained scface epfl rgb-nir scene dataset dataset modality train fully connected layer softmax activation deep features extracted original dataset images evaluate performance deep feature extractor different levels sensor resolution using previously learnt final fully connected layer softmax activation linear classifier. generative units trained using imagenet dataset augmented adding degraded versions imagenet images varying levels blur simulate varying sensor resolutions results shown tables concluded that although images scface rgb-nir scene datasets used training produced generative units generalize well. tables present performance results levels sensor resolution scface dataset rgb-nir scene dataset respectively. noted learn classifier generative sensing deep feature extractor instead trained baseline alexnet deep feature extractor. shown tables. visible spectrum near-infrared/infrared spectrum sensor resolution significantly affects accuracy baseline feature extractor drop respective average accuracies scface dataset drop respective average accuracies rgb-nir scene dataset relative original sensor resolution scface dataset generative sensing feature extractor significantly outperforms baseline feature extractor relative improvement mean accuracy visible spectrum infrared spectrum respectively. similarly rgb-nir scene dataset generative sensing feature extractor significantly outperforms baseline feature extractor relative improvement mean accuracy visible spectrum near-infrared spectrum respectively. large performance generative sensing feature extractor baseline alexnet feature extractor highlights generic nature modality-invariant sensor resolution-invariant features learnt generative sensing models. work presents deep learning based generative sensing framework attaining increased classification accuracy classification accuracy highend sensor using low-end sensor. highlow-end sensors different types case proposed framework seen transforming type sensor another type sensor terms borkar karam deepcorrect correcting models image distortions\" arxiv. russakovsky deng krause satheesh huang karpathy khosla bernstein berg fei-fei imagenet large scale visual recognition challenge international journal computer vision vol. grgic delac grgic scface surveillance cameras face database multimedia tools applications journal vol. february brown süsstrunk multispectral sift scene category recognition ieee international conference computer vision pattern recognition classification performance. achieved learned transformations perform selective feature regeneration improved classification. important enabling robust low-power low-cost sensing platforms work varying conditions without compromising recognition performance. references dodge karam understanding image quality affects deep neural networks international conference quality multimedia experience pages dodge karam study comparison human learning recognition performance visual deep distortions computer communications networks pages jul.-aug. dodge karam early human visual system compete deep neural networks? iccv workshop mutual benefits cognitive computer vision pages october krizhevsky sutskever hinton imagenet classification deep convolutional neural networks advances neural information processing systems fossum cmos image sensors electronic camera-on-achip ieee transactions electron devices vol. bigas cabruja forest salvi. review cmos image sensors microelectronics journal vol. sobrino frate drusch review thermal infrared applications requirements future high-resolution sensors ieee transactions geoscience remote sensing vol. rogalski martyniuk kopytko challenges small-pixel infrared detectors review reports progress physics vol. deshpande rock forsyth \"learning largescale automatic image colorization\" ieee international conference computer vision zhang isola a.a. efros \"colorful image colorization\" european conference computer vision leibe matas sebe welling computer vision eccv lecture notes computer science springer cham. limmer h.p.a. lensch \"infrared colorization using deep convolutional neural networks\" ieee international conference machine learning applications gatys a.s. ecker bethge neural algorithm artistic style\" arxiv. gatys a.s. ecker bethge \"image style transfer using convolutional neural networks\" ieee cvpr kniaza gorbatsevicha mizginova \"thermalnet deep convolutional network synthetic thermal image generation\" international isprs workshop psbb", "year": "2018"}