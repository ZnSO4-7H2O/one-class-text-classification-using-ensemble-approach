{"title": "Distilling Knowledge Using Parallel Data for Far-field Speech  Recognition", "tag": "eess", "abstract": " In order to improve the performance for far-field speech recognition, this paper proposes to distill knowledge from the close-talking model to the far-field model using parallel data. The close-talking model is called the teacher model. The far-field model is called the student model. The student model is trained to imitate the output distributions of the teacher model. This constraint can be realized by minimizing the Kullback-Leibler (KL) divergence between the output distribution of the student model and the teacher model. Experimental results on AMI corpus show that the best student model achieves up to 4.7% absolute word error rate (WER) reduction when compared with the conventionally-trained baseline models. ", "text": "order improve performance far-field speech recognition paper proposes distill knowledge close-talking model far-field model using parallel data. close-talking model called teacher model. farfield model called student model. student model trained imitate output distributions teacher model. constraint realized minimizing kullbackleibler divergence output distribution student model teacher model. experimental results corpus show best student model achieves absolute word error rate reduction compared conventionally-trained baseline models. close-talking setting automatic speech recognition systems achieved significant improvement deep neural network based acoustic models however farfield speech recognition tasks still challenging especially dealing speech collected single distant microphone. literatures utilize close-talking data together far-field data train acoustic models speech recognition. methods multi-condition training method uses data different conditions train acoustic models. method environment-aware training approach proposed closetalking features help extract environment features auxiliary information. works proposed enhanced speech train acoustic models speech recognition. dereverberation model used estimate close-talking data given far-field data researchers train dereverberation model recognition model independently. others propose joint training approach speech enhancement speech recognition tasks. moreover ravanelli propose novel network speech enhancement speech recognition tasks cooperate other. mentioned approaches able obtain obvious improvement. however close-talking data training data optimized reference. close-talking model guide training far-field model. recently qian propose share knowledge hidden layers closetalking far-field models. approach achieves promising improvement. however shares knowledge hidden layers rather transfer knowledge output layers models. therefore knowledge distillation proposed transfer knowledge output layers close-talking far-field models paper. concept knowledge distillation around decade general framework proposed hinton distill knowledge using high temperature. high level distillation contains training model. model trained mimic output distribution well-trained model. similarly several works knowledge distillation compress acoustic models. utilize large model train small model. chan propose transfer knowledge recurrent neural networks model small model. chebotar propose distill ensembles acoustic models single acoustic model. methods utilize kullbackleibler divergence minimize difference output distributions acoustic models. previous results show methods compress acoustic models effectively little performance loss. inspired methods paper uses divergence distill knowledge using parallel data improve performance far-field speech recognition. acoustic model trained close-talking data called teacher model. acoustic model trained far-field data called student model. student model trained imitate output distribution teacher model. difference output distributions models minimized divergence. addition paper investigates improvement student model influenced performance teacher models. main contributions paper follows distilling knowledge output layer close-talking model far-field model using divergence far-field speech recognition. investigating performance student model influenced different teacher models. experimental results corpus show best student model achieves absolute word error rate reduction compared conventionallytrained baseline models. results also show increases accuracy teacher model yield similar increases performance student model. rest paper organized follows. section describes knowledge distillation using parallel data. experiments presented section results discussed section paper concluded section distillation make teacher model transfer knowledge student model. student model trained mimic output distribution teacher model. thus student model forced close output distribution teacher model. constraint realized minimizing divergence output distributions models. letting denotes output probabilities teacher model denotes output probabilities student model difference output distributions models defined wished minimize denotes index senone denotes i-th senone referred input features close-talking speech referred input features far-field speech denotes posterior probability computed student model given denotes posterior probability computed teacher model given also defined equation divergence minimized minimizing cross entropy loss function. thus optimization distillation viewed standard normal backpropagation algorithm directly used train student model. thing needs changed hard label replaced called soft label. equation also indicates still transfer knowledge teacher model student model loss function network architecture student model different teacher model. needs output labels student model identical teacher model. approach simplified version high temperature based distillation proposed hinton training student model guided teacher model using parallel data. teacher models student models hybrid acoustic models. identical output labels senones. framework knowledge distillation far-field speech recognition shown fig. hard labels ùë°‚Ñéùëéùëüùëë generated gaussian mixture model hidden markov model model frame-level. gmm-hmm model trained closetalking data. hard labels one-hot vectors. example denotes hard labels frame. teacher model trained close-talking data hard labels ùë°‚Ñéùëéùëüùëë neural network teacher model based long short term memory bidirectional lstm training parameters teacher model fixed. teacher model used compute soft labels. soft labels ùë°ùë†ùëúùëìùë° computed teacher model using forward algorithm close-talking data frame-level. soft labels much information underlying label distribution hard labels. example denotes soft labels frame. probability frame belonging label probability frame belonging label student model based acoustic model. parallel relationship used align far-field data close-talking soft labels ùë°ùë†ùëúùëìùë°. student model trained using far-field data corresponding soft labels. training criterion equation parameters student model updated parameters teacher model changed training student model. decoding stage student model used compute posterior probabilities. acoustic likelihood computed combining posterior prior probabilities. thus proposed method need extra computation cost decoding. experiments conducted meeting corpus corpus consists hours meeting recordings. recordings range signals synchronized common timeline. three types recordings datasets. close-talking data collected individual headset microphones. far-field data collected single distant microphone using microphone array. far-field data collected multiple distant microphones using multiple microphones array. experiments datasets. three sets datasets respectively training development test training contains utterances hours. development utterances hours. test utterances hours. frame length frame shift input features gmm-hmm models -dim mfcc features. models gaussians. input features neural networks -dimensional mel-filter bank features plus delta delta-delta. vocabulary dictionary words. language model trigram. trained using training transcripts fisher english corpus. decoding procedure followed standard recipe. follow officially released kaldi recipe build gmm-hmm models first. distant-gmm trained far-field data. close-gmm trained closetalking data. distant-gmm senones. closegmm senones. far-field data dataset train models. called distant-dnn trained hard labels generated distant-gmm using dataset. called close-dnn trained hard labels generated close-gmm using dataset. models hidden layers sigmoid units layer. input layer models uses sliding context window frames. models trained using stochastic gradient descent mini-batch size initial learning rate results distant-gmm model models eval sets dataset listed table table find close-dnn model outperforms models eval sets obviously. results show close-talking hard labels leads obvious improvement. reason close-talking hard labels higher quality far-field hard labels. four teacher models trained using close-talking data dataset dnn-smbr lstm blstm. hard labels generated close-gmm model using dataset teacher models. lstm model uses single frame input. stacked lstm layers projection layer memory cells output units. initial learning rate momentum respectively. training carried truncated time algorithm. blstm model uses single frame input. stacked blstm layers projection layer memory cells output units. initial learning rate momentum respectively. training carried bptt algorithm. table find blstm teacher model achieves best performance. lstm teacher model outperforms teacher models. four teacher models transfer knowledge student models rest experiments. student models based acoustic models number parameters baseline closednn model. four student models trained using farfield data dataset s-dnn s-dnn-smbr slstm s-blstm. teacher model s-dnn model. s-dnn-smbr trained mimic dnnsmbr teacher model. teacher model s-lstm lstm model. s-blstm guided dnn-blstm teacher model. soft labels computed teacher models using dataset respectively. student models compared baseline close-dnn model. also compare proposed method methods. drsl method proposed multi-cond drjl-parallel drjl-front-back cfmks drjl+cfmks approaches proposed speech recognition models independently. multi-cond directly using data close-talking far-field train acoustic models. drjl-parallel joint training dereverberation speech recognition models sharing hidden layers. drjl-front-back dereverberation speech recognition models front-back structure. cfmks sharing knowledge hidden layers close-talking far-field models. models based. multi-cond drjl-parallel drjl-front-back cfmks models trained using hidden layers sigmoid units layer. drsl drjl-front-back dereverberation recognition models trained using hidden layers sigmoid units layer respectively. results student models models evaluated eval sets dataset listed table curves student models guided different teacher models eval dataset shown fig. table student models outperform baseline model models except drjl+cfmks model. s-blstm student model obtains best performance among models. achieves relative reduction eval compared baseline model obtains absolute reduction eval best model drjl+cfmks. s-dnn student model outperforms cfmks model absolute reduction also outperforms drjlfront-back model absolute reduction eval set. table also find multi-cond drsl obtain small gain baseline. drjl-frontback achieves improvement drsl. results consistent results fig. student model achieve better performance teacher model higher accuracy. s-blstm student model obtains absolute reduction s-dnn student model blstm teacher model achieves absolute reduction teacher model eval set. best student model outperforms baseline model conventionally-trained models. main reasons. teacher model capture accurate better phoneme features close-talking data. contrast phoneme features farfield data distorted reverberation noise. soft labels computed teacher model contain information underlying label distributions compared hard labels. thus student model easier learn well using accurate richer information. s-dnn student model outperforms cfmks model. main reason output layers stronger discriminative ability hidden layers. cfmks method shares knowledge hidden layers. proposed method transfers knowledge output layers. addition increases accuracy teacher models yield similar increases performance student model. teacher model higher accuracy student model train well using accurate soft labels. paper proposes distill knowledge teacher model student model using parallel data improve performance far-field speech recognition tasks. student model trained mimic output distribution teacher model. thus realized minimizing divergence output distributions models. experimental results corpus show best student model achieves absolute reduction compared conventionally-trained baseline models. results also show increases accuracy teacher model yield similar increases performance student model. moreover proposed method need extra computation cost decoding. future work plan ensemble teacher model improve performance student model apply approach tasks. hinton deng dahl mohamed jaitly senior vanhoucke nguyen nsainath deep neural networks acoustic modeling speech recognition shared views four research groups signal processing magazine ieee vol. kumatani mcdonough microphone array processing distant speech recognition close-talking microphones far-field sensors signal processing magazine ieee vol. weninger erdogan watanabe vincent roux hershey schuller speech enhancement lstm recurrent neural networks application noise robust proceedings lva/ica hinton vinyals dean distilling knowledge neural network neural information processing systems workshop deep learning representation learning workshop mccowan carletta kraaij ashby bourban flynn guillemot hain kadlec karaiskos meeting corpus proceedings international conference methods techniques behavioral research vol. senior beaufays long short-term memory based recurrent neural network architectures large vocabulary speech recognition proceedings interspeech povey ghoshal boulianne burget glembek goel hannemann motlicek qian y.m. schwarz silovsky stemmer vesely kaldi speechrecognition toolkit proceedings asru himawan motlicek imseng potard learning feature mapping using deep neural network bottleneck large vocabulary speech recognition proceedings icassp chen watanabe erdogan hershey speech enhancement recognition using multi-task learning long short-term memory recurrent neural networks proceedings interspeech mimura sakai kawahara joint optimization denoising autoencoder acoustic model based multitarget learning noisy speech recognition proceedings interspeech", "year": "2018"}