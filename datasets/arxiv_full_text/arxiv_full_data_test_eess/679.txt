{"title": "Multi-level Attention Model for Weakly Supervised Audio Classification", "tag": "eess", "abstract": " In this paper, we propose a multi-level attention model to solve the weakly labelled audio classification problem. The objective of audio classification is to predict the presence or absence of audio events in an audio clip. Recently, Google published a large scale weakly labelled dataset called Audio Set, where each audio clip contains only the presence or absence of the audio events, without the onset and offset time of the audio events. Our multi-level attention model is an extension to the previously proposed single-level attention model. It consists of several attention modules applied on intermediate neural network layers. The output of these attention modules are concatenated to a vector followed by a multi-label classifier to make the final prediction of each class. Experiments shown that our model achieves a mean average precision (mAP) of 0.360, outperforms the state-of-the-art single-level attention model of 0.327 and Google baseline of 0.314. ", "text": "abstract—in paper propose multi-level attention model solve weakly labelled audio classiﬁcation problem. objective audio classiﬁcation predict presence absence audio events audio clip. recently google published large scale weakly labelled dataset called audio audio clip contains presence absence audio events without onset offset time audio events. multi-level attention model extension previously proposed single-level attention model. consists several attention modules applied intermediate neural network layers. outputs attention modules concatenated vector followed multi-label classiﬁer make ﬁnal prediction class. experiments shown model achieves mean average precision outperforms state-of-the-art single-level attention model google baseline audio classiﬁcation aims predict presence absence audio events audio clip. attracted many interests recent years many applications multimedia information retrieval public surveillance many audio datasets relatively small compared large image dataset imagenet example urban sound dataset contains hours urban sound records samples. esc- dataset consists environmental recordings across classes. detection classiﬁcation acoustic scenes events challenge datasets comprised several hours data. recently google published large scale audio classiﬁcation dataset called audio consisting hours million human-labeled second audio clips covering audio categories. audio audio clip contains several labels speech park audio weakly labelled dataset presence absence audio events known audio clip without knowing onset offset time audio events. weakly labelled dataset duration audio events varies depending audio categories. audio events audio clip last several seconds speech audio events last hundreds milliseconds gunshot. solve weakly labelled data problem many methods proposed multiple instance learning applied weakly labelled audio classiﬁcation later kong proposed single-level attention model audio classiﬁcation outperforms method google baseline deep neural network system audio classiﬁcation. model consists three fully connected layers followed attention module. motivation attention module different segments audio clip contributes differently classiﬁcation audio clip. example segments containing event attended segments containing irrelevant noise ignored. shortcoming single-level attention model substantial information intermediate neural network layers disregarded. many works explored features intermediate neural network layers contains rich information. explored audio classiﬁcation performance improved concatenating features different intermediate neural network layers. addition using multi-level features found effective audio tasks also vision tasks. meng extracted features different layers deep cnn. features concatenated representation signiﬁcantly outperforms non-concatenated features inspired success multi-level representation expand kong’s model multi-level attention model. firstly attention modules used intermediate neural network layers. then outputs attention modules concatenated vector. finally fully connected layer sigmoid activation function utilized predict presence probability class. paper organized follows. section introduces related works. section introduces single-level attention model section describes proposed multilevel attention module. section shows experimental results. section concludes forecasts future work. audio classiﬁcation audio classiﬁcation attracted many attention recent years. representative challenges including dcase dcase dcase hidden markov models used model audio events negative matrix based methods applied learn dictionary audio events recently neural network based methods including fully connected neural networks convolutional neural networks implementation single-level attention model embedded mapping shown fig. ﬁrst part femb modeled three fully connected neural layers units. second part attention module described equation attention non-negative mapping classiﬁcation mapping modeled softmax function sigmoid function respectively. normalization applied ensures attention normalized. finally prediction obtained element-wise multiplication classiﬁcation output normalized attention output. many works explored using multi-level features intermediate layers neural networks promote audio image classiﬁcation performance propose extend single-level attention model section multi-level attention model paper. architecture proposed multi-level attention model shown fig. instead applying single-level attention model fully connected neural network multiple attention modules applied intermediate layers well. attention modules capture different level information. denote feedforward mappings activations intermediate layers number embedded mappings. feed-forward neural network written attention module concept attention module ﬁrst introduced natural language processing attention module allows deep neural networks focus relevant instances ignore irrelevant instances bag. successfully applied machine translation face detection image classiﬁcation captioning also utilized domain audio classiﬁcation audio consists million samples. classes current version. audio multilabel dataset audio clip several labels. google created audio transfer learning. pretraining stage billion -second audio clips youtube covering classes collected called youtube spectrogram size along time frequency axis extracted feature audio clip. then resnet- model trained using youtube data. trained resnet- later used feature extractor. pre-training stage million -second audio clips covering classes collected. spectrogram audio clip presented trained resnet- model extract bottleneck features. process audio clip compressed bottleneck features call collection features sample. feature dimension million samples constitute audio set. illustrate notation t-th bottleneck feature dimension sample audio bottleneck features. number classes. single-level attention model bottleneck feature presented trainable embedding mapping femb extract embedded feature concatenation embedded features. non-negative function determines much embedded feature attended ignored denotes classiﬁcation output embedded feature attention module ability ignore irrelevant sound segments background noise silences attend sound segments audio events. models shown table model -a--a represents attention modules applied fully connected layers. model -a--a-a represents three attention modules applied fully connected layers. fully connected layer embedded mappings consists hidden units followed relu activation function dropout used prevent overﬁtting dropout rate batch normalization applied speed training prevent overﬁtting. weights biases default-initialized keras adam optimizer learning rate used. batch size setting hyperparameters follows conﬁguration code made publicly available area true positive-false positive rate curve. true positive rate probability correctly classifying positive sample. false negative rate probability incorrectly classifying negative sample positive. google found awkward scaling hence another metric d-prime simple deterministic function applied. d-prime separation means unit-variance gaussians. assume score distributions positive negative samples unit-variance gaussians. then calculate d-prime directly auc. equation shows below fig. average precision results single-level multi-level attention models nine randomly selected classes. left black bar-graph scaled y-axis left-side represents relative lowest among models class. example lowest among models class \"speech\" relative class -a--a right brown bar-graph scaled y-axis right side represents absolute example -a--a class \"speech\" separately. ﬁrst rows table show results google’s benchmark without attention model kong’s result single-level attention model multi-level attention models outperform google’s baseline singlelevel attention model d-prime. best multi-level attention model -a--a using attention modules intermediate layers achieved outperforms singlelevel attention model google’s baseline system. reason good performance using multilevel attention model multi-level features extracted intermediate layers provide various representations attention module ﬁlter unrelated information feature. addition different classes favor different layer features last fully connected layer multi-level attention model automatically select best feature class weight parameters. comparing variants single-level attention model observed performance notably degrades number fully connected layers increased. attribute feature extracted deep fully connected layer worse discriminative power shallow layer addition investigate variants single-level multi-level attention model comparing average precision nine randomly selected classes shown figure class color bars plotted relative improvement bars plotted absolute classes speech whoop close contrast many classes breathing lower figure shows multi-level attention models don’t always achieve better performance classes single-level attention models. class \"piano\" model outperforms models -a--a--a -a--a. also observe different classes favor different models. example classes \"speech\" \"whoop\" \"breathing\" \"guitar\" \"train\" \"emergence vehicle\" favor model -a--a. however class \"groan\" favors model -a--a--a. overall ensure performance classiﬁcation consistently increases classes multi-level features concatenated -a--a best architecture. work introduced multi-level attention model addressing weakly labelled audio classiﬁcation problem audio set. experimental results showed effectiveness concatenating multi-level features. best result multi-level attention model successfully exceeds google’s benchmark previous state results. future work plan combine multi-scale multi-level features together train audio set. hershey chaudhuri ellis gemmeke jansen moore plakal platt saurous seybold architectures large-scale audio classiﬁcation acoustics speech signal processing ieee international conference srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol. ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning zhou khosla lapedriza oliva torralba learning deep features discriminative localization computer vision pattern recognition ieee conference ieee vijayanarasimhan grauman keywords visual categories multiple-instance learning forweakly supervised object categorization computer vision pattern recognition cvpr ieee conference zhao j.-y. milcut sweeping line multiple instance learning paradigm interactive image segmentation proceedings ieee conference computer vision pattern recognition duan w.-h. tsang text-based image retrieval using progressive multi-instance learning computer vision ieee international conference ieee amores multiple instance classiﬁcation review taxonomy comparative study artiﬁcial intelligence vol. mesaros heittola diment elizalde shah vincent virtanen dcase challenge setup tasks datasets baseline system dcase -workshop detection classiﬁcation acoustic scenes events deng dong socher l.-j. fei-fei imagenet large-scale hierarchical image database computer vision pattern recognition cvpr ieee conference ieee stowell giannoulis benetos lagrange plumbley detection classiﬁcation acoustic scenes events ieee transactions multimedia vol. gemmeke ellis freedman jansen lawrence moore plakal ritter audio ontology human-labeled dataset audio events ieee icassp multi-level multi-scale feature aggregation using pretrained convolutional neural networks music auto-tagging ieee signal processing letters vol. roth farag h.-c. shin turkbey summers deeporgan multi-level deep convolutional networks automated pancreas segmentation international conference medical image computing computer-assisted intervention. springer y.-t. peng c.-y. m.-t. k.-c. tsai healthcare audio event classiﬁcation using hidden markov models hierarchical hidden markov models multimedia expo icme ieee international conference bisot serizel essid richard supervised nonnegative matrix factorization acoustic scene classiﬁcation ieee aasp challenge detection classiﬁcation acoustic scenes events kong sobieraj wang plumbley deep neural network baseline dcase challenge proceedings dcase choi fazekas sandler automatic tagging using deep convolutional neural networks arxiv preprint arxiv. bahdanau bengio neural machine translation jointly learning align translate arxiv preprint arxiv. kiros courville salakhudinov zemel bengio show attend tell neural image caption generation visual attention international conference machine learning", "year": "2018"}