{"title": "Visual speech recognition: aligning terminologies for better  understanding", "tag": "eess", "abstract": " We are at an exciting time for machine lipreading. Traditional research stemmed from the adaptation of audio recognition systems. But now, the computer vision community is also participating. This joining of two previously disparate areas with different perspectives on computer lipreading is creating opportunities for collaborations, but in doing so the literature is experiencing challenges in knowledge sharing due to multiple uses of terms and phrases and the range of methods for scoring results.  In particular we highlight three areas with the intention to improve communication between those researching lipreading; the effects of interchanging between speech reading and lipreading; speaker dependence across train, validation, and test splits; and the use of accuracy, correctness, errors, and varying units (phonemes, visemes, words, and sentences) to measure system performance. We make recommendations as to how we can be more consistent. ", "text": "exciting time machine lipreading. traditional research stemmed adaptation audio recognition systems. computer vision community also participating. joining previously disparate areas different perspectives computer lipreading creating opportunities collaborations literature experiencing challenges knowledge sharing multiple uses terms phrases range methods scoring results. particular highlight three areas intention improve communication researching lipreading; effects interchanging speech reading lipreading; speaker dependence across train validation test splits; accuracy correctness errors varying units measure system performance. make recommendations consistent. machine lipreading classiﬁcation speech visual cues speaker long niche research problem ﬁeld speech processing. however recent developments deep learning attracted signiﬁcant interest advancements computer vision machine learning communities. ﬁrst machine lipreading approaches adaptations conventional audio-based speech recognition systems. challenges machine lipreading common acoustic recognition require different approaches whereas unique visual channel reasonable assert speech processing computer vision techniques complementary solving issues associated machine lipreading. however important that bridge areas research terminology performance metrics somewhat standardised. paper address potential discrepencies terminology experimental setup performance reporting appear speech vision publications. structure copyright document resides authors. distributed unchanged freely print electronic forms. paper follows; ﬁrst clarify distinction speech reading lipreading. discuss challenges speaker dependence lipreading systems deﬁne scope problem. next summarise different metrics currently used report performance lipreading machines. finally suggest recommendations moving forward understanding certain terms compare scoring methods. speech reading human readers interpret speech using information provided whole face body since knowledge facial expression gaze body gestures often helps provides semantic context makes decoding speech easier. computer science domain machine speechreading systems usually face decode speech content. figure contains examples region image might used machine speech reading. lipreading interpretation speech motion lips alone region image considered early machine lipreading approaches contain information regarding upper facial expression body language. human lipreaders read lips fact read cues provided speaker’s entire body. people including perfect hearing visual information speaker’s face body decode speech available. cases example noisy public house lipreader focus lips speaker prioritise cues lips information discount value data. rather focuses ones attention useful. difﬁcult track lips speaker isolation. lips skeletal structure deformable surface. orbicularis oris muscle encircles mouth allows puckering protrusion also plays role closure. composed four interlacing independent quadrants gives appearance circularity ﬁbers orbicularis oris connect parts face. figure ﬁbers cheek muscles decussate around lips strongly control motion. connection ﬁbers chin nose smaller signiﬁcant inﬂuence motion. often track whole face rather lips throughout video extra structure rest face enables easier tracking robust ﬁtting. example using active appearance models achieve better tracking track contour face facial features addition lips. informal terms used literature full face lips different stages lipreading process important researchers explicitly clarify whether features using derived full face lips. necessary reproducibility also enables gauge beneﬁt approach. first clarify mean speaker dependence independence lipreading. speaker independence machine lipreading achieved classiﬁcation models generalise spoken utterances talkers contained within training set. system works closed speakers tested speakers outside training assume approach speaker dependent. explain examples dataset contains utterances speaker build speaker dependent lipreading system example utterances training samples test samples. achieve speaker independence sufﬁcient separate speciﬁc utterances speaker i.e. sentences speaker train sentences test. alternatively speaker speaker dataset utterances speaker training samples build classiﬁer would test samples speaker vice versa. speaker independent lipreading figure ensures model learning classiﬁer biased identity speaker generalisable speakers. classiﬁcation methods require data divided train validation test sets validation contain speakers training speakers speakers must remain distinct test speaker independence goal. figure note duplicate speakers training validation sets must split samples i.e. sample speaker either training validation. separate model built speaker. modern approaches exploit extremely large datasets train speaker independent models good performance. sadly datasets publically available. challenge speaker independent machine lipreading discussed concluded multispeaker classiﬁers accuracy degrades signiﬁcantly offered exclusive explanations; hope generalisation methods large freely available audio-visual datasets addressed meantime studies attempted measure inﬂuence speaker dependence example understanding inﬂuence speaker identity speaker independence problem enable researchers tackle speaker adaptation question full assessment speaker identity beyond scope review. possible explanation speaker identity signiﬁcant affect lipreading implementation conventional tracking methods. tracking faces videos choice however features become speaker speciﬁc particularly models encapsulate appearance information meaning speaker speciﬁcity ingrained features thus making speaker generalisation challenging training classiﬁcation model. example table show size feature vectors representing tracked region speakers rmav audio-visual dataset split shape appearance. whilst shape lips speaker represented remarkably consistent number features number appearance features vary means different faces require appearance models variable complexity accurately represent particular speech movements generalisation difﬁcult. methods reporting performance machine lipreading adopted audio speech recognition systems. common published ﬁgures correctness accuracy shown equations respectively total number labels ground truth number deletion errors represents number substitution errors number insertion errors comparing ground truth transcript recognition transcript. however metrics used different ways machine lipreading; ﬁrstly measuring performance classiﬁer output labeled classiﬁer unit secondly measuring performance system using dictionary language decoder address variations turn. speech distinguish type error meaningful impact interpretation. difference estimated output grammatically correct simply understandable. threshold reading performance depend upon purpose. general conversation deaf community understanding intent communicator’s speech prioritised precise transcription. however security settings evidence gathering exact conﬁdent transcriptions essential remove ambiguity speaker uttered. explain types error shown equations example. suppose ground truth utterance john wanted visit shop groceries\". classiﬁers produce different outputs. deletion errors. possible output john wanted visit groceries\" three words missing; ‘to’ ‘shop’ ‘buy’. instance deletion errors. insertion errors. another possible output john wanted visit visit shop groceries word ‘visit’ included twice. insertion error. substitution errors. finally achieved classiﬁer output john wanted shop shop groceries\". word ‘shop’ identiﬁed word ‘visit’ substitution error. however whilst make distinction types error encounter standard practice weight inﬂuence error uniformly. study satki instead weights based upon brain signals visual speech perception. satki report insertion errors greatest negative inﬂuence understanding suggest weight errors. suggest insertion error distraction intended message. deletion errors assigned weight suggest error weighted higher because although data absent context offered preceding succeeding labels enables understanding though lexical interpolation prior language knowledge. substitutions weighted hypothesis difference visual classes. means class substitutions output transcript likely closely related classes mislead interpreter. machine lipreading attempts interpret words spoken visual representation sounds uttered. means levels ‘translation’ within process visual gestures phonemes phonemes words. previous literature reports lipreading performance based different units makes comparing performance difﬁcult. report word error rate others viseme error rate others accuracy units attempts made compare phonemes visemes boost e.g. visemes formally deﬁned understood omit neither. possible example build word classiﬁer followed language model measured terms viseme correctness. system would bizarre none-the-less possible. table shows sensible possibilities. position deﬁnitively select ‘right’ units dictate ‘best’ metric choice researcher decide. however make simple recommendation would help quickly easily compare results. metric subscript notation classiﬁer output superscript notation network/dictionary output. subscript could left column table superscript could right column table example would represent accuracy phoneme classiﬁers would represent viseme error rate using viseme based language model. would informative report scores achieved stages lipreading system understand whether recognition performance dependent language decoding step well-trained classiﬁers vary different lipreading system architectures. often useful visualise report results confusion matrix count number times unit recognised confused another reading values confusion matrix choices. either count probability class pr{c| inverse probability ˆc|c} represents single class. described different questions former looking class latter this class it?’. whilst researchers former yields higher accuracy scores investigating inter-class variabilities useful inverse probabilities. important ensure understanding classiﬁer output recommend authors clear values accuracy scores calculated fair benchmarking. uncommon computer vision literature report values measure classiﬁcation performance e.g. however inter-class variation phonemes visemes much smaller than example within images representing cats dogs building landscape. image confused reasonable confusion building. however issue causes speech classiﬁed output transcript signiﬁcantly different meaning resulting confusion talkers. imagine saying ‘pass salt’ misclassiﬁcation classes transcribed ‘pass malt’. phoneme different single substitution signiﬁcant. thus speech researchers general rule report result means original intention recognised accurately. thus recommend values reported value included also. note examples class confusions might useful understand relationships certain classes whether phonemes visemes. authors explicit whether developing speech lipreading systems community effectively compare methodologies. possible authors encouraged make code data available possible data clearly described approach reproduced. clariﬁed deﬁnition speaker dependent machine lipreading authors carefully consider split training validation test data prior model training. compare performance suggested simple notation suggest calculating performance inverse probabilities whilst short-term reduce accuracy scores long-term encourage recognition unknown evolving words. recommend values reported accuracy included separately alongside unit conclude mini review machine lipreading summarise clear differentiation machine lipreading speech reading. important future researchers correct terminology future publications help community understand data conclusions drawn upon. also discussed challenge speaker independence lipreading showing stem feature extraction method training parameters individual speakers. furthermore reviewed many inﬂuences accuracy scoring publications different ﬁelds recommended notation help compare results future. ﬁnal thought; fundamental motivation lipreading ability understand speech audio channel hampered noise. therefore essential future work includes acoustically challenging speech environments audio channel recognised. exciting renewed interest difﬁcult challenge research communities. ideas fresh perspectives hope robust speaker independent machine lipreading wild become reality. helen bear. visual gesture variability talkers continuous visual speech. british machine vision conference deep learning reading workshop. bmvc helen bear gari owen richard harvey barry-john theobald. observations computer lip-reading moving dream reality. spie security+ defence pages g–g. international society optics photonics ./.. helen bear stephen richard harvey. speaker independent machine reading speaker dependent viseme classiﬁers. joint international conference facial analysis animation audio-visual speech processing pages isca helen bear richard harvey barry-john theobald yuxuan lan. finding phonemes improving machine lip-reading. joint international conference facial analysis animation audio-visual speech processing pages isca classiﬁcation deep convolutional neural networks. weinberger neural pages associates -imagenet-classification-with-deep-convolutional-neural-networks. pdf. sakti odagaki sasakura neubig toda nakamura. eventrelated brain potential study impact speech recognition errors. signal information processing association annual summit conference asia-paciﬁc pages ./apsipa... kwanchiva thangthai helen bear richard harvey. comparing phonemes visemes based lipreading. british machine vision conference deep learning reading workshop. bmvc", "year": "2017"}