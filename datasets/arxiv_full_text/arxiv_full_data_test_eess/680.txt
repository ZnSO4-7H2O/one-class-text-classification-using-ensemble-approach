{"title": "Linear networks based speaker adaptation for speech synthesis", "tag": "eess", "abstract": " Speaker adaptation methods aim to create fair quality synthesis speech voice font for target speakers while only limited resources available. Recently, as deep neural networks based statistical parametric speech synthesis (SPSS) methods become dominant in SPSS TTS back-end modeling, speaker adaptation under the neural network based SPSS framework has also became an important task. In this paper, linear networks (LN) is inserted in multiple neural network layers and fine-tuned together with output layer for best speaker adaptation performance. When adaptation data is extremely small, the low-rank plus diagonal(LRPD) decomposition for LN is employed to make the adapted voice more stable. Speaker adaptation experiments are conducted under a range of adaptation utterances numbers. Moreover, speaker adaptation from 1) female to female, 2) male to female and 3) female to male are investigated. Objective measurement and subjective tests show that LN with LRPD decomposition performs most stable when adaptation data is extremely limited, and our best speaker adaptation (SA) model with only 200 adaptation utterances achieves comparable quality with speaker dependent (SD) model trained with 1000 utterances, in both naturalness and similarity to target speaker. ", "text": "speaker adaptation methods create fair quality synthesis speech voice font target speakers limited resources available. recently deep neural networks based statistical parametric speech synthesis methods become dominant spss back-end modeling speaker adaptation neural network based spss framework also became important task. paper linear networks inserted multiple neural network layers ﬁne-tuned together output layer best speaker adaptation performance. adaptation data extremely small low-rank plus diagonal decomposition employed make adapted voice stable. speaker adaptation experiments conducted range adaptation utterances numbers. moreover speaker adaptation female female male female female male investigated. objective measurement subjective tests show lrpd decomposition performs stable adaptation data extremely limited best speaker adaptation model adaptation utterances achieves comparable quality speaker dependent model trained utterances naturalness similarity target speaker. given adequate amount training data target speaker always build acoustic model generates speech similar target speaker herself. unfortunately time getting enough data target speaker trivial task. building voices limited data phoneme coverage could lead really poor voice quality intelligibility. reusing information existing source speaker models speaker adaptation obtain satisfactory voice font target speaker using limited target speaker data. speaker adaptation saves labor massive recording manually transcription checking ﬁnally makes cost creating voices small acceptable. conventional hidden markov model based speech synthesis system adaptation methods ﬁrstly build average voice using multiple speakers’ data conduct speaker adaptation huge average model small amount target speaker data compared large data requirement building model speaker adaptation adapt speaker independent model much smaller amount target speaker data. many effective speaker adaptation methods proposed hmm-based speech synthesis framework. maximum likelihood linear regression originally developed automatic speech recognition tasks extended speech synthesis mllr mean vectors state distributions average voice model transformed target speaker dependent model linear transformation. hidden semimarkov model based mllr adaptation transforms distributions spectrum pitch model also distribution duration model. besides constrained structural maximum posterior linear regression adaptation simultaneously transforms mean vectors co-variance matrices state output state duration distributions hsmms speech synthesis speaker adaptation. recent years neural networks based speech synthesis dominates back-end acoustic modeling speech synthesis powerful modeling capacity. it’s proved nn-based speech synthesis system obtains better voice quality conventional hmm-based method number model parameters many research works done investigate speaker adaptation nn-based speech synthesis. combining information multiple speakers multi-speaker proposed. it’s assumed difference among different speakers learned different output layers. speaker owns output layer hidden layers shared among speakers. different levels speaker adaptation performed. i-vectors augmented linguistic features represent speaker identity information input level learning hidden unit contributions used conduct model adaptation middle level. finally feature space transformations implemented output layer. also i-vector speaker code representing speaker combined linguistic features input features neural network based acoustic model. paper linear networks based speaker adaptation approach proposed speech synthesis. inspired linear networks based adaptation method speech recognition tasks introduce multiple layers speech synthesis structure ﬁne-tune together output linear layer speaker adaptation. moreover lrpd decomposition conducted remove redundant free parameters adaptation data small. objective subjective experiments show proposed methods render good adaptation ability terms naturalness similarity target speaker. remainder paper organized follows section introduces adaptation framework paper including multi-task dnn-blstm source acoustic model speech synthesis baseline describing framework based speaker adaptation introducing lrpd decomposition based method detail section evaluates adaptation methods using objective measurement subjective tests section draws conclusions ﬁnally. similar back-end base acoustic model shown left side fig. multi-task dnn-blstm hybrid model. input features back-end base acoustic model linguistic features including binary answers questions linguistic contexts numeric values e.g. absolute positions different levels units. output acoustic features include mel-cepstral coefﬁcients logarithmic fundamental frequency values band aperiodicities unvoiced/voiced identity. layer designed input features better bottom feature extraction leads faster overall convergence. left side fig. mel-cepstral coefﬁcients band aperiodicities unvoiced/voiced identity separate output layers sharing lower layers. preliminary experiments show multi-task learning renders stable synthesis voice large single output layer acoustic features. back-end model described section serves source structure following experiments. based adaptation originally explored speech recognition tasks structure shown right side fig. inserted multiple layers source nn-based acoustic model. according different positions based adaptation method includes linear input network linear hidden network linear output network inserted positions successive hidden layers. adaptation process ﬁrstly inserted speciﬁc positions source model linear transformation matrix initializing identity matrix initialized then adaptation data target speaker inserted layers optimized back-propagation keeping layers ﬁxed. it’s worth noting combined train better model achieve good adaptation voice quality. lrpd decomposition proposed reduce number free adaptation parameters inserted lrpd decomposition reduce speaker-speciﬁc footprint full adaptation matrix without signiﬁcant loss word error rate speech recognition tasks lrpd decomposition restructures adaptation matrix superposition diagonal matrix low-rank matrix shown fig. equation equation usk×r vsr×k small matrices dimension respectively. dk×k diagonal matrix. number parameters full-ln number parameters lrpd-ln much smaller full-ln. moreover lrpd-ln speaker adaptation suitable full-ln small target speaker adaptation data number parameters ﬁne-tuned also smaller. paper diagonal matrix dk×k initialized identity matrix ﬁne-tuned ﬁxed adaptation stage. it’s shown keeping matrix ﬁxed renders comparable performance ﬁne-tuning diagonal matrix identity matrix following experiments work. lrpd-ln trained ways initialize small matrices usk×r vsr×k randomly train inserted parameters target speaker data decompose well trained full-ln seed model ﬁne-tune target speaker’s data. preliminary experiments training methods shows comparable results simplicity train lrpd-ln ﬁrst method following experiments. adaptation experiments adaptation utterances number target speaker varies target speaker utterances held validation objective measurement another utterances held testing subjective tests. here focus adaptation scenario source speaker target speaker. source speaker model trained approximately utterances used source model speaker adaptation. investigate adaptation effect speakers different distances three kinds adaptation experiments designed female female male female female male. adaptation female female regarded adaptation similar speakers regarded adaptation far-away speakers since speakers different genders always different characters terms pitch spectrum. moreover speaker dependent systems target speaker different number utterances compared reference experiments. input linguistic features contain -dim binary features numerical features it’s -dim totally. output acoustic feature vectors -dim comprising -dim mel-cepstral coefﬁcients -dim -dim band aperiodicities -dim ﬂag. linear interpolation done unvoiced segments modeling input linguistic features output acoustic features normalized -mean unit-variance. synthesis stage predicted acoustic features de-normalized sent vocoder world synthesis. nodes lrpd-ln number parameters lrpd-ln full-ln training model adaptation model minimum square error adopted training criterion stochastic gradient descent based back-propagation used optimize model parameters. training limited data manually tuned learning rate corresponding hyper-parameters avoid overﬁtting. informal experiments it’s found inserting cannot achieve satisfactory adaptation results inspired combine ﬁne-tuning output layer full-ln/lrpd-ln based method following experiments. different insert positions different combinations compared preliminary experiments it’s found insert last hidden layer output layer together would achieve best performance setting used following experiments. objective measurement subjective tests conducted includes mel-cepstral experiments. objective measurement distortion root mean squared error unvoiced/voiced prediction errors overall multitask output layer. mean opinion score tests naturalness similarity target speaker conducted subjective comparison. utterance listened times different listeners. fig. shows relationship target speaker adaptation utterances amount system objective measurement detailed above. means model target speaker training data adaptation. means adaptation ﬁne-tuning output layer source model. full-ln lrpd-ln shown fig. number training/adaptation utterances increasing objective distance systems becomes closer indicates systems perform better data. compared model target speaker adaptation methods show better performance amount adaptation data. experimental results also reveal that ﬁne-tuning together inserted performance basic ﬁne-tuning output layer adaptation method jumps pink curve blue green curve. results indicate ﬁne-tuning output transformation layer without tuning hidden layers possible fully adapt target voice. case specially adaptation data available ol+ln method becomes huge adaptation data available. since lrpd-ln full-ln always ﬁne-tuned experiments lrpd-ln full-ln used indicate ol+lrpd-ln ol+full-ln short. moreover full-ln worse lrpd-ln adaptation utterances limited full-ln introduces much speaker-speciﬁc parameters causes over-ﬁtting lack data. adaptation utterances adequate full-ln better lrpd-ln. mainly because number speaker speciﬁc parameters lrpd-ln limited ﬁxed decomposition dimension naturalness similarity target speaker different systems shown fig. subjective tests performance system degrades quickly number adaptation utterances decrease lrpd-ln based adaptation stable compared full-ln moreover it’s found full-ln lrpd-ln show better performance number utterances. fullln lrpd-ln adaptation utterances achieve similar performance utterances. different objective measurements lrpd-ln outperforms full-ln adaptation data reaches over-ﬁtting makes synthesis voice stable sometimes sounds weird unintelligible. still draw conclusion over-ﬁtting makes full-ln become worse adaptation utterances deﬁcient. shown fig. fig. conclusion section made. lrpd-ln stable fullln performance declines quickly decreasing training utterances number. also full-ln lrpd-ln based methods still show better performance utterances number. similarly adaptation utterances full-ln lrpd-ln adaptation achieve comparable performance utterances. addition interesting fig. adaptation voices voice larger adaptation extremely small. indicate adaptation works best target speaker male adaptation data limited. however shrinks quickly adaptation data becomes larger. overall speaking fig. reveals trend fig. fig. paper based speaker adaptation methods investigated speaker adaptation speech synthesis lrpd decomposition employed make adapted voice stable. conducting speaker adaptation female female male female female male experimental results show lrpd decomposition performs stable adaptation data extremely limited. also best model adaptation utterances achieves comparable quality model trained utterances naturalness similarity target speaker. najim dehak patrick kenny r´eda dehak pierre dumouchel pierre ouellet front-end factor analysis speaker veriﬁcation ieee transactions audio speech language processing vol. pawel swietojanski steve renals learning hidden unit contributions unsupervised speaker adaptation neural network acoustic models spoken language technology workshop ieee. ieee zhao daisuke saito nobuaki minematsu speaker representations speaker adaptation multiple speakers blstm-rnn-based speech synthesis space vol. hieu-thi luong shinji takaki gustav henter junichi yamagishi adapting controlling dnn-based speech synacoustics speech signal thesis using input codes processing ieee international conference ieee joao neto lu´ıs almeida mike hochberg ciro martins luis nunes steve renals tony robinson speaker-adaptation hybrid hmm-ann continuous speech recognition system roberto gemello franco mana stefano scanzio pietro laface renato mori linear hidden transformations adaptation hybrid ann/hmm models speech communication vol. yuchen qian feng-long frank soong synthesis bidirectional lstm based recurrent neural networks fifteenth annual conference international speech communication association yong zhao jinyu yifan gong low-rank plus diacoustics agonal adaptation deep neural networks speech signal processing ieee international conference ieee masanori morise fumiya yokomori kenji ozawa world vocoder-based high-quality speech synthesis system real-time applications ieice transactions information systems vol. junichi yamagishi masatsune tamura takashi masuko keiichi tokuda takao kobayashi training method average voice model hmm-based speech synthesis ieice transactions fundamentals electronics communications computer sciences vol. christopher leggetter philip woodland maximum likelihood linear regression speaker adaptation continuous density hidden markov models computer speech language vol. masatsune tamura takashi masuko keiichi tokuda takao kobayashi speaker adaptation hmm-based speech synthesis system using mllr third esca/cocosda workshop speech synthesis masatsune tamura takashi masuko keiichi tokuda adaptation pitch spectrum takao kobayashi hmm-based speech synthesis using mllr acoustics speech signal processing proceedings.. ieee international conference ieee vol. junichi yamagishi katsumi ogata yuji nakano juri isogai takao kobayashi hsmm-based model adaptation algorithms average-voice-based speech synthesis acoustics speech signal processing icassp proceedings. ieee international conference ieee vol. i–i. yuji nakano makoto tachibana junichi yamagishi takao kobayashi constrained structural maximum posteriori linear regression average-voice-based speech synthesis ninth international conference spoken language processing junichi yamagishi takashi nose heiga zhen-hua ling tomoki toda keiichi tokuda simon king steve renals robust speaker-adaptive hmm-based text-to-speech synthesis ieee transactions audio speech language processing vol. junichi yamagishi takao kobayashi yuji nakano katsumi ogata juri isogai analysis speaker adaptation algorithms hmm-based speech synthesis constrained smaplr adaptation algorithm ieee transactions audio speech language processing vol. yuchen qian frank soong multispeaker modeling speaker adaptation dnn-based acoustics speech signal processing synthesis ieee international conference ieee", "year": "2018"}