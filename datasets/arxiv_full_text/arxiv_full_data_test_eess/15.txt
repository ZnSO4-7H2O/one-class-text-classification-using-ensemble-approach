{"title": "Learning Sparse Graphs Under Smoothness Prior", "tag": "eess", "abstract": " In this paper, we are interested in learning the underlying graph structure behind training data. Solving this basic problem is essential to carry out any graph signal processing or machine learning task. To realize this, we assume that the data is smooth with respect to the graph topology, and we parameterize the graph topology using an edge sampling function. That is, the graph Laplacian is expressed in terms of a sparse edge selection vector, which provides an explicit handle to control the sparsity level of the graph. We solve the sparse graph learning problem given some training data in both the noiseless and noisy settings. Given the true smooth data, the posed sparse graph learning problem can be solved optimally and is based on simple rank ordering. Given the noisy data, we show that the joint sparse graph learning and denoising problem can be simplified to designing only the sparse edge selection vector, which can be solved using convex optimization. ", "text": "paper interested learning underlying graph structure behind training data. solving basic problem essential carry graph signal processing machine learning task. realize this assume data smooth respect graph topology parameterize graph topology using edge sampling function. graph laplacian expressed terms sparse edge selection vector provides explicit handle control sparsity level graph. solve sparse graph learning problem given training data noiseless noisy settings. given true smooth data posed sparse graph learning problem solved optimally based simple rank ordering. given noisy data show joint sparse graph learning denoising problem simpliﬁed designing sparse edge selection vector solved using convex optimization. graphs offer describe explain relationships complex datasets central entity modern data analysis data deluge prominent particular nodes graph denote entities edges encode pairwise relationship entities. entities referred graph signals. examples complex-structured data beyond traditional time-series include data residing brain networks gene networks social networks recommendation systems transportation networks good quality graph central graph signal processing machine learning task. paper interested problem learning hidden graph topology behind data. sheer quantity data motivated select simplest graphical models adequately explain data. particular interested learning sparse graph i.e. graph limited number edges adequately explains input data. realize this make simple widely used assumption data smooth respect discovered graph. contributions paper threefold. first model graph learning problem edge selection problem parameterize graph sparse edge sampling vector. particular proposed model provides elegant handle control number edges thus graph sparsity. second case true smooth graph signals given graph learning problem solved optimally solution based simple rank ordering. finally given noisy graph signals i.e. joint sparse graph learning denoising problem provide onestep solution based convex optimization well algorithm based alternating minimization. problem learning graph laplacian weighted adjacency matrix smooth graph signals considered before learning sparse graphs true graph signals problem consider section studied graph learning problem posed constrained optimization problem constraint valid adjacency matrices optimization problem solved using iterative primal-dual algorithm. contrast modelling greatly simpliﬁes solution simple rank ordering. modelling inspired problem design edge weights maximize algebraic connectivity graph addressed. joint graph learning denoising problem addressed i.e. problem study section alternating minimization algorithm proposed alternating graph learning denoising graph learning optimization problem involves search space valid graph laplacians. contrary show problem solved one-step boils design sparse edge sampling function. graph topology identiﬁcation also investigated assumption eigenvectors graph laplacian known much stronger assumption. although eigenvectors computed graph data stationary respect graph graph signals need always vertex stationary. case estimated eigenvectors error free limited data records. existing approaches graph sparsiﬁcation achieved penalizing ℓ-norm graph laplacian matrix adjacency matrix shift operator however explicit handle control number edges unlike proposed approach. related line research investigate computing sparse graphs approximate given graph spectrally means laplacian matrices similar quadratic forms. consider dataset real valued elements deﬁned vertices undirected graph vertex denotes nodes edge reveals connection nodes. refer datasets graph signals. assume length graph signals i.e. known. however edge known. therefore assume complete graph boolean linear programming problem admits explicit solution computing optimal solution straightforward. solved sorting entries ascending ordering. speciﬁcally solution entries equal indices corresponding smallest entries others zero computationally sorting algorithm costs parallel implementation computational complexity give another interpretation result following remark. remark suppose graph signal stochastic covariance matrix e{xxt rn×n then solution would select edges nodes highest crosscorrelation i.e. edge node variables strongly correlated. this express cost function rn×n sample data covariance matrix. recalling deﬁnition easy term nodes highly correlated sufﬁcient samples compute sample covariance matrix. modelling graph topology edge selection vector graph learning problem solved optimally using simple elegant solution controlled sparsity level whereas optimizing directly graph laplacian adjacency matrix leads complicated suboptimal solution explicit handle control graph sparsity. undirected graph topology basically determined graph laplacian matrix essentially reveals connectivity graph. denote graph laplacian matrix complete graph nonzero equal symmetric matrix expressed terms so-called incidence matrix rn×m denote subgraph edge |es| refer subgraph edges k-sparse graph. connect k-sparse graph sparse edge selection vector edge belongs edge subset otherwise. terms |es| means finally write laplacian matrix k-sparse graph function graph signal deﬁned vertices graph. smoothness spectral content signal depend underlying graph topology. laplacian quadratic form given quantiﬁes smooth graph signal respect underlying graph particular signal smoothest respect graph edges values lsx. suppose given graph signals denoted vectors collected matrix {xk}l interested recovering graph laplacian prior information graph signals smooth respect k-sparse graph. formally state following. boolean cardinality constraints problem exists simple analytical solution based sorting i.e. solution entries equal {cm}m indices corresponding smallest entries {cm}m zeros otherwise. iterations initialized randomly generating uniform distribution alternating minimization method computationally attractive consists simple known solutions iteration. however algorithm converges stationary point suffers choice initial estimate. algorithm proposed also along lines alternating minimization except graph learning step involves complicated optimization space possible valid laplacian matrices. convex relaxation avoid issues related initialization alternating minimization algorithm follows propose one-step solution based convex relaxation. rewrite formulation alternatively recover based smoothness assumption typically leastsquares problem solved tikhonov regularization enforce prior information noiseless graph signal smooth respect underlying graph. speciﬁcally following optimization problem solved problem noncovex boolean cardinality constraints coupling optimization variables second term provide methods solve ﬁrst straightforward approach based alternating descent second based convex relaxation. optimization problem solved using alternating mink= given imization respect {xk}l problem reduces linear system unknown admits closed form solution; given {xk}l reduces boolean linear programming problem admits analytical solution respect based rank ordering. observations suggest iterative alternating minimization algorithm yielding successive estimates {xk}l ﬁxed alternately speciﬁcally iterate given ﬁxed {xk}l iteration i.e. solve using matrix inversion relaxing cardinality constraint boolean constraints linear inequality constraints related constraint optimization problem convex this introduce variable fig. sparse graph learning colored dots indicate temperature values. noiseless case. graph edges recovered solving noisy case convex relaxation used recover graph edges using sdpt candidate graph edges learn subgraph edges. begin with consider noiseless case true graph signal assumed known graph learning case amounts solving sorting problem. shown fig. learnt graph edges edges present nodes share similar values. although proposed approach doesn’t always ensure well-connected graph clusters entities similar values. fig. shows cost proposed closed-form sorting solution optimal lower existing iterative solution next consider noisy setting training data before perform joint graph learning denoising. fig. show learnt graph edges based convex relaxation approach explained sec. fig. evaluate denoising performance based learnt graph using evaluation set. particular show mean squared error different values noise level mean squared error computed independent monte carlo experiments. one-step solution based convex optimization leads lower error compared alternating minimization approaches general converge stationary point. also holds method developed sec. however stress fact proposed alternating minimization computationally much less expensive compared iterative solution graph learnt noiseless setting perform well denoising. nevertheless simple solution used generate base graph reﬁned speciﬁc graph inference problems. studied problem learning sparse graph adequately explains data smoothness prior. model graph learning problem design sparse edge sampling function. words express graph laplacian terms edge selection vector. considered noiseless noisy setting. noiseless setting designing edge selection vector elegant boils simple low-complexity sorting problem. however presence noise propose computationally cheap alternating minimization algorithm well one-step convex relaxation based solution. wmamat standard off-the-shelf solver used solving semidefinite program large-scale problems computationally cheaper ﬁrst-order methods solving derived size linear matrix inequality depends size training data number nodes. temperature measurements collected across weather stations french region brittany learn graph explains observed data; fig. observations weather station available snapshots training remaining ones evaluation set. observation graph nodes shown fig. colored dots indicate different temperature readings. convex optimization problems solved using toolbox internally calls shuman narang frossard ortega vandergheynst emerging ﬁeld signal processing graphs extending high-dimensional data analysis networks irregular domains ieee signal process. mag. vol. sandryhaila moura data analysis signal processing graphs representation processing massive data sets irregular structure ieee signal process. mag. vol.", "year": "2016"}