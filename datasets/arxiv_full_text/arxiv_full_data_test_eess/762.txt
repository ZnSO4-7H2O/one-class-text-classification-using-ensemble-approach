{"title": "Multi-Hypothesis Visual-Inertial Flow", "tag": "eess", "abstract": " Estimating the correspondences between pixels in sequences of images is a critical first step for a myriad of tasks including vision-aided navigation (e.g., visual odometry (VO), visual-inertial odometry (VIO), and visual simultaneous localization and mapping (VSLAM)) and anomaly detection. We introduce a new unsupervised deep neural network architecture called the Visual Inertial Flow (VIFlow) network and demonstrate image correspondence and optical flow estimation by an unsupervised multi-hypothesis deep neural network receiving grayscale imagery and extra-visual inertial measurements. VIFlow learns to combine heterogeneous sensor streams and sample from an unknown, un-parametrized noise distribution to generate several (4 or 8 in this work) probable hypotheses on the pixel-level correspondence mappings between a source image and a target image . We quantitatively benchmark VIFlow against several leading vision-only dense correspondence and flow methods and show a substantial decrease in runtime and increase in efficiency compared to all methods with similar performance to state-of-the-art (SOA) dense correspondence matching approaches. We also present qualitative results showing how VIFlow can be used for detecting anomalous independent motion. ", "text": "propose fully exploit embodied nature robotic systems begin fusing heterogeneous sensor measurements early possible learning multi-hypothesis feed-forward model receives heterogeneous information estimates dense image correspondences avoid computationally heavy direct image matching subsequent optimization outlier rejection current approaches main contribution paper end-to-end trainable unsupervised deep neural network architecture that learns estimate dense visual correspondence/ﬂow remainder paper organized follows section presents related work; section outlines deep network approach fusing noisy heterogeneous sensory inputs describes network architecture; section describes experimental approach; section describes evaluation procedures; section presents discusses experimental results; section offers concluding abstract— estimating correspondences pixels sequences images critical ﬁrst step myriad tasks including vision-aided navigation visual-inertial odometry visual simultaneous localization mapping anomaly detection. introduce unsupervised deep neural network architecture called visual inertial flow network demonstrate image correspondence optical estimation unsupervised multi-hypothesis deep neural network receiving grayscale imagery extra-visual inertial measurements. viflow learns combine heterogeneous sensor streams sample unknown un-parametrized noise distribution generate several probable hypotheses pixel-level correspondence mappings source image target image. quantitatively benchmark viflow several leading vision-only dense correspondence methods show substantial decrease runtime increase efﬁciency compared methods similar performance state-of-the-art dense correspondence matching approaches. also present qualitative results showing viflow used detecting anomalous independent motion. state estimation size weight power computation constrained robotic systems limited lightweight low-power sensing computational hardware forced use. viewed complex embodied agents robotic systems generate access wide variety varied sensory information such popular approach mitigating negative inﬂuences noisy swap-c constrained sensors fuse estimates array heterogeneous sensors deployed robot. swap-c constrained gps-denied navigation greatly inﬂuenced philosophy sensor fusion approaches visual-inertial odometry sensor array commonly consist camera inertial measurement unit particularly successful gps-denied swap-c constrained localization navigation. vision-aided approaches localization generally require ﬁnding correspondence mapping scene elements sequential image frames. to-date solutions correspondence problem review) remain william nothwang branch chief micro nano devices materials branch sensors electron devices directorate army research laboratory adelphi william.d.nothwang.civmail.mil correspondence problem viewed part general problem determining images relate another. correspondence problem traditionally addressed closed-form analytical approaches review correspondence problem) recent bio-inspired deep neural network approaches estimate spatial transformations image pairs begun show increasing success. unsupervised siamese-like deep network architectures based multiplicative interactions triplet learning rules used successfully relationship learning images cost computational runtime. architectures require expensive computations performed source target images greatly increase model complexities high-dimensional nature image data. learning approaches relied explicit supervised labeling random decision forest based approaches semantic segmentation approaches supervised approaches require expensive time-consuming labeling ultimately limits size useable datasets. self-supervised visual descriptor approach used learning rule requires priori labeling points source image target image already aligned thus correspondence already solved training set. approach instead unsupervised requires image data training. estimating optical similar estimating correspondence. usually approaches dense large displacement optical require ﬁrst estimating correspondences images performing type variational joint energy-based optimization includes assumptions local smoothness data-driven correspondence matching leave many outliers closest visual match corresponding pixels images optical ﬂow. words correspondences neighboring pixels quite noisy show large discontinuities. approaches epicflow example apply methodology ﬁrst ﬁnding correspondences images using computing dense optical ﬂow. however number learningbased systems instead learn endto-end models optical image inputs still require extensive computation approaches take optical input egomotion estimation view synthesis approach learned local pixellevel shifts order render unseen views objects scenes. approach similar mappings learned second pathway architectures. besides different network structures inclusion global spatial transformer module largest difference method rather learning generate novel viewpoints objects scenes learn reconstruct source image using pixel locations target image order compute estimates correspondence ﬂow. used depth data input network provide viflow depth. critical given locations points image scene afﬁne transformation directly performed project points image plane time time ti+. viflow designed swap-constrained applications intensity information single imager might available viflow network input single grayscale intensity image uses local pathway information) infer depth non-rigidity single grayscale image color imagery better enables explicit depth learning directly receives depth data additionally trained based sampled image reconstructive loss). vision-aided egomotion estimation independent motion sources visual scene introduce error optical-ﬂow derived estimates egomotion. number approaches independent motion detection take dense optical input approach described paper could used estimate independent motion detection algorithms viflow could augmented directly separate ego-motion induced induced independently moving scene elements previous approaches used ground-truth pose difference ground-truth pose differences contaminated heteroscedastic noise show unsupervised network learn estimate true pose change global afﬁne transform plus localized pixel-level coordinate shifts contrast present results experiments networks instead provided sensor data form single grayscale image measurements. additionally experimented networks also received intentional information form surrogate feed-forward motor signal place actual motor command benchmark datasets used kitti euroc provide motor commands. data traditional fusion approaches assume priori sensor dynamics models known intrinsic/extrinsic calibration parameters. similarly intentional information come joysticked motor commands command velocities freely available robotic system traditional closed-form solutions estimating change perception induced commanded motion requires hand-crafted system models relate motor inputs changes self-measured sensory perceptions. contrastingly viflow network requires explicitly deﬁned forward model motor commands affect sensory perceptions models calibration parameters. instead viflow side-steps need hand-crafted modeling learns combine heterogeneous inputs directly data. viflow designed compute estimates visuo-inertial correspondence. receives input single grayscale intensity image taken time extravisual estimate camera motion mi→i+ time time ti+. goal network motion estimates mi→i+ grayscale image predict image coordinate scene element captured words viflow learns estimate correspondence pixels images ii+. network architecture thought extension autoencoder. however rather learning features minimizing reconstruction error input projected feature-space re-projected outputspace viflow trained minimizing reconstruction error input reconstruction based sampled values previously unseen target image ii+. similar network viflow learns generate several hypothesis reconstructions along series parallel pathways. hypothetical reconstructions enable increased robustness noisy inputs. sample viflow effectively trained draw unknown noise distribution using call winner-take-all euclidean lowest error hypothesis. loss computed hypothesis error backpropagated parameters pathway. thus parameters contributed winning hypothesis updated remaining parameters left untouched. seem like loss rule lead network optimize single pathway practice case viflow networks continued multiple pathways throughout training testing. ﬁrst pathway viflow global shifter. given motion estimate uses several fully-connected layers approximate transformation transformation learning compute parameters afﬁne transformation matrix. global shifter applies afﬁne transformation generate expected coordinate shifts form hxwx grid represents pixel locations sample target image second pathway viflow local shifter pathway. receives source image input uses convolutionaldeconvolutional encoder-decoder also generate hxwx fig. histogram error ground truth kmeans cluster exemplar assigned. note error euroc heavier tailed compared error kitti indicative larger variance motion transforms euroc. surrogate feed-forward motor signals direct motor-command inputs available k-means ﬁtting clusters used ground truth position differences generate noisy estimates approximate direction motion evaluated networks inputs present results show higher-level signal encodes intentional information affect network performance. require data well ground truth pixel points limited choice public datasets. example could middlebury mpi-sintel visual benchmark datasets include data. thus used kitti euroc datasets evaluation. image kitti euroc cropped middle pixel region inputs network. details dataset dataset speciﬁc data preparation follow below. total number usable exemplars vicon room scenarios signiﬁcantly smaller kitti odometry dataset. thus elected augment datasets euroc including pairs sequential frames pairs separated four frames. resulted total examples used training used testing. imu+sffms conditions lookahead could anywhere four frames thus anywhere inputs euroc models used vector size exemplars regardless lookahead size ﬁrst entries output pixel shifts. however shifts intended modify coordinate shifts calculated global shifter pathway instances true motion scene element cannot calculated using single global transform spatial transformation form modiﬁed spatial transformer module integral component viflow network architecture explanation helps elucidate workings viflow. perform spatial transformation assume output pixels deﬁned regular grid {gj} pixels forming output feature height width grid. represent afﬁne transformation target coordinates mapped source coordinates according pixels depth global shifter pathway would able accurately project pixels correct positions post-movement. however case best result global shifter accomplish correct projection points belong dominant plane. local shifter able apply corrections coordinate changes computed global shifter allow differing object depths scene non-rigidity. given source image viflow’s local shifter pathway learns compute localized shifts shifts computed global pathway. thus viflow generates ﬁnal coordinate locations correspond prior image capture next entries correspond following capture. exemplars ahead remainder vector zeros. lookaheads last entries zeros; lookaheads three last zeros; ﬁnally lookaheads four vector fully populated. generate surrogate sffms performed k-means clustering ground truth position differences generate clusters encoded one-hot vector. kitti odometry kitti used sequences excluding sequence corresponding drive online time publication. resulted total image pairs training experiments randomly selected image source used successive image target. corresponding data collected kitti datasets preceding following data included example yielding length vector. ssfms generated above. viflow-imu networks four layers size used generate afﬁne transformation matrices. imu+ffms conﬁgurations sources extra-visual motion estimates described above extra-visual modality processed four layers concatenated vector length convolutional-deconvolutional encoder-decoder composed local shifter pathway used convolutional kernels stride two. encoder used layers ﬁlters decoder reversed using ﬁlters. results described paper used local shifter pathway parameters. shown fig. output ﬁfth convolutional layer concatenated last layer global shifter pathway single layer size ﬁrst deconvolutional decoder layer. trained three networks condition dataset results presented highest performing network condition. networks trained desktop computer intel processor nvidia titan gpus. predicted pixel correspondence source target images evaluated ground truth correspondence correspondence computed deepmatching algorithm correspondence computed deformable spatial pyramid matching algorithm euroc kitti sceneflow datasets images explicitly regularization computed matches also evaluated viflow several deep optical networks designed perform additional regularization outlier removal epicflow flownetc flownet emphasized approaches benchmark viflow networks receive extra-visual motion inputs unable identify approach literature exact input/output domain. table presents results various viflow networks kitti sceneflow euroc datasets compared flownetc flownet epicflow deepmatching identity mapping. identity mapping results computed assuming zero shown provide perspective quantitative results presented table approaches tested. course noted comparison approaches viflow networks approaches provided motion prior form data sffms both. calculate performance/runtime quotient ratios average end-point error inverse runtime scaled viflow networks generate best quotients indicating high efﬁciency. however viflow signiﬁcantly outperformed aepe computationally heavier epicflow flownet networks conditions. table benchmark results mhide networks compared flownetc flownet epicflow deepmatching kitti euroc datasets endpoint error last columns calculated runtime/performance quotients kitti euroc ratios aepe inverse runtime scaled viflow-ssfms even hypothesis case viflow-sffms outperformed correspondence approaches euroc managed opposite kitti. general viflow-sffms networks showed greatest performance improvements function hypothesis pathways event eventually surpassed viflow-imu kitti -hypothesis case. reults likely related heavier tailed error distributions euroc seen fig. compared kitti ssfms inputs suggests additional motion variability euroc increased noise k-means estimates network able sufﬁciently learn distribution number hypotheses allotted. performance difference kitti euroc suggest performance tied variability underlying motion experienced respective vehicles lower-variability kitti motions allowing better ﬁtting k-means clustering opposite euroc. viflow-imu+ssfms kitti single hypothesis version viflow-imu+ssfms showed signiﬁcant performance increase compared viflow-imu viflow-ssfms. combined performance increase imu+ssfms networks suggest information measurements ssfms signals convey complementary information enable joint-reduction uncertainty. however noted ssfms inputs actual motor signals fig. sample anomalous independent motion detection. notice model-error induced distant objects could potentially confused independent motion last viflow residuals. viflow also holds promise visual anomaly detector. speciﬁcally detector independent motion inconsistent global motion inferred visually motion priors encoded measurements sffms inputs. presents optical renderings alongside ground-truth computed flownet shows correspondence matchings discovered viflow. expected learned network correlated ego-motion network generally fails predict large sources independent motion. images shown fig. residuals reconstructed viflow source outputs original source images ii). network trained motion prior help calculate visual transformations scene biased toward global motions implicitly outputs mappings correspond ego-motion. thus seen fig. areas high residual magnitude correspond regions containing independently moving scene elements euroc dataset kitti sceneflow dataset shown unsupervised deep network learn efﬁciently estimate visual inertial measurements. resulting runtimes networks substantially faster vision-only matching algorithms similar performance deep matching approaches. transferability error correction network receives measurements learns integrate transform camera reference frame directly transferring models trained dataset given camera conﬁguration another dataset different different camera conﬁguration currently possible. viflow receive form intrinsics imu/camera extrinsics input measurements pre-calibrated. area future exploration incorporating estimated intricics pre-calibrated measurements explicit imu-camera extrinsics create parametrized version viflow transferred various datasets. however unclear parametrized network perform compared current iteration viflow learns optimally integrate data transform camera frame minimize residual error. viflow strictly feed-forward model limited ability correct error online. multihypothesis approach allows network handle certain types structured unstructured noise speculate viflow show sensitivity shifts relative positioning camera well abrupt collisions. future work evaluate viflow sensitives investigate online models error correction mitigate effects. anomaly detection model error presented above potential viflow detector anomalous independent motion. taking residual between source image viflow-reconstructed source image high-magnitude regions interest correspond independently moving scene elements. however regions correspond rois model failed accurately predict another area future work expanding viflow anomalous motion detector building additional mechanism separates model error independent motion-induced error. potential path segmentation impose shape-related constraints analyzing residuals. example model errors tend pronounced edges objects leading narrow bands error either dimensions rarely both. contrastingly independent motions present larger spatial extent dimensions simultaneously. thus detecting blobs high-magnitude residuals classifying according patterns spatial extent lead efﬁcient separation anomalous independent motion model error. surrogate motor signals work used kmeans clustering ground-truth poses form surrogate feed-forward motor signal additional network. signal exhibited high error error characteristics unlikely match realworld motor input. future work need examine network performs input true feed-forward motor signal. mayer saikia keuper dosovitskiy brox flownet evolution optical estimation deep networks ieee conference computer vision pattern recognition vol. wohlhart lepetit learning descriptors object recognition pose estimation proceedings ieee conference computer vision pattern recognition wang gupta unsupervised learning visual representations using videos proceedings ieee international conference computer vision taylor shotton sharp fitzgibbon vitruvian manifold inferring dense correspondences one-shot human pose estimation computer vision pattern recognition ieee conference shotton glocker zach izadi criminisi fitzgibbon scene coordinate regression forests camera relocalization rgb-d images proceedings ieee conference computer vision pattern recognition brachmann krull michel gumhold shotton rother learning object pose estimation using object coordinates european conference computer vision. springer long shelhamer darrell fully convolutional networks semantic segmentation proceedings ieee conference computer vision pattern recognition hariharan arbel´aez girshick malik hypercolumns object segmentation ﬁne-grained localization proceedings ieee conference computer vision pattern recognition revaud weinzaepfel harchaoui schmid epicﬂow edge-preserving interpolation correspondences optical proceedings ieee conference computer vision pattern recognition revaud weinzaepfel harchaoui schmid revaud weinzaepfel harchaoui schmid deepmatching hierarchical deformable dense matching international journal computer vision vol. dosovitskiy fischer hausser hazirbas golkov smagt cremers brox flownet learning optical convolutional networks proceedings ieee international conference computer vision pillai leonard towards visual ego-motion learning shamwell nothwang perlis deepefference learning predict sensory consequences action deep correspondence development learning epigenetic robotics ieee international conference ieee accepted. deep neural network approach fusing vision heteroscedastic motion estimates low-swap robotic applications multisensor fusion integration intelligent systems international conference ciliberto fanello natale metta heteroscedastic approach independent motion detection actuated visual sensors ieee/rsj international conference intelligent robots systems vasco glover mueggler scaramuzza natale bartolozzi independent motion detection event-driven cameras advanced robotics international conference nikolic rehder burri gohl leutenegger furgale siegwart synchronized visual-inertial sensor system fpga pre-processing accurate real-time slam robotics automation ieee international conference ieee furgale rehder siegwart uniﬁed temporal spatial calibration multi-sensor systems intelligent robots systems ieee/rsj international conference ieee obtain depth estimates point grayscale image rendered range images ground truth point cloud vicon room. image rendered depth pair traced pixel coordinate using horizontal vertical ﬁelds view calculated focal lengths camera matrix normalized resulting coordinates multiplied depth pixel location generate coordinates camera frame transforms vector camera frame world frame time another transformation matrix transforms vector camera frame world frame time calculated transform matrix", "year": "2018"}