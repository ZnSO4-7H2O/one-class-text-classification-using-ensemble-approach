{"title": "Audio to Body Dynamics", "tag": "eess", "abstract": " We present a method that gets as input an audio of violin or piano playing, and outputs a video of skeleton predictions which are further used to animate an avatar. The key idea is to create an animation of an avatar that moves their hands similarly to how a pianist or violinist would do, just from audio. Aiming for a fully detailed correct arms and fingers motion is a goal, however, it's not clear if body movement can be predicted from music at all. In this paper, we present the first result that shows that natural body dynamics can be predicted at all. We built an LSTM network that is trained on violin and piano recital videos uploaded to the Internet. The predicted points are applied onto a rigged avatar to create the animation. ", "text": "present method gets input audio violin piano playing outputs video skeleton predictions used animate avatar. idea create animation avatar moves hands similarly pianist violinist would audio. aiming fully detailed correct arms ﬁngers motion goal however it’s clear body movement predicted music all. paper present ﬁrst result shows natural body dynamics predicted all. built lstm network trained violin piano recital videos uploaded internet. predicted points applied onto rigged avatar create animation. pianists play musical piece piano body reacts music. ﬁngers strike piano keys create music. move arms play different octaves. violin players draw hand across strings touch lightly pluck strings hand’s ﬁngers. faster bowing produces faster music pace. interesting question body movement predicted computationally music signal? highly challenging computational problem. need good training videos need able accurately predict body poses videos build algorithm able correlation music body predict movement. figure method overview method gets input audio signal e.g. piano music lstm network predict body movement points turn used animate avatar show playing input music piano human body dynamics complex particularly given quality needed learn correlation audio. traditionally state prediction natural body movement video sequences used laboratory captured motion capture sequences. e.g. scenario would need bring pianist laboratory play several hours sensors attaches ﬁngers body joints. approach hard execute practice easily generalizable. could leverage publicly available videos highly skilled people playing online potentially allow higher degree diversity data. recently though estimating accurate body pose videos possible. year several methods appeared allow learn data wild. parallel number methods also showed remarkable results sync speech. i.e. given audio person saying sentence showed possible predict person’s mouth landmarks would move saying words. advancements inspired tackle challenging ideas predicting body ﬁngers movement music. goal paper explore it’s possible create natural logical body dynamics audio. note information like midi ﬁles potentially learn correlation exact piano keys music. focus creating avatar moves hands ﬁngers like pianist would. consider sets data piano violin recitals collected videos categories processed videos detecting upper body ﬁngers frame video total points frame points represent ﬁngers hand points upper body. visualize points animation avatar moves naturally according given audio input. solve problem steps. first build long-short-termmemory network learns correlation between audio features body skeleton landmarks. second part automatically animate avatar using predicted landmarks. ﬁnal output avatar moves according audio input. trained separate neural network. i.e. separate network violin separate network piano. output skeletons promising produce interesting body dynamics. encourage reader watch supplementary videos audio turned experience results. correlation speech facial movements researched extensively beginning classical works recently showing remarkable results generating high quality videos face talking audio animating avatars using speech three later papers extensive state summary facial animation speech. animating body pose music explored aware. however large body work three related areas behavioral studies people’s bodies react music sounds computer vision learning graphics research creating predicting natural body pose changes example learn walking dancing styles videos multimodal studies combining audio video input improve recognition facial expressions body poses. describe related works below. multi-modal studies shown combining audio visual inputs produces higher accuracy results modalities alone example facial expression recognition beneﬁt also getting voice input since emotion/pitch/loudness message help recognition wang showed body pose estimation works better audio input multi-speaker tracking combination modalities identify intent interact additional examples. relationship speech body rhythms investigated e.g. interesting results showing certain speech characteristics correlated body movement frequencies different emotional scenarios different types movements interactions e.g. regular social interaction researched correspondence pianists perform particular music found consistency perceive music-movement correspondences. also demonstrate correspondences emerge ﬂexibly i.e. musical excerpt correspond different variants movement. studies inspiration idea body movement possibly predicted music speech. large body work lstms predict edit future poses given short video movement even single photo e.g. state body pose estimation recognition techniques typically cnns e.g. earlier works showed possible learn predict motion style videos animate characters real-time works audio input. complementary area research predict audio video. inverse goal predicting video audio. examples include estimating speech face videos goal reading predicting sounds object make video photo finally explored creating audio driven video montages. explore estimation correlation. expect reproduce exact body motion fact assume transformation speech/music body motion unique create natural looking body pose makes sense music speech. variety classes videos correlation audio video interesting. e.g. videos dance people giving lectures playing musical instruments stand-up comedy name few. chose experiment several those. describe datasets process audio video signals training data prepared represented. training testing algorithm described section experimented data violin recitals piano recitals. full list urls available supplementary material example urls footnote below. figure shows example frames sets corresponding keypoints. videos used downloaded internet videos wild. made sure select videos favorable processing algorithms. speciﬁcally intuition behind choice video clear high quality music sound background noise accompanying instruments solo performance. video quality side searched videos high resolution stable ﬁxed camera bright lighting. preferred longer videos continuity. selected videos range min. found using recitals single person shows optimal sets satisfy goals. total hours violin recitals collected hours piano recitals. mfcc features shown successful previous identify classify different musical instruments. e.g. showed network trained mfcc coefﬁcients ﬂute piano violin recognize instruments used. also shown mfccs perform well capturing variation speech follow optimal process computing features described several modiﬁcation adjust datasets frame rate. speciﬁcally compute features stereo .khz sample rate audio perform normalization using ffmpeg choose window length /videofps fps= i.e. .ms. mcamera motion camera e.g. zoom inviewpoint change mstr rigid transformation person vaudio person body transformation audio e.g. drawing striking piano keys vperson non-rigid transformation person’s body correlated audio e.g. person pacing stage playing violin. assume ﬁxed stable camera solve scale translation rotation choosing reference points conﬁguration. assume vperson zero simplicity. goal predict vaudio. reduce dimensionality noise data compute coefﬁcients keypoints follows. keypoints given aligned points frame video collect points single matrix size number points frame point dimensions total number frames dataset. points reshaped vector length compute ﬁnal matrix across frames reduce dimensionality capture data. frame represented using coefﬁcients. allows reduce noise well reduce dimensionality. ﬁnal step upsample coefﬁcients linearly videofps. exact numbers implementation details also presented section goal learn correlation audio features body movements. this built lstm network shown recently lstms used successfully predict sync e.g. architecture visualized fig. chose unidirectional single layer lstm time delay. audio mfcc particular time instance coefﬁcients body keypoints memory. also fully connected layer ’fc’ found increase performance. experimented variations architecture show improvement results. include using norm loss global time delay multi-layer lstm training keypoints directly rather components. interested types keypoints body hand ﬁngers. typically estimation keypoints videos wild challenging large variation camera lighting fast movement divert type benchmark videos algorithms evaluate. recently however number methods appeared handle wild videos much better. came process allows sufﬁciently accurate keypoints described below. begin running video three libraries openpose provides face body hands keypoints maskrcnn deepface face recognition algorithm three libraries perform well benchmarks videos wild fails frames. noticed however failure success somewhat complementary across algorithms. thus video select single frame detection includes person interest reference frame. person’s face gets signature face recognition algorithm. consequent frame’s automatically eliminated it’s location reference frame face signature doesn’t match distance points consecutive frames big. thresholds experiments section. given hands points openpose appear box. choose among maskrcnn openpose body points based conﬁdence existence points case part points still missing exclude frame training. figure example frames eliminated training. ﬁxed plane). length forearm determines offset. first maximum forearm length calculated across frames forearm appeared maximum length reference original plane straight otherwise length zero perpendicular source plane. hands/fingers hand rotation determined root joint pinkie ﬁnger root joint pointer ﬁnger. e.g. right hand pointer joint stage left pinkie joint hand must rotated avatar’s right palm facing palm would facing opposite true. additionally ﬁnger angle reference point root calculated applied. rigging violin rigging violin done four points used constraint references. violins position constrained point attached head rotation determined using lookat constraint attached left hand. position constrained point right hand rotation uses lookat constraint attached bridge violin. running times hardware preprocessing train test runs done server nvidia gpus cores cpus ram. running time epochs training hours violin took hours piano dataset hours. running time test animation avatar real time. processing time calculate keypoints training openpose maskrcnn. data experimented violin recitals piano recitals. videos synchronized fps. randomly selected datasets validation training. sets trained tested completely independently i.e. violin piano net. removed approximately frames videos accurately predicted keypoints total number frames violin piano recitals. threshold removing frames points farther previous frame width frame test audio part training validation sets. evaluations experimented different parameters choices network provide comparisons tables optimal parameters speciﬁed sec. hyper-parameters search. errors tables presented pixels better. achieve good results important ﬁlter frames training data trained truncated back propagation time steps time delay dropout learning rate number components typically training epochs. network implemented caffe adam optimizer. input output normalized subtracting mean dividing variance. fig. show improvement coefﬁcients prediction function epochs piano set. keypoints estimated animate avatar using points. built augmented reality application using arkit runs phone real time. given sequence predicted points body avatar movements applied onto avatar. describe speciﬁc details. avatars used body models body bone rig. ﬁrst initialize aligning predicted points world coordinates. estimating average left right shoulder points across frames calculate rigid transformation avatar. consider body arms head ﬁngers separately. body arms head ﬁngers provided provided parts animated. finally apply root rotation offset match pose angle piano. depth piano calculated wrists position left closer gets camera. details violin. body chain created root node deﬁned average left right connected average left right shoulder. deﬁnes spine. estimate average spine length across frames scale avatar spine accordingly. table violin comparison errors training validation testing different parameter choices. errors presented pixels coeff hidden variables seq. length batch size learning rate time delay=ms dropout upsample table piano comparison errors training validation testing different parameter choices. errors presented pixels coeff hidden variables seq. length batch size learning rate time delay=ms dropout upsample detection wrong person recognition) errors drop signiﬁcantly ﬁltering data. case half training data improve training error test error larger. using less coefﬁcients better training data test error larger using coefﬁcients. using dropout doesn’t improve results case. time delay helps improve results. experimented giving random sequence test audio predict points nets test showed correlation movement predicted expected. results figure present representative results. show predicted keypoints different body poses well original frame context. keypoints show overlaid ground truth points visual comparison. note don’t expect points exactly same ﬁngers hands produce similar satisfactory movement goal paper. ground truth case result body pose detector mistaken. finally show failure cases shows piano violin. show limitations system currently system trained poses actual poses training videos consequently occlusions invisible points predicted well. high pace frequency parts videos body pose detectors create mistakes similarly case motion blur. causes network learn behave high frequency audio accordingly. proposed hypothesis body gestures predicted audio signal showed promising initial results. believe correlation audio human body promising variety applications vr/ar recognition. shown previously mouth animation done audio paper show initial results body animation. hope open research. number limitations many extensions would interesting explore. direction enable movement currently openpose maskrcnn estimate keypoints. points estimators. provided body keypoints estimator e.g. vnect smpl approach potentially extended handle motion well allow diverse human modeling. another direction predict occluded keypoints. currently predict visible points training frames points ignore frame. would interesting keypoints audio network extended predict occlusions e.g. piano left hand usually occludes right hand audio includes music played hands. finally getting good training data class activity straight forward. note constraints required sec. would interesting explore general network handle variety poses without need classify type action priori. alternatively would interesting incorporate video activity recognition learning framework.", "year": "2017"}