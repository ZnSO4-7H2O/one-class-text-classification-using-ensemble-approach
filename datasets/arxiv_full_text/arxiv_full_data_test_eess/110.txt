{"title": "Onsets and Frames: Dual-Objective Piano Transcription", "tag": "eess", "abstract": " We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100% relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions. ", "text": "advance state polyphonic piano music transcription using deep convolutional recurrent neural network trained jointly predict onsets frames. model predicts pitch onset events uses predictions condition framewise pitch predictions. inference restrict predictions framewise detector allowing note start unless onset detector also agrees onset pitch present frame. focus improving onsets offsets together instead either isolation believe correlates better human musical perception. approach results relative improvement note score maps dataset. furthermore extend model predict relative velocities normalized audio results natural-sounding transcriptions. automatic music transcription aims create symbolic music representation audio. converting audio recordings music symbolic form makes many tasks music information retrieval easier accomplish searching common chord progressions categorizing musical motifs. making larger collection symbolic music available also broadens scope possible computational musicology studies piano music transcription task considered difﬁcult even humans inherent polyphonic nature. accurate note identiﬁcations complicated note energy decays onset transcription model needs adapt note varying amplitude harmonics. nonnegative matrix factorization curtis hawthorne erich elsen jialin song adam roberts simon colin raffel jesse engel sageev oore douglas eck. licensed creative commons attribution international license attribution curtis hawthorne erich elsen jialin song adam roberts simon colin raffel jesse engel sageev oore douglas eck. onsets frames dual-objective piano transcription international society music information retrieval conference paris france early popular method used task polyphonic music transcription recent advancements deep learning neural networks attracted attention community particular success convolutional neural networks image classiﬁcation tasks inspired cnns two-dimensional time-frequency representations common input representations audio. authors demonstrated potential single cnn-based acoustic model accomplish polyphonic piano music transcription. considered approach inspired common models used speech recognition acoustic model recurrent neural network language model combined. paper investigate improving acoustic model focusing note onsets. note onset detection looks beginning note. intuitively beginning piano note easier identify amplitude note peak. piano notes onset also percussive distinctive broadband spectrum. model determined onset events condition framewise note detection tasks knowledge. previously demonstrated promise modeling onset events explicitly frameworks. work demonstrate model conditioned onsets achieves state performance large margin common metrics measuring transcription quality frame note note-with-offset. also extend model predict relative velocity onset. velocity captures speed piano depressed directly related loud note sounds. including velocity information transcription critical describing expressivity piano performance results much naturalsounding transcriptions. maps dataset contains audio corresponding annotations isolated notes chords complete piano pieces. full piano pieces dataset consist pieces rendered software synthesizers recordings pieces played yamaha disklavier player piano. synthesized pieces training split pieces played disklavier test split proposed constructing datasets also ensured music piece present set. including disklavier recordings individual notes chords training closer real-world testing environment because often access recordings testing piano training time. testing disklavier recordings also realistic many recordings interesting transcribe ones played real pianos. processing maps midi ﬁles training evaluation ﬁrst translate sustain pedal control changes longer note durations. note active sustain goes note extended either sustain goes note played again. process gives note durations text ﬁles included dataset. metrics used evaluate model frame-level note-level metrics including precision recall score. eval library calculate notebased precision recall scores. standard calculate versions note metrics requiring onsets within ground truth ignoring offsets also requires offsets resulting note durations within ground truth within whichever greater. frame-based scores calculated using standard metrics deﬁned also introduce note metric velocity transcription further described section frame note scores calculated piece mean per-piece scores presented ﬁnal metric given collection pieces. goal generate piano transcriptions contain perceptually relevant performance information audio recording without prior information recording environment characterization instrument. need numerical measure correlates perceptual goal. poor quality transcriptions still result high frame scores short spurious notes repeated notes held. note onsets important piece played onset information would either entirely staccato kind heuristic determine release notes. high note-with-offset score correspond transcription sounds good captures perceptual information onsets durations. adding velocity requirement metric ensures dynamics piece captured well. perceptually accurate metrics possible warrant research. work focus improving note-with-offset score also achieve state results common frame note scores extend model transcribe velocity information well. framewise piano transcription tasks typically process frames audio produce frames note activations. previous framewise prediction models treated frames independent equal importance least prior processed separate language model. propose frames important others speciﬁcally onset frame given note. piano note energy decays starting immediately onset onset easiest frame identify perceptually signiﬁcant. take advantage signiﬁcance onset frames training dedicated note onset detector using output detector additional input framewise note activation detector. also thresholded output onset detector inference process similar concurrent research described activation frame detector allowed start note onset detector agrees onset present frame. onset frame detectors built upon convolution layer acoustic model architecture presented modiﬁcations. librosa compute input data representation mel-scaled spectrograms amplitude input audio logarithmically-spaced frequency bins length window sample rate khz. present network entire input sequence allows feed output convolutional frontend recurrent neural network onset detector composed acoustic model followed bidirectional lstm units forward backward directions followed fully connected sigmoid layer outputs representing probability onset piano keys. frame activation detector composed separate acoustic model followed fully connected sigmoid layer outputs. output concatenated together output onset detector followed bidirectional lstm units forward backward directions. finally output lstm followed fully connected sigmoid layer outputs. inference threshold determine whether onset detector frame detector active. training rnns long sequences require large amounts memory generally faster larger batch sizes. expedite training split training audio smaller ﬁles. however splitting want audio notes onset detector would miss onset frame detector would still need predict note’s presence. found second splits allowed achieve reasonable batch size training least also forcing splits small number places notes active. notes active must split choose zero-crossing audio signal. inference performed original un-split audio ﬁle. ground truth note labels continuous time results audio processing spectrogram frames. quantize labels calculate training loss. quantizing frame size extend model adding another stack also predict velocities onset. stack similar others consists layers convolutions. stack connect two. velocity labels generated dividing velocities maximum velocity present piece. smallest velocity zero rather vmin vmax stack trained following loss averaged across batch various studies considered estimation dynamics recording given score knowledge work literature considering estimation dynamics alongside pitch timing information. result benetos noted review paper evaluating performance current systems estimation note dynamics addressed. evaluate velocity-aware model therefore propose additional criterion note-level precision recall scores. evaluating velocity predictions straightforward unlike pitch timing velocity absolute meaning. example transcriptions contained identical velocities except offset scaled constant factor would effectively equivalent despite reporting completely different velocities every note. address issues ﬁrst re-scale ground-truth velocities transcription range pmin/max denote midi pitch range piano roll number frames example ionset indicator function ground truth onset pitch frame ponset probability output model pitch frame denotes cross entropy. labels onset loss created truncating note lengths prior quantization. performed coarse hyperparameter search onset length found worked best. hindsight surprising also length frames almost onsets spanning exactly frames. labeling frame contains exact beginning onset work well possible mis-alignments audio labels. experimented requiring minimum amount time note present frame labeled found optimum value include presence. addition within frame-based loss term rame apply weighting encourage accuracy start note. note starts frame completes onset ends frame weight vector assigns higher weights early frames notes model incentivized predict beginnings notes accurately results frame onset predictors. several examples notes either last frames reactivate brieﬂy active while. frame results restricted onset detector shown magenta. many notes active frames corresponding onset detection removed shown cyan. cases note brieﬂy reactivated also removed corresponding second onset detected. despite optimizing inference speed network performs faster real time tesla midi ﬁles resulting inference experiments available https//goo.gl/magenta/ onsets-frames-examples. understand individual importance piece model conducted ablation study. consider removing onset detector entirely using onset information during inference making bi-directional rnns unidirectional removing onset detector entirely pre-training onset detector rather jointly training frame detector weighting frames equally sharing convolutional features between detectors removing connection onset frame detectors training using constant q-transform input representation instead mel-scaled spectrograms ﬁnally removing lstms sharing convolutional features results show importance onset information using onset information inference results signiﬁcant relative decrease note onset score relative decrease note-withoffset score increasing frame score slightly. despite increased frame score output sounds significantly worse. ears decrease transcription quality best reﬂected note-with-offset scores. model onset detector consisting convolutions followed bidirectional followed frame-wise loss worst metrics although still outperforms baseline model ablations indicate small impact component encouraging forward-only rnns small accuracy impact used online piano transcription. tried many architectures data augmentation strategies listed table none resulted improvement. signiﬁcantly augmenting training audio adding normalization reverb compression noise synthesizing training midi ﬁles synthesizers made difference. believe results indicate need much larger training dataset real piano recordings fully accurate label alignments. requirements satisﬁed current maps dataset recordings real pianos also satisﬁed musicnet alignments fully accurate notes matched according pitch onset/offset timing assemble pairs reference estimated velocities matched notes referred respectively. perform linear regression estimate global scale offset parameter squared difference pairs reference estimated velocities minimized finally match considered correct addition pitch timing match also satisﬁes |ˆve threshold used evaluations. precision recall scores recomputed normal based newly ﬁltered list matches. trained onsets frames model using tensorflow training dataset described section using batch size learning rate gradient clipping l-norm hyperparameter search conducted optimal learning rate. adam optimizer train steps. training takes hours gpus. hyperparameters used train models including ablation study except reproducing results hyperparameters respective papers used. source code model available https//goo.gl/magenta/ onsets-frames-code. comparison reimplemented models described ensure evaluation consistency. also compared commercial software melodyne version would liked compare anthemscore described well produces musicxml score quantized note durations instead midi millisecondscale timings accurate comparison possible. results evaluations summarized table onsets frames model produces better note-based scores also produces best frame-level scores notebased scores include offsets. example input spectrogram note onset output posteriorgrams inferred transcription recording outside training shown figure importance restricting frame activations based onset predictions inference clear second-tobottom image shows table precision recall results maps conﬁguration test dataset note-based scores calculated eval library frame-based scores deﬁned final metric mean scores calculated piece. midi ﬁles used calculate scores available https//goo.gl/ magenta/onsets-frames-examples. figure inference seconds maps mus-mz enstdkcl.wav bottom show log-magnitude mel-frequency spectrogram input framewise note probability onset probability posteriorgrams produced model corresponding estimated onsets notes thresholding ﬁnally resulting estimated transcription produced model alongside reference transcription. onset notes plot onset predictions shown black. notes corresponding onset prediction shown magenta notes ﬁltered onset predicted note shown cyan. bottom plot estimated transcription shown blue reference shown red. figure best viewed color. common dataset evaluation piano transcription tasks maps dataset particular enstdkcl enstdkam renderings collection pieces. several desirable properties pieces real music opposed randomly-generated sequences pieces played real physical piano opposed synthesizer multiple recording environments available main drawback dataset contains recordings. best measure transcription quality believe much larger dataset needed. however exists evaluations make full data currently available. many papers example restrict data used evaluation using close collection and/or ﬁrst seconds less ﬁle. believe method results evaluation representative real-world transcription tasks. example evaluating close collection raises note score evaluating ﬁrst seconds raises comparison achieved note score setting. model also evaluated using clips close collection additionally trained data test piano. method limits generalizability model produced note score addition small number maps disklavier recordings also noticed several cases disklavier appears skip notes played beginning velocity. beethoven sonata movement several notes played midi velocities mid-s clearly missing audio analysis needed determine frequently missed notes finally believe strict metrics adopted community. discussed section frame note onset scores enough determine whether transcription captured musically relevant information performance. present several audio examples https//goo.gl/magenta/ onsets-frames-examples illustrate point. metrics currently available believe notewith-offset velocity best compare models going forward. similarly current practice using tolerance note onset correctness allows much timing jitter. audio example illustrating point available url. suggest future work evaluate models tighter timing requirements. much work remains done observed achieving high accuracy increasingly difﬁcult timing precision increased part limited timing accuracy datasets currently available trained model resolution scores using existing metrics always high frame note note-with-offset audio examples higher resolution model also available url. examples higher time resolution evident model also produces extraneous notes. presented jointly-trained onsets frames model transcribing polyphonic piano music yields signiﬁcant improvements using onset information. model transfers well disparate train test distributions. current quality model’s output cusp enabling downstream applications symbolic automatic music generation. improve results need create dataset much larger representative various piano recording environments music genres training evaluation. combining improved acoustic model language model natural next step. another direction beyond traditional spectrogram representations audio signals. much worth listening examples transcription. consider mozart sonata movement. system good terms capturing harmony melody rhythm even dynamics. compare transcription systems difference quite audible. also successfully used model transcribe recordings musopen.org website completely unrelated training dataset. model even works surprisingly well transcribing harpsichord recording. audio examples available https// goo.gl/magenta/onsets-frames-examples. mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. emmanouil benetos simon dixon dimitrios giannoulis holger kirchhoff anssi klapuri. automatic music transcription challenges future directions. journal intelligent information systems sebastian b¨ock markus schedl. polyphonic piano note transcription recurrent neural networks. acoustics speech signal processing ieee international conference pages ieee tian cheng matthias mauch emmanouil benetos simon dixon attack/decay model piano transcription. proceedings international society music information retrieval conference ismir york city united states august ismir michael scott cuthbert christopher ariza. music toolkit computer-aided musicology symbolic music data. proceedings international society music information retrieval conference pages valentin emiya roland badeau bertrand david. multipitch estimation piano sounds using ieee probabilistic spectral smoothness principle. transactions audio speech language processing sebastian ewert meinard m¨uller. estimating note intensities music recordings. ieee international conference acoustics speech signal processing pages sebastian ewert mark sandler. piano transcription studio using extensible alternating directions framework. ieee/acm transactions audio speech language processing lufei yi-hsuan yang lee. polyphonic piano note transcription non-negative matrix factorization differential spectrogram. acoustics speech signal processing ieee international conference pages ieee rainer kelz matthias dorfer filip korzeniowski sebastian b¨ock andreas arzt gerhard widmer. potential simple framewise approaches piano transcription. arxiv preprint arxiv. colin raffel brian mcfee eric humphrey justin salamon oriol nieto dawen liang daniel ellis. eval transparent implementation common metrics. proceedings international society music information retrieval conference ismir. citeseer siddharth sigtia emmanouil benetos simon dixon. end-to-end neural network polyphonic piano music transcription. ieee/acm transactions audio speech language processing paris smaragdis judith brown. non-negative matrix factorization polyphonic music transcription. applications signal processing audio acoustics ieee workshop pages ieee christian szegedy vincent vanhoucke sergey ioffe shlens zbigniew wojna. rethinking inception architecture computer vision. proceedings ieee conference computer vision pattern recognition pages carl thom´e sven ahlb¨ack. polyphonic pitch detection convolutional recurrent neural networks. music information retrieval evaluation exchange herwaarden maarten grachten haas. predicting expressive dynamics piano performances using neural networks. proceedings conference international society music information retrieval pages", "year": "2017"}