{"title": "An Ensemble Framework of Voice-Based Emotion Recognition System for  Films and TV Programs", "tag": "eess", "abstract": " Employing voice-based emotion recognition function in artificial intelligence (AI) product will improve the user experience. Most of researches that have been done only focus on the speech collected under controlled conditions. The scenarios evaluated in these research were well controlled. The conventional approach may fail when background noise or nonspeech filler exist. In this paper, we propose an ensemble framework combining several aspects of features from audio. The framework incorporates gender and speaker information relying on multi-task learning. Therefore it is able to dig and capture emotional information as much as possible. This framework is evaluated on multimodal emotion challenge (MEC) 2017 corpus which is close to real world. The proposed framework outperformed the best baseline system by 29.5% (relative improvement). ", "text": "employing voice-based emotion recognition function artiﬁcial intelligence product improve user experience. researches done focus speech collected controlled conditions. scenarios evaluated research well controlled. conventional approach fail background noise nonspeech ﬁller exist. paper propose ensemble framework combining several aspects features audio. framework incorporates gender speaker information relying multi-task learning. therefore able capture emotional information much possible. framework evaluated multimodal emotion challenge corpus close real world. proposed framework outperformed best baseline system humans emotional creatures. people desire reaction others according emotion artiﬁcial intelligence system like human beings able reaction. capability relies recognizing emotion. emotion recognition therefore important product since make human-computer interface friendly improve user experience. emotion recognition system based audio requirement hardware even though multimodal speech processing improve speech related system performance therefore audio based emotion recognition system easier employed product means. however current voice-based products siri google voice search cortona lack emotion recognition capability make people feel machine. shows importance exploring emotion recognition system. researches done area decades work done data collected studio environment. data collection well controlled therefore data clean well segmented. be∗this work done author’s summer internship alibaba sides voice-based emotion recognition research done targeting speech. real application several problems make current developed emotion recognition system failed. first non-speech voice ﬁllers laugh whimper sigh etc. lexicon information contains emotion information. sometimes people perform non-speech voice ﬁller express emotion. second voice segment length vary large range. conventional feature extraction fail condition. third people control intonation lexical information express emotion. acoustic feature work case. address problems propose ensemble framework combines different aspects features audio develop emotion recognition system applicable real world. framework evaluated multimodal emotion challenge corpus. study focus categorical emotion recognition task deﬁned corpus collected capturing clips ﬁlms programs. clips contain background noise non-speech voice ﬁller close real world scenarios. rest parts paper organized following section reviews related work emotion recognition previous work audio-based approach; section describes corpus; section shows proposed approach including sub-system ensemble framework; section shows experiments results discuss results analysis; section concludes work discusses future work. voice-based emotion recognition done decades. extracted prosodic features speech applied majority voting subspace specialists. pilot study exploring static classiﬁer features speech-based emotion recognition. built phoneme-based dependent hidden markov model classiﬁer emotion recognition. work indicated speech contents related emotion. showed advantage static model. discussed feature emotion recognition task. feature sets proposed works showed reliable performance. also used lexical information besides acoustic showed helpful acoustic event identiaccepted ieee international conference acoustics speech signal processing ﬁcation. however work focused speech part rather non-speech voice ﬁllers. deep learning techniques emerging classiﬁer speech related machine learning area. deep learning techniques deep neural network convolutional neural network recurrent neural network able model feature high dimensional manifold space. static classiﬁer dynamic classiﬁer showed advantage emotion recognition task compared conventional approaches especially used attention based weighted pooling extract acoustic representation. showed advantage conventional hand crafted features. multi-task learning recently raised technique helping train better model main task however work used valence arousal auxiliary tasks difﬁcult get. study deep learning techniques multitask learning build better classiﬁer categorical emotion recognition. work includes following novelties. combine different features including classical hand-craft feature high level representation learned deep learning techniques. framework considered speech non-speech audio. multi-task learning techniques applied weighted pooling auxiliary tasks speaker gender classiﬁcation whose labels easily acquired. lexical information speech also incorporated system. framework targeting corpus close real world scenarios. study uses corpus corpus includes clips chinese ﬁlms plays talk shows. clip audio video. video resolution frames second audio sample rate mono channel. study downsample audios khz. total duration hours. clip label among eight classes angry worried anxious neutral disgust surprise happy. duration clips vary secs secs. average duration around secs. speakers corpus. gender almost balanced. male female ratio signal-to-noise ratio distribution shown figure clips captured controlled studio environment might background noise clips since challenge task training testing determined. statistics category listed table seen data imbalance distribution training testing sets similar. propose sub-systems ensemble framework. decisions fused linear combination. open source toolkit focal fuse decisions subsystems. determines linear combination weight minimizing cost log-likelihood ratio framework diagram shown figure multi-task built multi-task main task emotion classiﬁcation auxiliary tasks speaker gender classiﬁcation. system diagram shown figure assumption emotion related speaker gender inspires incorporate speaker gender information classiﬁer. feature provided interspeech paralinguistic challenge used proved work well emotion recognition speaker tasks. therefore select feature multi-task dnn. opensmile extract feature set. ivector proved work well speaker task. also used emotion classiﬁcation proved work better basic statistics like averaging summation able capture informative section rather silence pause part system diagram shown figure input feature sequential acoustic feature. acoustic feature includes mfccs zero crossing rate energy entropy energy spectral centroid spectral spread spectral entropy spectral spectral rolloff chroma vector chroma deviation harmonic ratio pitch. extracted window. window shifting step size weighted pooling obtained equation hidden value output long short-term memory layer time weight. time step scalar computed equation weight learned. equation softmax-like equation similar attention model learning expected segments interest assigned high weight segments silence pause assigned weight. model targeting speech also acoustic event like voice ﬁllers. model also multitask learning expected learn better representation weighted pooling compared single task. network hidden layers trunk part. ﬁrst fully connected layer relu neurons. second bidirectional lstm layer neurons. weighted pooling performed lstm layer. representation weighted pooling sent branch part. branch part task fully connected layer relu neurons. that softmax layer performing classiﬁcation. dropout rate proposed approach evaluated corpus. build baseline systems comparison. random forest single task emotion classiﬁcation. baseline systems take feature set. details experiment settings results described following sections. experiments setting training data hours testing data hour. batch size neural network training study multi-task trained optimizer fig. multi-task dnn. trunk part layers shared tasks. trunk part branch part tasks. main task emotion classiﬁcation. auxiliary tasks speaker gender classiﬁcations. compared feature high level feature contains speaker channel information. also selected input feature another multi-task dnn. ivector extractor trained based hours cellphone data utterance -dimension ivector extracted. ivector disadvantage reliable short utterance affected utterance duration. expect ivector feature complement other. study deﬁne architectures multi-task dnns according inputs since different dimension number. input multi-task hidden layers trunk part neurons layer. branch part hidden layer task neurons. that softmax layer task classiﬁcation. ivector input architecture same. trunk part neuron number layer branch part neuron number layer. neuron type rectiﬁed linear unit dropout rate lexical support vector machine also built sub-system support vector machine based lexical information. automatic speech recognition system trained hours data used recognize speech contents. framework time-delay acoustic model recognizing text libshorttext toolkit adopted text based emotion classiﬁcation feature term frequency inverse document frequency classiﬁcation based support vector classiﬁcation crammer singer. attention based weighted pooling utterances corpus containing non-speech voice ﬁller laugher whimper etc. additional utterance contain long silence pause. using feature accurately represent acoustic characteristics. study build sub-system attention based weighted pooling address issues. used model acoustic event voice ﬁller. attention based weighted pooling comparing baseline systems shows outperforms random forest shows better modeling capability even hours training data. multi-task taking feature absolute improvement compared baseline dnn. indicates speaker gender classiﬁcation tasks helpful emotion classiﬁcation. multitask taking ivector feature also outperforms baseline absolute gain. proves ivector also work well emotion classiﬁcation task. performance lexical weighted pooling lower baseline dnn. lexical relies text information perfectly reliable. weighted pooling shortage training data issue. hours training train sufﬁcient. performance fusion achieve highest maf. shows sub-systems ensemble framework complementing other. compared best baseline system single task ensemble framework offers absolute improvement study proposed ensemble framework categorical emotion recognition. proposed framework evaluated corpus whose data close real world scenarios. found multi-task learning auxiliary tasks speaker gender classiﬁcation helpful emotion classiﬁcation. labels tasks normally easily obtained. fusion different sub-systems achieved better performance. indicates capturing different aspects features input audio improve modeling capability. since evaluation done acted data paper proposed framework need evaluated data real world future. besides data needed training lead better performance. busso deng yildirim bulut c.m. kazemzadeh neumann narayanan analysis emotion recognition using facial expressions speech multimodal information sixth international conference multimodal interfaces icmi state college october press. fig. attention based weighted pooling rnn. lstm layer unrolled along time axis trunk part layers shared tasks. trunk part branch part tasks. main task emotion classiﬁcation. auxiliary tasks speaker gender classiﬁcations. trained adam optimizer. task weights emotion speaker gender classiﬁcation respectively. baseline single task three hidden layers. relu neurons ﬁrst relu neurons last one. architecture multi-task except auxiliary tasks part. baseline random forest trees depth. setting provided challenge organizer feature z-normalized mean variance training data. sequential feature z-normalized within utterance. training training data validation data. linear combination weight used fusion also trained validation set. evaluation metrics macro average fscore accuracy. computed averaging f-score class detection. accuracy computed dividing correctly detected sample number divided total sample number. since data imbalance accuracy accurate represent system performance adopted main metric. eyben w¨ollmer graves schuller douglas-cowie cowie on-line emotion recognition activation-valence-time continuum using acoustic linguistic cues journal multimodal user interfaces vol. march dehak p.j. kenny dehak dumouchel ouellet front-end factor analysis speaker veriﬁcation ieee transactions audio speech language processing vol. juan libshorttext library short-text classiﬁcation analysis rapport interne department computer science national taiwan university. software available http//www. csie. ntu. edu. tw/˜ cjlin/libshorttext cakir parascandolo heittola huttunen virtanen convolutional recurrent neural networks ieee/acm polyphonic sound event detection transactions audio speech language processing vol. dzmitry bahdanau chorowski serdyuk yoshua bengio end-to-end attention-based large icassp vocabulary speech recognition shanghai china apr. ieee", "year": "2018"}