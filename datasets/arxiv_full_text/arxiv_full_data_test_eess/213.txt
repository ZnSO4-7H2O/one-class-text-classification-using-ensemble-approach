{"title": "A Light-Weight Multimodal Framework for Improved Environmental Audio  Tagging", "tag": "eess", "abstract": " The lack of strong labels has severely limited the state-of-the-art fully supervised audio tagging systems to be scaled to larger dataset. Meanwhile, audio-visual learning models based on unlabeled videos have been successfully applied to audio tagging, but they are inevitably resource hungry and require a long time to train. In this work, we propose a light-weight, multimodal framework for environmental audio tagging. The audio branch of the framework is a convolutional and recurrent neural network (CRNN) based on multiple instance learning (MIL). It is trained with the audio tracks of a large collection of weakly labeled YouTube video excerpts; the video branch uses pretrained state-of-the-art image recognition networks and word embeddings to extract information from the video track and to map visual objects to sound events. Experiments on the audio tagging task of the DCASE 2017 challenge show that the incorporation of video information improves a strong baseline audio tagging system by 5.3\\% absolute in terms of $F_1$ score. The entire system can be trained within 6~hours on a single GPU, and can be easily carried over to other audio tasks such as speech sentimental analysis. ", "text": "lack strong labels severely limited state-of-the-art fully supervised audio tagging systems scaled larger dataset. meanwhile audio-visual learning models based unlabeled videos successfully applied audio tagging inevitably resource hungry require long time train. work propose light-weight multimodal framework environmental audio tagging. audio branch framework convolutional recurrent neural network based multiple instance learning trained audio tracks large collection weakly labeled youtube video excerpts; video branch uses pretrained state-of-the-art image recognition networks word embeddings extract information video track visual objects sound events. experiments audio tagging task dcase challenge show incorporation video information improves strong baseline audio tagging system terms score. entire system trained within hours single easily carried audio tasks speech sentimental analysis. environmental audio tagging task labeling audio recordings types sound events occurring them. conventional audio tagging systems supervised learning models require strong labels specify time span event. however strongly labeled datasets scarce usually domain speciﬁc limits potential supervised systems trained datasets scaled also severely impacts generalizability domains. tackle problem audio research community turned large-scale dataset weak labels. compared strongly labeled datasets weakly labeled datasets much less expensive collect scale cover wider range sound event types. weakly labeled datasets time spans events explicitly given presence absence events known. example google audio encompasses variety human animal sounds musical instruments genres common everyday environmental sounds. systems developed work primarily supported bosch graduate research fellowship school computer science cmu. work used bridges cluster extreme science engineering discovery environment supported grant number aci-. without knowing onset offset times sound events difﬁcult learn acoustic characteristics sounds know parts clip reﬂect label. multiple instance learning provides link clip-level labels individual frames clips thus enables supervised learning frame level. clip regarded frames instances bag. positive instance positive negative instances negative. mean time vision research community trying explore correlation audio video. ﬁeld computer vision revolutionized emergence massive labeled datasets learning deep representations recent progress ﬁeld enabled machines recognize scenes objects images videos accurately number pre-trained models good accuracy made available public. success visual recognition welltranslated recognition audio achieved signiﬁcant performance improvement state-of-the-art audio-tagging models transferring discriminative visual knowledge well established visual recognition models sound modality using unlabeled video bridge; trained visual audio networks matching audio video pairs without supervision networks exhibited performance even superior supervised models audio tagging tasks; showed possible synthesize audio tracks silent videos objects interacting materials could deceive human ear. however methods require long hours signiﬁcant computation resources train. consider question best worlds? leverage knowledge audio vision communities help audio tagging? work propose novel multimodal framework incorporates information audio video modalities without incurring excessive computational cost. hand train crnn network using audio data produce clip-level predictions based multiple instance learning paradigm. hand extract frames video data summarize video clips apply pre-trained inception model recognize visual objects frames pretrained glove vectors video labels sound events. ﬁnal predictions made fusing knowledge branches. evaluate multimodal system audio tagging task dcase challenge show proposed framework improves baseline audio-only system terms score. addition framework entry measures dissimilarity objects order express relationship representing objects binary membership matrix rn×n deﬁned means representative algorithm aims minimize total dissimilarity object repreij zijdij. ensure object represented object membership matrix must restrict number representative objects number non-zero rows must possible. sparse coding algorithm therefore tries solve following optimization problem i-th zeros otherwise regularization parameter controls size representative set. optimization hard must binary algorithm actually solves relaxed version problem real number representatives selected summing matrix picking rows largest sums. order select frames quickly necessary efﬁcient encoding frames feature vectors. light-weight convolutional alexnet purpose feed video frame network extract features conv layer. dissimilarity matrix made euclidean distances feature vectors. select frames clip. audio tagging based weakly labeled data formulated multiple instance learning problem. instancelevel label known; instead instances grouped bags labels bags. relationship label labels instances follows standard multiple instance assumption positive contains least positive instance negative instances negative. audio clip tagging task treat clip audio frames clip instances. predict probability event active frame; event aggregate frame-level predictions using pooling function obtain probability event active clip predicted probability event entire i-th clip predicted probability event j-th frame i-th clip. loss function constructed comparing clip-level prediction event cliplevel label event present otherwise. employ cross entropy loss averaged clips event types loss function minimized gradient-based algorithm back-propagation maximally scoring frame receives gradient passes frame-level classiﬁer. structure system using pooling shown fig. audio video tracks video clips often correlated synchronized also extract information video track indicative sounds audio track. predict sound events video track using following steps shown right branch fig. video track clip contains many video frames nearly identical others distinct. selecting subset frames equal intervals reduce burden subsequent object recognition frames selected guaranteed well represent entire clip. found important select representative frames preserve information video track. clustering problem could solved using conventional clustering algorithms k-means; used following sparse coding algorithm select representative robust outliers compared k-means. given objects sparse coding algorithm operates dissimilarity matrix rn×n frames pass pretrained inceptionnet model recognize objects them. frame produces distribution classes. reduce noise probabilities unlikely classes rectify distributions retaining classes renormalizing probability mass among themselves. obtain clip-level representation video information rectiﬁed distributions frames rectify result again. yields object classes probabilities. belief objects present video track clip want conﬁdence sound events interested conduct knowledge mapping using pre-trained glove vectors since pretrained glove vectors already contain vector vocabulary knowledge source adopt glove vectors descriptions visual objects sound events compute cosine similarity pair. object classes assign probability sound event closest terms cosine similarity; sound events receive probabilities object class. results estimation probability sound event clip indicated video track. audio track video track predict probability sound event. ﬁnal fusion step tries produce reﬁned prediction combining knowledge sources. combination implemented weighted average probabilities weights tunable sound event. better performing model individual class would higher weight class weigtht combination tuned toward achieving best overall score. evaluated framework data task dcase challenge data subset google audio collection million -second youtube video clips annotated types sound events occurring them. annotation covers audio event types weak sense information time span events known. dcase data focuses subset sound event types; include vehicle warning sounds interesting self-driving cars smart cities. corpus consists training clips public test clips private evaluation clips. test evaluation sets balanced across sound event types training not. reserved clips training make balanced validation used hyperparameter tuning. frame shift applied -point fourier transform frame aggregate output -dimensional spectral feature vector. -second clip represented feature matrix. feature matrix treated two-dimensional image passed series convolutional local pooling layers. pooling layers feature sequence represent equivalent frame rate original frame rate output last pooling layer bidirectional gated recurrent unit layer neurons direction. dropout rate applied pooling layer layer. finally fully connected layer neurons sigmoid activation function predicts probability sound event type frame aggregated across time using pooling function clip-level probabilities sound events. used stochastic gradient descent algorithm minimize cross entropy loss averaged clips sound event types. used batch size clips. applied nesterov momentum gradient clipping limit learning rate initialized decayed factor civilsiren screaming skateboard train trainhorn policecar fireengine motorcycle caralarm airhorn ambulance reversebeeps truck bicycle carpassingby fig. class-wise scores audio-only system audiovisual reranked system test data. indicates sound events visual information receives higher weight reranking. involved. bicycle class beneﬁted visual information because clips show people talking bicycles bicycles visible video track audible audio track. analysis result reveals problems data annotation. would like emphasize entire multimodel system consumed computational resources. crnn audio branch took hours train gpu; video branch used pretrained models took less minutes validation test evaluation data. work proposed multimodal framework audio tagging combines information audio video tracks video clips predict sound events audio track. system outperforms strong baseline system audio tagging task recent dcase challenge boosting test score framework also light-weight compared models visual information makes extensive pretrained state-of-the-art deep learning models avoids training scratch. future would delve understanding correspondence audio video explore optimal multi-modal system balances performance training resource trade-off. clip-level probabilities predicted network must thresholded generate binary predictions evaluation. imbalance sound event classes distribution predicted probabilities vary drastically across them found critical tune threshold class individually. primary evaluation metric used dcase challenge score micro-averaged across sound event types. devised iterative procedure tune class-speciﬁc thresholds optimize microaverage tune threshold class maximize class-wise repeatedly pick random class re-tune threshold optimize micro-average improvements could made. tuned thresholds validation data epoch training picked model reaching highest validation data ﬁnal model. thresholds obtained validation data directly applied test evaluation data. used procedure described section estimate probability sound events clip using visual information. -second clip consists video frames; selected frames among them. object recognition knowledge mapping produces probability types sound events. verify quality video-only branch calculated score sound event type using predictions validation data. video predictions outperformed audio-only baseline system seven sound events ambulance bicycle passing truck skateboard truck. combined outcome audio video branches using different weights seven sound events video branch performed better assigned weight video branch audio branch; remaining sound events gave weight video branch audio branch. weights tuned fullest; even better performance expected tuned carefully. table lists micro-average score development test evaluation baseline audio-only system multimodal system. help visual information achieved remarkable improvement three sets respectively. ﬁnal score test best performance known audio tagging task dcase challenge contribution visual information test score sound event type shown fig. fusion improved performance sound event types. signiﬁcant improvement came passing class. class confusable class contains positive examples totally missed baseline system. sound event types related types vehicles also improvement video tracks clearly show type vehicle ehsan elhamifar guillermo sapiro shankar sastry dissimilarity-based sparse subset selection ieee transactions pattern analysis machine intelligence vol. john towns timothy cockerill maytal dahan foster kelly gaither andrew grimshaw victor hazlewood scott lathrop dave lifka gregory peterson ralph roskies scott nancy wilkins-diehr xsede accelerating scientiﬁc discovery computing science engineering vol. juncheng florian metze shuhui samarjit comparison deep learning methods environmental sound detection acoustics speech signal processing ieee international conference ieee jort gemmeke daniel ellis dylan freedman jansen wade lawrence channing moore manoj plakal marvin ritter audio ontology human-labeled acoustics speech signal dataset audio events processing ieee international conference ieee olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge international journal computer vision vol. christian szegedy sergey ioffe vincent vanhoucke alexander alemi inception-v inception-resnet impact residual connections learning. aaai vol. yusuf aytar carl vondrick antonio torralba soundnet learning sound representations unlabeled video advances neural information processing systems andrew owens phillip isola josh mcdermott antonio torralba edward adelson william freeman visually indicated sounds proceedings ieee conference computer vision pattern recognition jeffrey pennington richard socher christopher manning glove global vectors word representation proceedings conference empirical methods natural language processing niluthpol mithun juncheng florian metze amit roy-chowdhury samarjit cmu-ucr-bosch trecvid video text retrieval trecvid workshop. proceedings nist.", "year": "2017"}