{"title": "Image denoising with generalized Gaussian mixture model patch priors", "tag": "eess", "abstract": " Patch priors have become an important component of image restoration. A powerful approach in this category of restoration algorithms is the popular Expected Patch Log-Likelihood (EPLL) algorithm. EPLL uses a Gaussian mixture model (GMM) prior learned on clean image patches as a way to regularize degraded patches. In this paper, we show that a generalized Gaussian mixture model (GGMM) captures the underlying distribution of patches better than a GMM. Even though GGMM is a powerful prior to combine with EPLL, the non-Gaussianity of its components presents major challenges to be applied to a computationally intensive process of image restoration. Specifically, each patch has to undergo a patch classification step and a shrinkage step. These two steps can be efficiently solved with a GMM prior but are computationally impractical when using a GGMM prior. In this paper, we provide approximations and computational recipes for fast evaluation of these two steps, so that EPLL can embed a GGMM prior on an image with more than tens of thousands of patches. Our main contribution is to analyze the accuracy of our approximations based on thorough theoretical analysis. Our evaluations indicate that the GGMM prior is consistently a better fit formodeling image patch distribution and performs better on average in image denoising task. ", "text": "abstract. patch priors become important component image restoration. powerful approach category restoration algorithms popular expected patch log-likelihood algorithm. epll uses gaussian mixture model prior learned clean image patches regularize degraded patches. paper show generalized gaussian mixture model captures underlying distribution patches better gmm. even though ggmm powerful prior combine epll non-gaussianity components presents major challenges applied computationally intensive process image restoration. speciﬁcally patch undergo patch classiﬁcation step shrinkage step. steps eﬃciently solved prior computationally impractical using ggmm prior. paper provide approximations computational recipes fast evaluation steps epll embed ggmm prior image tens thousands patches. main contribution analyze accuracy approximations based thorough theoretical analysis. evaluations indicate ggmm prior consistently better modeling image patch distribution performs better average image denoising task. introduction. image restoration process recovering underlying clean image degraded corrupted observation. images captured common imaging systems often contain corruptions noise optical motion blur sensor limitations and/or environmental conditions. reason image restoration algorithms widespread applications medical imaging satellite imaging surveillance general consumer imaging applications. priors natural images play important role image restoration algorithms. image priors used denoise regularize ill-posed restoration problems deblurring super-resolution name few. early attempts designing image priors relied modeling local pixel gradients gibbs distributions laplacian distributions hyper-laplacian distribution generalized gaussian distribution gaussian mixture models concurrently priors also designed modeling coeﬃcients image transformed domain using generalized gaussian scaled mixture gaussian priors wavelet curvelet coeﬃcients alternatively modeling distribution patches image ﬁelds experts learned patch dictionaries sparse low-rank properties stacks similar patches patch re-occurrence priors recently mixture models patch priors patches clean natural images using gaussian mixture model priors. agility model lies fact prior learned clean image patches eﬀectively employed restore wide range inverse problems. also easily extendable include constraints sparsity multi-resolution patches gmms patch priors make methods computationally tractable ﬂexible. although patch prior eﬀective popular article argue generalized gaussian mixture model better image patch prior modeling. compared gaussian model generalized gaussian distribution extra degree freedom controlling shape distribution encompasses gaussian laplacian models. beyond image restoration tasks ggds used several diﬀerent ﬁelds image signal processing including watermark detection texture retrieval voice activity detection audio encoding cite few. tasks ggds used characterize model prior distribution clean signals instance coeﬃcients frequency subbands natural images videos gradients x-ray images wavelet coeﬃcients natural textured ultrasound images tangential wavelet coeﬃcients three-dimensional mesh data short time windows speech signals frequency subbands speech audio signals paper step multi-variate scale shape parameter dimension. superior patch prior modeling capability ggmm illustrated figure ﬁgure shows histograms orthogonal projections subset clean patches onto eigenvectors covariance matrix single component gmm. illustrate diﬀerence shapes scales distributions dimension chosen projections corresponding least signiﬁcant eigenvalues. seen better obtained histograms gaussian model. additionally diﬀerent dimensions patch follow diﬀerent hence suﬃce model feature dimensions given cluster patches laplacian gaussian. therefore propose model patch priors ggmm distributed separate shape scale parameters feature dimension component. diﬀers recent related approach considered ggmm component ﬁxed shape parameter directions. contributions. goal paper measure improvements obtained image denoising tasks incorporating ggmm epll algorithm. unlike incorporates ggmm prior posterior mean estimator based importance sampling directly extend maximum posteriori formulation zoran weiss case ggmm priors. ggmm prior ability capture underlying distribution clean patches closely show introduces major computational challenges case. ﬁrst thought classiﬁcation task noisy patch assigned components mixture. second corresponds estimation task noisy patch denoised given belongs components mixture. interaction noise distribution prior ﬁrst show tasks lead group one-dimensional integration optimization problems figure histograms projection clean patches eigenvectors covariance matrix component mixture contribution clean patch histograms given membership values onto component histogram generalized gaussian distribution adjusted estimating parameters moment estimation comparisons also provided illustrations best obtained gaussian distribution. general admit closed-form solutions particular solutions approximations derived estimation/optimization problem contrast knowledge little known approximating classiﬁcation/integration contributions theoreticalapplication-oriented. major contribution paper theoretical nature develop accurate approximation classiﬁcation/integration problem. particular show approximation next generalize result theorem theorem that prove problems enjoy important desired properties proposition proposition theoretical results allow quantities approximated functions quickly evaluated order incorporated fast algorithms. last contribution experimental concerns performance evaluation proposed model image denoising scenario. reproducibility released implementation https//bitbucket.org/cdeledalle/ggmm-epll. potential impacts beyond image denoising. important note main contributions presented work namely approximations classiﬁcation estimation problems general techniques relevant wider area research problems image restoration. particular contributions apply problems underlying clean data modeled ggmm whereas observed samples corrupted gaussian noise. especially relevant machine learning scenarios ggmm trained clean data data provided testing time noisy. approximations used extend applicability aforementioned clean ggd/ggm based approaches less ideal testing scenario data corrupted noise. instance using techniques introduced paper could directly based voice activity model likelihood ratio test based detector fact suspect many studies signal processing limited gaussian laplacian signal priors complicated integration problem arising intricate interaction ggds gaussian noise. paper demonstrate diﬃculty eﬃciently overcome approximations. leads believe impact approaches presented paper useful image restoration also wider ﬁeld general signal processing applications. organization. explaining considered patch prior based restoration framework section derive ggmm based restoration scheme section approximations classiﬁcation estimation problems studied section section respectively. finally present numerical experiments results section image noisy linear observations linear operator noise component assumed white gaussian variance paper focus standard denoising problems identity matrix general settings account loss information blurring. typical examples operator pass ﬁlter include prior information model distribution patches found natural clean images. consider epll framework restores image maximum posteriori estimation patches since scans pixels image patches contribute loss many patches overlap. allowing overlapping important otherwise would appear blocking artifacts. practice increasing sequence considered optimization performed alternating minimization though little known convergence algorithm iterations produce remarkable results practice. follow epll settings prescribed performing iterations algorithm parameter ﬁrst estimate. minimization respect considering ﬁxed optimizing corresponds solving linear inverse problem tikhonov regularization. explicit solution known linear minimum mean square estimator obtained corresponds maximum posterior denoising problem patch prior patch contaminated gaussian noise variance solution optimization problem strongly depends properties chosen patch prior. algorithm follow exact procedure epll alternating proposed method using generalized gaussian mixture model represent patch prior. general scheme adopt solve ggmm prior inspired proposed zoran weiss simpler case gaussian mixture model prior. reason introduce simpler case prior exposing technical challenges arising ggmm prior section clusters ellipsoid shapes coeﬃcient follows gaus parameters sian distribution i.e. bell-shaped small tails. learned using expectation maximization algorithm dataset million clean patches size pixels randomly extracted training images since study focuses denoising consider enforce zero-mean assumption patches ﬁrst centered zero denoised using ﬁnally initial means added back. fact show corresponds modeling alternative investigated replace denoising problem minimum mean square error estimator mmse estimator deﬁned integration problem closed-form solution case priors. experimental scenarios estimator lead signiﬁcative improvements compared thus pursue idea. figure illustration epll framework image denoising prior. large collection patches ﬁrst extracted. patch optimal gaussian component picked based measure discrepancy given patch. gaussian component next used prior model denoise given patch linear shrinkage corresponding eigenspace depending estimated patches ﬁnally aggregated together weighted combined original noisy image produce ﬁrst estimate. procedure repeated times increasing values intractability mixture models thus making available model patch priors figure provides illustration epll framework steps involved solving optimization problem namely paper suggest using mixture generalized gaussian distributions enable image patches spread clusters bell shaped directions peaky large tails others. priors leads piece-wise linear estimator function ggmm prior lead piecewise non-linear shrinkage estimator. generalized gaussian mixture models. paper learn orthogonal transforms subset clean patches independent zero-mean coeﬃcients. instead assuming coeﬃcient distributions bell shaped consider scale shape distributions vary coordinate another motivation assume highly ﬂexible model based observation illustrated figure given transform corresponding cluster patches displayed histogram patch coeﬃcients diﬀerent coordinates. clearly observed shape distribution varies depending coordinate. peaky heavy tails therefore would faithfully captured gaussian distribution done epll contrast others bell shape would captured properly peaky heavy tailed distribution done instance sparse models shows cannot simultaneously decorrelate sparsify cluster clean patches coordinates. since coordinates reveal sparsity others reveal gaussianity propose ﬂexible model capture variations. propose using multi-variate zero-mean generalized gaussian mixture model multi-variate laplacian distribution subsequent ggmm laplacian mixture model multi-variate hyper-laplacian distribution subsequent ggmm hyper-laplacian mixture model choosing constant vector corresponds regularization motivated earlier unlike classical multivariate models allow entries vary coordinate another. best knowledge proposed work ﬁrst consider fully ﬂexible model. detailing usage ggmm priors epll framework digress brieﬂy explain procedure used training mixture generalized gaussian distributions diﬀerent scale shape parameters learned feature dimension. learning ggmms. parameter estimation carried using modiﬁed version expectation-maximization algorithm iterative algorithm performs iteration steps namely expectation step maximization stop known monotonically increase model likelihood converge local optimum. applying learn ggmm leverage standard strategies used parameter estimation and/or ggmm reported previous works m-step update shape parameter inspired mallat’s strategy using statistics ﬁrst absolute second moments ggds. since strategy uses method moments instead maximum likelihood estimation refer algorithm modiﬁed m-step moment step. also noticed shape parameters lead numerical issues leads local minima several degenerate components. reason step impose constraint consistency purposes keep training data number mixture components models used original epll algorithm speciﬁcally train models million clean patches randomly extracted berkeley segmentation dataset learn zero-mean generalized gaussian mixture ggmm model model initial values shape parameters modiﬁed algorithm iterations. observed figure obtained ggmm models underlying distributions cluster clean patches much better gmm. addition section ggmm estimation lead overﬁtting also better unseen clean patches. gmms multi-modality ggmm prior optimization problem highly non-convex. circumvent issue follow strategy used epll speciﬁc case gaussian mixture model prior. idea restrict involved logarithm component main advantage simpliﬁed version that virtue proposition underlying optimization becomes tractable separated one-dimensional optimization problems problem necessarily convex solution always uniquely deﬁned almost everywhere call shrinkage function. linear almost everywhere real function function often referred wiener shrinkage. discuss address question ﬁnding strategy choosing relevant component replace mixture distribution inside logarithm. optimal component obtained maximizing posterior figure illustration extension epll ggmm priors. general procedure illustrated similar original epll scheme described figure relies generalized gaussian distributions instead gaussian distributions. shape discrepancy function illustrated second depends given scale shape parameters components. section approximated based parameters four retrieved lookup tables finally shrinkage function illustrated bottom non-linear depends selected component. section approximated predeﬁned parametric functions depending range scale parameter lies. values shown bottom chosen sake illustration. corresponds negative logarithm distribution zero-mean generalized gaussian zero-mean gaussian random variables. generalized gaussian random variable becomes gaussian resulting distribution also gaussian zero-mean variance generalized gaussian random variable becomes laplacian distribution resulting convolution also closed form leads following discrepancy function best knowledge simple expressions values solution proposed express terms bi-variate fox-h function this rather cumbersome expression computationally demanding. practice special function requires numerical integration techniques complex lines thus diﬃcult numerically evaluate eﬃciently. since application need evaluate function large number times cannot utilize solution. estimate parameters best approximate either kurtosis tail cumulative distribution function. based approach discrepancy function would thus power function form |x|ν does indeed asymptotically behave power function small large values exponent quite diﬀerent asymptotics. believe diﬀerent behaviors important preserved application context. reason cannot modeled power function distribution. instead provide alternative solution able capture correct behavior asymptotics also permits fast computation. theoretical analysis. section perform thorough theoretical analysis discrepancy function order approximate accurately. ﬁrst introduce basic properties regarding discrepancy function. here slope reveals quadratic behavior discrepancy function. figure gives illustration resulting convolution discrepancy function log-discrepancy note quadratic metrics well-known non-robust outliers complete agreement fact gaussian priors thin tails. figure left right convolution gaussian distribution standard deviation gaussian distribution standard deviation corresponding discrepancy function logdiscrepancy function. diﬀerent asymptotics approximated log-linear function. interestingly exponent vicinity shows gaussian distribution involved convolution prevails laplacian distribution thus behavior quadratic. similarly exponent vicinity shows laplacian distribution involved convolution prevails gaussian distribution behavior linear. results supported figure illustrates resulting convolution discrepancy function figure left right convolution laplacian distribution standard deviation gaussian distribution standard deviation corresponding discrepancy function logdiscrepancy function. log-discrepancy function furthermore discrepancy function shares similar behavior well-known huber loss function known robust outliers. complete agreement fact laplacian priors heavier tails. figure left right convolution generalized gaussian distribution standard deviation gaussian distribution standard deviation corresponding discrepancy function log-discrepancy function. bottom shape parameter respectively. again exponent vicinity shows gaussian distribution involved convolution prevails generalized gaussian distribution behavior generalized gaussian distribution involved convolution prevails gaussian distribution behavior power function form results supported figure illustrates resulting convolution discrepancy function log-discrepancy function moreover discrepancy function shares similar behavior well-known robust m-estimator loss functions particular asymptotic case resembles tukey’s bisquare loss known figure shows evolution log-discrepancy function various values context three diﬀerent signal-to-noise ratios observe decreases left asymptotic behavior starts dominating right asymptotes. words intersection coined respectively using function relu leads approximation singularity crossing point. paper instead function softplus allows approximation converge smoothly asymptotes without singularity. behavior controlled parameter smaller value faster convergence speed asymptotes. figure gives illustration approximations log-discrepancy corresponding distribution obtained relu softplus. ﬁgure underlying functions obtained numerical integration large range value observe using softplus provides better approximation relu. depend original parameters previous analysis parameters non-linear functions parameters require either performing numerical integration evaluating special function discussed parameter requires numerical integration various optimization. reasons values cannot computed runtime. instead pre-compute four parameters oﬄine diﬀerent combinations motivated subsection resulting values stored four corresponding lookup tables. runtime parameters retrieved online bi-linear extrapolation interpolation. four lookup tables given figure section using approximation results substantial acceleration without signiﬁcant loss performance compared computing directly numerical integration runtime. theoretical analysis. except particular values problem explicit solutions. nevertheless shown problem admits solutions. implicitly characterized proofs found appendix properties show indeed shrinkage function shrinks input coeﬃcient according model modeled signal noise ratio small comparison likely noise component dominates underlying signal therefore shrunk towards similarly large likely preserved. even likely small since case large coeﬃcients favored prior. illustrations shrinkage functions various given figure implicitly deﬁned closed form expression general. nevertheless ﬁxed values estimated using iterative solvers newton descent halleys root-ﬁnding method. approaches converge quite fast practice reach satisfying solution within iterations. however since application interest need evaluate function large number times follow diﬀerent path order reduce computation time figure patches sorted norm gradient generated independently distributed according ggmm hlmm. ease visualization eigendirections corresponding variance chosen. near-constant patches variance smaller processing wiener ﬁltering) related tikhonov regularization ridge regression. shrinkage linear slope shrinkage goes increases shrinkage well-known soft-thresholding corresponding maximum posteriori estimator laplacian prior. authors shown shrinkage admits threshold closed-form expression shrinkage approximately equal hard-thresholding error though approximation seem coarse compared based iterative solvers observe signiﬁcant loss quality numerical experiments nonetheless alternative leads times speed-up evaluating shrinkage. experimental evaluation. section explain methodology used evaluate ggmm model present numerical experiments compare performance proposed ggmm model existing gmm-based image denoising algorithms. demonstrate advantage allowing ﬂexible ggmm model also present results using ggmm models ﬁxed shape parameters learning laplacian mixture model hyperlaplacian mixture model procedure described subsection force shape parameters equal respectively. model validation. discussed section figure illustrates validity model choices histograms diﬀerent dimensions single patch cluster. clearly shows importance allowing shape scale parameter vary across dimensions capturing underlying patch distributions. since ggmm falls class generative models also assess expressivity model analyzing variability generated patches ability generate relevant image features tested selecting component ggmm probability figure average log-likelihood non-overlapping patches images validation subset testing bsds dataset ggmm hlmm. total average images shown last column. sampling patches described figure presents collage patches independently generated procedure using ggmm hlmm. observed patches generated ggmm show greater balance strong/faint edges constant patches subtle textures models constant shape parameters hlmm. superiority ggmm model hlmm models also illustrated comparing log-likelihood achieved models clean patches natural images. note that maintain objectivity models tested data diﬀerent dataset used training. compute four above-mentioned models non-overlapping patches randomly selected images extracted bsds testing diﬀerent training images used algorithm observe ggmm better hlmm average images also better single image. given ggmm larger degree freedom study proves learning procedure fall prey over-ﬁtting extra ﬂexibility provided ggmm used capture relevant accurate image patterns. denoising evaluation. following veriﬁcation model provide thorough evaluation ggmm prior denoising task comparing performance epll uses prior hlmm models explained above. ease comparison utilize pipeline settings prescribed original epll algorithm reduce computation time epll-based algorithms utilize random patch overlap procedure introduced instead extracting patches iteration randomly selected diﬀerent subset overlapping patches consisting possible patches processed iteration. sake reproducibility results made matlab/mex-c implementation available online https//bitbucket.org/cdeledalle/ggmm-epll. image denoising performance comparison epll algorithm ggmm priors. psnr ssim values obtained bsds test standard images corrupted diﬀerent levels noise ﬁnally average images. algorithm results also included reference purposes. image denoising performance comparison epll algorithm diﬀerent priors. psnr values obtained bsds test standard images corrupted diﬀerent levels noise ﬁnally average images. evaluation carried classical images barbara cameraman hill house lena mandrill images taken bsds testing image corrupted independently independent realizations additive white gaussian noise standard deviation epll algorithm using mixture gaussian generalized gaussian priors indicated ggmm table results obtained algorithm also included reference purposes. stay focus paper i.e. eﬀect image priors epll-based algorithms excluded performance comparison discussions. denoising performance algorithms measured terms peak signal noise ratio structural similarity observed table general ggmm prior provides better psnr performance prior. terms ssim values ggmm prior comparable gmm. order demonstrate eﬀect ﬁxed values compared ﬂexible ggmm prior compare results ggmm laplacian mixture model hyper-laplacian mixture model priors scenarios results shown table ggmm prior provides better psnr performance average ﬁxed-shape priors. diﬀerences denoising performance also veriﬁed visually figure figure figure denoised images obtained using ggmm prior show much fewer artifacts compared gmm-epll results particular homogeneous regions. hand ggmm prior also able better preserve textures hlmm. figure close image castle bsds testing dataset noisy version degraded additive white gaussian noise standard deviation results epll four patch priors ggmm hlmm respectively. psnr ssim given bottom-left corner. figure close standard image cameraman. noisy version degraded additive white gaussian noise standard deviation results epll four patch priors ggmm hlmm respectively. psnr ssim given bottom-left corner. figure close standard image barbara. noisy version degraded additive white gaussian noise standard deviation results epll four patch priors ggmm hlmm respectively. psnr ssim given bottom-left corner. figure evolution performance epll ggmm misspeciﬁcation noise standard deviation performances measured terms psnr bsds dataset corrupted gaussian noise standard deviation three priors epll assuming ranging restoration model accurate ideally achieve optimal restoration performance using true degradation. verify this conducted denoising task image corrupted noise standard deviation used ggmm priors restoration framework assumed values ranging figure shows evolution average restoration performance images bsds testing varying noise variances. ggmm prior attains best performance noise variance used restoration model matches ground truth contrast ggmm reaches best performance larger value correct noise used degradation. tends under-smooth clean patches larger value required compensate mismatch assumed prior actual distribution restoration model. indicates ggmm better option model distribution image patches lmm. inﬂuence approximations. previous experiments using ggmm patch priors conducted based proposed approximations introduced section section figure table provide quantitative illustration speed-ups provided approximations eﬀect denoising performance. timings carried matlab intel core .ghz figure shows result obtained calculating original discrepancy function numerical integration shrinkage function halley’s root-ﬁnding method. makes denoising process extremely slow takes discrepancy function provides orders magnitude speed-up perceivable drop performance addition incorporating shrinkage approximation provides acceleration allows denoising complete less seconds minor drop psnr/ssim. indicated detailed proﬁles table shrinkage damaged additive white gaussian noise obtained respectively evaluating classiﬁcation shrinkage problem numerical solvers approximating classiﬁcation problem only approximating problems. psnr ssim given bottom-left corner. running time overall ggmm-epll runtime proﬁles ggmm-epll corresponding denoising experiment shown figure proﬁles obtained respectively evaluating classiﬁcation shrinkage problem numerical solvers approximating classiﬁcation problem only approximating problems. proﬁles split discrepancy shrinkage patch extration/reprojection. speed-up respect no-approximation indicated parenthesis major accelerations green. percentage time taken step respect overall execution time indicated time reading bottlenecks indicated red. conclusions discussion. work suggest using mixture generalized gaussians modeling patch distribution clean images. provide detailed study challenges encounters using highly ﬂexible ggmm prior image restoration place common prior. identify main bottlenecks restoration procedure using epll ggmm namely patch classiﬁcation step shrinkage step. main contributions paper thorough theoretical analysis classiﬁcation problem allowing introduce asymptotically accurate approximation computationally eﬃcient. order tackle shrinkage step collate extend existing solutions ggmm prior. numerical experiments indicate ﬂexible ggmm patch prior better modeling natural images mixture distributions constant shape parameters hlmm. image denoising tasks shown using ggmm priors often outperforms used epll framework. nevertheless believe performance ggmm prior scenarios falls short expected potential. given ggmm persistently better prior would expect ggmm-epll outperform gmm-epll consistently. postulate under-performance caused epll strategy optimization. even though ggmm prior improving quality global solution half quadratic splitting strategy used epll guaranteed return better solution non-convexity underlying problem. reason focus future work designing speciﬁc optimization strategies ggmm-epll leveraging better expressivity proposed prior model denoising general restoration applications. focus extending work employ ggmms/ggds model-based signal processing tasks. tasks estimating parameters ggmm directly noisy observations problem particular interest could beneﬁt approximations. learning priors noisy patches shown useful patch-based image restoration clean patches available priori adapt model speciﬁcities given noisy image another open problem analyze asymptotic behavior minimum mean square estimator shrinkage prior alternative shrinkage. could useful design accurate approximations general inference frameworks. last least characterizing exact asymptotic behaviors convolution arbitrary ggds investigated still open question. best knowledge study ﬁrst attempt towards goal always gaussian extending study general case challenging problem major interest signal processing tasks noise gaussian instead follows another ggd. part experiments presented paper carried using plafrim experimental testbed supported inria cnrs universit´e bordeaux bordeaux conseil r´egional d’aquitaine properties hold since convolution real even unimodal distributions even unimodal property follows fact convolution continuous bounded real functions continuous bounded. aharon elad bruckstein k-svd algorithm designing overcomplete dictionaries sparse representation ieee transactions signal processing aiazzi alparone baronti estimation based entropy matching generalized gaus buades coll j.-m. morel non-local algorithm image denoising computer vision pattern recognition cvpr ieee computer society conference vol. ieee dominguez-molina gonz´alez-far´ıas rodr´ıguez-dagnino monterrey practical procedure estimate shape parameter generalized gaussian distribution technique report eng. available http//www. cimat. mx/reportes/enlinea/i- eng. dugas bengio b´elisle nadeau garcia incorporating second-order functional knowledge better option pricing advances neural information processing systems fergus singh hertzmann roweis freeman removing camera shake single photograph transactions graphics vol. gazor zhang soft voice activity detector based laplacian-gaussian model ieee houdard bouveyron delon high-dimensional mixture models unsupervised image denoising preprint hal- aug. https//hal.archives-ouvertes.fr/ hal-. krupi´nski approximated fast estimator shape parameter generalized gaussian distribution small sample size bulletin polish academy sciences technical sciences martin fowlkes malik database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics computer vision iccv proceedings. eighth ieee international conference vol. ieee parameswaran c.-a. deledalle denis nguyen accelerating gmm-based patch priors image restoration three ingredients speed-up arxiv preprint arxiv. pascal bombrun j.-y. tourneret berthoumieu parameter estimation multivariate generalized gaussian distributions ieee transactions signal processing peppas formula average error probability dual-hop amplify-and-forward relaying systems generalized shadowed fading channels ieee wireless communications letters roenko lukin djurovic simeunovic estimation parameters generalized gaussian distribution communications control signal processing international symposium ieee roth black fields experts framework learning image priors computer vision pattern recognition cvpr ieee computer society conference vol. ieee sharifi leon-garcia estimation shape parameter generalized gaussian distributions subband decompositions video ieee transactions circuits systems video technology sulam elad expected patch likelihood sparse prior energy minimization methods computer vision pattern recognition lecture notes computer science tanabe farvardin subband image coding using entropy-coded quantization noisy chan oord schrauwen student-t mixture natural image patch prior application image compression. journal machine learning research wang bovik sheikh simoncelli image quality assessment error sapiro mallat solving inverse problems piecewise linear estimators gaussian mixture models structured sparsity ieee transactions image processing zoran weiss learning models natural image patches whole image restoration international conference computer vision ieee november https//doi.org/ ./iccv...", "year": "2018"}