{"title": "CNN+LSTM Architecture for Speech Emotion Recognition with Data  Augmentation", "tag": "eess", "abstract": " In this work we design a neural network for recognizing emotions in speech, using the IEMOCAP dataset. Following the latest advances in audio analysis, we use an architecture involving both convolutional layers, for extracting high-level features from raw spectrograms, and recurrent ones for aggregating long-term dependencies. We examine the techniques of data augmentation with vocal track length perturbation, layer-wise optimizer adjustment, batch normalization of recurrent layers and obtain highly competitive results of 64.5% for weighted accuracy and 61.7% for unweighted accuracy on four emotions. ", "text": "training procedure. hand also possible deep extracting high-level features ﬁnal time aggregation. test variety architectures different depths convolutional recurrent modules achieving best scores scenario. address challenges class imbalance data scarcity explored vocal tract length perturbation purpose data augmentation showed signiﬁcantly improves performance. line examined batch normalization applied recurrent layers network. finally noticed parameters convolutional bilstm layers trained different pace. tried take advantage observation per-layer adjustment update rule parameters unfortunately able make deﬁnite conclusion favor idea. iemocap collected university southern california standard datasets emotion recognition. consists twelve hours audio video recordings performed professional actors organized sessions dialogues actors different genders either playing script improvising. sample audio utterance assigned emotion label. labeling made students three time utterance. annotators allowed assign multiple labels necessary. ﬁnal true label utterance chosen majority vote emotion category highest vote unique. since annotators reached consensus often labeling improvised utterances scripted ones concentrate improvised part dataset. sake comparison prior stateof-the-art approaches predict four represented emotions neutral sadness anger happiness leave utterances total. iemocap dataset main drawbacks class imbalance small size. cope obstacles examined data augmentation means vocal tract length perturbation time oversampling least work design neural network recognizing emotions speech using iemocap dataset. following latest advances audio analysis architecture involving convolutional layers extracting high-level features spectrograms recurrent ones aggregating long-term dependencies. examine techniques data augmentation vocal track length perturbation layer-wise optimizer adjustment batch normalization recurrent layers obtain highly competitive results weighted accuracy unweighted accuracy four emotions. providing high quality interaction human machine challenging active ﬁeld research numerous applications. important part domain recognition human speech emotions computer systems. last years impressive progress achieved speech recognition means deep learning achievements also include signiﬁcant results speech emotion recognition e.g. work build neural network iemocap dataset achieve result highly competitive state art. treating problem deep learning either creates hand-crafted acoustic features used inputs neural network sends data preprocessing directly neural network. apply second strategy transforming audio signal spectrogram used input convolutional layers followed recurrent ones. choice architecture recently demonstrated competitive performance assumes main interpretations. hand adding convolutional layers beginning network efﬁcient reduce dimensionality data signiﬁcantly simplify knowledge present state achieved however cross-validation procedure performed paper includes folds dataset possible. hand experiments showed performance strongly depends part data used measuring scores. consequence results obtained -fold cross-validation without clariﬁcation data used measurement possible compare with. therefore propose -fold cross validation correct measuring scores iemocap dataset present results correspondingly. mentioned above iemocap dataset consists sessions conversation woman giving speakers total. order well model generalize different speakers took validation test sets correspond different speakers sessions. training composed four remaining sessions. course experiments observed performance strongly depends speakers chosen test therefore choose -fold cross-validation strategy order average possible choices dataset splitting. interestingly best knowledge results reported iemocap dataset obtained -fold cross-validation. case choice validation test sets rigorously deﬁned scores obtained possible compare with. evaluating model performance chose weighted unweighted accuracies. standard accuracy computed whole test set. average accuracies computed emotion separately. first compute metrics fold present scores average folds. since imbalanced datasets relevant characteristic rather concentrated efforts getting high line works iemocap. considered architectures convolutional layers bi-lstm layers dense layer softmax nonlinearity network optimization procedure used stochastic gradient descent momentum batch size regularization weights used l-regularization. signiﬁcant variety data samples time length performed zero-padding samples along time axis. order avoid aggregation artiﬁcially added time steps bi-lstm masking layer convolutional bi-lstm modules. size mask derived temporal size corresponding spectrogram action convolutional strides average standard deviation spectrogram pixels computed whole dataset along time frequency axes. normalization signiﬁcantly improves convergence time model. however applied networks small depth results strong overﬁtting. mentioned above conducted variety experiments different depths convolutional bi-lstm modules. presence pooling layers alternating convolutions noticeably decreased performance discarded beginning experiments. examined different scenarios shallow deep bi-lstm deep shallow bi-lstm deep deep bi-lstm represented classes dataset happiness anger. vtlp based speaker normalization technique considered implemented reduce interspeaker variability. difference human’s vocal tract length modeled rescaling peaks signiﬁcant formants along frequency axis factor taking values approximate range therefore order variablility estimate factor speaker accordingly normalize spectrograms. applied inversely idea used data augmentation order generate samples simply perform rescaling original spectrograms along frequency axis keeping scaling factor range approaches normalization augmentation pursue objective enforce invariance model speaker-dependent features since relevant classiﬁcation criterion. augmentation however easier implement don’t need estimate scaling factor speaker therefore stick option. fmax upper cut-off frequency deﬁned larger highest signiﬁcant formants therefore rescale frequencies rescale rest ensure considered diapason stays constant. tried strategies data augmentation. ﬁrst single uniformly distributed value sampled epoch used rescale training examples rescaling applied validation set. second strategy spectrogram rescaled individually generated training well validation sets. evaluation used majority vote model predictions eleven copies test present scores obtained second augmentation strategy provided best result. tab. present results best model also contributions performance techniques applied. oversampling allowed increase resulted decrease data augmentation vtlp increase metrics correspondingly. considering larger range frequencies increased finaly tab. present results fold scores obtained averaging best folds results obtained works fold cross-validation. also tried batch normalization implemented bi-lstm layers network. experiments observered data interest sensitive normalization. therefore choose conservative normalizing strategy implies averaging samples axes monitoring gradient network parameters observed gradient respect weights convolutional layers much larger respect weights bilstm observation allows interpretation regarding convolutional weights loss surface steeper deeper regarding weights bilstm. therefore gave nudge might interesting consider different update parameters namely learning rate momentum convolutional recurrent modules. apart varying conventional update parameters momentum optimizer also considered modiﬁcation introducing parameter stands momentum learning rate velosity correspondingly. coefﬁcient brought accumulate velocity expression provide better control momentum term optimizer. unfortunately experiments able draw deﬁnite conclusion favor layer-wise adjustment nevertheless interesting direction persue thorough experiments might give preferable result. here batch temporal frequency index respectively preactivation product sample time lengths batch feature number. batch normalization applied input contribution hidden state examined batch normalization applied architectures convolutional bi-lstm layers. experiments initial batch size demonstrated faster overﬁtting degradation performance compared baseline. experiments larger batch size showed strongly inﬂuences performance despite fact normalization performed along axes batch. scores obtained batch size almost reaches performance best model therefore possible augmenting batch size would lead even better results. unfortunately memory restrictions could verify work built neural network recognizing emotions speech using iemocap dataset. unlike prior results order measure model performance performed fold cross-validation appropriate dataset. adress issues scarcity class imbalance employed data augmentation means vtlp minor class oversampling. following modern trends speech analysis used mixed cnn-lstm architecture exploiting capacity convolutional layers extract high-level representations inputs. interestingly noticed parameters convolutional lstm layers trained different pace. tried take advantage observation per-layer adjustment update rule parameters unfortunately able make deﬁnite conclusion favor idea. nevertheless interesting direction persue thorough experiments might give preferable result. preserve signal structure much possible performed normalization layer-wise well batch-wise. nevertheless manage increase performance compared baseline might caused small batch size order available memory. medennikov prudnikov zatvornitskiy improving english conversational telephone speech recognition interspeech francisco isca international speech communication association saon sercu rennie h.-k. english conversational telephone speech recognition system interspeech francisco isca international speech communication association mower provost deep learning robust feature generation audiovisual emotion recognition ieee international conference acoustics speech signal processing vancouver canada ieee tashev high-level feature representation using recurrent neural network speech emotion recognition interspeech dresden germany isca international speech communication association satt rozenberg hoory efﬁcient emotion recognition speech using deep learning spectrograms interspeech stockholm sweden isca international speech communication association busso bulut c.-c. kazemzadeh mower chang narayanan iemocap interactive emotional dyadic motion capture database language resources evaluation vol. sainath vinyals senior convolutional long short-term memory fully connected deep neural networks ieee international conference acoustics speech signal processing laurent pereyra brakel zhang bengio batch normalized recurrent neural networks ieee international conference acoustics speech signal processing shanghai china ieee goel kingsbury data augmentation deep neural network acoustic modeling ieee international conference acoustics speech signal processing florence italy ieee", "year": "2018"}