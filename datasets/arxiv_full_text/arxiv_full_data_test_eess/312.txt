{"title": "NELS - Never-Ending Learner of Sounds", "tag": "eess", "abstract": " Sounds are essential to how humans perceive and interact with the world and are captured in recordings and shared on the Internet on a minute-by-minute basis. These recordings, which are predominantly videos, constitute the largest archive of sounds we know. However, most of these recordings have undescribed content making necessary methods for automatic sound analysis, indexing and retrieval. These methods have to address multiple challenges, such as the relation between sounds and language, numerous and diverse sound classes, and large-scale evaluation. We propose a system that continuously learns from the web relations between sounds and language, improves sound recognition models over time and evaluates its learning competency in the large-scale without references. We introduce the Never-Ending Learner of Sounds (NELS), a project for continuously learning of sounds and their associated knowledge, available on line in nels.cs.cmu.edu ", "text": "sounds essential humans perceive interact world captured recordings shared internet minute-by-minute basis. recordings predominantly videos constitute largest archive sounds know. however recordings undescribed content making necessary methods automatic sound analysis indexing retrieval. methods address multiple challenges relation sounds language numerous diverse sound classes large-scale evaluation. propose system continuously learns relations sounds language improves sound recognition models time evaluates learning competency large-scale without references. introduce never-ending learner sounds project continuously learning sounds associated knowledge available line nels.cs.cmu.edu. ability automatically recognize sounds essential large number applications identifying emergencies elderly care patients hospitals monitoring cameras unwanted privacy concerns allowing self-driving cars respond safely warning sounds emergency vehicles improving airport house surveillance number unusual phenomena acoustic signatures expanding interaction digital assistants non-verbal communication analyzing retrieving video content perhaps explored application year million minutes videos uploaded internet second; constitute consumer trafﬁc ability recognize sounds recordings organizing understanding exploiting rapidly growing audio multimedia data. recent years sound recognition research focused curated data guidelines although successful necessary literature explored challenges audio. curated audio recordings carefully collected recorded audio opposed mainly recorded unstructured manner; deﬁned task-oriented sound classes opposed unbounded number sound classes wide range topics; come limited classes samples contrast every-day growing number classes samples; rich descriptions content contrast descriptions insufﬁcient unavailable wrong. hence test state sound recognition performs context also explore paradigms learn ever-growing audio. existing sound recognition systems learn ﬁnite curated source learning limited scope source optimization objective improve learning time. address issues literature includes never-ending learning architectures learn many types knowledge years diverse sources using previously learned knowledge improve subsequent learning sufﬁcient self-reﬂection avoid learning stagnation pointed mitchell never-ending paradigm employed ongoing projects never-ending language learner text never-ending image learner images. however paradigm unexplored sound learning. examples tasks related paradigm learn associations sounds language continuously grow acoustic vocabularies improve robustness sound recognizers; evaluate subjectivity sound recognition absence prior knowledge source generation process. introduce never-ending learner sounds project large-scale continuous learning sounds associated knowledge mining web. examples associated knowledge semantics related objects events actions places cities qualities contribution begins working framework serves audio content indexing searching indexed sounds. since inception nels reported several research publications discussed section gandhian young technological innovation award india selected abstract qualcomm innovation fellowship. nels framework current form nels framework continuously crawls audio metadata youtube videos creates content-based index based vocabulary sounds. sound recognizers trained variety sources including audio itself. nels also evaluates quality recognition human feedback. audio content indexed combining crawled metadata sound recognition predictions human feedback. october crawled hours audio corresponding million video segments seconds. indexed audio content available search retrieval using engine website. figure framework serves continuously audio content indexer sound recognition evaluation search engine indexed audio. crawl module search query used crawl audio metadata youtube videos. query corresponds sound event labels four different datasets. video metadata extracted using pafy corresponds attributes title description. crawled metadata index audio content. although nels eventually feed different sound archives selected youtube ﬁrst source diversity sounds available metadata associated them. contrast audio-only recordings collecting audio videos poses several challenges. youtube contains massive amount videos proper formulation search query necessary ﬁlter videos higher chances containing desired sound event. typing query composed noun conditioner necessarily fetch video containing sound event associated metadata often corresponds visual content; contrary audio-only archives freesounds.org. therefore modiﬁed query combination keywords <sound event label> sound\" exampleair conditioner sound\". although results empirically improved sound event always found occurring even sometimes present within short duration overlapping sounds volume. discarded videos longer minutes shorter seconds either likely contain unrelated sounds short processed. hear learn module used sound events four annotated datasets train classiﬁers crawled youtube video segments unlabeled. class predictions also used index audio content. framework developed given guidelines datasets could added seamlessly. nels able take advantage existing curated annotations however dealing mismatch conditions. current four datasets audioset. esc- classes broad categories animals natural soundscapes water sounds human non-speech sounds interior/domestic sounds exterior sounds. dataset consists audio segments average duration seconds each. urbansoundsk classes like shot jackhammer children playing. dataset consists audio segments average duration seconds each. classes like passing bird singing door banging major sound contexts namely home context residential area. dataset consists audio segments average duration seconds each. audioset classes million audio segments average duration seconds each. audio datasets crawled video segments preprocessed classiﬁed. nels meant classiﬁer agnostic. currently follow convolutional neural networks classiﬁer setup described recordings segmented seconds converted -bit encoding mono-channel sampling rate ﬁles then extracted features comprising log-scaled mel-spectrograms mel-bands window size size lastly features used train multi-class classiﬁers using cnns datasets. website module line nels.cs.cmu.edu currently serves purposes. first evaluate sound recognition using human feedback include part audio indexing. second provide search engine indexed audio content. goal eventually able search audio based descriptive terms onomatopoeias acoustic content website provides search ﬁeld capture term sound searching. term mapped closest sound classes. mapping uses tool wordvec precomputed vocabulary thousand words called glove wordvec computes vector representations vocabulary words text-query. then computes cosine similarity text-query precomputed vocabulary list sounds. given list sound classes necessarily match words precomputed vocabulary consider words within similarity threshold else results retrieved. additionally provide another feature website user paste youtube video link second text ﬁeld nels yield dominant sound. displayed video segment resulting text-based search evaluated user button-options correct incorrect. whether human claims system’s predicted class present within segment not. examples seen figure discussion section discuss three sound learning challenges nels involved. relation sounds language. language describe audio content used search sounds help deﬁne sound vocabularies however relation sounds language still inchoate topic. better understand usage language indexing carried studies. figure examples indexed video segments using nels. first shows example can-opening sound title images clearly related. second shows example siren wailing sound title related visual sound source child rather electronic device. third shows example pig-oink sound matches visuals title text metadata. fourth shows thumbnail video indexed eventually deleted user. first sound recognition results evidenced although sound events quiet street busy street deﬁned audio streets qualiﬁer implied differences acoustic content. kind nuances described adjective-noun pairs verb-noun pairs collected thousand pair-labels derived different audio ontologies. audio recordings containing sound events pair-labels crawled collective archive freesounds.org. concluded despite subjectivity labels degree consistency sound events types pairs. second wanted identify text phrases contain notion audibility termed sound events. noted sound-descriptor phrases often disambiguated based whether preﬁxed words sound without changing meaning. hence matching combination sound phrase four words identify candidate phrases followed application rule-based classiﬁer eliminate noisy candidates obtained list sound labels. further applying classiﬁer features extracted dependency path manually listed acoustic scenes discovered sound labels also able discover ontological relations. example forests associated sounds birds singing breaking twigs cooing falling water. continuous semi-supervised learning sounds. nels take advantage existing curated sound datasets curated audio improve learning. previously semi-supervised selftraining approaches used improve sound event recognizers used earlier version nels. classiﬁers trained tested using dataset consisting labeled samples sounds. re-training used unlabeled youtube video segments. similar ﬁrst paper mismatched data achieved overall precision improvement. regardless improvement reached learning plateau. could mismatched conditions training self-training audio. initial class bias introduced hand-crafted dataset. ambiguous youtube audio self-train sound classes. hence learn daily growing source audio exploration needed. evaluation learning quality. nels indexes audio content segments unlabeled weak wrong labels. therefore essential methods automatic evaluation quality large-scale. solution include human intervention hence website allows collection human feedback asses correctness sound event indexing. nevertheless human feedback slow costly hence important combine methods estimate performance large-scale. used earlier version nels recognizer trained sound events three different datasets. after crawled audio youtube videos using sound event labels datasets search query. query combination keywords <sound event label> sound\" exampleair conditioner sound\". then evaluated highest- recognized segments sound class based types references human feedback search query. search query summary video’s metadata describing whole video interesting know extent holds video’s segment level. results showed performance trends using types references similar relatively close less absolute difference precision. trend suggests query could used lower-bound human inspection. words could serve preliminary reference evaluate sound recognition. exploration associated metadata multimedia cues could used alternative measurements.", "year": "2018"}