{"title": "Deep Learning for Lip Reading using Audio-Visual Information for Urdu  Language", "tag": "eess", "abstract": " Human lip-reading is a challenging task. It requires not only knowledge of underlying language but also visual clues to predict spoken words. Experts need certain level of experience and understanding of visual expressions learning to decode spoken words. Now-a-days, with the help of deep learning it is possible to translate lip sequences into meaningful words. The speech recognition in the noisy environments can be increased with the visual information [1]. To demonstrate this, in this project, we have tried to train two different deep-learning models for lip-reading: first one for video sequences using spatiotemporal convolution neural network, Bi-gated recurrent neural network and Connectionist Temporal Classification Loss, and second for audio that inputs the MFCC features to a layer of LSTM cells and output the sequence. We have also collected a small audio-visual dataset to train and test our model. Our target is to integrate our both models to improve the speech recognition in the noisy environment ", "text": "human lip-reading challenging task. requires knowledge underlying language also visual clues predict spoken words. experts need certain level experience understanding visual expressions learning decode spoken words. now-a-days help deep learning possible translate sequences meaningful words. speech recognition noisy environments increased visual information demonstrate this project tried train different deep-learning models lip-reading first video sequences using spatiotemporal convolution neural network bi-gated recurrent neural network connectionist temporal classification loss second audio inputs mfcc features layer lstm cells output sequence. also collected small audio-visual dataset train test model. target integrate models improve speech recognition noisy environment. ability recognizing said visual information impressive skill difficult task novice. task lipreading inherently ambiguous word level short sentences absence context secondly characters produce sequences called homophemes difficult distinguish. difficulty overcome using context neighborhood information. neighboring words help minimize ambiguity homophemes. human lipreading performance precise consequently enormous need automatic lip-reading system. many practical applications dictating instructions messages phone noisy environment transcribing re-dubbing archival silent films security biometric identification multi-talker simultaneous speech improving performance automated speech recognition general usually noisy environments speech recognition systems fails performs poorly extra noise signals. overcome problem improve performance speech noisy environments visual information. visual information help speech recognition systems. many challenges make lipreading task difficult major challenges perform phoneme classification using large non-public audio-visual dataset. recently proposed large complex end-to-end trainable network audio-visual speech recognition; model consists encoders decoder. named model watch listen attend spell network. first encoder encodes video frames passing convolutional layers followed stacked lstms. fixed size encoded vector stored. second encoder encodes audio mfcc features passing stacked lstms. later encoded vectors concatenated input decoder networks also consists stacked lstms decoder network outputs character sequence. lip-reading architecture networks network used lip-reading context prediction second network deployed speech recognition. following section details networks reported. figure shows lip-reading network figure illustrates configuration speech recognition network. figure lipnet architecture. sequence frames used input processed layers stcnn followed spatial max-pooling layer. features extracted processed bi-grus; time-step output processed linear layer softmax. end-to-end model trained language. model end-to-end sentence level lipreading model. model operates character level using spatio-temporal convolutions recurrent neural networks finally connectionist temporal classification loss second network processes audios features using lstms categorical cross entropy loss. next section summarizes literature review. section briefly described network architecture dataset explained section implementation details explained section finally experiments results discussed section large body work done reading using pre-deep learning methods. many approaches using convolutional neural networks proposed recognize phonemes visemes still images lips movement instead recognizing full words sentences. phoneme smallest distinguishable unit sound collectively make spoken word; viseme visual equivalent recently deep learning based lip-reading model proposed called lipnet consists three spatio-temporal convolutional layers followed bi-directional gated recurrent units finally connectionist temporal classification loss. reported accuracy grid dataset. phrases urdu language. corpus following words phrases shown table detailed information dataset available participant requested repeat word phrase times. corpus contains videos words phrases videos. convolutional neural networks stacked convolutional layers instrumental performance computer vision based tasks object recognition basic -dimensional convolution layer expression given recurrent neural networks improve propagating learning information time steps. used type known bidirectional output stcnn bi-gru denoted hidden layer bi-gru. zt}↦{hh ….ht} evaluate model lip-reading urdu speech words phrases constructed video-speech corpus. corpus video-audio recordings participants including male female. corpus contains words recorded videos cropped standard size applied viola jones face detector video detect face applied mouth detector detect lips cropped video contains lips movements. process illustrated figure speech recognition network contains lstms cells followed fully connected layer softmax classification layer. lstm takes mfcc features input classification layer classify word class. network architecture illustrated figure implemented network tensor flow platform performed several experiments first experiment performed testing lipnet dataset. noticed whatever video input lipnet output sentence consists words grid dataset used training lipnet sentence consists words. location words also fixed example known prior fourth word would english alphabet. secondly also noticed output lipnet varies time input video. results lipnet shown figure video based reading task reimplemented network proposed details network given figure. mainly network consists spatio-temporal convolutional layers bi-directional gated recurrent connectionist temporal classification loss first stcnn takes input frames once process together; stcnn followed max-pooling layer mask. third stcnn layer feature vector input bi-gru. type improves upon earlier rnns adding cells gates propagating information timesteps learning control information flow. similar long short-term memory bidirectional introduced increase amount input information available network. standard restrictions future information cannot reached current state. contrary bi-directional future information reached current state. analyzing results coded model tensorflow tried train lipnet dataset could train model loss. model unable compute loss backpropagate. second experiment performed training speech recognition system words dataset pose problem classification problem words dataset. trained different networks task first deep neural network second lstm based network. results showed lstm based network performs better dnn. moreover also trained networks urdu digits dataset taken csalt lab. results experiment tabulated table project attempted design audio-visual lipreading system urdu language. trained different models audio video separately. successfully trained audio model words digits urdu words digits corpus unable merge networks demonstrate results audio-visual lipreading noisy environments. apart implementation investigation models contributed small urdu language corpus lipreading. corpus consisting words phrases spoken users times total recorded pre-processed videos dataset. future develop model like lipreading would able graves fernández gomez schmidhuber connectionist temporal classification labelling unsegmented sequence data recurrent neural networks. proceedings international conference machine learning acm. mroueh marcheret goel deep multimodal learning audio-visual speech recognition. acoustics speech signal processing ieee international conference ieee.", "year": "2018"}