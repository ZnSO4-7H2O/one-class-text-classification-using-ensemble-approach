{"title": "Generalization Challenges for Neural Architectures in Audio Source  Separation", "tag": "eess", "abstract": " Recent work has shown that recurrent neural networks can be trained to separate individual speakers in a sound mixture with high fidelity. Here we explore convolutional neural network models as an alternative and show that they achieve state-of-the-art results with an order of magnitude fewer parameters. We also characterize and compare the robustness and ability of these different approaches to generalize under three different test conditions: longer time sequences, the addition of intermittent noise, and different datasets not seen during training. For the last condition, we create a new dataset, RealTalkLibri, to test source separation in real-world environments. We show that the acoustics of the environment have significant impact on the structure of the waveform and the overall performance of neural network models, with the convolutional model showing superior ability to generalize to new environments. The code for our study is available at https://github.com/ShariqM/source_separation. ", "text": "figure left right column examples source separation using spectrogram overlapped voices input. first spectrogram mixture. second source estimates using oracle third source estimates using method. problem source separation traditionally approached within framework computational auditory scene analysis methods typically rely upon features gammatone ﬁlters order representation data allow clustering methods segment individual speakers mixture. cases features parameterized allow learning approaches generative models factorial hidden markov models accomplish speech separation recognition sparse non-negative matrix factorization bayesian non-parametric models also used. however computational complexity inherent many approaches makes difﬁcult implement online setting robust efﬁcient. recent work shown recurrent neural networks trained separate individual speakers sound mixture high ﬁdelity. explore convolutional neural network models alternative show achieve state-of-the-art results order magnitude fewer parameters. also characterize compare robustness ability different approaches generalize three different test conditions longer time sequences addition intermittent noise different datasets seen training. last condition create dataset realtalklibri test source separation real-world environments. show acoustics environment signiﬁcant impact structure waveform overall performance neural network models convolutional model showing superior ability generalize environments. code study available https//github.com/shariqm/source separation. sound waveform arrives ears rarely comes single isolated source rather contains complex mixture multiple sound sources transformed different ways acoustics environment. central challenges auditory scene analysis separate components mixture individual sources recognized. generally requires form prior knowledge statistical structure sound sources common onset co-modulation continuity among harmonic components goal develop model learn exploit forms structure signal order robustly segment time-frequency representation sound waveform constituent sources *equal contribution redwood center theoretical neuroscience university california berkeley. correspondence shariq mobin <shariqmobinberkeley.edu>. uses bi-directional long short-term memory neural network learn useful embeddings time-frequency bins mixture. formulate objective function encourages embeddings cluster according source k-means applied partition source signals mixture. model improved work isik chen proposed simpler end-to-end models achieved impressive ∼.db signal-to-distortion ratio source estimation signals. work develop alternative model source separation based dilated convolutional neural network architecture show achieves similar state-of-the-art performance blstm model order magnitude fewer parameters. addition convolutional approach operate streaming signal enabling possibility source separation realtime. another goal study examine well different neural network models generalize inputs realistic. test models inputs containing long time sequences intermittent noise mixtures collected different recording conditions including realtalklibri dataset. success challenging domains critical progress continue source separation eventual goal able separate sources regardless speaker identities recording devices acoustical environments. figure shows three examples factors affect spectrogram recorded waveform. automatic speech recognition community begun discuss address generalization challenge less discussion context audio source separation. vision machine learning issue usually referred dataset bias models perform well training dataset fail generalize datasets. recent years main approach tackling issue data augmentation. speech community simulators different acoustical environments leveraged create data. show choice model architecture alone improve generalization. choice convolutional architecture inspired generalization power convolutional neural networks relative fully connected networks. compare performance model recurrent blstm models previous work show suffer tested figure original recording single female speaker librispeech dataset; recordings original waveform made three different orientations computer speaker recording device. chen propose framework single-channel speech separation. input signal length rf×t spectrogram computed using short-time fourier transform time-frequency spectrogram embedded k-dimensional latent space rf×t×k learnable transformation parameters turn converted back audio waveform inverse stft. attempt compute phase source estimates. instead phase mixture compute inverse stft magnitude source estimate spectrogram loss function mean-squared-error source estimate spectrogram supervised source target spectrogram rf×t×c variety neural network architectures potential candidates parameterize embedding function equation chen -layer bi-directional lstm architecture architecture utilizes weight sharing across time allows process inputs variable length. contrast convolutional neural networks capable sharing weights along time frequency axis. recently convolutional neural networks shown perform state-of-the-art music transcription ﬁlters convolve frequency time dimensions reason advantageous harmonic series exhibits stationarity property frequency dimension. speciﬁcally signal fundamental frequency harmonics equally spaced according following structure seen equal spacing successive harmonics figure prevent time-frequency embeddings negligible power interfering classiﬁcation tensor ﬁrst masked threshold tensor rf×t threshold tensor removes time-frequency bins below fraction highest power present training attractor points calculated using thresholded oracle mask absence oracle mask test time attractor points calculated using kmeans. embeddings pass corresponding time-frequency threshold clustered. networks incorporate feedback allow stable novel conditions seen training. absence recurrent memory ﬁlter dilation enables receptive ﬁeld integrate information long time sequences without loss resolution. furthermore incorporating ﬁxed amount future knowledge network straightforward ﬁxed-lag delay convolution show figure similar ﬁxed-lag smoothing kalman ﬁlters model dilated convolution koltun proposed dilation function replace pooling operations vision tasks. notational simplicity describe dilation dimension. method convolves input signal ﬁlter dilation factor input receptive ﬁeld unit upper layer dilated convolutional network grows exponentially function layer number shown figure applied time sequences useful property encoding long range time dependencies hierarchical manner without loss resolution occurs using pooling. unlike recurrent networks must store time dependencies scales single memory vector dilated convolutions stores dependencies distributed manner according unit layer hierarchy. lower layers encode local dependencies higher layers encode longer range global dependencies. models successfully used generating audio directly waveform construct mixture data sets according procedure introduced generated summing randomly selected waveforms different speakers signal-to-noise ratios uniformly distributed downsampled reduce computational cost. figure one-dimensional ﬁxed-lag dilated convolutions used model dilation factors bottom represents input successive another layer convolution. network ﬁxed-lag timepoints output decision current input. main motivation creating dataset record mixtures speech acoustics room deform high quality recording realistic one. datasets real mixtures exist exists dataset ground truth source waveforms available transcription speakers words given target outputs order understand well model generalizes real world mixtures created small test dataset ground truth source waveforms. realtalklibri test dataset created starting test-clean directory open librispeech dataset contains speakers. ﬁrst downsampled waveforms before. mixture dataset created sampling random speakers test-clean partition librispeech picking random waveform start time each playing waveforms logitech computer speakers seconds. waveforms speakers played separate channels linked left right computer speaker separated microphone computer different distances. recordings made sample rate using macbook order obtain ground truth individual speaker waveforms waveforms played twice isolation simultaneously speaker. ﬁrst recording represents ground truth second mixture. verify quality ground truth recordings constructed ideal binary mask performs well previous simulated datasets oracle performance figure realtalklibri data made recording sessions yielded hours data giving total hours test data. data available https//www.dropbox.com/s/pscejhkqdrxrk/rtl.tar.gz?dl= evaluate models single-channel simultaneous speech separation task. mixture waveforms transformed time-frequency representation using shorttime fourier transform log-magnitude features served input model. stft computed window length size hann window. scipy compute stft tensorflow build neural networks report results using distance measure source estimate true source waveform space. distance measure signal-to-distortion ratio introduced blind audio source separation metric less sensitive gain source estimate. compute results using version matlab bsseval toolbox python implementation code also available online network consists dilated convolutional layers made stacks stack dilation factor double layer. batch normalization applied layer residual connections used every layer table details. model ﬁxedlag response timepoints output network dimensionality number output time points number frequency bins ﬁnal number channels embedding dimensionality. training evaluation also function rather softmax computing mask equation adam optimizer used piecewise learning rate schedule boundaries= values initial learning rate reimplement danet blstm architecture containing layers hidden units forward backward lstm total hidden units. replicated training schedule using rmsprop algorithm starting learning rate exponential decay parameters decay steps= decay rate= begin evaluating models test dataset state-of-the-art results shown table model achieves best score using factor fewer parameters danet. dpcl score taken similar architecture danet therefore similar number parameters. model important difference however second neural network used enhance source estimate spectrogram achieve result. model still able exceed performance without extra enhancement network. addition model ﬁxed window future whereas blstm models access entire future. indicates convolution based architecture better solving source separation task less information comparison recurrent based architecture. table signal-to-distortion ratio competitor models proposed convolutional model oracle. model achieves best results using signiﬁcantly fewer parameters. score averaged examples. score model advantage second enhancement neural network improved source estimate spectrogram masking addition normal blstm. test time access labels attractors cannot computed using equation previous work k-means employed instead. order attractors good proxy k-means algorithm important attractors form dense clusters embeddings. without applying regularization attractors found case. main issue observed embeddings speakers mixture largely overlapping embeddings driven extremely apart order drive attractors apart. worked well training poorly test time solutions found k-means didn’t match attractors found training. order combat degeneracy normalize embeddings novel effective figure visualize embedding outputs model using single mixture across timepoints. embedding point corresponds single time-frequency mixed input spectrogram. embeddings colored diagram according oracle labelling speaker blue speaker notice network learned cluster embeddings according speaker belong i.e. high density embeddings left similarly blue embeddings right. structure allows k-means easily cluster centers match attractors used figure embeddings speakers time points projected onto -dimensional subspace using pca. orange points correspond time-frequency bins energy threshold embedding points total. ﬁrst experiment study well models work time-sequences longer trained i.e. previous work indicated recurrent architectures incorporate feedback function unpredictably sequence lengths beyond seen training. hand convolutional network architectures incorporate feedback. advantageous processing time sequences indeﬁnite length errors cannot accumulate time. since convolutional network stationary process convolved dimension hypothesize architecture operate robustly sequence lengths much longer seen training. results shown figure surprisingly results indicate blstm also able generalize sequences signiﬁcantly larger length contrary expectations. discuss possible explanations result next section. model able maintain performance across long sequence expected. second experiment interested models respond small bursts input data outside training distribution. believe blstm model might become unstable result inputs recurrent structure makes possible noise affect hidden state indeﬁnitely. took sequences length inserted white noise middle mixture disrupt models process. training distribution. trained models training tested test librispeech test realtalklibri test set. results shown figure model generalizes quite well dataset librispeech dataset losing performance. unfortunately degrades substantially using dataset. however model still outperforms danet model datasets. note oracle performance also degrades rtl. figure visualize mistakes network makes realtalklibri dataset. ﬁrst example indicates model strong enough bias harmonic structure contained speech classiﬁes frequencies fundamental different speaker harmonic frequencies fundamental. second example indicates model also issues temporal continuity speaker identity particular frequency bins varies sporadically across time. indicates still room improve generalization models modifying model architecture adding regularization. recurrent neural networks shown perform audio source separation high ﬁdelity. explored using convolutional neural networks alternative model. state-of-the-art results dataset using factor fewer parameters show convolutional models accurate efﬁcient audio source separation. model additional advantage working online ﬁxed-lag response sec. order study robustness models studied performance three different conditions longer time sequences intermittent noise datasets seen training. results length noise generalization experiments indicate blstm learns behave much like stationary process temporal dimension. observe substantial degradation performance perturbed noise. also performs consistently sequences signiﬁcantly longer seen training. figure results length noise generalization experiments. plots plot starting time speciﬁed x-axis time points later. blstm able operate long time sequences recover intermittent noise. figure spectrogram results shown figure again contrary belief blstm resilient noise model quickly recovers noise passes possible explanation blstm integrating information short time scales therefore forgets previous input data short number time steps. believe construct input models randomly sample starting time waveform. force blstm learn stationary function since must able separate mixture without information past hidden state. figure results models tested simulated mixtures librispeech simulated mixtures realtalklibri dataset. model performs best generalizes better librispeech fails alongside blstm generalizing real mixtures rtl. models trained wsj. model second neural network enhance source estimate spectrogram therefore advantage. model wasn’t available online testing librispeech rtl. hand stationarity property free convolutional model. motivates network architecture figure which design integrates local information past future. ﬁnal experiment showed convolutional neural network also generalized better librispeech dataset realtalklibri dataset introduced here. models robust datasets well deformations caused acoustics different environments critical progress audio source separation. realtalklibri dataset complements real-world speech datasets additionally providing approximate ground truth waveforms mixture currently available. looking forward improve generalization ability examples shown figure introducing training realtalklibri developing robust model architectures introducing regularizers structure speech creating powerful data augmentation tools. also believe models operate unknown number sources utmost importance ﬁeld audio source separation. figure left right column examples source separation realtalklibri dataset. source estimates using oracle. source estimates using method. model difﬁculty maintaining continuity speaker identity across frequencies harmonic stack across time references abadi mart´ın barham paul chen jianmin chen zhifeng davis andy dean jeffrey devin matthieu ghemawat sanjay irving geoffrey isard michael tensorﬂow osdi system large-scale machine learning. volume barker marxer ricard vincent emmanuel watanabe shinji. third chimespeech separation recognition challenge dataset task baselines. automatic speech recognition understanding ieee workshop ieee chen zhuo mesgarani nima. deep attractor network single-microphone speaker separation. acoustics speech signal processing ieee international conference ieee graves alex fern´andez santiago schmidhuber j¨urgen. lstm networks improved phoneme classiﬁcation recognition. artiﬁcial neural networks formal models applications– icann kinoshita keisuke delcroix marc yoshioka takuya nakatani tomohiro sehr armin kellermann walter maas roland. reverb challenge common evaluation framework dereverberation recognition reverberant speech. applications signal processing audio acoustics ieee workshop ieee krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition hershey john chen zhuo roux jonathan watanabe shinji. deep clustering discriminative embeddings segmentation separation. acoustics speech signal processing ieee international conference ieee wei-ning zhang glass james. unsupervised domain adaptation robust speech recognition variational autoencoder-based data augmentation. arxiv preprint arxiv. ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning isik yusuf roux jonathan chen zhuo watanabe shinji hershey john single-channel multispeaker separation using deep clustering. arxiv preprint arxiv. nakano masahiro roux jonathan kameoka hirokazu nakamura tomohiko nobutaka sagayama shigeki. bayesian nonparametric spectrogram modeling based inﬁnite factorial inﬁnite hidden markov model. applications signal processing audio acoustics ieee workshop ieee oord aaron dieleman sander heiga simonyan karen vinyals oriol graves alex kalchbrenner senior andrew kavukcuoglu koray. wavenet generative model audio. arxiv preprint arxiv. panayotov vassil chen guoguo povey daniel khudanpur sanjeev. librispeech corpus based public domain audio books. acoustics speech signal processing ieee international conference ieee raffel colin mcfee brian humphrey eric salamon justin nieto oriol liang dawen ellis daniel raffel colin. eval transparent implementation common metrics. proceedings international society music information retrieval conference ismir. citeseer sigtia siddharth benetos emmanouil dixon simon. end-to-end neural network polyphonic piano muieee/acm transactions audio transcription. speech language processing tieleman tijmen hinton geoffrey. lecture .rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning vincent emmanuel gribonval r´emi f´evotte c´edric. performance measurement blind audio source separation. ieee transactions audio speech language processing shinji nugraha aditya arie barker marxer ricard. analysis environment microphone data simulation mismatches robust speech recognition. computer speech language", "year": "2018"}