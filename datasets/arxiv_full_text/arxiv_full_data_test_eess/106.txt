{"title": "Sequence-to-Sequence ASR Optimization via Reinforcement Learning", "tag": "eess", "abstract": " Despite the success of sequence-to-sequence approaches in automatic speech recognition (ASR) systems, the models still suffer from several problems, mainly due to the mismatch between the training and inference conditions. In the sequence-to-sequence architecture, the model is trained to predict the grapheme of the current time-step given the input of speech signal and the ground-truth grapheme history of the previous time-steps. However, it remains unclear how well the model approximates real-world speech during inference. Thus, generating the whole transcription from scratch based on previous predictions is complicated and errors can propagate over time. Furthermore, the model is optimized to maximize the likelihood of training data instead of error rate evaluation metrics that actually quantify recognition quality. This paper presents an alternative strategy for training sequence-to-sequence ASR models by adopting the idea of reinforcement learning (RL). Unlike the standard training scheme with maximum likelihood estimation, our proposed approach utilizes the policy gradient algorithm. We can (1) sample the whole transcription based on the model's prediction in the training process and (2) directly optimize the model with negative Levenshtein distance as the reward. Experimental results demonstrate that we significantly improved the performance compared to a model trained only with maximum likelihood estimation. ", "text": "signal decoder produces grapheme current time-step maximal probability conditioned ground-truth grapheme history previous time-steps. training scheme usually referred teacher-forcing method however inference stage since ground-truth transcription known model must produce grapheme current timestep based approximation correct grapheme previous time-steps. therefore incorrect decision earlier time-step propagate subsequent time-steps. another drawback differences objective functions training evaluation schemes. training stage model mostly optimized combining teacher-forcing approach maximum likelihood estimation frame. hand recognition accuracy evaluated calculating minimum string edit-distance correct transcription recognition output. differences result suboptimal performance optimizing model parameter appropriate objective function crucial achieve good model performance words direct optimization respect evaluation metrics might necessary. paper propose alternative strategy training sequence-to-sequence adopting idea speciﬁcally utilize policy gradient algorithm simultaneously alleviate problems. treating decoder policy network agent able sample whole transcription based model’s prediction training process directly optimize model negative levenshtein distance reward. model thus integrates power sequence-to-sequence approach learn mapping speech signal text transcription based strength reinforcement learning optimize model performance metric directly. sequence-to-sequence model type neural network model directly models conditional probability source sequence length target sequence length common input sequence feature vectors like mel-spectral ﬁlterbank and/or mfcc. therefore rs×f number features total frame length utterance. output speech transcription sequence either phoneme grapheme sequence. figure shows overall structure attention-based encoder-decoder model consists encoder decoder attention modules. encoder task processes input sequence outputs representative information despite success sequence-to-sequence approaches automatic speech recognition systems models still suffer several problems mainly mismatch training inference conditions. sequence-to-sequence architecture model trained predict grapheme current time-step given input speech signal ground-truth grapheme history previous time-steps. however remains unclear well model approximates real-world speech during inference. thus generating whole transcription scratch based previous predictions complicated errors propagate time. furthermore model optimized maximize likelihood training data instead error rate evaluation metrics actually quantify recognition quality. paper presents alternative strategy training sequence-to-sequence models adopting idea reinforcement learning unlike standard training scheme maximum likelihood estimation proposed approach utilizes policy gradient algorithm. sample whole transcription based model’s prediction training process directly optimize model negative levenshtein distance reward. experimental results demonstrate signiﬁcantly improved performance compared model trained maximum likelihood estimation. sequence-to-sequence models recently shown effective many tasks machine translation image captioning speech recognition models able learn direct mapping variable-length source target sequences often known apriori using single neural network architecture. many complicated hand-engineered models also simpliﬁed letting dnns input output spaces therefore eliminate need construct separate components i.e. feature extractor acoustic model lexicon model language model commonly required conventional systems hidden markov model-gaussian mixture model -based hybrid hmm-dnn. generic sequence-to-sequence model commonly consists three modules encoder module representing source data information decoder module generating transcription output attention module extracting related information encoder representation based current decoder state. decoding scheme done based left-to-right decoding procedure. training stage given current input speech policy gradient policy gradient type reinforcement learning algorithm optimizing expected rewards respect parameterized policy apply idea policy gradient method need establish connection model reinforcement learning formulation. reinforcement learning reformulate system markov decision process state space possible actions transition probability reward function. here task generate text transcription given input speech waveform encoder-decoder neural network agent. time-step deﬁne state concatenation between decoder hidden state context information time action equals action space contains possible grapheme sentence symbols dataset. reward function task explained later section given pair speech transcription n-th index reward transcription compared groundtruth optimization target maximize expected reward ey|πθ] respect neural network parameter ﬁrst-order optimization method need calculate gradient expected rewards derived similar equation gradient minimum risk training objective however instead using ﬁnal reward distribute equally every timestep replace time-distributed reward provide informative reward timestep every sample. therefore replace utilize temporal structure γi−tr length transcription generalized equation accumulated future reward based current state action time-t discount factor reduce effect future rewards. score number hidden units encoder number hidden units decoder. finally decoder task predicts target sequence probability time based previous output context information formulated section introduce proposed approach integrates policy optimization standard encoder-decoder model. start describing policy gradient method followed reward construction agent. decoder. attention module extension scheme helps decoder relevant information encoder side based current decoder hidden states attention module produces context information time based encoder decoder hidden states following equation reward construction tasks systems evaluated based edit-distance levenshtein distance algorithm. therefore also construct reward function calculate utilizing edit-distance algorithm. deﬁne reward edit-distance function transcriptions substring index ground-truth length. intuitively calculate whether current transcription time-t decreases edit-distance compared previous transcription multiply positive reward edit-distance time smaller previous edit distance. speech dataset feature extraction study investigated performance proposed method identical deﬁnitions training development test sets kaldi recipe separated experiments using wsj-si wsj-si data training. used validation eval test set. used character sequence decoder target followed preprocessing steps proposed text utterances mapped -character letters alphabet apostrophes periods dashes space noise eos. experiments extracted dims mel-spectrogram features speech normalized every dimension zero mean unit variance. fig. comparison teacher-forcing policy gradient training processes. training stage teacher-forcing model conditioned ground-truth dataset. meanwhile policy gradient method model conditioned prediction previous time-step predicts current timestep output probability. model architecture encoder side input features linear layer hidden units followed leakyrelu activation function. used three bidirectional lstms encoder hidden units lstm improve running time reduce memory consumption used hierarchical subsampling bi-lstm layers reduced number encoder time-steps factor decoder side used -dimensional embedding matrix transform input graphemes continuous vector followed one-unidirectional lstms hidden units. scorer function inside attention module used scorers hidden units adam optimizer learning rate training phase started train model convergence. that continued training adding rl-based objective model stopped improving. rl-based objective tried four scenarios using different discount factors global reward calculate gradient based sampled sequences utterance. reward m-th sample based n-th utterance timestep length sample real world impractical integrate possible transcription calculate gradient expected reward therefore utilize monte carlo sampling sample transcription sequence model calculate gradient empirical expectation since reinforce gradient estimator usually noisy might hinder learning process several tricks reduce variance paper normalize reward moving average standard deviation time-step ﬁnal-reward normalize reward across samples. summarize explanation provide illustration fig. compares difference teacher-forcing policy gradient method training sequence-to-sequence model. teacher-forcing optimized trying maximize objective function dividing transcription length prevent decoder favoring shorter transcriptions. language model lexicon dictionary work. models implemented pytorch framework. table character error rate result baseline proposed models wsj-si wsj-si datasets. results produced without language model lexicon dictionary. table shows experiment results wsj-si wsj-si datasets. compared results several published models attention encoder-decoder joint ctc-attention model trained objective. also created baseline model attention encoder-decoder trained objective. difference attention encoder-decoder decoder calculate attention probability context vector based current hidden state instead previous hidden state. also reused previous context vector concatenating input embedding vector. explore several conﬁgurations using ﬁnal reward time distributed reward different values. result shows combining teacher forcing policy gradient approach improved model performance sigreinforcement learning subﬁeld machine learning creates agent interacts environment learn maximize rewards using feedback signal. many reinforcement learning applications exist including building agent learn play game without explicit knowledge control tasks robotics dialogue system agents limited areas reinforcement learning also adopted improving sequence-based neural network models. ranzato proposed idea combined reinforce objective training called mixer. early stage training ﬁrst steps trained remaining steps reinforce. decrease training progress time. using reinforce trained model using non-differentiable task-related rewards paper need deal scheduling sampling teacher forcing ground-truth. furthermore mixer sample multiple sequences based reinforce monte carlo approximation investigate mixer system. machine translation task shen improved neural machine translation model using minimum risk training google system combined objectives achieve better results. task shanon performed optimization sampling paths lattices used smbr training might similar reinforce algorithm. work applied ctc-based model. probabilistic perspective formulation resembles expected reward formulation used reinforcement learning. here formulation equally distribute sentence-level loss time-steps sample. contrast applied strategy task found using ﬁnal reward effective method training system loss diverged produced worse result. therefore proposed temporal structure applied timedistributed reward results demonstrate improved performance signiﬁcantly compared baseline system. introduced alternative strategy training sequence-tosequence models integrating idea reinforcement learning. proposed method integrates power sequenceto-sequence approaches learn mapping speech signal text transcription based strength reinforcement learning optimize model performance metric directly. also explored several different scenarios training rl-based objective. results show combining rl-based objective together objective signiﬁcantly improved model performance compared model trained objective. best system achieved wsj-si using time-distributed reward settings discount factor kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhudinov rich zemel yoshua bengio show attend tell neural image caption generation visual attention international conference machine learning oriol vinyals alexander toshev samy bengio dumitru erhan show tell neural image caption generator proceedings ieee conference computer vision pattern recognition dzmitry bahdanau chorowski dmitriy serdyuk philemon brakel yoshua bengio end-to-end attention-based large vocabulary speech recognition proc. icassp ieee william chan navdeep jaitly quoc oriol vinyals listen attend spell neural network large vocabulary conversational speech recognition acoustics speech signal processing ieee international conference ieee andros tjandra sakriani sakti satoshi nakamura attention-based wavtext feature transfer learning ieee automatic speech recognition understanding workshop asru okinawa japan december yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine translation system bridging human machine translation arxiv preprint arxiv. ronald williams simple statistical gradient-following algorithms connectionist reinforcement learning machine learning vol. shiqi shen yong cheng zhongjun maosong yang minimum risk training neural machine translation proceedings annual meeting association computational linguistics august berlin germany volume long papers evan greensmith peter bartlett jonathan baxter variance reduction techniques gradient estimates reinforcement learning journal machine learning research vol. andriy mnih karol gregor neural variational inference learning belief networks proceedings international conference international conference machine learning-volume jmlr. ii–. douglas paul janet baker design wall street journal-based corpus proceedings workshop speech natural language stroudsburg association computational linguistics. daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannemann petr motlicek yanmin qian petr schwarz silovsky georg stemmer karel vesely kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. dec. ieee signal processing society ieee catalog cfpsrw-usb. awni hannun andrew maas daniel jurafsky andrew first-pass large vocabulary continuous speech arxiv recognition using bi-directional recurrent dnns preprint arxiv. suyoun takaaki hori shinji watanabe joint ctcattention based end-to-end speech recognition using multiacoustics speech signal processtask learning ieee international conference ieee volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis human-level control deep reinforcement learning nature vol. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search nature vol. satinder singh michael kearns diane litman marilyn walker reinforcement learning spoken dialogue systems advances neural information processing systems", "year": "2017"}