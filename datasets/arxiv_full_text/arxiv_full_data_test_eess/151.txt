{"title": "Speech recognition for medical conversations", "tag": "eess", "abstract": " In this work we explored building automatic speech recognition models for transcribing doctor patient conversation. We collected a large scale dataset of clinical conversations ($14,000$ hr), designed the task to represent the real word scenario, and explored several alignment approaches to iteratively improve data quality. We explored both CTC and LAS systems for building speech recognition models. The LAS was more resilient to noisy data and CTC required more data clean up. A detailed analysis is provided for understanding the performance for clinical tasks. Our analysis showed the speech recognition models performed well on important medical utterances, while errors occurred in causal conversations. Overall we believe the resulting models can provide reasonable quality in practice. ", "text": "varying quality covers range speech patterns accents background noises vocabulary colloquial complex domain-speciﬁc language. handle long-form content interweaves clinically salient information casual chatter. developing system medical conversations complicated lack large corpora clean curated data build systems from. data gathered recording actual conversations results noisy data issue arising real conversations disﬂuencies simultaneous speech signal noise name few. dealing factor requires signiﬁcant amount data pre-processing work without difﬁcult train traditional speech recognition systems. further parts conversation important others casual conversations doctor patient important ones describing underlying symptoms treatments etc. recently neural network based speech recognition system built medical domain using relatively small medical speech data benchmarked medical transcriptionists speech recognition systems evaluated clinical question answering task shown domain adaptation language model improves accuracy interpreting spoken clinical questions signiﬁcantly language model adaptation using crowdsourced input data shown improve accuracy medical speech recognition system efﬁciency safety using speech recognition assisted clinical documentation compared keyboard mouse found sub-optimal speech recognition accuracy potential cause clinical harm recent development end-to-end speech models provides promising alternatives. listen attend spell model end-to-end model able learn language model part model itself. paper compare experiences building speech recognition system traditional based models uses initialization. model quite robust noisy transcripts require language model. phoneme based model hand works well signiﬁcant data cleaning effort undertaken matched language model developed domain. models achieved model achieved evaluated performance models capturing important medical phrases. selected subset conversations asked group professional medical scribe annotate important phrases conversation useful writing medical notes. model achieves precision recall bidirectional models precision recall unidirectional models important paper document experiences developing speech recognition medical transcription system automatically transcribes doctor-patient conversations. towards goal built system along different methodological lines connectionist temporal classiﬁcation phoneme based model listen attend spell grapheme based model. train models used corpus anonymized conversations representing approximately hours speech. noisy transcripts alignments corpus signiﬁcant amount effort invested data cleaning issues. describe two-stage strategy followed segmenting data. data cleanup development matched language model essential success based models. based models however found resilient alignment transcript noise require language models. models able achieve word error rate models able achieve analysis shows models perform well important medical utterances therefore practical transcribing medical conversations. index terms medical transcription conversational transcription end-to-end attention models transcription medical space started stenographers early century. proliferation technologies around healthcare system began adopt single speaker technology assist doctor dictations. recently widespread adoption electronic health record systems doctors spending hours hour workdays inside hours documentation alone. growing shortage primary care physicians higher burnout rates technology could accelerate transcription clinical visit seemed imminently useful. foundational technology information extraction summarization technologies build help relieve documentation burden. medical conversations patients providers several distinguishing characteristics normal dictations involves multiple speakers overlapping dialogue different distances microphones explored mechanisms building speech recognition models task used recurrent neural network connectionist temporal classiﬁcation used end-to-end models attention system consists acoustic model trained loss context-dependent phoneme outputs n-gram language model pronunciation dictionaries. decoding done using ﬁnite-state-transducer-based decoder. trained unidirectional bidirectional models task. end-to-end model attention consists encoder attention mechanism decoder encoder takes utterances input generates sequence hidden states decoder uses attention mechanism attend encoder output prediction step generate grapheme output sequentially. framework encoder plays role acoustic model processes speech utterance transforms high level representation decoder attends output sequence generate transcripts. decoder uses attention results prediction previous step decide attention current step. de-identiﬁed data used task acquired large dialogue research organization. audio conversations collected placing recording device clinic consent patient. recorded audio compressed sent human annotators transcription. ﬁnal transcripts contained speaker turns speaker codes transcription process personal data de-identiﬁed zeroing corresponding audio using special transcript. approximate speaker turn timestamps also provided. found timestamps often order seconds. conversation also received metadata included type interaction gender doctor unique identiﬁer doctor. make training testing tractable segmented conversations speaker turn segments. accuracy speaker turn boundaries training data found play major role performance models trained segments. tried following approaches better speaker turn segments. training bidirectional model original segments gave wer. speaker turn alignments added audio buffer beginning turn segment realigned buffered audio turn transcript. ﬁnally created speaker turn segments keeping audio better aligned data training testing forcealigned entire conversation audio conversation transcripts using two-pass forced alignment approach. first pass conﬁdence islands. align conversation transcript using conﬁdence-islands approach described recognized audio using constrained grammar constructed follows. allows paths contiguous sequence words ground truth transcript. audio segmented chunks recognized using out-of-domain voicemail acoustic model grammar constructed above. recognition results concatenated resulting full recognition text aligned actual human transcript. consecutive sections recognition result match actual human transcript assumed trustable word timestamps. segment recognition explicitly exclude ﬁrst last words conﬁdence islands since could partially ﬁxed boundary. timestamps words fall conﬁdence islands ﬁxed second pass forced-alignment. able align words using pass forcealignment. second pass remaining words. sequence unaligned consecutive words ﬁrst pass ground truth transcript compute corresponding audio segment using timestamps conﬁdence islands precede follow sequence ground truth. since words already aligned ﬁrst pass audio segments large able fully-constrained force alignment segment audio sequence unaligned words. using second pass method timestamps words. timings rest words interpolated neighbouring words passed forced-alignment. word-level alignments full audio segmented single speaker turns. found turn level alignments obtained method sufﬁciently accurate training acoustic models require buffered audio approach described section data comprises single channel transcribed conversations doctors patients clinical visits. total amount data hours. conversations represent types medical visits serve different purposes wellness visit type diabetes rheumatoid arthritis etc. conversation typically between single doctor patient sometimes also including nurse family member. average conversation long exceptional conversations long hrs. sampled conversations test ensuring conversation different doctor equal split male female doctors. conversations containing doctors excluded leaving conversations training doctor overlap test set. test conversations represented target disease areas. disease areas dermatology comprise interaction types eczema melanoma acne. conversations sampled non-target disease areas. training testing used speaker turn segments obtained pass force-alignment explained section general preprocessing removed speaker turn segment contained de-identiﬁed transcript test training set. manual inspection showed issues segmentation small utterances. reduce type errors removed speaker turn smaller words test set. training data removed single word turns. training attention models phoneme based models features used fast fourier transform ﬁlterbank channels computed audio frames stride input model stack frames take stride i.e. input ﬁlterbanks every computing ffts ignore frequencies make models robust noise prevent overtraining added artiﬁcial noise audio training called multi-style training setup training utterance combined different noises ranging found best results obtained noise added using ctc/cross entropy training criteria original audio used embr training task trained models context-dependent phoneme outputs. model architecture stack lstm layers feeding softmax output predicting phone units plus blank symbol. phones computed described training data initially based alignments out-of-domain voice-mail model. trained in-domain model loss phoneset realigned training data model computed phoneset used training models. trained bidirectional unidirectional models task. since conversational data segments generally enough silence give model time output phonemes frames. found good unidirectional model train models output delay ignore output model ﬁrst frames repeat last input frame times. decoding used -gram language model trained mixture medical voice search/dictation data using bayesian interpolation combining data different domains. pronunciations used supervised pronunciation dictionaries grapheme-tophoneme models chitecture stacked bi-directional lstms. bi-directional lstms combine information forward backward directions leading output hidden states access whole utterance information peaked local utterance frames. access whole utterance helps encoder acoustic model distinguish utterance signal environmental noise. beneﬁt make attention mechanism easier learn. since output hidden state contain mostly local utterance information access whole utterance distinguishing noise attention mechanism learn focus mainly frames corresponding current prediction target. model multi-headed attention extends conventional attention mechanism multiple heads head generate different attention distributions. scheduled sampling training decoder. beginning ground truth previous prediction training proceeds linearly ramp probability sampling model’s prediction stay sampling rate till end. decoder performs grapheme predictions. trained attention models phoneme based models using speaker turn based training data deﬁned section randomly sampled utterances training data made set. attention models shared train/test/dev data. trained unidirectional bidirectional models. unidirectional model output delay steps. model lstm layers units layer reached training criteria improved embr training. training model bidirectional lstm layers units layer gives improves embr training. table shows numbers model different subsets test set. tried adding convolution layers lstm layers encoder training without mtr. resulting model gives wer. adding scheduled sampling reduce latter adding convolution layers help tend provide worse quality therefore remove convolution layers encoder. without convolution layers goes adding multiheaded attention leads wer. also tried embr training improve further. encoder layers bi-directional lstms units direction decoder layers uni-directional lstms attention models models comparable sizes give similar embr training models. bigger attention model outperforms model. although attention models perform badly small utterances attention models performs better ctc. found small utterances like yeah attention model gets output correct time even though audio incomprehensible. attention model also performs better audio truncated abruptly beginning utterance. general attention model resilient data processing. isolation utterances often completely incomprehensible transcribers likely used context understand them. lots short words ﬁlled transcribers whether audible e.g. blood work transcribed blood work. vocal qualities make language hard understand e.g. accents laughing whispering etc. patient speech often softer distant mostly recording setup). audio also compressed likely caused loss quality. errors seem appear beginning utterances e.g. that’s needed becomes that’s need anecdotally observed speakers talking other model deletes words region audio. major portion errors accounted small speaker turn segments longer utterances model misses conversational part speech e.g. transcribed mean. general patient speaker turns worse doctors possibly recording device placed closer doctors hence doctors’ speech much clearer patient. detailed analysis respect speaker gender disease area shown table performance model important medical phrases collected supervised list medical phrases ground truth transcripts conversations evaluation often recognize phrases cortable modeling results. results base model without. results without embr training. re-tuned las’s architecture adding therefore improvement error rate reﬂect changes. rectly segments audio. collecting phrases group medical scribe asked mark important phrases conversation useful writing medical notes results showed model based recognition achieves precision recall bidirectional models precision recall unidirectional models recognizing important medical phrases. also noteworthy information particularly patient instructions oftentimes repeated doctor ensure comprehension patient mitigating errors. scanning transcript errors conversational unrelated medical terms. among errors related medical terms usually related acoustic modeling lack medical terms. example transcribe don’t penny don’t byetta penny model replaces common word medical term. external language model less casual conversational content learn from. work explored building automatic speech recognition models transcribing doctor patient conversation. collected large scale dataset clinical conversations designed task represent real word scenario explored several alignment approaches iteratively improve data quality. explored systems building speech recognition models. resilient noisy data required data clean detailed analysis provided understanding performance clinical tasks. analysis showed speech recognition models performed well important medical utterances errors occurred causal conversations. overall believe resulting models provide reasonable quality practice. would like thank jack michael pearson kasumi widner data business development management francoise beaufays claire jeff dean high-level sponsorship research. dall chakrabarti iacobucci hansari west complexities physician supply demand projections markit behalf association american medical colleges tech. rep. shanafelt hasan dyrbye sinsky satele sloan west changes burnout satisfaction work-life balance physicians general working population mayo clinic proceedings vol. edwards salloum finley fone cardiff miller suendermann-oeft medical speech recognition reaching parity humans. cham springer international publishing hakkani-tr towards spoken clinical-question answering evaluating adapting automatic spoken clinical questions speech-recognition systems informatics association journal vol. available +http//dx.doi.org/./amiajnl-- salloum edwards ghaffarzadegan suendermannoeft miller crowdsourced continuous improvement medical speech recognition workshops thirty-first aaai conference artiﬁcial intelligence hodgson magrabi coiera efﬁciency safety speech recognition documentation electronic health record journal american medical informatics association vol. available http//dx.doi.org/./jamia/ocx graves fernndez gomez connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks proceedings international conference machine learning icml liao mcdermott senior large scale deep neural network acoustic modeling semi-supervised training data youtube video transcription ieee workshop automatic speech recognition understanding lippmann martin paul multi-style training robust isolated-word speech recognition icassp ieee international conference acoustics speech signal processing vol.", "year": "2017"}