{"title": "Can we steal your vocal identity from the Internet?: Initial  investigation of cloning Obama's voice using GAN, WaveNet and low-quality  found data", "tag": "eess", "abstract": " Thanks to the growing availability of spoofing databases and rapid advances in using them, systems for detecting voice spoofing attacks are becoming more and more capable, and error rates close to zero are being reached for the ASVspoof2015 database. However, speech synthesis and voice conversion paradigms that are not considered in the ASVspoof2015 database are appearing. Such examples include direct waveform modelling and generative adversarial networks. We also need to investigate the feasibility of training spoofing systems using only low-quality found data. For that purpose, we developed a generative adversarial network-based speech enhancement system that improves the quality of speech data found in publicly available sources. Using the enhanced data, we trained state-of-the-art text-to-speech and voice conversion models and evaluated them in terms of perceptual speech quality and speaker similarity. The results show that the enhancement models significantly improved the SNR of low-quality degraded data found in publicly available sources and that they significantly improved the perceptual cleanliness of the source speech without significantly degrading the naturalness of the voice. However, the results also show limitations when generating speech with the low-quality found data. ", "text": "drawback studies using asvspoof corpus studio-recorded speech used generating spooﬁng materials. think potential spooﬁng scenarios reasonable expect attacks using unwillingly obtained speech motivated study feasibility training speech enhancement system obtaining publicly available data using tools enhance speech quality found data creating reasonable state-of-the-art spooﬁng materials. case using publicly available found data generating spoofed speech easy understand large amounts speech data publicly available sources likely data almost anybody found another. talking public personalities like president barack obama amount data publicly available immense. data commonly recorded non-professional acoustic environments homes ofﬁces. moreover recordings often made using consumer devices smartphones tablets laptops. therefore speech portions recordings typically poor quality contain large amount ambient noise room reverberation. however applications developed speaker adaptation speech synthesis voice conversion normally designed work clean data optimal acoustic quality properties. therefore quality systems trained using data found publicly available sources unknown. speciﬁc objective answer questions. first well train speech enhancement system enhance low-quality data found publicly available sources? second enhanced data produce effective spooﬁng materials using best available systems? generative adversarial networks consist adversarial models generative model captures data distribution discriminative model estimates probability sample came training data rather structure used enhance speech research reported here worked improving speech enhancement generative adversarial network speciﬁcally attempted make training process robust stable introducing modiﬁed training thanks growing availability spooﬁng databases rapid advances using them systems detecting voice spooﬁng attacks becoming capable error rates close zero reached asvspoof database. however speech synthesis voice conversion paradigms considered asvspoof database appearing. examples include direct waveform modelling generative adversarial networks. also need investigate feasibility training spooﬁng systems using low-quality found data. purpose developed generative adversarial networkbased speech enhancement system improves quality speech data found publicly available sources. using enhanced data trained state-of-the-art text-to-speech voice conversion models evaluated terms perceptual speech quality speaker similarity. results show enhancement models signiﬁcantly improved low-quality degraded data found publicly available sources signiﬁcantly improved perceptual cleanliness source speech without signiﬁcantly degrading naturalness voice. however results also show limitations generating speech low-quality found data. asvspoof commonly used database developing evaluating methods preventing spooﬁng automatic speaker veriﬁcation systems. recent studies shown good spooﬁng detection rates asvspoof corpus equal error rates close zero however recent advances text-to-speech voice conversion techniques speech synthesis ﬁeld become clear systems included corpus date. good examples state-of-the-art speech synthesis techniques include direct waveform modeling generative adversarial networks recent study speech synthesized using end-to-end direct waveform model rated natural human speech. preliminary study showed direct waveform models fool speaker recognition systems; however mel-spectra generated successfully fooled speaker recognition system segan exploits generative adversarial structure particular fashion. speech enhancement carried mainly using model follows encoder-decoder structure takes noisy speech input produces enhanced speech output similar u-net architecture used pixpix framework function model determine training whether enhanced speech detected fake real model fooled enhanced speech gradients model hand model cannot fooled enhanced speech gradient back-propagated model update order fool model thus biasing structure towards producing enhanced speech closer clean speech. found segan structure sensitive noise variations training making convergence difﬁcult. thus made modiﬁcations achieve robust training. ﬁrst created pre-trained baseline speech enhancement models used enhance speech content loss initial iterations generator model computed basis baseline enhanced speech instead clean speech. second modiﬁcation skip connection added around generator task generate enhanced speech scratch generate residual signal reﬁnes input noisy speech encourage generator learn detailed differences clean enhanced speech waveforms. used types speech corpora. corpora used train speech enhancement module constructed using publicly available data training process would replicable. corpus used source cloned voice constructed using number president barack obama’s public interventions obtained various sources. training speech enhancement module used subset centre speech technology research voice cloning toolkit corpus clean speech corpus. used different noisy iterations corpus create four additional corpora making speech enhancement signal robust noisy and/or reverberant environments corrupted corpora recorded collaboration cstr national institute informatics japan publicly available datashare repository university edinburgh. device-recorded vctk corpus create device-recorded vctk corpus re-recorded high-quality speech signals original vctk corpus playing back recording table characterization used obama’s found data. sources total length minimum segment duration maximum segment duration average segment duration estimated mean estimated variance ofﬁce environments using relatively inexpensive consumer devices. corpus enables speech enhancement system learn nuanced relationships high quality device-recorded versions audio. eight different microphones used simultaneously re-recording carried medium-sized ofﬁce background noise conditions resulted different conditions. used three artiﬁcially corrupted variations cstr vctk corpus noisy vctk reverberant vctk noisy reverberant vctk diverse portfolio possible speech corruptions enables speech enhancement system learn target different possibilities i.e. plain noisy reverberation compensation mixture both. mentioned above corpora based cstr vctk corpus speakers utterances represented edinburgh noisy speech dataset similar dr-vctk corpora. obama’s data found online mainly youtube videos transcriptions part description diverse sources interviews political meetings. recording conditions environments diverse ranging noisy large amounts reverberation noisy reverberant never achieving recording studio standards. audio channel split video channel automatically segmented long pauses down-sampled khz. transcription copied text ﬁle. table shows brief characterization data. histogram signal-to-noise ratio estimated using nist measurement tool shown ﬁgure evident vast majority speech signals compared conventional speech generation corpus standards. mean variance snrs estimated vctk corpus https//datashare.is.ed.ac.uk/handle// https//datashare.is.ed.ac.uk/handle// https//datashare.is.ed.ac.uk/handle// https//www.nist.gov/information-technologysimilar original segan training strategy extracted chunks waveforms using sliding window samples every samples testing time concatenated results stream without instead zero padding overlapping. last chunk pre-padded previous samples. batch optimization rmsprop learning rate batch size used. modiﬁed segan model converged epochs. selecting pre-enhancement method conducted preliminary experiments using postﬁsh hrnr sequential enhanced quality samples used compound method generate baseline models described section speech enhancement models trained applied noisy data. effect enhancement process evaluated estimating using nist tool. although likely best measure enhancement lack clean reference limited availability tools. estimation results show clear picture enhancement process regardless training data used improved average original obama voice data. particular training using noisy data considerably effective training using possibilities attributed fact reduces noise levels signal measure targets. actually improve perceptual quality voice signals. histograms showing improvement variants used shown ﬁgures respectively. mentioned previous section likely best measure enhancement. since ultimate objective research produce high quality synthetic speech speech synthesis voice conversion makes sense evaluate perceptual quality viewpoint human users. thus carried crowdsourced perceptual evaluation japanese native listeners. presented listeners screens corresponding eight evaluated conditions utterances. evaluators given mentioned above research create reasonable state-of-the-art spooﬁng materials. speciﬁcally aimed train waveform generation model replicate target speakers voice case recognizable voice president barack obama. moreover training done using easily available low-quality resources explained section kind data generally poor ensure reasonably good training speech synthesis systems. thus developed generative adversarial network-based speech enhancement system improving lowquality data point used properly train systems. large amount free publicly available resources training speech enhancement system testing determine best training regime strategy. trained speech enhancement system using seven different sources various amounts data summarized table motivation using three single-category sources four combinations expectation training using single-category source would work well corresponding type disturbance since data found come noisy poor-quality sources made sense combine different noisy corpora. moreover since much varied data possible helps neural networks generalize better combination corpora also effective. cleanliness result noisy-reverberant largest improvement. attributed original data recorded mostly noisy environments reverberation speech enhancement system targeting condition gives best improvement ﬁeld. cleanliness result similarly high attribute training done possible situations. hand cost applying speech enhancements consistent degradation perceived speech quality. implies speech enhancement focused cleanliness greatly reduce naturalness speech. means approaches providing biggest improvements noisy condition quality score noisy-reverberant condition quality score best produce clean speech speech processing. short seems trade-off quality improvement degradation cleanliness encouraging. look results condition combining possible data sources provided best cleanliness scores smallest quality degradations strongly suggests trained speech enhancement system variety degradation conditions gave system enough generalization capability enough knowledge human speech reduce noisiness maintaining much possible voice naturalness. generate spoofed speech used approaches waveforms cyclegan autoregressive neural network acoustic modeling approaches generate melspectrograms cyclegan converts mel-spectrogram source speaker mel-spectrogram retains speech contents overlays voice characteristics target speaker. contrast approach converts linguistic features extracted text mel-spectrogram target speaker. given generated mel-spectrogram target speaker wavenet neural network generates speech waveforms process generating spoofed speech illustrated ﬁgure decision mel-spectrogram acoustic feature based expected limitations traditional features estimation problematic original noisy speech signals enhanced signals considering noisy data found. also used increased number bands compared approaches expectation would help waveform model better participants able listen speech sample many times wanted could proceed next sample completed tasks. allowed return previous samples. samples selected basis length evaluating utterances seconds long. total meant sets evaluating evaluation utterances done times total sets. participants allowed repeat evaluation times thereby ensuring least different listeners. total listeners took part evaluation results perceptual evaluation also show clear picture. original obama voice data clear perception noisiness related factors even though perceived quality reasonably high studio recorded clean speech normally rated average whereas original obama voice data rated point less likely poor conditions sources recorded. figure diagram cyclegan. generators; discriminators. real distributions represent corresponding generated distributions. ‘ff’ means feed-forward neural network. cyclegan originally developed unpaired image-toimage translation consists generators discriminators shown ﬁgure generator serves mapping function distribution distribution generator serves mapping function discriminators estimate probability sample came real data rather generated sample shown ﬁgure cyclegan translation forward backward means translation translation learned simultaneously. furthermore input carried back original form translation direction thereby minimizing consistency loss expectation norm. structure possible keep part information unchanged input translated applying model thought feature distribution source speaker target speaker respectively. reconstructing input data linguistic information retained translation. additionally speaker individuality changed adversarial learning using adversarial loss integrating consistency loss adversarial loss learn mapping function using non-parallel database figure neural network acoustic modeling. tanh linear denote feedforward layers tanh identity activation function respectively. bi-lstm unilstm denote bi-directional uni-directional lstm layers. time delay block keeps mel-spectrogram frame sends uni-lstm frame president barack obama. original voice data enhanced data used. accordance source speaker’s data sets implemented three systems using japanese utterances using english utterances using mixture japanese english utterances. generator discriminator cyclegan fully connected neural network layers. -dimension vector consisting -dimension mel-spectrogram ﬁrst second derivatives input cyclegan. units hidden layers. sigmoid used activation function. batch normalization conducted hidden layer generators. batch size learning rate generators discriminators randomly selected frames respectively. acoustic model converts linguistic features extracted text acoustic features melspectrogram. speciﬁcally work given sequence linguistic feature {l··· frames acoustic model needs generate sequence acoustic features {a··· number frames. here denote linguistic features mel-spectrogram n-th frame. neural network illustrated ﬁgure used convert network feedforward layers bi-directional long-short-term-memory unit recurrent layer near input side. following three layers uses another uni-directional lstm layer. different ﬁrst lstm layer takes output previous layer also previous output whole network input. example takes input generates n-th frame. type data feedback widely used neural text generation machine translation network type feedback loop referred autoregressive model. note that natural training back evaluate generation capabilities proposed system carried second crowd-sourced perceptual evaluation japanese native listeners. evaluation conducted described section tasks different rate quality voice speech sample ignoring effects background noise rate similarity voice spoofed speaker ignoring speech quality cleanliness. compare target speaker presented participants additional samples containing linguistic content different used rating speech quality. participants asked rate scale similarity speakers. three different voice conversion systems aimed evaluate effect noise investigate whether cyclegan used cross-lingual systems. three systems aimed analyze effect generating spectrograms basis different conditions speech enhancement determine importance mimicking environmental conditions reference natural speech considering human perception. extracted using flite dimension feature vectors alignment information obtained using forced alignment hidden-semi markov model trained using mel-spectrograms. addition linguistic features numeric variable characterizing enhancement condition used input. together network trained using input features mel-spectrograms obtained enhanced speech method explained section dropout rate building state-of-the-art data-driven vocoder wavenet represents challenge trying types data found easy gather sufﬁcient data good enough process. advantage used another data-driven speech enhancement system comes play. hinted introduction section take advantage gan-based speech enhancement system generate multiple enhanced versions noisy speech data effectively multiplying amount training data available training system. research trained wavenet vocoder enhanced version original obama voice data. network structure illustrated ﬁgure vocoder works sampling rate khz. µ-law compressed waveform quantized bits sample. similar previous study network consists linear projection input layer wavenet blocks dilated convolution postprocessing block. k-th dilation block dilation size modulo operation. addition bi-directional lstm convolution layer used process input acoustic features. acoustic features every wavenet block contain -dimensional melspectrogram plus additional component specifying different speech-enhancing models produced speech waveform. participants able identify actual obama voice regardless environmental conditions also indicate able distinguish naturalness frequency speech regardless background noise and/or reverberation results copy synthesis using trained wavenet vocoder quite different wavenet system scripts previously successful generating speech clean speech used training suggesting difference nature data used training. possibility using mixture density networks generating output waveforms problematic modeling variance noisy data. looking system results quality generated speech change signiﬁcantly generation condition similar copy synthesis. adding small amount reverberation improved perceived speech quality even higher copy synthesis means reverberation mask part noise generated wavenet vocoder. signiﬁcant drop similarity score means cannot whether evaluators capable identifying obama’s voice. system results similar pattern. terms quality systems slightly signiﬁcantly better copy synthesis. probably systems trained selected data clean data used source speaker. terms similarity slightly signiﬁcantly better worse copy synthesis. comparing achieved similar scores speech quality speaker similarity. suggests cyclegan used train bilingual model. japanese english utterances mixed speech quality slightly higher systems. probably twice amount source speaker training data used. also evaluated built systems based anti-spooﬁng countermeasures. countermeasure used common gaussian mixture models back-end classiﬁer constant cepstral coefﬁcient features training countermeasure speaker-independent used alternative training sets train gmms ﬁrst contains training portion asvspoof data consisting spooﬁng attacks second consists converted audio samples submitted voice conversion challenge participants latter contains diverse stronger attacks. table shows evaluation results cqcc-gmm countermeasure scored found data obama results presented terms equal error rate spooﬁng countermeasure. higher table although systems paper advanced methods ones included current asvspoof countermeasure models still detect proposed samples using found data easily. process also additional speech enhancement process caused noticeable artifacts. introduced number publicly available known datasets proved extremely useful training speech enhancement models. application models corpus low-quality considerably degraded data found publicly available sources signiﬁcantly improved data. perceptual evaluation revealed models also signiﬁcantly improve perceptual cleanliness source speech without signiﬁcantly degrading naturalness voice common speech enhancement techniques applied. speech enhancement effective system trained using largest amount data available covered wide variety environmental recording conditions thereby improving generalization capabilities system. second perceptual evaluation revealed that generating synthetic speech noisy publicly available data starting become possible still obvious perceptual problems text-to-speech voice conversion systems must solved achieve naturalness systems trained using high quality data. therefore cannot recommend next-generation asvspoof data generated using publicly available data even adding paradigm speech generation systems must. acknowledgements work partially supported mext kakenhi grant numbers yamagishi kinnunen hanili sahidullah sizov evans todisco delgado asvspoof automatic speaker veriﬁcation spooﬁng countermeasures challenge ieee journal selected topics signal processing vol. june muckenhirn magimai-doss marcel end-to-end convolutional neural network-based voice presentation attack detection proc. ijcb oord dieleman simonyan wang skerry-ryan stanton weiss jaitly yang xiao chen bengio agiomyrgiannakis clark saurous tacotron towards end-to-end speech synthesis proc. interspeech kaneko kameoka hojo ijima hiramatsu kashino generative adversarial network-based postﬁlter statistical parametric speech synthesis proc. icassp shen schuster jaitly skerry-ryan saurous weiss pang agiomyrgiannakis zhang wang chen yang natural synthesis conditioning wavenet spectrogram predictions proc. icassp donahue prabhavalkar exploring speech enhancement generative adversarial networks robust speech recognition proc. icassp isola j.-y. zhou efros image-toimage translation conditional adversarial networks proc. cvpr yamagishi speech enhancement noise-robust text-to-speech synthesis system using deep recurrent neural networks interspeech investigating rnn-based speech enhancement methods noise-robust text-to-speech isca speech synthesis workshop weninger bergmann schuller introducing currennt–the munich open-source cuda recurrent neural network toolkit journal machine learning research vol. wang lorenzo-trueba takaki juvela yamagishi comparison recent waveform generation acoustic modelingmethods neuralnetwork-based speech synthesis proc. icassp april", "year": "2018"}