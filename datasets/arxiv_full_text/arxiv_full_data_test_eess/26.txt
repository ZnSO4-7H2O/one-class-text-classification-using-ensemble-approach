{"title": "Superfast Line Spectral Estimation", "tag": "eess", "abstract": " A number of recent works have proposed to solve the line spectral estimation problem by applying off-the-grid extensions of sparse estimation techniques. These methods are preferable over classical line spectral estimation algorithms because they inherently estimate the model order. However, they all have computation times which grow at least cubically in the problem size, thus limiting their practical applicability in cases with large dimensions. To alleviate this issue, we propose a low-complexity method for line spectral estimation, which also draws on ideas from sparse estimation. Our method is based on a Bayesian view of the problem. The signal covariance matrix is shown to have Toeplitz structure, allowing superfast Toeplitz inversion to be used. We demonstrate that our method achieves estimation accuracy at least as good as current methods and that it does so while being orders of magnitudes faster. ", "text": "abstract—a number recent works proposed solve line spectral estimation problem applying off-the-grid extensions sparse estimation techniques. methods preferable classical line spectral estimation algorithms inherently estimate model order. however computation times grow least cubically problem size thus limiting practical applicability cases large dimensions. alleviate issue propose low-complexity method line spectral estimation also draws ideas sparse estimation. method based bayesian view problem. signal covariance matrix shown toeplitz structure allowing superfast toeplitz inversion used. demonstrate method achieves estimation accuracy least good current methods orders magnitudes faster. problem line spectral estimation received signiﬁcant attention research community least years. reason many fundamental problems signal processing recast lse; examples include direction arrival estimation using sensor arrays bearing range estimation synthetic aperture radar channel estimation wireless communications simulation atomic systems molecular dynamics trying solve problem classical approaches include subspace methods music esprit estimate frequencies based estimate signal covariance matrix. approaches must augmented method estimation model order. popular choices include generic information theoretic criteria specialized methods sorte based eigenvalues estimated signal covariance matrix. subspace methods typically perform extremely well model order known estimation accuracy degrade signiﬁcantly model order unknown. inspired ideas sparse estimation compressed sensing many papers sparsity-based algorithms hansen fleury department electronic systems aalborg university aalborg denmark. electrical computer engineering department university california diego. work hansen supported danish council ieee. personal material permitted. permission ieee must obtained uses current future media including reprinting/republishing material advertising promotional purposes creating collective works resale redistribution servers lists reuse copyrighted component work works. appeared recent years e.g. particular problem simpliﬁed ﬁnite sparse reconstruction problem restricting frequencies grid. methods inherently estimate model order alleviating issues arising separate model order frequency estimation classical methods. granularity grid leads non-trivial trade-off accuracy computational requirements. forego grid so-called off-the-grid compressed sensing methods proposed methods provably recover frequencies noisefree case minimum separation condition. suffer prohibitively high computational requirements even moderate problem sizes bayesian view taken problem. model used stochastic extended sparsity-promoting prior coefﬁcients sinusoid components. thereby inherent estimation model order achieved. algorithms generally high estimation accuracy. per-iteration computational complexity cubic number sinusoidal components meaning runtime grows rapidly number components increases. work introduce superfast algorithm solving problem scenarios full measurement vector available modelling design basic algorithm present sec. based upon ideas main novelty resides computational aspects superfast lse. derived method based upon several techniques so-called superfast toeplitz inversion algorithm low-complexity capon beamforming gohberg-semencul formula non-uniform fast fourier transforms superfast algorithm following virtues inherently estimates model parameters noise variance model order per-iteration computational complexity. speciﬁcally scales length observed vector. show empirically converges iterations means large problem sizes algorithm computation time orders magnitude lower current methods. without penalty estimation accuracy. numerical experiments show superfast high estimation accuracy across wide range scenarios better state-of-the-art algorithms. synergistically computationally efﬁciently combining steps algorithm might appear easy fact. however case. algorithms beneﬁt terms computational effort approach extent achieved proposed algorithm. instance computational methods sec. embedded valse resulting scheme high estimation model order follow employ model kmax components. component associated activation variable deactivate activate activation variables collected sparse vector effective estimated model order given number active components. based write estimation model denotes probability density function circularly symmetric complex normal random variable mean covariance matrix assume arbitrarily small constant guarantees likelihood function bounded below. bernoulli prior used promote deactivation components active-component variance. sparsitypromoting priors previously used basis selection bernoulli-gaussian prior structure adopted ﬁrst introduced used even though modelled gaussian prior speciﬁcation signiﬁcantly general because variance component estimated numerical investigation demonstrate method works well even true density coefﬁcient gaussian. computational complexity variational estimation posterior frequencies. note algorithm performs valse signiﬁcantly reduced computational effort. completeness also present semifast version algorithm works subset entries measurement vector available. semifast algorithm per-iteration complexity number estimated sinusoids. algorithms similar per-iteration complexity derived observed algorithm converges smaller number iterations compared algorithm thus leading lower total runtime. outline sec. present modelling algorithm lse. low-complexity computational methods presented sec. sec. algorithm extended case multiple measurement vectors. numerical experiments presented sec. conclusions given sec. vii. notation write vectors matrices entry vector denoted entry matrix denoted aij. binary vector dimension denotes vector contains entries corresponding entry one. hadamard product denoted steering vector function entry gives fourier vector expθk) also deﬁne ψ)]. measurement matrix cm×n either identity matrix made subset rows diagonal matrix vector white gaussian noise vector component variance problem recovering model order along frequency coefﬁcient component estimation model inference approach present following adaptions ideas currently available literature. carefully combined ideas obtain iterative scheme implemented complexity described secs. achieving performance comparable state-of-the-art algorithms. algorithm based bayesian inference estimation model approximates speciﬁcally enable variables model parameters estimated minimizing i.e. seek maximum aposteriori estimate estimate algorithm employs block-coordinate descent method local minimum estimates computed estimated model order given number active comi.e. ˆz|| entries ponents estimated frequencies. corresponding coefﬁcients estimated follows. first write posterior expected posterior coefﬁcients corresponding inactive components coincides prior. interest integrating gives gaussian posterior point estimate needed estimate used. mentioned algorithm derived blockcoordinate descent method applied estimates updated following blocks update guaranteed increase note frequencies variances inactive components estimation frequencies coefﬁcient variances even remaining variables kept ﬁxed tractable global minimizer respect vector active component frequencies variances therefore resort numerical method. writing terms depend similar optimization problem involving frequencies solved newton’s method. directly applying approach case leads high computational complexity. methods based gradient descent also proposed observed using approach leads slow converge. concerned computational speed paper instead limited memory broyden-fletcher-goldfarb-shanno algorithm algorithm requires evaluation objective function gradient. following demonstrate evaluations performed complexity. time per-iteration l-bfgs linear namely number saved updates used l-bfgs. implementation observed l-bfgs converges small number iterations. l-bfgs algorithm requires initial estimate hessian subsequently updated iteration algorithm. every update activation variable results change dimension hessian changes). means implicit estimate hessian l-bfgs algorithm reinitialized rather frequently estimation scheme. result degree accuracy initialization hessian signiﬁcant impact convergence speed algorithm. therefore propose initialize l-bfgs diagonal approximation hessian. shown below diagonal entries hessian obtained computational complexity. initial estimate hessian must positive deﬁnite. achieved diagonal entries positive. entries diagonal hessian negative therefore replaced following values entries corresponding frequency variables diagonal hessian entries corresponding variance component heuristic values determined considering diagonally scaled version optimization problem follows required ﬁrstsecond-order partial derivatives evaluated current estimates hints obtained) estimation activation probability objective convex function variables ﬁxed global minimizer found differentiating setting equal zero. considering constraints update estimation noise variance even keeping remaining variables ﬁxed current estimate globally minimizing noise variance cannot found closed form. obvious alternative approach would incorporate estimation l-bfgs together estimation however observed approach exhibit slow convergence objective function rather variable sparse bayesian learning similar estimation problem solved successfully expectation-minimization algorithm. need reintroduce estimation problem. order show integrated coordinate-block descent method update guaranteed increase easiest directly upper bound associated derivation takes similar approach). updated estimate minimizer upper bound objective function obtain upper bound write terms objective function depend variables kept ﬁxed current estimates update could applied repeatedly since improved upper bound used time. since observed advantages simply perform update pass block-coordinate descent algorithm. also note even though known prone slow convergence speed observed empirically estimate converges fast typically within iterations. deactivation components describe activation deactivation components performed single likely replacement detector smlr previously demonstrated perform well first write terms depend variables pertaining component variables current estimate. based woodbury’s matrix inversion identity determinant lemma details) ˆc∼k ˆβi+aˆγ ˆz∼k ˆz∼k equal entry forced zero. matrix ˆc∼k thus marginal covariance matrix observation vector component deactivated. evaluate active component deactivated test objective increased i.e. test gives deactivation criterion component component activation describe method decide deactivated component activated. also involves estimating frequency variance component meaningful estimates available component activated. deactivated components equally good candidates activation. following refers arbitrary value exists components already activated activation step carried out. grid equispaced values i.e. /l}. restriction estimated frequencies grid mean ﬁnal frequency estimates grid reﬁned subsequent updates frequency vector. reason choice impact estimation activation procedure continues decrease objective obtained activating component i.e. −εl. inclusion constant purely technical simpliﬁes convergence analysis. chosen arbitrarily small select machine precision implementation. denote left-hand side quantity interpreted signal-to-noise ratio component sparse bayesian learning model used sparsity promotion activation criterion obtained algorithms using activation criterion known prone activation artefact components small seems arbitrary frequencies ˆθk. right-hand side always larger helps reduce number artefacts activated demonstrated favorable phenomenon caused average deﬁnition resembles approach). even still observed activation artefact components numerical investigations. therefore follow idea heuristically adjust criterion obtain |q∼k| adjustment threshold. speciﬁcally select numerical study sec. vi-b. numerical experiments show simple approach effective avoiding inclusion small spurious components. since heuristic criterion stricter numerical investigation shows algorithm invariant choice provided implementation equal rounded nearest power when component effectively deactivated corresponding coefﬁcient zero-mean prior zero variance effective deactivation also seen deﬁnition manifests theorem assume l-bfgs step converges stationary point sequence estimates obtained algorithm converges. further limit point stationary point sense karush-kuhn-tucker necessary conditions minimum fulﬁlled. step check component activation repeated components activated. updates step iterated either approximated squared newton decrement l-bfgs method times. observant reader noticed minimization must constrained turns constraint handled simple manner notice deactivation criterion always fulﬁlled sufﬁciently small. constraint therefore never active solution. therefore simply need restrict linesearch performed l-bfgs entry ever becomes negative. approaches zero deactivated step note approach resembles l-bfgs constraints except deactivation variables constraint active happens automatically algorithm. algorithm initialized components deactivated stage initial values entries matter since assigned corresponding component activated noise variance initialized .||y||/m activation probability initialized appendix discuss detail convergence properties algorithm. ﬁndings summarized here. show algorithm terminates ﬁnite time estimates guaranteed converge. denote limit points estimates converged algorithm reduces pure l-bfgs scheme estimates non-convexity objective function cannot guarantee convergence lbfgs despite this never observed nonconvergence algorithm. experiments always converged local minimum objective function. therefore rely vast amount experimental validation convergence l-bfgs assume convergence stationary point. particular following theorem. number sinusoids observed signal high algorithm spends signiﬁcant computational effort activating components time component activated values must evaluated calculate alleviate computational effort building initial active components propose ﬁrst iterations approximate scheme activating components place step approximate activation scheme proceeds follows calculate evaluate find local minimizers i.e. values neighbouring grid-points local minimizers candidate frequencies. component activation criterion fulﬁlled. component variance non-zero. decrease objective obeys ∆lmin/ ∆lmin largest decrease obtained activating component another candidate frequency iii. superfast method algorithm presented rather large computational complexity particular inversion calculation section discuss updates algorithm evaluated computational complexity exploiting inherent structure problem. particular discuss evaluate ln|c| yhc−y method presented applicable complete observation vector available i.e. case observation vector wide-sense stationary process covariance distance measure wrap-around distance deﬁned note thus strictly lower triangular unit upper triangular values computed generalized schur algorithm time alternatively levinson-durbin algorithm also used obtain decomposition time latter algorithm signiﬁcantly simpler implement faster small concluded levinson-durbin algorithm requires fewer total operations generalized schur algorithm calculate value objective function need yhc−y ln|c|. inspecting clear matrix-vector products involving convolutions. implemented using techniques. product yhc−y thus calculated time {ρi} known. unit lower triangular. diagonal matrix computed generalized schur algorithm. diagonal entries given since determinant triangular matrix product diagonal entries note evaluated techniques using recognize matrix-vector products involving fourier transforms evaluated equispaced grid. products approximated high precision time using non-uniform fast fourier nufft calculates fourier transform arbitrary points interpolation combined fft. approximation made arbitrarily accurate including points interpolation. nufft achieves time complexity number off-the-grid frequency points evaluated. complexity equal constant hidden big-o notation much higher nufft. found signiﬁcant speedups achieved using nufft direct computation evaluation matrix-vector products involving matrix. particular speedup arises fact longer needs formed. calculate frequency component processed activation stage must evaluate grid equispaced points. using fact beginning activation step component deactivated thus obtain required quantities inserting since equispaced grid products evaluated length-l fft. vector therefore easy ﬁnd. rewriting form seen also evaluated length-l fft. computations time-complexity already calculated). summary time complexity iteration algorithm described sec. dominated either calculation {ρi} generalized schur algorithm calculation choice complexity iteration semifast method method presented sec. applicable incomplete observation vector available i.e. following introduce computational method used subsampling scaling matrix i.e. cm×n consists rows diagonal matrix. method still obtain algorithm reasonable computational complexity iteration assuming relatively small coin algorithm semifast. small semifast algorithm faster superfast algorithm sec. therefore beneﬁcial even complete data case. evaluated nufft time forming easy inversion time needed obtain approach thus hinges sufﬁciently small inverse calculated reasonable time. denote index observed entries indexing. φmim customary numerical linear algebra would recommend explicitly evaluate inverse instead numerically stabler faster approach calculating cholesky decomposition need evaluate matrix-vector products involving easily evaluated decomposition forward-backward substitution. also calculate ˆσ−| directly cholesky decomposition. clear easily found using techniques. obtain ﬁrst note ﬁrst term vector |φmim|. second term found using nufft form aha. using cholesky decomposition second term calculated time insightful reader noticed calculations required similar required svm. particular matrix unchanged methods calculating matrix-vector products involving presented secs. utilized. expressions involving must calculated observation vector means case complete observations generalized schur algorithm used algorithm per-iteration complexity incomplete observations semifast method used per-iteration complexity experiments signal model following wrap-around distance used differences frequencies unless otherwise noted true frequencies drawn randomly minimum separation frequencies speciﬁcally frequencies generated sequentially frequency drawn uniform distribution true coefﬁcients generated i.i.d. random entry drawn follows. first circularly-symmetric complex normal random variable standard deviation drawn. coefﬁcient found arg. resulting random variable property |˜αk| i.e. components signiﬁcant magnitude. speciﬁcation ensure components distinguished noise. generating algorithm presented sec. assumes single measurement vector discuss extension case multiple measurement vectors case particular importance array processing number observation points determined number antennas array. typically small thus limits estimation accuracy. hand often easy obtain multiple observation vectors across entries practically unchanged. signal model reads extend algorithm case impose estimation model form contains kmax components activated based variables kmax. likelihood observation vectors reads procedure estimate variables follows straightforwardly method used case. provide brief discussion derivation update equations; refer sec. details. estimate ﬁrstsecond-order derivatives lmmv needed. denote derivative replaced ∂lmmv misleading since trial considered unsuccessful single component misestimated; example component represented estimate components close frequencies. therefore introduce deﬁned follows success function denotes indicator function. reported averaged number monte carlo trials. takes values achieved estimated components vicinity true components true components vicinity estimated components. determine sensible value activation threshold following experiment conducted. consider complete data case number components ﬁxed larger tendency activate artefact components relatively large k/n. algorithm provided knowledge kmax activation probability ﬁxed algorithm activation criterion algorithm cases successfully estimates frequencies without artefacts. algorithm terminated test successfully recovered kmax increased procedure activating component sec. ii-c difference left-hand right-hand sides saved. refer difference criterion expressed fig. show histograms value obtained successful recoveries three different values experiment repeated required number successful recoveries obtained; trials without successful recovery discarded. cases thus correspond cases artefact would activated using criterion heuristic criterion corresponds fig. clearly seen threshold sensible value precludes almost artefact components activated. seen threshold works well compare superfast algorithm following reference algorithms variational bayesian line spectral estimation atomic soft thresholding gridless spice esprit gridded solution obtained least absolute shrinkage selection operator solved using sparsa solution primal problem directly provides estimate signal vector solution known biased towards all-zero solution so-called debiased solution obtained recovering frequencies dual estimating coefﬁcients least-squares. report debiased solution. frequencies separated least algorithm known exactly recover frequencies noise-free case noisy case recovery guarantee exists bound estimation error signal vector known unfortunately error bound apply debiased solution report herein. esprit requires estimate signal covariance matrix model order. former obtained averaged sample covariance matrix computed signal vector split signal vectors length using forwardbackward smoothing. model order estimated sorte lasso solution obtained using grid size observed improvement performance achieved ﬁner grid. regularization parameter lasso selected proposed knowledge true noise variance. debiased solution returned sparsa solver. proportion monte carlo trials frequency vector successfully recovered. successful recovery understood correct estimation model order ||d||∞ ./n. association entries obtained using hungarian published code github.com/thomaslundgaard/superfast-lse. based implementation generalized schur algorithm nufft available cims.nyu.edu/cmcl/nufft/nufft.html. large range values. investigated whether large desired components precluded activation. results following investigations obtained good performance algorithm across wide selection scenarios indicates selected large. fig. show performance results versus snr. ﬁrst notice superfast outperforms algorithms three metrics shown values. region algorithm reliably recover correct model order frequencies. plots nmse superfast valse generally achieve best approximation true frequencies. small performance terms nmse oracle evaluated algorithms uncertainty frequency estimation esprit observed weakest performance especially terms nmse. algorithms sorte estimate model order eigenvalues signal covariance. hard distinguish signal eigenvalues noise eigenvalues leading observed deterioration performance. medium high algorithm tends slightly overestimate model order hypothesise systematic overestimation model order avoided adjusting regularization parameter used ast. would however mean would perform worse scenarios. exactly weakness methods involving regularization parameters. finally note lasso never able successfully estimate model order grid. particular true frequency component estimated nonzero entries neighbouring gridpoints. visible lasso indeed estimates frequencies vicinity true frequencies. applications e.g. channel estimation wireless communications reconstructed signal frequencies themself interest. case lasso preferable simplicity. grid approximation lasso performs little worse best gridless algorithms terms nmse. ability separate components beyond rayleigh limit known super resolution. results fig. illustrate super resolution ability algorithms. experiment generate pairs frequencies varying distance frequencies pair. pairs well separated model order frequencies cannot recovered every case separation less since close nmse close oracle three algorithms handle closely located components well sense good approximation frequencies obtained. esprit give rather large nmse separation small. despite fact esprit show signiﬁcantly worse compared superfast lse. observed algorithms signiﬁcantly underestimate model order cases resulting large contribution nmse. esprit shows worse super resolution ability superfast valse covariance matrix size formed thus reducing effective signal length. fig. reports performance incomplete data case. measurement matrix generated randomly selecting rows identity matrix. observation indices chosen include ﬁrst last indices remaining indices obtained uniform random sampling without replacement. subset algorithms applicable case. proposed algorithm implemented using techniques described sec. refer semifast lse. semifast valse largely show performance slightly higher nmse higher nmse caused outliers signiﬁcantly underestimates model order. lasso observed reasonable nmse unable correctly estimate frequencies inspired concept phase transitions compressed sensing perform experiment shows similar phenomenon lse. particular demonstrate algorithm region space system parameters almost perfectly recover frequencies region cannot fairly sharp transition two. results terms seen fig. ﬁrst note rather poor performance consistent observation fig. signiﬁcantly turning attention valse esprit algorithms generally deal well large number components sense signiﬁcantly seen fig. superfast largest region high probability successful recovery fig. simulation results varying complete data signal length number components results averaged monte carlo trials. legend applies plots. nmse oracle shown. fig. simulation results closely located components complete data frequencies generated pairs pair varying intra-pair separation location pairs generated randomly non-paired frequencies least apart signal length results averaged monte carlo trials. legend applies plots. nmse oracle shown. fig. simulation results incomplete data i.e. contains rows selected random. signal length number components results averaged monte carlo trials. legend applies plots. nmse oracle shown. results obtained using matlab macbook pro. avoid differences results originating amount parallelism achieved implementation matlab restricted single computational thread. part code algorithm spends signiﬁcant time implemented native code matlab’s codegen feature. varying ﬁrst observe small moderate problem sizes difference lasso proposed algorithms small difference mainly implementation details. large-n region superfast semifast approximately order magnitude faster lasso. observe asymptotic per-iteration complexity superfast semifast describes scaling total runtime well number iterations stays practically constant. state-of-the-art methods valse time complexity worse results large runtimes even problem size moderate large problem sizes time complexity esprit evident superfast/semifast lasso signiﬁcantly outperform esprit. fig. show results illustrating computation time scales analysis assume first note runtime lasso practically constant complete data case per-iteration complexity superfast scales linearly practice slower scaling means values large enough reach asymptotic region. simulations large cannot need approximately hold incomplete data case runtime semifast increases quickly lasso faster semifast lse. however asymptotic complexity reached experiment runtime dominated calculation complexity presented low-complexity algorithm line spectral estimation. computational methods complete incomplete data cases presented along extension case multiple measurement vectors. fig. simulation results showing phase transitions complete data plots show block success rate. frequencies generated closely located pairs following methodology described caption fig. number pairs selected speciﬁed ratio achieved closely possible. signal length results averaged monte carlo trials. fig. runtimes seconds versus problem size show complete incomplete data case. number components values averaged monte carlo trials. incomplete data case also plot asymptotic per-iteration complexity superfast semifast lse. fig. runtimes seconds versus number components show complete incomplete data case. problem size values averaged monte carlo trials. show results algorithms runtime lower incomplete data case also plot asymptotic per-iteration complexity superfast semifast lse. proposed algorithm falls category bayesian methods line spectral estimation. bayesian methods widely accepted high estimation accuracy drawback class methods historically large computational complexity. respect work makes important contribution making bayesian methods viable practice. core computational method complete data case lies application gohberg-semencul formula toeplitz signal covariance matrix. many methods line spectral estimation toeplitz covariance matrices core conjecture computational comnumerical experiments show superfast algorithm high estimation accuracy. example fig. superfast attains high frequency recovery rates much larger scenarios reference algorithms. time algorithm computation time makes highly-accurate feasible problems size much larger methods currently available literature practically deal with. discuss convergence proposed blockcoordinate descent algorithm. introduce iteration index estimated variables. algorithm produces sequences blocks estimates denoted ˆzi} ˆβi} denote value objective function iteration ﬁrst note updates estimates guaranteed increase objective {li} non-increasing sequence. since also bounded thus converges. therefore proposed algorithm terminates ﬁnite time. unfortunately convergence {li} imply convergence sequences estimates. even exact minimization block variables block-coordinate descent non-convex functions stuck inﬁnite cycle complicated fact algorithm approximately solves minimization blocks. proposition shows exact minimization block block-coordinate descent converges objective function strictly quasiconvex blocks. objective function strictly quasiconvex thus hope prove convergence scheme which lieu computing exact minimizer merely descent property block. approach show convergence shares overall idea many details differ. discuss convergence properties ﬁrst derive number lemmas. theorem proved appendix. notational simplicity take convention block-coordinate descent algorithm cycles block updates following order ﬁnally example found based strictly speaking deﬁned algorithm affect correctness analysis. convergent subsequence i.e. least limit point. proof variables deﬁned closed bounded set. since limβ→∞ restrict closed bounded determined initial value objective function. similar argument holds lemma follows bolzanoweierstrass theorem. proof activation component gives decrease least since {li} lower bounded ﬁnitely many activations. since cannot deactivations activations also number deactivations ﬁnite. thus ﬁnite number changes ˆzi} converges. denote limit point lemma sequence converges. further limit point unique global minimizer fact deﬁned global minimizer global minimizer depend ˆβi− ˆθi− ˆγi−. lemma sequence ˆβi} converges limit point further every limit point remaining variables limit point local minimum boundary stationary point proof theorem convergence unique limit follows immediately assumption lemmas prove second statement ﬁrst note constant respect entries follows assumption kmax limit point. similarly limit point. lemma limit point result follows immediately. s.-y. kung arun state-space singular-value decomposition-based approximation methods harmonic retrieval problem optical soc. america vol. dec. dublanchet idier duwaut direction-of-arrival frequency estimation using poisson-gaussian modeling proc. ieee int. conf. acoust. speech signal process. vol. apr. thomas lundgaard hansen received b.sc. m.sc. electrical engineering aalborg university denmark respectively. since pursuing ph.d. degree wireless communication aalborg university. visiting scholar university california diego usa. recipient best student paper award ieee sensor array multichannel signal processing workshop also received award efondet master’s thesis. research interests include signal processing machine learning optimization wireless communication. bernard henri fleury received diplomas electrical engineering mathematics respectively ph.d. degree electrical engineering swiss federal institute technology zurich switzerland. since department electronic systems aalborg university denmark professor communication theory. till head section ﬁrst digital signal processing section later navigation communications section. partly afﬁliated researcher telecommunications research center vienna austria. prof. fleury’s research interests cover numerous aspects within communication theory signal processing machine learning mainly wireless communication systems networks. bhaskar currently distinguished professor electrical computer engineering department holder ericsson endowed chair wireless access networks university california diego. prof. elected fellow ieee recipient ieee signal processing society technical achievement award. prof. rao’s interests areas digital signal processing estimation theory optimization theory applications digital communications speech signal hansen m.-a. badiu fleury sparse bayesian learning algorithm dictionary parameter estimation proc. ieee sensor array multichannel signal process. workshop jun. champagnat goussard idier unsupervised deconvolution sparse spike trains using stochastic approximation ieee trans. signal process. vol. dec. shutin buchgraber kulkarni poor fast variational sparse bayesian learning automatic relevance determination superimposed signals ieee trans. signal process. vol. dec.", "year": "2017"}