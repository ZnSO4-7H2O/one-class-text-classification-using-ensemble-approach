{"title": "Training Recurrent Neural Networks as a Constraint Satisfaction Problem", "tag": "eess", "abstract": " This paper presents a new approach for training artificial neural networks using techniques for solving the constraint satisfaction problem (CSP). The quotient gradient system (QGS) is a trajectory-based method for solving the CSP. This study converts the training set of a neural network into a CSP and uses the QGS to find its solutions. The QGS finds the global minimum of the optimization problem by tracking trajectories of a nonlinear dynamical system and does not stop at a local minimum of the optimization problem. Lyapunov theory is used to prove the asymptotic stability of the solutions with and without the presence of measurement errors. Numerical examples illustrate the effectiveness of the proposed methodology and compare it to a genetic algorithm and error backpropagation. ", "text": "cope deficiencies researches proposed training methods supervised learning global optimization approaches learning approaches learn internal structure neural network learning internal weights neural network. learning internal structure neural network makes approaches efficient less reliant parameters selected user learning methods tiling algorithm cascade-correlation algorithm stepnet scaled conjugate algorithm among others incremental supervised learning approaches network size grows training phase result over-fitting supervised learning approaches prune over-fitted network training however methods successfully applied large scale practical problems contrast conjugate gradient methods attractive large scale problems fast convergence rate quasi-newton methods sophisticated alternative conjugate gradient methods supervised reliance exact approximation hessian matrix makes inefficient applications global optimization methods another alternative cope deficiencies newton-based methods learn internal structure neural networks. genetic algorithms simulated annealing widely used train neural networks optimize network structure approaches assume quality network related network topology parameters. alopex another global optimization approach trains network using correlation changes weights changes error function. local computations alopex suitable parallel computation taboo search another stochastic approach frequently used train neural networks. find optimal near optimal solution optimization problem implementation taboo search easier global optimization methods method generally applicable wide variety optimization problems researchers used combination global optimization methods training neural networks. ga-sa combination genetic algorithm simulated annealing. ga-sa uses genetic algorithm make simulated annealing faster reduce training time novel another hybrid approach uses trajectory-based method find feasible abstract‚Äîthis paper presents approach training artificial neural networks using techniques solving constraint satisfaction problem quotient gradient system trajectory based method solving csp. study converts training neural network uses find solutions. finds global minimum optimization problem tracking trajectories nonlinear dynamical system stop local minimum optimization problem. lyapunov theory used prove asymptotic stability solutions without presence measurement errors. numerical examples illustrate effectiveness proposed methodology compare genetic algorithm error backpropagation. minsky papert showed two-layer perceptron cannot approximate functions generally took nearly decade researchers show multilayer feedforward neural networks universal approximators since then neural networks successfully used various science engineering applications however training learning internal structure neural networks remained challenging problem researchers. training neural networks requires solving nonlinear nonconvex optimization problem researchers proposed different approaches solving classical optimization methods first methods used training neural networks. widely used training algorithm error backpropagation minimizes error function using steepest decent algorithm although error backpropagation easy implement disadvantages newtonbased optimization algorithms including slow convergence rate trapping local minima. local minima decrease generalization ability neural network slack variable introduced transform inequality constraints equality constraints. global minimum optimal solution original csp. nonlinear dynamical system equations defined based constraint chiang showed stable equilibrium points local minimums unconstrained minimization problem possible feasible solutions original csp. solution starting initial point initial time called trajectory orbit. equilibrium manifold path connected component assuming orbit equilibrium manifold stable exist asymptotically stable. equilibrium manifold equilibrium manifold stable unstable. equilibrium manifold pseudo-hyperbolic jacobian eigenvalues zero real part normal space exist locally homeomorphic projection dimension equilibrium manifold. stability region stable equilibrium manifold open connected invariant defined although global optimization methods applied training neural networks promising global optimization approaches used neural network training. quotient gradient system trajectory based method find feasible solutions constraint satisfaction problems. searches feasible solutions along trajectories nonlinear dynamical system paper exploits train artificial neural networks transforming training data transforms resulting unconstrained minimization problem. constructing unconstrained minimization problem nonlinear dynamical system defined. using fact equilibrium points local minima unconstrained minimization problem neural network trained integrating time reaches equilibrium point. method easy implement constructing nonlinear dynamical system similar deriving equations steepest descent algorithm. algorithm finds multiple local minima optimization forward backward integration qgs. provides easy straightforward approach find multiple local minima optimization problem. however like global optimization methods finding local minima takes time newton-based methods. numerical examples show outperforms error backpropagation genetic algorithm resulting network better generalization capability. preliminary version paper compares method error backpropagation presented solving optimization problems different initial points approaches cope local minima newtonbased methods. however selected initial points stability region stable equilibrium point makes inefficient. uses backward integration escape stability region stable equilibrium point enters stability region another equilibrium point forward integration. allows explore bigger region search local minima. simple implementation along global optimization property justify training method artificial neural networks. remainder paper organized follows section presents methodology. section describes structure neural network. application training neural network presented section section establishes stability proposed method examines effect input errors stability. numerical examples provided section section presents conclusion. active field research artificial intelligence operations research. chiang used trajectories nonlinear dynamical system find solutions csp. section reviews work forms basis approach neural network training presented section function approximation required many fields science engineering. neural networks general function approximators successfully applied different function approximation applications based nature application researchers developed different versions neural networks feedforward networks recurrent neural networks liquid state networks wavelet networks among others study consider three-layer fully recurrent neural network smooth activation functions. fig. illustrates internal structure neural network. network input ùë¢ùëõ]t ùë¶ÃÇùë°]t. network described network weights matrices whose size dependent number network inputs outputs hidden layer nodes. network inputs outputs hidden layer nodes ùëÖùëö√óùëõ ùëÖùëö√óùëö ùëÖùë°√óùëö cost function training neural network traditional squared errors transversality condition assumption defined follows. manifolds codimensions intersect transversally every exist open neighborhood system functions {ùê∑‚Ñéùëñ linearly independent following theorem assures stability redefines stability boundary assumptions a-a. next theorem shows solving equivalent finding stable equilibrium manifolds qgs. theorem consider associated quotient gradient system. assumptions hold following stable equilibrium manifold feasible region csp. cases must escape equilibrium manifold enter stability region another stable equilibrium manifold. equilibrium manifold feasible region process repeated enters stability region feasible equilibrium manifold satisfies stopping criterion. feasible manifold reached integrated time equilibrium point reached. escape basin attraction stable equilibrium point integrated backward time unstable point reached. thus solving optimization problem becomes series forward backward integrations stopping criteria satisfied. find local minima possible feasible solutions csp. train neural networks using consider training equality constraints transform unconstrained minimization problem second part chiang‚Äôs work equilibrium points local minimums unconstrained minimization problem. measurements available written locally positive definite function state equal zero global optima optimization problem. thus locally positive definite function vicinity equilibrium point. derivative lyapunov function along system trajectories derivative lyapunov function negative definite vicinity equilibrium point i.e. vicinity local minimum optimization problem. jacobian positive definite vicinity equilibrium points minima cost function. therefore equilibrium points locally asymptotically stable. proof consider lyapunov function repeated measurement full rank therefore ùê∑ùíâùê∑ùíâtis positive definite matrix. assume ùúémin smallest singular value positive definite matrix ùê∑ùíâùê∑ùíât. derivative lyapunov function written ùúémin smallest eigenvalue therefore ‚àíùúéminùëâ exponentially stable. bounded input output assumption yields ‚Äñùê∑ùíâ‚Äñ bounded. therefore spectral radius consequently smallest singular value finite. measurement errors noise make measurements inaccurate destabilize system. fortunately tolerate relatively large measurement error. neural networks measurement errors lead errors neural network inputs. consider function measurement errors change assuming small train neural network using fact equilibrium points unconstrained minimization problem. therefore algorithm needs find equilibrium point escape equilibrium point move toward another equilibrium point qgs. first step integrate starting point need feasible find equilibrium point. next escape stability region stable equilibrium point unstable point backward integration time. eigenvalues jacobian matrix used measure stability instability. algorithm continues cannot find equilibrium point satisfies stopping criterion. neural network training equilibrium points considered zero-dimensional equilibrium manifolds assumptions hold. activation function neural network one-one invertible function proper map. assumption also holds asymptotically stable proper. assuming activation functions neural network continuously differentiable continuously differentiable hence lipschitz domain contains equilibrium point. assume perturbation term satisfies linear growth bound theorem assume input output network bounded corresponding neural network inputs hidden layer nodes measurements. equilibrium perturbed asymptotically stable illustrate effectiveness training neural networks trained network nonlinear system identification compare results genetic algorithm results genetic algorithm optimization uses matlab optimization toolbox population size roulette selection adaptive feasible mutation scattered crossover fitness scaling best results. system input system output. zero-mean normally distributed standard deviation input neural network target output training. initial network parameter values zero-mean normally distributed standard deviation optimal number hidden layer nodes found total number training sets activation function neural network tangent hyperbolic function initializing random initial values finds local minima optimization problem. local minimum best generalization capability global minimum close optimal solution optimization problem. table summarizes mean squared error test data network genetic algorithm network error backpropagation network. network less genetic algorithm network networks outperform backpropagation trained network. simulation results included brevity including generalization errors demonstrate back propagation gives much worse results networks. hence include back propagation remainder example. shows outputs test data. training results networks fig. shows trained network better generalization performance random input test data smaller generalization error. system input system output. zero-mean normally distributed standard deviation input system target output training. initial network parameter values zero-mean normally distributed standard deviation optimal number hidden layer nodes found total number training sets initializing random initial values finds local minima optimization problem. local minimum best generalization capability global minimum close optimal solution optimization problem. table summarizes test data trained network genetic algorithm trained network error backpropagation trained network. smaller genetic algorithm trained network. networks outperform error backpropagation trained network. example simulation results much worse back propagation networks omit back propagation results remainder example. shows outputs system trained network genetic algorithm trained network. fig. shows outputs test data. fig. shows trained network better performance train data fig. fig. shows generalization error trained network genetic algorithm trained network. networks similar performance training data input fig. illustrates trained network better generalization capability terms maximum generalization error percentage mean squared error test data. average absolute generalization error trained network average absolute generalization error genetic algorithm trained network shows trained network outperforms genetic algorithm trained network random input test data. fig. shows generalization error trained network genetic algorithm trained network. networks similar performance training data input fig. fig. show trained network better generalization capability. average absolute generalization error trained network average absolute generalization error genetic algorithm trained network study introduce training algorithm neural network using qgs. uses trajectories nonlinear dynamical system find local minima optimization problem. local minimum best generalization capability global minimum optimization problem. simulation results shows trained network performs better networks trained using genetic algorithm error backpropagation. particular networks better generalization properties faster training time comparison genetic algorithm robust errors inputs. contrast newton based methods need multiple initial values find multiple local minima need huge number measurements training. therefore particularly suited applications limited number available input-output measurements. future work exploit projected gradient system together develop training algorithm neural networks searching local minima optimization problem. d.e. rumelhart g.e. hinton r.j. williams learning internal representations error propagation rumelhart mcclelland editors parallel distributed processing explorations microstructure cognition chapter press cambridge c.l.p. chen jiyang instant learning supervised learning neural networks rank-expansion algorithm ieee international conference neural networks ribert stocker lecourtier ennaji survey supervised learning evolving multi-layer perceptrons ieee international conference computational intelligence multimedia applications knerr personnaz dreyfus single-layer learning revisited stepwise procedure building training neural network neurocomputing nato series series springer pp.. khodabandehlou sami fadali echo state versus wavelet neural networks comparison application nonlinear system identification ifac-papers online vol. issue https//doi.org/./j.ifacol... h.th. jongen jonker twilt nonlinear optimization finite dimensions morse theory chebyshev approximation transversality flows parametric aspects kluwer academic h.d. chiang dynamical trajectory-based methodology systematically computing multiple optimal solutions general nonlinear programming problems ieee trans. automatic control vol.", "year": "2018"}