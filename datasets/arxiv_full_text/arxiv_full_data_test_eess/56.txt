{"title": "Some observations on computer lip-reading: moving from the dream to the  reality", "tag": "eess", "abstract": " In the quest for greater computer lip-reading performance there are a number of tacit assumptions which are either present in the datasets (high resolution for example) or in the methods (recognition of spoken visual units called visemes for example). Here we review these and other assumptions and show the surprising result that computer lip-reading is not heavily constrained by video resolution, pose, lighting and other practical factors. However, the working assumption that visemes, which are the visual equivalent of phonemes, are the best unit for recognition does need further examination. We conclude that visemes, which were defined over a century ago, are unlikely to be optimal for a modern computer lip-reading system. ", "text": "quest greater computer lip-reading performance number tacit assumptions either present datasets methods review assumptions show surprising result computer lip-reading heavily constrained video resolution pose lighting practical factors. however working assumption visemes visual equivalent phonemes best unit recognition need examination. conclude visemes deﬁned century unlikely optimal modern computer lip-reading system. consistent sustained interest building computer systems understand humans saying without hearing audio channel.– obvious applications systems security also noisy environments cockpits battleﬁelds crowds audio recognition likely impossible highly degraded. early work consisted small vocabularies single speakers high-deﬁnition video often talker would wear special lipstick allow easy segmentation analysis lips. subsequently understanding problem improved lip-reading outdoor conditions -voclabularies looks feasible. problem speaker dependence still partially solved. surprising recent result characterisation eﬀect resolution lip-reading. informal understanding relatively high resolution required practice reported that provided tracking perfect fewer pixels give acceptable results. observation oﬀ-axis lip-reading gave slightly better performance full frontal seems comes lip-reading one’s intuition might often wrong indeed experimenters ﬁeld often confounded counter-intuitive illusions ﬁeld mcgurk eﬀect. experimental recognition systems audio almost always built using phonemes. appears good agreement phonemes appear major languages expected frequency might phonetic units recognised sequence language model generates hypotheses words sentences. modern speech recognition language models powerful important subject decades work. clearly huge advantage lip-reading system re-using language model many lip-reading systems recognise using visual units visemes feed sequence acoustic language model modiﬁed cope visemes. visemes exist form postulated linguists e.g. many choices visemes. however surprisingly examinations visemes give best performance fragile performance compared phonetic recognition. phoneme generally regarded smallest sound uttered. viseme often said visual equivalent phoneme precisely deﬁned– working deﬁnition viseme phonemes identical appearance lips’. therefore phoneme falls viseme class viseme represent many phonemes many mapping. similar simpliﬁed version audio recognition whereby seek identify string unique phonemes recognizer based upon training data correctly labelled phoneme. visual-only recognition concept building recognizers based upon visual-only training samples correctly labelled according viseme mapping. still debate correct phoneme-to-viseme mapping many suggested interest contribution viseme recognition performance. look particular visemes contribute recognition accuracy. measure reduction unique visemic recogniser contribution value whole task accurate recognition continuous speech. demonstrate inﬂuence reduced recogniser classes visual speech recognition compare outputs audio recognition data. fair comparison groupings phonemes faux ‘audio-viseme’ recognisers audio data. audio recognition higher quantity classiﬁers proposed viseme classes therefore hypothesise visual classes bigger variance use/purpose towards whole recognition task. anticipate fewer visemes used visual speech recognition ‘audio-visemes’ audio recognition. ﬁrst steps figure full face active appearance models track faces videos lip-only aams using methods produce sets talker-dependent features; shape-only visual features appearance-only visual features. shape features based solely upon shape positioning duration talker speaking e.g. landmarks figure landmark positions compactly represented using linear model form mean shape modes. appearance features computed pixels original images warped mean shape. mean appearance appearance described modal appearances rosetta raven data four videos recitations edgar allen poe’s poem ‘the raven’. talkers male female. neither trained actors recite poem intended others randomly. training frames hand-labelled shape model face lips build preliminary model talker. models ﬁtted inverse compositional ﬁtting remaining frames thus tracked ﬁtted full-face talker-dependent aams full resolution lossless frame images next create sub-model lips talker decomposing full face models ﬁtted landmarks shape appearance parameters frame extracted. talker retain shape appearance parameters talker shape appearance parameters. restrict feature parameters retain variation mean model produced using whole tracked video data. implement extracted features address co-articulation used phoneticalignment production ground-truth benchmark forced-alignment within training process recognizers. benchmark measuring recognition outputs produce ground-truth viseme transcription using carnegie mellon university north american pronunciation dictionary word transcription. convert phonetic transcript viseme transcript assuming visemes listed table combination montgomery al’s vowel mapping walden’s consonant mapping. limited availability large datasets documented work within restrictions short datasets. note provide adequate training examples visemes. happens group untrainable visemes single garbage viseme. case select sample threshold visemes grouped. figure shows occurrence visemes listed table data table shows revised viseme mapping. talker test fold randomly selected lines poem replacement. remaining lines used training folds. repeating times gives ﬁve-fold cross-validation. note visemes cannot equally represented folds. recognition hidden markov models implemented hidden markov toolkit initialised using ‘ﬂat start’ method using prototype states mixture components information training samples. choose states mixture components based upon. deﬁne viseme plus silence short-pause labels re-estimate parameters four times pruning. next tool hhed together short-pause silence models states three re-estimating hmms times. hvite used force-align data using word transcript∗. hmms re-estimated twice more however force-aligned viseme transcript rather original viseme transcript used previous re-estimations. complete recognition using hmms require word network continuous speech dataset. hlstats hbuild make bi-gram word-level network finally hvite used network support recognition task hresults gives correctness accuracy viseme recognition values viseme confusion matrix folds. provided reader technical details enable repeatability experiments. please contact author original videos. plotted probability correct recognition using appearance-only features viseme. usual talkers better recognised shape appearance†. note right-hand point visual silence phoneme. general visual silence quite variable compared audio silence talkers breathe show emotion. however here source text poem well-deﬁned visual silence periods start line. observed human reading visual cues reliable humans combined rich contextual information interpret ‘ﬁll gaps’ talker saying. therefore hypothesis robust audio recognition based upon large spread recognised phones resilience recognition number phones contributing accuracy. visually human lip-readers anticipated fewer visemes would perform equivalent recognition such graph would demonstrate steeper performance decline performing visemes. figure greater decline left right visemes visual features audio talkers. also note error bars position viseme increase consistent hypothesis audio recognition spread visemes correct. visemes /v/. vowels front-of-mouth consonant visemes figure demonstrates shallower decline left right shape graph figure still greater decline visual features audio. error bars increase position viseme‡. shape graph figure similar audio video implies appearance-based recognition similar noisy acoustic recognition talkers hence less fragile. visemes figure i.e. identical shape-only ﬁrst positions. error bars increase consider small data available makes recognition unreliable less well trained classiﬁers. reduced impact ‡note order audio viseme ordering identical figures experiment. table visemes ordered correctness showing example viseme best performing viseme overall. natural diﬀerences ranking signiﬁcant. compare viseme ordering compute spearman rank correlation coeﬃcient results shown tables also shown p-value null signiﬁcant threshold underlined. talker poor audio performance tends degrade audio correlation. lip-reading depend audio though results conﬁrm strong relation shape-only viseme-only classiﬁcation. also note audio ranking similar video ranking although previously noticed rapid drop-oﬀ video. table provided overall mean standard error accuracy score whole viseme recognition performance folds. talker outperforms talker features visual features also larger degree error. appearance features outperform shape talkers audio outperforms appearance talkers. seen figures recognition based upon larger spread visemes shape models audio largest spread visemes hence important results illuminate often made observation lip-reading fragile. words cannot build classiﬁers critical visemes lip-reading impossible. human lip-reading context humans often trained recognise small number critical gestures processed sophisticated language context model create transcript. important acknowledge work ﬁeld focuses improving mean accuracies visemes conceal real source overall performance. system achieves mean viseme accuracy maybe contains supremely accurate viseme classiﬁer maybe system classiﬁers much modest performance. paper therefore raises diﬀerent tactics improving lip-reading systems. either makes best viseme classiﬁers better focuses upon improving worst. stage know tactic likely successful hope methodology allows future work focus attention likely good. davis mermelstein comparison parametric representations monosyllabic word recognition continuously spoken sentences acoustics speech signal processing ieee transactions mcgurk macdonald hearing lips seeing voices nature jeﬀers barley thomas springﬁeld bozkurt erdem erzin erdem ozkan comparison phoneme viseme based binnie jackson montgomery visual intelligibility consonants lipreading screening test implications aural rehabilitation journal speech hearing disorders carnegie mellon university pronounciation dictionary massaro press walden prosek montgomery scherr jones eﬀects training visual recognition consonants journal speech language hearing research young evermann gales hain kershaw moore odell ollason povey valtchec woodland cambridge university engineering department", "year": "2017"}