{"title": "Sequence-based Multi-lingual Low Resource Speech Recognition", "tag": "eess", "abstract": " Techniques for multi-lingual and cross-lingual speech recognition can help in low resource scenarios, to bootstrap systems and enable analysis of new languages and domains. End-to-end approaches, in particular sequence-based techniques, are attractive because of their simplicity and elegance. While it is possible to integrate traditional multi-lingual bottleneck feature extractors as front-ends, we show that end-to-end multi-lingual training of sequence models is effective on context independent models trained using Connectionist Temporal Classification (CTC) loss. We show that our model improves performance on Babel languages by over 6% absolute in terms of word/phoneme error rate when compared to mono-lingual systems built in the same setting for these languages. We also show that the trained model can be adapted cross-lingually to an unseen language using just 25% of the target data. We show that training on multiple languages is important for very low resource cross-lingual target scenarios, but not for multi-lingual testing scenarios. Here, it appears beneficial to include large well prepared datasets. ", "text": "techniques multi-lingual cross-lingual speech recognition help resource scenarios bootstrap systems enable analysis languages domains. end-to-end approaches particular sequence-based techniques attractive simplicity elegance. possible integrate traditional multi-lingual bottleneck feature extractors front-ends show end-to-end multi-lingual training sequence models effective context independent models trained using connectionist temporal classiﬁcation loss. show model improves performance babel languages absolute terms word/phoneme error rate compared mono-lingual systems built setting languages. also show trained model adapted cross-lingually unseen language using target data. show training multiple languages important resource cross-lingual target scenarios multi-lingual testing scenarios. here appears beneﬁcial include large well prepared datasets. typically fewer hyper-parameters tune. show sequence training multi-lingual settings create feature extractors directly ported languages using linear transformation re-trained data opening door end-to-end language universal speech recognition. early works multi-lingual cross-lingual speech recognition involved language independent features like articulatory features train based systems. authors used subspace gaussian mixture model phonemes different languages together. authors introduce shared phone build based language independent acoustic models show adaptation pre-existing models towards language. on-set deep learning focus models shifted learning features across languages mapped space authors looked unsupervised pretraining different languages cross lingual recognition. dominant architecture multi-lingual cross-lingual speech recognition state-of-the-art speech recognition systems human-like performance trained hundreds hours well-annotated speech. since annotation expensive time-consuming task similar performance typically unattainable resource languages. multi-lingual cross-lingual techniques allow transfer models features well-trained scenarios large amounts training data available cannot transcribed otherwise hard come standard approach train context dependent hidden markov model based deep neural network acoustic model bottleneck layer using frame based criterion large multilingual corpus network bottleneck layer used language-independent feature extractor adapting language. generating model requires preparation frame level segmentation language usually achieved training separate mono-lingual systems ﬁrst. cumbersome multi-step process. moreover speaking style acoustic quality linguistic properties recordings different across languages segmentations inconsistent across languages thus sub-optimal generating features language. hand end-to-end training approaches directly model context independent phones elegant greatly facilitate speech recognition training. require explicit alignment transcriptions training data so-called shared hidden layer model data passed series shared feed-forward layers separated multiple language-speciﬁc softmax layers trained using cross-entropy architecture also used bottleneck feature extractor language independent features extracted target-language acoustic model built. authors showed multilingual models adapted speciﬁc language improve performance further. work presented bottleneck features multi-lingual systems showed feature porting possible gave competitive results compared systems mono-lingual features. approaches constructed shared language independent phone could also adapted target language. proposed model inspired former approach tries learn latent features sharing hidden layers across languages. connectionist temporal classiﬁcation lends low-resource multi-lingual experiments systems built tend signiﬁcantly easier train trained using hidden markov models shows multi-lingual systems shared phones improve performance limited data setting. knowledge prior work looked learning bottleneck like features based model seen performs multilingually cross-lingually adaptation. paper several languages iarpa’s babel project test model. mostly telephony conversational speech data resource language. accompanied lexicon dictionary extended speech assessment methods phonetic alphabet format. table summarizes amount training data hours along number phonemes present languages used experiments full language pack condition. model trained loss sequence based model automatically learns alignment input output introducing additional label called blank symbol corresponds output’ prediction. given sequence acoustic feathis work used releases iarpa-babelb-v. iarpa-babelbv.b iarpa-babelb-v.b iarpa-babelb-v.a iarpa-babelb-v.g iarpababelb-v.b iarpa-babelb-v.by iarpa-babelbv.d iarpa-babelb-v. testing. tures label sequence model tries maximize likelihood possible paths lead correct label sequence reduction. reduced path obtained grouping duplicates removing aabc). like loss along stacked bidirectional lstm layers encode acoustic information make frame-wise predictions. multi-lingual model share bidirectional lstm encoding layer till ﬁnal layer project learned embedding layer phones respective target languages. intuition behind model training language help better regularization weights learning better representation features trained data. hypothesize ﬁnal phoneme discrimination learned linear projection last layer. figure shows schematic diagram multi-lingual model. mathematically written unlike bottleneck layer whole model sequence trained based loss. note recognize sequence phonemes much harder problem. traditional hmm/dnn systems perform frame-wise recognition individual phonemes usually relying alignments generated mono-lingual models. considered much simpler task recognition phone sequence. align project goals chose perform experiments four languages closest/ maximum phone overlap kurmanji kazakh turkish mongolian haitian. used -layer bidirectional lstm network cells direction performed best average across majority babel languages systematic search experiment. table shows results. consistency used absolutely identical settings across languages perform language-speciﬁc tuning choosing lowest perplexity language model between -gram -gram models wfst-based decoding. techniques blank scaling applying softmax temperature often improve results signiﬁcantly apply consistency. multi-lingual experiments -layer bilstm network cells layer shared encoded representation. again setup performed best average larger languages. multi-lingual training mling improves average keeping lstm layers shared across languages. ﬁne-tune entire model towards language speciﬁcally performance improves further average baseline. roughly double amount training data adding switchboard training mling training data performance improves again universal language-speciﬁc case. overall improve absolute line results reported comparable tasks discussed section donor model single softmax train varying amounts data target language kurmanji case. figure shows different donor models behave situation. crosslingual case becomes beneﬁcial train lstm layers many different languages possible single related language outperforms adaptation larger amount data unrelated language large mono-lingual systems multi-lingual systems. improvements become smaller training performed data more even re-estimation softmax layer beneﬁts data. given adding seemingly unrelated high resource language improved performance model four resource languages studied impact varying source extra data. speciﬁcally replaced switchboard corpus four unrelated babel languages composed tamil amharic pashto tagalog. results test data summarized table adding switchboard data outperforms adding unrelated babel languages. main goal creation multi-lingual recognizer veriﬁed models trained single babel language plus switchboard outperform ﬁne-tuned mling+swbd system clear pattern languages. indicates generally beneﬁcial train multi-lingual systems closely related languages and/or large amounts well-prepared unrelated mono-lingual data adding large number languages fact prevent model training well. thus seems multi-lingual systems indeed learn portable language independent representation useful porting language sheer amount data less beneﬁcial. order study extent sequence models learned useful bottleneck like discriminatory audio features independent input language attempt port model unseen language. trained model languageindependent feature extractor linearly separate language phoneme sequence. this replace softmax layer figure shows related unrelated languages multilingual system surpasses mono-lingual baseline original data seen. behavior retraining seems independent original trained languages. compare softmax adaptation full network adaptation kurmanji swahili languages training. donor models. figure shows small amounts adaptation data target language related pre-trained languages softmax adaptation competitive initialization many languages beneﬁcial. multi-lingual settings seems beneﬁcial train related languages only large amounts clean data; beneﬁt simply training many languages. thus possible combine e.g. switchboard babel data. resource cross-lingual scenarios possible adapt model previously unseen language re-training softmax layer only. models learn language independent representation input softmax layer. training models trained related languages help training many languages rather large amounts data. data available whole network retrained effect choice language multi-lingual training disappears. future work investigating decoding output using phoneme based neural language models trained nonparallel text thereby facilitating zero-resource speech recognition. project sponsored defense advanced research projects agency information innovation ofﬁce program resource languages emergent incidents issued darpa/io contract hr-c-. gratefully acknowledge support nvidia corporation donation titan pascal used research. work used extreme science engineering discovery environment supported national science foundation grant number oci-. speciﬁcally used bridges system supported award number aci- pittsburgh supercomputing center heigold vanhoucke senior nguyen ranzato devin dean multilingual acoustic models using distributed deep neural networks acoustics speech signal processing ieee international conference ieee gr´ezl karaﬁ´at vesely adaptation multilingual stacked bottle-neck neural network structure language acoustics speech signal processing ieee international conference ieee gr´ezl karaﬁat janda study probabilistic bottle-neck features multilingual environment automatic speech recognition understanding ieee workshop ieee tong garner bourlard investigation deep neural networks multilingual speech recognition training adaptation proc. interspeech number epfl-conf-. imseng povey motlicek schultz bourlard multilingual deep neural network based acousacoustics modeling rapid language adaptation speech signal processing ieee international conference ieee graves fern´andez gomez schmidhuber connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks proceedings international conference machine learning. miao gowayyed metze eesen end-to-end speech recognition using deep models wfst-based decoding automatic speech recognition understanding ieee workshop ieee saon kurata sercu audhkhasi thomas dimitriadis ramabhadran picheny l.english conversational telephone speech arxiv preprint recognition humans machines arxiv. stolcke grezl m.-y. hwang morgan vergyri cross-domain cross-language portability acoustic features estimated multilayer perceptrons acoustics speech signal processing icassp proceedings. ieee international conference ieee vol. i–i. knill gales rath woodland zhang s.-x. zhang investigation multilingual deep neural networks spoken term detection automatic speech recognition understanding ieee workshop ieee metze schultz multilingual bottle-neck features application under-resourced languages proc. workshop spoken language technologies under-resourced languages cape town; africa mica. stuker metze schultz waibel integrating multilingual articulatory features speech recognition eighth european conference speech communication technology burget schwarz agarwal akyazi feng ghoshal glembek goel karaﬁ´at povey multilingual acoustic modeling speech recognition based subspace gaussian mixture models acoustics speech signal processing ieee international conference ieee", "year": "2018"}