{"title": "Visual Speech Enhancement", "tag": "eess", "abstract": " When video is shot in noisy environment, the voice of a speaker seen in the video can be enhanced using the visible mouth movements, reducing background noise. While most existing methods use audio-only inputs, improved performance is obtained with our visual speech enhancement, based on an audio-visual neural network. We include in the training data videos to which we added the voice of the target speaker as background noise. Since the audio input is not sufficient to separate the voice of a speaker from his own voice, the trained model better exploits the visual input and generalizes well to different noise types. The proposed model outperforms prior audio visual methods on two public lipreading datasets. It is also the first to be demonstrated on a dataset not designed for lipreading, such as the weekly addresses of Barack Obama. ", "text": "formance compared prior common audio-visual datasets grid corpus tcd-timit designed audio-visual speech recognition reading. also demonstrate speech enhancement public weekly addresses barack obama. traditional speech enhancement methods include spectral restoration wiener ﬁltering statistical modelbased methods recently deep neural networks adopted speech enhancement generally outperforming traditional methods previous methods single-channel speech enhancement mostly audio input. train deep autoencoder denoising speech signal. model predicts mel-scale spectrogram representing clean speech. pascual generative adversarial networks operate waveform level. separating mixtures several people speaking simultaneously also become possible training deep neural network differentiate unique speech characteristics different sources e.g. spectral bands pitches chirps shown despite decent overall performance audio-only approaches achieve lower performance separating similar human voices commonly observed same-gender mixtures different approaches exist generation intelligible speech silent video frames speaker ephrat generate speech sequence silent video frames speaking person. model uses video frames corresponding optical output spectrogram representing speech. owens recurrent neural network predict sound silent videos people hitting scratching objects drumstick. recent research audio-visual speech processing makes extensive neural networks. work ngiam seminal work area. demonstrate cross modality feature learning show better features modality learned audio video present feature learning time. multi-modal neural networks audiovisual inputs also used reading sync robust speech recognition video shot noisy environment voice speaker seen video enhanced using visible mouth movements reducing background noise. existing methods audio-only inputs improved performance obtained visual speech enhancement based audio-visual neural network. include training data videos added voice target speaker background noise. since audio input sufﬁcient separate voice speaker voice trained model better exploits visual input generalizes well different noise types. proposed model outperforms prior audio visual methods public lipreading datasets. also ﬁrst demonstrated dataset designed lipreading weekly addresses barack obama. index terms speech enhancement visual speech processing speech enhancement aims improve speech quality intelligibility audio recorded noisy environments. applications include telephone conversations video conferences reporting more. speech enhancement also used hearing aids speech recognition speaker identiﬁcation speech enhancement subject extensive research recently beneﬁted advancements machine reading speech reading propose audio-visual end-to-end neural network model separating voice visible speaker background noise. model trained speciﬁc speaker used enhance voice speaker. assume video showing face target speaker available along noisy soundtrack visible mouth movements isolate desired voice background noise. idea training deep neural network differentiate unique speech auditory characteristics different sources effective several cases performance limited variance sources shown show using visual information leads signiﬁcant improvement enhancement performance different scenarios. order cover cases target background speech totally separated using audio information alone training data videos synthetic background noise taken voice target speaker. videos training data trained model better exploits visual input generalizes well different noise types. figure illustration encoder-decoder model architecture. sequence video frames centered mouth region convolutional neural network creating video encoding. corresponding spectrogram noisy speech encoded similar fashion audio encoding. single shared embedding obtained concatenating video audio encodings consecutive fully-connected layers. finally spectrogram enhanced speech decoded using audio decoder. work also done audio-visual speech enhancement separation kahn milner hand-crafted visual features derive binary soft masks speaker separation. propose convolutional neural network model enhance noisy speech. network gets sequence frames cropped speaker’s lips region spectrogram representing noisy speech outputs spectrogram representing enhanced speech. gabbay feed video frames trained speech generation network spectrogram predicted speech construct masks separating clean voice noisy input. speech enhancement neural network model gets inputs sequence video frames showing mouth speaker; spectrogram noisy audio. output spectrogram enhanced speech. network layers stacked encoder-decoder fashion encoder module consists dual tower convolutional neural network takes video audio inputs encodes shared embedding representing audio-visual features. decoder module consists transposed convolutional layers decodes shared embedding spectrogram representing enhanced speech. entire model trained end-to-end. input video encoder sequence consecutive gray scale video frames size cropped centered mouth region. using frames worked well number frames might also work. video encoder consecutive convolution layers described table layer followed batch normalization leaky-relu non-linearity pooling dropout previously done several audio encoding networks design audio encoder convolutional neural network using spectrogram input. network consists convolution layers described table layer followed batch normalization leaky-relu nonlinearity. strided convolutions instead pooling order maintain temporal order. video encoder outputs feature vector values audio encoder outputs feature vector values. feature vectors concatenated shared embedding representing audio-visual features values. shared embedding block consecutive fullyconnected layers sizes respectively. resulting vector audio decoder. audio decoder consists transposed convolution layers mirroring layers audio encoder. last layer size input spectrogram representing enhanced speech. network trained minimize mean square error loss output spectrogram target speech spectrogram. adam optimizer initial learning rate back propagation. learning rate decreased learning stagnates i.e. validation error improve epochs. neural networks multi-modal inputs often dominated inputs different approaches considered overcome issue previous work. ngiam proposed occasionally zero input modalities training input modality idea adopted reading speech enhancement order enforce using video features adds auxiliary video output resemble input. enforce exploitation visual features introducing training strategy. include training data samples added noise voice speaker. since separating overlapping sentences spoken person impossible using audio information network forced exploit visual features addition audio features. show model trained using approach generalizes well different noise types capable separate target speech indistinguishable background speech. experiments video resampled fps. video divided non-overlapping segments frames each. every frame crop mouth-centered window size pixels using mouth landmarks facial landmarks suggested video segment used input therefore size normalize video inputs entire training data subtracting mean video frame dividing standard deviation. corresponding audio signal resampled khz. shorttime-fourier-transform applied waveform signal. spectrogram used input neural network phase kept aside reconstruction enhanced signal. stft window size samples equals milliseconds corresponds length single video frame. shift window length samples time creating overlap mel-scale spectrogram computed multiplying spectrogram mel-spaced ﬁlterbank. mel-scale spectrogram comprises frequencies slice spectrogram pieces length milliseconds corresponding length video frames resulting spectrograms size temporal samples frequency bins. sample frames grid tcdfigure timit mandarin speaker obama datasets. bounding boxes mark mouthcentered crop region. obama videos varied background illuminations resolutions lighting. perform experiments speakers grid audiovisual sentence corpus large dataset audio video recordings sentences spoken people also perform experiments tcdtimit dataset consists volunteer speakers around videos each well three lipspeakers people specially trained speak helps deaf understand visual speech. speakers recorded saying various sentences timit dataset prepared audio-visual dataset containing video recordings utterances mandarin sentences spoken native speaker. sentence contains chinese characters phoneme designed distribute equally. length utterance approximately seconds. utterances recorded quiet room sufﬁcient light speaker captured frontal view. assess model’s performance general conditions compared datasets speciﬁcally prepared lip-reading. purpose dataset containing weekly addresses given barack obama. dataset consists videos minutes long. dataset varies greatly scale background lighting face angle well audio recording conditions includes unbounded vocabulary. evaluate model several speech enhancement tasks using four datasets mentioned sec. cases background speech sampled librispeech dataset. ambient noise different types noise rain motorcycle engine basketball bouncing etc. speech noise signals mixed training testing except mandarin experiment protocol used. sample target speech mixed background speech ambient noise another speech target speaker. call latter case self mixtures. report evaluation using objective scores measuring noise reduction pesq assessing improvement speech quality since listening audio samples essential understand effectiveness speech enhancement methods supplementary material available project page show effectiveness using visual information training competitive audio-only version model similar architecture training baseline involve self mixtures since audio-only separation case ill-posed. seen audio-only baseline consistently achieves lower performance model especially self mixtures improvement speech quality obtained all. order validate assumption stating using samples target speaker background noise makes model robust different noise types well cases background speech indistinguishable target speech train model without self mixtures training set. shown model capable separating speech samples voice although access visual stream. detailed results presented table show model outperforms previous work vidspeech gabbay audio-visual datasets grid tcd-timit well achieves significant improvements pesq dataset obama. seen results somewhat less convincing tcd-timit dataset. possible explanation might smaller amount clean speech training data compared experiments mandarin experiment follow protocol train model proposed dataset containing speech samples mixed engine ambient noise interfering noise types different conﬁgurations. table shows model achieves slightly better performance proposed test noted pesq accurate chinese end-to-end neural network model separating voice visible speaker background noise presented. also effective training strategy audio-visual speech enhancement proposed using noise overlapping sentences spoken person. training builds model robust similar vocal characteristics target noise table evaluation model comparison baselines previous work. model achieves signiﬁcant improvement noise reduction speech quality different noise types. text discussion. speakers makes effective visual information. proposed model consistently improves quality intelligibility noisy speech outperforms previous methods public benchmark datasets. finally demonstrated ﬁrst time audio-visual speech enhancement general dataset designed lipreading research. model compact operates short speech segments thus suitable real-time applications. average enhancement segment requires processing note method fails input modality missing since training audio video used. ﬁeld combined audio-visual processing active. recent work showing much progress appeared camera ready deadline includes bronkhorst cocktail party phenomenon review research speech intelligibility multiple-talker conditions acta acustica united acustica vol. january kolbæk z.-h. jensen speech intelligibility potential general specialized deep neural network based speech enhancement systems ieee/acm trans. audio speech language processing vol. j.-c. s.-s. wang y.-h. j.-c. tsao h.-w. chang h.-m. wang audio-visual speech enhancement based multimodal deep convolutional neural network ieee transactions emerging topics computational intelligence vol. ephrat mosseri lang dekel wilson hassidim freeman rubinstein looking listen cocktail party speaker-independent audio-visual model speech separation arxiv.", "year": "2017"}