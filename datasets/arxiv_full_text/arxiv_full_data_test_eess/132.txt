{"title": "End-to-end learning for music audio tagging at scale", "tag": "eess", "abstract": " The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models - using waveforms as input with very small convolutional filters; and models that rely on domain knowledge - log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogram-based ones in large-scale data scenarios. ", "text": "lack data tends limit outcomes deep learning research particularly dealing end-to-end learning stacks processing data waveforms. study tracks annotated musical labels available train end-to-end models. large amount data allows unrestrictedly explore different design paradigms music auto-tagging assumption-free models using waveforms input small convolutional ﬁlters; models rely domain knowledge log-mel spectrograms convolutional neural network designed learn timbral temporal features. work focuses studying types deep architectures perform datasets variable size available training magnatagatune million song dataset private dataset songs. experiments suggest music domain assumptions relevant enough training data available thus showing waveform-based models outperform spectrogrambased ones large-scale data scenarios. fundamental goal music informatics research automatically structure large music collections. music audio tagging task consists automatically estimating musical attributes song including moods language lyrics year composition genres instruments harmony rhythmic traits. thus estimates useful deﬁne semantic space advantageous automatically organizing musical libraries. many approaches considered task recent publications showing promising results using deep architectures work conﬁrm trend studying deep architectures conceived considering opposite design strategies perform several datasets datasets unprecedented size songs. provided sizable amount data available study investigate learning capabili jordi pons oriol nieto matthew prockup erik schmidt andreas ehmann xavier serra. licensed creative commons attribution international license attribution jordi pons oriol nieto matthew prockup erik schmidt andreas ehmann xavier serra. end-to-end learning music audio tagging scale international society music information retrieval conference paris france ties architectures. speciﬁcally investigate whether architectures based domain knowledge overly constrain solution space cases large training data available essence study certain architectural choices limit model’s capabilities learn data. main contribution work show little model assumptions required music auto-tagging operating large amounts data. section discusses main deep architectures identiﬁed audio literature section describes datasets used work section presents architectures study section provides discussion results conclusions drawn section order facilitate discussion around current audio architectures divide deep learning models parts front-end back-end figure frontend part model interacts input signal order latent-space backend predicts output given representation obtained front-end. following present main frontback-ends identiﬁed literature. front-ends. generally comprised convolutional neural networks since learn efﬁcient representations sharing weights along signal. front-ends divided groups depending used input signal waveforms spectrograms further design ﬁlters either based domain knowledge not. example leverages domain knowledge front-end waveforms designed length ﬁlter window length stft spectrogram front-end used vertical ﬁlters learn timbral representations horizontal ﬁlters learn longer temporal cues generally single ﬁlter shape used ﬁrst layer recent works report performance gains using several ﬁlter shapes ﬁrst layer using many ﬁlters promotes richer feature extraction ﬁrst layer facilitates leveraging domain knowledge designing ﬁlters’ shape. example waveform front-end using many long ﬁlters motivated perspective multiresolution time-frequency transform since known patterns spectrograms occurring different time-frequency scales intuitively incorporate many vertical and/or horizontal ﬁlters spectrogram front-end summarize using domain knowledge designing models allows naturally connect deep learning literature previous signal processing work. hand domain knowledge used common employ deep stack small ﬁlters e.g. sample-level frontend used waveforms ﬁlters used spectrograms models based small ﬁlters make minimal assumptions local stationarities signal structure learned hierarchically combining small-context representations. architectures small ﬁlters ﬂexible models able potentially learn structure given enough depth data. back-ends. among different back-ends used audio literature identiﬁed main groups ﬁxedlength input back-end variable-length input backend. generally convolutional nature front-end allows process different input lengths. therefore back-end unit adapt variable-length feature ﬁx-sized output. former group models assume input kept constant examples front-ends based feed-forward neural-networks fully-convolutional stacks second group deal different input-lengths since model ﬂexible least input dimensions examples back-ends using temporal-aggregation strategies max-pooling average-pooling attention models recurrent neural networks given songs generally different lengths types back-ends ideal candidates music processing. however despite different-length nature music many works employ ﬁxed-length input back-ends since architectures tend simpler perform well study different deep architectures music autotagging perform music collections different sizes magnatagatune dataset music audio clips predicting top- tags dataset popular benchmark auto-tagging. although million song dataset name indicates songs available audio ﬁles proper annotations available previews dataset constitutes biggest public dataset available music auto-tagging making data highly appropriate benchmarking. private dataset consisting songs training validation test available study. .m-songs dataset track-level humanexpert annotations summarized follows jazz rock world music classical music etc. large audio datasets exist free music archive audioset since previous works mainly used employ datasets assess studied models public data. despite interest using brevity restrict study datasets already cover wide range different sizes. finally audioset used since content music. initial exploration different architectures introduced section select models based opposite design paradigms processing waveforms design minimal assumptions task hand; another spectrograms design heavily relies musical domain knowledge. goal compare models providing insights whether domain knowledge required designing deep learning models. section provides discussion around architectural choices introduces basic conﬁguration setup also accessible online. waveform model selected observing sample-level front-end remarkably superior waveform-based front ends shown original paper result particularly compelling front-end rely domain-knowledge design. note waveforms model without preprocessing small ﬁlters considered design make strong assumptions informative local stationarities waveforms. therefore samplelevel seen problem agnostic front-end potential learn audio task provided enough depth data available. given large amount data available study sample-level front-end particular interest strong learning potential solution space constrained severe architectural choices relying domain knowledge. figure bottom-left back-end. top-left waveform front-end. right spectrogram front-end. deﬁnitions stands feature map’s vertical axis batch norm max-pool. hand experimenting spectrogram front-ends found domain knowledge intuitions valid guides designing deep architectures. example front-ends based many vertical horizontal ﬁlters ﬁrst layer consistently superior front-ends based single vertical ﬁlter shown recent publications note former frontends learn spectral temporal representations already ﬁrst layer known important musical cues; latter learn spectral representations. moreover observed frontends based deep stack ﬁlters achieving equivalent performances former front-end input segments shorter noted literature considering longer inputs computational price deeper model increases longer inputs implies having larger feature maps every layer therefore memory consumption. reason refrained using deep stack ﬁlters front-end because vram enough input audio using back-end. hence making domain knowledge also provides guidance minimizing computational cost model since using single layer many vertical horizontal ﬁlters efﬁciently capture receptive ﬁeld without paying cost going deep. finally note front-ends using many vertical horizontal ﬁlters ﬁrst layer example deep architectures relying domain knowledge design. considering previous discussion select sample-level front-end main part assumption-free model waveforms; spectrogram front-end many vertical horizontal ﬁlters model designed considering domain knowledge. experiments share back-end enables fair comparison among previously selected front-ends. unless otherwise stated following speciﬁcations ones used experiments throughout document refer speciﬁcations basic conﬁguration shared back-end. consists three layers pooling layers dense layer figure introduced residual connections model explore deep architectures take advantage large data available. although adding residual layers drastically improve results observed adding residual connections stabilized learning slightly improving performance used dcnn ﬁlters computationally efﬁcient shaped extracted features considered across reasonable amount temporal context also make drastic temporal pooling ﬁrstly down-sampling temporal dimensionality feature maps; secondly making global pooling mean statistics. global pooling strategy allows variable length inputs network therefore model classiﬁed variable-length input back-end. finally dense layer units connects pooled features sigmoidal output. based sample-level front-end composed seven d-cnn batch norm pool layers figure layer ﬁlters. .m-songs dataset model capacity nine layers ﬁlters. hierarchically combining smallcontext representations making pooling sample-level front-end yields feature audio segment further processed previously described back-end. spectrogram front-end. firstly audio segments converted log-mel magnitude spectrograms normalized zero-mean unit-var. secondly vertical horizontal ﬁlters explicitly designed facilitate learning timbral temporal patterns present spectrograms note figure spectrogram front-end single-layer many ﬁlter shapes branch timgrouped branches bral features lower branch temporal features branch designed capture pitchinvariant timbral features occurring different time-frequency scales spectrogram. pitch invariance enforced enabling ﬁlters convolve frequency domain max-pooling feature across vertical axis note several ﬁlter shapes used efﬁciently capture many different time-frequency patterns table .m-songs average results using different training-set sizes. baseline gbts+features facilitate learning e.g. kick-drums string ensemble instruments lower branch meant learn temporal features designed efﬁciently capture different time-scale representations using several long ﬁlter shapes ﬁlters operate energy envelope obtained mean-pooling frequency-axis spectrogram. computing energy envelope considering high frequencies together minimizing computations model note frequency/vertical convolutions performed convolutions. thus domain knowledge also providing guidance minimize computational cost model. output branches merged previously described back-end used going deeper. details online implementation. parameters. dropout every dense layer relus non-linearities models trained employing adam optimizer. minimize .m-songs dataset minimize cross entropy datasets. training data converted audio patches prediction aims consider whole song. several predictions computed song averaged. although models capable predicting tags variable-length inputs ﬁxed length patches since preliminary experiments observed predicting whole song yielded worse results averaging several patch predictions. future work study behavior ways exploit fact whole song generally available. experimental setup. baseline system consisting music feature extractor model based gradient boosted trees predicting tags predicting individually aims turn hard problem multiple problems. careful inspection dataset reveals that among tags different data distributions dominate annotations tags bi-modal distributions annotations zero classiﬁed; tags pseudo-uniform distributions regressed. regression example acoustic indicates acoustic song zero zero electronic music song string quartet. classiﬁcation example genre example songs cataloged since dataset large taxonomy contains dozens genres. sets performance measurements roc-auc pr-auc classiﬁcation tags regression tags. roc-auc error given classiﬁcation tags highly unbalanced also consider pr-auc metric since indicative roc-auc cases roc-auc pr-auc higher score better lower better. studied spectrogram waveform models following basic conﬁguration composed parameters respectively. given unprecedented size dataset focus models scale trained different amounts data songs. average results shown table figure quantitative results. training models songs took days songs week songs less weeks. deep learning models trained tracks achieve better results baseline every metric. however deep learning models trained tracks perform worse baseline. result conﬁrms deep learning models require large datasets clearly outperform strong methods based feature-design although note large datasets generally available audio tasks. moreover biggest performance improvement w.r.t. baseline seen pr-auc provides informative picture performance dataset unbalanced addition best performing model based waveform front-end capable outperforming spectrogram model every metric trained songs. result conﬁrms waveform sample-level front-ends great potential learn large data since solution space constrained severe architectural choice. hand architectural choices deﬁning spectrogram front-end might constraining solution space. architectural constraints harmful training data scarce strong regularization solution space limit learning capacity model scenarios large training data available songs results. observe figure linear models obtained results study behavior. training songs available trend lines show spectrogram models tend perform better. however training songs available lines show waveform models outperform spectrogram ones. worth mentioning observed trends consistent throughout metrics roc-auc pr-auc finally note room improving models study e.g. could address data imbalance problem training improve back-end exploring alternative temporal aggregation strategies. qualitative results. since ﬁrst report deep music tagging model trained large dataset also perceptually assess quality estimates. compared predictions best performing models predictions baseline human-annotated ground-truth tags. interesting examples identiﬁed qualitative experiment available online. first observed deep learning model biased towards predicting popular tags note expected since addressing data unbalancing issue training. second observe baseline model predicts mutually exclusive tags high conﬁdence e.g. predicted high scores east coast west coast east cost song baroque period classic period bach aria. however deep learning model able better differentiate similar mutually exclusive tags. suggests deep learning advantage compared traditional approaches since mutually exclusive relations jointly encoded within model. magnatagatune dataset experimental setup. state-of-the-art models baselines performance metrics .m-songs dataset rocauc pr-auc note labels binary. baseline results roc-auc) computed using slightly different version dataset includes songs lasting seconds. result cleaner version dataset songs instead although dataset cleans potential noisy annotations decided original dataset easily compare results former works. thus fairly compare models samplecnn reproduce work considering original dataset achieving score roc-auc. given less noise present samplecnn dataset seems reasonable performance higher obtained implementation. experiments divided parts waveform spectrogram models tables amenable size dataset feasible comprehensive study investigating different architectural conﬁgurations. speciﬁcally study waveform spectrogram architectures behave modifying capacity frontback-ends. example experiment ﬁlters table consists dividing number ﬁlters available waveform front-end two. means ﬁlters instead ﬁlters basic conﬁguration. also apply methodology spectrogram front-ends add/remove capacity increasing/decreasing number available ﬁlters. running front-end experiments ﬁxed back-end select promising ones proceed back-end study waveforms ﬁlters spectrograms ﬁlters ﬁxed front-end every experiment modify capacity back-end changing number ﬁlters every layer changing number output units since basic conﬁguration leads relatively models size dataset experiments explore smaller back-ends. inputs since longer inputs yield worse results quantitative results. waveform spectrogram models study outperform proposed baselines represent current state-of-the-art. further performance quite robust number parameters model. although best results achieved models higher capacity performance difference between small large models minor means relatively small models reasonable tagging music. finally spectrogram models perform better waveform models small public dataset aligns previous works using datasets similar size consequently results conﬁrm domain knowledge intuitions valid guides designing deep architectures scenarios training data scarce. million song dataset experimental setup. state-of-the-art models baselines performance metrics .m-songs dataset roc-auc pr-auc note labels binary. experiments validate studied models biggest public dataset available. models following basic conﬁguration results shown table quantitative results. spectrogram model outperforms waveform model public dataset having training songs. furthermore spectrogram model performs equivalently ‘multi-level multiscale’ best performing method literature denoting musical knowledge utility design models msd. additionally waveform model performs worse waveform-based models also employ sample-level front-ends. performance decrease could caused samplecnn methods average estimates song compensate possible faults song-level predictions method averages predicting consecutive patches major difference samplecnn waveform model latter employs global pooling strategy could remove potentially useful information model. besides best performing waveform-based model also achieves lower scores best performing spectrogram-based ones. considering outstanding results report waveform model trained songs could argue lack larger public datasets limiting outcomes deep learning research music autotagging particularly dealing end-to-end learning stacks processing data waveforms. study presents ﬁrst work describing different deep music auto-tagging architectures perform depending amount available training data. also present architectures yield results stateof-the-art. architectures based conceptually different design principles based waveform front-end domain knowledge inspired design; other spectrogram front-end makes domain knowledge justify architectural choices. results suggest models relying domain knowledge play relevant role scenarios sizable datasets available shown that given enough data assumption-free models processing waveforms outperform rely musical domain knowledge. thierry bertin-mahieux daniel ellis brian whitman paul lamere. million song dataset. international society music information retrieval conference ning chen shijun wang. high-level music descriptor extraction algorithm based combination multi-channel cnns lstm. international society music information retrieval conference keunwoo choi george fazekas mark sandler. automatic tagging using deep convolutional neural networks. international society music information retrieval conference keunwoo choi gy¨orgy fazekas mark sandler kyunghyun cho. convolutional recurrent neural networks music classiﬁcation. ieee international conference acoustics speech signal processing micha¨el defferrard kirell benzi pierre vandergheynst xavier bresson. dataset music analysis. international society music information retrieval conference jort gemmeke daniel ellis dylan freedman jansen wade lawrence channing moore manoj plakal marvin ritter. audio ontology human-labeled dataset audio events. ieee international conference acoustics speech signal processing edith kris west michael mandel mert stephen downie. evaluation algorithms using games case music tagging. international society music information retrieval conference honglak peter pham largman andrew unsupervised feature learning audio classiﬁcation using convolutional deep belief networks. advances neural information processing systems jongpil juhan nam. multi-level multiscale feature aggregation using pretrained convolutional neural networks music auto-tagging. ieee signal processing letters jongpil jiyoung park keunhyoung luke juhan nam. sample-level deep convolutional neural networks music auto-tagging using waveforms. sound music computing conference jongpil jiyoung park keunhyoung luke juhan nam. samplecnn end-to-end deep convolutional neural networks using small ﬁlters music classiﬁcation. applied sciences phan lars hertel marco maass alfred mertins. robust audio event recognition -max pooling convolutional neural networks. arxiv preprint arxiv. jordi pons thomas lidy xavier serra. experimenting musically motivated convolutional neural networks. international workshop contentbased multimedia indexing pages ieee jordi pons xavier serra. designing efﬁcient architectures modeling temporal features convolutional neural networks. ieee international conference acoustics speech signal processing jordi pons olga slizovskaia rong gong emilia g´omez xavier serra. timbre analysis music audio signals convolutional neural networks. european signal processing conference matthew prockup andrew asman fabian gouyon erik schmidt oscar celma youngmoo kim. modeling rhythm using tree ensembles music genome project. machine learning music tara sainath weiss andrew senior kevin wilson oriol vinyals. learning speech frontend waveform cldnns. sixteenth annual conference international speech communication association schluter sebastian bock. improved musical onset detection convolutional neural networks. ieee international conference acoustics speech signal processing mohamed sordo cyril laurier oscar celma. annotating music collections content-based similarity helps propagate labels. international society music information retrieval conference", "year": "2017"}