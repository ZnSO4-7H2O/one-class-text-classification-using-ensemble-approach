{"title": "Trace norm regularization and faster inference for embedded speech  recognition RNNs", "tag": "eess", "abstract": " We propose and evaluate new techniques for compressing and speeding up dense matrix multiplications as found in the fully connected and recurrent layers of neural networks for embedded large vocabulary continuous speech recognition (LVCSR). For compression, we introduce and study a trace norm regularization technique for training low rank factored versions of matrix multiplications. Compared to standard low rank training, we show that our method leads to good accuracy versus number of parameter trade-offs and can be used to speed up training of large models. For speedup, we enable faster inference on ARM processors through new open sourced kernels optimized for small batch sizes, resulting in 3x to 7x speed ups over the widely used gemmlowp library. Beyond LVCSR, we expect our techniques and kernels to be more generally applicable to embedded neural networks with large fully connected or recurrent layers. ", "text": "markus kliegl siddharth goyal kexin zhao kavya srinet mohammad shoeybi baidu silicon valley artiﬁcial intelligence {klieglmarkusgoyalsiddharthzhaokexinsrinetkavya mohammad}baidu.com propose evaluate techniques compressing speeding dense matrix multiplications found fully connected recurrent layers neural networks embedded large vocabulary continuous speech recognition compression introduce study trace norm regularization technique training rank factored versions matrix multiplications. compared standard rank training show method leads good accuracy versus number parameter trade-offs used speed training large models. speedup enable faster inference processors open sourced kernels optimized small batch sizes resulting speed widely used gemmlowp library. beyond lvcsr expect techniques kernels generally applicable embedded neural networks large fully connected recurrent layers. embedded applications machine learning seek models accurate possible given constraints size latency inference time. many neural networks parameters computation concentrated basic building blocks convolutions. tend dominate example image processing applications. dense matrix multiplications found example inside fully connected layers recurrent layers lstm. common speech natural language processing applications. building blocks natural targets efforts reduce parameters speed models embedded applications. much work topic already exists literature. brief overview section trace norm regularization describe trace norm regularization technique accompanying training methodology enables practical training models competitive accuracy versus number parameter trade-offs. automatically selects rank eliminates need prior knowledge suitable matrix rank. efﬁcient kernels inference explore importance optimizing batch sizes on-device inference introduce kernels processors vastly outperform publicly available kernels batch size regime. topics discussed sections respectively. although conducted experiments report results context large vocabulary continuous speech recognition embedded devices ideas techniques broadly applicable deep learning networks. work compressing neural network large gemms dominate parameters computation time could beneﬁt insights presented paper. work closely related prabhavalkar rank factored acoustic speech models similarly trained initializing weights truncated singular value decomposition pretrained weight matrices. technique also applied speech recognition mobile devices build method adding variational form trace norm regularization ﬁrst proposed collaborative prediction also applied recommender systems technique gradient descent recently justiﬁed theoretically furthermore neyshabur argue trace norm regularization could provide sensible inductive bias neural networks. best knowledge ﬁrst combine training technique prabhavalkar variational trace norm regularization. rank factorization neural network weights general subject many works approaches dense matrix compression include sparsity hash-based parameter sharing parameter-sharing schemes circulant toeplitz generally low-displacement-rank matrices kuchaiev ginsburg explore splitting activations independent groups. akin using block-diagonal matrices. rank factorization well studied effective technique compressing large matrices. prabhavalkar rank models trained ﬁrst training model unfactored weight matrices initializing model factored weight matrices truncated unfactored model truncation done retaining many singular values required explain speciﬁed percentage variance. weight matrices stage nonzero singular values truncated used warmstarting stage would yield much better even error-free approximation stage matrix. suggests applying sparsity-inducing penalty vector singular values stage training. known trace norm regularization literature. unfortunately known directly computing trace norm gradients would computationally feasible context large deep learning models. instead propose combine two-stage training method prabhavalkar indirect variational trace norm regularization technique describe technique detail section report experimental results section first introduce notation. denote trace norm matrix singular values matrix. trace norm also referred nuclear norm schatten -norm literature. furthermore denote frobenius norm matrix deﬁned frobenius norm identical schatten -norm matrix i.e. norm singular value vector matrix. following lemma provides variational characterization trace norm terms frobenius norm. procedure take advantage characterization follows. first large gemm model replace weight matrix product second replace original loss function hyperparameter controlling strength approximate trace norm regularization. proposition ciliberto guarantees minimizing modiﬁed loss equation equivalent minimizing actual trace norm regularized loss modiﬁcation described section show actually necessary train stage model convergence switching stage making transition earlier training time substantially reduced. report results experiments related trace norm regularization. baseline model forward-only deep speech model train evaluate widely used wall street journal speech corpus. except minor modiﬁcations described appendix follow closely original paper describing architecture refer reader paper details inputs outputs exact layers used training methodology purposes paper sufﬁce parameters computation dominated three layers fully connected layer. four layers compress low-rank factorization. described appendix factorization scheme since work focuses compressing acoustic models language models error metric report character error rate rather word error rate size latency constraints vary widely across devices whenever possible compare techniques comparing accuracy versus number parameter trade-off curves. cers reported computed validation separate training set. corpus relatively small around hours speech models tend beneﬁt substantially regularization. make comparisons fair also trained unfactored models regularization term searched hyperparameter space exhaustively. trace norm regularization found beneﬁcial introduce separate λrec λnonrec parameters determining strength regularization recurrent non-recurrent weight matrices respectively. addition λrec λnonrec initial experiments also roughly tuned learning rate. since learning rate found optimal nearly experiments used experiments reported section. dependence ﬁnal λrec λnonrec shown figure separate λrec λnonrec values seen help trace norm regularization. however trace norm regularization appears better λrec multiple λnonrec rather tuning parameters independently. ﬁrst question interested whether modiﬁed loss really effective reducing trace norm. interested relative concentration singular values rather absolute magnitudes introduce following nondimensional metric. figure nondimensional trace norm coefﬁcient versus strength regularization type regularization used training. left results non-recurrent weight third layer λrec right results recurrent weight third layer λnonrec plots weights similar. figure truncated rank required explain variance weight matrix versus type regularization used training. shown results nonrecurrent recurrent weights third layer. plots weights similar. show appendix scale-invariant ranges rank matrices maximal-rank matrices singular values equal. intuitively smaller better approximated rank matrix. shown figure trace norm regularization indeed highly effective reducing nondimensional trace norm coefﬁcient compared regularization. high regularization strengths regularization also leads small values. however figure apparent comes expense relatively high cers. shown figure translates requiring much lower rank truncated explain variance weight matrix given cer. although -regularized models occasionally achieve rank observe relatively high cer’s weights. note also form regularization important dataset. unregularized baseline model achieves relatively cer. regularization type took three best stage models used truncated weights initialize weights stage models. varying threshold variance explained truncation stage model resulted multiple stage models. stage models trained without regularization initial learning rate three times ﬁnal learning rate stage model. shown figure best models either trace norm regularization exhibit similar accuracy versus number parameter trade-offs. comparison also warmstarted stage models unregularized stage model. models seen signiﬁcantly lower accuracies accentuating need regularization corpus. previous sections trained stage models epochs full convergence trained stage models another epochs full convergence. since stage models drastically smaller stage models takes less time train them. hence shifting stage stage transition point earlier epoch could substantially reduce training time. section show indeed possible without hurting ﬁnal accuracy. speciﬁcally took stage trace norm models section resulted best stage models section section interested parameters accuracy trade-off used stage model warmstart number stage models different sizes. section instead ﬁxed target parameters ﬁxed overall training budget epochs vary stage stage transition epoch. stage runs initialize learning rate learning rate stage model transition epoch. learning rate follows schedule trained single model epochs. before disable regularization stage stage model parameters whereas trace norm stage model parameters slightly larger factorization. since stage models roughly parameters training time approximately proportional number parameters stage models train faster respectively trace norm stage models. consequently large overall training time reductions achieved reducing number epochs spent stage trace norm. results shown figure based left panel evident lower transition epoch number without hurting ﬁnal cer. cases even marginal improvements. transition epochs least also slightly better results trace norm right panel plot convergence transition epoch trace norm model’s barely impacted transition whereas models figure left versus transition epoch colored type regularization used training stage model. right training progresses colored type regularization used stage dotted line indicates transition epoch. table three tiers rank speech recognition models production server model internal test set. table illustrates effect shrinking acoustic model. large server-grade language model used rows. huge jump transition epoch. furthermore plot suggests total epochs sufﬁced. however savings reducing stage epochs negligible compared savings reducing transition epoch. rank factorization techniques similar described section able train large vocabulary continuous speech recognition models acceptable numbers parameters acceptable loss accuracy compared production server model table shows baseline along three different compressed models much lower number parameters. tier- model employs techniques sections consequently runs signiﬁcantly faster tier- model even though similar number parameters. unfortunately comes expense loss accuracy. although rank factorization signiﬁcantly reduces overall computational complexity lvcsr system still require optimization achieve real-time inference mobile embedded devices. approach speeding network low-precision -bit integer representations weight matrices matrix multiplications type quantization training reduces memory well computation requirements network introducing relative increase wer. quantization embedded speech recognition also previously studied possible reduce relative increase quantizing forward passes training relative losses compressing acoustic language models much larger pursue present study. this work done prior development trace norm regularization. long training cycles hours speech used section started pretrained models. however techniques section entirely agnostic differences. figure comparison kernels gemmlowp library matrix multiplication iphone iphone raspberry benchmark computes random matrix dimension random matrix dimension batch size. matrices unsigned -bit integer format. perform precision matrix multiplications originally used gemmlowp library provides state-of-the-art precision gemms using unsigned -bit integer values however gemmlowp’s approach efﬁcient small batch sizes. application lvcsr embedded devices single user dominated batch size gemms sequential nature recurrent layers latency constraints. demonstrated looking simple cell form cell contains main gemms ﬁrst sequential requires gemm batch size second principle performed higher batch sizes batching across time. however choosing large batch sizes signiﬁcantly delay output system needs wait future context. practice found batch sizes higher around resulted high latencies negatively impacting user experience. motivated implement custom assembly kernels -bit architecture improve performance gemms operations. methodological details paper. instead making kernels implementation details available https//github.com/paddlepaddle/farm. figure compares performance implementation gemmlowp library matrix multiplication iphone iphone raspberry model farm kernels signiﬁcantly faster gemmlowp counterparts batch sizes peak single-core theoretical performance iphone iphone raspberry giga operations second respectively. theoretical achieved values mostly kernels limited memory bandwidth. detailed analysis refer farm website. finally combining rank factorization techniques appendix quantization farm kernels well using smaller language models could create range speech recognition models suitably tailored various devices. shown table worked compressing reducing inference latency lvcsr speech recognition models. better compress models introduced trace norm regularization technique demonstrated potential faster training rank models speech corpus. reduce latency inference time demonstrated importance optimizing batch sizes released optimized kernels platform. finally combining various techniques would like thank gregory diamos christopher fougner atul kumar julia sharan narang thuan nguyen sanjeev satheesh richard wang wang zhenyao helpful comments assistance various parts paper. also thank anonymous referees comments greatly improved exposition helped uncover mistake earlier version paper. references raziel alvarez rohit prabhavalkar anton bakhtin. efﬁcient representation execution deep acoustic models. proceedings annual conference international speech communication association https//arxiv.org/abs/. dario amodei sundaram ananthanarayanan rishita anubhai jingliang eric battenberg carl case jared casper bryan catanzaro qiang cheng guoliang chen deep speech endto-end speech recognition english mandarin. international conference machine learning wenlin chen james wilson stephen tyree kilian weinberger yixin chen. compressing neural networks hashing trick. international conference machine learning kyunghyun bart merri¨enboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. syntax semantics structure statistical translation emily denton wojciech zaremba joan bruna yann lecun fergus. exploiting linear structure within convolutional networks efﬁcient evaluation. advances neural information processing systems song huizi william dally. deep compression compressing deep neural networks pruning trained quantization huffman coding. international conference learning representations forrest iandola song matthew moskewicz khalid ashraf william dally kurt keutzer. squeezenet alexnet-level accuracy fewer parameters and¡ model size. arxiv preprint arxiv. zhiyun vikas sindhwani tara sainath. learning compact recurrent neural networks. acoustics speech signal processing ieee international conference ieee mcgraw rohit prabhavalkar raziel alvarez montse gonzalez arenas kanishka david rybach ouais alsharif has¸im alexander gruenstein franc¸oise beaufays personalized speech recognition mobile devices. acoustics speech signal processing ieee international conference ieee behnam neyshabur ryota tomioka nathan srebro. search real inductive bias role implicit regularization deep learning. workshop track iclr arxiv preprint arxiv.. rohit prabhavalkar ouais alsharif antoine bruguier mcgraw. compression recurrent neural networks application lvcsr acoustic modeling embedded speech recognition. acoustics speech signal processing ieee international conference ieee tara sainath brian kingsbury vikas sindhwani ebru arisoy bhuvana ramabhadran. lowrank matrix factorization deep neural network training high-dimensional output targets. acoustics speech signal processing ieee international conference ieee vincent vanhoucke andrew senior mark mao. improving speed neural networks cpus. proc. deep learning unsupervised feature learning nips workshop volume ﬁrst inequality holds since singular values nonnegative inequality strict unless vanishes. second inequality comes application jensen’s inequality strict unless thus replacing preserves ||σ|| increasing ||σ|| unless zero. similarly replacing preserves ||σ|| decreasing ||σ|| unless simple argument contradiction follows minima occur case maxima occur case case visualized figure ﬁxed ||σ|| ||σ|| vary minimum ||σ|| happens either zero. values ||σ|| ||σ|| result similarly maximum ||σ|| happens resulting across several data sets model architectures consistently found sizes recurrent layers closer input could shrunk without affecting accuracy much. related phenomenon observed prabhavalkar rank approximations acoustic model layers using rank required explain ﬁxed threshold explained variance grows distance input layer. reduce number parameters baseline model speed experiments thus chose adopt growing dimensions. since hope compression techniques studied paper automatically reduce layers near-optimal size chose tune dimensions simply picked reasonable afﬁne increasing scheme dimensions dimension ﬁnal fully connected layer. partially joint factorization. concatenate recurrent matrices single matrix likewise concatenate non-recurrent matrices single matrix apply rank factorization separately. authors opted lstm analog completely joint factorization choice parameter sharing thus highest potential compression model. however decided partially joint factorization instead largely reasons. first pilot experiments found matrices behave qualitatively quite differently training. example large data sets matrices trained scratch factored form whereas factored matrices need either warmstarted trained unfactored model trained signiﬁcantly lowered learning rate. second split advantageous terms computational efﬁciency. non-recurrent gemm sequential time dependency thus inputs batched across time. finally compared partially joint factorization completely split factorization found former indeed better accuracy versus number parameters trade-offs. results experiment shown table switching -dimensional linear spectrograms -dimensional spectrograms reduces per-timestep feature dimension roughly factor furthermore likely owing switch could reduce frequency-dimension size convolution ﬁlters factor combination means reduction compute ﬁrst second convolution layers reduction compute ﬁrst layer. gram-ctc recently proposed extension training models output variable-size grams opposed single characters using gram-ctc able increase time stride second convolution layer factor little loss though double number ﬁlters convolution layer compensate. effect roughly speedup second third layers largest. speed makes size increase softmax layer slightly complex language model decoding using gram-ctc. however given target accuracy found gram-ctc models could shrunk much models means rank factorization. effect technique increase model size exchange reduced latency. baseline model deep speech model three forward-gru layers dimension described amodei baseline model used experiments narang paper also obtained sparse data points plot. shown also versions baseline model dimension scaled overall models rank factorizations non-recurrent recurrent weight matrices seen provide best parameters trade-off. rank models growing dimensions partially split form rank factorization discussed sections models labeled fast addition gram-ctc described section features reduced convolution ﬁlter sizes described section preliminary comparison past experiments setup perfectly controlled models were example trained epochs others. suspect that given effort similar adjustments like growing dimensions sparse models could made competitive rank models. even given computational advantage rank approach unstructured sparsity chose focus former going forward. course rule potential usefulness other structured forms sparsity embedded setting.", "year": "2017"}