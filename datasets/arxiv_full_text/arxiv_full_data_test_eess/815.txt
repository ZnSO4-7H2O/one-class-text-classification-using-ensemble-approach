{"title": "An improved DNN-based spectral feature mapping that removes noise and  reverberation for robust automatic speech recognition", "tag": "eess", "abstract": " Reverberation and additive noise have detrimental effects on the performance of automatic speech recognition systems. In this paper we explore the ability of a DNN-based spectral feature mapping to remove the effects of reverberation and additive noise. Experiments with the CHiME-2 database show that this DNN can achieve an average reduction in WER of 4.5%, when compared to the baseline system, at SNRs equal to -6 dB, -3 dB, 0 dB and 3 dB, and just 0.8% at greater SNRs of 6 dB and 9 dB. These results suggest that this DNN is more effective in removing additive noise than reverberation. To improve the DNN performance, we combine it with the weighted prediction error (WPE) method that shows a complementary behavior. While this combination provided a reduction in WER of approximately 11% when compared with the baseline, the observed improvement is not as great as that obtained using WPE alone. However, modifications to the DNN training process were applied and an average reduction in WER equal to 18.3% was achieved when compared with the baseline system. Furthermore, the improved DNN combined with WPE achieves a reduction in WER of 7.9% when compared with WPE alone. ", "text": "reverberation additive noise detrimental effects performance automatic speech recognition systems. paper explore ability dnn-based spectral feature mapping remove effects reverberation additive noise. experiments chime- database show achieve average reduction compared baseline system snrs equal greater snrs results suggest effective removing additive noise reverberation. improve performance combine weighted prediction error method shows complementary behavior. combination provided reduction approximately compared baseline observed improvement great obtained using alone. however modifications training process applied average reduction equal achieved compared baseline system. furthermore improved combined achieves reduction compared alone. index terms speech recognition reverberation dnn-based speech enhancement indoor environment reverberation additive noise detrimental effects performance systems therefore challenge implement methods capable reducing eliminating reverberation additive noise speech signals methods divided three classes depending implemented system front-end; back-end; speech preprocessing preprocessing methods also known speech enhancement neural networks speech distortion removal new. multilayer perceptron inspired lateral inhibition process proposed cancel additive noise. methods focused learning clean version mfcc features using either denoising autoencoders recurrent neural network furthermore spectral subtraction particularly dnn-based spectral feature mapping proposed. sfm-dnn capability spectral features filterbank features process additive noise reverberation canceled reduced. however resulting sfm-dnn removes additive noise effectively reverberation. sfm-dnn built multilayer perceptron composed hidden layers units output layer units. activation functions hidden output layer correspond sigmoid function. input spectrograms extracted corrupted voice samples. reference used train filterbank features extracted corresponding clean speech samples. input features normalized zero mean unit variance feature vectors temporal dynamics incorporates rich information speech neighborhood frames included current frame. reference features normalized range trained using backpropagation mini-batch stochastic gradient descendent algorithm. adaptive gradient descendent used optimization technique cost function based mean square error. widely employed enhancement method dereverberation corresponds significant improvements accuracy different reverberant conditions method focused reverberation suppression means iterative estimation linear regression filter coefficients. technique performs coefficient estimation using short time fourier transform process reverberation free speech time waveform obtained. paper improve sfm-dnn performance making wpe. furthermore introduced modifications sfm-dnn training process improve effectiveness distortion removal. result final equal relative reduction high compared baseline system noisy training achieved. results reported paper competitive published elsewhere using database presently employed also reduction compared table shows wers reported validated results local system. results suggest sfm-dnn ability reduce effect additive noise rather reverberation. reason could improve sfm-dnn performance combining method focused reverberation reduction i.e. wpe. cascade configuration enhancement methods deserves brief discussion. instance discussed channel response effectively removed additive noise cancelled considering former supposed time-invariant latter non-stationary. however case considered here reverberation additive noise assumed non-stationary. consequently order cascade removal would seem relevant. however case considered here cascade sequence order constrained input output sfm-dnn. result proposed scheme shown fig. figure shows reductions compared baseline system provided sfmdnn methods snr. sfm-dnn scheme provides reduction decreases increases. actually equal sfm-dnn introduces distortion. results suggest sfm-dnn cancels effectively additive noise rather reverberation. contrast provides reduction original sfm-dnn section present justify combination sfm-dnn wpe. section describes modifications improvements incorporated sfm-dnn training process. discussions conclusions presented sections respectively. experiments conducted using kaldi toolkit system based dnn-hmm input -dimentional filterbank deltas delta-delta dynamic features -frame context window. dnn-hmm trained using tied triphone state targets obtained clean gmm-hmm alignment. realignment performed using trained retrain performed figure block diagram proposed cascade sequence methods. represent respectively speech signal temporal domain free reverberation speech time waveform spectral magnitude resulting filterbank enhanced speech signal. figure shows results experiments obtained baseline sfm-dnn wpe+sfm-dnn systems. compared baseline system sfm-dnn wpe+sfm-dnn provided reductions equal respectively turn validates proposed approach illustrated fig. however wpe+sfm-dnn. consequently sfm-dnn resulting improved wpe+sfm-dnn system performance sfm-dnn. result motivated improve incorporating modifications training process. order improve sfm-dnn effectiveness explored dropout cross-validation alternative inputreference normalization. reference training procedures described employed replicate results table large neural networks difficult cope overfitting. alternative address problem without increasing size database apply dropout scheme temporarily deactivates units training fig. shows graphic description method. overfitting also observed multilayer perceptron backpropagation-based training procedure stopped right time avoid problem cross validation method applied development i.e. performing given number training epochs training paused development propagated compute corresponding cost error. development error cost increases reaches convergence criterion training stopped neural network parameters previous epoch retrieved. otherwise training process resumed. paper cross-validation scheme performed training epoch. sfm-dnn implementation cross-validation carried chime- track development set. training stopped conditions reached development error cost increases respect previous epoch; development error cost decreases less respect previous epoch. discussed neural network training procedure sensitive previous normalization performed input reference features training stage. also results reported suggest utterance-by-utterance normalizations give better results than instance speaker-by-speaker basis. consequently applied input reference data utterance-by-utterance basis. alone baseline system wpe+sfm-dnn-e reductions equal respectively. final average compares favorably published elsewhere database i.e. chime-. paper effectiveness spectral feature mapping improved remove effects reverberation additive noise. experiments chime- database show effective removing additive noise reverberation. counteract limitation combined shows complementary behavior respect improvement recognition accuracy snr. also modifications training process successfully applied. consequence final average equal relative reduction high compared baseline system noisy training achieved. results competitive published elsewhere using chime- database. delcroix yoshioka ogawa kubo fujimoto kinoshita espi araki hori nakatani strategies distant speech recognitionin reverberant environments eurasip journal advances signal processing vol. kinoshita delcroix yoshioka nakatani sehr kellermann maas reverb challenge acommon dereverberation recognition reverberant speech proc. waspaa paltz vincent barker watanabe roux nesta matassoni second ‘chime’speech separation recognition challenge overview challenge systems outcomes automatic speech recognition understanding ieee workshop olomouc tsilfidis mporas mourjopoulos fakotakis automatic speech recognition performance different room acoustic environments without dereverberation preprocessing computer speech language incorporated dropout cross-validation utterance-byutterance normalization sfm-dnn training procedure. wers provided resulting system sfm-dnn-e shown fig. figure shows evolution error cost training development data. early stop training procedure avoid overfitting effect. according table able achieve similar results reported sfm-dnn. accordingly sfm-dnn increased compared baseline system i.e. noisy training conditions less presence additive noise presence reverberation i.e. mentioned above results motivation include method i.e. reduce effect reverberation. seen fig. presents complementary behavior compared sfm-dnn respect improvement recognition accuracy snr. result wpe+sfm-dnn system reduced compared baseline. however higher reduction wpe+sfm-dnn. sfm-dnn improved incorporating modifications training procedure. seen fig. resulting sfm-dnn-e system provided lower original sfm-dnn. compared srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overfitting journal machine learning research vol. fredes novoa king stern becerra yoma locally normalized filter banks applied deep neural-network-based robust speech recognition ieee signal processing letters vol. maas o’neil hannun recurrent neural network feature enhancement chime challenge proceedings chime workshop machine listening multisource environments held conjunction icassp weninger watanabe tachioka schuller deep recurrent de-noising auto-encoder blind dereverberation reverberated speech recognition proceedings icassp florence italy bagchi fosler-lussier wang deep neural network based spectral feature mapping robust speech recognition proceedings interspeech dresden germany nakatani yoshioka kinoshita miyoshi juang speech dereverberation based variancenormalized ieee transactions audio speech language processing vol. erdogan hershey watanabe roux deep recurrent networks recognition single-channel speech nonstationary background audio robust speech recognition springer nathwani morales-cordovilla sivasankaran illina vincent extended experimental investigation uncertainty propagation noise robust proceedings hands-free speech communications microphone arrays francisco abadi barham chen chen davis dean devin ghemawat irving isard kudlur levenberg monga moore murray steiner tucker vasudevan warden wicke zheng tensorflow system large-scale machine usenix symposium operating systems design savannah povey ghoshal boulianne goel hannemann qian schwarz stemmer kaldi speech recognition toolkit ieee automatic speech recognition understanding workshop hawaii", "year": "2018"}