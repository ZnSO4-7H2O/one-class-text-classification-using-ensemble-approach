{"title": "Decoding visemes: improving machine lipreading", "tag": "eess", "abstract": " Machine lipreading (MLR) is speech recognition from visual cues and a niche research problem in speech processing & computer vision. Current challenges fall into two groups: the content of the video, such as rate of speech or; the parameters of the video recording e.g, video resolution. We show that HD video is not needed to successfully lipread with a computer. The term \"viseme\" is used in machine lipreading to represent a visual cue or gesture which corresponds to a subgroup of phonemes where the phonemes are visually indistinguishable. A phoneme is the smallest sound one can utter, because there are more phonemes per viseme, maps between units show a many-to-one relationship. Many maps have been presented, we compare these and our results show Lee's is best. We propose a new method of speaker-dependent phoneme-to-viseme maps and compare these to Lee's. Our results show the sensitivity of phoneme clustering and we use our new knowledge to augment a conventional MLR system. It has been observed in MLR, that classifiers need training on test subjects to achieve accuracy. Thus machine lipreading is highly speaker-dependent. Conversely speaker independence is robust classification of non-training speakers. We investigate the dependence of phoneme-to-viseme maps between speakers and show there is not a high variability of visemes, but there is high variability in trajectory between visemes of individual speakers with the same ground truth. This implies a dependency upon the number of visemes within each set for each individual. We show that prior phoneme-to-viseme maps rarely have enough visemes and the optimal size, which varies by speaker, ranges from 11-35. Finally we decode from visemes back to phonemes and into words. Our novel approach uses the optimum range visemes within hierarchical training of phoneme classifiers and demonstrates a significant increase in classification accuracy. ", "text": "©this copy thesis supplied condition anyone consults understood recognise copyright rests author quotation thesis information derived therefrom published without author’s prior written consent. speech bimodal. means modes information acoustic visual. humans signals understand speech others given acoustic recognition studied ﬁfty years surprising acoustic recognition mature visual-only recognition signiﬁcant increases performance speech recognition systems although remain susceptible noise imagine trying recognise pilot’s speech background noise aeroplane engine cockpit. case audio signal severely deteriorated noise environment. however noise aﬀect visual signal. thus desire recognise speech visual signal alone born. visual signal used combination acoustic signal audio-visual speech recognition possibility using visual signal alone. latter conﬁguration machine lip-reading topic thesis. lip-reading challenging task. researchers investigate avsr common audio recognition dominate beneﬁt lip-reading nevertheless make pure lip-reading successful would beneﬁts audio-visual recognition. furthermore scenarios impractical senseless install close microphone. example might interactive booth busy station airport poor signalto-noise ratio distance person screen. practice however major good machine lip-reading system would part avsr system. zidane opposite pitch football head-butted italian player without apparent justiﬁcation. action earned card consequently france went lose match world later transpired admitted materazzi zidane provoked targeted insult late family member. case machine lip-reading system present conﬁrm provocation whilst zidane would still carded would materazzi. thus playing outcome match world could diﬀerent. history great number silent videos. common examples silent entertainment ﬁlms historical documentaries. professional human lip-reader assist researchers comprehend soldier’s conversations went battle battle preparations. similarly shown lip-readers used home movies hitler give historians insight infamous ﬁgure interest. long debate silent entertainment ﬁlms ﬁlms ever scripted audio could captured video channel. learn that ﬁlms fact fully scripted human lip-reading experiments variation scripts fully noticeable. collectively human nature interested history learn historical evidence motivator achieving robust automatic lip-reading systems. theobald examine lip-reading enforcement. note law-enforcement many departments would beneﬁt automatic lip-reading system. present technique improved lip-reading whereby extracted features modiﬁed increase classiﬁcation performance. modiﬁcation amplifying feature parameters exaggerate gestures recorded camera. technique tested using phonetically balanced corpus syntactically correct sentences. data little contextual information remove eﬀects context network support. machine lip-reading would help enforcement robust lip-reading ﬁlmed conversations criminal acts e.g. cctv could evidence prosecution oﬀenders. murder case arelene fraser fraser caught imprisoned. evidence used prosecution included transcripts provided professional lip-reader jessica rees whilst perpetrator thought committed ‘perfect’ murder took steps avoid conversations being overheard thought could read lips. transcripts fraser’s conversations prosecutors turned co-conspirators witnesses fraser prosecuted. however later reliability lip-reading transcripts evidence successfully challenged human lip-readers unreliable. conversation also subject situation conversation means good lip-reader particular speaker could either misinterpret alternative speaker lip-reading person another place fail comprehend speech uttered. furthermore human lip-readers expensive examples consuelo gonsales jessica rees operate quoted basis. know robust lip-readers rare often verifying accuracy lip-reading performance ground truth rarely available. controlled experiments ground truth exists investigation eﬀect likeability individuals lip-read conversation status relationship showed good relationship increases accuracy lip-reading interpretation. apply observation real world scenario introducing lip-reader someone know personally video documentary deteriorates conﬁdence lip-reading ability robust. idea supported nichie’s lip-reading practice handbook chapter suggested value practicing lip-reading rightly attached teacher’s personality success. summerﬁeld describes reasons distinguish poor good lip-readers. list deduced results series experiments show achievement rates lip-reading tests range achievement rates vary parameter selections experiment chosen speciﬁc task addressed. particular accuracy metric classiﬁcation unit signiﬁcant aﬀect compare investigations. intelligence verbal reasoning mcgrath showed fundamental level intelligence verbal reasoning essential able lip-read beyond limit skills could raise human comprehension further. training human lip-readers either self-studied trained manner practice skill lip-reading shown better received training also shown human lip-readers actually worse training eﬀect present humans read videos rather presence speaker human brain. suggestion lip-reading diﬃcult learn dependent upon low-level neural processes. suggestion however received reproducible results support proposition comforts human lip-reading possible however challenging. closeness conversation participants studies show relationship description talking personable knowledge speaker interpreter improve human lipreading knowledge conversation context without constraint ‘rules’ language limit probable utterance lip-reading becomes almost impossible akin guessing experiments showed recognising isolated sentences scoring simply guessing context alone. summary main application machine lip-reading system would situation audio signal video either absent noisy comprehend alternative human lip-readers expensive unreliable. conventional lip-reading system consists sequence tasks shown figure work focuses classiﬁcation task. currently make assumptions tracking face video order extract features undertake machine lip-reading. ﬁrst task left hand side figure face tracking. means locate face image track throughout whole video sequence. tracking process often completed ﬁtting model frame data structure containing information face time. examples work showing face ﬁnding tracking example tracking methods active appearance models linear predictors discuss methods chapter second task centre figure visual feature extraction. using ﬁtted data parameters task extract features contain solely information pertaining speaker’s lips. third ﬁnal task right hand side figure classiﬁcation. train kind classiﬁcation model using visual features training data classiﬁers classify unseen test data. classiﬁcation produces output compared ground truth evaluate accuracy classiﬁers. literature methods feature extraction methods tracking faces images lip-reading. however date accepted method facto method extracting lip-reading features. lieu this zhou questions feature extraction speciﬁcally reading primarily cope speaker identity dependency visual data? also incorporate temporal information visual speech? intent second question capturing co-articulation eﬀects features. zhou categorise comprehensive range feature extraction techniques four groups image-based e.g. geometric-feature-based e.g. model-based e.g. categorisation serves show breadth current research features. however attention feature extraction address challenges machine lip-reading. improvements still made classiﬁcation stage lip-reading also. therefore much thesis focused classiﬁcation rather additional tasks tracking feature extraction. dismissive feature extraction tracking requirements rather wish focus work improve classiﬁcation methods. figure shows situation trying recreate text mind speaker. speaker articulates diﬀerently identity individual speaker signiﬁcant aﬀect eﬃcacy lipreading. visual signal also aﬀected speaker’s pose motion expression. cameras typically many parameters might aﬀect lipreading. these mention frame rate resolution highly probable signiﬁcant. motion pose expression frame rate resolution colour classiﬁer unit choice feature type classiﬁer technology multiple persons speaker identity rate speech table listed assessed number environmental aﬀects machine lip-reading. number factors diﬃcult control machine lip-reading. include limited lighting identity motion emotion expression. table attempt systematic study aﬀects. considering initially problem speakerdependent lip-reading three factors immediate interest resolution appear studied systematically unit choice feature type likely highly signiﬁcant performance. time-being speaker identity rate speech ignored since constant given speaker. choice feature studied quite well number ‘contests’ feature types conclusion state active appearance models highly likely give best known performance. features subject next chapter. however choice visual unit analogous quantity phoneme intriguing. phoneme smallest sound uttered viseme precisely deﬁned however working deﬁnition viseme phonemes identical appearance lips. therefore many phonemes fall viseme class many-to-one mapping. alternative deﬁnitions visemes viseme example seen repeatable visual gesture. alternative deﬁnitions explored visemes based upon articulatory gestures similar visual appearance. tentative conclusion visemes based upon articulatory gestures deﬁnition perform better. study looks recognition synthesis studies visemes considered ‘temporal units describe distinctive speech movements visual speech articulators’ many deﬁnitions choose from continue recognition working deﬁnition viseme group phonemes identical appearance lips’. thus study starts problems resolution systematically studied isolation observing eﬀects noise unit selection likely highly signiﬁcant. study items necessary discuss third aﬀect classiﬁcation highly sensitive feature selection. previous chapter asserted feature choice likely highly signiﬁcant. chapter therefore examine full processing chain detail tracking classiﬁcation dwelling methods special relevance thesis. linear predictors person-speciﬁc data-driven facial tracking method. devised primarily observing visual changes face speech make possible cope facial feature conﬁgurations present training data treating feature independently. speech means isolating lips eyes outline face etc. linear predictor part tracking mechanism. central point around support pixels used identify change position central point time. central point visually seen landmark outline feature. landmarks represent changing shape something morphing time. method shape pixel information surrounding linear predictor position intrinsically linked. single alone enough provide robust accurate tracking explains rigid ﬂocks selected grouped around central feature restrict motion within boundary reduce susceptibility noise. successfully used track objects motion tool face partially oﬀ-screen real-time tracking requires user guess real time appropriate simple task perform accuracy consistency tested rosetta raven dataset found features still outperformed features. active appearance model combined shape appearance trained model used tracking face throughout video sequence. model constructed small training subset type point distribution model used represent shape face varies speech. shape coordinates vertices make mesh training creates mean model permitting deviations within predetermined range variance. training co-ordinate vectors used model creation occluded landmarks omitted mean shape formation. normalised meshes built manually trained data translation scale rotation vector values landmarks upon face. principal component analysis provides eigenvectors independent shape model becomes meshes mean shape coeﬃcient shape parameters eigenvectors covariance matrix largest eigenvalues. assume orthonormal always perform linear reparameterisation landmarks chosen model sub-shapes within face outline hairline eyes nose lips. hand label training images. meshes constructed hand labelling normalised procrustes analysis apply pca. example full face shape model shown figure figure landmarks majority modelling inner outer contours. independent appearance uses appearance data base mesh allows linear variation shape whilst maintaining compact model. also denotes pixels inside base mesh. thus build statistical model. training image warped match mean shape identify shape-free area training image. shape-free area normalised linear transform texture model built eigen-analysis equation coeﬃcients appearance parameters base appearance appearance image eigenvectors covariance matrix. appearance plus combination images mean image eigenimages largest eigenvalues. demonstrated combination appearance shape models signiﬁcantly improves lip-reading performance work presented unless explicitly stated otherwise. combination model types requires single parameter represent relationship shape appearance. independent shape appearance aams shape parameters appearance parameters distinct. combined model parameters shown equation equation usage common parameter intrinsically ties models together warping image shape model represent appearance shape variation face. initialise shape parameters equation generate shape appearance parameters generate appearance instance built piecewise aﬃne warp base mesh shape finally using inverse compositional algorithm frames video sequence algorithm uses coordinate frame image coordinate frame aam. initiate best starting position ﬁrst image frame video sequence receives manually labelled shape iterating frame video turn backwards warp used warp image onto base mesh landmark positions converge place match corresponding pixels frames. movement frames lower frame rate tracking diﬃcult create greater variation frame images. discrete cosine transform technique converting signal elementary frequency components words transforms image spatial frequency domain separating image parts unequal importance. many variants lip-reading avsr authors d-dct applied twodimensional frame image throughout video. example create d-dct features co-eﬃcient vectors extracted information region interest image machine lip-reading lips. observed state features appearance parameters outperform feature types like sieve features eigen-lip features suggesting appearance informative shape. also pixel methods beneﬁt image normalisation remove shape aﬃne variation region interest method classiﬁed words rmav dataset recommended future creating classiﬁers viseme labels lipreading advises information inner mouth. prior training shape models temporal models dynamics required used. cope feature conﬁgurations present training data. multiple grouped ﬂocks robustness. face knowledge required training modelling. supervised. feature dependence improves tracking. ﬁtted model either solely shape model appearance model combined model shape appearance pixel related triangular section shape model. active appearance model built training data images. work presented thesis chose aams. whilst features outperform geometric features state outperform features. results suggest features outperform aams complete experiments initial aams performed poorly however authors also note aams good ones reasons could attributed either; modeling tracking errors. insuﬃcient training data eﬀects. first generalised enough training data classify test data secondly undertrained well tracking face. noted comparing features neti diﬀerent regions interest feature types. features work presented chapters particularly continuous speech experiments newer datasets conﬁdence aams state tracked well frames achieved using higher number landmarks rather hidden markov models used speech classiﬁcation time acoustic audio-visual visual-only classiﬁcation. channels speech considered time series i.e. produce data points causal manner. domains applied hmms sets temporal data handwriting sequences energy consumption. markov model made number states connected states. connection transition probabilities moving states connected nth-order markov chain inherent assumption state transitions dependent upon previous states. markov chain stochastic process output sequence states. practically speech classiﬁcation ﬁrst order model normally used. ﬁrst order state transitions dependent current state. probabilities possible actions time dependent upon state time value second stochastic process concerned emission probabilities. state associated probability density function used feature vectors determines emission probabilities particular feature vector output state state. whereas markov chain output sequence states means output feature vector. emission probabilities function state knowledge state hidden observer network hmms labelled representative unit. visual speech units referred visemes acoustic speech phoneme labels used. simple speech classiﬁcation tasks limited datasets words used unit label. additional hmms also built model silence start utterances shorter silence pauses words. work presented thesis hmms monophones. provides tools enable users build speech processing tools including recognisers estimators. main algorithm used estimation baum-welch algorithm algorithm used classiﬁcation viterbi algorithm book details background full current version full information implementation use. commonplace acoustic speech classiﬁcation current lip-reading literature using machine lip-reading allows easy replication results. achieved ubiquity generally high performance conﬁdent results close best achievable performance adopt similar strategies described previous works. explain types errors example. suppose ground truth utterance john wanted visit shop groceries. classiﬁers produce diﬀerent outputs. possible output john wanted visit groceries three words missing. ‘to’ ‘shop’ ‘buy’. instance deletion errors. another possible output john wanted visit visit shop groceries word ‘visit’ included twice. insertion error. finally achieved classiﬁer output john wanted shop shop groceries. word ‘shop’ identiﬁed word ‘visit’ substitution error. hcompv used start subject prototype determining number states mixtures. based upon data within whole dataset states equal. uses prototype deﬁnition training data initialises every local mean global mean across whole set. covariances updated. herest balm-welsh re-estimation using training fold samples transcription using labels. herest uses embedded training simultaneously updated hmms within systems using training data available within fold. particularly important syshhed permits tying together states within model allow fast transitions states shorter markov chains. particularly useful similar short models silence short pauses words. hvite commonly used forced alignment hmms using ground truth transcription also crucial classiﬁcation task. using trained hmms hvite attempts recognise test samples produces classiﬁcation output. hresults compares classiﬁcation output ground truth hresults provides statistics accurate recognisers been primarily correctness unit network level also includes model-level accuracy chapter summarises datasets used work presented throughout thesis. note thesis machine lip-reading audio-visual datasets commonplace since researchers often wish compare visual-only performance audio audio-visual performance purposes audio-visual integration summary common avsr databases presented table result values listed original presented papers referenced column results vary based upon speciﬁc experiments content classiﬁcation units original intent dataset. databases available non-english therefore considered here. work presented thesis rosetta raven database selected resolution robustness experiment chapter continuous structured speech. means good quantity data also speech constrained meaning task simpler av-timit better controlled experiment measure aﬀects single parameter. note austalk av-timit lvcsr proprietary thus freely available. tinuous speech datasets good phoneme coverage subject viseme mapping selected also good viseme coverage however smaller datasets including limited vocabularies quantity visemes risk inter-class skew. therefore preliminary experiments later chapters undertaken ﬁrst avletters proof concept conﬁrmation hypotheses sound repeating experiments rmav. rmav sentences selected resource management data ensures good phoneme coverage content. rmav selected extracted features available enabled focusing classiﬁcation task rather tracking extracting features. accommodate breadth possible pronunciations number dictionaries available machine lip-reading. dictionaries words phoneme sequences subject pronunciation habits speaker. described here ﬁrstly used conjunction rosetta raven data secondly beep used later chapters avletters rmav. carnegie mellon university north american pronunciation dictionary known uses phonemes also encodes whether vowels carry levels lexical stress either -none -primary -secondary. lexical stress relative emphasis placed upon certain syllables within word. including lexical stress representations dictionary phonemes. containing words based arpabet symbol developed speech recognition uses. dictionary used american speakers speaking english i.e. american english. cambridge university british english pronunciation dictionary known beep phonemes mapped words allowing duplicate pronunciations word. example word ‘read’ phonetically ‘/r/ /eh/ /d/’ read book last night’ ‘/r/ /d/’ like read’. dictionary used british speakers english. avletters version avletters dataset single word dataset four british english speakers reciting letters alphabet seven times. present quantity visemes data stage dependent upon viseme used speakers dataset seen figure videos frames table describes features extracted videos. features derived tracking full-face active appearance model throughout video extracting features containing area. therefore contain information representing speaker’s lips none rest face. speakers similar number parameters contained features. combined features concatenation shape appearance features features retain variance facial shape appearance information. table number parameters shape appearance combined shape appearance features speaker avletters dataset speaker. features retain variance facial information. dataset recorded ucla january eamon keogh formulated attempt provide standardised audio-visual machine learning problem comprises four videos consist north american untrained speakers reciting e.a.poe’s ‘the raven’. poem published linguistic content raven make interesting dataset narrative uses stylised language including internal rhyme alliteration. poem described generally trochaic octameter trochaic octameter rarely used meter poetry. within line trochaic octametric poem eight trochaic metrical feet. eight feet consist syllables ﬁrst stressed latter unstressed giving rise down’ eﬀect professional recitation. pairing stressed unstressed syllable trochaic however appear followed speakers dataset. linguistic terms videos phonemes present minor noted variation occurrences video phonemes namely /uw/ /au/ /ae/ /eh/ /ey/ less instances within whole data set. phonemes lexical stress shown numbers naming convention comes american english phonemes used pronunciation dictionary. again quantify viseme counts dataset varies viseme used particular experiment. data used machine lip-reading system need extract features. training images speaker video used together make single model tracking rest video. full face used track face robust ﬁtting whereas lip-only used extract lip-only feature. features retained speakers face shape appearance variance throughout video used resolution work described chapter assessing contribution individual visemes within chapter table number parameters shape appearance combined shape appearance features rosetta raven dataset speakers. features retain variance facial information. formerly known lilir rmav dataset consists british english speakers utterances speaker resource management context independent sentences totals around words each. noted sentences selected rmav speakers signiﬁcantly version full dataset transcripts. selected phonetician maintain much coverage phonemes possible. original videos recorded high deﬁnition full-frontal position. individual speakers tracked using table number parameters shape appearance combined shape appearance features rmav dataset speakers. features retain variance facial information. chapter identiﬁed number factors aﬀects machine lipreading often diﬃcult control lighting pose identity motion emotion linguistic content expression. address challenges turn. ability recognise gestures throughout video addressed tracking part lip-reading task. systems commonly used tracking faces videos machine lip-reading. systems active appearance models linear predictors systems eﬀective even quality videos tracking motion face speech. chapter described systems full. methods make assumptions motion within videos locally aﬃne whereas aams globally aﬃne. therefore minor issue remains non-aﬃne transformations. literature eﬀects pose computer lip-reading. look expression recognition human computer interactions present improvement expression recognition computers humans pose rotated others kumar kaucic look visual speech classiﬁcation suggest proﬁle view gives better classiﬁcation. however also processed visual features longer time period duration marked endpoints speech utterance consider co-articulation within tests isolate longer time window pose improved classiﬁcation. considering lip-reading study examines eﬀects human sentence perception across three viewing angles relation camera position full-frontal view angled view side view performance female adult post-lingual hearing loss measured accuracy angle. study used single-subject alternating treatment design three treatment angles randomly presented every session. accuracy session compared determine eﬀective viewing angle speaker. results indicated side-view angle eﬀective percentage gain improvement greatest combination consistent upward trend data points across treatment sessions. performance frontal-view angled-view angles also successful signiﬁcantly full-frontal. results preliminary eﬀort indicate value treatment visual sentence perception three angles including non-traditionally targeted side view human lip-reading. preliminary studies non-frontal pose aﬀects lip-reading found small vocabulary used order simplify recognition task measuring eﬀects features extracted non-frontal camera positions. classiﬁers trained frontal features tested non-frontal features results showed greater oﬀfrontal angle became word error rate increased. however frontal view features provided inferior recognition oﬀ-angle features distinction studies visual noise image backgrounds original videos. avsr databases recorded face frontal alternative idea lipreading non-frontal camera angles frontal-trained classiﬁers using mapping recorded angle estimated actual angle speaker camera presented work dataset recorded speciﬁcally mapping technique results support observations observation larger oﬀ-camera angles smaller feature vector higher order features preferable. studies aﬀect pose machine lip-reading taken lucey proprietary dataset. authors undertake three activities small vocabulary speakers; comparing frontal proﬁle view lip-reading performance also take challenge experimenting concatenating frontal proﬁle view features multi-view features attempting lip-read using single pose-invariant normalisation method. results task support seen whereby frontal features outperformed proﬁle features. considered datasets recorded controlled conditions minimal noise. results multi-view features marginally better frontal signiﬁcantly better proﬁle features. w.e.r reduces proﬁle features frontal features best multi-view features achieved w.e.r. achieved simply concatenating finally third test lucey develop single pose-invariant model lip-reading regardless pose test data. compare diﬀerent pairings features training/testing split. example using frontal features training testing frontal features. using features test proﬁle features vice versa. third training model using split included experiment setup. also adopted projection features alternative feature space features alternative testing data three training options tests showed best recognition training test features matched. didn’t match w.e.r dramatically increased example train/test pairing w.e.r train/test pair achieves w.e.r however authors also show mitigated projection test proﬁle data back frontal feature space train/test split recovers w.e.r back transformation principle also used presented view-independent lip-reading system. investigation uses continuous speech corpus compared small vocabulary dataset later study acknowledges human lip-readers preference non-frontal view suggests could attributed protrusion. diﬀerent approach feature transform presented development system shows computer lip-reading independent speaker pose. challenge machine lip-reading video person meaning track faces number solutions. demonstrates multiple person tracking also implemented simple system. also person re-identiﬁed videos either second view space alternate perspective person moves location. example speaker identiﬁcation method detailed detail lip-reading multiple people recognises consonants visual vowels. whilst none papers directly tested concurrent speech would interesting know eﬀect speakers talking unison would cause upon current lip-reading systems. presents audio-visual system automatically detects talking person using video audio data single microphone. visual-only classiﬁers improved robust visual-only system machine lip-reading still needs developed classiﬁers essential part system. studies eﬀect video frame-rate human speech intelligibility video communications suggest lower frame rates encourage humans over-articulate compensate reduced visual information available akin visual lombard eﬀect. therefore asked computer need information lip-read speaker recorded video sequence? study observes face-to-face human interactions articulation relaxed. could instance computer needs extra visual information throughout recording much lack visual information impact classiﬁcation performance? lack video recording quality aﬀect classiﬁcation? another study frame rate computer lip-reading tells greatest classiﬁcation achieved frame rate used training testing data. perhaps unsurprising shown training test data sets frame rates classiﬁcation drops frame rate training data lower test data. show longer words easier classify. would interesting visemes. also shows dependency frame rates classiﬁcation accuracy speaker. training test data same similar frame rates recommended training data higher frame rate test data. observes word classiﬁcation rates vary non-linear fashion frame rate reduced caused particular words recognised. duration utterance eﬀect classiﬁcation rate paper. people diﬀerent speaking styles accents rates speech. people talk fast slow talk side mouth others naturally over-articulate others facial hair occludes visibility movement speech. rate speech alters utterance duration articulator positions. therefore sounds produced particularly visible appearance altered. authors present experiment measures eﬀect speech rate shows eﬀect signiﬁcantly higher visual speech acoustic. variable people undertake elocution classes myriad reasons. examples include call centre employees undertaking ‘accent neutralisation’ courses make approachable target customers supported state speakers nonprestige dialects countries take elocution courses respond newschapter reviewed environmental aﬀects lip-reading classiﬁcation. whilst many controlled seen literature eﬀects managed also note previously considered challenges outdoor video poor lighting agile motion overcome regards studies aﬀects resolution limited literature found time writing examines this. experiments touch area interest investigations recognition noisy images. investigation eﬀects compression artefacts visual noise localisation errors training presented authors undertake experiments ﬁrst includes attention spatial resolution inclusion features three diﬀerent resolutions interesting resolutions selected diﬀering aspect ratios controlled method resolution variation. also eﬀect spatial resolution measured presented rather included property tests frame rate contrast. neither papers consider simple removal information smaller image compared larger one. therefore testing necessary given that point known speaker reduced linguistic context classiﬁcation rates high fair sensitivity found parameters associated left hand side figure nevertheless surprisingly little attention paid systematic review cameras parameters. therefore ﬁrst practical experiment ‘what lowest resolution machine lip-read?’. discussed machine lip-reading depends factors diﬃcult control lighting identity motion pose rate speech expression factors video resolution controllable. surprising speciﬁc systematic complete study eﬀect resolution lipreading non-noisy conditions. tendency without evidence assume high resolution video produce better classiﬁcation results study measure eﬀect resolution classiﬁcation needed undertaken chapter. work rosetta raven dataset already described section feature extraction however undertake image preprocessing. four videos dataset converted images ﬀmpeg using image encoding build initial active appearance model tracking video select ﬁrst frame nine others randomly. frames hand-labelled model face including facial outline eyebrows eyes nose lips. track face preliminary ﬁtted inverse composition ﬁtting unlabelled frames figure show speaker tracked full-face mesh ﬁrst sentence raven once upon midnight dreary used tracking speaker face. eyes eyebrows nose face outline lips. purpose allow obtain robust full face model extract features information classiﬁcation. speaker lips sub-model seen figure landmarks outer contour inner contour. next video frames used high-resolution tracking down-sampled required resolutions nearest neighbour sampling upsampled bilinear sampling provide sets frames original video. diﬀerent sampling method upsample provided consistent visual degradation information resulting images show reduction resolution minimum consistent processing artefacts compared sampling methods. frames remind reader point interest study aﬀect resolution loss lip-reading information rather aﬀect would also tracking process. trackers lose track quite easily resolutions lossy images wish overwhelmed catastrophic errors caused tracking issues artefacts often solved ways accordingly ﬁtted original full resolution reﬁtting lips model feature extraction. consequently shape features experiment figure downsampling frame images format original captured images nearest neighbour down-sampled images bilinear sampled restored pictures without original high deﬁnition information. image processing method speciﬁc research question limitations resolution achieving machine reading? minimised eﬀects compression artefacts using successful pair algorithms downsampling upsampling respectively. using dataset recorded laboratory controlled conditions white noise occlusions. course methods available simply ﬁlling feature vectors zeros represent loss data resizing smaller images back original size. major advantage method encourages good tracking good tracking complete direct comparison classiﬁcation outputs features derived videos varying resolution information. speaker shape appearance parameters speaker seven shape appearance parameters retained. number parameters chosen retain variance facial information usual table presented chapter listened recitation poem produced ground truth text word transcript converted american english phoneme-level transcript using pronunciation dictionary introduced chapter then using viseme mapping based upon walden’s consonants montgomery al.’s vowel phoneme-to-viseme mapping viseme transcript created. thus translated recitation words phonemes ﬁnally visemes. viseme classiﬁcation selected phonemes small data beneﬁts reducing number classiﬁers needed increasing training data available viseme classiﬁer. note visemes equally represented data shown viseme histogram figure chapter whist volumes figure lower equivalent histogram continuous speech dataset distributions similar. speaker test fold randomly selected lines poem. remaining lines used training fold. repeating times gives ﬁve-fold cross-validation. note visemes cannot equally represented folds. figure occurrence frequency visemes speaker based upon ground truth transcripts rosetta raven dataset speakers using walden’s montgomery’s visemes. classiﬁcation hidden markov models built hidden markov toolkit already introduced section initialised using ‘ﬂat start’ method prototype states mixture components information training samples. five states mixtures selected based upon work deﬁned viseme plus silence short-pause labels re-estimate parameters four times pruning. tool hhed ties together short-pause silence models between states three re-estimating hmms times. hvite used force-align data using word transcript. create viseme version dictionary word-to-viseme mapping viseme dictionary produce time-aligned viseme transcription includes natural breakpoints words. hmms re-estimated twice more. however forcealigned viseme transcript replaces original viseme transcript used previous re-estimations. word network needed complete classiﬁcation. hlstats hbuild used together twice make unigram word-level network bigram word-level network finally hvite used diﬀerent network support classiﬁcation task hresults gives correctness accuracy values. accuracy selected measure rather correctness since accounts errors. including insertion errors important notoriously common lip-reading. insertion error occurs recogniser output extra words/visemes present original transcript example could once upon midnight dreary figures plotted diﬀerent resolutions along x-axis mean viseme correctness y-axis speaker. supported unigram language network bigram language network respectively. speaker shape classiﬁcation shown blue appearance classiﬁcation black. speaker shape appearance classiﬁcation plotted green respectively. corresponding graphs mean accuracy classiﬁcation shown figures four ﬁgures include standard error folds. figure plots viseme accuracy unigram network y-axis points negative values. worse chance demonstrates debilitating eﬀect insertion errors language network strong enough sieve classiﬁcation output. viseme correctness supported unigram word network shown figure slow signiﬁcant decrease classiﬁcation resolutions decrease size along x-axis. point appearance features drop shape features. trend matched experiments figures figures however normalised account actual diﬀerences information resolutions. list resolutions section equal interval size. therefore replot results measuring resting lip-pixels cover lip-shape. resting pixel distance shown figure pixels rest position whereas lip-height approximately pixels. worst performance speaker using shape-only features. shape features vary resolution variation curve cross-fold validation error nevertheless variation within standard error signiﬁant. surprise shape features scale invariant. poor performance usual lip-reading dominated insertion errors usual explanation eﬀect shape data contain characteristic shapes indistinct shapes easier classiﬁer insert garbage symbols learn duration symbol indistinct start shape co-articulation. suggest speaker distinctive shapes scores better shape feature distinctive shapes classiﬁcation models diﬀerentiate deﬁnitively. however appearance features interest since varies downsample. resolutions lower four pixels diﬃcult conﬁdent shape information eﬀective. however basic problem high error rate therefore supportive word model required figures shows classiﬁcation accuracy versus resolution bwn. also includes sub-plots magnify right-most part graph. again shape models perform worse appearance models looking magniﬁed plots appearance never becomes poor shape performance even resolutions. accuracies clear inﬂection point around four pixels pixels performance declined signiﬁcantly. table insertion deletion substitution error counts classiﬁcation transcripts smallest resolution largest resolution minimum required pixel height pixels lip. values total folds cross validation. substitutions) occur classiﬁcation resolutions identiﬁed minimum pixel threshold well after. values total errors folds cross validation. speaker deletion substitution errors increase longer enough pixels diﬀerentiate lips. speaker substitution errors increase deletion errors decrease insigniﬁcantly using rosetta raven data shown lip-reading classiﬁers threshold eﬀect resolution. trained tested viseme classiﬁers measured eﬀect classiﬁcation accuracy systematically reduced resolution information video. best recognition achieved accuracy speaker appearance data bigram word level language model ﬁrst time dataset used baseline future uses. contrary common assumption practice unexpected observation remarkable resilience resolution machine lip-reading. given modern experiments lip-reading usually take place high-resolution video example) disparity measured performance assumed performance remarkable. results show successful lip-reading needs minimum four pixels realisation minimum number pixels piece information area machine lip-reading. previous research area focused noisy images eﬀect noise word error rates audio-visual speech recognition system. experiments corroborating results support premise less information lip-reading negatively aﬀected also lower bound resolution essential good lip-reading. must forgotten higher resolution video beneﬁcial tracking task previous work demonstrates factors considered negatively eﬀect lip-reading classiﬁcation oﬀ-axis views actually ability improve performance lower resolution video detrimental ﬁrst assumed. therefore conclude that real situations limitations lip-reading likely come factors environment. rather poor performance lip-reading almost certainly limitations signal lip-signal challenging decode needed better understanding visual signal components learnt. reason turn problem understanding visemes. chapter ﬁrst investigation understanding visemes. undertake complicated experiments attempt re-design augment visemes useful understand already tested. currently always whole visemes include large number phonemes. would nice know therefore chapter describes investigation diﬀerence contribution accuracy viseme within set. analysis confusion matrices produced viseme classiﬁcation obtained comparing classiﬁcation output ground truth transcript time-aligned provides measurements viseme contributions classiﬁcation. enables compare viseme within others determine contributes accurate machine lip-reading. additionally balance shape appearance viseme probabilities reviewed type feature contributes classiﬁcation. also compare visual classiﬁcation audio using viseme classiﬁer labels audio features demonstrates relationship viseme classiﬁcation accuracy spread individual viseme contribution classiﬁcation. study continues rosetta raven features extracted section short datasets these provide adequate training examples visemes. group untrainable visemes single garbage viseme. case estimate samples minimum threshold mitigate bias caused variation training samples classiﬁer. thus visemes grouped giving table already reviewed original dataset chapter figure shows occurrence visemes listed original phoneme-to-viseme x-axis probability correct classiﬁcation viseme trained appearance model y-axis probability correct classiﬁcation viseme trained shape model. results speaker blue speaker visemes plotted rank always match speaker. example second position speaker whereas speaker /v/. ranked visemes listed table ﬁfth useful viseme gives superior classiﬁcation speakers. conventional wisdom appearance features give best results studio-type conditions good tracking whereas shape features robust appearance. note right-hand point visual silence viseme speaker speaker general visual silence quite variable compared audio silence speakers breathe show emotion. however x-axis varies performance best performing viseme left hand side visual shape appearance features silence features. next best viseme varies either /v/. phonetically indistinct viseme appears ﬁller viseme. observed human lip-reading reliable visual cues humans combined rich contextual information interpret ‘ﬁll gaps’ speaker saying therefore hypothesis robust audio classiﬁcation based upon large spread recognised phonemes resilience classiﬁcation number phonemes contributing accuracy. visually human lip-readers anticipated fewer visemes would perform equivalent classiﬁcation figure greater decline left right visemes visual features audio speakers. additionally error bars position viseme increase speaker provides evidence support hypothesis audio classiﬁcation spread visemes correct. visemes i.e. identical shape-only ﬁrst positions. error bars increase data available makes classiﬁcation unreliable less well trained classiﬁers. means estimated threshold minimum training samples classiﬁer high enough. impact reduced /gar/ viseme note figure similarities performing visemes training samples. table lists mean ranking visemes speakers shape models visemes tested mapping speaker’s appearance models. table also gives mean viseme ranks speaker speakers models. rankings similar pairings. tables summarise similarities feature types speakers using spearman rank correlation ranked viseme outputs. signiﬁcant threshold underlined. conﬁrms strong relation shape-only appearance-only classiﬁcation. conditions appearance features outperform shape real world conditions shape information robust absence non-noisy appearance data strong coupling previous work shows modes information complimentary recommend both without forgetting real world artefacts motion blur signiﬁcantly deteriorate appearance information. also note speaker audio ranking similar video ranking although previously noticed rapid drop-oﬀ video. summarise chapter shown that assumption classiﬁers trained suﬃcient data order single viseme performances fairly consistent across feature modes audio shape appearance. also noted visual classiﬁers depend highly select visemes performing well audio classiﬁers. observation fragile machine lip-reading re-enforced work. critical seven visemes cannot built suﬃciently trained classiﬁers lip-reading impossible. human trained lip-read many follow method recognising small number gestures process using sophisticated knowledge language context create classiﬁcation output transcript acknowledge work ﬁeld focuses improving mean accuracies visemes conceal real source overall performance. system achieves mean viseme accuracy contains supremely accurate viseme classiﬁers maybe system large number classiﬁers achieve modest performance. work seen correlation spread viseme contributions classiﬁcation viseme classiﬁcation performance higher classiﬁcation achieved equally useful visemes rather visemes usefulness ranges poor excellent. chapter therefore suggests diﬀerent strategies improving future lip-reading systems; option makes select best viseme classiﬁers better option focuses upon improving worst stage contribute all. comment time approach likely successful observations allow future work focus attention likely good. work suggests visemes largely responsible accurate classiﬁcation whereas appearance seven visemes audio least ten. means appears fewer recognizable shapes distinguishable appearances turn sounds. relates overall viseme classiﬁcation audio results better appearance turn better shape. suggest good threshold viseme training samples less standard error away mean number training samples visemes set. stricter threshold used ensure bias towards particular viseme class could dominate classiﬁcation accuracy set. deeper understanding visemes individual capabilities move onto investigating relate phonemes acoustic units speech. reminded viseme working deﬁnition viseme visual equivalent phoneme move review number phoneme-to-viseme mappings presented literature order assess optimal machine lip-reading. computer lip-reading literature debate mapping phonemes visemes. chapter avletters dataset used train test classiﬁers using phoneme-to-viseme mappings effect word classiﬁcation accuracy measured. chapter also presents tests data-driven method devising speaker-dependent phoneme viseme maps using phoneme confusions. method inﬂuenced perception bias since confusions based machine observations human perception. compare word classiﬁcation achieved maps best performing previously published phonemeto-viseme mapping. demonstrate whilst diﬀerences viseme previously suggested best mapping speakers mapping used benchmark compare performance data-derived speaker-dependent visemes. summary published maps provided tables list exhaustive mappings vary focus consonants speaker-dependent ordering useful starting points purpose study would like phoneme-to-viseme mappings include phonemes transcript dataset accurately reﬂect range phonemes used full vocabulary. therefore mappings used pairing mappings suggested literature e.g. vowels consonants. full list mappings used tables total consonant maps eight vowel maps identiﬁed paired provide maps test. questions are; conventional machine lip-reading system correct viseme mappings machine lip-reading? possible method selecting better phoneme-to-viseme mappings? fisher’s full phoneme viseme mappings classes found tables diﬀerences classiﬁcations based around diﬀerent groupings phonemes literature know number recent attempts compare these part following list reasons given discrepancies classiﬁer sets. context speech presented context inﬂuence consonants appear lips. real tasks context enable easier distinction indistinguishable phonemes syllable tests. clustering criteria grouping methods vary authors. example ‘phonemes said belong viseme clustered percent correct identiﬁcation viseme threshold typically correct. stricter grouping criterion higher threshold visemes identiﬁed.’. avletters dataset used train test classiﬁers based upon mappings. features used known outperform feature methods machine lip-reading tables show phonemes original map. utterance short data need implement within features address co-articulation. table described sources derivation methods phoneme-to-viseme maps used comparison study. majority constructed using human perception testing test subjects e.g. finn used kricos data-driven methods recent e.g. lee’s visemes presented hazen’s remaining visemes based around linguistic/phonemic rules. example clustering method hazen involved bottom-up clustering using maximum bhattacharyya distances measure similarity between phoneme-labelled models. models represented gaussian distributions. clustering phonemes manually merged /em/ /en/ /s/. viseme phoneme sets {/ei/ {/ei/ /æ/} {//} {/i/ /y/} {/au/} {/o/ /oi/ /u/} {/u/ /w/} {/u/ /h/} {/e/ /ai/ {/u/} {/u/ /o/} {/au/ /oi/} /a/} {/æ/ /ai/ /ei/} /i/} {/a/ /ai/ /ei/ /i/} {/oi/ /o/} {/au/} /u/} {/i/ /i/} {/e/ /ei/ /æ/} {/a/ /au/ /ai/ {/o/ /oi/ /u/} {/u/ /u/} {/i/ /i/} {/e/ /ei/ /ai/} {/a/ {/u/ //}{/oi/} {/i/ /hh/} {/au/ /u/} {/u/ /u/} {/o/ /oi/ /au/ /h/} {/u/ /u/} {/æ/ /ei/ /ai/} {/i/ {/uw/} {/u/ /u/} {/au/} {/i/ /ay/} {//} {/iy/ /æ/} {/e/ /i/} {/u/} /ei/} details volume training samples available. note include phonemes british english phonetic alphabet known problem visual speech research limitation lack suﬃciently large datasets available motivates drive better mappings potentially avoid need associated cost obtaining large audio-visual speech datasets. introduces confusion machine lip-reading. attempt measure level confusion simple ratio metric proportion phonemes visemes shown compression factor visemes number visemes number phonemes. compression factors maps described table ideal ratio phoneme viseme mapping would mean identifying phoneme uniquely. however still need cluster phonemes lack visual distinction phonemes. thus higher compression factor better means less dependency upon language network decoding visemes back phonemes. silence garbage visemes included cfs. deliberate omission following phonemes mappings required /si/ /axr/ /en/ /el/ /em/ /axr/ /em/ /epi/ /tcl/ /dcl/ /en/ /gcl/ kcl/) /axr/ /em/ /el/ /nx/ /en/ /dx/ /eng/ /ux/ american diacritics appropriate british english phonetic dataset. moreover kricos provides speaker-dependent visemes generalised viseme phoneme sets {/p/ /m/} {/f/ /v/} {/t/ /d/} {/s/ /z/} {/k/ /g/} {/w/} {/r/} {/l/ /n/} {/t/ /z/} {/g/ /n/} {/l/ /t/} {/s/ /z/} {/ts/ /dz/ /z/} {/t/ /d/} {/r/} {/f/ /v/} {/p/ /m/} {/p/ /m/} {/w/} {/f/ /v/} {/t/} {/l/} {/d/ /n/} {/s/ /ts/ /j/} {/y/ /n/} {/p/ /m/} {/t/ /d/} {/w/ /s/} {/k/ /g/} {/s/ /ts/ /j/} {/y/} {/z/} {/f/} {/v/} {/t/ /r/} {/k/ /m/} {/p/ /b/} {/f/ /v/} {/s/ /dz/ /ts/} {/t/ /l/} {/p/ /m/} {/f/} {/r/ /w/} {/s/ /dz/ /ts/} {/l/} {/r/} {/y/} {/b/ /p/} {/s/ /h/} {/ts/ /dz/ /z/} {/t/ /k/} {/n/} {/f/ /v/} {/p/ /m/} {/f/ /v/} {/k/ /g/} {/s/ /ts/ /dz/} {/t/} {/n/ /d/} {/l/} {/r/} {/f/ /v/} {/r/ /w/} {/p/ /m/} {/t/ /d/} {/ts/ /dz/ /z/} {/s/ /z/} {/d/ /t/} {/g/ /n/} {/p/ /m/} {/f/ /v/} {/w/ /r/} {/t/ /z/} {/k/ /g/} {/l/} {/t/ /d/} {/s/ /ts/ /dz/} {/d/ /d/} {/g/ /h/} {/dz/ /ts/ /z/} {/p/ /m/} {/f/ /v/} {/r/ /w/} {/l/ /y/} {/s/ /z/} {/t/ /n/} {/s/ /dz/ /ts/} {/p/ /m/} {/t/ /d/} {/f/ /v/} {/n/ /w/} {/p/ /m/} {/f/ /v/} {/w/ /w/} {/r/} {/s/ /z/} {/s/ /ts/ /j/} {/t/} {/l/} {/k/ /n/} {/h/} {/t/ /n/} {/y/} {/p/ /m/} {/f/ /v/} /d/} {/s/ /z/} {/w/} {/s/ /z/} {/r/} {/l/} {/t/ /j/} {/p/ /m/} {/f/ /v/} /w/} {/t/ /ts/ /dz/ /h/} observations montgomerys visemes multiple-choice intelligibility test confusions among sounds produced similar articulatory positions bottom-up clustering confusions post-training sensory cognitive correlates hierarchical clustering human testing merging fisher visemes unknown data-driven linguistics decision tree clusters human observations human observation tests using common mixtures phonemes method reproducible. viseme include phonemes present ground truth transcript grouped garbage viseme measure performance viseme sets previously prescribed literature. note phonemes dataset mapping includes phonemes vocabulary. method speaker-dependent classiﬁcation tests combined shape appearance features uses classiﬁers built features selected dataset described chapter videos tracked full-face features extracted consist information. classiﬁers based upon viseme labels within map. ground truth measuring correct classiﬁcation viseme transcription produced using beep british english pronunciation dictionary word transcription. classiﬁcation output viseme level script mapped sentence level classiﬁcation. working british english phonetic transcript converted viseme transcript assuming visemes mapping tested test using leave-one-out seven-fold cross validation. seven folds selected seven utterances alphabet speaker avl. hmms initialised using ‘ﬂat start’ training re-estimated eight times force-aligned using htk’s hvite. training completed re-estimating hmms three times force-aligned transcript. section classiﬁcation performance hmms measured correctness insertion errors consider acknowledged word classiﬁcation high performing viseme classiﬁcation. however viseme tested diﬀerent number phonemes visemes common comparator words used compare diﬀerent viseme sets. diﬀerence rather individual performance interest investigation. word level correctness rather viseme level correctness normalises sets fair comparison. compare values accuracy literature namely speaker-dependent results signiﬁcantly higher values achieved. however paper experiments designed measure eﬃcacy multi-speaker classiﬁers thus authors permitted diﬀerent parameters avletters data achieves unsupervised random forest classiﬁcation technique. performs results here. however unsupervised method inhibits option knowing visual units used forest. priority comparison study measure eﬀects viseme selection rather optimising classiﬁcation method individual speaker bare cost overall classiﬁcation learning gained observations comparing viseme sets. figures show word correctness percentage aggregated respective heat maps phoneme-to-viseme maps figures figure shows consonant maps along x-axis consonant pairing vowel plotted respective consonant position x-axis. shows diﬀerences consonant eﬀect vowel maps consonant map. figure vice versa. black line mean word comparing consonant maps figure shows disney vowels signiﬁcantly worse others paired consonant maps. vowels overlap majority error bars suggesting little signiﬁcant diﬀerence whole group although bozkurt vowels consistently mean upper error disney jeﬀers hazen vowels. comparing vowel maps figure hazen best consonants margin mean whereas woodward franks bottom performers. figures show performance viseme maps averaged across speakers signiﬁcant diﬀerence ‘best’ visemes individual speakers arises unique everyone articulates speech. figures critical diﬀerence plots viseme class sets based upon classiﬁcation performance critical diﬀerence measure conﬁdence intervals diﬀerent machine learning algorithms. assumptions within critical diﬀerence measured results ‘reliable’ algorithms evaluated using random samples standard metrics results consistent random sampling across folds assumptions concern. selected critical diﬀerences evaluate performance multiple classiﬁers previous studies consider applicability statistics tested dataset figure comparison vowel labelled viseme sets. starting left-hand side ﬁgure shows nichie montgomery hazen disney vowels critically diﬀerent signiﬁed black horizontal crossing respective lines left side ﬁgure. likewise montgomery hazen disney jeﬀers vowels also critically diﬀerent other. bars alone demonstrate nichie’s vowels critically diﬀerent jeﬀers neti bozkurt’s. right hand side graph bozkurt’s vowels critically diﬀerent neti’s vowels. interesting figure appear perform signiﬁcantly diﬀerently vowel visemes. fact whilst bozkurt nichie vowels critically diﬀerent other adjacent classiﬁcation performance. gives hope optimal visemes possible eﬀect clusters phonemes varies speciﬁc phonemes clustered. figures demonstrate signiﬁcant diﬀerence subsets viseme sets based upon insigniﬁcant variation within sub-set. suggests could dependency viseme sets groupings align derivation method mappings. mean word classiﬁcation speakers folds plotted figures looking confusion factors best performing speaker suggests good preparation phonemes visemes ideally around approximately phonemes viseme. also lee. highest performing word classiﬁcation consonants vowels displayed figure interestingly highest number visemes second part phoneme-to-viseme mapping study three approaches used better method mapping phonemes visemes. ﬁrst approach uses common pairs phonemes existing mappings. regularly grouped together visemes popular phoneme-subgroups high occurrence across sets. ﬁrst approach uses number occurrences size subgroup weighting grouping together phonemes i.e. highest weighted phonemesubgroup grouped viseme ﬁrst without duplicating phonemes viseme. maps used clustering process devised diﬀerent reasons helps unfigure previously presented phoneme-to-viseme maps include vowel consonant phonemes word correctness plotted count visemes phoneme-to-viseme map. mappings phoneme allocated viseme class. maps tested dataset using classiﬁcation method described section results best performing comparison study benchmark measure improvements. ﬁrst approach ﬁnding speaker-independent uses commonly coupled phonemes build visemes. detail visemes previous maps searched make full dictionary unique pairs phonemes. associated dictionary entry count many times appear deﬁned comparison study section htk. phoneme pair list sorted descending occurrence count. passing list next phoneme pair assigned viseme class based upon matching phonemes phoneme permitted added viseme. priority given pairings higher count. particular phoneme never coupled phonemes phoneme forms unique viseme own. {/d/ /t/} {/b/ /p/} {/g/ /y/} {/f/ /v/} {/o/ /o/} {/e/ /i/} {/ts/ /dz/ /z/} {/a/ /ao} {/s/ /z/} {/dh/ /t/} {/r/ /w/} {/æ/ /ei/ /i/} {/a/ /ai/ /ai/ /i/} {/au/ /u/} second third approaches identifying visemes speaker-dependent data-driven based phoneme confusions within classiﬁer. ﬁrst undertaking work complete classiﬁcation using phoneme labelled classiﬁers. classiﬁers built ﬂat-started hmms force aligned training data speaker. hmms re-estimated times total seven folds leave-one-out cross validation. overall classiﬁcation task perform well particularly isolated word dataset. however tool hresults used output confusion matrix fold detailing phoneme labels confuse others often. data-driven speaker-dependent approaches ﬁrst step completing phoneme classiﬁcation essential create data derive maps from. ‘strictly-confused’ viseme second step deriving check single-phoneme visemes. phonemes correctly recognised themselves false positive/negative classiﬁcations permitted single phoneme visemes. figure highlighted true positive classiﬁcations false positives false negative classiﬁcations blue shows phoneme ‘single-phoneme viseme’ deﬁnition. true positive value zero false classiﬁcations. figure demonstration confusion matrix showing confusions phoneme-labelled classiﬁers used clustering create speaker-dependent visemes. true positive classiﬁcations shown confusions either false positives false negatives shown blue. estimated classes listed horizontally real classes vertical. action followed deﬁning combinations remaining phonemes grouped visemes identifying grouping contains largest number confusions ordering viseme possibilities descending size {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} {/p/ /p/} grouping rule states phonemes grouped viseme class phonemes within candidate group mutually confusable. means pair phonemes within viseme must total false positive false negative classiﬁcation greater zero. phoneme assigned viseme class longer considered grouping possible phoneme combinations include viseme discarded. ensures phonemes belong single viseme. next iteration clustering algorithm identiﬁes combination remaining phonemes correspond next largest number confusions phonemes merged. leaves ﬁnal visemes table original phoneme classiﬁcation produced confusion matrices permit confusions vowel consonant phonemes. section previously presented maps vowel consonant phonemes commonly mixed within visemes. therefore make types maps permits vowels consonant phonemes mixed within viseme second restricts visemes vowel consonant putting extra condition checking confusions greater zero. remembered phonemes present ground truth transcripts recognised included phoneme confusion matrix. remaining phonemes assigned viseme grouped single garbage /gar/ viseme. approach ensures phonemes confused grouped viseme lose ‘rarer’ less common visual phonemes. example /ea/ /oh/ /ao/ original transcript placed /gar/. speaker /gar/ also contains /ay/ speaker /gar/ also contains show speaker’s phoneme classiﬁcation outputs. task undertaken four speakers dataset. ﬁnal maps shown table /ai/ /u/} {/b/ /ei/ {/d/ /s/} {/ts/ /l/} /v/} {/w/} {/f/} {/k/} /v/} {/dz/ /z/} {/a/ /u/} {/t/} /ai/ /ei/ /s/} {/e/ /y/} {/l/ /n/} {/b/ /p/} {/z/} {ts/} {/t/} {/a/} {/dz/ /k/} /f/} {/u/ /u/} {/ei/ /n/} {/d/ /p/} {/b/ /s/} {/l/ /m/} /e/} {/i/} {/u/} {/a/} {/dz/} {/u/} {/z/} {/y/} {/ts}/ {/ai/} {//} {/a/} {/dz/} {/u/} {/k/ /w/} {/v/} {/z/} /ai/ /ei/ {/m/ /n/} /p/} {/k/ /w/} {/d/ /s/} {/dz/ /t/} {/f/} {/v/} {/a/} {/z/} {/ts/} {/b/} {/u/} {/u/} {/l/} {/u/} {/b/} /u/} {/a/ /ei/} /ei/} {/d/ {/ts/ {/k/} {/z/} {/w/} {/f/} {/m/ /n/} {/dz/ /v/} {/b/ /y/} {/ai/ /ei/ /u/} {/u/} {//} {/e/} {//} {/a/} {/v/ /w/} {/dz/ /y/} {/d/ /b/} {/t/} {/k/} {/ts/} {/l/ /n/} {/f/ /s/} {/ei/ /i/} {/ai/} /e/} {//} {/d/ /t/} {/l/ /m/} {/k/ /w/} {/v/} {/ts/} {/u/} {/y/} {/u/} {/a/} {/z/} {/f/ /n/} {/b/ /s/} {/dz/} /ai/ /ei/} /e/} {/m/ /n/} {/k/ /l/} {/dz/ /t/} {/d/ /s/} {/ts/} {/u/} {/y/} {/u/} {/a/} {/w/} {/f/} {/v/} {/b/} disadvantage strictly confusable viseme contains spurious single-phoneme visemes phoneme cannot grouped because confused phonemes viseme. types phonemes likely either borderline cases extremes viseme cluster i.e. subtle visual similarities phoneme cluster occur frequently enough training data diﬀerentiated phonemes. address complete second pass-through strictly-confused visemes listed table begin visemes currently stand relax condition requiring confusion phonemes. single phoneme viseme allocated previously existing viseme confused phoneme viseme. figure confused /p/. viseme value confusion decide allocate follows. therefore; total confusion whereas total confusion select viseme confusion incorporate unallocated phoneme /p/. reduces number viseme classes merging single-phoneme visemes table form second shown table added beneﬁt also increased number training samples classiﬁer. remember versions table mixed vowel consonant phonemes second divided vowels consonant phonemes still applies relaxed-confused visemes sets. means four types speaker-dependent phoneme-to-viseme maps described table strictly-confused maps table become relaxed maps table visemes deﬁned relaxed remaining phonemes confusions assigned viseme phoneme-pair confusions used remaining phonemes appropriate viseme even though confuse phonemes already {/b/ /ei/ /k/} /ai/ /u/} {/dz/ /z/} {/a/ /u/} {/d/ /t/} {/ts/ /l/} /v/}{// /v/} {/a/ /ai/ /ei/ /ts/} {/e/ /y/} {/l/ /n/} /f/} {/z/} {/b/ /p/} {/u/ /u/} {/dz/ /k/} /ai/ /ei/ /n/} /ts/} {/b/ /v/} {/l/ /u/} {/dz/} {/u/} {/z/} {/d/ /t/} {/k/ /w/} {/a/} /ai/ /ts/ /ei/ {/a/ /n/} /y/} {/dz/ /t/} {/k/ /w/} {/u/} {/d/ /s/} {/b/} /u/} {/a/ /ai/} /ei/} {/b/ /y/} {/d/ /t/} {/k/} {/z/} {/m/} {/l/} {/ts/} {/dz/ /z/} {/a/ /ai/ /ei/ /u/} {/k/ /w/} {/ts/ /n/} {/f/ /s/} {/dz/ /y/} {/b/ /d/} {/z/} /ai/ /ei/} /e/} {/b/ /v/} {/d/ /t/} {/l/ /m/} {/y/} {/dz/} {/u/} {/z/} {/u/} /e/} {/k/ /w/} {/f/ /n/} {/a/} {/ts/} /ai/ /ei/} {/ts/ /w/} {/d/ /v/} {/m/ /n/} {/f/} {/a/} {/dz/ /t/} {/u/} {/u/} {/y/} {/b/} figure shows word correctness common phoneme-pair visemes lee’s benchmark. surprise common-pair visemes worse gave maximum performance original mappings used deduce map. however overlap error bars shows speakers signiﬁcant reduction. unfortunately particular viseme group visemes particularly contribute correctness. figure four speaker-dependent maps tested speaker plotted x-axis compare diﬀerence word classiﬁcation benchmark comparison study black. speaker speaker viseme signiﬁcantly improves upon lee’s performance although improvements speaker speaker strictly-confused split viseme improves upon lee’s previous best word classiﬁcation. figure compares mixed consonant vowel maps split consonant vowel maps also measured word correctness yaxis. split maps always better mixed speakers. figure shows comparison strictly-confused loosely confused viseme classes. strict confusions better four speakers. speakers highest ratio phonemes visemes figure four variants maps plotted speaker all-speaker mean number visemes set. splitting vowel consonant phonemes gives greater number classiﬁers reduces number training samples class results higher correctness speakers. shows right training samples important simply ‘more data’. whilst showing smaller eﬀect graphs left hand side figure shows relaxing confusable phonemes negative inﬂuence even though reduces number visemes increases training samples class good training samples include class. approachtoﬁndingvisemesplottedagainstthecountofvisemeswithineachset. figure.howcorrectnessvarieswithquantityofvisemesineachset.allfourvariantsonaspeaker-dependentdata-driven matrices. signiﬁcant step viseme contribution longer needed speaker-dependent visemes need class labels within set. analysis visemes within also used proposes threshold subject information features. using combined shape appearance features removes threshold ﬁgures show irrespective method phoneme-clustering used devising visemes greater number visemes higher overall classiﬁcation. important overall classiﬁcation higher chapter described comprehensive study previously suggested maps shown lee’s best previously published maps. data-driven approach respects speaker individuality speech uses demonstrate second data-driven method tested strictlyconfused viseme derivation split vowel consonant phonemes improve word classiﬁcation. call speaker-dependent visemes ‘bear visemes’ author’s surname show conventional lip-reading system figure steps highlighted dash-edged boxes. phoneme confusion driven visemes possible contains insufﬁcient samples fairly identify confusion. whilst improving performance classiﬁers still need data training. reduction word correctness data-driven confused mixed visemes attributed mixing vowels consonants work shows keeping separate improvement possible. ratio phonemes visemes useful secondary confusions phonemes help discriminate phonemes within visemes improved word classiﬁcation. discriminate words visually similar still need able reverse mapping. work highlights training samples worse less training samples boundary good samples blurred. designed implemented method producing speaker-dependent visemes showing speaker identity important good machine lip-reading classiﬁcation. speaker dependence prevalent machine lipreading systems need cast eyes towards diﬃcult task speakerindependent classiﬁcation next chapter investigates. audio speech machine lip-reading speaker identity important accurate classiﬁcation know major diﬃculty visual speech labelling classiﬁer units need address questions; extent maps independent speaker? might speaker independent maps examined? alongside this would useful understand interactions model training data classes. therefore chapter dataset rmav dataset train test classiﬁers based upon series mappings. current time good machine lip-reading performances achieved speaker dependent classiﬁcation models means test speaker must included within classiﬁer training data. speaker independent machine lipreading less successful large scale investigations shown speaker independence viable. neti state created multi-speaker classiﬁers contingency speaker independent models fail generalise well unseen speakers. preliminary experiments multi-speaker classiﬁers considered needed. however achieved state-of-the-art modelling permitted increase word error rate speakers implies enough data speakers speaker independence obstacle surmountable achieving generalisation large scale. majority papers referenced thesis example speaker-dependent experiments still used greater results speaker independence rare diﬃcult achieve. continuous speech datasets interesting note still speaker-dependent tests note single speaker-dependent others multi-speaker dependent crux point test speaker samples included training data. contrast avicar ibm’s lvcsr achieve speaker-independent success. former speciﬁc dataset speech latter available suﬃce best datasets available thus understand speaker independence visual speech ability classify speaker involved classiﬁer training. diﬃcult unsolved problem. conﬁdent that visual speech identiﬁcation person speaking important. could wonder large enough dataset signiﬁcant number speakers could suﬃcient train classiﬁers generalised cover whole population including independent speakers. still struggle without dataset size needed test theory particularly know much ‘enough’ data speakers. example study speaker independence machine lip-reading authors compare single speaker multi-speaker speaker independent classiﬁcation using types classiﬁers however investigation uses word labels classiﬁers interested know results could improved using either phonemes speaker-dependent visemes. nine phoneme-to-viseme maps rmav maps constructed using separate training test data seven fold cross-validation rmav maps fold cross-validation. variation folds volume data dataset. toolkit classiﬁers built viseme classes map. hmms ﬂat-started hcompv re-estimated times forced alignment seventh eighth re-estimates. ﬁnal steps classiﬁcation using hvite output results hresults. models three state hmms associated gaussian mixture components keep results comparable previous work. rmav bigram word network built hbuild hlstats classiﬁcation measured correctness beep pronunciation dictionary used throughout experiments british english speakers. means derived speaker trained using visual speech data speaker tested using visual speech data speaker example would designate result testing constructed speaker using data speaker train viseme models testing speaker data. experiments need baseline comparison. select speaker-dependent maps based previous literature provide best results. baseline tests involved additional tests rmav remember speakers rmav speakers speakers rmav. tests speaker-dependent speaker used create train models testing data. tables depict tests constructed. cluster phonemes ground truth appear output phoneme classiﬁer. every viseme listed associated mutually-confused phonemes e.g. speaker made phonemes /iy/ /uw/}. know four phonemes /iy/ /uw/} confused three second tests within experiment start look using maps diﬀerent test speakers. means classiﬁers trained single speaker used recognise data alternative speakers. multi-speaker forms viseme classiﬁer labels third experiments. constructed using phoneme confusions produced speakers data shown table four speakers table rmav speakers. finally last tests looks speaker independence maps themselves. maps derived using speakers confusions test speaker. time substitute symbol place list speaker identifying numbers meaning ‘not including speaker tests maps many-to-one mapping. creates possibility creating visual homophones translating phonetic transcript viseme transcript. example data phonetic realisation word ‘/b//iy/’ ‘/d//iy/’. using translate visemes identical ‘/v//v/’ permitting variations pronunciation total tokens word translated visemes listed table homophones means greater chance substitution errors reduced correct classiﬁcation. score addresses phonemes within viseme total number phonemes clustered number visemes within ignores ordering visemes within set. example explain similarity algorithm imagine phoneme-to-viseme maps shown figure similarity measure calculated compare maps used experiments pairs results shown tables values closest zero show similar maps thus closer diﬀerent maps are. compared maps datasets biased eﬀects caused disparity word content data size. unsurprisingly rmav dataset maps similar volume speakers folds phoneme classiﬁcation chance unique phonemes confused. phonemes diﬀerent all. compare maps tables similarities. mostly speaker time removed within maps. however compared speaker-dependent maps table diﬀerent picture seen. speaker signiﬁcantly aﬀected introduction /uw/ viseme /v/. speaker word classiﬁcation less half speaker performance guessing. correlates similar tests independent hmm’s attributed possible eﬀects either visual units incorrect trained incorrect speaker. figures show tests continuous speech data. reassuring speakers signiﬁcantly deteriorate classiﬁcation rates speaker used train classiﬁer test speaker. example look speaker leftmost side figure test speaker speaker speaker-dependent maps speakers used build hmms classiﬁers. tested speaker maps models speakers show signiﬁcant reduction word correctness. eight speakers within standard error. figure rmav speakers four similar trend speaker showing variation three speakers. lip-read speaker actually signiﬁcant improvement using model speaker less signiﬁcant improvements speakers figure speaker models majorly improve classiﬁcation speaker however whilst signs possibly making strides towards speaker independent classiﬁcation speaker figure shows common trend overlap continuous speech speakers natural variation attributed speaker identity. trained relevant speaker tests speaker speaker speaker ﬁnally speaker word correctness improved substantially implies previous poor performance figure choice visemes rather badly trained hmms. tests classiﬁers constructed single-speaker dependent phoneme-toviseme maps speakers rmav tested others. baseline maps results shown hmms trained speakers tests classiﬁers constructed single-speaker dependent phoneme-toviseme maps speakers rmav tested others. baseline maps results shown hmms trained speakers tests classiﬁers constructed single-speaker dependent phoneme-toviseme maps speakers rmav tested others. baseline maps results shown hmms trained speakers tests classiﬁers constructed single-speaker dependent phoneme-toviseme maps speakers rmav tested others. baseline maps results shown hmms trained speakers eﬀects unit selection. using speaker example figure three maps signiﬁcantly reduce correctness speaker contrast speaker signiﬁcantly reducing maps maps signiﬁcantly improve classiﬁcation speaker suggests speakers identity important good classiﬁcation used. individuals simply easier read similarities certain speakers learned properly speaker able better classify rarer visual distinctions phonemes similar speakers. figure speaker particularly robust visual unit selection classiﬁer labels. conversely speakers really aﬀected visemes interesting note variability previously considered speakers dependent good visual classiﬁers mapping back acoustics utterances others much. again number visual classiﬁers really vary subject speaker identity. figure shows mean word correctness classiﬁers speaker rmav. y-axis shows word correctness x-axis speaker point. also plotted random guessing standard error folds. speaker best performing speaker irrespective selected. speakers similar standard error mean within bound. suggests subject speaker similarity possibility improve classiﬁcation correctness another speakers visemes weaker figure all-speaker mean word classiﬁcation correctness classiﬁers constructed single-speaker dependent phoneme-to-viseme maps twelve speakers rmav tested others. baseline maps performance viseme ranked speaker weighting eﬀect tests. scores table increases performance within error range scores outside error range scores decreases classiﬁcation performance values negative. therefore values show best four maps followed ﬁnally susceptible speaker identity avl. note order matches decreasing order quantity visemes speaker-dependent viseme sets i.e. similar phoneme classes visemes better classiﬁcation performance. ties table larger maps create less homophones. pairs /eh/} {/m/ /n/} {/ey/ /iy/} present three speakers /iy/} {/l/ /m/} pairs speakers. singlephoneme visemes {/ts/} present three times {/k/} {/w/} {/z/} twice. lesson figure selection incorrect units measure listed table rmav speakers. observation table speaker right column. speaker dependent speaker make overall improvement speakers classiﬁcation crucially speaker visemes make improvement classiﬁcation. speaker improves speakers others show negative eﬀect reinforces assertion visual speech dependent upon individual also evidence exceptions rule. order rmav signiﬁcant diﬀerence speaker speaker word classiﬁcation reduced eradicated. interesting speaker speaker-dependent classiﬁcation best speakers performs multi-speaker viseme classes signiﬁcantly. maybe speaker unique visual talking style reduces similarities speakers likely /iy/ phoneme classiﬁed viseme whereas re-appears mall. phoneme /iy/ common phoneme data. suggests best avoid high volume phonemes speakerdependent visemes trying maximise speaker individuality make better viseme classes. plotted experiments rmav speakers figures continuous speech speaker signiﬁcantly negatively aﬀected using generalised multispeaker visemes whether visemes include test speakers phoneme confusions not. reminds dependency speaker identity machine lip-reading scale eﬀect depends speakers compared. exception speaker insigniﬁcant decrease correctness using visemes. therefore could possible making multi-speaker visemes based upon groupings visually similar speakers even better visemes could created. challenge remains knowing speakers isolated word classiﬁcation main conclusion chapter shown comparing figures figure reduction performance figure system classiﬁcation models trained speaker test speaker. raised question degradation wrong choice speaker identity mismatch training test data samples. concluded that whilst wrong unit labels conducive good lip-reading choice phonemeto-viseme causes signiﬁcant degradation accurate classiﬁcation rather speaker identity. regain performance irrespective whether chosen diﬀerent speaker multi-speaker independently speaker. observation important tells repertoire visual units across speakers vary signiﬁcantly. comforting since prospect classiﬁcation using symbol alphabet varies speaker daunting. reinforced tables diﬀerences speakers signiﬁcant ones. however seen exceptions within continuous speech speakers whereby eﬀect selection prominent sharing hmms trained non-test speakers completely detrimental. gives hope similar visual speakers ‘good’ training data speaker independence whether classiﬁer viseme selection might possible. provide analogy; acoustic speech could accented norfolk speaker requires diﬀerent phonemes standard british talker? answer represented phonemes; individuality phonemes diﬀerent way. comparing multi-speaker maps visemes whereas single-speaker-dependent maps range visemes performs maps. conclude high risk over-generalising speaker-dependent attempting multi-speaker speaker-independent phonemeto-viseme mappings. something seen rmav experiments. therefore must consider speaker-dependency varies also contribution viseme within also contributes word classiﬁcation performance idea ﬁrst shown highlighted phonemes good subset potenresults present combination certain phoneme groups combined speaker-dependent visemes latter provide lower contribution overall classiﬁcation would improve speaker-independent maps speaker-dependent visual classiﬁers. compared accuracy around rmav. attribute training data volumes dataset. voice dataset used publicly available speakers word vocabulary compared rmav speakers words speaker. often said machine lip-reading high variability speakers. clariﬁed state high variability visual cues given language high variability trajectory visual cues individual speakers ground truth. continuous speech seen speaker identity aﬀects visemes also robustness speakers classiﬁcation varies response changes this. implies dependency upon number visemes within individuals investigate next chapter. many-to-one relationship traditional mappings phonemes visemes resulting visemes always smaller phonemes. know beneﬁt training samples class compensates limited data currently available datasets disadvantage generalisation diﬀerent articulated sounds. optimal viseme classes need minimise generalisation maintain good classiﬁcation also maximise training data available. chapter shown maps derived automatically phoneme confusions. by-product clustering phonemes classiﬁcation data option control many visemes contains within phoneme clustering algorithm. allows precision answering questions optimal number visemes. many visemes optimum number? optimum vary speaker visual speech? work rmav dataset beep pronunciation dictionary figure shows high level overview experiment. begins performing classiﬁcation using phoneme-labelled classiﬁers. provides speaker-dependent confusion matrices used cluster together single phonemes subgroups call them visemes. time around adopt diﬀerent phoneme clustering process process mapping derived every time pair classes re-classiﬁed class grouping. maximum phonemes phonetic transcript rmav speakers. means create maps speaker. actual number maps produced subject number phonemes matched during phoneme classiﬁcation ﬁrst step produces phoneme confusion matrices create phoneme clusters visemes. phoneme classiﬁed either incorrectly correctly included resulting confusion matrix visemes created. thus sets viseme labels labelling hmms repeating word classiﬁcation task. classiﬁers. performance relevant here rather improvement variance classes provide. important reader remember presentation method suggestion particular clustering algorithm deliver optimum visemes rather address need case method enable controlled comparison phoneme viseme distributions number classes reduces. step implements -fold cross-validation replacement sentences speaker randomly selected test samples included training folds. using toolkit implement classiﬁers hmms initialised ﬂat-start method re-estimated times forced alignment seventh eighth estimates. prototype based upon gaussian mixture components three state hmms. included single-state tied short-pause ‘sp’ short silences words sentence utterances. bigram word network used support classiﬁcation. phonemes clustered viseme classes speaker follows; step produces confusion matrices speaker summed together form confusion matrix representing confusions speaker. clustering begins phoneme confusion matrix probability that given classiﬁcation phoneme really subscript indicates elements merging phonemes done looking confused phonemes hence create class confusions pm−. phonemes assigned classes vowels consonants. vowels consonants mixed. pair highest merged. equal scores broken randomly. process repeated intermediate step forms possible visual units. controlled approach method used chapter incorporates conclusions vowel consonant phonemes clustered together devising phoneme-to-viseme mappings. example mapping shown table similar step step involves implementation -fold cross-validation replacement sentences speaker randomly selected test samples included training folds. using toolkit hidden markov model classes viseme labelled hmms ﬂat-started re-estimated times forced alignment seventh eighth estimates. prototype used bigram word network supports classiﬁcation along application grammar scale factor transition penalty important diﬀerence time around viseme classes used classiﬁcation labels. using sets classes shown step confusing lips perform classiﬁcation class set. total sets smallest classes largest classes phoneme thus largest speaker repeat phoneme classiﬁcation task using phonemes originally recognised step figures show word correctness plotted y-axis speakers. viseme sets identiﬁed number visemes within plotted increasing order along x-axis. also plotted green guessing weighted visual homophones transcripts. calculated viseme sets containing fewer visemes produce viseme strings represent word homophones. eﬀect homophones seen left side graphs figures viseme sets fewer visemes. correctness scores signiﬁcantly chance albeit still low. variation speakers expected clear overall trend. superior performances found larger numbers visemes. important point authors report word accuracy viseme performance using word unit language network. unhelpful masks eﬀect homophones using network level unit rather accuracy viseme models themselves. reported eﬀect needing larger numbers visemes would visible. also figures highlighted class sets show signiﬁcant improvement classiﬁcation adjacent units right side along x-axis. identify pairs classes which merged class signiﬁcantly improve classiﬁcation. table lists special viseme combinations. referencing back speaker demographics apparent pattern viseme combinations. evidence reinforce knowledge speakers visually unique reminded diﬃcult ﬁnding cross-speaker visemes phonemes require alternative groupings individual. conventional wisdom visemes needed lip-reading example) countered experiments phoneme classiﬁcation signiﬁcantly diﬀerent viseme classiﬁcation. however simpliﬁcation assert better lip-reading achieved phonemes visemes shown signiﬁcance. generally speaking larger numbers visemes out-perform smaller numbers. however classiﬁcation aggregated figure mean word correctness classiﬁcation speakers within error monotonic trend. figure also plotted system error instead guessing. system error calculated using ground truth transcript test data place classiﬁers output hresults obtain errors caused system rather classiﬁers. fortunately zero demonstrating robustness lip-reading system. to-viseme maps typically generate visemes consonant visemes vowel visemes jeﬀers eight three respectively figures show deﬁnite rapid drop-oﬀ performance sets contain fewer visemes region contains optimum viseme three speakers chance. mean speaker shown optimal number visual units optimal number related conventional viseme deﬁnitions neither number phonemes. table shows correctness speakers phoneme classiﬁcation. implication that speakers possible conclude small number visemes optimal. however considering speakers much likely phonemes provide better classiﬁer labels classiﬁcation. visual units represent mouth shape appearances versus introduction homophones. large numbers visemes close phonetic classiﬁcation risk visual units visually distinctive several models match particular sub-sequence. latter problem creates decoding lattice several near equal probability paths which turn implies state-of-the-art language models would improve results still further. recent work presents evidence viseme labels needed because enough data classiﬁers based upon phoneme labels outperform viseme classiﬁcation additionally seen challenges using viseme/phoneme labelled classiﬁers including; homophone eﬀect enough training data class consequential lack diﬀerentiation classes many classes distinguish them. seen figure replotted word correctness speakers section onto graph. figure shows previous results derived using algorithm described able generate viseme sets varying size. x-axis runs y-axis shows word correctness classiﬁers trained viseme viseme set. lines rmav speakers. viseme sets listed table know sometimes units traditional visemes phonemes better classiﬁcation visual speech signal. evidence pointing towards larger number visual units previously thought sensible. extreme example assume visual unit phoneme problem identical gestures appear separate visemes. examine this propose concept adopting weak learning hierarchical classiﬁer training. intention test method improve phoneme classiﬁcation without need training data approach shares training data across models. premise avoids negative eﬀects introducing homophones assist identiﬁcation subtle important diﬀerences visual gestures representing alternative phonemes. crucially method means increasing valid training data without needing create record remember chapter using wrong clusters phonemes worse using none. weak learning alternative approach training classiﬁcation models lip-reading. weak learning traditionally applied ensembles classiﬁers classiﬁers produces stronger classiﬁer independently-weak-trained classiﬁers acknowledging poor performance viseme labelled classiﬁers assume weakly trained. whilst outperform guessing strongly trained classiﬁers thus adopt method boosts weakly trained viseme classiﬁers strongly trained phoneme classiﬁers hope achieve signiﬁcantly higher classiﬁcation rates. also encourages training data weak-learning phase specialised training speciﬁc phoneme samples phoneme classiﬁer training phase. therefore last investigation thesis attempt modify lip-reading process apply weak learning classiﬁer training test visual signal better translated visemes phonemes better train classiﬁers volume visual data whilst improving classiﬁcation. method addresses challenges identiﬁed chapter thus far. additional beneﬁt revised classiﬁcation process weak learning model training phase phoneme classiﬁcation longer need consider post-classiﬁcation-processing weighted ﬁnite state transducers reverse phoneme-to-viseme mapping order real phoneme recognised. figure performance classiﬁers small numbers visemes poor large number homophones. large numbers visemes appear noticeably improve correctness many phonetic variations look similar lips. numbers printed black signiﬁcantly improving viseme sets identiﬁed number visemes set. therefore focus viseme sets range speakers experiments using weak learning. basis training approach hierarchically train classiﬁers. figure shows stylised illustration phonemes visemes phoneme assigned viseme going learn intermediate hmms identical viseme hmms. create models phonemes. example associated initialised replicas /v/. likewise initialised replicas /v/. retrain phoneme models using training data. full; initialise viseme hmms hcompv tool hcompv used initialising hmms deﬁnes models equal prototype based upon gaussian mixture components three states. trained times over including short pause model state tying forced alignment re-estimates classiﬁcation viseme deﬁnitions used initialised deﬁnitions phoneme labelled hmms respective viseme deﬁnition used phonemes relative phoneme-to-viseme map. phoneme hmms retrained used classiﬁcation. part classiﬁcation bigram network apply grammar scale factor apply transition penalty implemented using -fold cross-validation advantage approach phoneme classiﬁers seen mostly positive cases therefore good mode matching disadvantage limited exposure negative cases less visemes. investigating correct unit selection classiﬁers must forget unit selection language network used decode classiﬁcation transcripts. means need review eﬀect language network unit choice ﬁnal experiment. using common process previously described lip-reading perform classiﬁcation using speaker-dependent visemes phonemes word hmms optional unit networks listed table means answer question dependency unit choice classiﬁer labels unit supporting language network?’. linguistic content dataset impact computer lip-reading classiﬁcation performance. stylised texts structure restrictions speech utterance organised therefore classiﬁcation becomes simpler task. case rmav dataset challenge lip-reading continuous speech much diﬃcult complexity task grows size variability said order how. part classiﬁcation task error rate come from? phonemes/visemes currently recognisable? mean phonemes help classiﬁcation task others classiﬁer place weight phonemes improve classiﬁcation performance? within classiﬁcation grammar networks built probability statistics training data priori knowledge linguistic content word phoneme level improve lip-reading classiﬁcation. considering natural continuous speech makes word phoneme/viseme network exceptionally large order permit order table all-speaker error counts diﬀerent combinations units classiﬁers bigram support networks. units vertically network units horizontally table. combination utterances. likewise higher-order n-gram language model improve classiﬁcation rates cost model disproportionate intention develop better classiﬁers. dictionaries help deﬁne vocabulary recognised natural speech happens word uttered previously known? slang term example. entry required made phonemes already exist. figure eﬀects support network unit choice varying classiﬁer units measured speaker mean correctness units supported viseme network shown blue phoneme networks eﬀects network units shown figure plots units x-axis classiﬁcation correctness error bars show standard error. using viseme network shows worst classiﬁcation. attributed volume homophones introduced translating words phonemes visemes. longer consider option. interesting word phoneme networks. phoneme network greatly improves classiﬁcation viseme hmms word network. phoneme hmms diﬀerence phoneme word network standard error identical. thus phoneme word networks ﬁnal method. figure correctness viseme classiﬁers either phoneme word language models weak learned phoneme classiﬁers either phoneme word language models averaged speakers. x-axis figure size viseme sets figure remind reader range optimal number visemes phoneme label classiﬁers improve classiﬁcation. baseline viseme classiﬁcation word network shown blue signiﬁcantly diﬀerent conventionally learned phoneme classiﬁers. based unit selection language network study section surprise using phoneme network instead word network support viseme classiﬁcation signiﬁcantly improve mean correctness score viseme sizes speakers guessing interesting weakly-trained phoneme hmms signiﬁcantly better viseme hmms. original work phoneme hmms gave all-speaker mean here regardless size original viseme almost double. weakly learnt phoneme classiﬁers word network gain mean phoneme classiﬁers supported phoneme network correctness gain range gains supported speaker mean minimum maximums listed table gain scores potential viseme-to-phoneme mappings show little diﬀerence phoneme-to-viseme best knowing visemes initialise phoneme classiﬁers. results including baseline signiﬁcantly better guessing speaker-dependent graphs shown signiﬁcant diﬀerences viseme sizes shown figure disappeared learning diﬀerences visemes incorporated training phoneme classiﬁers turn better trained speaker-dependent results hierarchical learning intriguing rmav published start thesis showed average viseme accuracy here presented word accuracy present viseme accuracy hierarchical training method transformed viseme classiﬁers phoneme labelled classiﬁers reporting phoneme accuracy provides classiﬁcation. beneﬁcial phoneme transcripts comprehensible less homophones reduces dependency language model comprehension. intriguing observation comparing phoneme network visemes weakly taught phonemes. speakers weakly learned phonemes always important right network unit. seen figures speaker’s rewatching original videos estimate speakers categorise either ‘older’ ’younger’ speaker. speakers less signiﬁcant diﬀerence eﬀect weak learning younger. implies lip-read younger person need support language model older speaker. informal observation young people co-articulation older people something investigation. chapter described viseme derivation method allows construct number visual units. reader reminded proposal method best visemes priority objective case method enabling comparison viseme sets controlled manner. presence optimum number visemes within classes result competing eﬀects. ﬁrst number visemes shrinks number homophones rises becomes diﬃcult recognise words second number visemes rises suﬃcient training data longer available order learn subtle diﬀerences lip-shapes again correctness drops. thus theory optimum number visual units lies beween practice optimum number phonemes twelve choice visual units lip-reading caused debate. researchers visemes adduced example fisher others noted lip-reading using phonemes give superior performance visemes here supply evidence nuanced hypothesis intermediary units convenience call visemes provide superior performances provided derived analysis data. furthermore presented novel learning algorithm shows improved performance data-driven visemes used hierarchical classiﬁer training. essence method re-train viseme models fashion similar weak learning order become better phoneme-labelled classiﬁers. produces signiﬁcantly better classiﬁcation second augmentation lip-reading system. shown figure extra steps dash-edged boxes right hand side. original research question understand visemes order augment replace current classiﬁers conventional automatic lip-reading systems? learnt experiments that lower limit resolution machine lip-read least pixels lip. long videos speakers least achieve lip-reading. important high resolution video person’s face away pixels less would worse close resolution video also limitation useful speaker-independent visemes within towards overall recognition. badly trained viseme worse viseme represent certain phonemes training visemes enough need data training data detrimental classiﬁcation less. comparison many phoneme viseme maps literature seen little diﬀerence lee’s marginally outperforms others majority previous presented maps designed observations human lip-readers biased towards individual perception human participating. higher performing maps recent presentations data driven and/or machine trained. clustering phonemes visemes conﬁdence vowel consonant phonemes isolated. shown methods devising speaker-dependent visemes whereby permitted mixing phonemes second method restricted clustering. second method signiﬁcantly outperformed former. speaker individuality important visual speech recognised devising viseme viseme sets small negatively aﬀected homophone confusions. sets large able trained suﬃciently achieve good classiﬁcation. means wrong speaker training volume combination size viseme fragile. shown range optimum sizes demonstrated varies speaker higher phoneme-to-viseme maps previously presented literature. show speaker dependent recognition range choices selecting visual units containing fewer members phoneme sets outperform phoneme labelled classiﬁers. considered however speaker independent recognition still likely phonemes desirable choice classiﬁer units consistent across speakers. thus speaker-dependent recognition right visemes outperform phoneme-labelled classiﬁers also used help train phoneme classiﬁers classify visual speech signiﬁcantly better support good classiﬁers seen eﬀect diﬀerent unit labels supporting language network. best results achieved unit labels classiﬁers network classiﬁcation signiﬁcantly aﬀected not. therefore purposes decoding phonemes back words spoken preferred network unit words conclusion answer research question improved machine lip-reading adopting current classiﬁcation system speaker-speciﬁc phoneme confusions within clustering algorithm produce speaker-dependent viseme sets which turn make good prototype classiﬁers train phoneme labelled classiﬁers. these together word labelled language network mean decode visemes improve machine lip-reading classiﬁcation. machine lip-reading large complicated problem. many sub-problems need solved within challenge achieve high consistent classiﬁcation. remaining problems grouped three classes using trees categories detection classiﬁcation estimation classiﬁcation problems remain include classiﬁcation rates still need improvement considered robust speaker independence classiﬁer training test data produce good results. speech recognition maturing ﬁeld research refer introduction remember motivation improve machine lipreading classiﬁcation major reasons. firstly system would applicable range areas entertainment criminal detection secondly main expectation lip-reading system integration system avsr. robust lip-reading system could improve robustness accuracy avsr system better visual channel alone fallback times audio signal drops deteriorated noise. goal raises number signiﬁcant questions extend beyond demands achieving visual-only speech recognition. audio-visual signal fusion environmental noise camera/microphone movement three examples challenges avsr diﬃcult problem classify acoustic utterances signal sparse visual cues whilst acoustic recognition achieving ubiquity commercial applications machine lip-reading achieve level robustness. independence speaker identity camera view occlusions language still need robustly accomplished technology reality. ibrahim almajai milner jonathan darch. analysis correlation audio visual speech features clean audio feature prediction noise. interspeech pages isca international phonetic association. handbook international phonetic association guide international phonetic alphabet. cambridge university press leonard baum petrie george soules norman weiss. maximization technique occurring statistical analysis probabilistic functions markov chains. annals mathematical statistics pages helen bear stephen richard harvey. speaker independent machine reading speaker dependent viseme classiﬁers. joint international conference facial analysis animation audio-visual speech processing pages isca helen bear richard harvey barry-john theobald yuxuan lan. resolution limits visual speech recognition. image processing ieee international conference pages ieee helen bear richard harvey barry-john theobald yuxuan lan. finding phonemes improving machine lip-reading. joint international conference facial analysis animation audio-visual speech processing pages isca helen bear richard harvey barry-john theobald yuxuan lan. phoneme-to-viseme maps best improve visual-only computer lip-reading? advances visual computing pages springer helen bear gari owen richard harvey barry-john theobald. observations computer lip-reading moving dream reality. spie security+ defence pages g–g. international society optics photonics christian benoit thierry guiard-marigny adjoudani. components face humans machines best speechread? volume pages berlin nato-asi series springer carl binnie pamela jackson allen montgomery. visual intelligibility consonants lipreading screening test implications aural rehabilitation. journal speech hearing disorders richard bowden stephen richard harvey yuxuan eng-jon gari owen barry-john theobald. recent developments automated lip-reading. symposium photonics intelligent engineering security+ defence pages j–j. international society optics photonics elif bozkurt erdem engin erzin tanju erdem ozkan. comparison phoneme viseme based acoustic units speech driven realistic animation. proceedings signal processing communications applications pages luca cappelletta naomi harte. phoneme-to-viseme mapping visual speech recognition. international conference pattern recognition applications methods pages martin cooke barker stuart cunningham shao. audiovisual corpus speech perception automatic speech recognition. journal acoustical society america stephen richard harvey yuxuan jacob newman barry theobald. challenge multispeaker lip-reading. proceedings international conference auditory-visual speech processing pages james crowley francois berard. multi-modal tracking faces video communications. computer vision pattern recognition proceedings. ieee computer society conference pages ieee andrzej ehrenfeucht david haussler michael kearns leslie valiant. general lower bound number examples needed learning. information computation william fisher george doddington kathleen goudiemarshall. darpa speech recognition research database speciﬁcations status. proceedings darpa workshop speech recognition pages georgios galatas gerasimos potamianos alexandros papangelis fillia makedon. audio visual speech recognition noisy visual environments. proceedings international conference pervasive technologies related assistive environments page john gowdy amarnag subramanya chris bartels bilmes. based multi-stream models audio-visual speech recognition. acoustics speech signal processing proceedings.. ieee international conference volume pages ieee timothy hazen. visual model structures synchrony constraints audio-visual speech recognition. ieee transactions audio speech language processing timothy hazen kate saenko chia-hao james glass. segment-based audio-visual speech recognizer data collection development initial experiments. proceedings international conference multimodal interfaces icmi pages york acm. martin heckmann fr´ed´eric berthommier christophe savariaux kristian kroschel. eﬀects image distortions audio-visual speech recognition. avsp -international conference audio-visual speech processing martin heckmann kristian kroschel christophe savariaux fr´ed´eric berthommier dct-based video features audio-visual speech recognition. interspeech hieronymus mckelvie mcinnes. acoustic sentence level lexical stress hsmm speech recognition. ieee international conference acoustics speech signal processing volume pages ieee sarah hilder richard harvey barry-john theobald. comparison human machine lip-reading. proceedings international conference audio-visual speech processing pages xiaopeng hong hongxun yuqi rong chen. based visual feature extraction method lip-reading. intelligent information hiding multimedia signal processing iih-msp’. international conference pages ieee dominic howell barry-john theobald stephen cox. confusion modelling automated lip-reading using weighted ﬁnite-state transducers. auditory-visual speech processing pages huang tsuhan chen. tracking multiple faces humancomputer interfaces virtual environments. proceedings ieee international conference multimedia expo volume pages jintao jiang abeer alwan lyme bernstein edward auer patricia keating. similarity structure perceptual physical measures visual consonants across talkers. proceedings ieee international conference acoustics speech signal processing volume pages kush kumar tsuhan chen richard stern. proﬁle view reading. acoustics speech signal processing icassp ieee international conference volume pages iv–. ieee yuxuan richard harvey theobald eng-jon richard bowden. comparing visual features lipreading. proceedings international conference auditory-visual speech processing pages yuxuan barry-john theobald richard harvey eng-jon improving visual features lip-reading. proceedrichard bowden. ings international conference audio-visual speech processing soonkyu dongsuk yook. audio-to-visual conversion using hidden markov models. proceedings paciﬁc international conference artiﬁcial intelligence pages springer charay lerdsudwichai mohamed abdel-mottaleb. algorithm multiple faces tracking. multimedia expo icme’. proceedings. international conference volume pages ii–. ieee luhong liangi feiyue huang neﬁan. multi-stream audio-video large-vocabulary mandarin chinese speech database. multimedia expo icme ieee international conference volume pages vol. june patrick lucey gerasimos potamianos sridha sridharan. visual speech recognition across multiple views. visual speech reognition segmentation mapping jiˇr´ı matas karel zimmermann tom´aˇs svoboda adrian hilton. learning eﬃcient linear predictors motion estimation. computer vision graphics image processing pages springer iain matthews bangham richard harvey stephen cox. nonlinear scale decomposition based features visual speech recognition. proceedings european signal processing conference pages iain matthews gerasimos potamianos chalapathy neti juergen luettin. comparison model transform-based visual features audio-visual lvcsr. null page ieee neﬁan luhong liang xiaobo xiaoxing kevin murphy. dynamic bayesian networks audio-visual speech recognition. eurasip journal advances signal processing e.j. bowden. robust lip-tracking using rigid ﬂocks selected linear predictors. ieee international conference automatic face gesture recognition e.j. bowden. robust facial feature tracking using shapeconstrained multi-resolution selected linear predictors. ieee transactions pattern analysis machine intelligence eng-jon yuxuan barry theobald harvey bowden. robust facial feature tracking using selected multi-resolution linear predictors. ieee international conference computer vision pages sept adrian pass jianguo zhang darryl stewart. investigation features multi-view lipreading. image processing ieee international conference pages ieee eric patterson sabri gurbuz zekeriya tufekci john gowdy. cuave audio-visual database multimodal human-computer interface research. acoustics speech signal processing ieee international conference volume pages ii–. ieee yuru tae-kyun hongbin zha. unsupervised random forest manifold alignment lipreading. proceedings ieee international conference computer vision pages gerasimos potamianos hans peter graf eric atto. image transform approach based automatic lipreading. image processing icip proceedings. international conference pages ieee gerasimos potamianos chalapathy neti juergen luettin iain matthews. audio-visual automatic speech recognition overview. issues visual audio-visual speech processing jerker r¨onnberg stefan samuelsson bj¨orn lyxell. conceptual constraints sentence-based lipreading hearing-impaired. hearing psychology speechreading auditory–visual speech pages karl schwerdt james crowley. robust face tracking using color. proceedings fourth ieee international conference automatic face gesture recognition. pages ieee ayaz shaikh dinesh kumar jayavardhana gubbi. visual speech recognition using optical support vector machines. international journal computational intelligence applications stommel langer herzog k.-d. kuhnert. fast robust bit-rate representation sift surf features. ieee international symposium safety security rescue robotics pages taylor b.-j. theobald matthews. eﬀect speaking rate audio visual speech. ieee international conference acoustics speech signal processing pages sarah taylor moshe mahler barry-john theobald iain matthews. dynamic units visual speech. proceedings siggraph/eurographics conference computer animation pages eurographics association barry theobald richard harvey stephen colin lewis gari owen. lip-reading enhancement enforcement. optics photonics counterterrorism crime fighting volume pages spie melanie vitkovitch paul barber. visible speech function image quality eﬀects display parameters lipreading ability. applied cognitive psychology brian walden robert prosek allen montgomery charlene scherr carla jones. eﬀects training visual recognition consonants. journal speech language hearing research xiaozhou lijun zhiwei qiang avatar-mediated face tracking reading human computer interaction. proceedings annual international conference multimedia multimedia pages york acm. wong chng phooi seng li-minn siew chin chew king hann lim. multi-purpose audiovisual unmc-vier database multiple variabilities. pattern recognition letters", "year": "2017"}