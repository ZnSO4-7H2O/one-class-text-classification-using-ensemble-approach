{"title": "DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant  Kernels and Random Features", "tag": "eess", "abstract": " Acoustic scene recordings are represented by different types of handcrafted or Neural Network-derived features. These features, typically of thousands of dimensions, are classified in state of the art approaches using kernel machines, such as the Support Vector Machines (SVM). However, the complexity of training these methods increases with the dimensionality of these input features and the size of the dataset. A solution is to map the input features to a randomized lower-dimensional feature space. The resulting random features can approximate non-linear kernels with faster linear kernel computation. In this work, we computed a set of 6,553 input features and used them to compute random features to approximate three types of kernels, Gaussian, Laplacian and Cauchy. We compared their performance using an SVM in the context of the DCASE Task 1 - Acoustic Scene Classification. Experiments show that both, input and random features outperformed the DCASE baseline by an absolute 4%. Moreover, the random features reduced the dimensionality of the input by more than three times with minimal loss of performance and by more than six times and still outperformed the baseline. Hence, random features could be employed by state of the art approaches to compute low-storage features and perform faster kernel computations. ", "text": "acoustic scene recordings represented different types handcrafted neural network-derived features. features typically thousands dimensions classiﬁed state approaches using kernel machines support vector machines however complexity training methods increases dimensionality input features size dataset. solution input features randomized lower-dimensional feature space. resulting random features approximate non-linear kernels faster linear kernel computation. work computed input features used compute random features approximate three types kernels gaussian laplacian cauchy. compared performance using context dcase task acoustic scene classiﬁcation. experiments show both input random features outperformed dcase baseline absolute moreover random features reduced dimensionality input three times minimal loss performance times still outperformed baseline. hence random features could employed state approaches compute low-storage features perform faster kernel computations. dcase task acoustic scene classiﬁcation aims identify recording belonging predeﬁned sceneclasses characterizes environment example park home ofﬁce. typically approaches capture diverse characteristics audio signal computing different types features either hand-crafted derived neural networks features commonly highdimensionality state approaches classiﬁed using support vector machines best known member kernel methods. kernel methods kernel trick property employs non-linear kernel function operate high-dimensional space computing inner products pairs transformed input features. inner products computed stored kernel gram matrix computing time storage complexity increases dimensionality number input features. solution compute random features well studied mainly shift-invariant kernels closed form. process maps input features lower-dimensional random space. then resulting random features approximate non-linear kernels linear kernel computations hence speeding kernel matrix generation. paper evaluated random features context dcase task acoustic scene classiﬁcation first computed input features thousand dimensions computed random features approximate three types shift-invariant kernels gaussian laplacian cauchy. type features input random classiﬁed using svm. experiments show baseline outperformed features. moreover random features reduced dimensionality three times minimal loss performance times still outperformed baseline. paper organized follows section describe detail kernel functions used. section present experiments results task finally section conclude discussing scope presented technique well future directions. section describe computation random features three types shift-invariant kernels context svm. acoustic scene classiﬁcation explored state approaches based kernel methods non-linear decision boundaries using kernel function. function takes input features space yield output scene classes paper consider moreover kernel function expressed positive-deﬁnite yields value corresponding inner product function maps space generally higher dimensionality better class separability. however computing kernel function could become prohibitive task dimensionality input large size training large. happens because order learn decision boundary function input audio corresponding labels dataset need compute value every element cauchy kernel less known comparison previous computing kernel even expensive task high-dimensional vectors mathematical form hence beneﬁting speed processing random features. deﬁne kernel kernel method perform non-linear classiﬁcation solving quadratic optimization dual form taking advantage kernel trick kernel trick uses nonlinear function input features high-dimensional feature space computing kernel matrix. using non-linear shift-invariant kernel using input features could approximated linear using random features. kernel matrix resulting computing inner product random features correspond approximation kernel matrix using input features shiftinvariant kernel. linear computation important implication libraries optimized problems. experiments addressed dcase task acoustic scene classiﬁcation evaluate compare performance input features using svms three non-linear shiftinvariant kernels random features corresponding three kernel types using linear svms. pipelines illustrated figure figure acoustic scene dataset used extract input features recording. then input features used train different ways. pass features directly non-linear shift-invariant kernel ﬁrst compute random features pass linear kernel svm. lastly trained used multi-class classiﬁcation test recordings. although different random features mappings proposed different kernel functions focused random features shift-invariant kernels. kernel shift-invariant matrix vector components function element-wise. randomness comes generation components comes uniform distribution comes fourier transform function therefore approximation stated equation depends kernel function involved distribution used generate matrix paper focus three well studied shift-invariant kernel functions gaussian laplacian cauchy. deﬁnition corresponding distributions used generate random features described below. acoustic scene beach cafe/restaurant city center forest path grocery store home library metro station ofﬁce park residential area train tram overall various acoustic scenes minutes long divided cross-validation folds. original recordings split segments length seconds. recordings made using binaural microphone recorder using sampling rate resolution. acoustic scenes cafe restaurant city center forest path grocery store home lakeside beach library metro station ofﬁce residential area train tram urban park. extracted large audio features proposed later used compute random features. include different features capture different information acoustic scenes consist multiple sound sources. computed open-source feature extraction toolkit opensmile using conﬁguration emolarge.conf. features divided four categories cepstral spectral energy related voicing extracted every frames. moreover included functionals mean standard deviation percentiles quartiles linear regression functionals local minima/maxima. total dimensionality feature vector ﬁrst experiments aimed evaluate large input features non-linear svms asc. used input features train three types non-linear shift-invariant svms also included linear kernel parameter tuned using search grid linear kernel ﬁxed cases performance measured using accuracy. accuracy average classiﬁcation accuracy validation folds provided challenge. additionally explored different values obtaining best results gaussian kernel laplacian kernel cauchy kernel. training models fold normalized input features respect training set. computed mean standard deviation using feature subtracted mean divided standard deviation every training testing sets. classiﬁcation performance kernel types similar shown table generally non-linear kernels tend perform better linear kernels however it’s uncommon similar performance class separability given features complex could case. among best classiﬁed scene-classes cafe/restaurant grocery store improvements second experiments aimed show random features linear similar performance non-linear svms. this used training testing input features compute random features corresponding three shift-invariant kernels described section then random features used train linear kernel. performance employing random features indeed compared input features non-linear shown table results improve dimensionality random features increases hence showing minimal loss performance compared previous non-linear svms. notice always lower original dimensionality input features. would increased value would improvement performance convergence values table reported dcase baseline tailored multi-class single label classiﬁcation setup network output layer consisting softmax type neurons representing classes framebased decisions combined using majority voting obtain single label classiﬁed segment. classiﬁcation resulted accuracy outperformed absolute using input features laplacian kernel. table overall accuracy computing random features using linear depending value dimensionality random features. note values smaller input features larger values closer ones table tained similar performance dcase baseline gaussian cauchy kernels. thus reducing dimensionality sixth original dims. moreover reduction dimensionality obtained minimal loss absolute gaussian cauchy kernels. note dcase challenge submitted system using input features laplacian kernel svm. overall classiﬁcation comparison reported baseline advantage random features reduce signiﬁcantly amount storage computational processing reducing dimensionality using linear inner products. unlike dimensionality reduction methods technique presented paper need heavy computation cost like computing eigenvectors need generate random numbers appropriate kernel-related distribution. moreover machine learning algorithms employ kernels could beneﬁted. multiple applications take advantage random features. example state techniques currently dealing features dims hundreds thousands segments passed linear svms. another example audio recorded local devices sent cloud technique helps compress information reducing cost transmission preserve privacy. instance compute random features keeping parameters private. thus still process transformed data cloud linear models without revealing actual data. paper addressed task acoustic scene classiﬁcation outperformed baseline accuracy using large acoustic features non-linear svms. additionally computed random features approximated three types shift-invariant kernels passed linear svm. showed dimensionality decreased sixth minimal degradation performance results signiﬁcant implications data context high dimensional features must stored quickly processed. geiger schuller rigoll large-scale audio feature extraction acoustic scene classiﬁcation ieee workshop applications signal processing audio acoustics. elizalde kumar shah badlani vincent lane experiments dcase challenge acoustic scene classiﬁcation sound event detection real life recording dcase workshop detection classiﬁcation acoustic scenes events mesaros heittola diment elizalde shah vincent virtanen dcase challenge setup tasks datasets baseline system proceedings detection classiﬁcation acoustic scenes events workshop november submitted.", "year": "2018"}