{"title": "CREPE: A Convolutional Representation for Pitch Estimation", "tag": "eess", "abstract": " The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application. ", "text": "notable trend methods derivation better pitch detection system solely depends cleverly devising robust candidate-generating function and/or sophisticated post-processing steps i.e. heuristics none directly learned data except manual hyperparameter tuning. contrasts many problems music information retrieval like chord beat detection data-driven methods shown consistently outperform heuristic approaches. possible explanation since fundamental frequency low-level physical attribute audio signal directly related periodicity many cases heuristics estimating periodicity perform extremely well accuracies close leading consider task solved problem. this however always case even performing algorithms like pyin still produce noisy results challenging audio recordings sound uncommon instruments pitch curve ﬂuctuates fast. particularly problematic tasks require ﬂawless estimation using output pitch tracker generate reference annotations melody multi-f estimation paper present novel data-driven method monophonic pitch tracking based deep convolutional neural network operating time-domain signal. show approach crepe obtains state-of-the-art results outperforming heuristic approaches pyin swipe robust noise too. show crepe highly precise maintaining pitch accuracy even strict evaluation threshold cents. python implementation proposed approach along pre-trained model crepe made available online easy utilization reproducibility. crepe consists deep convolutional neural network operates directly time-domain audio signal produce pitch estimate. block diagram proposed architecture provided figure input -sample excerpt time-domain audio signal using sampling rate. convolutional layers result -dimensional latent representation connected densely output layer sigmoid activations corresponding -dimensional output vector this resulting pitch estimate calculated deterministically. task estimating fundamental frequency monophonic sound recording also known pitch tracking fundamental audio processing multiple applications speech processing music information retrieval. date best performing techniques pyin algorithm based combination pipelines heuristics. techniques perform well average remain many cases fail correctly estimate pitch. paper propose data-driven pitch tracking algorithm crepe based deep convolutional neural network operates directly time-domain waveform. show proposed model produces state-of-the-art results performing equally better pyin. furthermore evaluate model’s generalizability terms noise robustness. pretrained version crepe made freely available open-source python module easy application. estimating fundamental frequency monophonic audio signal also known pitch tracking pitch estimation longstanding topic research audio signal processing. pitch estimation plays important role music signal processing monophonic pitch tracking used method generate pitch annotations multi-track datasets core component melody extraction systems pitch estimation also important speech analysis prosodic aspects intonations reﬂect various features speech pitch deﬁned subjective quality perceived sounds precisely correspond physical property fundamental frequency however apart rare exceptions pitch quantiﬁed using fundamental frequency thus often used interchangeably outside psychoacoustical studies. convenience also terms interchangeably throughout paper. computational methods monotonic pitch estimation studied half-century many reliable methods proposed since. earlier methods commonly employ certain candidate-generating function accompanied prepost-processing stages produce pitch curve. functions include cepstrum autocorrelation function average magnitude difference function normalized cross-correlation function proposed rapt praat cumulative mean normalized difference function proposed recent approaches include swipe performs template matching spectrum sawtooth waveform pyin probabilistic variant uses hidden markov model decode fig. architecture crepe pitch tracker. convolutional layers operate directly time-domain audio signal producing output vector approximates gaussian curve equation used derive exact pitch estimate equation nodes output layer corresponds speciﬁc pitch value deﬁned cents. cent unit representing musical intervals relative reference pitch fref deﬁned function frequency target outputs train model -dimensional vectors dimension represents frequency covering cents corresponding ground truth fundamental frequency given magnitude one. order soften penalty near-correct predictions target gaussian-blurred frequency energy surrounding ground truth frequency decays standard deviation cents real numbers loss function optimized using adam optimizer learning rate best performing model selected training validation accuracy longer improves epochs epoch consists batches examples randomly selected training set. convolutional layer preceded batch normalization followed dropout layer dropout probability architecture training procedures implemented using keras order objectively evaluate crepe compare performance alternative algorithms require audio data perfect ground truth annotations. especially important since performance compared algorithms already high. light this cannot dataset medleydb since annotation process includes manual corrections guarantee perfect match annotation audio affected degree human subjectivity. guarantee perfectly objective evaluation must datasets synthesized audio perfect control resulting signal. datasets ﬁrst rwc-synth contains hours audio synthesized music database used evaluate pyin important note signals dataset synthesized using ﬁxed small number sinusoids meaning dataset highly homogenous timbre represents over-simpliﬁed scenario. evaluate algorithms realistic conditions second dataset collection monophonic stems taken medleydb re-synthesized using methodology presented uses analysis/synthesis approach generate synthesized track perfect annotation maintains timbre dynamics original track. dataset consists tracks instruments totaling hours audio henceforth referred mdb-stem-synth. train model using -fold cross-validation using train validation test split. mdb-stem-synth artistconditional folds order avoid training testing artist result artiﬁcially high performance artist album effects evaluation algorithm’s pitch estimation measured pitch accuracy chroma accuracy cent thresholds metrics measure proportion frames output output algorithm within cents ground truth. reference implementation provided eval compute evaluation metrics. compare crepe current state monophonic pitch tracking represented pyin swipe algorithms. examine noise robustness algorithm also evaluate pitch tracking performance degraded versions mdb-stem-synth using audio degradation toolbox four different noise sources provided white pink brown. noise actual recording sound crowded white noise random signal constant power spectral density frequencies. pink brown noise highest power spectral density frequencies densities fall decade respectively. used seven different signal-to-noise ratio values table shows pitch estimation performance tested datasets. rwc-synth dataset crepe yields close-toperfect performance error rate lower baselines order magnitude. high accuracy numbers encouraging achievable thanks highly homogeneous timbre dataset. order test generalizability algorithms timbrally diverse dataset evaluated performance mdb-stem-synth dataset well. notable degradation performance rwc-synth signiﬁcant baseline algorithms implying crepe robust complex timbres compared pyin swipe. finally algorithms compare scenarios deviation estimated pitch true value could detrimental table report lower evaluation tolerance thresholds cents well standard cents threshold reference. threshold decreased difference performance becomes accentuated crepe outperforming percentage points evaluation tolerance lowered cents. suggests crepe especially preferable even minor deviations true pitch avoided best possible. obtaining highly precise pitch annotations perceptually meaningful transcription analysis/resynthesis applications. noise robustness many applications like speech analysis mobile phones smart speakers live music performance. figure show pitch estimation performance affected additive noise present input signal. crepe maintains highest accuracy levels noise white noise levels except highest level pink noise. brown noise exception pyin’s performance almost unaffected noise. attributed fact summarize conﬁrmed crepe performs better cases performance varies depending spectral properties noise noise level higher indicates approach reliable reasonable amount additive noise. crepe also stable exhibiting consistently lower variance performance compared baseline algorithms. gain insight crepe model figure visualize spectra convolutional ﬁlters ﬁrst layer neural network histograms ground-truth frequencies right plot. noticeable ﬁlters learned rwc-synth dataset spectral density concentrated between ground-truth frequencies mostly indicates ﬁrst config. pitch tracking performance additive noise signals present. error bars centered average pitch accuracies span ﬁrst standard deviations. brown noise notable exception crepe shows highest noise robustness general. fig. fourier spectra ﬁrst-layer ﬁlters sorted frequency peak magnitude. histograms right show distribution ground-truth frequencies corresponding dataset. volutional layer model learns distinguish frequencies overtones rather fundamental frequency. ﬁlters focusing overtones also visible mdb-stem-synth peak frequencies ﬁlters range well distribution dataset case majority ﬁlters overlap ground-truth distribution unlike rwc-synth. possible explanation since timbre rwc-synth ﬁxed identical tracks model able obtain highly accurate estimate modeling harmonics. conversely timbre heterogeneous complex case mdbstem-synth model cannot rely solely harmonic structure requires ﬁlters capture periodicity directly addition harmonics. cases suggests neural network adapt distribution timbre frequency dataset interest turn contributes higher performance crepe compared baseline algorithms. mdb-stem-synth dataset contains tracks different instruments electric bass male singer common instruments occur tracks. figure plot performance crepe tracks respect instrument track. notable model performs worse instruments higher average frequencies performance also dependent timbre. crepe performs particularly worse tracks dizi chinese transverse ﬂute tracks came artist placed split. means fold dizi tracks test training validation sets contain single dizi track model fails generalize previously unseen timbre. instruments occur dataset performance decent timbres deviate instruments dataset. ﬂute violin although many tracks instrument training performance sound tested tracks high compared tracks instruments. performance piccolo tracks error dataset annotation inconsistent correct pitch range instrument. unsurprisingly model performs well test tracks whose timbre frequency range well-represented training set. paper presented novel data-driven method monophonic pitch tracking based deep convolutional neural network operating time-domain input crepe. showed crepe obtains state-of-the-art results outperforming pyin swipe datasets homogeneous heterogeneous timbre respectively. furthermore showed crepe remains highly accurate even strict evaluation threshold cents. also showed cases crepe robust added noise. ideally want model invariant transformations affect pitch changes distortion reverberation. invariance induced architectural design model translation invariance induced pooling layers model well deep image classiﬁcation models. however straightforward design model architecture speciﬁcally ignore pitch-preserving transformations. still intriguing problem build architecture achieve this could data augmentation generate transformed degraded inputs effectively make model learn invariance. robustness model could also improved applying pitch-shifts data augmentation cover wider pitch range every instrument. addition data augmentation various sources audio timbre obtained software instruments; nsynth example training dataset generated sound software instruments. pitch values tend continuous time crepe estimates pitch every frame independently without using temporal tracking unlike pyin exploits using enforce temporal smoothness. potentially improve performance crepe even enforcing temporal smoothness. future plan means adding recurrent architecture model could trained jointly convolutional front-end form convolutional-recurrent neural network rachel bittner justin salamon mike tierney matthias mauch chris cannam juan pablo bello medleydb multitrack dataset annotation-intensive research. proceedings ismir conference vol. juan bosch emilia g´omez melody extraction symphonic classical music comparative study mutual agreeproceedings ment humans algorithms conference interdisciplinary musicology matthias mauch chris cannam rachel bittner george fazekas justin salamon jiajie juan bello simon dixon computer-aided melody note transcription using tony software accuracy efﬁciency proceedings first international conference technologies music notation representation john dubnowski ronald schafer lawrence rabiner real-time digital hardware pitch detector ieee transactions acoustics speech signal processing vol. myron ross harry shaffer andrew cohen richard freudberg harold manley average magnitude difference function pitch extractor ieee transactions acoustics speech signal processing vol. paul boersma accurate short-term analysis fundamental frequency harmonics-to-noise ratio sampled sound proceedings institute phonetic sciences vol. matthias mauch simon dixon pyin fundamental frequency estimator using probabilistic threshold distributions acoustics speech signal processing ieee international conference ieee adrian knesebeck z¨olzer comparison pitch trackers real-time guitar effects proceedings international conference digital audio effects eric humphrey juan pablo bello rethinking automatic chord recognition convolutional neural networks machine learning applications international conference ieee vol. sebastian b¨ock markus schedl enhanced beat tracking context-aware neural networks proceedings international conference digital audio effects justin salamon rachel bittner jordi bonada juan jos´e bosch vicente emilia g´omez guti´errez juan bello analysis/synthesis framework automatic annotation multitrack datasets proceedings ismir conference rachel bittner brian mcfee justin salamon peter juan pablo bello deep salience representations tracking polyphonic music proceedings ismir conference sergey ioffe christian szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple prevent neural networks overﬁtting. journal machine learning research vol. franc¸ois chollet keras python deep learning library masataka goto hiroki hashiguchi takuichi nishimura ryuichi music database popular classical jazz music databases. proceedings ismir conference vol. justin salamon emilia g´omez daniel ellis ga¨el richard melody extraction polyphonic music signals approaches applications challenges ieee signal processing magazine vol. colin raffel brian mcfee eric humphrey justin salamon oriol nieto dawen liang daniel ellis colin raffel eval transparent implementation common metrics proceedings ismir conference matthias mauch sebastian ewert audio degradation toolbox application robustness evaluation proceedings ismir conference curitiba brazil accepted. brian mcfee eric humphrey juan pablo bello software framework musical data augmentation international society music information retrieval conference ismir. jesse engel cinjon resnick adam roberts sander dieleman douglas karen simonyan mohammad norouzi neural audio synthesis musical notes wavenet autoencoders arxiv preprint arxiv.", "year": "2018"}