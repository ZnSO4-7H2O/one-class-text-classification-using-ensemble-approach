{"title": "Extension of PCA to Higher Order Data Structures: An Introduction to  Tensors, Tensor Decompositions, and Tensor PCA", "tag": "eess", "abstract": " The widespread use of multisensor technology and the emergence of big data sets have brought the necessity to develop more versatile tools to represent higher-order data with multiple aspects and high dimensionality. Data in the form of multidimensional arrays, also referred to as tensors, arises in a variety of applications including chemometrics, hyperspectral imaging, high resolution videos, neuroimaging, biometrics, and social network analysis. Early multiway data analysis approaches reformatted such tensor data as large vectors or matrices and then resorted to dimensionality reduction methods developed for classical two-way analysis such as PCA. However, one cannot discover hidden components within multiway data using conventional PCA. To this end, tensor decomposition methods which are flexible in the choice of the constraints and that extract more general latent components have been proposed. In this paper, we review the major tensor decomposition methods with a focus on problems targeted by classical PCA. In particular, we present tensor methods that aim to solve three important challenges typically addressed by PCA: dimensionality reduction, i.e. low-rank tensor approximation, supervised learning, i.e. learning linear subspaces for feature extraction, and robust low-rank tensor recovery. We also provide experimental results to compare different tensor models for both dimensionality reduction and supervised learning applications. ", "text": "abstract. widespread multisensor technology emergence data sets brought necessity develop versatile tools represent higher-order data multiple aspects high dimensionality. data form multidimensional arrays also referred tensors arises variety applications including chemometrics hyperspectral imaging high resolution videos neuroimaging biometrics social network analysis. early multiway data analysis approaches reformatted tensor data large vectors matrices resorted dimensionality reduction methods developed classical two-way analysis pca. however cannot discover hidden components within multiway data using conventional pca. tensor decomposition methods ﬂexible choice constraints extract general latent components proposed. paper review major tensor decomposition methods focus problems targeted classical pca. particular present tensor methods solve three important challenges typically addressed dimensionality reduction i.e. learning linear subspaces feature extraction robust low-rank tensor recovery. also provide experimental results compare diﬀerent tensor models dimensionality reduction supervised learning applications. principal component analysis oldest widely used methods dimensionality reduction data science. goal reduce dimensionality data i.e. extract low-dimensional subspaces high dimensional data preserving much variability possible past decades thanks simple non-parametric nature used descriptive adaptive exploratory method numerical data various types. currently commonly used address three major problems data science dimensionality reduction large high dimensional data sets i.e. low-rank subspace approximation subspace learning machine learning applications robust low-rank matrix recovery missing samples grossly corrupted data however advances mostly limited vector matrix type data despite fact continued advances information sensing technology making large-scale multi-modal multi-relational datasets evercommonplace. indeed multimodal data sets commonly encountered huge variety applications including chemometrics hyperspectral imaging high resolution videos neuroimaging biometrics social network analysis applied higher order data sets standard vector matrix models shown inadequate capturing cross-couplings across diﬀerent modes burdened increasing storage computational costs therefore growing need type methods learn tensor data respecting inherent multi-modal structure multilinear dimensionality reduction subspace estimation. purpose survey article introduce well familiar methods vector type data tensors toward discussing extensions variants tensor type data. although many excellent review articles tutorials book chapters written tensor decomposition focus review article extensions pca-based methods developed address current challenges listed tensors. example provides fundamental theory methods tensor decomposition/compression focusing particular models parafac tucker models without much emphasis tensor network topologies supervised learning numerical examples contrast diﬀerent models. similarly focuses parafac tucker models special emphasis uniqueness representations computational algorithms learn models real data. cichocki hand mostly focus tensor decomposition methods data applications providing depth review tensor networks diﬀerent network topologies. current survey diﬀers ways. first focus current survey introduce methods address three main challenges application areas currently targeted tensor type data. therefore current survey reviews dimensionality reduction linear subspace learning methods tensor type data well extensions robust tensor type data. second current survey attempts give comprehensive concise theoretical overview diﬀerent tensor structures representations also provides experimental comparisons diﬀerent tensor structures dimensionality reduction supervised learning applications. order accomplish goals review three main lines research tensor decompositions herein. first present methods tensor decomposition aimed low-dimensional/low-rank approximation higher order tensors. early multiway data analysis relied reshaping tensor data matrix resorted classical matrix factorization methods. however matricization tensor data cannot always capture interactions couplings across diﬀerent modes. reason extensions two-way matrix analysis techniques nonnegative matrix factorization developed order better address issue dimensionality reduction tensors. reviewing basic tensor algebra section discuss extensions section particular review major tensor representation models including canonical decomposition also known parallel factor model tucker multilinear singular value decomposition tensor networks including tensor-train hierarchical tucker major topologies. methods discussed respect dimensionality reduction capabilities uniqueness storage requirements. section concludes empirical comparison several tensor decomposition methods’ ability compress several example datasets. second section summarize extensions linear subspace learning methods context tensors. include multilinear principal component analysis tensor rank-one decomposition tensor-train hierarchical tucker methods utilize models introduced section order learn common subspace collection tensors given training set. common subspace used project test tensor samples lower-dimensional spaces classify framework found applications supervised learning settings particular face object recognition images collected across diﬀerent modalities angles next section address issue robust low-rank tensor recovery grossly corrupted noisy higher order data diﬀerent tensor models introduced section finally section provide overview ongoing work area large tensor data factorization computationally eﬃcient implementation existing methods. d-mode dth-order tensor simply d-dimensional array complex valued data cn×n×···×nd given dimension sizes given this entry tensor indexed index vector ×···× entry indexed always denoted aii...id entry position index vector tensor always referred jth-mode remainder paper vectors always bolded matrices capitalized tensors order potentially italicized tensor entries/scalars written lower case. fibers slices sub-tensors. encountered higher order tensor often beneﬁcial look correlations across diﬀerent modes. reason many lower-order sub-tensors contained within given higher-order tensor given special names thereby elevated special status. subsection deﬁne these. fibers -mode sub-tensors given d-mode tensor cn×n×···×nd. speciﬁcally mode-j ﬁber -mode sub-tensor indexed mode given choices {j}. mode-j ﬁber denoted paper slices always -mode sub-tensors given d-mode tensor cn×n×···×nd. particular mode-j slice -mode sub-tensor indexed mode given choice mode-j slice denoted easy always mode-j slices given cn×n×···×nd. example consider -mode tensor cm×n×p. mode- slice given -mode sub-tensor ai=k cm×n. mode- slices figure mode- slices viewed. vectorization cn×n×···×nd always reshape vector denoted cnn···nd. process accomplished numerically e.g. recursively vectorizing last modes according rowrapidly retrieved formula d-mode tensor partitioning modes diﬀerent column subsets modes often considered mode-j variants mentioned hierarchical decomposition methods.) ∈\\{j} note columns ordered varying index largest non-j mode fastest followed varying second largest non-j mode second fastest etc.. figure mode- matricization cn×n mode- matricization mode- slice note still consider mode- slice ai=c indexed above. result e.g. considered meaningful considered meaningless that e.g. meaningless. even though ai=c modes still consider ﬁrst third modes though potentially counterintuitive ﬁrst notational convention simplify interpretation expressions like going forward. tensor products j-mode products. occasionally desirable build one’s higher order tensor using lower order tensors. particularly true builds using vectors part e.g. parafac/candecomp decomposition techniques reason worth stating basic properties mainly mode-j products bilinear commute diﬀerent modes combine reverse order mode. following simple lemma formally lists important properties. another represent tensor products deﬁne index contractions. index contraction possible values repeated indices tensors. example maβ= aαβbβγ thought contraction index also conαβδνµ= aαβδσbβγµcδνµωeνρα indices contracted produce four-mode tensor. representation section focus tensor decomposition methods provide low-rank approximations multilinear datasets reducing complexity similar pca/svd matrices. advantages using multiway analysis two-way analysis terms uniqueness robustness noise computational complexity shown many studies section review commonly used tensor models representation compression present results uniqueness storage complexities. empirically evaluate compression versus approximation error performance several methods three diﬀerent higher order datasets. positive integer weight rank-one tensor factor mode unit norm alternatively represented mode products diagonal core tensor entries factor matrices main restriction parafac model factors across diﬀerent modes interact factorwise. example -mode tensor factor corresponding ﬁrst mode interacts factors second third modes. uniqueness parafac model. given tensor rank parafac decomposition essentially unique i.e. factor matrices unique common permutation scaling columns given number terms. alternatively kruskal provided results uniqueness -mode depending matrix k-rank diﬀerence input tensor approximation minimized. attractive method since ensures improvement solution every iteration. however practice existence large amount noise high order model prevent converge global minima require several thousands iterations diﬀerent methods proposed improve performance accelerate convergence rate algorithms number particular techniques exist line search extrapolation methods compression instead alternating estimation all-at-once algorithms algorithm conjugate gradient algorithm nonnegative damped gauss-newton algorithms fast studied deal problems slow convergence cases. another approach consider decomposition joint diagonalization problem nd]s square factor matrices core tensor matrices obtained ...×d denotes transpose factor matrix along mode. common tucker decomposition assume rank less compression multilinear-rank-r approximation dth-order tensor rn×n...×nd represented using parameters contrast parafac tucker models allow interactions factors obtained across modes core tensor includes strength interactions. summary tucker sum-of-outer products models argue general form contains other. however distinguishes uniqueness. tucker decomposition unique bases subspaces chosen arbitrarily choice compensated within core tensor. reason tucker model unique unless additional constraints placed and/or core tensor. constraints orthogonality nonnegativeness sparsity independence smoothness imposed factor matrices obtain unique decompositions higher order special case tucker decomposition obtained adding orthogonality constraint component matrices. hosvd factor matrices left singular vectors ﬂattening hosvd n-rank approximation obtained truncating orthogonal factor matrices hosvd resulting truncated hosvd. orthogonality core tensor hosvd unique speciﬁc multilinear rank. computational issues shown optimization problem solved approach iteratively method known higher-order orthogonal iteration many applications hosvd considered suﬃciently good serve initial value algorithms ﬁnding best approximation tensor networks. tensor decompositions parafac tucker decompose complex high dimensional data tensors factor tensors matrices. tensor networks hand represent higher-order tensor sparsely interconnected lower-order tensors typically rd-order th-order tensors called core provide computational storage beneﬁts formally tensors some indices contracted according pattern. contraction open indices results another tensor. important property total number operations must done obtain ﬁnal result contraction depends order indices contracted i.e. given tensor many diﬀerent representations ﬁnding optimal order indices contracted crucial step eﬃciency decomposition. optimized topologies yield simpliﬁed convenient graphical representations higher-order tensor data commonly encountered tensor network topologies include hierarchical tucker tree tensor network state tensor train tensor networks cycles projected entangled pair states projected entangled pair operators uniqueness noted above given tensor many diﬀerent representations general unique representation. uniqueness various models diﬀerent constraints still active ﬁeld research hierarchical tensor decomposition. reduce memory requirements tucker decomposition hierarchical tucker decomposition proposed hierarchical tucker decomposition recursively splits modes based hierarchy creates binary tree containing subset modes node factor matrices obtained matricization tensor corresponding subset modes node. however matricization diﬀerent mode-n matricization tensor rows correspond modes columns store indices remaining modes. constructed tree structure yields hierarchy amongst factor matrices whose columns span children exists transfer matrix rt×rt. assuming ht-rank-r approximation rn×n×...×nd requires storing leaf nodes nodes parameters tensor train decomposition. tensor train decomposition interpreted special case nodes underlying tensor network connected cascade train. proposed compress large tensor data smaller core tensors model allows users avoid exponential growth tucker model provides eﬃcient rrm−×rm imth lateral slice core rrm−×nm×rm tt-rank computational issues decomposition obtained sequence svds. first obtained mode- matricization rank note that rr×nn...nd. rrn×n...nd reshaped version then rr×n×r obtained reshaping left-singular vectors rrn×r rank rank rr×n...nd. repeating procedure core tensors obtained sequence decompositions speciﬁc matricizations storage complexity ttrank-r approximation dth-order tensor rn×n...×nd important note format known matrix product state representation open boundary conditions quantum physics community advantages tt/mps model simpler practical implementation binary tree needs determined simplicity computation computational eﬃciency although format used widely signal processing machine learning suﬀers couple limitations. first model requires rank- constraints border factors i.e. matrices. second importantly multiplications cores permutation invariant requiring optimization ordering using procedures mutual information estimation drawbacks recently addressed tensor ring decomposition decomposition removes unit rank constraints boundary cores utilizes trace operation decomposition removes dependency core order. tensor singular value decomposition t-svd deﬁned order tensors based t-product algebra behind t-svd diﬀerent regular multilinear algebra depends linear operators deﬁned third-order tensors. approach third-order tensor rn×n×n rn×n×n orthogonal tensors respect operation n×n×n tensor whose rectangular frontal slices diagonal entries called singular values denotes t-product deﬁned circular convolution fourier domain. t-svd deﬁnes notion tubal rank tubal rank deﬁned number non-zero singular tubes moreover unlike tucker models truncated frobenius norm error. rank-r approximation rn×n×n represented using entries. empirical comparison diﬀerent tensor decomposition methods. section candecomp/parafac tucker hosvd decompositions compared terms data reduction rate normalized reconstruction error. data sets used purpose data database contains images taken individual diﬀerent several software packages used generate results. tucker methods evaluated using tensorlab package structure detection exploitation option disabled order avoid complications larger datasets number rank- tensors decomposition given input parameter evaluate approximation error respect compression rate. hooi input cell array factor matrices diﬀerent modes used initialization main algorithm. generate initial factor matrices hosvd computed tool original version code slightly modiﬁed input parameter disable structure exploitation option ﬁeld options.exploitstructure false cpd.m. large-scale memory threshold also changed mtkrprod.m prevent large intermediate solutions causing memory issues. tensor-train toolbox used generate results tensor-train method. input parameter accuracy data stored tensor-train format. similarly hierarchical tucker toolbox used generate results hierarchical tucker method. input parameter maximal hierarchical rank. diﬀerent ranges input parameters method summarized table parameters chosen yield comparable reconstruction errors compression rates across diﬀerent methods. following observations made figure diﬃculty converging input rank parameter large especially larger data sets. however provides best compression performance converges. data speciﬁcally perform well compression ranges. however outperforms methods rates. especially case noted largest number modes tensors considered paper tensor network models expected perform better higher number modes. hosvd hooi provide similar results data sets hooi performing slightly better. generally appear provide best compression versus error results methods regimes work well exception coil dataset outperform higher compression rates. data compression methods used help reduce size data. data size compared terms bytes needed represent method’s output versus input. precision used data equal measuring total number elements input output arrays. figure experimental results. compression versus error performance hooi hosvd data set. data results methods. data results without higher compression values. coil- data results methods. coil- data without higher compression values. section review recent developments supervised learning using tensor subspace estimation methods. methods discussed section employ tensor decomposition methods tensor subspace learning extract low-dimensional features. ﬁrst start giving overview trivial based tensor vectorization. next review pca-like feature extraction methods extended tucker tensor order facial recognition computer vision image processing tasks several variants methods). applications preprocessed pictures individuals treated individual -mode tensors. order help provide additional information individual might imaged several diﬀerent conditions collection individual’s images across additional data individual. objective would perform across individuals’ face image data order come reduced face model could later used various computer vision tasks optimal computing singular value decomposition rnn...nd×m matrix whose columns vectorized image tensors rnn...nd. result obtain vectorized eigenface basis rnn...nd reshaped back image tensor rn×n×···×nd. though conceptually simple approach still encounters signiﬁcant computational challenges. particular total dimensionality tensor extremely large making computation expensive storage basis tensors ineﬃcient. challenges involving computation lenges involving eﬃcient storage computation high dimensional tensors obtained multilinear principal component analysis ﬁrst tensor approach discuss mpca closely related tucker decomposition dth-order tensor mpca independently discovered several diﬀerent settings last decades. ﬁrst mpca variants appear signal processing community focused ndorder tensors e.g. dpca improving image classiﬁcation database querying applications methods later generalized handle tensors order subsequent work tailored general methods several diﬀerent applications including variants based non-negative factorizations audio engineering weighted versions signal classiﬁcation online versions tracking variants binary tensors incremental versions streamed tensor data mpca performs feature extraction determining multilinear projection captures original tensorial input variation similar goals vector type data. solution iterative nature proceeds decomposing original problem series multiple projection subproblems. mathematically general mpca methods solve following problem additional details) given higher-order single remaining free mode’s projection matrix order minimize much possible. optimizing single free mode accomplished exactly computing matricized version tensor data. next iteration previously free mode ﬁxed diﬀerent mode’s projection matrix optimized instead etc.. although theoretical guarantees convergence mpca total scatter non-decreasing function mpca converges fast shown multiple empirical studies. also shown mpca reduces dpca recently mpca generalized uniﬁed framework called generalized tensor includes mpca robust mpca simultaneous low-rank approximation tensors robust slrat special cases. generalization obtained considering diﬀerent cost functions tensor approximation i.e. change diﬀerent ways centering data. couple important points distinguish mpca hosvd/tucker model discussed section first goal mpca common low-dimensional subspace across multiple tensor objects resulting projections capture maximum total variation input tensorial space. hosvd hand focuses low-dimensional representation single tensor object goal obtaining low-rank approximation small reconstruction error i.e. compression. second issue centering ignored tensor decomposition focus predominantly tensor approximation reconstruction. approximation/reconstruction problem centering essential mean main focus attention. however machine learning applications solutions involve eigenproblems non-centering potentially aﬀect per-mode eigendecomposition lead solution captures variation respect origin rather capturing true variation data. finally initialization usually done using hosvd ﬁrst columns full tucker decomposition kept equivalent hosvd-based low-rank tensor approximation ﬁnal projection matrices diﬀerent solution hosvd. tensor rank-one decomposition mentioned above mpca approximates given data subspaces reminiscent tucker decomposition similarly trod approach formulates tensors terms subspaces reminiscent parafac/candecomp decomposition given higher-order data subspace minimizing found using greedy least squares minimization procedure iteratively compute underlying tensor objects conform rank model method suﬀers high computational complexity slow convergence rate tensor train tensor train hierarchical tucker representation shown provide alleviations storage requirements tucker representation recently decomposition methods extended feature extraction machine left unfolding operator resulting matrix takes ﬁrst mode indices indices third mode indices column indices matrix concatenates vectorizations sample points representation matrix. wang propose approach based successive svd-algorithm computing decomposition followed thresholding singular values. obtained subspaces across mode left-orthogonal mixed-canonical form tensor train decomposition i.e. product left right common factors satisfy leftright-orthogonality conditions. optimizing positions tensor modes obtain reduced rank representation tensor extract features. implementation algorithm relies successive sequences svds followed thresholding. finally subspaces across mode extracted similar minimizes mean square error. estimating dimension tree subspaces hard problem current applications considered balanced tree tt-tree using suboptimal algorithm proposed algorithm variation hierarchical computing representation single tensor object. algorithm takes collection tensors computes hierarchical space using given tree. subspaces corresponding node computed using truncated node unfolding transfer tensors computed using projections tensor product subspace node’s children. empirical results indicate tt-pca performs better ht-pca terms classiﬁcation error given number training samples tensor embedding methods. past decade embedding methods developed feature extraction dimensionality reduction various machine learning tasks extended tensor objects methods take data directly form tensors allow relationships dimensions tensor representation eﬃciently characterized. moreover methods estimate intrinsic local geometric topological properties manifold embedded tensor space. examples include tensor neighborhood preserving embedding tensor locality preserving projection tensor local discriminant embedding methods optimal projections preserve local topological structure manifold embedded tensor space determined iterative algorithms. early work focused tucker model deﬁne optimal projections recently tnpe extended model empirical comparison tensor approaches. section evaluate performance tensor based tensor models; tucker tensor train resulting mpca tt-pca approaches currently methods widely used supervised learning. assess performance methods binary classiﬁcation performed three data sets including coil- eyfdb mnist databases. coil- data reshaped original structure i.e. -dimensional tensor samples class available. objects labels picked angle lighting conditions used generate images size leads tensor r××× containing samples classes. however classiﬁcation diﬃcult. therefore data reshaped r××× providing mnist data contains pixel images handwritten digits digit consisting samples resulting -mode tensor r××. digits case classes picked randomly nearest neighbor classiﬁer used evaluating classiﬁer accuracy. training samples holdout samples training ratio varied value threshold also varied eﬀect number training samples compression ratio classiﬁcation success rate addition data classiﬁcation repeated multiple times randomly selecting training samples. classiﬁcation experiments coil- eyfdb mnist data repeated times respectively. average standard deviation values viewed tables observed coil- eyfdb data performs better compared mpca across diﬀerent sizes training compression ratios especially smaller training ratios case mnist data performance approaches almost identical mpca performing slightly better. fact mnist tucker model better variations across samples within class approximated well number factor matrices across mode. results illustrate tensor network models possible obtain high compression learn discriminative low-dimensional features simultaneously. intrinsic limitation vector tensor based dimensionality reduction methods sensitivity presence outliers current decompositions focus getting best approximation tensor minimizing frobenius norm error. practice underlying tensor data often low-rank even though actual data outliers arbitrary errors. thus possible robustify tensor decompositions reconstructing low-rank part corrupted version. recent years seen emergence robust tensor decomposition methods early attempts focused solving robust tensor problem using matrix methods i.e. applying robust matrix slice tensor matrix obtained ﬂattening tensor. however matrix methods ignore tensor algebraic constraints tensor rank diﬀers matrix rank constraints. recently problem low-rank robust tensor recovery addressed within hosvd t-svd frameworks. text model. given input tensor goal recover σiui low-rank -mode tensor model sparse tensor. thogonal low-rank tensors proposed algorithm uses non-convex rank estimate updated eigenvector computation sparse estimate updated thresholding residual algorithm proceeds stages rank projection l-rank space hard thresholding residual considered algorithm proven rank-r orthogonal tensor block sparse corruption tensor robust hosvd. context hosvd robust low-rank tensor recovery methods rely principal component pursuit proposed method referred higher-order robust direct application rpca deﬁnes rank tensor based problem solved using alternating direction augmented lagrangian method. second model mixture model requires tensor component low-rank matrix relaxed version singleton model requires tensor low-rank modes simultaneously. deﬁnition tensor rank leads following convex optimization problem sparse corruption tensor well dense noise tensor i.e. low-rank tensor sparse corruption tensor locations nonzero entries unknown magnitudes nonzero entries arbitrarily large noise tensor i.i.d observation model authors consider general linear observation model obtain estimation error bounds tensor i.e. low-rank sparse tensor. equivalent problem solved using admm. robust t-svd. recently tensor rpca problem extended t-svd induced tensor tubal rank tensor nuclear norm low-rank tensor recovery problem posed minimizing tubal rank component instead nuclear norms unfolded matrices. results convex optimization problem. prove certain incoherence conditions solution convex surrogate tucker rank replaced tubal rank perfectly recovers low-rank sparse components provided tubal rank using outlier-robust tensor method simultaneous low-rank tensor recovery outlier detection. or-tpca recovers low-rank tensor component minimizing tubal main advantage t-svd based robust tensor recovery methods respect robust hosvd methods tensor nuclear norm instead tucker rank optimization problem. shown tensor nuclear norm equivalent nuclear norm block circulant matrix preserves relationships among modes better depicts lowpaper provided overview main tensor decomposition methods data reduction compared performance diﬀerent size tensor data. also introduced tensor methods extensions learning robust low-rank tensor recovery applications. particular note empirical results section illustrate often used tensor decomposition methods depend heavily characteristics data thus point need improved decomposition algorithms methods combine advantages diﬀerent existing methods larger datasets. recent research focused techniques block term decompositions order achieve results. btds admit modeling complex data structures represent given tensor terms rank factors necessarily rank one. enhances potential modeling general phenomena seen combination tucker decompositions. availability ﬂexible computationally eﬃcient tensor representation tools also impact ﬁeld supervised unsupervised tensor subspace learning. even though focus paper mostly tensor data reduction methods learning approaches important note recent years seen growth learning algorithms tensor type data. simple extensions subspace learning methods vector type data tensors whereas others extend manifold learning tensor type data computer vision applications. dimensionality tensor type data increases also growing need hierarchical tensor decomposition methods visualization representational purposes. particular area large volumetric data visualization tensor based multiresolution hierarchical methods tamresh considered area data reduction denoising multiscale hosvd methods proposed increase tensor data dimensionality also require development parallel distributed implementations diﬀerent decomposition methods diﬀerent strategies large-scale matrices encountered vector tensor type data already considered eﬃcient implementation decomposition methods.", "year": "2018"}