{"title": "A joint separation-classification model for sound event detection of  weakly labelled data", "tag": "eess", "abstract": " Source separation (SS) aims to separate individual sources from an audio recording. Sound event detection (SED) aims to detect sound events from an audio recording. We propose a joint separation-classification (JSC) model trained only on weakly labelled audio data, that is, only the tags of an audio recording are known but the time of the events are unknown. First, we propose a separation mapping from the time-frequency (T-F) representation of an audio to the T-F segmentation masks of the audio events. Second, a classification mapping is built from each T-F segmentation mask to the presence probability of each audio event. In the source separation stage, sources of audio events and time of sound events can be obtained from the T-F segmentation masks. The proposed method achieves an equal error rate (EER) of 0.14 in SED, outperforming deep neural network baseline of 0.29. Source separation SDR of 8.08 dB is obtained by using global weighted rank pooling (GWRP) as probability mapping, outperforming the global max pooling (GMP) based probability mapping giving SDR at 0.03 dB. Source code of our work is published. ", "text": "contrast approaching moving away car. many audio datasets contain tags presence absence audio events audio recordings. referred weakly labelled data many audio tagging datasets weakly labelled often larger strongly labelled datasets utilize methods including joint detectionclassiﬁcation model attention localization model multi-instance learning methods used. source separation used sound event detection unsupervised source separation methods including computation audio scene analysis uses timefrequency masking emulate human performs source separation supervised source separation methods need clean sources training achieved state-of-the-art performance. paper joint separation-classiﬁcation model proposed train source separation model wld. proposed framework consists parts. ﬁrst part separation mapping representation audio signal segmentation masks audio event. second part classiﬁcation mapping segmentation mask corresponding audio tag. source separation stage separated sources different classes obtained segmentation masks. remainder paper organized follows section discusses convolutional neural network. section proposes source separation framework. section shows experimental results. section concludes proposes future work. convolutional neural networks used initially image classiﬁcation recently successful audio processing including speech recognition audio classiﬁcation audio classiﬁcation waveform transformed representations treated image input consists several convolutional layers contains several trainable ﬁlters trained learn local patterns feature map. downsampling usually follows convolutional laysource separation aims separate individual sources audio recording. sound event detection aims detect sound events audio recording. propose joint separation-classiﬁcation model trained weakly labelled audio data tags audio recording known time events unknown. first propose separation mapping time-frequency representation audio segmentation masks audio events. second classiﬁcation mapping built segmentation mask presence probability audio event. source separation stage sources audio events time sound events obtained segmentation masks. proposed method achieves equal error rate outperforming deep neural network baseline source separation obtained using global weighted rank pooling probability mapping outperforming global pooling based probability mapping giving source code work published. sound event detection aims detect speciﬁc audio events audio recording. many applications daily life example detecting baby home detecting tapping sound ofﬁce monitoring alarm gunshot public area. hand source separation aims separate individual sources recording used many current models trained using supervised learning methods supervised learning methods need labelled onset offset time audio events call strongly labelled data labelling time consuming difﬁcult scale addition onset offset time audio events ambiguous fade fade effect example aration stage representation input waveform passed mapping segmentation masks. input representation multiplied segmentation mask obtain separated representation event corresponding audio tag. inverse transform applied separated representation audio using phase original waveform obtain separated waveform audio finally result audio event obtained corresponding segmentation masks. apply spectrogram input representation good representation audio tagging apply model separation mapping modeled model shown remove downsampling layers keep resolution segmentation mask input representation. high resolution segmentation mask useful source separation. number feature maps last convolutaional layer number audio events separate followed. sigmoid nonlinearity applied feature maps obtain segmentation ensure values segmentation masks segmentation mask representation similar class activation mapping weak image localization classiﬁcation mapping maps segmentation mask presence probability corresponding tag. classiﬁcation mapping modeled example global pooling global average pooling global weighted rank pooling represents c-th segmentation mask indexes time frequency bin. returns highest value feature map. performs well classiﬁcation tends underestimate units events segmentation mask unit highest value passed next layer reduce size feature maps. finally global pooling feature usually used select prominent unit feature followed fully connected neural network classiﬁcation section joint separation-classiﬁcation model trained proposed. idea related object localization weakly labelled images labels image known location objects unknown. class activation mapping applied highlight class-speciﬁc discriminative regions localize objects weakly labelled data. similar weakly labelled image data many audio datasets contain tags audio recording happening time events unknown. proposed separation-classiﬁcation model shown fig. input audio waveform transformed time-frequency representation spectrogram spectrogram. simplify notation abbreviate ﬁrst part model separation mapping input representation segmentation masks number audio tags segmentation mask k-th audio tag. values segmentation mask between source separation. mapping parametrized trainable parameters. second part model classiﬁcation mapping segmentation mask corresponding audio represents presence probability k-th event audio recording. compound model mapping input representation audio tags training phase model trained end-to-end sepfig. convolutional neural network based weak source separation. spectrogram used representation. separation mapping modeled cnn. classiﬁcation mapping applied segmentation mask obtain prediction audio tags. separation stage separated waveforms obtained segmentation masks. task dcase data challenge investigate extract several rare audio events dataset rare audio events second clips acoustic scene dataset. altogether clips created training single labelled multilabelled. presence absence audio events audio clip known. audio mixtures converted monaural sampling rate khz. spectrograms frequency bins used representation. fourier transform hamming window size overlap samples used ensure frames seconds clip. apply visual geometry group like consists convolutional layers. layer consists feature maps followed batch normalization relu nonlinearity. dropout rate applied regularize overﬁtting. value gwrp hyper-parameters chosen empirically affect result much. learned segmentation masks using different classiﬁcation mappings visualized ﬁrst column shows spectrogram babycry glassbreak gunshot. second column shows ideal binary mask audio events. column shows segmentation masks learned using gwrp classiﬁcation mapping respectively. observed tends underestimate presence audio events segmentation mask. gwrp performs better learning segmentation mask dataset. table shows separation results different audio tags evaluated results without separation listed baselines. gwrp performs better terms babycry glassbreak gunshot background without separation gap. table shows source separation using proposed model outperforms signiﬁcantly baseline withseparation. table also shows number time frames multiplied number frequency bins. contrast averages values units segmentation mask tends overestimate events segmentation mask segmentation masks obtained model contains presence audio events representation hence achieved sound event segmentation domain. paper simply average frequency axis obtain time axis. section apply proposed model modiﬁed detection rare audio sound events dataset task dcase challenge dataset consists rare events including babycry gunshot glassbreak. background sounds come acoustic scene dataset fig. visualization segmentation masks using different global pooling strategy. ﬁrst column shows spectrogram babycry glassbreak gunshot sound noisy background. second column shows ideal binary mask. third ﬁfth column shows segmentation masks learned using global pooling global average pooling global weighted rank pooling respectively. table shows frame wise sound event detection equal error rate using different global pooling strategies. gwrp outperforms baselines gmp. results correspondent visualization segmentation masks table published source code work. paper joint separation-classiﬁcation model presented sound event detection source separation. separation mapping input time-frequency representation segmentation masks classiﬁcation mapping segmentation mask audio proposed. obtain frame wise sound event detection outperforms baseline average source separation using global weighted rank pooling compared using global pooling. future research improving source separation quality using model. research supported epsrc grant ep/n/ making sense sounds research scholarship china scholarship council thank sacha krstulovic giacomo ferroni adrian stepien audio analytic discussions audio event detection. valenzise gerosa tagliasacchi antonacci sarti scream gunshot detection localization audio-surveillance systems ieee conference advanced video signal based surveillance ieee heittola mesaros virtanen eronen sound event detection multisource environments using source separation machine listening multisource environments foster sigtia krstulovic barker plumbley chime-home dataset sound source recognition domestic environment ieee workshop applications signal processing audio acoustics ieee mesaros heittola diment elizalde shah vincent virtanen dcase challenge setup tasks datasets baseline system proceedings detection classiﬁcation acoustic scenes events workshop kong huang wang plumbley attention localization based deep convolutional recurrent model weakly supervised audio tagging arxiv preprint arxiv. zhou khosla lapedriza oliva torralba learning deep features discriminative loproceedings ieee conference calization computer vision pattern recognition oquab bottou laptev sivic object localization free?-weakly-supervised learning convolutional neural networks proceedings ieee conference computer vision pattern recognition ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning", "year": "2017"}