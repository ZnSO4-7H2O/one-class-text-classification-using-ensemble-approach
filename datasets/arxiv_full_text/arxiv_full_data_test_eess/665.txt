{"title": "Efficient and Accurate MRI Super-Resolution using a Generative  Adversarial Network and 3D Multi-Level Densely Connected Network", "tag": "eess", "abstract": " High-resolution (HR) magnetic resonance images (MRI) provide detailed anatomical information important for clinical application and quantitative image analysis. However, HR MRI conventionally comes at the cost of longer scan time, smaller spatial coverage, and lower signal-to-noise ratio (SNR). Recent studies have shown that single image super-resolution (SISR), a technique to recover HR details from one single low-resolution (LR) input image, could provide high-quality image details with the help of advanced deep convolutional neural networks (CNN). However, deep neural networks consume memory heavily and run slowly, especially in 3D settings. In this paper, we propose a novel 3D neural network design, namely a multi-level densely connected super-resolution network (mDCSRN) with generative adversarial network (GAN)-guided training. The mDCSRN quickly trains and inferences and the GAN promotes realistic output hardly distinguishable from original HR images. Our results from experiments on a dataset with 1,113 subjects show that our new architecture beats other popular deep learning methods in recovering 4x resolution-downgraded im-ages and runs 6x faster. ", "text": "abstract. high-resolution magnetic resonance images provide detailed anatomical information important clinical application quantitative image analysis. however conventionally comes cost longer scan time smaller spatial coverage lower signal-to-noise ratio recent studies shown single image super-resolution technique recover details single low-resolution input image could provide high quality image details help advanced deep convolutional neural networks however deep neural networks consume memory heavily slowly especially settings. paper propose novel neural network design namely multi-level densely connected super-resolution network generative adversarial network ‚Äìguided training. mdcsrn trains inferences quickly promotes realistic output hardly distinguishable original images. results experiments dataset subjects shows architecture outperforms popular deep learning methods recovering resolutiondowngraded images runs faster. high spatial resolution produces detailed structural information benefiting clinical diagnosis decision making accurate quantitative image analysis. however hardware physics limitations high-resolution imaging comes cost long scan time small spatial coverage signal noise ability restore image single low-resolution input would potentially overcome drawbacks. therefore single image super-resolution attractive approach requires scan provide output without extra scan time. challenging problem underdetermined nature infinite number images produce image resolution degradation. makes difficult accurately restore texture structural details. large portion previous methods frame convex optimization problem find plausible solution balancing regularization terms however regularization terms require priori knowledge image distribution often based experimental assumptions. popular constraints like total variation implicitly assume image piecewise constant problematic images many local details tiny structures. hand learning-based approaches require well-defined priors. especially deep learning-based techniques shown great improvement sisr images abundant details non-linearity extraordinary ability imitate accurate transformation difficult cases. super-resolution convolutional neural networks recent faster-srcnns draw attention showed simple structured cnns produce outstanding sisr results natural images. however previous adapted deep-learning approaches fully solve puzzle medical image problem. first many medical images volumes previous cnns work slice slice discarding information continuous structures third dimension. second models parameters models raising challenge memory consumption computational expenses making less practical. finally widely used optimization objective pixel/voxel-wise error like mean squared error model estimation reference mentioned derivative peak signal noise ratio directly represent visual quality restored images. thus using target leads overall blurring perceptual quality. paper propose multi-level densely connected super-resolution networks fully solve problems. utilizing densely connected network mdcsrn extremely light-weight. optimized intensity difference provides state-of-art performance keeping model much smaller faster. trained generative adversarial network improves further outputting sharper realistic-looking images. proposed sisr neural network model aims learn image prior inversely mapping image reference image. model takes images produce images. training reference used guide optimization model‚Äôs parameters. deployment images generated model based input details provided followings background resolution downgrading process image image presented function causing loss resolution. sisr process find sisr approach three different steps optimized together feature extraction manifold learning image reconstruction. training difference reconstructed images ground truth images used adjust reconstruction layer restore better images manifold also guide extraction accurate image features. mingling different components makes possible neural network achieve state-of-art performance among sisr techniques training network intuitive optimize reconstruction minimizing voxel wise difference absolute difference mean square error however minimizing loss leads solutions resemble voxel-wise average possible candidates penalize formation artificial image features neighbor patch level. thus output tends over-blurred implausible human eye. better optimization incorporated idea ledig generative adversarial network ‚Äìbased loss function. framework networks generator discriminator basic idea train produce images rich details simultaneously training distinguish given image either real generated. training good classifier separate real generated images generate realistic looking images according advantage using optimized without predesigned loss function specific task. sisr srgan proposed showed adding gan‚Äôs loss guide training yields high perceptual quality. however training presents challenges. training must balanced evolve together. either becomes strong training fail learn nothing natural images effort made stabilize training process. however approaches greatly rely network structure described newer architectures like densenet. stabilize training process wasserstein authors observed failure training optimization toward kullbackleibler divergence real generated probability. little overlap them common early stage training gradient discriminator vanish training stall. address issue wgan proposed. loss function approximately optimizes earth mover distance always guide generator forward. wgan enables almost failfree training produces quality good vanilla gan. additionally distance real generated images regarded indicator image quality. work used wgan additional guiding training. need efficient super-resolution network shown super-resolution models outperforming counterparts large margin thanks fact model directly learns structure volumetric images however significant drawback model deep learning model usually much larger number parameters extra dimensions convolutional filters. example relatively shallow fsrcnn recently densenet shown using dense skip connections dramatically reduce network size maintaining state-of-art performance natural image classification. even memory-efficient densenets many parameters constructed basic idea densely connection densenet applied here also include architecture uses extra level skip connections. helps reduce parameter number also speeds computation. discuss detailed design mdcsrn following section. fig. architecture denseblock convolutions mdcsrn-gan network. mdcsrn. first convolutional layer outputs feature maps compressor shrinks feature maps convolution. final reconstruction layer another convolution. identical srgan except batchnorm replaced layernorm suggested wgan-gp. recent study shows densely connected super-resolution network single denseblock already capable capturing image features restoring super-resolution images outperforming state-of-art techniques. improvement network performance required make deeper model catch complex information process. however memory consumption densenet increases dramatically number layers increases makes feasible train deploy deeper dcsrn. fig. denseblock takes output previous denseblock directly connected reconstruction layer following principle densenet. skip connections provide direct access former layers including input enables uninterrupted gradient flow proven efficient less overfit. however unlike original densenet pooling layer mdcsrn mdcsrn make full information full resolution. another improvement convolutional layer compressor following denseblocks. attribute empower deep learning models generalize well model information compression forces model learn universal features avoid overfitting. design compressors bottleneck network width denseblock. expected provide least benefits reduce memory consumption hyperbolically linearly dependent depth; equally weight denseblock preventing later denseblocks dominating network parameters thereby forcing network overlook local image features central super-resolution task. design loss function work utilized gradient penalty variants wgan namely wgan-gp speed training convergence. loss function parts intensity loss ùëôùëúùë†ùë† gan‚Äôs discriminator loss ùëôùëúùë†ùë† hyperparameter used absolute difference converting image k-space applying fft; downgrading resolution truncating outer part k-space factor converting back image space applying inverse linearly interpolating original image size. mimics actual acquisition images scanners. dataset data preparation better demonstrate generalization deep learning model used large publicly accessible brain structural database human connectome project. images total subjects acquired siemens platform using -channel head coil multiple centers. images come high spatial resolution isotropic matrix size high-quality ground truth images provide detailed small structures perfect case project. whole dataset split training validation evaluation test samples subject. subjects image patches appear twice different subsets. validation used monitoring training process evaluation used hyper-parameters selection. test final performance evaluation avoid fine-tuning model favorable test data. original images used ground-truth images degraded used exact process patching data augmentation however merged patches without overlapping makes model even faster results less blurring. left margin pixels avoid artifacts edge. training parameters experiment setting models implemented tensorflow workstation nvidia gpu. denseblock mdcsrn setting similar dcsrn convolutional layers filter size growth rate comparison picked relatively small network fsrcnn complicated state-of-art srresnet selected hyper-parameters according fsrcnn extended convolution fsrcnn srresnet. non-gan networks adam optimizer learning rate used minimize loss function batch size trained steps significant improvement afterward. experiments transfer weights well-trained mdcsrn non-gan training initial first steps trained discriminator only. every steps training discriminator trained generator once; every steps train discriminator extra step alone makes sure discriminator always well-trained suggested wgan. adam optimizer used optimize network steps little improvement that. demonstrate effectiveness mdcsrn compared dcsrn made four different network setups varied block number unit number. network single -unit denseblock annotated network four denseblocks dense-units annotated respectively. used three image metrics subject-wise average structural similarity index peak signal noise ratio normalized root mean squared error measure similarity image reference image down-sampled plane. quantitative results non-gan approaches shown table parameters running speed networks also listed table dcsrn mdcsrn depth network later obtained marginally better results reduce parameters running time among variants largest network best performance without much sacrifice speed. table results ssim psnr nrmse different dcsrn architectures. depth slightly better performance less number parameters computation operation. deepest network average runtime whole subject around seconds best performance. parm time table performance comparison bicubic interpolation fsrcnn srresnet proposed mdcsrn mdcsrn provides similar image quality srrestnet faster provides much better image quality bicubic interpolation fsrcnn. *fsrcnn large kernels extremely computationally expensive though small takes longer time mdcsrn small filters mdcsrn compared bicubic interpolation well neural networks fsrcnn srresnet mdcsrn obtained large advantage fsrcnn methods slightly better srresnet runs faster. additionally mdcsrn-gan provides much sharpened visually plausible images compared non-gan approaches. fig. demonstrates super-resolution results random subject resolution degrading plane. among nongan methods small vessels mdcsrn distinguishable neural networks. however mdcsrn-gan provides much better overall image quality vessel maintains shape size ground-truth image gaps vessel gray matter also much clearer mdcsrn-gan result almost indistinguishable ground truth. presented novel sisr method based mdcsrn-gan mri. showed mdcsrn-gan recover local image textures details accurately times quickly current state-of-art deep learning approaches. technique would allow -fold reduction scan time maintaining virtually identical image resolution quality.", "year": "2018"}