{"title": "A Hierarchical Latent Vector Model for Learning Long-Term Structure in  Music", "tag": "eess", "abstract": " The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the \"posterior collapse\" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a \"flat\" baseline model. An implementation of our \"MusicVAE\" is available online at http://g.co/magenta/musicvae-code. ", "text": "latent-space averaging using figure demonstration musicvae. latent codes bottom sequences averaged decoded model produce middle sequence. latent-space mean involves similar repeating pattern sequence higher register intermittent pauses like bottom sequence. audio example available online supplement. figs. appendix baseline comparison. millions pixels generating synthetic audio hundreds thousands timesteps achieving state-of-the-art performance semi-supervised learning tasks wide variety methods used deep generative modeling including implicit models generative adversarial networks explicit deep autoregressive models pixelcnn wavenet work focus deep latent variable models variational autoencoder advantage models explicitly model latent vector either inferred existing data sampled distribution variational autoencoder proven effective model producing semantically meaningful latent representations natural data. however thus seen limited application sequential data demonstrate existing recurrent models difﬁculty modeling sequences long-term structure. address issue propose hierarchical decoder ﬁrst outputs embeddings subsequences input uses embeddings generate subsequence independently. structure encourages model utilize latent code thereby avoiding posterior collapse problem remains issue recurrent vaes. apply architecture modeling sequences musical notes exhibits dramatically better sampling interpolation reconstruction performance baseline model. implementation musicvae available online. generative modeling describes framework estimating underlying probability distribution used generate data facilitate wide range applications sampling novel datapoints unsupervised representation learning estimating probability existing datapoint learned distribution. much recent progress generative modeling expedited deep neural networks producing deep generative models leverage expressive power deep networks model complex high-dimensional distributions. practical achievements include generating realistic images latent space. ideally latent vector captures pertinent characteristics given datapoint disentangles factors variation dataset. autoencoders also model likelihood provides efﬁcient mapping latent vector back data space. interest deep latent variable models primarily comes increasing creative applications machine learning arises surprising convenient characteristics latent spaces typically learned models. example averaging latent codes datapoints possess given attribute produces so-called attribute vector used make targeted changes data examples. encoding datapoint attribute obtain latent code subtracting corresponding attribute vector adding another attribute vector decoding resulting latent vector produce realistic manifestation initial point attributes swapped another example interpolating latent vectors decoding points trajectory produce realistic intermediate datapoints morph characteristics ends smooth semantically meaningful way. work deep latent variable models focused continuous-valued data ﬁxed dimensionality e.g. images. modeling sequential data less common particularly sequences discrete tokens musical scores typically require autoregressive decoder. partially autoregression often sufﬁciently powerful autoencoder ignores latent code shown success short sequences deep latent variable models successfully applied long sequences. address introduce novel sequential autoencoder hierarchical recurrent decoder helps overcome aforementioned issue modeling long-term structure recurrent vaes. model encodes entire sequence single latent vector enables many creative applications enjoyed vaes images. show experimentally model capable effectively autoencoding substantially longer sequences baseline model decoder rnn. paper focus application modeling sequences musical notes. western popular music exhibits strong long-term structure repetition variation measures sections piece music. structure also hierarchical–songs divided sections broken measures beats further music fundamentally multi-stream signal sense often involves multiple players strong inter-player dependencies. unique properties additional potential creative applications make music ideal testbed sequential autoencoder. covering background work approach builds describe model novel architectural enhancements. provide overview related work applying latent variable models sequences. finally demonstrate ability method model musical data quantitative qualitative evaluations. fundamentally model autoencoder i.e. goal accurately reconstruct inputs. however additionally desire ability draw novel samples perform latent-space interpolations attribute vector arithmetic. properties adopt framework variational autoencoder. successfully using vaes sequences beneﬁts additional extensions objective. following subsections cover prior work forms backbone approach. common constraint applied autoencoders compress relevant information input lower-dimensional latent code. ideally forces model produce compressed representation captures important factors variation dataset. pursuit goal variational autoencoder introduces constraint latent code random variable distributed according prior data generation model consists encoder approximates posterior decoder parameterizes likelihood practice approximate posterior likelihood distributions parameterized neural networks parameters respectively. following framework variational inference posterior inference minimizing divergence approximate posterior encoder true posterior maximizing evidence lower bound expectation taken respect kl-divergence. naively computing gradient elbo infeasible sampling operation used obtain common case diagonal-covariance gaussian circumvented replacing interpreting elbo used considering terms kl||p) separately. ﬁrst thought requiring high samples qλ–ensuring accurate reconstructions. second encourages close prior–ensuring generate realistic data sampling latent codes presence terms suggests trade-off quality samples reconstructions–or equivalently between rate distortion stems interpretation kl||p) measures amount information required code samples using utilizing threshold therefore amounts giving model certain budget free bits learning approximate posterior. note modiﬁed objectives longer optimize lower bound likelihood custom still refer resulting models variational autoencoders. likelihood divergence terms sufﬁciently small held-out test data. practical test properties interpolate points latent space test whether corresponding points data space interpolated semantically meaningful way. concretely latent vectors corresponding datapoints perform linear interpolation latent space computing goal satisﬁed realistic datapoint transitions semantically meaningful vary perceptually similar small note prior latent space spherical gaussian samples high-dimensional priors practically indistinguishable samples uniform distribution unit hypersphere practice therefore spherical interpolation instead additional test whether autoencoder useful creative applications measures whether produces reliable attribute vectors. attribute vectors computed average latent vector collection datapoints share particular attribute. typically attribute vectors computed pairs attributes e.g. images people without glasses. model’s ability discover attributes tested encoding datapoint attribute subtracting attribute vector latent code adding attribute vector testing whether decoded result appears like original datapoint attribute instead experiments latent space manipulation techniques demonstrate power proposed model. broad goal autoencoder learn compact representation data. creative applications additional uses latent space learned model first given point latent space maps realistic datapoint nearby latent space points datapoints semantically similar. extrapolation implies points along continuous curve connecting points latent space decodable series datapoints produce smooth semantic interpolation data space. further requirement effectively mandates latent space smooth contain holes i.e. isolated regions realistic datapoints. second desire latent space disentangles meaningful semantic groups dataset. wide variety neural network structures considered encoder decoder network present work interested models recurrent encoder decoder concretely encoder recurrent neural network processes input sequence produces sequence hidden states parameters distribution latent code function decoder uses sampled latent vector initial state decoder autoregressively produces output sequence yt}. model trained reconstruct input sequence learn approximate posterior close prior standard vae. main drawbacks approach. first rnns state size layers latent dimensions. standard vaes parametrize latent distribution bidirectional recurrent encoder ideally gives parametrization latent distribution longer-term context input sequence. prior work decoder recurrent typically simple stacked rnn. decoder uses latent vector initial state proceeds generate output sequence autoregressively. preliminary experiments found using simple decoder resulted poor sampling reconstruction long sequences. believe caused vanishing inﬂuence latent state output sequence generated. deﬁne special case then latent vector passed fully-connected layer followed tanh activation initial state conductor rnn. conductor produces embedding vectors subsequence. experiments two-layer unidirectional lstm conductor hidden state size output dimensions. conductor produced sequence embedding vectors individually passed shared fully-connected layer followed tanh activation produce initial states ﬁnal bottom-layer decoder rnn. decoder autoregressively produces sequence distributions output tokens subsequence softmax output layer. step bottom-level decoder current conductor embedding concatenated previous output token used input. experiments used -layer lstm units layer decoder rnn. principle autoregressive decoder still allows posterior collapse problem model effectively learns ignore latent state. simliar important limit scope decoder force latent code model long-term structure. decoder simple reducing receptive ﬁeld direct analogy exists typically used powerful autoregressive models sequences. result decoder recurrent sufﬁciently powerful produce effective model data disregard latent code. latent code ignored divergence term elbo trivially zero despite fact model effectively longer acting autoencoder. second model must compress entire sequence single latent vector. approach shown work short sequences begins fail sequence length increases following section present latent variable autoencoder overcomes issues using hierarchical decoder. high level model follows basic structure used previously-proposed vaes sequential data however introduce novel hierarchical decoder demonstrate produces substantially better performance long sequences section schematic model musicvae shown fig. bidirectional encoder encoder two-layer bidirectional lstm network process input sequence obtain ﬁnal state vectors second bidirectional lstm layer. concatenated produce fullyconnected layers produce latent distribution parameters rnns principle unlimited temporal receptive ﬁeld. around this reduce effective scope bottom-level decoder allowing propagate state within output subsequence. described above initialize subsequence state corresponding embedding passed conductor. implies decoder longer-term context using embeddings produced conductor turn depend solely latent code. experimented autoregressive version conductor decoder state passed back conductor subsequence found exhibited worse performance. believe combined constraints effectively force model utilize conductor embeddings extension latent vector order correctly autoencode sequence. many common sources sequential data text consist solely single stream i.e. sequence source producing tokens. however music often fundamentally multi-stream signal–a given musical sequence consist multiple players producing notes tandem. modeling music therefore also involve modeling complex inter-stream dependencies. explore possibility introducing trio model identical basic musicvae except produces separate distributions output tokens–one three instruments hierarchical decoder model consider separate streams orthogonal dimension hierarchy separate decoder instrument. embeddings conductor initialize states instrument separate fully-connected layers followed tanh activations. baseline decoder single split output produce per-instrument softmaxes. closely related model aforementioned recurrent like ours model effectively uses rnns encoder decoder. careful optimization demonstrate ability generate interpolate sentences modeled character level. similar model also proposed applied limited success music. approach also extended utilize convolutional encoder decoder dilated convolutions primary difference models decoder architecture; namely hierarchical rnn. decoder various additional models autoregressive decoders also proposed. consider extensions recurrent rnns replaced feed-forward convolutional networks. pixelvae marries pixelcnn applies result task natural image modeling. similarly variational lossy autoencoder combines pixelcnn/pixelrnn decoder. authors also consider limiting power decoder using expressive inverse autoregressive flow prior latent codes. another example recurrent encoder decoder sketchrnn successfully models sequences continuously-valued coordinates. hierarchical paragraph autoencoder proposed several parallels work. also consider autoencoder hierarchical rnns encoder decoder level hierarchy corresponds natural subsequences text however impose constraints latent code result unable sample interpolate sequences. model otherwise differs bidirectional encoder lack autoregressive connections ﬁrst level hierarchy. broadly model considered sequenceto-sequence framework encoder produces compressed representation input sequence used condition decoder generate output sequence. example nsynth model learns embeddings compressing audio waveforms downsampling convolutional encoder reconstructing audio wavenet-style decoder recurrent sequence-to-sequence models often applied sequence transduction tasks input output sequences different. nevertheless sequence-to-sequence autoencoders occasionally considered e.g. auxiliary unsupervised training method semi-supervised learning paragraph autoencoder described above. again approach differs impose structure compressed representation perform sampling interpolation. finally many recurrent models proposed recurrent states stochastic latent variables dependencies across time particularly similar example model also utilizes hierarchical encoder decoder. model uses levels hierarchy generates stochastic latent variable subsequence input sequence. crucial difference class models single latent variable represent entire sequence allows creative applications interpolation attribute manipulation. demonstrate power musicvae carried series quantitative qualitative studies music data. first demonstrate simple recurrent like described effectively generate interpolate short sequences musical notes. then move signiﬁcantly longer note sequences novel hierarchical decoder necessary order effectively model data. verify assertion provide quantitative evidence able reconstruct interpolate between model attributes data signiﬁcantly better baseline. conclude series listening studies demonstrate proposed model also produces dramatic improvement perceived quality samples. data source midi ﬁles widelyused digital score format. midi ﬁles contain instructions notes played individual instrument song well meter information. collected million unique ﬁles provided ample data training models. extracted following types training data midi ﬁles -bar melodies -bar drum patterns -bar trio sequences consisting separate streams melodic line bass line drum pattern. details dataset creation process refer appendix ease comparison also evaluated reconstruction quality publicly available lakh midi dataset appendix modeled monophonic melodies basslines sequences note events. resulted dimensional output space note-on tokens midi pitches plus single tokens note-off rest. drum patterns mapped drum classes deﬁned general midi standard canonical classes represented possible combinations hits categorical tokens. timing cases quantized notes note intervals consisted events. result two-bar data resulted sequences -bar data hierarchical models meaning models trained using adam learning rate annealed exponential decay rate batch size -bar models gradient updates respectively. used cross-entropy loss ground-truth output scheduled sampling -bar models teacher forcing -bar models. proof modeling musical sequences recurrent possible ﬁrst tried modeling -bar monophonic music sequences decoder. model given tolerance free bits cost weight annealed exponential rate scheduled sampling introduced inverse sigmoid rate found model highly accurate reconstructing input also able produce compelling interpolations samples. words learned effectively latent code without suffering posterior collapse exposure bias particularly evidenced relatively small difference teacher-forced sampled reconstruction accuracy despite success model unable reliably reconstruct -bar sequences. example discrepancy teacher-forced sampled reconstruction accuracy increased motivated design hierarchical decoder described section following sections provide extensive comparison proposed model baseline. table reconstruction accuracies calculated teacherforcing full sampling. values reported held-out test set. softmax temperature used cases meaning sampled directly logits. begin evaluate whether hierarchical decoder produces better reconstruction accuracy -bar melodies drum patterns. -bar models give tolerance free bits table shows per-step accuracies reconstructing sequences test set. mentioned above signs posterior collapse baseline leading reductions accuracy teacher forcing removed inference. hierarchical decoder increases next-step prediction accuracy reduces exposure bias better learning latent code. hierarchical model decrease sampling accuracy versus teacher forcing ranges general also reconstruction errors made models reasonable e.g. notes shortened beat addition notes appropriate key. also explored performance models multi-stream trio dataset consisting -bar sequences melody bass drums. single-stream data hierarchical model able achieve higher accuracy baseline exhibiting much smaller teacher-forced sampled performance. figure latent-space interpolation results. values averaged interpolated sequences. x-axis denotes interpolation sequence left right. sequencenormalized hamming distance sequence interpolated points. distance symmetric shown. bottom relative probability according independently-trained -gram language model. creative purposes desire interpolations smoothly varying semantically meaningful. fig. compare latent-space interpolations decoder hierarchical decoder baseline naive blending sequences averaged behavior interpolating between -bar melodies evaluation dataset unique evaluation melodies using softmax temperature sample intermediate sequences. constructed baseline data interpolations sampling bernoulli random variable parameter choose element either sequence time step i.e. graph fig. shows hamming distance i.e. proportion timestep predictions differ interpolation sequence increases monotonically methods. data interpolation varies linearly expected following mean bernoulli distribution. hamming distances also vary monotonically latent space interpolations showing decoded sequences morph smoothly less like sequence like sequence example reconstructions don’t remain mode several steps jump suddenly another. samples non-zero hamming distance endpoints imperfect reconstructions hierarchical decoder lower intercept higher reconstruction accuracy. bottom graph fig. ﬁrst trained -gram language model melody dataset show normalized cost interpolated sequence given model cost interpolated sequence interpolation amount costs endpoint sequences large hump data interpolation shows interpolated sequences data space deemed language model much less probable original melodies. model better produces less coherent interpolated sequences hierarchical model produces interpolations equal probability originals across entire range interpolation. fig. shows example melodies corresponding midpoint musicvae space. interpolation synthesizes semantic elements melodies similar repeating pattern higher register intermittent sparsity like shared musical key. hand baseline data interpolation mixes resulting harmonic rhythmic dissonance also exploit structure latent space attribute vectors alter attributes given sequence. apart score itself midi contains limited annotations deﬁned attributes trivially computed directly note sequence diatonic membership note density average interval note syncopation. appendix full deﬁnitions. ideally computing difference figure adding subtracting different attribute vectors latent space decoding result produces intended changes side effects. vertical axis denotes attribute vector applied horizontal axis denotes attribute measured. figs. example piano rolls appendix descriptions attribute. average latent vector sequences exhibit extremes attribute adding subtracting latent codes existing sequences produce intended semantic manipulation. test this ﬁrst measured attributes random training examples. attribute ordered amount attribute exhibited partitioned quartiles computed attribute vector subtracting mean latent vector bottom quartile mean latent vector quartile. sampled random vectors prior added subtracted vectors attribute measured average percentage change attributes sequence decoded unaltered latent code. results shown fig. general applying given attribute vector consistently produces intended change targeted attribute. also cases increasing attribute decreases another believe largely heuristics capture overlapping characteristics. interested evaluating attribute vector manipulations labels non-trivial compute future work. figure results listening tests. black error bars indicate estimated standard deviation means. double asterisks pair indicate statistically signiﬁcant difference ranking. fig. shows number comparisons composition model selected musical. listening test clearly demonstrates improvement sample quality gained using hierarchical decoder–in cases hierarchical model preferred dramatically often model rate evaluation data. fact hierarchical drum model preferred often real data difference statistically signiﬁcant. likely listener bias towards variety true drum data realistic also repetitive perhaps less engaging. further kruskal-wallis test ratings showed statistically signiﬁcant difference models melodies trios drums. post-hoc analysis using wilcoxon signedrank test bonferroni correction showed participants rated samples hierarchical models musical samples corresponding models participants also ranked real data musical samples models signiﬁcant difference samples hierarchical models real data. capturing whether samples model sound realistic difﬁcult purely quantitative metrics. compare perceived sample quality different models therefore carried listening studies melodies trio compositions drum patterns. participants presented -bar compositions either sampled models extracted evaluation dataset. asked thought musical likert scale. study ratings collected source proposed musicvae recurrent variational autencoder utilizes hierarchical decoder improved modeling sequences long-term structure. experiments music data thoroughly demonstrated quantitative qualitative experiments model achieves substantially better performance baseline. future work interested testing model types sequential data. facilitate future research recurrent latent variable models make code pre-trained models publicly available. authors wish thank david inspiration guidance. thank claire kayacik cheng-zhi anna huang assistance user study analysis. thank erich elsen additional editing. goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets. advances neural information processing systems gulrajani kumar ahmed taiga visin vazquez courville pixelvae latent variable model natural images. fifth international conference learning representations bengio vinyals jaitly shazeer scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems heaﬁeld kenlm faster smaller language model queries. proceedings sixth workshop statistical machine translation association computational linguistics bowman vilnis vinyals jozefowicz bengio generating sentences continuous space. proceedings twentieth conference computational natural language learning chen kingma salimans duan dhariwal schulman sutskever abbeel variational lossy autoencoder. fifth international conference learning representations higgins matthey burgess glorot botvinick mohamed lerchner betavae learning basic visual concepts constrained variational framework. fifth international conference learning representations engel resnick roberts dieleman simonyan norouzi neural audio synthesis musical notes wavenet autoencoders. international conference machine learning kingma salimans jozefowicz chen sutskever welling improved variational inference inverse autoregressive ﬂow. advances neural information processing systems yang salakhutdinov berg-kirkpatrick improved variational autoencoders text modeling using dilated convolutions. international conference machine learning mikolov sutskever chen corrado dean distributed representations words phrases compositionality. advances neural information processing systems raffel ellis extracting ground-truth information midi ﬁles midifesto. proceedings international society music information retrieval conference serban sordoni lowe charlin pineau courville bengio hierarchical latent variable encoder-decoder model generating dialogues. thirty-first aaai conference artiﬁcial intelligence oord kalchbrenner vinyals espeholt graves kavukcuoglu conditional image advances generation pixelcnn decoders. neural information processing systems datasets built ﬁrst searching publicly-available midi ﬁles resulting million unique ﬁles. removed identiﬁed having non-/ time signature used encoded tempo determine boundaries quantizing notes trio data used -bar sliding window extract unique sequences containing instrument program number piano chromatic percussion organ guitar interval bass interval drum single consecutive rests instrument. multiple instruments three categories took cross product consider possible combinations. resulted million examples. table reconstruction accuracies lakh midi dataset calculated teacher-forcing full sampling. values reported held-out test set. softmax temperature used cases meaning sampled directly logits. easier comparison also trained -bar models publicly available lakh midi dataset makes subset dataset described above. extracted million melodies million drum patterns thousand trios full lmd. models trained hyperparameters used full dataset. ﬁrst evaluated lmd-trained melody model subset full evaluation made excluding examples train set. found less difference reconstruction accuracies lmdtrained original model. table report reconstruction accuracies -bar models trained evaluated lmd. accuracies slightly higher table conclusions regarding relative performance models hold. figure varying amount note density attribute vector. amount varies steps central sequence corresponding attribute vector. audio example available online supplement. figure additional resamplings latent code semantically similar speciﬁc notes vary sampling autoregressive decoder. audio example available online supplement. figure subtracting adding diatonic attribute vector note sequence middle. ease interpretation notes diatonic scale shown white notes outside scale shown black. audio example available online supplement. figure subtracting adding note syncopation attribute vector note sequence middle. ease interpretation ﬁrst sequence’s bars shown. vertical lines indicate note boundaries. white black indicate syncopated non-syncopated notes respectively. audio example available online supplement. figure subtracting adding note syncopation attribute vector note sequence middle. ease interpretation ﬁrst sequence’s bars shown. vertical lines indicate quarter note boundaries. white black indicate syncopated non-syncopated notes respectively. audio example available online supplement. figure example interpolation -bar melody musicvae latent space. vertical axis pitch horizontal axis time. sampled interpolated sequences test-set sequences left right ends. -bar sample shown different background color. audio extended -step interpolation sequences available online supplement.", "year": "2018"}