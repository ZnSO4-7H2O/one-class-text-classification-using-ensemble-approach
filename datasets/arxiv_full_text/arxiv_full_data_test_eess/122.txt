{"title": "Learning flexible representations of stochastic processes on graphs", "tag": "eess", "abstract": " Graph convolutional networks adapt the architecture of convolutional neural networks to learn rich representations of data supported on arbitrary graphs by replacing the convolution operations of convolutional neural networks with graph-dependent linear operations. However, these graph-dependent linear operations are developed for scalar functions supported on undirected graphs. We propose a class of linear operations for stochastic (time-varying) processes on directed (or undirected) graphs to be used in graph convolutional networks. We propose a parameterization of such linear operations using functional calculus to achieve arbitrarily low learning complexity. The proposed approach is shown to model richer behaviors and display greater flexibility in learning representations than product graph methods. ", "text": "graph convolutional networks adapt architecture convolutional neural networks learn rich representations data supported arbitrary graphs replacing convolution operations convolutional neural networks graph-dependent linear operations. however graph-dependent linear operations developed scalar functions supported undirected graphs. propose class linear operations stochastic processes directed graphs used graph convolutional networks. propose parameterization linear operations using functional calculus achieve arbitrarily learning complexity. proposed approach shown model richer behaviors display greater ﬂexibility learning representations product graph methods. large amounts data rich interactions characteristic complex networks observed brain imaging social networks motivate need rich representations learning. convolutional neural networks oﬀer means learning rich representations data composing convolutions pooling nonlinear activation functions however bruna argued success convolutional neural networks images video speech attributed special statistical properties domains generalize data arbitrary graph structure. motivates generalization convolution data supported arbitrary graph bruna graph-speciﬁc convolution deﬁned consistent graph fourier transform proposed shuman formulation eigenvectors graph laplacian form basis linear operations well imparting topology graph. learning algorithm optimizes eigenvalues linear operator. application learning representation so-called graph convolutional networks yielded state-of-the-art results applications network analysis computer graphics medical imaging procedure limitations chapter aims address them. first clear apply techniques stochastic processes graphs second graph signal processing approach advocated shuman works undirected graphs. limitations severely reduce domains learning representations apply. chapter aims address gaps proposing theoretical framework designing learning graph-speciﬁc linear operators stochastic processes directed graphs. together pooling operations nonlinear activation functions hypothesized linear operators would lead learning rich representations stochastic processes graphs. throughout consideration given learning complexity. paper proceeds follows. section discusses work related ﬁltering linear modeling stochastic processes supported graphs. section establishes preliminary notation theory harmonic analysis. sec. motivates learning covariant linear operations proposes theoretical framework designing them. section proposes functional calculus design learn covariant learning representations graph structured data featured recent review articles bronstein hamilton reviews discuss deﬁning convolutional neural networks graph-speciﬁc linear operators bruna bruna learnable parameters comprise eigenvalues linear operator eigenvectors ﬁxed graph laplacian. deﬀerrard kipf welling learnable parameters instead coeﬃcients polynomial graph laplacian reducing learning complexity leading superior results application. graph signal processing proposed sandryhaila moura shuman extends traditional tools time-series signal processing scalar functions supported nodes graph. theoretical extensions transforms sampling ﬁltering established applied various domains. recent review ortega analysis stochastic processes supported graph special case graph signal processing ﬁrst addressed sandryhaila moura work authors propose generalization graph signal processing approach multi-variate observations modeled factor graph. idea underlies work loukas foucard grassi speciﬁcally address time-varying graph signals. preliminaries graph nodes edges consider stochastic processes take values indexed time i.e. sequence vector-valued functions cn}t∈z. image function thought representing attribute vertices graph indexed time. attribute could action posting liking message individual social network recorded activity electrode brain region. laurent operator seen generalization convolution much like convolution diagonalized fourier transform laurent operators multiplicatively fourier transform. deﬁne fourier goal paper present framework learning robust representations stochastic processes graphs. important learning generalization invariance covariance particular group actions. example convolutional neural networks depend covariance convolution translation much stronger results exist holomorphic annulus satisﬁed constants negligible. assume cn×n indeed holomorphic matrix-valued function appropriate annulus. then holomorphic functions moreover almost everywhere full discussion analytic tion leads sublinear learning complexity achieved compactly supported convolutions. deﬀerrard propose scalar graph signal case learn polynomials graph laplacian instead spectral multipliers bruna corollary framework would polynomials necessarily satisﬁes deﬁne linear transformations polynomials special case comprehensive theory functional calculus. develop theory fully section order deﬁne parameterizations arbitrarily complexity. approach oﬀers controlled learning complexity. parameter holomorphic function deﬀerrard could polynomial degree parameterization class holomorphic functions includes polynomials even necessary simply connected open set. ﬁnite union disjoint open sets j=uj need holomorphic restriction j=γj means deﬁne holomorphic functions eαz+β total three learnable parameters parameterization using functional calculus chosen arbitrarily section compare proposed approach factor graph model time-varying graph signals proposed sandryhaila moura approach primarily motivated eﬃcient numerical implementation. choose example group generator illustrate analytically diﬀerence approach. highlight advantages richness graphical model separation spectrum. figure visual depiction weighted edges oﬀers richer model temporal interaction nodes. edges connect nodes across zero three time steps whereas temporal interaction across time step. note alternative graphical models fig. greater ﬂexibility facilitate modeling complex systems since edges oﬀer pathways nodes interact inﬂuence system behavior. importantly even simple example fact yield complex behaviors shown following. deﬁne invariant subspaces element associated subspace shows modes deﬁned range manifest signiﬁcantly diﬀerent behaviors diﬀerent frequencies. frequency-dependent modes network somehow lost factor graph model. regardless frequency behavior nodes same. important implications learning informative representations complex systems display e.g. cross-frequency coupling. priori know important behaviors interactions network tasks discrimination regression compression. powerful ﬂexible model learn relevant representations exceedingly important. ∪ω∈λ+ ∪ω∈λ− compose holomorphic functions deﬁne open ∪ω∈λ+ ∪ω∈λ− ∪ω∈+ ∪ω∈− compose also holomorphic functions however spectrum separable must deﬁne open compare spectra fig. spectra well separated important application functional calculus. deﬁne diﬀerent holomorphic function restricted separable compact described sec. instance deﬁne learn simultaneous projections associated rich inter-relationships modes stochastic process graph learnable parameters µ−}. proposed theoretical framework learning robust representations stochastic processes directed graphs arbitrarily complexity. applied theory example problem illustrates advantages proposed approach factor graph models proposed sandryhaila moura speciﬁcally proposed framework yields greater model expressiveness. importantly", "year": "2017"}