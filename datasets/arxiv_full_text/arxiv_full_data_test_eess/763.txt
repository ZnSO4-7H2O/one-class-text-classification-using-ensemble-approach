{"title": "Gaussian Processes Over Graphs", "tag": "eess", "abstract": " We propose Gaussian processes for signals over graphs (GPG) using the apriori knowledge that the target vectors lie over a graph. We incorporate this information using a graph- Laplacian based regularization which enforces the target vectors to have a specific profile in terms of graph Fourier transform coeffcients, for example lowpass or bandpass graph signals. We discuss how the regularization affects the mean and the variance in the prediction output. In particular, we prove that the predictive variance of the GPG is strictly smaller than the conventional Gaussian process (GP) for any non-trivial graph. We validate our concepts by application to various real-world graph signals. Our experiments show that the performance of the GPG is superior to GP for small training data sizes and under noisy training. ", "text": "abstract—we propose gaussian processes signals graphs using apriori knowledge target vectors graph. incorporate information using graphlaplacian based regularization enforces target vectors speciﬁc proﬁle terms graph fourier transform coeffcients example lowpass bandpass graph signals. discuss regularization affects mean variance prediction output. particular prove predictive variance strictly smaller conventional gaussian process non-trivial graph. validate concepts application various real-world graph signals. experiments show performance superior small training data sizes noisy training. gaussian processes natural extension ubiquitous kernel regression bayesian setting regression parameters modelled random variables gaussian prior distribution given training observations gaussian processes generate posterior probabilities target output inputs observations function training data input kernel function gaussian process models variants applied number diverse ﬁelds model predictive control system analysis latent variable models multi-task learning image analysis synthesis speech processing magnetic resonance imaging gaussian processes also extended non-stationary regression setting regression complex-valued data recently gaussian processes shown useful training analysis deep neural networks gaussian process viewed neural network single inﬁnite-dimensional layer hidden units prediction performance gaussian process depends availability training data progressively improving training data size increased. many applications however required make predictions using limited number observations corrupted observation noise. cases providing additional structure helps improve prediction performance signiﬁcantly. article advocate graph signal processing improving propose gaussian processes incorporating apriori knowledge vector-valued target output vectors underlying graph. forms natural bayesian extension kernel regression graph signals proposed recently particular target vectors enforced follow pre-speciﬁed proﬁle terms graph fourier coefﬁcients lowpass bandpass high-pass. show turn translates speciﬁc structure prior distribution target vectors. derive predictive distribution general input given training observations prove graph signal structure leads decrease variance predictive distribution. hypothesis incorporating graph structure would boost prediction performance. validate hypothesis various real-world datasets temperature measurements ﬂow-cytometry data functional data tracer diffusion experiment pollution studies. though consider mainly undirected graphs characterized graph-laplacian analysis also discuss approach extended handle directed graphs using appropriate regularization. preliminaries graph signal processing graph signal processing signal processing graphs deals extension several traditional signal processing methods incorporating graph structural information includes signal analysis concepts fourier transforms ﬁltering wavelets ﬁlterbanks multiresolution analysis denoising dictionary learning stationary signal analysis spectral clustering principal component analysis approaches based graph signal ﬁltering also proposed recently several approaches proposed learning graph directly data recently kernel regression based approaches also developed graph signals brieﬂy review relevant concepts graph signal processing used analysis development. review included keep article self-contained. edge nodes denoting absence edge. since graph undirected symmetric edge-weights graph-laplacian matrix deﬁned diagonal degree matrix diagonal element given elements symmetric undirected graph construction nonnegative eigenvalues smallest eigenvalue equal zero. graph signal mdimensional vector denotes value signal node denotes transpose operation. smoothness measured using takes similar value across connected nodes agreement intuitively expects smooth signal. similarly high-frequency non-smooth signal dissimilar values across connected nodes equivalently large value notion frequency graph signals. {λi}m {vi}m respectively. then eigenvalue decomposition diag j··· diagonal matrix whose values penalize promote speciﬁc coefﬁcients. example energy predominantly vectors indices corresponding diagonal entries zero assign large values remaining diagonal entries. properties note graph signals encountered practice usually smooth associated graph. simplest cases generating smooth graph signal penalize turn frequencies linearly setting corresponds vjlt smooth graph signal becomes standard practice order eigenvectors graph-laplacian according smoothness terms eigenvectors referred graph fourier transform basis vectors since generalize notion discrete fourier transform denote eigenvalue ordered then observe words eigenvectors corresponding smaller vary smoothly graph large exhibit variation across graph. turn gives intuitive frequency ordering basis vectors. coefﬁcients graph signal deﬁned inner product signal basis vectors coefﬁcient vector deﬁned derive expressions graph signal particular proﬁle closest given signal shall used analysis later. signal graph signal speciﬁed graph frequency proﬁle closest obtained solving following generative model function rk×m denotes regression coefﬁcient matrix. consider isotropic gaussian prior precision entries notation denotes vectorization obtained concatenating columns single vector. then principal assumption predicted target vector regression output graph fourier spectrum characterized diagonal spectrum matrix however necessarily satisfy requirement arbitrary choice since also ﬁxed apriori assumed graph-speciﬁc. becomes clear prior chosen promotes required graph fourier spectrum. next discuss strategy formulating prior distribution. given regression output generated using ﬁxed drawn graph signal closest obtained using generative model following result thus note regression regression coefﬁcient matrix produces graph signals possessing desired graph fourier proﬁle. case regression output smooth graph. vectorizing sides using properties kronecker product next develop gaussian process graphs ﬁrst note existence input enters equation form inner products φφφ. inner product φφφφφφ measure similarity inputs. keeping mind generalize inner-product φφφφφφ valid kernel function inputs kernel matrix denoted γ−φφ entry following gaussian distributed zero mean following covariance turn implies choosing prior regression coefﬁcients form yields graph signals speciﬁed graph fourier spectrum graph laplacian matrix pose regression problem following form note case completely disconnected graph corresponds conventional gaussian process graph adjacency matrix equal identity matrix correspondingly then covariance joint distribution given graph atleast connected subgraph atleast eigenvalue strictly greater thus barring completely disconnected graph pathological case graph connections across nodes results words variance strictly reduces comparison conventional regression words introducing non-zero connections among nodes ensures marginal variance strictly lesser obtained conventional regression case. also note order reduction variance signiﬁcant must large turn implies connected communities within graph must strong algebraic connectivity case regression output smooth graph signals sense graph k-connected components disjoint subgraphs. number zero eigenvalues equal nunber connected components graph observe ∆σσσn schur complement fc−f since schur complement positivedeﬁnite matrix also positive-deﬁnite already proved positive-deﬁnite theorem follows preceding analysis also observe variance joint distribution hence σσσgn inversely related graph regularization parameter large implies small value therefore small value note since bayesian linear regression graphs developed section special case gaussian process graphs kernels shall refer former gaussian process-linear latter gaussian process-kernel correspondingly refer conventional versions gp-l gp-k respectively. next show graph information reduces variance predictive distribution. implies models observed training samples better conventional theorem graph structure reduces variance marginal distribution results smaller variance since positive semideﬁnite construction kernel matrix {θi}n denote eigenvalues eigenvectors respectively. then eigenvalue decomposition given comparison conventional mean obtained setting case smooth graph signals implying prediction mean lower contributions higher graph-frequencies turn shows performs noise-smoothening making graph topology. however imply value arbitrarily large. large turn force resulting target predictions close smoother eigenvectors desirable since reduces learning ability note since gpg-l special case gpg-k theorem analysis following apply directly also gpg-l. analysis assumed underlying graph target vectors undirected hence characterized symmetric positive semideﬁnite graph-laplacian matrix discuss derived case directed graphs adjacency matrix graph assymetric. case directed graphs following metric popular quantifying smoothness graph signals denote graph signal adjacency matrix respectively adjacency matrix assumed normalized maximum eigenvalue modulus equal unity signal smooth directed graph small value. measures difference signal one-step diffused ’graph-shifted’ version thereby measuring difference signal value node neighbours weighted strength edge nodes. case directed graphs adopt approach undirected graph case important distinction instead solving solve following problem observation similar analysis undirected graphs possible show variance predictive distribution reduces using predictive mean smooth graph. interest space avoid repetition include analysis directed graphs here. show mean predictive distribution graph signal graph fourier spectrum adheres condition imposed demonstrate computing graph fourier spectrum predictive mean. using eigendecompostions since function eigenvalue eigenvalue shall alternatively notation speciﬁc following analysis. similarly expressed eigenvectors vi⊗ui. component prediction mean along graph eigenvector given consider application various real-world signal examples. examples consider undirected graphs. interest compute predictive distribution given noisy targets corresponding inputs assumption target vector smooth underlying graph. graph-regularization evaluate prediction performance normalizedmean-square-error deﬁned follows noisy training targets adding white gaussian noise snrlevels nmse prediction mean testing data averaged different random choices training testing sets shown monte carlo simulation check robustness. observe linear kernel regression cases outperforms signiﬁcant margin particularly small training data sizes expected. trend also similar larger subsets nodes full considered. results reported brevity avoid repetition. denotes mean predictive distribution true value target matrix means contain noise. noisy target matrix generated obtained adding white gaussian noise precision parameter case real-world examples compare performance following cases regression next apply temperature measurements populated cities sweden period three months october december data available publicly swedish meteorological hydrological institute goal perform one-day temperature prediction given temperature data particular predict temperature next input-target data pairs total half used training rest testing. consider geodesic graph analysis entry graph adjacency matrix given denotes geodesic distance cities. order remove self loops diagonal zero. generate noisy training data adding zero-mean white gaussian noise true temperature measurements. figure show nmse obtained testing data averaging different random partitioning total dataset training testing datasets. observe outperforms linear kernel regression cases. consider application ﬂow-cytometry data considered sachs consists response signalling level proteins different experiment cells. since protein signal values large dynamic range positive values perform experiments signals obtained taking logarithm base reducing dynamic range. ﬁrst measurements analysis. symmetricized version directed unweighted acyclic graph proposed sachs among proteins choose proteins arbitrarily input make predictions remaining proteins forms target vector lying graph nodes. perform experiment times monte carlo simulation random divide total dataset training testing datasets. average nmse testing datasets shown figure levels observe trend outperforms linear kernel cases sample sizes corrupted noise. ﬁrst consider functional magnetic resonance imaging data obtained cerebellum region brain original graph consists nodes corresponding different voxels cerebellum region. voxels mapped anatomically following atlas template refer details graph construction associated signal extraction. consider subset ﬁrst vertices analysis. goal ﬁrst vertices input make predictions remaining vertices forms output thus target signals graph dimension corresponding adjacency matrix shown figure total graph signals corresponding different measurements single subject. portion signals training remaining testing. construct next experiment atmospheric tracer diffusion measurements obtained european tracer experiment tracked concentration perﬂuorocarbon tracers released atmosphere starting ﬁxed location observations collected experiments span hours ground stations europe giving sets measurements total measurements. goal predict tracer concentrations half total ground stations using concentrations remaining locations. target signal graph signal graph nodes denotes measurement index. corresponding input vector also length illustrate ground station locations schematic figure output nodes correspond target shown markers corresponding edges whereas rest markers denote input. simulate noisy training adding white gaussian noise different levels training data. consider geodesic distance based graph. graph constructed manner like graph used temperature data experiment previously. randomly divide total dataset samples equally training test datasets. compute nmse different approaches averaging different randomly drawn training subsets size full training size plot nmse function figures levels observe graph structure enhances prediction performance signﬁcantly noisy sample size conditions. codes relevant experiments article made available https//www.researchgate.net/proﬁle/arun venkitaraman https//www.kth.se/ise/research/reproducibleresearch.. developed gaussian processes signals graphs employing graph-laplacian based regularization. gaussian process graphs shown consistent generalization conventional gaussian process provably results reduction uncertainty output prediction. turn implies gaussian process graph better model target vectors lying graph comparison conventional gaussian process. observation important cases available training data limited quantity quality. expectation motivation incorporating graph structural information would help gaussian process make better predictions particularly absence sufﬁcient reliable training data. experimental results real-world graph signals illustrated indeed case. chowdhary kingravi vela bayesian nonparametric adaptive control using gaussian processes ieee transactions neural networks learning systems vol. march tresp schwaighofer learning gaussian processes international multiple tasks proceedings conference machine learning ser. icml york available http //doi.acm.org/./. bonilla chai williams multi-task gaussian process prediction advances neural information processing systems platt koller singer roweis eds. curran associates inc. available http //papers.nips.cc/paper/-multi-task-gaussian-process-prediction.pdf wang zhang single image super-resolution using gaussian process regression dictionary-based sampling studentlikelihood ieee trans. image process. vol. july kwon tompkin theobalt efﬁcient learning image super-resolution compression artifact removal semi-local gaussian processes ieee trans. pattern analysis machine intelligence vol. sept koriyama nose kobayashi statistical parametric speech synthesis based gaussian process regression ieee selected topics isignal process. vol. april dong thanou frossard vandergheynst learning graphs signal observations smoothness prior corr vol. abs/. available http//arxiv.org/abs/. leonardi richiardi gschwind simioni j.-m. annoni schluep vuilleumier ville principal components functional connectivity approach study dynamic brain connectivity rest neuroimage vol. technique gaussian process regression based speech synthesis using feature space transform ieee intl. conf. acoust. speech signal process. march moungsri koriyama kobayashi duration prediction using multiple gaussian process experts gpr-based speech synthesis ieee intl. conf. acoust. speech signal process. march sj¨olund eklund ¨ozarslan knutsson gaussian process regression turn non-uniform undersampled diffusion data diffusion spectrum imaging ieee intl. symp. biomedical imaging april bertrand perrot ardon bloch classiﬁcation data using deep learning gaussian process-based model selection ieee intl. symp. biomedical imaging april mu˜noz-gonz´alez l´azaro-gredilla figueiras-vidal laplace approximation divisive gaussian processes nonstationary regression ieee trans. pattern analysis machine intelligence vol. march boloix-tortosa arias-de-reyna payan-somet murillo-fuentes complex-valued gaussian processes regression widely non-linear approach corr vol. abs/. available http//arxiv.org/abs/. shuman narang frossard ortega vandergheynst emerging ﬁeld signal processing graphs extending highdimensional data analysis networks irregular domains ieee signal process. mag. vol. sandryhaila moura data analysis signal processing graphs representation processing massive data sets irregular structure ieee signal process. mag. vol.", "year": "2018"}