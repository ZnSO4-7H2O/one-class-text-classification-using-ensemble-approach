{"title": "MIMO Graph Filters for Convolutional Neural Networks", "tag": "eess", "abstract": " Superior performance and ease of implementation have fostered the adoption of Convolutional Neural Networks (CNNs) for a wide array of inference and reconstruction tasks. CNNs implement three basic blocks: convolution, pooling and pointwise nonlinearity. Since the two first operations are well-defined only on regular-structured data such as audio or images, application of CNNs to contemporary datasets where the information is defined in irregular domains is challenging. This paper investigates CNNs architectures to operate on signals whose support can be modeled using a graph. Architectures that replace the regular convolution with a so-called linear shift-invariant graph filter have been recently proposed. This paper goes one step further and, under the framework of multiple-input multiple-output (MIMO) graph filters, imposes additional structure on the adopted graph filters, to obtain three new (more parsimonious) architectures. The proposed architectures result in a lower number of model parameters, reducing the computational complexity, facilitating the training, and mitigating the risk of overfitting. Simulations show that the proposed simpler architectures achieve similar performance as more complex models. ", "text": "abstract—superior performance ease implementation fostered adoption convolutional neural networks wide array inference reconstruction tasks. cnns implement three basic blocks convolution pooling pointwise nonlinearity. since ﬁrst operations welldeﬁned regular-structured data audio images application cnns contemporary datasets information deﬁned irregular domains challenging. paper investigates cnns architectures operate signals whose support modeled using graph. architectures replace regular convolution so-called linear shift-invariant graph ﬁlter recently proposed. paper goes step framework multiple-input multipleoutput graph ﬁlters imposes additional structure adopted graph ﬁlters obtain three architectures. proposed architectures result lower number model parameters reducing computational complexity facilitating training mitigating risk overﬁtting. simulations show proposed simpler architectures achieve similar performance complex models. convolutional neural networks emerged information processing architecture choice wide range ﬁelds diverse pattern recognition computer vision medicine solving problems involving inference reconstruction tasks cnns demonstrated remarkable performance well ease implementation online computational complexity cnns take input data process several layers performs three simple operations output previous layer. three operations convolution pointwise nonlinearity pooling. objective architecture progressively extract useful information local features global aspects data. mainly achieved combination convolution pooling operations sequentially combine data located away. nonlinearity dons architecture enough ﬂexibility represent richer class functions describe problem. work supported ccf- spanish mineco grants tec--r tec--r. gama ribeiro dept. electrical systems eng. univ. pennsylvania. marques dept. signal theory comms. king juan carlos univ. leus dept. microelectronics delft univ. technology. emails {fgamaaribeiro}seas.upenn.edu antonio.garcia.marquesurjc.es g.j.t.leustudelft.nl training datasets means backpropagation algorithm implies architecture capable learning useful features task hand. intimately related capability successful training fact ﬁlters used small thus containing parameters making training easier. while convolution pooling well-deﬁned regular domains time space contemporary data increasingly described domains exhibit irregular behavior examples including marketing social networks genetics objective extending remarkable performance cnns broader data domains extensions capable processing network data developed survey. particular works make concept graph ﬁlters extend convolution operation graph signals leveraging framework multiple-input multiple-output existing results paper proposes three novel architectures gf-based cnns. main idea replace bank parallel structured ﬁltering block reduces degrees freedom layer. architecture facilitates training incurs reduced computational complexity beneﬁcial avoid overﬁtting. section introduces notation reviews existing gfbased cnns framework mimo gfs. section describes novel architectures. section presents simulations showing beneﬁts schemes. concluding remarks provided section input data deﬁned ﬁeld output data function general objective machine learning estimate learn function neural network information processing architecture aims constructing estimator consists concatenation layers applies three simple operations output layer before namely linear transform pointwise nonlinearity pooling operator. formally estimator written ···◦ denotes operations applied layer denote n-dimensional output layer deﬁned ﬁeld linear transform ﬁelds pointwise nonlinearity pooling operator. then layer described known linear shift-invariant since lsi-gfs regarded generalization convolutions operate graph signals operator used extend cnns operate graph signals speciﬁcally assume layer output consists features deﬁned nconsidered graph signal node graph described rn×n then features concatenated vector )t]t rfn×. assume linear transform constructs features existing ones. then regarded mimo since takes input signals outputs graph signals. denoting kronecker matrix product output convolution operation graph signals compactly written mimo follows rf×f− contains ﬁlter taps corresponding lsi-gfs employed. precisely denoting ﬁlter taps ﬁlter k]ff k−ff]t written construction builds different lsi-gfs features contained previous layer totaling lsi-gfs. total number trainable parameters thus f−fk. written differently mimo represents perlayer architecture proposed finally respect pooling operation multiscale hierarchical clustering reduce size graph layer employed yielding corresponding layer also computational performance issues clustering alternative approaches rely pooling exist work focus convolutional operation cnns graph signals letting user determine preferred choice pooling scheme. paper proposes three architectures cnns graph signals obtained imposing certain parsimonious representation mimo matrices hk}. resulting architectures yield considerably lower number trainable parameters reducing complexity network well avoiding certain pitfalls overﬁtting particular assumes linear operator comprised collection ﬁlters small support hf}. then application linear operator yields collection signals {h∗x− element considered feature assumed −]i−k using ﬁlters small support twofold goal. first convolution operation linearly relates nearby values consolidates feature value aggregates local information. second ﬁlters parameters therefore easy learned data. also note pooling operation serves function changing resolution data layer nearby values related convolution operator actually located away. convolution pooling operations tandem guarantee aggregates information different levels local global. operation convolution particular depends upon existence notion neighborhood. notion also exists domains like manifolds graphs thus convolution extended operate signals deﬁned irregular domains. particular signals deﬁned graphs start considering graph nodes edges function assigns weights edges. neighborhood node deﬁned nodes notations place graph signal deﬁned associates real value element graph signal conveniently represented vector i-th element corresponding value signal node order relate values graph signal node neighborhood make matrix description graph. precisely rn×n graph shift operator matrix whose element nonzero note then linear combination values signal neighbors. precisely that second equality follows {i}. operation basic element extend notion convolution signals deﬁned graphs; e.g. first observe collects information one-hop neighborhood node collects information k-hop neighborhood node. denote collection note output features actually same since depend thus ﬁlter taps given actually yield single output feature graph signal given could particularly useful reducing operational complexity subsequent layers since graph signals handled easily. observe imposed structure containing distinct ﬁlters yields trainable parameters. convolution features third approach reducing number parameters involved consider ﬁlters applied sequentially progressively input features yielding output features resemble convolutions input features. hp−q+ ﬁlter taps then third proposed strategy matrix becomes observe output feature thought convolution input feature vector collection ﬁlter taps given {hkp hkp+−q}. note also consecutive output features weigh input features similarly. acts smoother input features. matrix toeplitz matrix thus elements total number trainable parameters curse dimensionality simplicity focus speciﬁc layer hence dropping subscript notations. denote input rqn× input features denote output features output features length ﬁlters matrix ﬁlter taps rp×q elements hkpq ﬁlters represented vector ﬁlter taps rk×. equation becomes first propose aggregate input features reduce number ﬁlters. instead designing different ﬁlters input features ﬁrst aggregate input features graph signal proceed design different ﬁlters applied graph signal. strategy amounts designing ﬁlter taps then matrix becomes replication ﬁrst column observe structure imposed leads different lsi-gfs therefore number trainable parameters reduced effect ﬁlter observed ﬁrst aggregate input features applying different second proposed architecture consists designing ﬁlter input feature consolidating ﬁltered input features single output feature. order this need design lsi-gfs described ﬁlter taps speciﬁc compare performance three architectures proposed sections iii-a iii-b iii-c involve respectively parameters involves parameters ﬁrst testcase consider synthetic dataset source figure accuracy source localization problem. results averaged across different graph realizations. clarity ﬁgures error bars represent estimated variance. function probability keeping sample training phase i.e. probdropout. function noise test set. function number training samples. overall observe full architecture yields performance similar proposed section iii-a. localization problem different diffused graph signals processed determine single node originated them. second testcase news dataset wordvec embedding underlying graph classify articles different categories problems evaluate architecture convolutional layers ﬁrst generating features second outputting features. length pooling employed number nodes graph speciﬁc testcase. denote architecture ones developed sections iii-a iii-b iii-c respectively. number parameters convolutional layers selected nonlinearity relu applied layer architectures include readout layer. training stage problems adam optimizer learning rate employed epochs batch size consider connected stochastic block model graph nodes divided communities intracommunity edge probability intercommunity edge probability denote adjacency matrix. representing graph signal taking value node elsewhere signal wtδc diffused version sparse input unknown objective determine source originated signal irrespective time create ntrain labeled training samples wtδc chosen random. create test ntest samples fashion i.i.d. zero-mean gaussian noise variance signals classiﬁed wtδc goal first source localization problem different realizations randomly generated graphs. graph train four architectures using dropout probability keeping training sample total number training samples trained architectures tested test samples graph contaminated noise variance results listed table accuracy averaged samples different graph realizations. observe architecture proposed section iii-a outperforms full architecture orders magnitude less parameters. also architectures yields reasonable performances. additionally tests changing values several simulations parameters. fig. observe accuracy obtained varying probability keeping training samples. noted architecture performs well full architecture. also observed architectures signiﬁcant variance implies depend heavily topology graph. effect noise test samples observed fig. observe four architectures relatively robust noise. architectures exhibit performance highest noise value simulated. finally fig. show performance four architectures function number training samples. full architecture improve performance training samples considered huge increase training samples. increase observed remaining architectures although performance behaves somewhat erratically afterwards. simulations observe architecture performs well full architecture utilizing almost orders magnitude less parameters. also observe architectures high dependence topology network. classiﬁcation articles news dataset consists texts graph signals constructed document represented using normalized bag-of-words model underlying graph support constructed using graph wordvec embedding considering common words. adopted normalized laplacian. dropout used training phase. accuracy results listed table demonstrating architectures outperform full requiring almost times less parameters. paper studied problem extending cnns operate graph signals. precisely reframed existing architectures concept mimo leveraged structured representations ﬁlters reduce number trainable parameters involved. proposed three architectures arises adopting different parsimonious model mimo matrices. resulting architectures yield lower number trainable parameters reducing computational complexity well helping avoiding certain pitfalls training like overﬁtting curse dimensionality. applied three proposed architectures synthetic problem source localization compared performance complex full mimo model. analyzed performance function dropout probability training phase noise test samples number training samples. noted proposed architecture aggregates input features performance similar full model involving orders magnitude less parameters. architectures offer comparable performance certain values analyzed parameters. finally utilized proposed architectures problem classifying articles news dataset. case observed proposed parsimonious models outperform full model. defferrard bresson vandergheynst convolutional neural networks graphs fast localized spectral ﬁltering neural inform. process. syst. barcelona spain dec. nips foundation. kipf welling semi-supervised classiﬁcation graph convolutional networks int. conf. learning representations toulon france apr. assoc. comput. linguistics. huang maaten weinberger densely ieee comput. soc. conf. connected convolutional networks comput. vision pattern recognition honolulu july ieee comput. soc. mikolov chen corrado dean efﬁcient estimation word representations vector space int. conf. learning representations scottsdale assoc. comput. linguistics.", "year": "2018"}