{"title": "Harnessing Structures in Big Data via Guaranteed Low-Rank Matrix  Estimation", "tag": "eess", "abstract": " Low-rank modeling plays a pivotal role in signal processing and machine learning, with applications ranging from collaborative filtering, video surveillance, medical imaging, to dimensionality reduction and adaptive filtering. Many modern high-dimensional data and interactions thereof can be modeled as lying approximately in a low-dimensional subspace or manifold, possibly with additional structures, and its proper exploitations lead to significant reduction of costs in sensing, computation and storage. In recent years, there is a plethora of progress in understanding how to exploit low-rank structures using computationally efficient procedures in a provable manner, including both convex and nonconvex approaches. On one side, convex relaxations such as nuclear norm minimization often lead to statistically optimal procedures for estimating low-rank matrices, where first-order methods are developed to address the computational challenges; on the other side, there is emerging evidence that properly designed nonconvex procedures, such as projected gradient descent, often provide globally optimal solutions with a much lower computational cost in many problems. This survey article will provide a unified overview of these recent advances on low-rank matrix estimation from incomplete measurements. Attention is paid to rigorous characterization of the performance of these algorithms, and to problems where the low-rank matrix have additional structural properties that require new algorithmic designs and theoretical analysis. ", "text": "low-rank modeling plays pivotal role signal processing machine learning applications ranging collaborative ﬁltering video surveillance medical imaging dimensionality reduction adaptive ﬁltering. many modern high-dimensional data interactions thereof modeled lying approximately low-dimensional subspace manifold possibly additional structures proper exploitations lead signiﬁcant reduction costs sensing computation storage. recent years plethora progress understanding exploit low-rank structures using computationally eﬃcient procedures provable manner including convex nonconvex approaches. side convex relaxations nuclear norm minimization often lead statistically optimal procedures estimating low-rank matrices ﬁrst-order methods developed address computational challenges; side emerging evidence properly designed nonconvex procedures projected gradient descent often provide globally optimal solutions much lower computational cost many problems. survey article provide uniﬁed overview recent advances low-rank matrix estimation incomplete measurements. attention paid rigorous characterization performance algorithms problems low-rank matrix additional structural properties require algorithmic designs theoretical analysis. ubiquity advanced sensing imaging technologies produce vast amounts data unprecedented rate. fundamental goal signal processing extract possibly track evolution relevant structural information faithfully high-dimensional data ideally minimal amount computation storage human intervention. overcome curse dimensionality important exploit fact real-world data often possess low-dimensional geometric structures. particular structures allow succinct description data number parameters much smaller ambient dimension. popular postulate low-dimensional structures sparsity signal represented using nonzero coeﬃcients proper domain. instance natural image often sparse representation wavelet domain. ﬁeld compressed sensing made tremendous progress capitalizing sparsity structures particularly solving under-determined linear systems arising sample-starved applications medical imaging spectrum sensing network monitoring. applications compressed sensing techniques allow faithful estimation signal interest number measurements proportional sparsity level much fewer required traditional techniques. power compressed sensing made disruptive technology many applications magnetic resonance imaging cardiac cine scan performed within seconds patients breathing freely. sharp contrast previous status scan takes minutes patients need hold breaths several times sparsity model powerful original framework compressed sensing mainly focuses vector-valued signals admit sparse representations priori known domain. however knowledge sparsifying domains always available thus limiting applications. fortunately resort general notion sparsity versatile handling matrix-valued signals ensemble vector-valued signals without need specifying sparsifying basis. paper review powerful generalization sparsity termed low-rank model captures much broader class low-dimensional structures. roughly speaking model postulates matrix-valued signal approximately low-rank. view column matrix data vector equivalent saying data approximately lies low-dimensional unknown subspace. historically exploitation low-rank structures begin even earlier sparsity. particular low-rank assumption underlies classical principal component analysis builds observation real-world data variance ﬁrst principal components. low-rank structures arise various physical reasons engineering designs. face recognition face images found trace -dimensional subspace approximately convex reﬂect light according lambert’s radar sonar signal processing signals reside approximately low-dimensional subspace transmitting using small waveforms construct certain beam patterns. low-rank structures also arise modeling interactions diﬀerent objects. example clustering embedding pairwise interactions objects often expressed low-rank matrix given collected data problem infer hidden low-dimensional subspace captures information relevant subsequent tasks detection clustering parameter estimation. traditional methods singular value decomposition ﬁnding principal subspaces typically require data fully observed. however modern data applications often involve estimation problems number measurements much smaller ambient dimension regime similar setting compressed sensing. refer problem low-rank matrix estimation emphasizing fact under-sampled measurements partial observations. examples problems abundant. recommendation systems goal estimate missing ratings given small number observed ones. sensor networks important problem infer locations sensors pairwise distance measures available sensors within certain radius other. wideband spectrum sensing reduce sampling rate popular approach estimate signal subspace bearing parameters randomly sub-sampling outputs array. applications desirable develop low-rank matrix estimation algorithms statistically eﬃcient achieving estimation errors minimal amount measurements computationally eﬃcient running time storage cost. particular focus paper algorithms come provable guarantees statistical computation eﬃciency. search algorithms part motivated remarkable success story compressed sensing many provable methods developed sparse models. handling general low-rank structures poses challenges well opportunities. study low-rank matrix estimation attracted attention many researchers diverse communities including signal processing machine learning statistics mathematical programming computer science elaborate below enterprise much fruitful resulting many powerful algorithms novel analytical techniques deep theoretical insights. survey article complementary nice overview article low-rank matrix recovery davenport romberg diﬀerent focuses. particular focusing recent algorithmic advancements computational statistical guarantees highlight eﬀectiveness ﬁrst-order methods convex nonconvex optimization. also speciﬁc emphasis unique applications involving structured matrix completion. paper organizations rest paper organized follows. section motivates low-rank models perspectives modeling data correlations lifting vector-valued problems. section describes basic mathematical setup low-rank estimation problem. section discusses theory algorithms low-rank matrix estimation convex optimization. section discusses theory algorithms low-rank matrix estimation nonconvex optimization. section discusses structured matrix completion low-rank matrices additional structural constraints using several concrete examples. numerical examples real-world recommendation dataset showcased section paper concluded section notations throughout paper boldface capital letters denote matrices transpose entry. similarly boldface lower-case letters denote vectors conjugate transpose i-th entry. expectation denoted addition stand spectral norm frobenius norm norm norm rows) trace nuclear norm matrix matrices size trb) denotes trace inner product. notation diag denotes diagonal matrix whose diagonal entries given vector denote i-th standard basis vector section elucidate motivations studying low-rank modeling low-rank matrix estimation problems. start classical viewpoint justify low-rank priors data matrix perspective bias-variance trade-oﬀs modeling correlations data observations. next argue low-rank structures arise powerful reformulation quadratic optimization problems lifting matrix space. last least provide list sources low-rank structures wide range science engineering problems. correlation-aware modeling data matrices ﬁrst motivate low-rank models general principle bias-variance trade-oﬀ signal estimation processing given noisy data classical viewpoint articulated scharf tufts consider stationary signal covariance matrix rn×n. suppose eigenvalue decomposition given rn×n eigenvectors diag eigenvalues arranged non-increasing sequence. deﬁne rn×r matrix whose columns eigenvectors matrix corresponding largest eigenvalues. sees projection operator onto subspace spanned top-r eigenvectors elements noise vector independent variance estimate signal using reduced-rank model rank projecting observed data onto r-dimensional principal subspace estimate given figure mean squared error decomposition bias variance signal estimation problem additive gaussian noise moderate assuming low-rank model. suggests beneﬁcial apply reduced-rank model data correlated. decomposition increases rank bias estimate decreases whereas corresponding variance increases. therefore choice rank controls trade-oﬀ bias variance whose constitutes total estimation error. importantly many real-world datasets decaying spectrum notation means eigenvalues covariance matrix decrease rapidly. mentioned insight foundation moreover observed across wide range applications including power systems recommendation systems internet traﬃc weather data. consequently beneﬁcial employ small rank variance controlled bias remains small long residual eigenvalues decreases quickly. mind show fig. mean squared error function rank well decomposition bias variance; assumed spectrum decays rate signal-to-noise ratio equal clear ﬁgure employing appropriate low-rank estimator induces much lower mean squared error full-rank one. particular optimal rank much smaller ambient dimension spectrum decays fast. lifting quadratic bilinear optimization problems another important source low-rank structures solving quadratic/bilinear optimization problems. example consider phase retrieval problem important routine x-ray crystallography optical imaging goal recover vector given magnitudes linear measurements nonlinear nature equations diﬃcult solve directly particularly problem size large. popular approach solving equations called lifting rewrites equations terms matrix variable casts problem recovering rank-one matrix linear measurements similar formulation used blind deconvolution problem; lifting approach applied classes quadratic equations whose lifting formulations lead low-rank matrices rank larger one. instance problem sensor network localization goal determine locations points/sensors {xi}n lying r-dimensional euclidean space given subset pairwise distances. complete pairwise euclidean distances arranged matrix rn×n interestingly pairwise distance fact linear function rank-r positive semideﬁnite matrix rn×r; precisely therefore problem determining locations sensors equivalent recovering low-rank lifted matrix linear measurements form detailed treatment powerful reformulation. recommendation systems matrix user ratings items often approximately low-rank user preferences typically depend small number underlying factors hence ratings correlate other. background video usually changes slowly frame frame hence stacking frames columns lead approximately low-rank matrix similar low-rank structures arise smoothness properties visual physical objects quantum state tomography density matrix pure nearly pure quantum state approximately low-rank exploited problem state reconstruction small number pauli measurements sparse graphical model latent variables show using schur complement inverse marginal covariance matrix observed variables approximated matrix rank equal number latent variables matrices certain monotonicity properties well-approximated matrix rank much smaller ambient dimension. matrices arise example measuring pairwise comparison scores objects possess underlying ordering list continues much longer. ubiquity structures either physical property engineering choice makes low-rank models useful motivates extensive study low-rank matrix estimation problem. section formally deﬁne problem low-rank matrix estimation recovery low-rank matrix number measurements much smaller dimension matrix. rn×n matrix-valued signal interest. denote mentioned many modern applications directly observe rather given under-determined indirect noisy measurements assume access linear measurements form rn×n linear measurement operator deﬁned vector noise terms. denote conjugate operator primarily concerned estimating measurements direct approximation eckart-young theorem impossible. instead need develop alternative methods low-rank solution best noisy under-determined linear equations categorize low-rank matrix estimation problem main types based structure measurement operator low-rank matrix completion directly observes subset entries aims interpolate missing entries. case sparse matrix single entry equal corresponding observed index. development eﬃcient algorithms low-rank estimation owes much inspiration success compressed sensing there convex relaxation approach based -minimization widely used recovering sparse signals. low-rank problems role norm replaced matrix counterpart namely nuclear norm convex surrogate rank. idea gives rise convex optimization approaches low-rank estimation based nuclear norm minimization approach forth fazel seminal work approach since extensively developed expanded remains mature well-understood method estimating low-rank matrices. section provide survey algorithmic approach associated theoretical results. convex relaxation nuclear norm minimization begin deriving nuclear norm minimization algorithm convex relaxation rank minimization. recall linear measurement model seek low-rank solution. natural approach matrix minimum rank consistent measurement formulated optimization problem rank however non-convex function rank minimization known np-hard general. develop tractable formulation observes rank equal number nonzero singular values. therefore analogously using norm convex surrogate sparsity replace rank singular values quantity known nuclear norm case measurements noisy seeks matrix small nuclear norm approximately consistent measurements formulated either regularized optimization problem guarantees matrix sensing hope recovering output sensing process sensing operator needs possess certain desirable properties distinguish diﬀerent low-rank matrices. property called restricted isometry property stipulates viewed mapping lower-dimensional space preserves euclidean distances low-rank matrices give general notion distances mapping measured diﬀerent norms deﬁnition reminiscent similar notion name used sparse signal recovery literature imposed sparse vectors certifying whether holds given operator known np-hard nevertheless turns generic sensing operator drawn certain random distributions satisﬁes high probability. example rip-/p holds nuclear norm minimization approach guarantees exact stable recovery low-rank matrix noise-free noisy cases shown following theorem adapted theorem suppose noise satisﬁes satisﬁes rip-/p solution nuclear norm minimization algorithms satisﬁes error bound guarantees matrix completion incoherence case matrix completion additional complication arises impossible recover low-rank matrix also sparse. particular samples small subset entries likely most nonzero entries missed. means sensing operator used matrix completion cannot satisfy rip. therefore problem well-posed need restrict attention low-rank matrices whose mass concentrate entries. property formalized notion incoherence measures alignment column/row spaces low-rank matrix standard basis vectors deﬁnition matrix rn×r orthonormal columns orthogonal projection onto column space incoherence parameter deﬁned easy incoherence parameter satisﬁes bound n/r. smaller column space spread coordinates. matrix incoherence parameters determined singular vectors independent singular values. noiseless setting nuclear norm minimization perfectly recover incoherent low-rank matrix soon number measurements slightly larger degrees freedom matrix. recovery guarantees proved reﬁned series work theorem adapted state-of-the-art. theorem suppose entry observed independently probability satisﬁes coupon-collecting argument fact show impossible recover matrix less measurements using algorithm. therefore theorem shows nuclear norm minimization near-optimal terms sample complexity logarithmic factor remarkable fact considering using convex relaxation rank. noisy case study performance nuclear norm minimization terms recovery error done example state performance guarantee taken assume entries noise independent variance scaled then solving regularized nuclear norm minimization problem parameter obtain solution satisfying first-order algorithms nuclear norm minimization principle possible solve nuclear norm minimization problems high numerical accuracy using oﬀ-the-shelf semideﬁnite programming solvers however solvers typically based interior-point methods extremely slow size matrix large. example sdpt handle matrices dimensions larger thousands memory requirements. computational issue motivates development fast alternatives handle signiﬁcantly larger problems. first-order algorithms become appealing candidate per-iteration cost well ﬂexibility incorporate speciﬁc structures semideﬁnite programs arise low-rank matrix estimation. long still growing list algorithms including singular value thresholding accelerated proximal gradient descent variant fista matrix completion augmented lagrangian multiplier methods frank-wolfe cogent adcg name few. below discuss representative algorithms fista solving regularized problem frank-wolfe solving constrained problem algorithms provide stage understanding many algorithms. important subroutine many aforementioned algorithms singular value thresholding operator mathematically deﬁned proximal mapping respect nuclear norm fact operator eﬃciently computed leveraged many ﬁrst-order algorithms. fista algorithm regularized problem given algorithm upper bound lipschitz constant fista makes nesterov’s momentum acceleration speed convergence. denote objective function iterations. iteration partial needed evaluate operator. large-scale problems still slow require large memory. case make modern randomized techniques numerical linear algebra speed computation standard frank-wolfe method solving constrained problem presented algorithm iteration algorithm requires computing rank-one done using power iteration lanczos methods. therefore frank-wolfe typically much lower computational cost per-iteration methods based operation. however standard frank-wolfe converge slowly practice. achieve \u0001-accuracy i.e. frank-wolfe requires iterations quite slow. variants frank-wolfe faster convergence lower memory footprint actively developed recently exploiting problem structures. list including cogent in-face extended frank-wolf adcg block frank-wolfe sketchycgm still growing. seen computational concern solving rank minimization problems assuaged extent convex relaxation resulting semideﬁnite programs solved time polynomial matrix dimension. however large-scale problems dimension order millions solving semideﬁnite programs even using ﬁrst-order methods still computationally infeasible fundamental bottleneck storing optimizing matrix variable. issue severely limits applicability convex relaxation methods. overcome diﬃculty recent line work studies computationally eﬃcient methods based nonconvex optimization. methods work directly original nonconvex rank-constrained rn×n given loss function typically convex idea reparametrization trick writing rank-r matrix factorization form rn×r rn×r enforce low-rank constraint directly leading following equivalent formulation lqqt pairs global optima problem certain conditions discussed below. reformulation brings signiﬁcant computational gain since rank often much smaller min{n size variables roughly linear rather quadratic leading possibility designing linear-time algorithms amenable problems large scale. surprisingly even though burer-monteiro formulation nonconvex global optima sometimes found eﬃciently using various iterative procedures; moreover rigorous guarantees derived statistical accuracy resulting solution. indeed several iterative schemes computational cost proportional poly size input least iteration typically much lower results developed still growing line recent work devote rest section presenting representative results therein. neither function low-rank matrices convex standard global convergence theory convex optimization apply here. recent breakthrough based realization convexity fact necessary convergence iterative schemes; instead long gradients function always point towards desired solution iterates make progress along right direction. note property concerns geometry itself largely independent speciﬁc choice algorithm among three options projected gradient descent approach stands simple form cheap per-iteration cost eﬃciency constrained problems. thus projected gradient descent focal point survey. playing role statistical modeling probabilistic analysis show desired geometric properties high probability probabilistic generative models data thereby circumventing worst-case hardness low-rank matrix estimation problem instead focusing average-case behavior. existing results direction divided categories. ﬁrst line work reviewed section shows iterative algorithms converge desired solution rapidly initialized within large neighborhood around ground truth; moreover good initial solution obtained eﬃciently simple procedures second line work reviewed section concerns global landscape loss function aims show spite non-convexity local minima fact close desired solution appropriate sense whereas stationary points possess descent direction; properties guarantee convergence iterative algorithms initial solution. types results merits hence complementary other. review representative results categories noiseless matrix sensing matrix completion. readers referred extensions noisy case. simplicity assume truth exactly rank-r bounded condition number σ/σr thus eﬀectively hiding dependence moreover measure convergence algorithms purpose reconstructing shall consider directly reconstruction error ltrtt respect following theorems guarantee initial solution reasonably close desired solution iterates converge linearly ground truth conditions similar nuclear norm minimization. moreover provably good initial solution using so-called spectral method involves performing partial svd. particular compute svdr] matrix sensing svdr] matrix completion. theorem suppose sensing operator satisﬁes rip-/ parameter max{δr ¯δr} suﬃciently small constant then initial solution svdr] satisﬁes theorems guarantees gradient descent iterates enjoy geometric convergence global optimum initial solution suﬃciently close ground truth. comparing guarantees nuclear norm minimization gradient descent succeeds similar sample complexity condition computational cost signiﬁcantly lower. discuss overall computational complexity simplicity shall assume matrix sensing maximum time multiplying matrix vector compatible dimension. gradient step performed time complexity computing initial solution subtler. ﬁrst note standard matrix perturbation bounds show matrix singular values suﬃciently close condition theorem particular cσr/ r-th singular values cσr/ away corresponding singular values small constant. properties need compute exact singular values/vectors order meet initialization condition rather suﬃces rank-r approximation property cσr/ done using example randomized procedure takes time compute; together overall time complexity achieving ε-accuracy. matrix completion obtain initial solution follow similar arguments show suﬃces compute rank-r approximation matrix p−pω close suﬃciently large spectral gap. since p−pω sparse matrix support computing approximation done time using randomize procedure step gradient descent requires computing gradient projection onto involve operations sparse matrices supported thin matrices done time therefore projected gradient descent achieves \u0001-accuracy running time remark reﬁned analysis gradient descent fact possible drop projection step onto matrix completion without performance loss; particular long constant gradient descent converges geometrically needs iterations reach \u0001-accuracy reconstruction error measured frobenius norm ltrtt also spectral norm ltrtt entry-wise inﬁnity norm ltrtt singular value projection algorithm geometric convergence entry-wise inﬁnity norm also established work without need additional regularization separate initialization procedure. remark noisy setting algorithms applied without change error bounds hold additional term depends noise. matrix completion term noise matrix supported observed indices. term bounded various noise models. example i.i.d. gaussian entries zero mean variance global geometry saddle-point escaping algorithms recent line work studies global geometry burer-monteiro formulation well computational implications algorithms starting arbitrary initial solution results based geometric notion called strict saddle property λmin exists local minimum figure figure provide examples one-dimensional function two-dimensional function respectively satisfy strict saddle property deﬁnition intuitively strict saddle property ensures whenever already close local minimum current solution either large gradient descent direction hessian negative eigenvalue. therefore local search algorithms capable ﬁnding descent direction make progress decreasing value eventually converge local minimum. many algorithms shown enjoy property include cubic regularization trust-region algorithms stochastic gradient descent recent variants algorithm describe algorithm namely perturbed gradient descent algorithm based standard gradient descent algorithm following additional steps gradient small indicating potential closeness saddle point adds random perturbation current iterate last perturbation occurs tthres iterations function value decrease suﬃciently since terminates outputs iterate last perturbation. figure example one-dimensional strict saddle function function local minima also global minima well local maximum region function satisﬁes λmin local minima rest real line function satisﬁes |∇g| delve details saddle-escaping algorithms parameter choices run-time guarantees somewhat technical. rather purpose analyzing low-rank matrix estimation problems simply rely existence algorithms fact running time depends polynomially problem parameters. summarized following theorem abstracts results work cited above. theorem assume β-smooth strict saddle. exist algorithms output solution ζ-close local minimum required number iterations upper bounded polynomial function specializing low-rank matrix estimation problem remains verify loss function deﬁned strict saddle local minima good sense correspond low-rank matrix equal true matrix consider loss function matrix sensing figure example two-dimensional strict saddle function function local minima strict saddle point xsaddle hessian saddle point strictly negative eigenvalue eigenvector regularization parameters max{x regularization plays similar role projections plpr previously used encourages incoherence replacing projections regularization leads unconstrained formulation strict-saddle framework above. follow theorems show loss functions indeed desired strict-saddle property high probability sample complexity conditions similar before. theorem suppose measurement operator satisfy rip-/ parameter max{δr ¯δr} loss function satisﬁes following combining theorem theorem theorem conclude iterative algorithms optimizing factor variables converge globally pair satisfying polynomially number iterations arbitrary initial solutions long escape saddle points. refer readers discussions. neighborhood global minimum loss function essentially strongly convex saddle point. within neighborhood gradient descent iterative algorithms converge geometrically. iterative algorithms escape saddle points enter neighborhood global minima polynomial time. alternatively solution neighborhood performing matrix appropriately constructed observations. comparing approaches non-convex matrix estimation discussed last subsections also strengths. ﬁrst approach taken section focuses convergence algorithms proper initialization procedure. approach immediately leads simple eﬃcient algorithms provably geometric convergence linear time-complexity. also readily extends problems additional structural constraints involve complicated loss functions involve non-smooth loss function whose hessian deﬁned. however ﬁnding good initialization scheme non-trivial settings actually harder part problem. second approach taken section instead focuses global geometric landscape problem. approach conceptually elegant nicely decomposing geometric aspect algorithmic aspect problem. computationally eliminates need careful initialization resulting run-time guarantees somewhat weaker super-linear dimensions. course made distinction approaches mostly ease review state-of-the-art; given rapid developments area expect approaches improved expanded eventually merged. concluding section deeper reason low-rank matrix estimation benign nonconvex problem. loss function low-rank matrix estimation problem often viewed certain precise sense perturbed version objective function i.e. ﬁnding best rank-r approximation frobenius norm factorized form example matrix completion loss function regularization omitted exactly equal objective expectation. problem arguably well-understood tractable non-convex problem addition closed-form solution problem satisﬁes geometric properties mentioned last subsections particular local minima saddle points expressed terms non-top eigen components respectively. probabilistic assumptions sensing operators geometric algorithmic properties problem essentially preserved incomplete observations high probability long issue incoherence appropriately accounted for. many applications low-rank matrix estimation problems possess additional structures need carefully exploited present examples section hankel matrix completion recovery clustering matrices. hankel matrix completion imagine interested estimating spectrum time series direction-of-arrivals returns sensor array. model signal interest weighted complex exponentials i.e. figure illustration structured matrices considered section observation pattern hankel matrix completion problem. cluster matrix aﬃnity matrix cluster matrix recovery problem nodes ordered according cluster structure. except nodes randomly permuted. spectral compressed sensing concerns problem recovering signal subset entries. important problem super resolution imaging system identiﬁcation denoting index observed entries goal recover given literature compressed sensing problem typically approached ﬁrst discretizing parameter space solving minimization problem standard sparse recovery. however approach sensitive discretization used cannot resolve parameters fall grid turns certain mild conditions problem solved exactly structured matrix completion problem without assuming discretization/grid. insight lies exploiting shift invariance property embedded structure complex harmonics. done constructing n-by- hankel matrix spanned signal vector diag n−n+ deﬁned similar decomposition shows rank) equality holds poles distinct. representation structured low-rank matrix leveraged facilitate recovery un-measured entries particular recover missing measurements seeking hankel matrix smallest nuclear norm figure illustrates observation pattern hankel matrix recovery problem highly structured. parametric model deﬁne notion incoherence bears interesting physical whose absolute value decays inverse proportionally respect |z|. given poles construct gram matrices corresponding column space space entries matrices speciﬁed poles well-separated incoherence parameter bounded small constant poles closer gram matrices become poorly-conditioned resulting large therefore incoherence parameter provides measure hardness recovery problem terms relative positions poles. theorem summarizes performance guarantees emac algorithm. theorem suggests hankel-structured low-rank matrix faithfully recovered using number measurements much smaller dimension recently shown hankel matrix completion also eﬃciently solved using non-convex burer-monteiro factorization projected gradient descent approach described section similar conditions similar results obtained block hankel toeplitz low-rank matrix completion well. interestingly toeplitz matrix additionally positive-semideﬁnite incoherence condition relaxed exploring connection carathéodory’s theorem; cluster matrices suppose given aﬃnity matrix rn×n nodes measure pairwise similarity/aﬃnity nodes example indicator friendship facebook users similarity movies netﬂix. goal partition nodes several clusters nodes within clusters high aﬃnity values. problem known clustering community detection. number clusters size k-th cluster denotes -by- all-one matrix. clear rank equal number clusters moreover matrix several additional structural properties binary block-diagonal positive-semideﬁnite diagonal entries equal one. fact nodes cluster tend high aﬃnity values captured model form noise encapsulates inherent randomness/uncertainty pairwise aﬃnity measure. many applications aﬃnity values pairs nodes unknown costly measure case observe subset entries setup clustering problem cast noisy low-rank matrix completion problem algorithms theory last sections immediately applied. notably take advantage additional structures cluster matrix obtain stronger performance guarantees. particular sometimes possible recover exactly even presence noise. brieﬂy review result consider setting noise term bernoulli τout; case nodes clusters higher probability high aﬃnity value. figure illustrate cluster matrix aﬃnity matrix nodes ordered according cluster structure shows aﬃnity matrix except nodes randomly permuted typically observed practice. before assume entry observed probability model sometimes referred stochastic block model planted partition model literature model consider maximum likelihood estimator derive convex relaxation replacing non-convex constraints leads semideﬁnite nuclear norm regularizer linear inequality constraints program words provided observation probability diﬀerence τout large enough solution convex relaxation formulation guaranteed structures cluster matrix equal exactly. clustering classical problem studied extensively huge body literature. perspective casting low-rank matrix recovery problem relatively recent proves fruitful leading number algorithms theoretical results. detailed account developments outside scope article refer readers recent surveys references therein. section showcase numerical results applying matrix completion approach real dataset namely movielens dataset dataset consists ratings taking values users movies. work popular version data partitions entire dataset ‘u.data’ training ‘ua.base’ test ‘ua.test’ test contains exactly ratings user. delete movies ratings available. denote original incomplete rating matrix generated rows corresponding users columns corresponding movies index available ratings contained ‘u.data’. notation -by- matrix further denote disjoint index sets ratings training testing respectively experiment training data input matrix completion algorithm. completed matrix produced algorithm denoted used estimate unobserved ratings evaluated test demonstrate performance three matrix completion algorithms accelerated proximal gradient singular value projection bi-factored gradient descent algorithms existing implementations publicly available codes mostly adopt default settings adjustments tailored speciﬁc dataset. error metric normalized mean absolute errors training test deﬁned respectively ﬁrst employ accelerated proximal gradient algorithm proposed disable adaptive updating regularization parameter implementation instead ﬁxed one. maximum rank maximum number iterations experiment observe algorithm usually meets stop criteria iterations hence stops early reaching maximum number iterations. using diﬀerent values regularization parameter outputs estimate matrix diﬀerent ranks. fig. shows relation regularization parameter rank fig. shows nmaes training data test data diﬀerent ranks. minimum nmae test data achieved regularization parameter rank estimate next consider singular value projection algorithm proposed stopping criteria vtol maximum number iterations again algorithm usually stops early reaching iterations. step size chosen |ω|/ fraction available ratings training set. rank estimate matrix user-speciﬁed tuning parameter svp. nmaes training data test data shown fig. minimum nmae test data achieved rank lastly apply bi-factored gradient descent algorithm proposed variant projected gradient descent algorithm applied non-convex burer-monteiro factorization formulation described section maximum number iterations convergence tolerance similarly before bfgd typically terminates early experiment. step size default setting implementation. nmaes bfgd training data test data figure performance movielens dataset rank estimated matrix respect regularization parameter. nmaes training data test data respect regularization parameter. make several observations experiment results. first consistently across three algorithms training error generally goes rank becomes larger whereas test error exhibits u-shape behavior decreasing ﬁrst increasing later. phenomenon accordance bias-variance tradeoﬀ principle described section particular shows using low-rank model helpful reducing variance prevents overﬁtting. second three algorithms achieve minimum test nmae around using rank small optimal values rank likely highly noisy nature movielens dataset suppressing variance crucial good performance test set. finally estimation/prediction performance algorithms similar computational costs running times memory usage vary. costs depend heavily speciﬁc implementations termination criteria used provide detailed comparison here. low-rank matrices represent important class signals low-dimensional intrinsic structures. article presented recent developments low-rank matrix estimation focusing setting incomplete measurements additional structural constraints. particularly emphasized remarkable modeling power low-rank matrices useful range problems much wider name suggest including presence low-rank structures obvious all. terms algorithms theory attention paid integration statistical computational considerations fast algorithms developed applicable large-scale problems time enjoy provable performance guarantees mild assumptions. seen recent progress made possible combining techniques diverse ﬁelds; particular convex nonconvex optimization well probabilistic analysis play role. conclude mentioning topics future directions covered article. focused matrix sensing completion problems linear measurements. many low-rank estimation problems amenable convex nonconvex optimization-based algorithms enjoy similar geometric properties performance guarantees. partial list problems includes phase retrieval blind deconvolution robust dictionary learning lifting mixture problems low-rank phase retrieval community detection synchronization problems broadly applications low-rank matrix recovery well beyond setting linear measurements least-squares objectives. prime examples include low-rank matrix recovery quantized categorical non-gaussian data ranking comparison-based observations problems involve general objective functions constraints depend speciﬁc observation schemes noise structures. another promising line research aims exploiting hidden low-rank structures settings problem surface nothing low-rank matrices structures reveal suitable transformation approximation. problems type include latent variable models certain smoothness/monotonicity properties another topic much interest select model rank automatically robustly quantify eﬀect model mismatch. important issues even standard matrix sensing completion; discussed issues detail survey. finally omitted many low-rank recovery algorithms directly based optimization including various spectral methods kernel nearest neighbor type methods algorithms combinatorial ﬂavor. algorithms particularly useful problems involving complicated discrete time-evolving structures active/adaptive sampling procedures. topics subject active research tremendous potential.", "year": "2018"}