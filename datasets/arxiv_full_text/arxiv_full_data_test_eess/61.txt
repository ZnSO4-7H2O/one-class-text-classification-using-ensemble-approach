{"title": "Decoding visemes: improving machine lipreading", "tag": "eess", "abstract": " To undertake machine lip-reading, we try to recognise speech from a visual signal. Current work often uses viseme classification supported by language models with varying degrees of success. A few recent works suggest phoneme classification, in the right circumstances, can outperform viseme classification. In this work we present a novel two-pass method of training phoneme classifiers which uses previously trained visemes in the first pass. With our new training algorithm, we show classification performance which significantly improves on previous lip-reading results. ", "text": "systematic study varying number visemes conducted generated viseme sets varying size. used build hidden markov model classiﬁers every viseme set. initialised hmms trained using herest options required model states together force align hmms time-aligned ground truth producing classiﬁcation output. output classiﬁcation supported word bigram model created hbuild hlstats. finally classiﬁcation output compared ground truth measure efﬁcacy measured using correctness undertake machine lip-reading recognise speech visual signal. current work often uses viseme classiﬁcation supported language models varying degrees success. recent works suggest phoneme classiﬁcation right circumstances outperform viseme classiﬁcation. work present novel two-pass method training phoneme classiﬁers uses previously trained visemes ﬁrst pass. training algorithm show classiﬁcation performance signiﬁcantly improves previous lip-reading results. machine lip-reading classiﬁcation utterance visual-only signal many obstacles overcome. some pose motion resolution studied measured including selection phoneme-to-viseme mapping however visemes precisely deﬁned. many working deﬁnitions offered phonemes identical appearance lips visual equivalent phoneme however challenges using viseme labelled classiﬁers including homophone effect enough training data class consequential lack differentiation classes many visemes within set. recently evidence viseme labels needed enough data classiﬁers based phoneme labels outperform viseme classiﬁcation phonemes well studied idea attractive. however others tested small numbers visual units visemes found also give acceptable results would helpful able systematically vary number visual units hence devise optimal strategies learning. rest paper follows; summary analysis effect varying quantity visemes lip-reading performance presented followed short test unit selection effects classiﬁer supporting network results used introduce hypothesis applying weak learning classiﬁer figure shows previous results derived using algorithm described algorithm works merging visemes. example label visemes obtained label visemes. merging stage measure difference correctness compared previous set. signiﬁcant differences figure shown black dots number represents size signiﬁcant set. figure performance classiﬁers visemes poor large number homophones. example homophone data words port bass. using speaker -viseme become ‘/v/ /v/’ i.e. single identiﬁer identifying distinct words. thus distinguishing port bass impossible. large numbers visemes appear improve correctness probably because observed before many phonemes look similar lips looking figure appears sweet spot optimality might found visemes sizes comparable experiments select speakers dataset presented seven male female speakers utters sentences individual speakers tracked using active appearance models extracted features consist concatenated shape appearance information representing mouth area face. previous work essentially examined different algorithms. ﬁrst data labelled phonemes hcompv initialise phoneme classiﬁers repetitions herest train classiﬁers. system advantage output sequence phonemes disadvantage phoneme models hard train. alternative smaller number visemes. data labelled visemes learned viseme classiﬁers hcompv followed herest. method hybrid. initially learn visemes trained visemes become starting point phoneme classiﬁers train phoneme models repeated applications herest thus obtained phoneme models initialisation based upon learned visemes. process illustrated figure example associated initialised replicas likewise initialised replicas retrain phoneme models using training data. full; initialise viseme hmms hcompv. prototype based upon gaussian mixture components three states re-estimated times herest including short pause model state tying forced alignment re-estimates hvite. steps figure classiﬁcation viseme deﬁnitions used initialised deﬁnitions phoneme labelled hmms respective viseme deﬁnition used phonemes relative phoneme-to-viseme mapping. phoneme hmms retrained used classiﬁcation. amendment training analogous weak learning. complete classiﬁcation twice. first phoneme bigram network second word bigram network. apply grammar scale factor transition penalty hvite. implemented using -fold cross-validation replacement advantage training approach phoneme classiﬁers seen positive cases therefore good mode matching disadvantage exposed negative cases degree visemes. systems study paper components. ﬁrst component classiﬁer takes data attempts estimate probable string units. second component language model modiﬁes string basis knowledge units co-located training data. practice course components work together intermediate uncorrected string. figure shows mean speaker-dependent correctness. examine conﬁgurations phoneme classiﬁcation measure phoneme correctness. data series figure word classiﬁcation measure word correctness. lower data series figure blue red. word correctness guessing duplicated plotted orange. series bigram phoneme networks lower series uses viseme classiﬁer upper phonemes denoted wlt. lower pair series bigram word networks show difference visemes method training phoneme classiﬁers. confusion unit measure correctness. possible example build word classiﬁer followed bigram word network measured terms viseme correctness. system would bizarre none-the-less possible. table shows sensible possibilities. ﬁrst table viseme classiﬁer followed viseme bigram network viseme correctness table correctness always measured units classiﬁer. dashed lines group different correctness units. group show viseme correctness compared other second group show phoneme correctness bottom word correctness. data large vocabulary eliminate word level classiﬁers impractical. leaves viseme classiﬁers viseme word network worst performing consider option either. convenience data plotted figure error bars standard error. table minimum maximum range mean correctness measured speakers various methods. table shows word correctness bottom table phoneme correctness. figures show example performances three speakers. whilst monotonic graphs much smoother speaker-dependent graphs shown encouraging implies algorithm optimising learning speaker-dependent phoneme-to-viseme mapping. figure shows that certain numbers visemes certain speakers weak learning method gives improvement. however right number visemes particular speaker method always give signiﬁcant improvement. looking figure appeard regions training method gives marginal improvement. speakers regions. think presence regions associated speakers co-articulation others. true phonemes blurred together learning difﬁcult performance declines. enough speakers make anything speculation stage. observation young people coarticulation people something investigation. choice visual units lip-reading caused debate. workers visemes adduced example fisher others noted lip-reading using phonemes gives superior performance visemes here supply evidence nuanced hypothesis ﬁrst presented intermediary units convenience call visemes provide superior performances provided derived analysis data. good number visemes higher previously thought. paper presented novel learning algorithm shows improved performance datadriven visemes using intermediate step training phoneme classiﬁers. essence method retrain viseme models fashion similar weak learning. two-pass approach training data improved training phoneme labelled classiﬁers increased classiﬁcation performance. helen bear richard harvey barry-john theobald yuxuan finding phonemes improving machine lip-reading joint international conference facial analysis animation audio-visual speech processing isca william fisher george doddington kathleen goudie-marshall darpa speech recognition research database speciﬁcations status proc. darpa workshop speech recognition yuxuan barry-john theobald richard harvey eng-jon richard bowden improving visual features lip-reading international conference audio-visual speech processing vol. e.j. bowden robust facial feature tracking using shape-constrained multi-resolution selected linear predictors ieee transactions pattern analysis machine intelligence vol. helen bear richard harvey barry-john theobald yuxuan resolution limits visual speech recognition ieee international conference image processing ieee helen bear richard harvey barry-john theobald which phoneme-to-viseme maps yuxuan best improve visual-only computer lip-reading? advances visual computing springer helen bear gari owen richard harvey barryjohn theobald some observations computer lipreading moving dream reality spie security+ defence. international society optics photonics g–g. timothy hazen visual model structures synchrony constraints audio-visual speech recognition audio speech language processing ieee transactions vol. elif bozkurt erdem engin erzin tanju erdem ozkan comparison phoneme viseme based acoustic units speech driven realistic animation proceedings signal processing communications applications", "year": "2017"}