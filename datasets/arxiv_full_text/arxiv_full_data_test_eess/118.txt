{"title": "TasNet: time-domain audio separation network for real-time,  single-channel speech separation", "tag": "eess", "abstract": " Robust speech processing in multi-talker environments requires effective speech separation. Recent deep learning systems have made significant progress toward solving this problem, yet it remains challenging particularly in real-time, short latency applications. Most methods attempt to construct a mask for each source in time-frequency representation of the mixture signal which is not necessarily an optimal representation for speech separation. In addition, time-frequency decomposition results in inherent problems such as phase/magnitude decoupling and long time window which is required to achieve sufficient frequency resolution. We propose Time-domain Audio Separation Network (TasNet) to overcome these limitations. We directly model the signal in the time-domain using an encoder-decoder framework and perform the source separation on nonnegative encoder outputs. This method removes the frequency decomposition step and reduces the separation problem to estimation of source masks on encoder outputs which is then synthesized by the decoder. Our system outperforms the current state-of-the-art causal and noncausal speech separation algorithms, reduces the computational cost of speech separation, and significantly reduces the minimum required latency of the output. This makes TasNet suitable for applications where low-power, real-time implementation is desirable such as in hearable and telecommunication devices. ", "text": "robust speech processing multi-talker environments requires effective speech separation. recent deep learning systems made signiﬁcant progress toward solving problem remains challenging particularly real-time short latency applications. methods attempt construct mask source time-frequency representation mixture signal necessarily optimal representation speech separation. addition time-frequency decomposition results inherent problems phase/magnitude decoupling long time window required achieve sufﬁcient frequency resolution. propose time-domain audio separation network overcome limitations. directly model signal time-domain using encoder-decoder framework perform source separation nonnegative encoder outputs. method removes frequency decomposition step reduces separation problem estimation source masks encoder outputs synthesized decoder. system outperforms current state-of-the-art causal noncausal speech separation algorithms reduces computational cost speech separation signiﬁcantly reduces minimum required latency output. makes tasnet suitable applications low-power real-time implementation desirable hearable telecommunication devices. real-world speech communication often takes place crowded multi-talker environments. speech processing system designed operate conditions needs ability separate speech different talkers. task effortless humans proven difﬁcult model machines. recent years deep learning approaches signiﬁcantly advanced state problem compared traditional methods typical neural network speech separation algorithm starts calculating short-time fourier transform create timefrequency representation mixture sound. bins correspond source separated used synthesize source waveforms using inverse stft. several issues arise framework. first unclear whether fourier decomposition optimal transformation signal speech separation. second stft transforms signal complex domain separation algorithm needs deal magnitude phase signal. difﬁculty modifying phase majority proposed methods modify magnitude stft calculating time-frequency mask source synthesize using masked magnitude spectrogram original phase mixture. imposes upper bound separation performance. even though several systems proposed phase information design masks phase-sensitive mask complex ratio mask upper bound still exists since reconstruction exact. moreover effective speech separation stft domain requires high frequency resolution results relatively large time window length typically speech music separation minimum latency system bounded length stft time window limits systems short latency required telecommunication systems hearable devices. natural overcome obstacles directly model signal time-domain. recent years approach successfully applied tasks speech recognition speech synthesis speech enhancement waveformlevel speech separation deep learning investigated yet. paper propose time-domain audio separation network neural network directly models mixture waveform using encoder-decoder framework performs separation output encoder. framework mixture waveform represented nonnegative weighted basis signals weights outputs encoder basis signals ﬁlters decoder. separation done estimating weights correspond source mixture weight. weights nonnegative estimation source weights formulated ﬁnding masks indicate contribution source mixture weight similar masks used stft systems. source waveforms reconstructed using learned decoder. signal factorization technique shares motivation behind independent component analysis nonnegative mixing matrix semi-nonnegative matrix factorization however unlike semi-nmf weights basis signals learned nonnegative autoencoder framework encoder convolutional layer decoder deconvolutional layer scenario mixture weights replace commonly used stft representations. since tasnet operates waveform segments small system implemented real-time latency. addition lower latency tasnet outperforms state-of-art stft-based system. applications require real-time processing noncausal separation module also used improve performance using information entire signal. encoder decoder modules construct nonnegative autoencoder waveform mixture nonnegative weights calculated encoder basis signals ﬁlters decoder. separation performed mixture weight matrix using subnetwork estimates mask source. rn×l rn×l vectors length mixture weight vector. denotes sigmoid activation function denotes convolution operator. k-th segment entire mixture signal length normalized unit norm reduce variability. convolution applied rows step motivated gated approach used language modeling empirically performs signiﬁcantly better using relu sigmoid system. estimation source masks done deep lstm network model time dependencies across segments followed fully-connected layer softmax activation function mask generation. input lstm network sequence mixture weight vectors output network source mask vectors procedure estimation masks mask estimation masks generated several lstm layers followed fully-connected layer softmax function activation. parameters gain bias vectors jointly optimized network. normalization step results scale invariant mixture weight vectors also enables efﬁcient training lstm layers. mixture weight vector weight vector source separating sources representation reformulated estimating weight matrix source given mixture weight subject comparison matrix factorization algorithms basis signals required distinct statistical properties explicit frequency band preferences constraints imposed here. instead basis signals jointly optimized parameters separation network during training. moreover synthesis source signal weights basis signals done directly time-domain unlike inverse stft step needed based solutions. figure shows structure network. contains three parts encoder estimating mixture weight separation module decoder source waveform reconstruction. combination fig. time-domain audio separation network models signal time-domain using encoder-decoder framework perform source separation nonnegative encoder outputs. separation achieved estimating source masks applied mixture weights reconstruct sources. source weights synthesized decoder. rk×n weight matrix source note applied original mixture weight instead normalized weight timedomain synthesis sources done matrix multiplication between basis signals rn×l segment operation also formulated linear deconvolutional operation corresponds ﬁlter jointly learned together parts network. inverse operation convolutional layer section finally scale recovered signals reverse effect normalization discussed section concatenating recoveries across segments reconstruct entire signal source. evaluated system two-speaker speech separation problem using wsj-mix dataset contains hours training hours validation data. mixtures generated randomly selecting utterances different speakers wall street journal training mixing random signal-to-noise ratios five hours evaluation generated using utterances unseen speakers dataset. reduce computational cost waveforms down-sampled khz. since output network waveforms estimated clean signals directly source-to-distortion ratio training target. scale-invariant source-to-noise ratio used evaluation metric place standard training target. si-snr deﬁned parameters system include segment length number basis signals conﬁguration deep lstm separation network. using grid search found optimal samples designed layer deep uni-directional lstm network hidden units layer followed fully-connected layer hidden units generates -dimensional mask vectors. noncausal conﬁguration bi-directional lstm layers number hidden units layer direction. identical skip connection added output second last lstm layers. training batch size initial learning rate causal system noncausal system halve learning rate accuracy validation improved consecutive epochs. apply curriculum training strategy similar fashion start training network second long utterances continue training second long utterances afterward. table shows performance system well three stateof-art deep speech separation systems deep clustering permutation invariant training deep attractor network tasnet-lstm represents causal conﬁguration uni-directional lstm layers. tasnet-blstm corresponds system bi-directional lstm layers noncausal cannot implemented real-time. systems show best performance reported dataset. causal conﬁguration proposed tasnet system signiﬁcantly outperforms state-of-art causal system uses representation input. noncausal conﬁguration system outperforms previous systems including two-stage systems dpcl++ upit-blstm-st second-stage enhancement network. note system contain regularizers recurrent dropout post-clustering steps mask estimation table compares latency different causal systems. latency system ttot expressed parts initial delay system required order receive enough samples produce ﬁrst output. processing time segment estimated average per-segment processing time across entire test set. model pre-loaded titan pascal separation ﬁrst segment started. average processing speed segment system less resulting total system latency comparison stft-based system requires least time interval start processing addition processing time required calculation stft separation inverse stft. enables system preform situation tolerate short latency hearing devices telecommunication applications. noncausal networks. figure shows frequency response basis signals sorted center frequencies observe continuous transition high frequency showing system learned perform spectral decomposition waveform similar ﬁnding also observe frequency bandwidth increases center frequency similar mel-ﬁlterbanks. contrast basis signals tasnet higher resolution lower frequencies compared stft. fact basis signals center frequencies indicate importance low-frequency resolution accurate speech separation. analysis network representation transformation lead better understanding network separates competing speakers paper proposed deep learning speech separation system directly operates sound waveforms. using autoencoder framework represent waveform nonnegative weighted learned basis signals. time-domain separation problem solved estimating source masks applied mixture weights. experiments showed system times faster compared state-of-art stftbased systems achieved signiﬁcantly better speech separation performance. audio samples available po-sen huang minje mark hasegawa-johnson paris smaragdis joint optimization masks deep recurrent neural networks monaural source separation ieee/acm transactions audio speech language processing vol. yusuf isik jonathan roux zhuo chen shinji watanabe john hershey single-channel multi-speaker separation using deep clustering interspeech morten kolbæk dong zheng-hua jesper jensen multitalker speech separation utterance-level permutation invariant training deep recurrent neural networks ieee/acm transactions audio speech language processing vol. zhuo chen nima mesgarani deep attractor network single-microphone speaker separation acoustics speech signal processing ieee international conference ieee zhuo chen nima mesgarani hakan erdogan john hershey shinji watanabe jonathan roux phase-sensitive recognition-boosted speech separation using deep recurrent neural networks acoustics speech signal processing ieee international conference ieee donald williamson yuxuan wang deliang wang complex ratio masking monaural speech separation ieee/acm transactions audio speech language processing vol. zhuo chen john hershey jonathan roux nima mesgarani deep clustering conventional networks music separation stronger together acoustics speech signal processing ieee international conference ieee tara sainath weiss andrew senior kevin wilson oriol vinyals learning speech front-end waveform cldnns sixteenth annual conference international speech communication association aaron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu wavenet generative model audio arxiv preprint arxiv. soroush mehri kundan kumar ishaan gulrajani rithesh kumar shubham jain jose sotelo aaron courville yoshua bengio samplernn unconditional end-to-end neural audio generation model arxiv preprint arxiv. fa-yu wang chong-yung tsung-han chan wang nonnegative least-correlated component analysis separation dependent sources volume maximization ieee transactions pattern analysis machine intelligence vol. ehsan hosseini-asl jacek zurada olfa nasraoui deep learning part-based representation data using ieee sparse autoencoders nonnegativity constraints transactions neural networks learning systems vol. andre lemme ren´e felix reinhart jochen jakob steil online learning generalization parts-based image representations non-negative sparse autoencoders neural networks vol. chorowski jacek zurada learning understandable neural networks nonnegative weight constraints ieee transactions neural networks learning systems vol. paris smaragdis shrikant venkataramani neural network alternative non-negative audio models acoustics speech signal processing ieee international conference ieee kaiming xiangyu zhang shaoqing jian identity mappings deep residual networks european conference computer vision. springer guide conarxiv preprint emmanuel vincent r´emi gribonval c´edric f´evotte performance measurement blind audio source separation ieee transactions audio speech language processing vol. tasha nagamine nima mesgarani understanding representation computation multilayer perceptrons case study speech recognition international conference machine learning", "year": "2017"}