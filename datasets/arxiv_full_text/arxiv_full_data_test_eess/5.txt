{"title": "Automatic Photo Adjustment Using Deep Neural Networks", "tag": "eess", "abstract": " Photo retouching enables photographers to invoke dramatic visual impressions by artistically enhancing their photos through stylistic color and tone adjustments. However, it is also a time-consuming and challenging task that requires advanced skills beyond the abilities of casual photographers. Using an automated algorithm is an appealing alternative to manual work but such an algorithm faces many hurdles. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Further, these adjustments are often spatially varying. Because of these characteristics, existing automatic algorithms are still limited and cover only a subset of these challenges. Recently, deep machine learning has shown unique abilities to address hard problems that resisted machine algorithms for long. This motivated us to explore the use of deep learning in the context of photo editing. In this paper, we explain how to formulate the automatic photo adjustment problem in a way suitable for this approach. We also introduce an image descriptor that accounts for the local semantics of an image. Our experiments demonstrate that our deep learning formulation applied using these descriptors successfully capture sophisticated photographic styles. In particular and unlike previous techniques, it can model local adjustments that depend on the image semantics. We show on several examples that this yields results that are qualitatively and quantitatively better than previous work. ", "text": "zhicheng university illinois urbana champaign zhang† carnegie mellon university baoyuan wang microsoft research sylvain paris adobe research yizhou university hong kong university illinois urbana champaign photo retouching enables photographers invoke dramatic visual impressions artistically enhancing photos stylistic color tone adjustments. however also time-consuming challenging task requires advanced skills beyond abilities casual photographers. using automated algorithm appealing alternative manual work algorithm faces many hurdles. many photographic styles rely subtle adjustments depend image content even semantics. further adjustments often spatially varying. characteristics existing automatic algorithms still limited cover subset challenges. recently deep machine learning shown unique abilities address hard problems resisted machine algorithms long. motivated explore deep learning context photo editing. paper explain formulate automatic photo adjustment problem suitable approach. also introduce image descriptor accounts local semantics image. experiments demonstrate deep learning formulation applied using descriptors successfully capture sophisticated photographic styles. particular unlike previous techniques model local adjustments depend image semantics. show several examples yields results qualitatively quantitatively better previous work. authors’ baoyuanwmicrosoft.com sparisadobe.com yizhouyacm.org. †this work conducted zhang intern microsoft research. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies show notice ﬁrst page initial screen display along full citation. copyrights components work owned others must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists component work works requires prior speciﬁc permission and/or fee. permissions requested publications dept. inc. penn plaza suite york permissionsacm.org. yyyy -/yyyy/-artxxx ./xxxxxxx.yyyyyyy http//doi.acm.org/./xxxxxxx.yyyyyyy prevalence digital imaging devices social networking sharing photos social media become quite popular. common practice type photo sharing artistic enhancement photos various apps instagram. general photo enhancement artistic tries correct photographic defects also aims invoke dramatic visual impressions stylistic even exaggerated color tone adjustments. traditionally high-quality enhancement usually hand-crafted well-trained artist extensive labor. work study problem learning artistic photo enhancement styles image exemplars. speciﬁcally given image pairs representing photo pixel-level tone color enhancement following particular style wish learn computational model novel input photo apply learned model automatically enhance photo following style. learning high-quality artistic photo enhancement style challenging several reasons. first photo adjustment often highly empirical perceptual process relates pixel colors enhanced image information embedded original image complicated manner. learning enhancement style needs extract accurate quantitative relationship underlying process. quantitative relationship likely complex highly nonlinear especially enhancement style requires spatially varying local adjustments. nontrivial learn computational model capable representing complicated relationship accurately large-scale training data likely necessary. therefore seek learning model scalable respect feature dimension data size efﬁciently computable high-dimensional large-scale data. second artistic enhancement typically semantics-aware. artist individual pixels; instead he/she sees semantically meaningful objects determines type adjustments improve appearance objects. example likely artist pays attention improve appearance human ﬁgure region fig. example semantics-aware photo enhancement style extends cross processing effect local manner. left input image; middle enhanced image deep learning based automatic approach; right groundtruth image manually enhanced photographer applied different adjustment parameters different semantic regions. results effects section supplemental materials. photo. would like incorporate semantics-awareness learning problem. challenge representation semantic information learning learned model perform image adjustments according speciﬁc content human artists present automatic photo enhancement method based deep machine learning. approach recently accumulated impressive successes domains computer vision speech analysis semantics data plays major role e.g. motivated explore class techniques context. address challenges mentioned above cast exemplar-based photo adjustment regression problem deep neural network multiple hidden layers represent highly nonlinear spatially varying color mapping input enhanced images. deep neural network universal approximator represent arbitrarily complex continuous functions also compact model readily scalable respect high-dimensional large-scale data. feature design issue signiﬁcantly affect effectiveness dnn. make sure learned color mapping responds complex color semantic information design informative discriminative feature descriptors serve input dnn. input image pixel feature descriptor consists three components reﬂect respectively statistical semantic information pixel contextual global levels. global feature descriptor based global image statistics whereas context feature descriptor based semantic information extracted large neighborhood around pixel. understanding image semantics made possible recent advances scene understanding object detection. existing algorithms annotate input image pixels semantics information annotated images incorporated novel context feature descriptor. introduces ﬁrst automatic photo adjustment framework based deep neural networks. variety normal artistic photo enhancement styles achieved training distinct model enhancement style. quality results superior existing methods. —our framework adopts informative discriminative image feature descriptors pixel contextual global levels. context descriptor exploits semantic analysis multiscale spa—our method also includes effective algorithm choosing representative subset photos large collection photo enhancement model trained chosen subset still produce high-quality results novel testing images. contribution work application deep machine learning context standard learning procedure claim contribution design learning algorithm itself. similarly propose possible design semantic context descriptor demonstrate effectiveness comprehensive exploration design space descriptors beyond scope paper. traditional image enhancement rules primarily determined empirically. many software tools perform fully automatic color correction tone adjustment adobe photoshop google auto awesome microsoft ofﬁce picture manager. addition tools exists much research either interactive automatic color tone adjustment. automatic methods typically operate entire image global manner without taking image content consideration. address issue kaufman introduces automatic method ﬁrst detects semantic content including faces well shadowed salient regions applies sequence empirically determined steps saturation contrast well exposure adjustment. however limit approach output style hard-coded algorithm cannot easily tuned achieve desired style. comparison shall data-driven approach easily trained produce variety styles. further techniques rely ﬁxed pipeline inherently limited ability achieve user-preferred artistic enhancement effects especially exaggerated dramatic ones. practice ﬁxedpipeline technique works well certain class adjustments produces approximate results effects outside class. instance well tonal global transforms fig. architecture dnn. neurons dash line indicate compute cost function note weights connections blue neurons yellow neurons elements quadratic color basis activation function yellow purple neurons identity function. training error backpropagation starts output layer connection weights dash line already ﬁxed. high-frequency pixelwise color variations difﬁcult model force choose mapping function sensitive high-frequency details. mapping function often leads noisy results relatively smooth regions. tackle problem color basis vector pixel rewrite expresses mapped color result applying color transform matrix color basis vector vector function taking different forms works different types color transforms. paper work color space color afﬁne color transforms. quadratic color transforms since per-pixel color basis vector varies similar frequencies pixel colors absorb much high-frequency color variation. factorizing color variation associated focus modeling spatially smooth otherwise highly nonlinear part model local edits kaufman perform well predetermined semantic categories handle elements outside set. comparison deep learning provides universal approximator trained per-style basis success approach. another line research photo adjustment primarily datadriven. learning based image enhancement image restoration shown promising results therefore received much attention. kang found image quality assessment actually much personalized results automatic method learning individual preferences global photo adjustment. bychkovsky introduces method based gaussian processes learning tone mappings according global image statistics. since methods designed global image adjustment consider local image contexts cannot produce spatially varying local enhancements. wang proposes method based piecewise approximation learning color mapping functions exemplars. consider semantic contextual information either. addition fully automatic relies interactive soft segmentation. infeasible technique automatically enhance collection images. comparison paper proposes scalable framework learning user-deﬁned complex enhancement effects exemplars. explicitly performs generic image semantic analysis image enhancement models trained using feature descriptors constructed semantic analysis results. hwang proposes context-aware local image enhancement technique. technique ﬁrst searches similar images similar pixels within them ﬁnally apply combination enhancement parameters similar pixels considered pixel test image. sufﬁciently large image database method works well. practice nearest-neighbor search requires fairly large training challenging create slow search thereby limiting scalability approach. another difference approach that locate similar pixels method uses lowmid-level features whereas also consider high-level semantics. shall result section differences signiﬁcant impact adjustment quality several cases. discuss cast exemplar-based photo adjustment regression problem solve regression problem. photo enhancement style represented exemplar image pairs respectively images enhancement. premise exists intrinsic color mapping function maps pixel’s color corresponding pixel’s color every goal train approximate function using applied images enhance style there. pixel image value simply color image pixel whereas input complex depends color also additional local global information extracted thus formulate parametric function represents parameters represents feature vector encompasses color well additional local global information. formulation training function input image visualization per-pixel quadratic color transforms matrix. image fig. right visualizes coefﬁcient matrix pixel locations. coefﬁcients linearly mapped visualization image better contrast. visualization illustrates properties quadratic color transforms spatially varying smooth much high-frequency content suppressed. fig. multiscale spatial pooling schema. pooling region compute histogram semantic categories. shown three-scale scheme pooling regions. experiments four-scale scheme pooling regions. multi-layer deep neural networks proven able represent arbitrarily complex continuous functions network acyclic graph node neuron. neurons organized number layers including input layer hidden layers output layer. input layer directly maps input feature vector i.e. problem. output layer maps elements color transform neuron within hidden layer output layer takes input responses neurons preceding layer. connection pair neurons associated weight. denote output j-th neuron l-th layer. weight associated connection j-th neuron l-layer k-th neuron layer activation function typically nonlinear. choose rectiﬁed linear unit activation function networks. compared widely used activation functions hyperbolic tangent tanh sigmoid relu advantages including inducing sparsity hidden units accelerating convergence training process. note nonlinear activation function neurons output layer. output neuron output layer linear combination inputs preceding layer. figure shows overall architecture extra layers output layer computing product color transform color basis vector. given neural network architecture color mapping function space spanned neural networks architecture different weight parameters network architecture ﬁxed given training dataset classic error backpropagation algorithm train weights. addition apply dropout training strategy shown useful improving generalization capability. output neuron hidden layers zero probability neurons dropped contribute forward pass participate error backpropagation. experiments show adding dropout training typically reduces relative prediction error testing data actually makes signiﬁcant difference visual quality enhanced results. feature descriptor sample pixel serves input layer neural network. three components ﬁnally superpose detection maps onto parsing obtain ﬁnal semantic label photographic practice global attributes overall impressions average intensity image least partial inﬂuence artists decide enhance image. therefore incorporate global image features feature representation. speciﬁcally adopt types global features proposed including intensity distribution scene brightness equalization curves detail-weighted equalization curves highlight clipping spatial distribution altogether give rise -dimensional vector. contextual features characterize distribution semantic categories building person tree image. features extracted semantic analysis results within local region surrounding sample pixel. typical image semantic analysis algorithms include scene parsing object detection scene parsing tries label every pixel image semantic category. object detection hand trains highly specialized detector every category objects scene parsing good labeling categories characteristic shape relatively consistent texture. categories large scale typically form background image. object detectors better locating categories better characterized overall shape local appearance. categories smaller scale typically occupy foreground image. types techniques complementary other perform semantic analysis using combination scene parsing object detection algorithms. figure illustrates fusion example scene parsing detection results. existing algorithms automatically annotate input image pixels semantics information annotated images gathered novel context feature descriptor. pixel annotation perform scene parsing using state-of-theart algorithm semantic categories scene parsing include object types road river ﬁeld grass. scene parsing step obtain parsing denoted pixel receives category label indicating high probability corresponding pixel input image covered semantic instance category. apply state-of-the-art object detector detect pixels covered predeﬁned foreground object types include person train building. detection step obtain conﬁdence predeﬁned type. fuse conﬁdence maps choosing every pixel object label highest conﬁdence value. fused detection denoted merge pixel labels conﬁdence larger predeﬁned threshold used overwrite corresponding labels since scene parsing object detection results tend noisy rely voting automatic image segmentation perform label cleanup merged label map. within image segment reset label every pixel appears frequently segment. experiments adopt image segmentation algorithm cleaned becomes ﬁnal semantic label label. given ﬁnal semantic label entire input image construct contextual feature descriptor sample pixel represent multiscale object distributions surroundings. sample point ﬁrst deﬁne series nested square regions centered edge length regions follows geometric series i.e. making feature representation sensitive semantic contents nearby locations farther away. subdivide ring every consecutive squares eight rectangles shown figure thus total regions including original regions series well regions generated subdivision. regions compute semantic label histogram number bins equal total number semantic categories histograms nine smaller regions within spatial pooling make feature representation robust better tolerate local geometric deformations. ﬁnal contextual feature descriptor deﬁned concatenation semantic label histograms. multiscale context descriptor partially inspired shape contexts however unlike shape context descriptor regions subregions either rectangles squares facilitate fast histogram computation based integral images practice pre-compute integral images semantic category. value histogram calculated within constant time extremely fast compared computation shape contexts. best knowledge method ﬁrst explicitly constructs semantically meaningful contextual descriptors learning complex image enhancement models. important verify whether complexity contextual features necessary learning complex spatially varying local adjustment effects. compared results feature descriptor closest codeword codebook vector quantization image viewed codewords quantizing feature descriptors image. build histogram every image using codewords codebook histogram bins. value histogram equal number times corresponding codeword appears image. histogram image subset images initial image collection compute accumulated histogram simply performing elementwise summation individual histograms images evaluate representative power using cross denotes j-th element large cross entropy implies codewords corresponding histogram bins evenly distributed images vice versa. thus encourage even coverage feature space selected images essentially need solution following expensive combinatorial optimization practice seek approximate solution progressively adding image every time desired number images subset. every time added image maximizes cross entropy expanded subset. process illustrated algorithm proposed method well suited learning complex highly nonlinear photo enhancement styles especially style requires challenging spatially varying local enhancements. successful local enhancement rely content speciﬁc local region also contents surrounding areas. sense operations could easily result complex effects require stylistic even exaggerated color transforms making previous global methods local empirical methods inapplicable. contrast method designed address challenges help powerful contextual features strong regression capability deep neural networks. fully evaluate method hired professional photographer carefully retouched three different stylistic local effects using hundreds photos. section reports experiments conducted evaluate performance method. although technique designed learn complex local effects readily applied global image adjustments without difﬁculty. experiments section supplemental materials show technique achieves superior performance visually nuobtained without contextual features well obtained simpler contextual features based pooling region size largest region. figure contextual features able produce local adjustment results closest ground truth. discussion. addition semantic component feature vectors major difference previous work. shown figure result section design propose component effective produces signiﬁcant improvement practice. said acknowledge options possible believe exploring design space semantic descriptors exciting avenue future work. training mapping function using images prefer make pixels dense sampling would result unbalanced training data. example could many pixels large regions relatively smaller person regions could eventually result serious bias trained mapping function. addition overly dense sampling unnecessarily increases training cost need handle millions pixel samples. therefore apply superpixel based method collect training samples. training image ﬁrst apply graph-based segmentation divide image small homogeneous irregularly shaped patches called superpixel. note superpixel smooth region larger region high-frequency details. require color transform returned mapping function centroid superpixel used predicting sufﬁcient accuracy adjusted color pixels within superpixel. avoid bias randomly sample ﬁxed number pixels every superpixel. superpixel original images pixels sampled revise cost function follows reﬂect superpixel-based sampling local smoothness requirement. represents trained weights neural network feature vector constructed pixel closest centroid denotes color basis vector sample pixel within denotes adjusted color sample within example-based photo enhancement example images demonstrate certain enhancement style often need manually prepared human artists. labor intensive task adjust many images image multiple attributes regions adjusted. therefore much desired pre-select small number representative training images reduce amount human work required. hand make learned model achieve strong prediction capability necessary selected training images reasonable coverage feature space. section introduce cross-entropy based scheme selecting subset representative training images large collection. ﬁrst learn codebook feature descriptors codewords running k-means clustering feature descriptors collected training images. every original merically compared state-of-the-art methods mit-adobe fivek dataset. objectively evaluate effectiveness method conducted user studies obtained positive results. experimental setup neural network setup. throughout experiments paper ﬁxed input layer hidden layers output layer number neurons hidden layers empirically number neurons output layer equal number coefﬁcients predicted color transform. experiments conﬁrmed quadratic color transforms faithfully reproduce colors adjusted images afﬁne color transforms. therefore neurons output layer three color channels. data sampling. since learn pixel-level color mappings every pixel within image potential training sample. practice segment image around superpixels randomly select pixels. therefore example even example image pairs learning speciﬁc local effect number training samples large million. large-scale training largely eliminate risk overﬁtting. typically takes hours ﬁnish training neural network medium size training dataset hundreds images. nevertheless trained neural network needs second enhance -pixel wide test image. image enhancement learned color mappings. learned parameters neural network image enhancement stage apply feature extraction pipeline input image training stage. ﬁrst perform scene parsing object detection apply graph-based segmentation obtain superpixels. likewise also extract feature vector centroid every superpixel apply color transform returned neural network every pixel within superpixel. speciﬁcally adjusted color pixel computed superpixel covers learning local adjustments three stylistic local effects manually downloaded images flickr resized larger dimension pixels. images chosen training remaining images testing. professional photographer used photoshop retouch images produce datasets three different stylistic local effects. could perform wide range operations adjust images including selecting local objects/areas region selection tool creating layers layer masks blending different layers using various modes name few. reduce subjective variation retouching used actions tool records sequence operations repeatedly applied selected image regions. ﬁrst local effect foreground pop-out created increasing contrast color saturation foreground salient objects/regions decreasing color saturation background. performing operations foreground salient regions need interactively segmented using region selection tools photoshop. segmented regions used dataset production used enhancement pipeline. local effect makes foreground objects visually vivid making background less distractive. figure show three examples automatically enhanced results groundtruth results photographer. refer supplemental materials training data well enhanced testing photos. second effect local xpro created generalizing popular cross processing effect local manner. within photoshop photographer ﬁrst predeﬁned multiple proﬁles speciﬁcally tailored semantic categories used scene parsing object detection section proﬁles share common series operations adjustment individual color channels color blending across color channels hue/saturation adjustment well brightness/contrast manipulation name few. nonetheless proﬁle deﬁnes distinct adjustment parameters tailored corresponding category. retouching photo photographer used region selection tools isolate image regions applied suitable proﬁle image region according speciﬁc semantic content within region. avoid artifacts along region boundaries could also slightly adjust color/tone local regions application proﬁles. although proﬁles roughly follow cross processing style choice local proﬁles additional minor image editing heavily inﬂuenced photographer’s personal taste naturally learned exemplars. figure show three examples effect compare enhanced results groundtruth results. figure shows another example effect. increase diversity complexity asked photographer create third local effect watercolor tries mimic certain aspects watercolor painting style. example watercolors tend brighter lower saturation. within single brush region color variation also tends limited. photographer ﬁrst applied similar operations foreground pop-out effect input images including increasing contrast saturation foreground regions well decreasing background regions. addition brightness foreground background regions increased different amounts. created layers brush effects brightened image using larger brushes layer smaller other. ﬁrst layer brush size foreground background also different. finally layers composited together using ’lighten’ mode photoshop. overall effect results highly complex spatially varying color transforms force neural network heavily rely local contextual features regression. figure show enhanced results three testing examples corresponding groundtruth results. simulate brush strokes applying color transform pixels superpixel calculate average color within superpixel superpixel another example watercolor effect well visualized superpixels automatic results look visually similar ones produced photographer. refer supplemental materials examples enhanced effect. note intention rigorously simulating watercolors experimentally validating technique able accurately learn complex local adjustments. successfully learn enhancement effect important make adjustments individual training images consistent. practice found following strategies helpful increasing consistency across image set. first artistic adjustment image involves personal taste photographer result could quite different different photographers. fig. examples three stylistic local enhancement effects. input images. enhanced results groundtruth foreground pop-out effect. enhanced results groundtruth local xpro effect. enhanced results groundtruth watercolor effect. fig. example watercolor local effect. input image. visualization superpixels used simulating brush strokes. superpixel ﬁlled random color. enhanced result. ground truth. therefore always deﬁne retouching style using photos adjusted photographer. means even input content retouched results different photographers always deﬁned different styles. second inform photographer semantic object categories scene parsing object detection algorithms aware consequently apply similar adjustments visual objects semantic category. third actions tool photoshop faithfully record proﬁles applied different semantic categories. improves consistency color transforms applied image regions similar content context. important point underlying color mappings local effect datasets truly global. spatially vary within image domain. verify this collect pixels semantic region image. drawing scatter plots different semantic regions using pixel color pairs input retouched images able visualize spatially varying color transforms. example figure clearly shows color transforms differ building grass road regions. also method successfully learn spatially varying complex color transforms. conducted comparison adopts local piecewise approximation approach. however lack discriminative contextual features learned adjustment parameters tend similar across different regions verify generalization capability based photo adjustment models trained using image pairs. mentioned earlier actual number training samples exceeds number training image pairs thousands superpixels within training image pair. shown fig. apply trained models novel testing images signiﬁcant visual differences images training set. visual objects images either unique appearances unique spatial conﬁgurations. illustrate this show similar training fig. comparison local xpro effect. left input image; right enhanced image bottom left enhanced image approach; bottom right enhanced image photographer. enhanced image approach closer ground truth generated photographer. images share largest number object region categories testing image also content layout similar possible. mountain input image appearance spatial layout different training images. bottom appearances spatial conﬁguration people also quite different training images. despite differences trained models still able adjust input images plausible way. demonstrate importance contextual features learning local adjustments subsection. first calculate distance cielab color space input images ground truth produced photographer local effect datasets shown second column table numerically reﬂect magnitude adjustments photographer made input images. second numerically compare testing errors enhanced results without contextual feature third fourth columns table experiments show without contextual features testing errors enhanced results tend relatively high. mean error cielab color space reaches respectively foreground pop-out local xpro watercolor effects. hand including proposed contextual feature errors drop signiﬁcantly indicating necessity features. fig. scatter plots color mappings. middle input image semantic label groundtruth local xpro effect. left right color mapping scatter plots four semantic regions. semantic region three scatter plots corresponding color channels. scatter plot visualizes sets points take original value channel horizontal coordinate respectively predicted groundtruth adjusted values channel vertical coordinate. fig. effectiveness contextual features. left input; middle left enhanced without context; middle enhanced simple contextual features middle right enhanced contextual features; right ground truth. obvious among enhanced results enhanced using contextual features closest ground truth. simpler intuitive contextual feature descriptor pooling region size largest region found simple contextual features helpful reducing errors effective ours. taking local watercolor painting effect example observed corresponding mean error drops still obviously higher multiscale features because multiscale pooling regions features achieve certain degree translation rotation invariance crucial histogram based representation. also performed visual comparisons. fig. shows example. without contextual feature local regions enhanced photo might exhibit severe color deviation ground truth. shown figure color transforms helps absorb highfrequency color variations enables regress spatially smooth otherwise highly nonlinear part color mapping. highlight beneﬁts using color transforms train different regress retouched colors directly. architecture similar described section except neurons output layer represent enhanced cielab color. compare testing errors foregronud pop-out local xpro datasets table datasets testing error increases indicates color transforms beneﬁcial task. complexity based model primarily determined number hidden layers number neurons layer. note complexity architecture meet inherent complexity learning task. sufﬁcient complexity handle given task trained model would even able accurately learn samples training set. hand complexity exceeds inherent complexity given task exists risk overﬁtting trained model would able generalize well novel testing data even though could make training error small. nature learning task paper regression problem. shown feedforward neural network single hidden layer used universal regressor necessary number neurons hidden layer varies inherent complexity given regression problem. practice however easier achieve small training error deeper network relatively small number neurons hidden layers. assess impact design choices large number training samples used training. lasso random forest directly regress target cielab colors using feature training including pixelwise features global features contextual features. hyperparameters lasso random forest tuned using cross validation. make fair comparison also adapted directly regress target cielab colors. comparison errors summarized table iii. signiﬁcantly outperforms lasso foreground pop-out local xpro datasets obtains slightly lower errors watercolor dataset. compared random forest obtains lower testing errors foreground pop-out watercolor datasets. local xpro dataset random forest obtains lower numerical errors dnn. however visual inspection found colors generated random forest spatially smooth blocky artifacts prevalent enhanced images shown figure regression results random forest based values retrieved various leaf nodes spatial smoothness retrieved values cannot guaranteed. contrast trained generates spatially smooth colors give rise visual artifacts. mit-adobe fivek dataset contains images retouched well trained photographers results groups global adjustment styles. learn pixel-level color mappings would million training samples total half images used training. fig. examples novel image enhancement. example watercolor effect. bottom example local xpro effect. example input image enhanced result ground truth training images similar input image. note input images examples signiﬁcant visual differences images training set. architecture evaluate dnns varying number hidden layers neurons. keep held-out images validation vary number training images step size evaluate impact size training set. repeat experiments times random training testing partitions report averaged results. foreground pop-out dataset used study. summarizes experimental results. overall neural networks single hidden layer deliver inferior performance deeper networks. dnns hidden layers perform well hidden layers. hidden layers number training images exceeds testing error signiﬁcantly improve more. summary dnns hidden layers achieve testing errors execute faster hidden layers training testing stages. therefore ﬁnally hidden layers neurons throughout paper. proves effective regressing spatially varying complex color transforms three local effect datasets. also great interest evaluate performance regressors datasets. speciﬁcally chose compare popular regression methods lasso random forest lasso random forest scalable compared method using experimental settings testing datasets work. testing datasets used random randomly selected testing images group mit-adobe fivek dataset high variance images selected testing group mit-adobe fivek dataset comparison results numerical errors shown second third columns table method capable achieving much better prediction performance terms mean errors predeﬁned datasets. figure shows error histograms method testing datasets. errors produced method mostly concentrated lower histograms. figure shows visual comparison enhanced result closer ground truth produced photographer. performance differences could explained follows. technique based nearest-neighbor search requires fairly large training slow search. result technique divides similarity based search levels. ﬁrst searches similar images similar pixels within them. two-level strategy accelerates search large percentage similar pixels even chance utilized search image level leaves dissimilar images still contain many similar pixels. hand deep neural network based method powerful nonlinear regression technique considers training data simultaneously. thus method stronger extrapolation capability nearest-neighbor based approach exploits limited number nearest neighbors. reason nearest-neighbor based approach also sensitive noisy inconsistent adjustments training data. another comparison follow setting used work experimented training images group reported mean error channel only. shown ﬁrst column table obtained slightly smaller mean error channel remaining testing images. fig. comparison training image selection schemes. compared sensor placement based mutual information cross-entropy based method achieves better performance especially number selected images small. band shaded light blue shows standard deviations errors scheme. method compared naive random selection sensor placement method used interestingly random selection scheme used neural network based solution achieves signiﬁcantly better fig. visual comparison left input image; middle left groundtruth enhanced image expert middle right enhanced image approach; right enhanced image ance randomly chose including indoor images outdoor images used user study. testing images also collected groundtruth images enhanced images produced method invited participants including females males ages ranging participants little experience using professional photo adjustment tools experience photo enhancement apps instagram. experiment carried asking participant open static website using prepared computer -inch monitor resolution. test image ﬁrst show input groundtruth image pair participants know input image enhanced photographer show enhanced images automatically generated method hwang random left/right layout without disclosing enhanced method. participant asked compare ground truth vote following three choices left image enhanced better right image enhanced better hard choose. collected votes distributed among three choices. figure shows comparison voting results enhanced images produced method received votes indoor outdoor categories. comparison indicates that visual perspective method produce much better enhanced images second user study tries verify whether method capability enhance target effect statistically signiﬁcant manner. conduct study chose test images local effect datasets described section test data. asked participants ﬁrst study join second study. interface designed follows. screen show ground truth enhanced image produced photographer hired show pair images left original image right enhanced image produced method. asked participant assign score input enhanced images considering criteria time closely image conforms impression given ground truth visual quality image. words enhanced image looks visually pleasing closer ground truth receive higher score. convenience participants simply discretized range scores levels. image looks extremely close ground truth scored collected sets scores original enhanced images respectively. conducted paired t-test sets scores found two-tail p-value indicating fig. comparison instagram. left input images middle results; right results instagram. shows earlybird effect bottom shows nashville effect. comparison indicates enhancement results trained color mappings close ground truth generated instagram. accuracy gaussian process based method. primarily strong nonlinear regression power exhibited deep neural networks rich contextual feature representation built semantic analysis. compared sensor placement cross-entropy based method also achieves better performance especially number selected images small indicates method superior learning enhancement styles small number training images. instagram become popular apps mobile phones. instagram hundreds ﬁlters applied achieve different artistic color tone effects. example frequently used lo-fi ﬁlter boosts contrast brings warm tones; rise ﬁlter adds golden glow hudson casts cool light. speciﬁc effect randomly chose images mitadobe fivek instagram enhance them. among resulting pairs images half used training half testing. veriﬁed whether images adjusted trained color mapping functions similar ground truth produced instagram ﬂavor reverse engineering task. experiments indicate instagram effects relatively easy learn using method. figure shows learning results popular effects. paper demonstrated effectiveness deep learning automatic photo adjustment. cast problem learning highly nonlinear mapping function taking bundled features input layer deep neural network. bundled features include pixelwise descriptor global descriptor well novel contextual descriptor built scene parsing object detection. conducted extensive experiments number effects including conventional artistic ones. experiments show proposed approach able effectively learn computational models automatic spatially-varying photo adjustment. limitations. approach relies scene parsing object detection build contextual features. however general still challenging problems computer vision pattern recognition. mislabeling semantic propagate contextual features adversely affect photo adjustment. shows example foreground pop-out effect. ‘sea’ right side mistakenly labeled ‘mountain’ saturation contrast incorrectly increased. scene parsing object detection rapidly developing areas accurate techniques emerging could adopted system produce reliable semantic label maps. another failure case shown adjustments group mit-adobe fivek dataset learnt. method produces insufﬁcient brightness adjustment leads dimmer result ground truth. fact distance input image ground truth signiﬁcantly higher mean distance dataset. trained using available training samples individual adjustments signiﬁcantly deviating average adjustment semantic object type likely treated outliers cannot correctly learnt. system employs deep fully connected neural network regress spatially varying color transforms. exist many design choices architecture including number hidden layers number neurons layer type neural fig. failure cases. failure case foreground pop-out effect. semantic label area incorrect semantic labeling highlighted. correspondingly area receives incorrect adjustments result. bottom another failure case high variance test mit-adobe fivek dataset. activation functions. together give rise time-consuming trial-and-error process search suitable architecture given task. addition behaves black completely clear network combines features different scales predicts ﬁnal color transforms. fact interpreting internal representations deep neural networks still ongoing research topic acknowledgments grateful vladimir bychkovsky sung hwang fruitful discussions suggestions. work partially supported hong kong research grants council general research funds breiman random forests. machine learning bychkovsky paris chan durand learning photographic global tonal adjustment database input/output image pairs. proceedings ieee conference computer vision pattern recognition. cvpr felzenszwalb huttenlocher efﬁcient graphbased image segmentation. int. comput. vision hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors. corr abs/.. krizhevsky sutskever hinton imagenet advances classiﬁcation deep convolutional neural networks. neural information processing systems bartlett pereira burges bottou weinberger eds. vincent larochelle bengio manzagol p.-a. extracting composing robust features denoising autoencoders. international conference machine learning.", "year": "2014"}