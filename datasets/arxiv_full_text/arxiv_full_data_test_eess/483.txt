{"title": "Articulatory information and Multiview Features for Large Vocabulary  Continuous Speech Recognition", "tag": "eess", "abstract": " This paper explores the use of multi-view features and their discriminative transforms in a convolutional deep neural network (CNN) architecture for a continuous large vocabulary speech recognition task. Mel-filterbank energies and perceptually motivated forced damped oscillator coefficient (DOC) features are used after feature-space maximum-likelihood linear regression (fMLLR) transforms, which are combined and fed as a multi-view feature to a single CNN acoustic model. Use of multi-view feature representation demonstrated significant reduction in word error rates (WERs) compared to the use of individual features by themselves. In addition, when articulatory information was used as an additional input to a fused deep neural network (DNN) and CNN acoustic model, it was found to demonstrate further reduction in WER for the Switchboard subset and the CallHome subset (containing partly non-native accented speech) of the NIST 2000 conversational telephone speech test set, reducing the error rate by 12% relative to the baseline in both cases. This work shows that multi-view features in association with articulatory information can improve speech recognition robustness to spontaneous and non-native speech. ", "text": "acoustic conditions. advanced variants dnns convolutional neural nets recurrent neural nets long short-term memory nets time-delay neural nets vgg-nets significantly improved recognition performance bringing closer human performance abundance data sophistication deep learning algorithms advancement speech recognition performance. role acoustic features explored comparable detail potential contribution performance gains unknown. paper focuses acoustic features investigates selection improves recognition performance datasets switchboard fisher evaluated nist test explored different ways using feature space maximum-likelihood regression transform tried learning fmllr transforms directly using filterbank features learning fmllr transform cepstral version features performing inverse discrete cosine transform fmllr features generate fmllr version filterbank features. experiments demonstrated feature improve performance combinations helped individual features isolation traditionally used mel-filterbank features. articulatory features found useful improving recognition performance switchboard callhome subsets nist test set. findings indicate better acoustic improve speech recognition performance using standard acoustic modeling techniques demonstrate performance good obtained sophisticated acoustic models exploit temporal memory. sake simplicity used acoustic model experiment baseline system’s performance directly state-of-the-art performance reported expect results using carry neural network architectures well. paper explores multi-view features discriminative transforms convolutional deep neural network architecture continuous large vocabulary speech recognition task. mel-filterbank energies perceptually motivated forced damped oscillator coefficient features used feature-space maximum-likelihood transforms combined multi-view feature single acoustic model. multi-view feature representation demonstrated significant reduction word error rates compared individual features themselves. addition articulatory information used additional input fused deep neural network acoustic model found demonstrate reduction switchboard subset callhome subset nist conversational telephone speech test reducing error rate relative baseline cases. work shows association articulatory information improve speech recognition robustness spontaneous non-native speech. index terms— multi-view features feature combination large vocabulary continuous speech recognition robust speech recognition articulatory features spontaneous speech typically contains significant amount variation makes difficult model automatic speech recognition systems. variability stems varying speakers pronunciation variations speaker stylistic differences varying recording conditions many factors. recognizing words conversational telephone speech quite difficult spontaneous nature speech informality speaker variations hesitations disfluencies etc. switchboard fisher data collections large collection datasets used extensively researchers working conversational speech recognition recent trends speech recognition demonstrated impressive performance switchboard fisher data. deep neural network based acoustic modeling become state-of-the-art automatic speech recognition systems demonstrated impressive performance gains almost tried languages outline paper follows. section present dataset recognition task. section describe acoustic features articulatory features used experiments. section presents acoustic language models used experiments followed experimental results section conclusion future directions section acoustic models experiments trained using switchboard fisher corpora. first investigated contributions features models trained dataset training data consisted hours speech data. evaluated contributions features using acoustic models trained combination models evaluated using nist test consists hours audio hours callhome audio. language model training data included words switchboard callhome switchboard cellular transcripts words fisher transcripts words broadcast news transcripts language model training data words conversational text retrieved searching conversational n-grams extracted transcripts -gram language model generated based word probability estimates superarv language model class-based language model classes derived constraint dependency grammar parses first pass decoding -gram pruned improve efficiency full -gram used rescore lattices generated first pass. used mel-filterbank energies baseline feature features generated using implementation distributed kaldi toolkit second feature damped oscillator coefficients features model auditory hair cells using bank forced damped oscillators gammatone filtered band-limited subband speech signals used forcing function. oscillation energy damped oscillators used features power-law compression. performed fmllr transform acoustic features trained gaussian mixture models generate alignments training dataset learn fmllr transform feature sets. investigated approaches directly learned fmllr transforms -dimensional filterbank features investigated learning fmllr transform using cepstral version features. cepstral version features helps decorrelate features turn adheres diagonal covariance assumption gmms. fmllr transform learned using dimensional cepstral features fmllr transform performed idct features obtained generate fmllr version filterbank features. articulatory features estimated using system described performs speech-to-articulatory speechinversion. speech-inversion acoustic features extracted speech signal case modulation features used articulatory trajectories. articulatory features contain time domain articulatory trajectories eight dimensions reflecting glottal aperture velic opening aperture protrusion tongue location degree tongue body location degree. details regarding articulatory features extraction provided trained acoustic models speech recognition tasks. generate alignments necessary training system gaussian mixture model hidden markov model based acoustic model first trained flat-start used produce senone gmm-hmm system produced context-dependent states training set. fully connected model trained using features turn used generate senone alignments train baseline acoustic models presented work. input features acoustic models formed using context window frames acoustic models trained using cross-entropy followed sequence training using maximum mutual information criterion model convolutional filters size used convolutional layer pooling size without overlap. subsequent fully connected network five hidden layers nodes hidden layer output layer included many nodes number states given dataset. networks trained using initial four iterations constant learning rate followed learning-rate halving based cross-validation error decrease. training stopped significant reduction crossvalidation error noted cross-validation error started increase. backpropagation performed using stochastic gradient descent mini-batch training examples. work investigated modified deep neural network architecture jointly model acoustic articulatory spaces shown figure modified architecture parallel input layers used accept acoustic features articulatory features. input layer tied acoustic feature consists convolutional layer filters input layer tied articulatory features feed-forward layer neurons. feature maps convolutional layer outputs feed-forward layer fully connected transforms. table demonstrates fmllr transformed features always performed better features without fmllr transform. also models always gave better results confirming similar observations studies reported earlier also note table shows features performed slightly better features fmllr transform performance improvement pronounced subset nist test set. next step investigated efficacy feature combination focused acoustic models. appended articulatory features extracted training nist test sets combined mfb+fmllr doc+fmllr features mfb+fmllr doc+fmllr features added them. table presents wers obtained evaluating trained different combinations features. note models using used fused cnn-dnn architecture shown figure jointly modeling dissimilar acoustic articulatory spaces. combining mfb+fmllr doc+fmllr features trained model instead. number convolutional filters experiments kept patch size increased eight twelve case combined acoustic features opposed able shows articulatory features helped lower cases. feature always found perform slightly better mfbs best results obtained features combined together indicating benefit using multiview features. note additional neurons used accommodate features hence models comparable sizes. benefit articulatory features stemmed complementary information contain demonstrated earlier studies overall f-cnn-dnn system trained mfb+fmllr doc+fmllr demonstrated relative reduction compared mfb+fmllr baseline subsets nist figure fused cnn-dnn acoustic model. convolution input layer accepts acoustic features input feedforward input layer accepts articulatory features variables) input. initially validated performance features using hours training dataset. baseline models five hidden layers respectively neurons layer trained features fmllr transformed version nist dataset used cross-validation acoustic model training step. table presents word error rates baseline model trained data evaluated nist test cross-entropy training sequence training using mmi. table also shows results obtained features without fmllr transform. present results found always better results training. explored learning fmllr transform directly filterbank features learning fmllr transforms full dimensional cepstral versions features applying transform performing idct able shows performance fmllr transforms learned cepstral version features better ones directly filterbank features expected cepstral features uncorrelated adheres diagonal covariance assumption models used learn table shows system fusion results dumping -best lists rescored lattices individual system different front-end features fmllr i.e. mfb+doc mfb+doc+tv conducting m-way combination subsystems using n-best rover implemented srilm system fusion experiment subsystems equal weights n-best rover. seen table n-best rover based -way -way system fusion produced relative reduction compared best single system evaluation sets respectively. note first table last table i.e. best single system. last fusion combining individual systems presented table reported results exploring multiple features english data. observed fmllr transform helped reduce baseline system significantly. observed using multiple acoustic features helped improving overall accuracy system. robust features articulatory features significantly reduced challenging callhome subset nist evaluation accented speech subset. developed fusedcnn-dnn architecture input convolution performed acoustic features articulatory features process feed-forward layer. found architecture effective combining acoustic features articulatory features articulatory features capture complementary information addition resulted best single system performance relative reduction evaluation sets respectively compared mfb+fmllr baseline. note study language model optimized. future studies investigate neural network-based language modeling techniques known perform better word n-gram lms. also advanced acoustic modeling timedelayed neural nets long short-term memory neural nets nets also explored performance mostly reported using features multi-view features help improve performance. next step focused training acoustic models using -hour swb+fsh data focusing acoustic models multi-view features. note baseline model used generate alignments part hours training consequence number senone labels remained -hour models. table presents results hours trained models. model configurations parameter size kept -hour models. figure shows additional training data resulted significant performance improvement subsets nist test set. adding dataset resulted relative reduction respectively subsets nist test using mfb+fmllr features. similar improvement observed doc+fmllr features well relative reduction subsets observed data added training data. note subset nist test challenging subset contains non-native speakers english hence introducing accented speech evaluation set. articulatory features helped reduce error rates test sets indicating robustness model spontaneous speech native non-native speaking styles. corpus contains speech quite diverse speakers helping reduce subset significantly subset trend reflected results reported literature able demonstrates benefit using multi-view features trained mfb+fmllr doc+fmllr resulted reducing relatively evaluation sets respectively compared best single feature system doc+fmllr. articulatory features form used addition mfb+fmllr doc+fmllr features f-cnn-dnn model best performance single acoustic model obtained produced relative reduction evaluation sets respectively compared acoustic model trained mfb+fmllr doc+fmllr features. mitra franco graciarena damped oscillator cepstral coefficients robust speech recognition proc. interspeech mitra sivaraman espy-wilson saltzman tiede hybrid convolutional neural networks articulatory acoustic information based speech recognition speech communication vol. mitra sivaraman bartels wang espy-wilson vergyri franco joint modeling articulatory acoustic spaces continuous speech recognition tasks proc. icassp march veselý ghoshal burget povey sequence-discriminative training deep neural networks interspeech mitra espy-wilson saltzman goldstein articulatory information noise robust speech recognition ieee trans. audio speech language processing vol. iss. mitra sivaraman espy-wilson saltzman articulatory features deep neural networks role speech recognition proc. icassp florence mitra wang stolcke richey yuan liberman articulatory features large vocabulary speech recognition proc. icassp vancouver bulyko ostendorf stolcke. \"getting mileage text sources conversational speech language modeling using class-dependent mixtures\" proceedings hlt. wang stolcke harper linguistically motivated language model conversational speech recognition proc. icassp stolcke bratt butzberger franco gadde plauche richey shriberg sonmez weng zheng march hub- conversational speech transcription system proc. nist speech transcription workshop college park stolcke srilm extensible language modeling toolkit proc. icslp waibel hanazawa hinton shikano lang phoneme time-delay neural networks ieee transactions acoustics speech signal processing vol. mar. cieri miller walker from switchboard fisher telephone collection protocols uses yields proc. eurospeech evermann h.y. chan m.j.f. gales mrva p.c. woodland training lvcsr systems thousands hours data proc. icassp matsoukas j.-l. gauvain adda colthurst c.-l. kimball lamel lefevre makhoul advances transcription broadcast news conversational telephone speech within combined ears bbn/limsi system ieee transactions audio speech language processing vol. stolcke chen franco gadde graciarena m.-y. hwang kirchhoff mandal morgan recent innovations speech-to-text transcription sri-icsi-uw ieee transactions audio speech language processing vol. ljolje at&t lvcsr system nist lvcsr workshop j.-l. gauvain lamel schwenk adda chen lefevre conversational telephone speech recognition proc. ieee icassp vol. ieee seide conversational speech transcription using context-dependent deep neural networks proc. interspeech saon sercu rennie english conversational telephone speech recognition system proc. interspeech stolcke droppo comparing human machine errors conversational speech transcription proc. interspeech mohamed g.e. dahl hinton acoustic modeling using deep belief networks ieee trans. aslp vol. hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath kinsgbury deep neural networks acoustic modeling speech recognition ieee signal process. mag. vol. qian very deep convolutional neural networks lvcsr proc. interspeech senior beaufays fast accurate recurrent neural network acoustic models speech recognition proc. interspeech senior beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling proc. interspeech peddinti chen manohar povey khudanpur aspire system robust lvcsr tdnns i-vector adaptation rnn-lms proc. asru godfrey holliman switchboard- release linguistic data consortium philadelphia povey ghoshal boulianne burget glembek goel hannemann motlıcek qian schwarz kaldi speech recognition toolkit proc. asru", "year": "2018"}