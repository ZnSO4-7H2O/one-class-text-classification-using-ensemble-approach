{"title": "Gated Recurrent Networks for Seizure Detection", "tag": "eess", "abstract": " Recurrent Neural Networks (RNNs) with sophisticated units that implement a gating mechanism have emerged as powerful technique for modeling sequential signals such as speech or electroencephalography (EEG). The latter is the focus on this paper. A significant big data resource, known as the TUH EEG Corpus (TUEEG), has recently become available for EEG research, creating a unique opportunity to evaluate these recurrent units on the task of seizure detection. In this study, we compare two types of recurrent units: long short-term memory units (LSTM) and gated recurrent units (GRU). These are evaluated using a state of the art hybrid architecture that integrates Convolutional Neural Networks (CNNs) with RNNs. We also investigate a variety of initialization methods and show that initialization is crucial since poorly initialized networks cannot be trained. Furthermore, we explore regularization of these convolutional gated recurrent networks to address the problem of overfitting. Our experiments revealed that convolutional LSTM networks can achieve significantly better performance than convolutional GRU networks. The convolutional LSTM architecture with proper initialization and regularization delivers 30% sensitivity at 6 false alarms per 24 hours. ", "text": "abstract‚Äî recurrent neural networks sophisticated units implement gating mechanism emerged powerful technique modeling sequential signals speech electroencephalography latter focus paper. significant data resource known corpus recently become available research creating unique opportunity evaluate recurrent units task seizure detection. study compare types recurrent units long short-term memory units gated recurrent units evaluated using state hybrid architecture integrates convolutional neural networks rnns. also investigate variety initialization methods show initialization crucial since poorly initialized networks cannot trained. furthermore convolutional gated recurrent networks address problem overfitting. experiments revealed convolutional lstm networks achieve significantly better performance convolutional networks. convolutional lstm architecture proper initialization regularization delivers sensitivity false alarms hours. diagnosis clinical conditions epilepsy dependent electroencephalography recording brain‚Äôs electrical activity electrodes placed scalp. delivering conclusive diagnosis brain-related illness without often unfeasible large amounts time required specialized neurologists interpret records created workflow bottleneck neurologists overwhelmed amount data needs manually reviewed great need partial complete automation analysis process automated technology slowly emerging fill void automatic analysis scans reduces time diagnosis reduces error enhances neurologist‚Äôs ability administer medications. ability search records symbolically greatly accelerates review process. paper focus specifically problem seizure detection. many algorithms applied problem including time‚Äìfrequency analysis methods nonlinear statistical models modern machine learning approaches neural networks support vector machines despite much progress current analysis methodologies perfect significant data resource known corpus become available interpretation creating unique opportunity advance technology. using subset data manually annotated seizure events novel deep structure recently introduced achieves false alarm rate signals system integrates convolutional neural networks recurrent neural networks deliver state performance. paper goal investigate rnns using high-performance architecture described improved initialization methods regularization approaches. recurrent neural network extension conventional feedforward neural network handle variable-length input. handles variable-length sequence recurrent hidden state whose activation time dependent previous time. standard rnns hard train well-known vanishing exploding gradient problems address vanishing gradient problem gated recurrent network architectures long short-term memory unit gated recurrent unit proposed input gate forget gate cell state output gate block output time instance respectively; input time ‚àóare weight matrices applied input recurrent hidden units respectively; ùëîare sigmoid tangent activation functions respectively; peep-hole connections biases respectively; means element-wise product. architecture similar lstm without separate memory cell. unlike lstm include output activation functions peep-hole connections. also integrates input forget gates update gate balance previous activation candidate activation reset gate allows forget previous state basic architecture employs convolutional recurrent neural network presented figure architecture integrate cnns cnns lstm long-term dependencies. structure currently uses lstms. however easily replace lstms grus. feature extraction performed using fairly standard linear feature extraction approach popularized applications speech recognition also first second derivatives features since provide small improvement performance drawing video classification analogy input data first layer cnns composed frames distributed time frame image width equal length feature vector height equals number channels number image channels equals one. input data consists frames equal window length multiplied number samples second. optimized system window duration seconds first convolutional layer filters frames eegs distributed time size using kernels size stride first pooling layer takes input vector frames distributed time size applies pooling size process repeated times convolutional layers kernels size respectively pooling layers pooling size output third pooling layer flattened frames size convolutional layer filters output flattening layer using kernels size decreases dimensionality space apply maxpooling layer size decrease dimensionality input deep bidirectional lstm network dimensionality output space output last bidirectional lstm layer -way sigmoid function produces final classification epoch. epochs typically duration. figure deep recurrent convolutional architecture two-dimensional decoding signals integrates cnns cnns lstm networks shown. structure lstms easily replaced grus. overcome problem overfitting force system learn robust features regularizations used first layers cnns. increase nonlinearity exponential linear units used adam used optimization process along mean squared error loss function lack data resources used train sophisticated statistical models compounds major problem automatic seizure detection. inter-rater agreement especially considering short seizure events manual annotation large amount data team certified neurologists extremely expensive time consuming. difficult employ large numbers board-certified neurologists perform task. study reporting results seizure corpus dataset publicly available subset corpus focuses problem seizure detection. summary corpus shown table comparison performance convolutional recurrent neural networks using lstm architectures sensitivity range shown table related curve illustrated figure systems evaluated using method scoring popular research community known overlap method true positives defined number epochs identified seizure reference annotations correctly labeled seizure system. true negatives defined number epochs correctly identified non-seizures. false positives defined number epochs incorrectly labeled seizure false negatives defined number epochs incorrectly labeled non-seizure. sensitivity shown table computed tp/. specificity computed tn/. false alarm rate number hours. comparing results cnn/lstm cnn/gru demonstrated figure find lower false positive rates cnn/lstm significantly better performance cnn/gru fact unit controls flow information like lstm unit memory unit. lstms remember longer sequences better grus outperform task since seizure detection requires modeling long distance relationships. additionally training cnn/lstm. hence training time systems comparable since cycles used training convolutional layers. neural networks determining proper initialization strategy parameters model part difficulty training. hence investigated variety initialization methods using structure introduced figure results presented table related curve illustrated figure experiments observed proper initialization weights convolutional recurrent neural network critical convergence. example initialization zeros ones methods completely convergence process. also table performance system sensitivity change different initialization methods. decrease performance deceleration convergence arises initializations result deeper layers receiving inputs small variances turn slows back propagation retards overall convergence process. best performance achieved using orthogonal initialization. method simple effective combatting exploding vanishing gradients. orthogonal matrices preserve norm vector eigenvalues absolute value one. means that matter many times perform repeated matrix multiplication resulting matrix explode vanish also orthogonal matrices columns rows orthonormal another helps weights learn different input features. overfitting serious problem deep neural nets many parameters. study used five popular regularization methods address problem. using techniques apply penalties layer parameters optimization penalties incorporated loss function network optimizes. alternative approach used dropout prevents units co-adapting much randomly dropping units connections neural network training also studied impact introducing zero-centered gaussian noise network results experiments presented table along curve figure generally best performance move towards rate dropout delivers lower rate. additionally found primary error modalities observed false alarms generated brief delta range slowing patterns intermittent rhythmic delta activity. closed-loop experiments showed regularizing methods presented table playing important role increasing false alarms slowing patterns. even though dropout effective cnns dropout placed kernels leads diminished results. solve problem future work efficient bayesian convolutional neural network explored places probability distribution cnn‚Äôs kernels approach offers better robustness overfitting small data show improve robustness training process. paper investigated deep learning architectures automatic classification using cnns. lstms outperformed grus. also studied initialization regularizations networks. future research designing powerful architecture based reinforcement learning concepts. also optimizing regularization initialization algorithms approaches. goal approach human performance range sensitivity false alarm rate hours robust training procedures needed make technology relevant wide range healthcare applications. research reported publication recently supported national human genome research institute national institutes health award number uhg. content solely responsibility authors necessarily represent official views national institutes health. material also based part upon work supported national science foundation grant iip-. opinions findings conclusions recommendations expressed material author necessarily reflect views national science foundation. corpus work funded defense advanced research projects agency auspices doug weber contract temple university‚Äôs college engineering temple university‚Äôs office senior vice-provost research. harati golmohammadi lopez obeid picone improved event classification using differential energy proceedings ieee signal processing medicine biology symposium zhang delving deep rectifiers surpassing human-level performance imagenet classification proceedings ieee international conference computer vision obeid picone machine learning approaches automatic interpretation eegs biomedical signal processing data sejdik falk eds. boca raton florida press n/a. lopez suarez jungries obeid picone automated identification abnormal eegs. ieee signal processing medicine biology symposium philadelphia pennsylvania usa. direito teixeira ribeiro castelo-branco sales dourado modeling epileptic brain states using spectral analysis topographic mapping neurosci. methods vol. temko thomas marnane lightbody boylan eeg-based neonatal seizure detection support vector machines clin. neurophysiol. vol. gotman automatic recognition epileptic seizures electroencephalogr. clin. neurophysiol. vol. nov. gotman automatic detection seizures spikes. clin. neurophysiol. vol. varsavsky mareels patient un-specific detection epileptic seizures changes variance proceedings annual international conference ieee engineering medicine biology society swisher white mace dombrowski diagnostic accuracy electrographic seizure detection neurophysiologists non-neurophysiologists adult using panel quantitative trends clin. neurophysiol. vol. jan. golmohammadi ziyabari shah obeid picone deep architectures automated seizure detection scalp eegs proceedings aaai conference artifical intelligence", "year": "2018"}