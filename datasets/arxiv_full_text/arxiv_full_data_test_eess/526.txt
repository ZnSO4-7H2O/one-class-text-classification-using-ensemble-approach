{"title": "Sounderfeit: Cloning a Physical Model with Conditional Adversarial  Autoencoders", "tag": "eess", "abstract": " An adversarial autoencoder conditioned on known parameters of a physical modeling bowed string synthesizer is evaluated for use in parameter estimation and resynthesis tasks. Latent dimensions are provided to capture variance not explained by the conditional parameters. Results are compared with and without the adversarial training, and a system capable of \"copying\" a given parameter-signal bidirectional relationship is examined. A real-time synthesis system built on a generative, conditioned and regularized neural network is presented, allowing to construct engaging sound synthesizers based purely on recorded data. ", "text": "meanwhile also shown generative network conditioned known parameters make possible control output example generate known digit class trained mnist digits. work concepts combined explore whether adversarial autoencoder conditioned known parameters parameter estimation resynthesis tasks. essence seek network simultaneously learn mimic transfer function parameters data periodic signal well data parameters using adversarial training regularize distribution latent space. latent dimensions provided network capture variance explained conditional parameters usually refered image synthesis literature style; audio represent internal state stochastic sources variance unrepresented parameters e.g. low-frequency oscillators. results visualized preliminary evaluations performed. problems came issues dataset rather network architecture training algorithms conclude lessons learned synth cloning handle sampling. real-time synthesis system sounderfeit built generative regularized neural network presented allowing construct engaging sound synthesizers based purely recorded data optional conditioning prior knowledge parameters. adversarial autoencoder conditioned known parameters physical modeling bowed string synthesizer evaluated parameter estimation resynthesis tasks. latent dimensions provided capture variance explained conditional parameters. results compared without adversarial training system capable copying given parameter-signal bidirectional relationship examined. real-time synthesis system built generative conditioned regularized neural network presented allowing construct engaging sound synthesizers based purely recorded data. autoencoder artiﬁcial neural network conﬁguration network weights trained minimize difference input output essense learning identity function. forced bottleneck layer parameters network made represent data lowdimensional code call latent parameters. recently adversarial conﬁgurations proposed method regularizing latent parameter space order match given distribution advantages two-fold ensure available range uniformly covered making useful interpolation space; maximally reduce correlation parameters encouraging represent orthogonal given network sufﬁcient capacity encode functional relationship experiments described herein periodic signal speciﬁed small number parameters sought nonetheless features complexity related sound synthesis. thus physical modeling synthesizer proved good choice. used bowed string model synthesis toolkit uses digital waveguide synthesis controlled parameters pressure force string; velocity velocity across string; position distance string-bow intersection bridge; frequency controls length delay lines thus tuning instrument. parameters represented values thus worry physical units paper; parameters linearly scaled range input neural network. data similarly scaled input linear descaling output performed diagrams paper. additionally per-element mean standard deviations across entire dataset subtracted divided respectively order ensure similar variance discrete step waveform period. extract data program written evaluate bowed string model combination position pressure integers velocity volume parameters held value instance last periods oscillation kept since parameter combinations give rise stable oscillation recordings output lower span rejected giving total recordings evenly distributed parameter range. frequency selected count samples capture periods—some parameter combinations changed tuning slightly inspection periods concatenated showed minimal deviation frequency wide variety parameters. periods recorded order minimize impact possible reproduction artifacts edges recording overlap-add synthesis. recordings phase-aligned using cross-correlation analysis representative random sample differentiated ﬁrst-order difference sample-to-sample differences thus used training data normalized stated above. reproduction thus consists de-normalizing concatenating ﬁrstorder integrating ﬁnal signal. dataset refer bowed. recently-published similar work recommends using log-spectrum representations rather audio however found since concentrating small two-period phase-aligned samples featuring relatively small amount variance learning audio signal problem. future work possible different/better results could using log-spectrum representation manner avoided need perform phase reconstruction. differentiated representation also helped suppress noise. discussed below parameter estimation data successful based dataset. resolve this second extended dataset bowed created similar manner however instead recording steady state portion synthesizer executed continuously changing parameters randomly random intervals. allowed capture dynamic regimes. samples uniformly covering parameter range captured bowed. principle autoencoder reproduce input exactly possible work also wish estimate parameters used generate data. thus additionally condition part latent space adding loss related parameter reconstruction. somewhat different providing conditional parameters input encoder note presence latent parameters allows fact assume signal purely deterministic known parameters. instance physical signal maybe internal state variables taken account initial conditions acoustic characteristics room reverb considered priori. naturally less deterministic signal known parameters must left latent parameters poorer expect parameter reconstruction code used middle layer autoencoder called latent parameters shall refer trained encode data distribution conditional posterior probability distribution mentioned general useful regularize match desired distribution. several methods exist purpose variational autoencoder uses kullbackleibler divergence given prior distribution. measures difference prior possible. adversarial conﬁguration proposed regularize based negative likelihood discriminator adversarial regularization discriminator used judge whether posterior distribution likely produced generator thus sampled rather sampled example distribution often normal uniform distribution. discriminator outputs real samples fake samples training loss generator also encoder autoencoder maximizes probability fooling discriminator thinking real sample discriminator simultaneously tries increase accuracy distinguishing samples samples thus thus encoder eventually generates posterior similar first autoencoder network composed encoder decoder/generator discriminator designed analogously notational convenience also deﬁne sampled sampled batch size. ﬁrst last dimensions respectively. current work simple one-hidden-layer anns nonlinearity linear outputs data described below composed -wide vectors acceptable results using hidden layers half size depending experiment. bias vectors corresponding sizes accordingly. hyperparameter random search used guide automatically select hyperparameters. stochastic gradient descent optimiser learning rate used train full autoencoder weights minimizing data reconstruction loss parameter reconstruction loss back-propagation. weighting parameter described below. experiments performed using tensorflow framework implemented differentiation gradient descent algorithms. small batch size used experiment evaluated after batches. found smaller batch sizes worked better adversarial conﬁguration since updates step interleaved. matrices sampled independently step uniform distribution range inclusive. conditions tested order explore role conditional latent parameters. number known parameters dataset tried training bowed dataset without extra latent parameter. label conditions respectively. third condition like condition without adversarial regularization order understand latent parameters help capture unknown variance tested condition treating pressure known parameter leaving position captured accomplished simply repeating missing parameter thus labeled parameters effects adversarial regularization thereupon conﬁgurations conditonal parameters without discriminator explored named respectively. figure demonstrates results comparing middle bottom curves trouble values pressure extremes position autoencoder able less encode distribution dataset. curve generated explicitly specifying parameters instead letting autoencoder infer them demonstrates output parameter-driven reconstruction held constant. although perfect reproduction demonstrates parameters conditioned according dataset thus models data-parameter relationship. figure parameter estimation performance network bowed full dataset error=.; bowed half dataset error=.; bowed full dataset error=.; bowed half dataset error=.. notices values signal matches well others varies target signal. example case high values push signal towards sharp peaks values tend towards oscillations; resemble condition different aspects. meanwhile consistency stylistic inﬂuence signal different values pressure. finally look encoder performance producing signal synthesizer parameter trajectory starting variation pressure variation position variation parameters. figure shows actually rather disappointing performance respect however clarify information present previous analysis estimation clearly better pressure easily disturbed changes position. nonetheless tendency estimate right direction rather ﬂipping center. since varying hyperparameters network solve problem hypothesized error could come sources ambiguities dataset—indeed examines shape signal position changes notices symmetry values either side pos=. consequence inverse problem underspeciﬁed leading ambiguity parameter estimate. underrepresented variance dataset; testing data varies continuously parameters dataset constructed based per-parameter steady state. investigate this network trained half dataset consisting samples bowed position furthermore mentioned extended dataset bowed constructed based random parameter variations. results figure show training half-bowed dataset changed character errors improve overall however extended bowed dataset gave improved parameter estimation much improved position case. thus concluded sources contributed parameter estimation difﬁculties. also examine using network performs parameter conditioned. figure seen signals match many cases similar indeed figure however given since parameter horizontal axis position conditioned indeed reﬂected variable automatically since principle source variance unexplained conditioned parameter pressure. figure shows resulting parameter space parameters left absorbed unsupervised latent space. indeed seems regularization system encouraged cover entire range variance dataset. without regularization relationship inferred variables —although appears complex could captured pearson’s correlation—while completely gone regularized version star shape generated without regularization autoencoder attempts maximally separate various aspects variance reduced -dimensional space useful data analysis produce good interpolation space. finally found small decoder network weights biases overlap-add synthesis could performed real time laptop computer thus create data-driven wavetable synthesizer call sounderfeit number adjustable parameters. regularization encourages parameter space interesting sense represent orthogonal axes within distribution cover deﬁned range tend towards uniform coverage without holes. demonstrated figure like many machine learning approaches quality results depend strongly hyperparameters used network size architecture learning rates conditional regularization weights etc. must adapted practical notes found getting adversarial method properly regularize latent variables presence conditional variables somewhat tricky; batch size relative learning rates played balancing generator discriminator performances. research adversarial methods current area investigation community many techniques could apply here; moreover comparison variational methods needed. expectedly found parameter estimation extremely sensitive phase alignment; tried randomizing phase examples training gave better parameter estimates quite damaging autoencoder performance. general oversensitivity phase problem method downside time domain representation. nevertheless attempted outline potential autoencoders latent spaces audio analysis synthesis based speciﬁc signal source. simple fully-connected single-layer architecture explored; myriad improvements could likely made using convolutional layers different activation functions etc. important quality speciﬁc results wish point modular approach autoencoders enable modeling oscillator periods known unknown parameters that contrast larger datasets covering many instruments interesting insights even small data. motivation work could questioned sense black model seemingly bring much table presence existing semantically-rich physical model. indeed work digital synthesizer used easy gain access dataset. shown results best parameters found combination automatic manual optimisation speciﬁc dataset demonstrate principles design however noted actual results varied sometimes unexpectedly small changes parameters. hyperparameter optimization non-trivial especially comes audio mean squared error reveal much perceptual quality results trial error game. thus truly universal turn-key synthesizer copier would require future work measuring combined hypercost balances well desire good reproduction good parameter estimation quality well-distributed latent parameters. work could beyond mean squared error involve paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin. tensorflow large-scale machine learning heterogeneous systems available http//tensorﬂow.org. fairly complicated clean signal small number parameters. principle method could used much richer real instrument recordings provided measurement estimate acoustically-relevant parameters available. example applied recorded vowel vocalizations vowel category single continuous variable creating real-time vowel synthesizer similar bowed string results controllable vowel knob characteristics represented latent space. another question perform simultaneous estimation generation network. indeed part long-term goals play somewhat latent space using known audio community cross-synthesis machine learning community style transfer i.e. swapping bottom halfs autoencoder networks allowing drive synthesizer conditioned latent parameters estimated incoming signal.", "year": "2018"}