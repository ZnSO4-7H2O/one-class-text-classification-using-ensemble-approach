{"title": "Multilingual Speech Recognition With A Single End-To-End Model", "tag": "eess", "abstract": " Training a conventional automatic speech recognition (ASR) system to support multiple languages is challenging because the sub-word unit, lexicon and word inventories are typically language specific. In contrast, sequence-to-sequence models are well suited for multilingual ASR because they encapsulate an acoustic, pronunciation and language model jointly in a single network. In this work we present a single sequence-to-sequence ASR model trained on 9 different Indian languages, which have very little overlap in their scripts. Specifically, we take a union of language-specific grapheme sets and train a grapheme-based sequence-to-sequence model jointly on data from all languages. We find that this model, which is not explicitly given any information about language identity, improves recognition performance by 21% relative compared to analogous sequence-to-sequence models trained on each language individually. By modifying the model to accept a language identifier as an additional input feature, we further improve performance by an additional 7% relative and eliminate confusion between different languages. ", "text": "dnn-based systems multilingual still require language-speciﬁc pronunciation models language models means often models must know speech language identity inference moreover usually optimized independently case errors component propagate subsequent components seen training. sequence-to-sequence models fold single network making attractive explore multilingual speech recognition. building multilingual sequence-tosequence model requires taking union languagespeciﬁc grapheme sets training model jointly data languages. addition simplicity end-to-end nature models means model parameters contribute handling variations different languages. attention-based sequence-to-sequence model based listen attend spell model details explained next section. work similar similarly proposes end-to-end trained multilingual recognizer directly predict grapheme sequences distantly related languages. utilize hybrid attention/connectionist temporal classiﬁcation model integrated independently trained grapheme paper simpler sequence-to-sequence model without explicit study corpus closely related indian languages. show model jointly trained across data indian languages without explicit language speciﬁcation consistently outperforms monolingual models trained independently language. even without explicit language speciﬁcation model rarely confused languages. also experiment certain language-dependent variants model. particular obtain largest improvement conditioning encoder speech language identity. also several experiments synthesized data gain insights behavior models. multilingual model unable code-switch between languages indicating language model dominating acoustic model. finally language-conditioned model able transliterate urdu speech hindi text suggesting model learned internal representation disentangles underlying acoustic-phonetic content language. section describe listen attend spell attention-based sequence-to-sequence model proposed chan well proposed modiﬁcations support recognition multiple languages. training conventional automatic speech recognition system support multiple languages challenging subword unit lexicon word inventories typically language speciﬁc. contrast sequence-to-sequence models well suited multilingual encapsulate acoustic pronunciation language model jointly single network. work present single sequence-to-sequence model trained different indian languages little overlap scripts. speciﬁcally take union language-speciﬁc grapheme sets train grapheme-based sequence-to-sequence model jointly data languages. model explicitly given information language identity improves recognition performance relative compared analogous sequence-to-sequence models trained language individually. modifying model accept language identiﬁer additional input feature improve performance additional relative eliminate confusion different languages. speech recognition made remarkable progress past years services google voice search supporting languages. expanding coverage world’s languages great interest academia industry. however many cases resources available train large vocabulary continuous speech recognizers severely limited challenges meant perennial interest multilingual cross-lingual models allow knowledge transfer across languages thus relieve burdensome data requirements previous work multilingual speech recognition limited making acoustic model multilingual multilingual require common phone others share acoustic model parameters swap structure proposed lower layers deep neural network shared across languages output layer languagespeciﬁc. alternatively multilingual bottleneck features feature extractor used either gaussian mixture model begin training joint model consisting model described previous section trained directly combined multilingual dataset. model given explicit indication training dataset composed different languages. however show later model still able recognize speech multiple languages despite lack runtime language-speciﬁcation. also experiment variant joint model architecture trained multitask learning conﬁguration jointly recognize speech simultaneously predict language. language annotation thus utilized training passed input inference. order predict language average encoder output across time frames compute utterance-level feature. averaged feature passed softmax layer predict likelihood speech belonging language softmax encoder-conditioned decoder-conditioned encoder+decoder-conditioned variants. contrast model language used part training cost. conduct experiments data nine indian languages shown table corresponds total hours training data hours test data. nine languages little overlap character sets exception hindi marathi devanagari script. small overlap means output vocabulary multilingual models union character sets also quite large containing characters. separate validation sets around utterances language used hyperparameter tuning. utterances dictated queries collected using desktop mobile devices. sequence-to-sequence model consists three modules encoder decoder attention network trained jointly predict sequence graphemes sequence acoustic feature frames. -dimensional log-mel acoustic features computed every window. following stack consecutive frames stride stacked frames factor downsampling enables simpler encoder architecture encoder comprised stacked bidirectional recurrent neural network reads acoustic features outputs sequence high-level features encoder similar acoustic model system. previous hidden state decoder ˜yt− character embedding vector typical practice rnn-based language models. decoder analogous language model component pipeline system asr. posterior distribution output time step given multilingual models multilingual scenario given languages independent character sets {cc··· training sets combined training dataset thus given union datasets language baseline train nine monolingual models independently data language. tune hyperparameters marathi reuse optimal conﬁguration train models remaining languages. best conﬁguration marathi uses layer encoder comprised bidirectional long short-term memory cells layer decoder containing lstm cells layer. regularization apply small weight penalty gaussian weight noise standard deviation parameters training steps. monolingual models converge within gradient steps. since multilingual training corpus much larger able train joint larger multilingual model without overﬁtting. training validation also union language-speciﬁc validation sets. best conﬁguration uses layer encoder comprised bilstm cells layer decoder containing lstm cells layer. multitask model among work best. restricted values large language prediction task would dominate primary task small additional task would effect training loss. conditional models -dimensional language embedding. regularization gaussian weight noise standard deviation training steps. multilingual models trained approximately million steps. models implemented tensorflow trained using asynchronous stochastic gradient descent using workers. initial learning rate monolingual models multilingual models learning rate decay models. ﬁrst compare language-speciﬁc models joint model trained languages. shown table joint model outperforms language-speciﬁc models languages. fact joint model decreases weighted average wers across languages weighted number words relative monolingual models. result quite interesting joint model single model compared different monolingual models unlike monolingual models joint model language-aware runtime. finally large performance gain joint model also second compare joint model multitask trained variant. shown right columns table model shows limited improvements joint model. might following reasons static choice since language prediction task easier dynamic high initially decays time might better suited language prediction mechanism averaging encoder outputs might ideal. learned weighting encoder outputs similar attention module might better suited task. third table shows joint models conditioned language outperform joint model. encoder-conditioned model better decoder-conditioned model indicating form acoustic model adaptation towards different languages accents occurs encoder conditioned. addition conditioning encoder decoder improve much conditioning encoder suggesting feeding encoder language information sufﬁcient encoder outputs decoder anyways attention mechanism. comparing model performances across languages models perform worst malayalam kannada. hypothesize agglutinative nature languages makes average word longer languages fig. confusion matrices joint encoder-conditioned models truncated precision joint model rarely confused languages conditioning removes rare cases almost completely. compared languages like hindi gujarati. example average training word malayalam characters compared hindi. fact found contrast character error rate hindi malayalam quite close. often model confuse languages? ability proposed model recognize multiple languages comes potential side effect confusing languages. lack script overlap indian languages exceptions hindi marathi means surface analysis script used model output good proxy tell model confused languages not. carry analysis word level check output words graphemes single language mixture. test word ﬁrst ground truth language case failure test languages. word cannot expressed using character single language classify mixed. result joint encoder-conditioned model summarized figure models rarely confused languages result joint model interesting given lack explicit language awareness showing model implicitly learning predict language also interesting observe conditioning joint model language confusion languages. joint model perform code-switching? joint model theory capacity switch languages. fact code-switch english indian languages presence english words training data. interested testing model could also code-switch pair indian languages seen training. purpose created artiﬁcial dataset selecting tamil utterances appending number hindi utterances break between. disappointment model able code-switch all. picks scripts sticks manual inspection shows that model chooses hindi transcribes hindi part utterance similarly model chooses tamil transcribes tamil part rare occasions also transliterates hindi part. suggests language model dominating acoustic model points overﬁtting known issue attention-based sequence-to-sequence models conditioned model output mismatched language interesting question model obey acoustics faithful language answer this created artiﬁcial dataset urdu utterances labeled hindi language transcribed encoderconditioned model. turns model extremely faithful language sticks hindi’s character set. manual inspection outputs reveals model transliterates urdu utterances hindi suggesting model learned internal representation disentangles underlying acoustic-phonetic content language identity. present sequence-to-sequence model multilingual speech recognition able recognize speech without explicit language speciﬁcation. also propose simple variants model conditioned language identity. proposed model variants substantially outperform baseline monolingual sequence-tosequence models languages rarely chooses incorrect grapheme output. model however cannot handle code-switching suggesting language model dominating acoustic model. future work would like integrate conditional variants model separate language-speciﬁc language models improve recognition accuracy. would also like compare proposed models traditional models live trafﬁc data. exploration reasons lack would like thank rohit prabhavalkar yonghui vijay peddinti zhifeng chen patrick nguyen helpful comments. also thankful anonymous reviewers helpful comments. laurent besacier etienne barnard alexey karpov tanja schultz automatic speech recognition under-resourced languages survey speech communication vol. fuliang weng harry bratt leonardo neumeyer andreas stolcke study multilingual speech recognition proc. eurospeech deng dong yi-fan gong alex acero chin-hui study multilingual acoustic modeling large vocabulary proc. ieee conference acoustics speech signal processing lukas burget petr schwarz mohit agarwal pinar akyazi feng arnab ghoshal ondrej glembek nagendra goel martin karaﬁat daniel povey ariya rastrow richard rose samuel thomas multilingual acoustic modeling speech recognition based subspace gaussian mixture models proc. ieee conference acoustics speech signal processing samuel thomas sriram ganapathy hynek hermansky multilingual features resource lvcsr systems proc. ieee conference acoustics speech signal processing georg heigold vincent vanhoucke andrew senior patrick nguyen marc’aurelio ranzato matthieu devin jeff dean multilingual acoustic models using distributed deep neural networks proc. ieee conference acoustics speech signal processing arnab ghoshal pawel swietojanski steve renals multilingual training deep neural networks proc. ieee conference acoustics speech signal processing jui-ting huang jinyu dong deng yifan gong cross-language knowledge transfer using multilingual deep neural network shared hidden layers proc. ieee conference acoustics speech signal processing shinji watanabe takaaki hori john hershey language independent end-to-end architecture joint language speech recognition proc. ieee workshop automatic speech recognition understanding imseng povey motlicek schultz bourlard multilingual deep neural network based acoustic modeling rapid language adaptation proc. ieee conference acoustics speech signal processing zolt´an t¨uske joel pinto daniel willett ralf schl¨uter investigation cross-and multilingual features under matched mismatched acoustical conditions proc. ieee conference acoustics speech signal processing william chan navdeep jaitly quoc oriol vinyals listen attend spell neural network large vocabulary conversational speech recognition proc. ieee conference acoustics speech signal processing rich caruana multitask learning machine learning alex graves practical variational inference neural netproc. neural information processing systems jeffrey dean greg corrado rajat monga chen matthieu devin quoc mark marc’aurelio ranzato andrew senior paul tucker yang andrew large scale distributed deep networks proc. neural information processing systems prahallad lavanya prahallad kishore ganapa thiraju madhavi simple approach building transliteration editors indian languages journal zhejiang universityscience vol.", "year": "2017"}