{"title": "Convolutional Dictionary Learning: A Comparative Review and New  Algorithms", "tag": "eess", "abstract": " Convolutional sparse representations are a form of sparse representation with a dictionary that has a structure that is equivalent to convolution with a set of linear filters. While effective algorithms have recently been developed for the convolutional sparse coding problem, the corresponding dictionary learning problem is substantially more challenging. Furthermore, although a number of different approaches have been proposed, the absence of thorough comparisons between them makes it difficult to determine which of them represents the current state of the art. The present work both addresses this deficiency and proposes some new approaches that outperform existing ones in certain contexts. A thorough set of performance comparisons indicates a very wide range of performance differences among the existing and proposed methods, and clearly identifies those that are the most effective. ", "text": "current leading algorithms based alternating direction method multipliers decomposes problem subproblems solved soft-thresholding efﬁcient non-iterative solution domain design convolutional dictionary learning algorithms less straightforward. algorithms adopt usual approach standard dictionary learning alternating sparse coding step updates sparse representation training data given current dictionary dictionary update step updates current dictionary given sparse representation. inherent computational cost latter update makes problem difﬁcult problem. recent batch-mode algorithms share structure introduced primary features augmented lagrangian methods solution computationally expensive subproblems frequency domain. earlier algorithms exist thorough literature review) since less effective consider here focusing subsequent methods proposed number improvements algorithm including efﬁcient sparse representation dictionary updates different augmented lagrangian structure better convergence properties unfortunately absence thorough performance comparisons provides comparisons well absence careful exploration optimum choice algorithm parameters works difﬁcult determine abstract—convolutional sparse representations form sparse representation dictionary structure equivalent convolution linear ﬁlters. effective algorithms recently developed convolutional sparse coding problem corresponding dictionary learning problem substantially challenging. furthermore although number different approaches proposed absence thorough comparisons makes difﬁcult determine represents current state art. present work addresses deﬁciency proposes approaches outperform existing ones certain contexts. thorough performance comparisons indicates wide range performance differences among existing proposed methods clearly identiﬁes effective. sparse representations become widely used successful models inverse problems signal processing image processing computational imaging. reconstruction signal sparse representation respect dictionary matrix linear i.e. computing sparse representation given signal referred sparse coding usually involves solving optimization problem. solving problems involving images signiﬁcant size representations typically independently applied sets overlapping image patches intractability learning unstructured dictionary matrix mapping vector space dimensionality number pixels entire image. convolutional form sparse representations replaces unstructured dictionary linear ﬁlters {dm}. case reconstruction representation {xm} entire image instead small image patch. form representation ﬁrst introduced time label translation-invariant sparse representations recently enjoyed revival interest convolutional sparse representations inspired deconvolutional networks interest spurred development efﬁcient methods number greedy matching pursuit type algorithms developed translation-invariant sparse representations recent algorithms largely concentrated convolutional form standard basis pursuit denoising problem worth noting however solution based fista gradient computed frequency domain generally less effective admm solution exhibits relatively small performance difference larger values typically used three recent methods receive thorough attention listed above. algorithm addresses variant problem customized neural signal processing relevant imaging applications appeared ﬁnalizing paper feasible include analysis main experimental comparisons. however since authors made implementation method publicly available include method additional performance comparisons sec. svii supplementary material. main contributions present paper providing thorough performance comparison among different methods proposed allowing reliable identiﬁcation effective algorithms. algorithms proposed different derivations fact closely related fall within class algorithm. constraint norms ﬁlters required avoid scaling ambiguity ﬁlters coefﬁcients. training images considered dimensional vectors number pixels image denote number ﬁlters number training images respectively. problem non-convex variables {dm} {xmk} convex {xmk} {dm} constant vice versa. standard dictionary learning usual approach minimizing functional alternate updates sparse representation dictionary. design algorithm therefore decomposed three components choice sparse coding algorithm choice dictionary update algorithm choice coupling mechanism including many iterations update performed alternating internal variables transferred across alternating. zero-padded common spatial dimensions straightforward dealing complication consider zero-padded constraint requires zero outside desired support. denote projection operator zeros regions ﬁlters outside desired support write constraint combines support constraint normalization constraint algorithms solving problem discussed sec. iii. common feature methods need solve linear system includes data ﬁdelity case step problem solved frequency domain critical difference composed independent components rank instead rank efﬁcient sherman morrison solution cannot directly exploited. property makes dictionary update inherently computationally expensive sparse coding stage complicating design algorithms leading present situation less clarity best choice dictionary learning algorithm choice sparse coding algorithm. iterations dual variable corresponding constraint apply admm problem variable splitting introducing auxiliary variable constrained equal primary variable leading equivalent problem soft-thresholding function sign vector considered applied element-wise denoting element-wise multiplication. computationally expensive step requires solving linear system since large matrix impractical solve linear system using approaches effective convolutional dictionary. possible however exploit efﬁcient implementation convolution convolution theorem. transforming domain gives denotes variable structure consists concatenated diagonal matrices linear system decomposed independent linear systems left hand side consisting diagonal matrix plus rank-one component solved efﬁciently exploiting sherman-morrison formula convolutional form method optimal directions constraint ﬁlter normalization. develop algorithms solving problem spatial domain solve critical subproblems frequency domain. want solve {dm} relatively small support computing convolutions frequency domain need work linear system decomposed independent linear systems contrast left hand side consisting diagonal matrix plus rank component precludes direct sherman-morrison formula conjugate gradient obvious approach solving without explicitly construct matrix apply iterative method conjugate gradient experiments reported indicated solving system relative residual tolerance better sufﬁcient dictionary learning algorithm converge reliably. number iterations required substantially reduced using solution previous outer iteration initial value. iterated sherman-morrison since independent linear systems decomposed left hand side consisting diagonal matrix plus rank component iteratively apply sherman-morrison formula obtain solution approach effective small moderate performs poorly large since computational cost spatial tiling efﬁcient solution sherman-morrison formula possible. pointed larger training images spatially tiled form single large image problem solved section convenient introduce different blockmatrix vector notation coefﬁcient maps dictionary overload usual symbols emphasize corresponding roles. deﬁne deﬁne update coupling sparse coding dictionary update stages typically solved iterative algorithms many algorithms working variable used represent current solution. major design choices coupling alternating optimization stages therefore alternative strategy coupling subproblems auxiliary variables admm algorithm tends stable requiring multiple iterations alternating converging faster. general form matrix block-circulant matrix constructed blocks since multiplication dictionary block vector block-circulant matrix equivalent convolution additional dimension equivalent problem represents method. iterative shrinkage-thresholding algorithm accelerated proximal gradient method used recent online algorithm previously considered dictionary update batch-mode dictionary learning algorithm. parameter controlling gradient descent step size. parameter computed adaptively using backtracking step size rule experiments reported used constant simd method proposed maps dictionary update problem equivalent problem method achieves considering array training images single training volume. corresponding dictionary ﬁlters also inherently constraint modiﬁed require zero ﬁrst slice ﬁnal results ﬁlters desired. admm consensus proposed entirely distinct methods turns closely related method admm consensus data ﬁdelity term constraint expressed domain. since notation cumbersome point illustrated case argument easily generalized arbitrary method solving involves appending impulse ﬁlter dictionary solving problem constrains coefﬁcient corresponding ﬁlter zero mask unity unconstrained mask zero approaches provide similar performance major difference former complicated implement latter restricted addressing problems zero entries. mask decoupling approach experiments reported since require restrictions masked convolutional dictionary learning wish learn dictionary data missing samples reason concerned possibility boundary artifacts resulting circular boundary conditions associated computation convolutions domain useful introduce variant includes spatial mask represented diagonal matrix problem structure difference replacement norm indicator function constraint set. admm iterations thus largely differences norm replaced indicator function constraint step corresponding computationally expensive solve expensive section re-use variant notation introduced sec. iii-b. masked dictionary update solved hybrid mask decoupling admm consensus approaches formulated different methods solving problem. proposed exploits mask decoupling technique involving applying alternative variable splitting give admm problem channel dictionary ﬁlter channel training signal. denote number channels before separately consider sparse coding dictionary updates alternating minimization functional. form single-channel problem iterations admm algorithm solve signiﬁcant difference sec. ii-a matrix block structure whereas block structure. corresponding frequency domain matrix decomposed components rank decomposed components rank consequently dictionary update algorithms discussed sec. also applied multi-channel problem step corresponding projection onto dictionary constraint e.g. replaced step corresponding proximal operator norm e.g. iterated sherman-morrison method effective multi-channel presented section extension framework sec. iii. application extension masked framework sec. straightforward supported software implementations problem solved fista described sec. iii-d term complicated presence spatial mask. difﬁculty handled transforming back forth spatial frequency domains convolution operations computed efﬁciently frequency domain masking operation computed spatial domain i.e. discussed distinct ways deﬁning convolutional representation multi-channel data singlechannel dictionary together distinct coefﬁcient maps channel multi-channel dictionary together shared coefﬁcient maps. since dictionary learning problem former case straightforward section compare computational performance various approaches discussed carefully selecting optimal parameters algorithm ensure fair comparison. proceeding results computational experiments summarize dictionary learning algorithms compared. instead using complete dictionary learning algorithm proposed prior work consider primary contribution works dictionary update method incorporated algorithm structure demonstrated effective auxiliary variable coupling single iteration subproblem alternating. since sparse coding stages same algorithm naming based dictionary update algorithms. spatial tiling algorithm uses dictionary update proposed effective variable coupling alternation strategy discussed admm consensus algorithm uses dictionary update technique proposed substantially effective variable coupling alternation strategy discussed algorithm parallel implementation sparse coding dictionary update stages. steps stage completely parallelizable training image index steps dictionary update synchronization point step independent dictionary estimates averaged projected update consensus variable processes share. algorithm uses dictionary update proposed effective variable coupling alternation strategy discussed cases slightly better time performance obtained performing iterations sparse coding update followed single dictionary update consider complication here. šorel šroubek observe admm consensus problem inherently parallelizable actually implement corresponding algorithm parallel form allow resulting computational gain quantiﬁed empirically. structure single-channel case solutions different channel dictionaries independent dictionary update multi-channel case computationally challenging single channel case. discussion reveals interesting dual relationship number images coefﬁcient number channels dictionary solving problem proximal algorithms admm fista controls rank expensive subproblem convolutional sparse coding stage controls rank main subproblem convolutional dictionary update. addition algorithms appropriate large case dictionary update also suitable large case sparse coding vice versa. extended consensus algorithm based dictionary update constructed hybrid dictionary update methods proposed effective variable coupling alternation strategy discussed extended consensus parallel algorithm m-cns parallel implementation sparse coding dictionary update. steps stage steps dictionary update completely parallelizable training image index synchronization point step independent dictionary estimates averaged projected update consensus variable processes share. addition algorithms listed above investigated stochastic averaging admm proposed implementation algorithm based method found promising computational cost iteration convergence competitive methods considered here. however since number algorithm details provided possible implementation omits critical components. results therefore included order avoid making unfair comparison. compare dictionary learning algorithm algorithms reported substantially faster. include algorithms either main experiments implementations practical large number different training image sets parameter choices used experiments include algorithms additional performance comparisons sec. svii supplementary material. multi-channel problems included main experiments space limitations relevant experiments provided sec. sviii supplementary material. per-iteration computational complexities methods summarized table instead specifying dominant terms include major contributing terms provide detailed picture computational cost. methods scale linearly number ﬁlters number images except variants scale inclusion dependency parallel algorithms provides conservative view behavior. practice either scaling weak scaling number available cores exceeds weak scaling exceeds number available cores. memory usage depends method implementation methods memory requirement main variables. used training sets images. sets nested sense images also present larger sets. parent images consisted greyscale images size pixels derived mirflickr-m dataset cropping rescaling conversion greyscale. additional images size source used test allow comparison generalization performance taking account possible differences overﬁtting effects different methods. greyscale images divided pixel values within interval highpass ﬁltered subtracting lowpass component computed tikhonov regularization gradient term regularization parameter image data directly included mirflickr-m dataset resolution since dataset image classiﬁcation tasks. therefore identiﬁed downloaded original images used construct mirflickr-m dataset. fig. dictionary learning comparison images decay value cbpdn functional respect time iterations. tiled overlap time plot cns-p overlap iterations plot. image selected grid search functional values obtained iterations values admm dictionary updates values fista dictionary updates. grid resolutions fig. dictionary learning comparison images decay value cbpdn functional respect time iterations. overlap time plot cns-p overlap iterations plot. best method i.e. ones yielding lowest value functional iterations selected center ﬁner grid search functional values obtained iterations logarithmically spaced points logarithmically spaced points logarithmically spaced points optimal parameters method taken yielding lowest value functional iterations ﬁner grid. procedure repeated sets images. indication sensitivities different methods parameters results coarse grid search image found sec. supplementary material. optimal parameters determined grid searches summarized table compare performance methods learning dictionary ﬁlters size sets images setting sparsity parameter using parameters determined grid searches method. fig. dictionary learning comparison images decay value cpbdn functional respect time iterations. overlap time plot cns-p overlap iterations plot. avoid complicating comparisons used ﬁxed penalty parameters without adaptation methods apply relaxation methods admm algorithms. similarly used ﬁxed fista without applying backtracking step-size adaptation rule. performance terms convergence rate functional respect case methods quite similar performance terms functional value convergence respect iterations. larger training sizes somewhat better performance respect iterations poor performance respect time. substantially better time scaling depending relative residual tolerance. experiments ﬁxed tolerance resulting computation times comparable methods. smaller tolerance leads better convergence respect iterations substantially worse time performance. method behaves similarly admm consensus expected relationship established sec. iii-c larger memory footprint. spatial tiling method hand tends slower convergence respect iterations time methods. explore performance methods since provide substantial advantages others. parallel regular consensus evolution cbpdn functional respect iterations former requires much less computation time fastest method overall. moreover parallel consensus exhibits almost ideal parallelizability overhead scaling linearly competitive computation times. fista also competitive achieving good results less time serial methods even outperforming time performance cns-p case shown fig. believe variation relative performance unstable dependence functional illustrated example fig. supplementary material. functional decreases slowly decreased increases rapidly minimum reached constraint discussed sec. vii-g. experiments algorithms include spatial mask mask identity allow comparison performance algorithms without spatial mask. plots comparing evolution masked cbpdn fig. dictionary learning spatial mask comparison images decay value masked cbpdn functional respect time iterations masked versions algorithms. m-cns m-cns-p overlap iterations plot. fig. dictionary learning spatial mask comparison images decay value masked cbpdn functional respect time iterations masked versions algorithms. m-cns m-cns-p overlap iterations plot. fig. dictionary learning spatial mask comparison images decay value masked cbpdn functional respect time iterations masked versions algorithms. m-cns m-cns-p overlap iterations plot. best performance consistently provides good performance terms convergence respect computation time despite additional ffts discussed sec. v-c. parallel hybrid mask decoupling/consensus method m-cns-p competitive approach problem providing best time performance lagging slightly behind m-fista contrast corresponding mask-free variants m-ism worse performance terms time iterations. suggests m-cg requires value relative residual tolerance smaller produce good results expense much longer computation times. exception cost computing masked version increases computation time masked versions slightly worse mask-free variants general using masked versions leads marginal decrease convergence rate respect iterations small increase computation time. fig. evolution cbpdn functional test using partial dictionaries obtained training images. tiled overlap time plot cns-p overlap iterations plot. fig. evolution cbpdn functional test using partial dictionaries obtained training images. tiled large overlap time plot cns-p overlap iterations plot. fig. evolution cbpdn functional test using partial dictionaries obtained training images masked versions algorithms. m-cns m-cns-p overlap iterations plot. fig. evolution cbpdn functional test using partial dictionaries obtained training images masked versions algorithms. m-cns m-cns-p overlap iterations plot. dictionaries learned different methods experiments image test used learning. methods discussed saved dictionaries iteration intervals training. dictionaries used sparse code images test allowing evaluation evolution test cbpdn functional dictionaries change training. results dictionaries learned training images shown figs. respectively corresponding results algorithms spatial mask shown figs. respectively. note time axis plots refers time dictionary learning code used generate relevant dictionary time sparse coding test set. expected independent method dictionaries obtained training images exhibit better performance ones trained images. overall performance training good predictor performance testing suggests functional value sufﬁciently large training reliable indicator dictionary quality. convenient general approach parameter selection. section show possible construct heuristics allow reliable parameter selection best performing methods considered here. parameter scaling properties estimates parameter scaling properties respect derived sec. siii supplementary material. problem without spatial mask scaling properties derived sparse coding problem dictionary updates based admm equality constraint admm consensus fista. estimates indicate scaling penalty parameter convolutional sparse coding scaling penalty parameter dictionary update admm equality constraint admm consensus scaling step size fista derivations tiled methods lead simple scaling relationship included. problem spatial mask scaling properties derived sparse coding problem dictionary updates based admm blockconstraint extended admm consensus. scaling penalty parameter masked version convolutional sparse coding scaling penalty parameter dictionary update extended consensus framework simple rule scaling block-constraint admm sec. v-a. parameter selection guidelines derivations discussed indicate optimal algorithm parameters expected either constant linear parameters effective algorithms i.e. fista m-cns performed additional computational experiments estimate constants scaling relationships. cns-p m-cns-p parameter dependence serial counterparts therefore evaluated separately. similarly m-fista included experiments functional evolution fista identity mask training size constructed ensemble training sets size random selection image training set. algorithm dependence convergence behavior algorithm parameters evaluated computing iterations algorithm members ensemble size grids values admm dictionary updates values fista dictionary updates. parameter grids consisted logarithmically spaced points ranges speciﬁed table iii. parameter ranges corresponding functional values remained within optimal values. fig. contour plots ensemble median normalized functional values different algorithm parameters. black lines correspond level curves indicated values plotted surfaces dashed lines represent parameter selection guidelines combine analytic derivations empirical behavior plotted surfaces. normalized results training dividing minimum functional computed statistics normalized values sets size statistics reported plots sec. supplementary material also aggregated contour plots median normalized functional values displayed fig. contour plots horizontal axis corresponds number training images vertical axis corresponds parameter interest. scaling behavior optimal parameter clearly seen direction valley contour plots. parameter selection guidelines obtained manual ﬁtting constant linear scaling behavior contour plots plotted also summarized table fig. guideline m-cns appear follow path level curves. select guideline follow path theoretical estimate scaling properties parameter sec. siii-g supplementary material constant path suggested level curves leads logarithmically decreasing curve would reach negative parameter values sufﬁciently large reliable explanation unexpected behavior level curves suspect related loss diversity training image sets since sets images chosen ﬁxed images. also worth noting upper level curves larger functional values e.g. follow unexpected decreasing path. guarantee convergence fista inverse gradient step size greater equal lipschitz constant gradient functional fig. level curves guideline correspond potentially unstable regime functional value surface large gradient. gradient surface much smaller guideline indicating convergence sensitive parameter value region. chose guideline precisely biased towards stable regime. parameter selection guidelines presented section expected reliable training data similar characteristics used experiments i.e. natural images pre-processed described sec. vii-c similar sparsity parameter i.e. nevertheless since scaling properties derived sec. siii supplementary material remain valid reasonable expect similar heuristics albeit different constants would hold different training data sparsity parameter settings. results indicate distinct approaches dictionary update problem provide leading algorithms. serial processing context fista dictionary update proposed outperforms methods including consensus without spatial mask. seem surprising considering admm outperforms fista problem easily understood taking account critical difference linear systems need solved tackling convolutional dictionary update problems proximal methods admm fista. case major linear system solved frequency domain structure allows efﬁcient solution sherman-morrison formula providing advantage admm. contrast except case highly efﬁcient solution convolutional dictionary update giving advantage methods fista employ gradient descent steps rather solving linear system. parallel processing context consensus dictionary update proposed used together alternative algorithm structure proposed leads algorithm best time performance mask-free problem hybrid mask decoupling/consensus dictionary update proposed provides best time performance masked problem. interesting note that despite clear suitability admm consensus framework convolutional dictionary update problem parallel implementation essential outperforming methods; signiﬁcantly outperformed fista dictionary update even method competitive optimal algorithm parameters leading methods considered tend quite stable across different training sets similar type provided reliable heuristics selecting parameters provide good performance. noted however fista appears sensitive parameter admm methods penalty parameter. additional experiments reported supplementary material indicate fista parallel consensus methods scalable relatively large training sets e.g. images pixels. computation time exhibits linear scaling number training images number dictionary ﬁlters close linear scaling number pixels image limited experiments involving color dictionary learning indicate additional computational cost compared greyscale dictionary learning moderate. comparisons publicly available implementations complete methods authors indicate that method heide scale well training images sets even moderate size exhibiting slow convergence respect computation time. consensus method proposed gives good performance consensus method šorel šroubek converges much slowly learn dictionaries properly normalized ﬁlters. method papyan converges rapidly respect number iterations appears scale well training size slower fista parallel consensus methods respect time resulting dictionaries offer competitive performance leading methods proposed terms performance testing image sets. quan w.-k. jeong compressed sensing reconstruction dynamic contrast enhanced using gpu-accelerated convolutional sparse coding ieee intl. symp. biomed. imag. apr. doi./isbi.. zhang patel convolutional sparse low-rank coding-based rain streak removal proc. ieee winter conference applications computer vision march doi./wacv.. boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers foundations trends machine learning vol. doi./ degraux kamilov boufounos online convolutional dictionary learning multimodal imaging proc. ieee conf. image process. beijing china sep. doi./icip... almeida figueiredo deconvolving images unknown boundaries using alternating direction method multipliers ieee trans. image process. vol. aug. doi./tip.. sim¸sekli gramfort learning morphology brain signals using alpha-stable convolutional sparse coding advances neural information processing systems guyon luxburg bengio wallach fergus vishwanathan garnett eds. arxiv.. engan aase husøy method optimal directions frame design proc. ieee int. conf. acoust. speech signal process. vol. doi./icassp.. afonso bioucas-dias figueiredo augmented lagrangian approach constrained optimization formulation imaging inverse problems ieee trans. image process. vol. mar. doi./tip.. huiskes thomee trends ideas visual concept detection flickr retrieval evaluation initiative proc. international conference multimedia information retrieval doi./. kavukcuoglu sermanet boureau gregor mathieu lecun learning convolutional feature hierarchies visual recognition adv. neural inf. process. syst. zeiler taylor fergus adaptive deconvolutional networks high level feature learning proc. ieee int. conf. comp. vis. barcelona spain nov. doi./iccv.. wohlberg convolutional sparse representations image model impulse noise restoration proc. ieee image video multidim. signal process. workshop bordeaux france jul. doi./ivmspw.. penalty parameter grid searches discussed sec. vii-d main document generate surfaces representing functional value ﬁxed number iterations plotted parameters sparse coding dictionary update components dictionary learning algorithm. surfaces corresponding coarse grids training images shown figs. fig. grid search surfaces conjugate gradient iterated sherman-morrison algorithms surface represents value cbpdn functional main document) iterations different parameters fig. grid search surfaces masked conjugate gradient masked iterated sherman-morrison masked consensus masked fista algorithms surface represents value masked cbpdn functional main document) iterations different parameters order estimate scaling properties algorithm parameters respect training size consider case training size changed replication data. removing complexities associated characteristics individual images simpliﬁed scenario allows analytic evaluation conditions equivalent problem obtained size changed. practice changing involves introducing different training images cannot expect scaling properties hold exactly represent best possible estimate depends properties training images themselves. fig. grid search surfaces spatial tiling consensus frequency domain consensus fista algorithms surface represents value cbpdn functional main document) iterations different parameters problem augmented lagrangian case twice augmented lagrangian case penalty parameter therefore expect optimal penalty parameter remain constant changing number training images problem augmented lagrangian case twice augmented lagrangian case penalty parameter therefore expect optimal penalty parameter remain constant changing number training images problem augmented lagrangian case twice augmented lagrangian case penalty parameter also twice penalty parameter used case. therefore expect optimal penalty parameter scale linearly changing number training images problem augmented lagrangian case terms twice augmented lagrangian case well term case. therefore simple rule scale optimal penalty parameter changing number training images however worth noting scaling relationship could obtained replacing constraint equivalent constraint appropriate rescaling scaled dual variable exhibits gradient step same requires gradient step parameter reduced factor compensate doubling gradient. therefore expect optimal parameter inverse gradient step size scale linearly changing number training images problem augmented lagrangian case twice augmented lagrangian case penalty parameter therefore expect optimal penalty parameter remain constant changing number training images respectively. combinations number size images chosen maintain constant number pixels training provides useful simultaneously exploring performance variations respect images derived images mirflickr-m dataset pre-processed described sec. vii-c main document. results using methods discussed analyzed main document computed using python implementation sporco library linux workstation equipped xeon cpus. problem augmented lagrangian case twice augmented lagrangian case penalty parameter therefore expect optimal penalty parameter remain constant changing number training images experiments determine median stability optimal parameters across ensemble training sets size discussed sec. vii-g main document. corresponding results plotted figs. plots represent median quartiles full range variation normalized functional values obtained parameter value different image subsets sizes lines connect medians distributions parameter value. seen figs. fista skewed sensitivity plots inverse gradient step size. related requirement mentioned main document greater equal lipschitz constant gradient functional guarantee convergence algorithm. although constant always computable experiments able estimate threshold indicates change behavior expected becomes greater lipschitz constant. variation normalized functional values comparable methods parameters values greater threshold. however values smaller threshold instability causes much larger variance normalized functional values. decided clip large vertical ranges resulting large variances left plots order clearly display scaling useful range result interquartile range boxes left incomplete lower part full range variation visible. order evaluate performance methods larger training sets images different sizes performed additional experiments including comparisons original implementations competing algorithms. used training sets images sizes pixels pixels pixels experiments learned dictionary ﬁlters size setting sparsity parameter parameters methods according scaling rules discussed sec. vii-g main document using ﬁxed penalty parameters without adaptation methods. contrast experiments reported main consisted additional images size pixels used purpose experiments reported main document. evaluation performed sparse coding images test computing evolution cbpdn functional series dictionaries. allows comparison generalization performance taking account possible differences overﬁtting effects different methods also allows fair comparison methods avoiding difﬁculty comparing training functional values computed differently different implementations. used default parameters demonstration scripts distributed publicly available matlab implementations authors efforts adjust default parameters implementations methods obtain better results unsuccessful least part slow convergence methods absence parameter selection discussion guidelines provided authors. training objective function shown fig. images fig. images fig. clear images. cns-p consistently achieves best performance converging smoothly slightly smaller functional value methods cases except fig. also exhibits fastest convergence methods compared. contrast fista results less stable presenting wild oscillations beginning small oscillations nevertheless achieving similar ﬁnal functional values cns-p. method papyan rapid convergence terms iterations time performance worst three methods. fista instability automatically corrected using backtracking step-size adaptation rule however uni-directional correction backtracking rule always increases evolution functional smooth also tends converge larger functional value. reasonable approach methods converge monotonically fista consider solution time step best solution obtained step opposed solution speciﬁcally step effect smoothing functional value evolution. experiments used ﬁxed value accordance parameter rules described main document report actual convergence without post processing since accurately illustrates real fista behavior tradeoff convergence smoothness ﬁnal functional value determined parameter testing results obtained additional images size displayed fig. images fig. images fig. images. note that comparisons main document time axis plots refers time dictionary learning code used generate relevant dictionary time sparse coding test set. testing plots show methods perform expected training comparison cns-p achieving better performance also test followed fista. results method papyan always worse match functional values achieved either cns-p fista. methods testing results better dictionary ﬁlters obtained training images followed dictionary ﬁlters obtained training images worst results obtained dictionary ﬁlters obtained training images particular cns-p functional increases near evolution fig. believe overﬁtting effects cases resulting mismatch training validation image sizes. additional experiments conﬁrmed functional decreases monotonically fig. dictionary learning spatial mask comparison images pixels decay value masked cbpdn functional respect time iterations masked versions algorithms. fig. dictionary learning spatial mask comparison images pixels decay value masked cbpdn functional respect time iterations masked versions algorithms. entries uniform random distribution. three different random masks generated images pixels pixels pixels. methods used randomly generated masks. corresponding results shown fig. images fig. images fig. images. resemble results obtained unmasked variants m-cns-p yielding fastest convergence smallest ﬁnal masked cbpdn functional values followed m-fista. m-fista still initially unstable cases convergence becomes much smoother unmasked variant learning. since m-cns-p m-fista converge similar functional value learning difﬁcult differences computation time plots m-cns-p almost faster m-fista. functional values masked method papyan inaccurate since mask taken account calculation. fair comparison however made evaluating cbpdn functional sparse coding test dictionary ﬁlters learned training. results shown fig. images fig. images fig. size images testing corresponds size images training set. nevertheless decided testing experiments corresponding functionals would comparable across different training sets. fig. time iteration cns-p fista decreases slowly increasing decreasing i.e. roughly linear number pixels training image set. since results fig. show algorithms scale linearly implies algorithms approximately linear scaling well. slight deviation linearity attributed complexity ffts used algorithms method papyan seems sensitive scaling time iteration increasing increases requires time iteration cns-p fista. fig. dictionary learning spatial mask comparison images pixels decay value masked cbpdn functional respect time iterations masked versions algorithms. product remains unchanged. difference time iteration unmasked masked variants larger m-fista m-cns-p. conversely time iteration unmasked masked variants decreases method papyan smaller larger increases slightly larger smaller behavior expected complexity analysis. finally worth noting that quantify optimality parameters selected guidelines discussed sec. vii-g main document appear provide good performance even substantially larger problems considered here used develop guidelines. contrast found parameter selection problematic methods proposed authors discussed sec. svii. section compare scaling respect number ﬁlters leading methods method papyan dictionaries ﬁlters size learned iterations training greyscale images described main document. time iteration three methods compared fig. shows three methods exhibit linear scaling number ﬁlters. experiments address issue ﬁlter size. performance dft-domain methods proposed roughly independent ﬁlter size spatial domain methods papyan become expensive ﬁlter size increases. addition multi-scale dictionaries easily supported dft-domain methods much difﬁcult support spatial domain methods. images. again note testing results case better methods methods overﬁtting effects cases although less signiﬁcant unmasked ones. also clear testing results m-cns-p m-fista much better masked method papyan compared performance methods learning dictionary ﬁlters size setting sparsity parameter parameters methods according scaling rules discussed sec. vii-g main document using ﬁxed penalty parameters without adaptation methods. relaxation methods used parameters competing methods default parameters included respective demonstration scripts. before additional images size pixels used test evaluate dictionaries learned. again report evolution cbpdn functional test provide meaningful comparison independent training functional evaluation implemented method slightly different expressions sometimes calculated un-normalized dictionaries. fig. dictionary learning comparison images pixels decay functional value training respect time iterations cns-p fista method papyan consensus method šorel šroubek. fig. dictionary learning comparison images pixels decay functional value training respect time iterations cns-p fista method papyan method heide al.. fig. dictionaries obtained training images pixels. direct outputs cns-p fista implementation method papyan produce dictionaries normalized implementation consensus method šorel šroubek produces dictionaries norms greater implementation method heide produces dictionaries norms smaller papyan fista initially exhibiting oscillatory behavior. since methods šorel šroubek heide perform multiple inner iterations sparse coding dictionary learning subproblems outer iteration iteration counts methods reported product inner outer iterations. method heide starts large functional value slow converge. consensus method šorel šroubek appears achieve signiﬁcantly lower functional values methods results comparable since dictionary ﬁlters properly normalized. ﬁnal dictionaries computed displayed fig. images internal admm iterations methods mostly linear scaling problem size variables exception image size scaling shared methods compute convolutions frequency domain. corresponding scaling spatial domain method papyan number samples ﬁlter kernel scaling image size frequency domain methods replaced linear scaling ﬁlter size. suggests frequency domain methods preferred images moderate size moderate large ﬁlter kernels spatial domain methods advantage large images small ﬁlter kernels. sparse coding results test shown fig. note cns-p fista produce smallest cbpdn functional values followed method papyan results methods šorel šroubek well heide much worse. since functional value evolution method heide highly oscillatory iteration plot best functional value obtained point instead functional value iteration. terms time evolution clear cns-p fastest converge followed fista method papyan al.. methods šorel šroubek heide slow even relatively small dataset. intended demonstrate multi-channel capability discussed sec. main document. provide results leading approaches proposed compare algorithms heide šorel šroubek papyan since none corresponding publicly available implementations support multi-channel cdl. color images used experiments derived images mirflickrm dataset pre-processed described sec. vii-c main document. parameters cns-p method using parameter selection rules single channel problem without additional tuning. rules also used parameters fista method rule multiplied stable convergence. per-iteration computational complexities methods including sparse coding convolutional dictionary learning subproblems summarized table complexity expressions methods papyan heide reproduced provided works expression provided šorel šroubek modiﬁed make explicit dependence number dictionary ﬁlters size channels learned color images size using sparsity parameter setting results experiment reported fig. comparing single-channel dictionary learning results dictionary size training image number images size reported fig. main document seen cns-p requires time compute greyscale result compared color result fista requires time compute greyscale result compared color result. additional cost learning color dictionary color images quite moderate considering three times training data used. similarly experiments saved dictionaries regular intervals training used additional color images size pixels source testing. compared methods sparse coding color images test computing evolution cbpdn functional series multi-channel dictionaries. fig. shows cnsp performs slightly better fista testing although cns-p convergence less smooth ﬁnal stages compared single-channel cases perhaps suboptimal parameter selection. evaluation multi-channel performance including parameter selection guidelines left future work.", "year": "2017"}