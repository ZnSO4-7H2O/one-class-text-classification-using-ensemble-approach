{"title": "Blind Channel Equalization using Variational Autoencoders", "tag": "eess", "abstract": " A new maximum likelihood estimation approach for blind channel equalization, using variational autoencoders (VAEs), is introduced. Significant and consistent improvements in the error rate of the reconstructed symbols, compared to constant modulus equalizers, are demonstrated. In fact, for the channels that were examined, the performance of the new VAE blind channel equalizer was close to the performance of a nonblind adaptive linear minimum mean square error equalizer. The new equalization method enables a significantly lower latency channel acquisition compared to the constant modulus algorithm (CMA). The VAE uses a convolutional neural network with two layers and a very small number of free parameters. Although the computational complexity of the new equalizer is higher compared to CMA, it is still reasonable, and the number of free parameters to estimate is small. ", "text": "proposed solutions expectation maximization algorithm approximated require iterative application forward-backward viterbi algorithms. complexities algorithms exponential channel memory size prohibitive. alternative paper propose approximated estimate using variational autoencoder method vaes widely used literature deep learning unsupervised semi-supervised learning generative model given observations data. demonstrate signiﬁcant consistent improvements quality detected symbols compared baseline blind equalization algorithms. fact channels examined performance blind channel equalizer close performance non-blind adaptive linear minimum mean square error equalizer equalization method enables lower latency acquisition unknown channel response. although computational complexity vaebce higher compared still reasonable number free parameters estimate small. communication channel modeled convolution input {xk} causal ﬁnite impulse response time invariant ﬁlter size followed addition white gaussian noise equalizer fig. reconstructs estimate transmitted symbol sequence {ˆxk}. suppose observe ﬁnite window measurements data clarity presentation assume input signal causal refer assumption later. abstract—a maximum likelihood estimation approach blind channel equalization using variational autoencoders introduced. signiﬁcant consistent improvements error rate reconstructed symbols compared constant modulus equalizers demonstrated. fact channels examined performance blind channel equalizer close performance nonblind adaptive linear minimum mean square error equalizer. equalization method enables signiﬁcantly lower latency channel acquisition compared constant modulus algorithm uses convolutional neural network layers small number free parameters. although computational complexity equalizer higher compared still reasonable number free parameters estimate small. index terms—variational autoencoders blind channel equalization deep learning convolutional neural networks constant modulus algorithm maximum likelihood. following amazing success deep learning methods various tasks techniques recently considered communication problems. example deep learning methods considered problem channel decoding authors proposed autoencoder communication system short block codes deep learning-based detection algorithms used channel model unknown. work considers transmission noisy intersymbol interference channel unknown impulse response. equalization methods channels using neural networks dealt extensively literature paper consider case input sequence also unknown blind channel equalization required. following blind equalization step apply decision directed equalization using blind equalization estimation initial value. blind channel equalization special type blind deconvolution input constrained known discrete constellation known statistics. standard approach tackle problem constant modulus algorithm blind neural networkbased algorithms using constant modulus criterion also proposed literature work propose approach blind channel equalization using maximum likelihood criterion. criterion already used blind channel equalization however transmitted message i.i.d. sequence additive white gaussian noise. throughout paper assume qpsk modulation although derivation extended constellations. hence vectors written combinations real imaginary components that +j·xq +j·hq element noise sequence complex gaussian statistically independent real complex components variance given statistically independent normally distributed. conditional density function conditional density function propose using estimation channel impulse response search vector chanw maximize noise variance estimate strong asymptotic optimality properties particular asymptotic efﬁciency criterion hand claim asymptotic consistency however applying accurate criterion problem difﬁcult since ﬁrst expressed multi-dimensional integral since assume uniformly distributed transmitted sequence. however kind problem shown various applications possible dramatically simplify estimation problem using variational approach estimation approach instead directly maximizing maximizes lower bound follows. shown parametrized conditional density function. instead directly maximizing maximizes lower bound fact shown searching possible conditional densities obtains estimate typically implemented neural networks. problem given encoder given following model decoder implementation decoder acts equalizer used fully convolutional network architecture convolutional layers output channels corresponding real imaginary parts convolution input output layers also separated channels corresponding real imaginary components input output probabilities convolutional layers dimensional residual connection non-linear activation function ﬁrst layer softsign function deﬁned x|x|+ which experiments proved converge faster functions leakyrelu tanh. non-linear activation function second layer sigmoid function ensures outputs represent valid probability values. decoder neural network depicted fig. compute term analytically. possible special structure problem since generator model linear. analytic computation cannot implemented vaes general. instead random variable model continuous reparameterization trick used discrete reparametrization trick cannot applied. recently approximations discrete proposed first deﬁnition have ﬁrst layer ﬁlter complex coefﬁcients second layer. hence total number free parameters model size estimated channel impulse response encoder addition real parameters decoder. generated qpsk random symbols training set. applied convolution channel impulse response added white gaussian noise signal noise ratio range deﬁned h/w). train model update step sampled training mini-batch single sub-sequence length figs. present results respectively. seen vaebce signiﬁcantly outperforms baseline blind equalizers quite close performance non-blind adaptive mmse. following experiment compared equalization algorithms number training symbols varied update step sampled training mini-batch single subsequence length used channel signal arbitrary time. therefore causality assumption hold. however edge effect decays increases. causality assumption equivalent zero-padding left convolution according results size alternatively assume h/). accordingly apply zero-padding left right. convolution size used second approach experiments although performance similar performance ﬁrst approach. experiments used mini-batch operation mode gradient descent parameters update step considered gradient loss using subsequence training data note loss function consists data entropy term wish maximize i.i.d assumption symbols autoencoder distortion term. also note method also provides estimated channel response part learning process. implemented blind equalizer using tensorﬂow framework provides automatic differentiation loss function. algorithm compared adaptive neural network blind equalization algorithms. also compared linear neural network clarity include results graphs since blind nncma outperformed linear neural network. addition compared performance adaptive mmse non-blind equalizer observes actual transmitted sequence. baseline algorithms single pass data training. order improve performance modiﬁed sufﬁciently many passes data. experiments used adam algorithm minimize loss function. complexity adam similar plain gradient descent. note experiments blind equalization methods recover transmitted bits unknown delay rotation constellation qpsk means need examine four different possible rotations results presented following experiments obtained averaging independent training data. training data used test data symbols calculate symbol error rate deﬁned impulse response above. fig. presents results snr=db. again vaebce algorithm signiﬁcantly outperforms baseline blind equalization algorithms. recall that accordance loss function part model training also learn estimated channel impulse response. assess robustness length estimated channel impulse response. denote actual estimated channel impulse responses respectively. first suppose length equal length general equalization small observed small distance ||h− ˆh||. distance monotonically decreasing snr. length smaller length model appeared learn nearly equal central part length larger length model appeared learn finally report number parameter updates required convergence vaebce algorithm. generated data described ﬁrst experiment. train model sampled mini-batch single subsequence length given training symbols. algorithm train convergence achieved. number iterations channel reported fig. either increase number required iterations decreases. channel number iterations similar. note using equalization algorithms viterbi forwardbackward algorithms would require trellis diagram farsad goldsmith detection algorithms communication systems using deep learning arxiv preprint arxiv. burse yadav shrivastava channel equalization using neural networks review ieee transactions systems cybernetics part applications reviews vol. o’shea pemula batra clancy radio transformer networks attention models learning synchronize wireless systems signals systems computers asilomar conference abadi agarwal barham brevdo chen citro corrado davis dean devin tensorﬂow large-scale machine learning heterogeneous distributed systems arxiv preprint arxiv. ranhotra kumar magarini mishra performance comparison blind non-blind channel equalizers using artiﬁcial neural networks international conference ubiquitous future networks introduced novel algorithm blind channel equalization using showed signiﬁcantly improved performance compared baseline cma-based blind channel equalization algorithms. particular vaebce required signiﬁcantly less training symbols measure. fact performance vaebce equalizer close performance supervised linear adaptive mmse equalizer. equalizer simple fcn. contrasted alternative blind equalization methods require trellis-based equalizer much costly implement. future research extend method generalized setups higher constellations nachmani be’ery burshtein learning decode linear codes using deep learning annual allerton conf. communication control computing september arxiv preprint arxiv.. nachmani marciano lugosch gross burshtein be’ery deep learning methods improved decoding linear codes ieee selected topics signal proc. special issue machine learning cognition radio communication radar accepted publication arxiv preprint arxiv..", "year": "2018"}