{"title": "Eventness: Object Detection on Spectrograms for Temporal Localization of  Audio Events", "tag": "eess", "abstract": " In this paper, we introduce the concept of Eventness for audio event detection, which can, in part, be thought of as an analogue to Objectness from computer vision. The key observation behind the eventness concept is that audio events reveal themselves as 2-dimensional time-frequency patterns with specific textures and geometric structures in spectrograms. These time-frequency patterns can then be viewed analogously to objects occurring in natural images (with the exception that scaling and rotation invariance properties do not apply). With this key observation in mind, we pose the problem of detecting monophonic or polyphonic audio events as an equivalent visual object(s) detection problem under partial occlusion and clutter in spectrograms. We adapt a state-of-the-art visual object detection model to evaluate the audio event detection task on publicly available datasets. The proposed network has comparable results with a state-of-the-art baseline and is more robust on minority events. Provided large-scale datasets, we hope that our proposed conceptual model of eventness will be beneficial to the audio signal processing community towards improving performance of audio event detection. ", "text": "audio event detection recently investigated similar computer vision perspective exploits effectiveness deep cnns order exploit vision inspired cnns audio event detection audio signal usually converted time-frequency representation spectrogram. using multiple convolutional layers multiple convolutional groups computer vision inspired cnns become state-of-the-art terms performance event detection classiﬁcation. paper borrow concept objectness propose similar analogue audio signals termed eventness. represented time-frequency domain audio events reveal patterns spectrograms event speciﬁc geometric structure. geometric structures provide information frequencies comprise audio event vary time. patterns spectrograms thought synonymously objects occurring natural images. therefore look leverage number components objectness-based deep learning harness temporal localization audio events spectrograms. proposed eventness model adapt state-ofthe-art object detection network namely faster r-cnn audio event detection. audio signals ﬁrst converted spectrograms linear intensity mapping used separate spectrogram distinct channels. pre-trained vision based used extract feature maps spectrograms faster r-cnn. noted feature maps produced pre-trained vision based detect objects natural images also employed similar fashion detect patterns present spectrograms. best knowledge ﬁrst time audio event detection temporal localization approached vision-inspired angle i.e. objectness audio spectrograms. compared based audio event detection models differs least aspects. first propose regions events directly instead inferpaper introduce concept eventness audio event detection part thought analogue objectness computer vision. observation behind eventness concept audio events reveal -dimensional time-frequency patterns speciﬁc textures geometric structures spectrograms. time-frequency patterns viewed analogously objects occurring natural images observation mind pose problem detecting monophonic polyphonic audio events equivalent visual object detection problem partial occlusion clutter spectrograms. adapt state-of-the-art visual object detection model evaluate audio event detection task publicly available datasets. proposed network comparable results state-of-theart baseline robust minority events. provided large-scale datasets hope proposed conceptual model eventness beneﬁcial audio signal processing community towards improving performance audio event detection. recently objectness-based deep learning networks using region proposals achieved state-of-the-art performance natural object detection tasks dramatic improvement object detection achieved feeding convolutional neural network class-agnostic objectness region proposals. later extend so-called multibox model integrated objectness region proposal component deep neural network improving performance. drawback methods however large number parameters utilized different features region proposals causing dramatic decrease processing speed. overcame decrease processing speed implementing so-called faster r-cnn shares feature objectness region proposal ring values based concatenation heuristics merging same-class neighbors performing sequence labeling using viterbi algorithms estimating distance current time onset offset event second previous models small temporal windows typically capture complete non-speech events therefore large temporals windows order seconds tens seconds. evaluate model publicly available datasets namely urbansoundk detection classiﬁcation acoustic scenes events qualitatively quantitatively. qualitative analysis examine ability proposed model exploit temporal spectral content region proposals even overlapping events. quantitative analysis compare proposed model state-of-the-art baseline showing comparable performance achieved along robustness infrequent events. audio signals ﬁrst segmented long time windows length converted log-scaled melspectrograms. resulting log-scaled mel-spectrograms normalized range linear intensity segment mapping used separate original spectrogram distinct channels. note mapping process used greyscale image higher dimensional space i.e. colormap shown outperform single channel inputs based audio event classiﬁers. mappings proposed based ﬁrstsecond-order differences log-scaled mel-spectrograms so-called delta deltadelta coefﬁcients however experiments previously described linear intensity segment mappings shown better performance. linear intensity segment mapping effectively quantizes original spectrogram based spectral intensities strong spectral values prominent channel weak spectral values prominent another. hypothesize subtle shapes introduced mapping better exploited convolutional layers network. resultant -channel spectrogram mapping pre-trained vgg- produce feature map. convolutional layers used pre-trained vgg- i.e. discard fully connected classiﬁcation layers. figure shows envisaged model audio signals ﬁrst converted log-scaled mel-spectrogams. feature created passing log-scaled melspectrogram pre-trained based vgg- model. feature faster r-cnn consisting main components event classiﬁer described section respectively. noted event classiﬁer resulting feature output pre-trained vgg- generate region proposals classify audio event. proposed model uses anchors generate multiple region proposals based output feature pre-trained vgg- cnn. particular location feature generate multiple region proposals different scales aspect ratios. intuition behind using anchor approach estimate complete object inferred even partial observation occlusion e.g. left image figure even though occluded pedestrians still generates full proposal entire bus. directly translated overlapping audio events time event classiﬁer employs region interest pooling layer extract normalize region proposals rpn. pooling layer performs similar operation conventional max-pooling layer except take inputs non-uniform sizes obtain ﬁxed-size feature maps. focusing region proposals provided classiﬁer ignore noisy areas feature map. besides classifying audio event component also reﬁnes region bounding box. allows accurate bounding derived larger view event instead location feature map. event classiﬁer uses ﬁnal softmax layer predict audio event. given audio large audio segments ﬁrst extracted. -band log-scaled melspectrograms generated audio segments window size length feature maps generated described section classiﬁer. used anchors location feature generate multiple eventness proposals. performance metrics used either segment based event based level segment-based metrics score error rate denoted ersb used compare predicted events ground truth labels segments second long. metrics applied event-based metrics score error rate denoted ereb compare amount overlap predicted event ground truth labels. deﬁnitions error rates follow perform qualitative analysis proposed model urbansoundk dataset consists audio clips comprised classes. select target classes good performance pilot experiment classes speciﬁcally horn bark gunshot siren jackhammer. randomly embed events classes background noise dcase dataset meant mimic multisource conditions. produced total training clips testing clips polyphonic. figure shows monophonic spectrograms i.e. siren horn shot jackhammer bark polyphonic spectrograms i.e. horn-dog bark shot-dog bark bark-siren barkdog bark figure also overlay region proposals areas surrounded green boxes ground truth labels areas surrounded boxes predicted labels. darker shade indicates higher conﬁdence level prediction. general region proposals able capture audio events temporal resolution. figure besides correctly identifying siren audio event eventness model surprisingly detected another audio event erroneously omitted ground truth labels. furthermore figure eventness model detects audio event corrupted noise erroneously omitted ground truth labels. different temporal lengths i.e. bark event shorter siren event differentiate partially overlapping events. furthermore proposed model also provides region proposals based spectral content noticed sizes along frequency axis i.e. vertical size bounding different unique event. figure audio event occurs overlap. remarkably eventness model extrapolates single event. direct result using larger temporal windows eventness model larger global view compared work using smaller temporal windows. quantitatively evaluate eventness model urbansoundk dataset described section dcase task dataset. dcase dataset pre-divided folds select ﬁrst fold testing synthesize audio clips remaining folds contain clips total). synthesized clips generated randomly assigning annotated events background noise dcase even select event synthesize selected event usually overlapped events natural recording environment. audio clips test kept unchanged. table shows performance proposed eventness model baseline model provided dcase utilizes layer neural network small temporal windows seen eventness model outperforms baseline event-based metrics segment-based metrics. eventness model focusing larger temporal segments capture entire event whereas baseline leverages smaller temporal segments better suited segment-based tasks. proposed concept eventness audio events detection utilizing vision inspired cnn. sharing similar characteristics vision based counterpart objectness natural object detection eventness leverage components natural object detection detect audio events present spectrograms. evaluated faster r-cnn adaptation audio data qualitative experiment quantitative experiment. results showed proposed eventness model detected audio events spectrogram images comparable baseline model. moreover eventness model robust classifying infrequency events. qualitative results also showed eventness model exploit temporal spectral content audio events. work paper proof-of-concept eventness model. future several modiﬁcations research directions explored. first feature maps generated vgg- pre-trained imagenet dataset meaning tailored natural image representations. recently audioset dataset released millions audio clips thereby allowing trained purely audio data removing natural image representations inherent feature maps. models generate feature maps also explored limited model. second current using locations feature generate proposals. however eventness model determine areas spectrogram highest importance classiﬁcation task. therefore propose attention based models automatically learn proposals. finally would like modify model true end-to-end representation learned i.e. audio ﬁles used input instead spectrograms input faster r-cnn. annamaria mesaros toni heittola aleksandr diment benjamin elizalde ankit shah emmanuel vincent bhiksha tuomas virtanen dcase challenge setup tasks datasets baseline system proc. detection classiﬁcation acoustic scenes events workshop annamaria mesaros toni heittola tuomas virtanen database acoustic scene classiﬁcation sound event detection proc. european signal processing conference ieee jort gemmeke daniel ellis dylan freedman jansen wade lawrence channing moore manoj plakal marvin ritter audio ontology human-labeled dataset audio events proc. ieee int. conf. acoustics speech signal process. ross girshick jeff donahue trevor darrell jitendra malik rich feature hierarchies accurate object detection semantic segmentation proc. ieee conf. computer vision pattern recognition shaoqing kaiming ross girshick jian faster r-cnn towards real-time object detecadvances tion region proposal networks neural information processing systems joseph redmon santosh divvala ross girshick farhadi look once uniﬁed real-time object detection proc. ieee conf. computer vision pattern recognition naoya takahashi michael gygli beat pﬁster gool deep convolutional neural networks data augmentation acoustic event detection proc. interspeech axel plinge rene grzeszick gernot fink bag-of-features approach acoustic event detection proc. ieee int. conf. acoustics speech signal process. ieee emre cakir toni heittola heikki huttunen tuomas virtanen polyphonic sound event detection using multi label deep neural networks proc. ieee int. joint conf. neural networks annamaria mesaros toni heittola antti eronen tuomas virtanen acoustic event detection real life recordings proc. european signal process. conf. jort gemmeke lode vuegen peter karsmakers bart vanrumste exemplar-based approach audio event detection proc. ieee workshop applications signal process. audio acoustics", "year": "2017"}