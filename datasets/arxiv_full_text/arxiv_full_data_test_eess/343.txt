{"title": "A Distributed Processing Architecture for Modular and Scalable Massive  MIMO Base Stations", "tag": "eess", "abstract": " In this work, a scalable and modular architecture for massive MIMO base stations with distributed processing is proposed. New antennas can readily be added by adding a new node as each node handles all the additional involved processing. The architecture supports conjugate beamforming, zero-forcing, and MMSE, where for the two latter cases a central matrix inversion is required. The impact of the time required for this matrix inversion is carefully analyzed along with a generic frame format. As part of the contribution, careful computational, memory, and communication analyses are presented. It is shown that all computations can be mapped to a single computational structure and that a processing node consisting of a single such processing element can handle a broad range of bandwidths and number of terminals. ", "text": "baseband processing distributed among modules connected array. modules contain rf-front ends digital baseband processing digital interconnection link four neighbors. system design along cost power consumption issues analyzed. however details baseband processing performed impact timing constraints. authors propose distributed processing based cots processors. however timing constraints considered lte-frame structure taken consideration. additionally systems dimensioned meet maximum obtainable throughputs considered speciﬁcations. distributed architecture number antennas base station easily scaled. centralized architecture increasing number antennas less requires complete redesign system. distributed architecture number antennas increased adding another node contains antenna circuitry associated processing. furthermore distributed architecture enables performing computations close antenna possibly integrated chip radio. case component failure modularity allows single node replaced instead replacing large centralized unit. additionally centralized implementations required data rate read uplink data adcs feed downlink data dacs grows number antennas making high systems many antennas. finally higher manufacturing yield expected since chip smaller distributed architecture. current work node system architecture proposed distributed modular scalable. supports conjugate beamforming zero forcing minimum mean-square error latter cases matrix inversion performed central unit. computational difference mmse mmse regularization term added performing matrix inverse. also done central unit. therefore discuss explicitly node processing mmse main contributions work abstract—in work scalable modular architecture massive mimo base stations distributed processing proposed. antennas readily added adding node node handles additional involved processing. architecture supports conjugate beamforming zero-forcing mmse latter cases central matrix inversion required. impact time required matrix inversion carefully analyzed along generic frame format. part contribution careful computational memory communication analyses presented. shown computations mapped single computational structure processing node consisting single processing element handle broad range bandwidths number terminals. wireless communication opens many opportunities challenges ﬁfth generation wireless infrastructure many antennas base station side commonly referred massive mimo large scale mimo using many antennas compared number terminals transmit receive power antenna processing associated antenna reduced. furthermore total energy user potentially reduced system level compared traditional antenna solutions. however massive mimo promising technology still obstacles overcome systems type deployed. exists number demonstrators used demonstrate feasibility techniques. number antennas demonstrators typically supports terminals. addition work done implement parts involved processing either using centralized node processing distributing processing several nodes centralized node architectures typically case antennas terminals considered. deterministic scheduling/control nodes system design space exploration showing proposed node architecture used rather large scenarios preliminary version current manuscript presented compared distributed architecture suggest using tree interconnection nodes although proposed approach also used interconnection topologies. especially perform analysis computational timing requirements propose detailed node architecture along scheduling computations inter-node communication. compared distributed architectures propose optimized node architecture instead using generic processors. additionally timing constraints selected frame format carefully analyzed. proposed system architecture consists central control unit scalable part illustrated fig. responsible performing operations error correction coding/decoding operations associated network layers medium access control scalable part responsible channel estimation linear precoding linear decoding symbols transmitted base station. every node scalable part contains computational blocks associated antenna inter-node communication links. nodes combined chip different granularity. main difference latency inter-node communication within chip clock cycles inter-chip range hundred clock cycles assuming serial link clock frequency hundreds mhz. assumed nodes clocked synchronously. downlink operation feeds modulated symbols terminal nodes. uplink operation nodes compute estimated symbols transmitted terminals sends ccu. work propose connect nodes k-ary tree however nodes connected topologies well. worth pointing that independently interconnection topology accumulation data nodes node transmit data node avoid duplicate transmissions accumulations. means accumulating data always performed tree structure independently interconnection topology. modifying interconnection topology number hops accumulating data changed. trees inherent advantages disadvantages compared array topologies. profound advantages tree structure number routing hops nhops grows logarithmically number nodes tree opposed proportionately square root arrays. figure shows different arrays tree topology. systems large number antennas major beneﬁt using mmse processing latency propagating data tree affects system design. processing latency important. another design trade-off needs considered fault tolerance. ordinary tree structure node fails operation entire branch able communicate rest tree. array topology could mitigated routing data past failing node. however increases node complexity since routing mechanism must implemented. additionally aspect physical antenna placement cable routing. systems antennas placed array array-based node topologies advantage simpler cable routing. systems antennas scattered irregular pattern advantage lost. pilot signals received node data necessary data estimate channels users without inter-node communication. done multiplying received pilot signals scalar local channel estimate vector yipilot downlink data transmission symbol vector precoded sent antennas cm×. done multiplication linear precoding matrix cm×k. considered algorithms precoding description number terminals ofdm dft/idft lenght ofdm subcarriers utilized uplink ofdm symbols pilot uplink ofdm symbols pilot downlink ofdm symbols number hops furthest node sample rate duration ofdm symbol duration frame time compute matrix inverse latency sending value link word length partial results frame starts uplink ofdm symbols terminals transmits data base station. comes uplink pilot symbol terminals transmit unique pilot sequence used estimate uplink radio channel. another uplink ofdm symbols sent pilot. comes guard interval switch uplink downlink operation. base station transmits ofdm symbols terminals. frame duration utilize proposed architecture efﬁciently algorithms used must expressed distributed manner. processing divided three phases channel estimation uplink data decoding downlink data precoding. node computes local contribution sums together contributions child nodes sending upwards ccu. performs matrix inversion redistributes results downwards tree. node receives inverted matrix computes local precoding/decoding vector. multiplying local sample local decoding column local contribution received symbol vector computed. local contributions calculated node sent upwards tree structure. contributions added together propagate ccu. since local contribution calculated using local sample column decoding matrix entire decoding matrix need available nodes. computation performed subcarrier node matrix gram matrix local channel estimate vector node computed locally without inter-node communication since required data obtained channel estimation. hermitian matrix thus entries must computed. computation performed node reduces computational complexity reduces amount data sent tree. instead propagating matrix values hermitian property values needs propagated. however computational load node increased since must computed node. matrix computed propagated downwards tree structure nodes. nodes calculate local detection precoding vectors multiplying inverted matrix local channel estimate vector. computation performed node process determining local precoding/decoding vector illustrated fig. leaf nodes computes local contribution gram matrix respectively sends parent node description channel estimation local contribution gram matrix local contribution contributions child nodes subcarriers precoded symbol subcarriers wi/ai local precoding value transmitted node inner product precoding matrix symbol vector similarly decoding case node requires precoding matrix perform precoding. thus entire matrix need distributed nodes. computation performed subcarrier node massive mimo ofdm system ofdm modulation demodulation performed antenna. therefore fft/ifft must performed node ofdm symbol length fft/ifft nfft number subcarriers utilized nsc. shown section viii processing element performs computations node enough support large range different combinations number terminals channel bandwidth. therefore beneﬁcial common structure involved computations discussed earlier. channel estimation requires multiplications shown fig. uplink decoding node performs multiplication adds data nodes tree binary tree shown fig. downlink precoding sum-of-products locally computed consists multiplication accumulation shown fig. finally ifft consists butterﬂy operations twiddle factor multiplications. considering operations figs. makes sense radix- decimation time algorithm. algorithm property butterﬂy operation twiddle factor multiplication front inputs shown fig. although exist many radix- algorithm radix- algorithm property every butterﬂy. note often believed corresponds bit-reversed input order normal output order. however case butterﬂy computation order cases multiple processing elements used processing element selection reconsidered. case might beneﬁcial different computational tasks different processing elements enabling specialized structures given task. similarly computational requirements antenna beneﬁcial interleave computations antenna single processing element. computed every frame. requires inverted matrix redistributed nodes decoding process start. another possibility compute node like conjugate beamforming case multiply inverted matrix results reaches ccu. distributed parts decoding could started independently matrix inversion. precoding matrix computed every frame. requires inverted matrix available node precoding start. multiplying complex conjugate inverted matrix symbol vectors sent nodes inverted matrix needed node precoding step. partitioning computations inverted matrix need redistributed nodes. however computational load signiﬁcantly increased. computational load node slightly reduced since precoding decoding still performed distributedly. difference computation need performed. computational complexity task shown table selected frame format major limitations computational resources. first since frame repeated cyclically computations frame must performed duration frame tframe. yields average number operations sample received nopavg. number operations needs performed obtain precoding/decoding vector ﬁrst term corresponds demodulating ofdm symbol second term estimating channels third term computing local contribution channel gram matrix fourth term multiplying inverted matrix local channel estimates ﬁrst term corresponds demodulating ofdm symbol second term computing local contribution received symbol vector. number operations required downlink ofdm symbol ﬁrst term corresponds performing precoding subcarrier utilized second term performing ofdm modulation. number operations performed uplink downlink ofdm symbols same total number uplink ofdm symbols number downlink ofdm symbols. without considering data dependencies critical paths theoretical lower bound number operations sample node must able perform. limitation downlink symbols must processed respective deadlines. practice critical paths schedule frame. figure shows computational tasks performed node critical paths frame important times possibility buffer process uplink ofdm symbols. critical paths computations receiving pilot symbol estimating channels computing local contribution gram matrix performing centralized matrix inversion computing local precoding/decoding vector ﬁnally performing precoding downlink ofdm symbol. number operations critical path downlink symbol nopcpi nopweights inopofdm ndl}. time available perform operations critical path downlink symbol receiving pilot symbol transmitting downlink symbol ofdm symbols including guard interval. however time local gram matrices propagated inverse computed result redistributed nodes total takes tinv nhopstlink computations critical paths performed. hence worst case average number computations sample critical paths computing dotted segments critical path times indicate computations critical path performed period. gray boxes illustrate uplink ofdm symbols must stored processing. system speciﬁcations kept number uplink downlink ofdm symbols increased average number operations sample entire frame increases well. guard intervals becoming less signiﬁcant increasing number ofdm symbols. number uplink downlink ofdm symbols large number operations sample seen matrix inversion time tinv total inter-node communication latency nhopstlink affect computational requirements. ﬁxed internode communication latency behavior displayed fig. inversion times marked fig. ﬁrst time tinva time critical path requires equally many operations sample frame average second time tinvb number operations critical path grows larger number operations sample asymptotic case. figure shows number operations sample varying tinv changes number ofdm symbols frame changed. fig. number ofdm symbols small. case signiﬁcant nopsavg nopsasymptotic tinva tinvb. number ofdm symbols increases gaps decreases shown fig. additionally seen fig. number ofdm symbols large time tinvb acts deadline matrix inversion. inverse received later tinvb required number operations cases nops integer. however ˆnops will hence slack time used increase number terminals number antennas and/or matrix inversion time tinv. tinv tinva slack time used process uplink symbols nulpb downlink symbols discussed below. alternatively pilot symbol moved closer downlink symbols i.e. decrease discussed section ii-a. section focuses analysis made processing. yields similar results signiﬁcant difference. precoding decoding vector obtained channel estimation means computational tasks central matrix inversion wi/ai performed. results dimensioning memories node part depend frame structure chosen part scheduling computations inter-node communication. fig. gray boxes illustrate uplink ofdm symbols must stored locally node processed. number symbols must stored number variables lifetime node seen fig. times ofdm symbol sampled antenna stored memory node. period number variables grows nfft. duration slightly shorter ofdm symbol cyclic preﬁx stored. time ofdm demodulation starts ﬁnished time computation made in-place meaning additional memory strictly required. however towards computation variables discarded subcarriers seen tinvb identical downlink symbols. number operations sample critical paths point tinvb. tinv tinvb critical path last downlink symbol always require highest number operations sample critical paths. similarly tinv tinvb critical path ﬁrst downlink symbol requires higher number operations sample. utilized. decoding starts time data expansion factor since subcarrier multiplied decoding vector. decoding ﬁnished knsc variables. number variables exist lifetime uplink symbol must stored. advantages distributing computations among multiple nodes number values needs sent centralized structure system grows proportionately number terminals rather number antennas system massive mimo systems clearly advantageous. tree structure frame corresponds local contributions gram matrix symbol vector estimates ˜yi. values used computations thus longer word length wcomp required. required upwards link datarate downwards propagation differs word length modulated symbols much shorter. downwards symbols propagated nodes using shorter word length wsymbol. however inverted matrix still needs represented wcomp. number bits sent downwards also noted word lengths data different. centralized architecture word length depends number bits proportional wadc. distributed architecture transmitted number bits proportional kwcomp. since sum-of-products product term sample value expect general wcomp wadc. however total number bits transmitted central unit still signiﬁcantly smaller. furthermore important intermediate values properly scaled terms added along path central unit. balancing computations communication memories obtain optimized architecture different types resources must balanced. here processing communication memory capabilities included. considering inter node communication uplink ofdm symbol number stored variables node seen fig. sampling radio ﬁnished number stored variables number existing variables fig. output data decoding process data dependencies current node. variables need sent parent node perform decoding process. fig. time time taken perform decoding. time time taken send local contributions decoded signal vectors parent node. noted decoding starts number variables needs stored locally increases data expansion decoding time decreases variables sent parent node thus needing stored. extreme cases behavior. ﬁrst tends inﬁnity. case variables must stored locally none sent link. clearly solution feasible. extreme means output variables sent directly parent node need stored locally. described earlier decoding parent node cannot performed decoding output sent link. implication system designed rate processing rate sending variables nodes same. channel estimation computing matrix. inverted matrix received precoding/decoding vector computed. stage uplink downlink symbols processed. order reduce number uplink symbols needs buffered processing nulpb uplink symbols processed. downlink symbols processed order meet deadlines. downlink symbols ﬁnished node computes uplink symbols processed. uplink symbols processed last downlink symbol transmitted guard interval. another uplink symbol computed pilot next frame sampled. remaining nulpb uplink symbols processed node waits inverted matrix next frame. seen processing fully deterministic asymptotic case hence simple control unit implemented different system parameters conﬁgured. non-asymptotic case general structure implemented. however processing possibly distributed differently within frame slightly ﬂexible control unit required. alternatively control signals stored acting instruction memory. computational tasks table inter-node data dependencies described section iii. local operation performed corresponding contributions child nodes must sent inter-node link. latency sending value link tlink. level tree computations must skewed amount order parent node receive data processing. seen later section viii many system scenarios covered single processing element node. hence focus here. inspection arithmetic operations fig. reveals input port processing element connected speciﬁc data. means types data need port instance twiddle factors channel estimates precoding/decoding vector connected input multiplier. taking account leads proposed node architecture shown fig. node architecture uses processing element shown fig. twiddle factor memory implemented since twiddle factors static. channel estimates precoding/decoding vector memories single port memories either written read cycle. although precoding decoding precoding/decoding vector required channel estimates must stored precoding/decoding values computed hence must stored. simplicity select separate nodes tree processed downwards rather forwarded next node. implication required send data rate processed. however prevent need large buffers node making desirable. feeding nodes data rather straightforward trade-off link data rate buffer size. computational tasks data dependencies using processing seen fig. schedule correctly scaled rather made illustrate data dependencies need better realization. e.g. clear time frame operations currently performed. therefore makes sense move parts computations obtain better utilization processing elements. come cost memory data must stored rather processed directly. here node processing element considered. processing element assumed support required number operations sample asymptotic case i.e. ˆnops nopsasymptotic. schedule created fig. initially node wait pilot ofdm symbol. computations determining precoding/decoding vector started. includes performing vii. example lte-like system specifications here requirements lte-like system using processing considered. typical speciﬁcation considered earlier work. system speciﬁcations seen table speciﬁcation throughput direction. centralized matrix inversion performed either using exact approximate algorithm. shown complexity similar best exact algorithm neumann series approximation three terms. cases matrix inversion performed less using processing element running mhz. memory allocation channel estimate instead using e.g. sample memory. sample memory complex divided three separate memories shown fig. ﬁrst memory radio input buffer stores data converter. size memory fig. memories shown two-port memories able read write simultaneously. many cases beneﬁcial single-port memories half size instead. input output buffers straightforward memories alternating reading writing. processing buffer also possible using e.g. approach case tinva tinvb therefore based selecting running fsample sufﬁcient leading ˆnops case second downlink ofdm symbol imposes limit number computational resources. hence nulpb nulbuffered schedule computations lte-like system derived seen fig. deriving schedule rather straightforward since tasks performed sequentially. slack time determining local precoding/decoding vector wi/ai start precoding utilized modify speciﬁcation discussed below. clock frequency required lte-like example problem achieve modern process technology through e.g. pipelining straightforward since execution deterministic. hence possible change bandwidth and/or number terminals. here consider three different clock frequencies system otherwise lte-like case. figure shows bounds bandwidth number terminals given clock frequency. fig. asymptotic case shown nops nopsasymptotic meaning number ofdm symbols frame large. fig. frame format lte-case used. cases average number computations entire frame used. thus assumed matrix inversion performed fast enough inﬂuence required number operations second i.e. tinv tinva. noted fig. increasing channel bandwidth factor roughly requires number simultaneous terminals reduced factor. here length nfft number subcarriers utilized scaled linearly bandwidth channel. usually case since length favorably selected power two. plots still give good estimate available design space. work scalable system architecture using distributed processing proposed base station massive mimo system. shown computations associated antenna distributed earlier studied cases simple single processing element running hundred modest amount memory required. shown feasible simple synchronous control nodes inter-node communication handled available slack time used modify speciﬁcations system. tweaking parameters redoing calculations investigate conﬁgurations supported ˆnops example matrix inversion time increased exactly node architecture. alternatively number users increased assuming matrix inverse time increases cubically. assumption nops selected case either running running used. alternatively example nhops increased leading maximum antennas assuming binary tree. even though increased increasing ˆnops pose limitation cases. want process uplink symbol ﬁrst downlink symbol i.e. nulpb must select ˆnops move pilot symbol symbol closer downlink symbols i.e. ˆnops although equality hold general. halving matrix inversion time leads ˆnops cases. illustrates critical paths limiting increasing computational capabilities i.e. decreasing skaran chen cavallaro goldstein studer decentralized beamforming massive mu-mimo cluster proc. ieee global conf. signal inform. process. ieee vector-perturbation technique near-capacity multiantenna multiuser inversion regularization ieee communication-part channel trans. commun. vol. jan. bertilsson gustafsson larsson scalable architecture massive mimo base stations using distributed processing proc. asilomar conf. systems computers wanhammar integrated circuits. academic press wanhammar hardware efﬁcient control memory addressing high-performance processors ieee trans. signal process. vol. mar. ingemarsson gustafsson hardware architecture positive deﬁnite matrix inversion based decomposition backsubstitution proc. asilomar conf. systems computers gustafsson bertilsson klasson ingemarsson approximate neumann series exact matrix inversion massive mimo? proc. ieee symp. comput. arithmetic invited paper. case connecting nodes binary tree primarily studied although architecture readily extended k-ary tree. worth noting array architecture static scheduling behave binary ternary tree hence concept used array interconnect additional simple routing logic surrounding processing node. processing core small also interest possibly node chip reducing amount inter-chip communication channels. exact granularity left future work. architecture supports conjugate beamforming zero forcing mmse processing. latter cases matrix inversion performed central control unit computations distributed. impact matrix inversion latency pilot position computational requirements node studied related. puglielli townley lacaille milovanovi´c trotskovsky whitcombe narevsky wright courtade alon nikoli´c niknejad design energycost-efﬁcient massive mimo arrays proc. ieee vol. mar. prabhu rodriguez edfors pj/b mb/s massive mimo precoder-decoder fd-soi proc. ieee solid-state circuit conf.", "year": "2018"}