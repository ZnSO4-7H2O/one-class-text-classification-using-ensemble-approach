{"title": "Online Convolutional Dictionary Learning", "tag": "eess", "abstract": " While a number of different algorithms have recently been proposed for convolutional dictionary learning, this remains an expensive problem. The single biggest impediment to learning from large training sets is the memory requirements, which grow at least linearly with the size of the training set since all existing methods are batch algorithms. The work reported here addresses this limitation by extending online dictionary learning ideas to the convolutional context. ", "text": "recent approaches solving batch methods alternating minimization {xkm} {dm} minimization subproblem approximated performing iterations admm large update subproblem computationally expensive since depends maps size each thus preventing large training set. purpose present work develop online dictionary learning methods training data sets much larger presently feasible. number different algorithms recently proposed convolutional dictionary learning remains expensive problem. single biggest impediment learning large training sets memory requirements grow least linearly size training since existing methods batch algorithms. work reported addresses limitation extending online dictionary learning ideas convolutional context. sparse representations dictionary learning become ubiquitous techniques signal image processing computer vision machine learning dictionary learning algorithms batch methods require access training data start training data size limited amount available memory. online methods contrast designed operate small subsets training data time making possible process large training data limited memory. methods continuously aggregate past training data updating current learned dictionary incorporate sparse codes obtained training data. updates depend accumulating sparse codes computed training require accessing previous sparse codes. methods therefore constant memory computation cost linear total training data size. consider linear representation signal size represent dictionary representation. convolutional representation impledm dictionary ﬁlters representation {xm}m coefﬁcient maps having size signal given {dm} sparse convolutional representation obtained solving convolutional basis pursuit denoising ιcpn indicator function constraint ﬁlter support normalisation however approach overﬁt never converge dictionary represents features entire training sequence. better approach inspired introduces surrogate function current time dictionary result accumulation past coefﬁcient maps computed then-available dictionaries. balance accumulated past contributions information provided training samples compute weighted combination contributions routinely done online schemes combination considers strongly recent updates since result extensively trained dictionary. popular approach solving quadratic minimization problems like fast iterative shrinkage-thresholding computes gradient step. according notation section loss function written thus gradient surrogate function computed signiﬁcant improvement take advantage convolutional property linear operator convolution implemented frequency domain using fast fourier transform inspired frequency domain fista variant cbpdn problem propose frequency domain fista solve described algorithm loss function frequency domain form regulated forgetting exponent reasonable choice since increases factor increases reﬂecting increasing accuracy past information training progresses. large forgetting factor expected lead stable algorithm since training signals given nearly equal weights information accumulated however also leads slow convergence. extreme case recovers small forgetting factor conversely leads faster convergence since gives past less accurate information lower weights. factor small surrogate function overwhelmingly inﬂuenced current training signal causing convergence unstable. experiments sample image obtain small regions algorithm called. experiments reported circular boundary conditions rather careful boundary handling would necessary smaller regions. call approach online-samp.. accumulation weighted forgetting factor derived modiﬁed surrogate function yield expression main algorithm given algorithm dupdate step calls frequency domain fista described algorithm direct extension online approach regular dictionary learning leads matrices size prohibitive except small however since frequency-domain product non-zero values structure exploited obtain corresponding reduction storage requirements experiments conducted using matlab running workstation intel xeon cpus clocked .ghz. dictionary size training testing image size dictionaries evaluated comparing functional values obtained computing cbpdn test set. training consists images test consists separate images. four training images standard images remainder training images testing images cropped rescaled images variety scenes obtained flickr. efﬁcient algorithm requires good choice forgetting factor paper used forgetting factor evolution deﬁned crucial exponent parameter tune discussed section experiments reported compare convergence resulting difference choices full training images. algorithm faster increases increases continuously algorithm gets unstable thus reasonable choice. section online convolutional dictionary learning training images parameter compared batch learning training sizes result shown according ﬁgure online convolutional dictionary learning best seconds. batch learning small training leads inaccurate results large training leads large computational cost. online learning handles image time accumulates previous information compact making efﬁcient large training set. avoid high memory usage technique proposed section consider sample region size found experimentally good choice size shown fig. online-samp. scheme stable online scheme without sampling still shows good performance enough iterations requires substantially less memory shown table proposed ﬁrst online convolutional dictionary learning algorithms capable learning training image arbitrary size. approaches based extension ideas online dictionary learning standard sparse representations. ﬁrst processes entire training image time; memory cost vastly better cost would correspond direct extension prior methods standard sparse representations still high large. second approach reduces memory usage sampling regions training image expense somewhat worse convergence behaviour. fig. comparison convergence behaviour online dictionary learning algorithm different forgetting exponents note functional value evaluated testing training set. wohlberg convolutional sparse representation color images proc. ieee southwest symposium image analysis interpretation santa mar. doi./ssiai.. aharon elad bruckstein k-svd algorithm designing overcomplete dictionaries sparse representation ieee transactions signal processing vol. doi./tsp.. naderahmadian tinati beheshti generalized adaptive weighted recursive least squares dictionary learning signal processing vol. doi./j.sigpro...", "year": "2017"}