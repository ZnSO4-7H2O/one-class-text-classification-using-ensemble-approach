{"title": "Polyphonic Music Generation with Sequence Generative Adversarial  Networks", "tag": "eess", "abstract": " We propose an application of sequence generative adversarial networks (SeqGAN), which are generative adversarial networks for discrete sequence generation, for creating polyphonic musical sequences. Instead of a monophonic melody generation suggested in the original work, we present an efficient representation of a polyphony MIDI file that simultaneously captures chords and melodies with dynamic timings. The proposed method condenses duration, octaves, and keys of both melodies and chords into a single word vector representation, and recurrent neural networks learn to predict distributions of sequences from the embedded musical word space. We experiment with the original method and the least squares method to the discriminator, which is known to stabilize the training of GANs. The network can create sequences that are musically coherent and shows an improved quantitative and qualitative measures. We also report that careful optimization of reinforcement learning signals of the model is crucial for general application of the model. ", "text": "propose application sequence generative adversarial networks generative adversarial networks discrete sequence generation creating polyphonic musical sequences. instead monophonic melody generation suggested original work present efﬁcient representation polyphony midi simultaneously captures chords melodies dynamic timings. proposed method condenses duration octaves keys melodies chords single word vector representation recurrent neural networks learn predict distributions sequences embedded musical word space. experiment original method least squares method discriminator known stabilize training gans. network create sequences musically coherent shows improved quantitative qualitative measures. also report careful optimization reinforcement learning signals model crucial general application model. automatic music generation concept creation continuous audio signal discrete symbolic sequence represents musical structure computational models autonomous continuous audio signal includes waveform spectrogram data structure. discrete symbolic sequence includes midi piano roll. paper focus polyphonic music generation midi system creates chords melodies simultaneously. recent advancements deep learning brought wide range applications image speech recognition machine translation bioinformatics also getting attention music generation various approaches specially recurrent neural networks widely used music language modeling since process time series information central role musical structure. generative adversarial networks frameworks deep learning achieving state-of-the-art performance generative tasks. however gans difﬁcult train discrete sequences continuous data results limited applications domains discrete data. sequence generative adversarial networks ﬁrst models overcome limitation combining reinforcement learning gans learning discrete sequence data. seqgan model consists rnns sequence generator convolutional neural networks discriminator identiﬁes whether given sequence real fake. seqgan successfully learns artiﬁcial real-world discrete data used language modeling monophonic music generation. results original work shown strong potential application seqgans automatic music generation. however original work shown rather simple approaches melody generation using melody part midi music constraining available words model -key pitches. contrast polyphonic music generation system compose chords melodies simultaneously appealing greatly improve realism computer-generated music. consideration leads question represent language symbolic music model effectively leverage. would like design word representation polyphonic symbolic music minimal hand-designed preprocessing would negatively impact representational power. addition would like model fully incorporate structure data distribution polyphonic music including chords keys dynamic timings. based pioneering work apply seqgan purpose polyphonic music generation. speciﬁcally propose simple efﬁcient word token formulation polyphonic midi sequence learned seqgan. representation capture multiple keys durations midi music sequence word embedding. since integrated duration notes word representations recurrent networks learn sequences dynamic timings. proposed method condenses duration octaves keys melodies chords single word vector representation recurrent neural networks learn predict distributions sequences embedded musical word space. sampled sequences trained networks show long-term structures musically coherent show improved quantitative measure bleu score perceptive quality mean opinion score adversarial training. discuss advantages limitations approach furl approaches addition seqgan. using task advantage ability utilize well-deﬁned music theories calculate reward signals leveraged model compared end-to-end training approaches advantage allowing guide network prior knowledge music steering model user preferred musical styles. interaction composer generator important factors music generation task. therefore various conditional mechanisms music generation developed midinet model generates monophonic note sequence conditioned primer melody chord sequence. however symbolic representation music able distinguish single long note multiple repeating notes work. midinet generate polyphonic music priming given chord condition. work instead explores unconditioned polyphonic music generation distilling necessary information word embedding space letting model learn embedded space. note conditional generation also possible method priming pre-deﬁned word sequences unconditional generation. c-rnn-gan uses rnns sequence generator incorporates gans framework parallel work. however uses real-valued feature representation midi modeling tone length frequency intensity time four real-valued scalars. rnns trained real-valued feature space challenge training gans discrete data discussed above. work based framework natively handle discrete sequence gans. efﬁcient representation musical data crucial ability model learn musical structure. notable examples include performance emphasizes training dataset musical representation interesting elements deep learning-based music generation. performance uses midi representation handles expressive timing dynamics considered compressed version ﬁxed time step. midi music dataset used nottingham database collection british american folk tunes. note original work also used dataset used monophonic melody part ﬁxed time steps training evaluation. extend representation dataset polyphonic sequences. used music python package preprocessing midi data input sequence postprocessing output sequence back midi depicted figure midi nottingham dataset consists parts melody chords. midi refer comprehensive survey deep learningbased music generation. rnns widely used task sequence generation designed processing time-series sequences. primarily used language modeling rnns also applied music generation based discrete sequences notably midi piano rolls. long short-term memory variant module rnns incorporates contextual memory cells gates information learn forget alleviates long-term dependency problem rnns recent models rnns typically lstm building block. based success lstm handle longterm dependency studies music generation using lstm. however problem called exposure bias discrete sequence generation using lstm model trained maximum likelihood method. case out-of-sample discrete sequence training discrepancy training inference occurs sampled output previous time step used input current time step. seqgan addresses problem considering sequence generation problem sequential decisionmaking process reinforcement learning further calculate reward signals time step seqgan incorporates gans discriminator cnns provide scores identify whether given sequence real fake. pretrained negative log-likelihood loss generator rnns trained policy gradient method signals. speciﬁcally generator uses average discriminator outputs sequences generated monte carlo search rollout policy estimated reward. rollout policy current generator. generator updated following equations policy parameterized generator action value function sequence following policy actual implementation replaced output discriminator mentioned above. denotes sequence generator token time step parameters generator updated gradient ascent method. parameters discriminator trained loss. dataset loaded note parsed list containing start time duration octave pitch velocity. chords assigned different indices different sets pitches. example different indices pitch list. incorporated approximately pitch sets pitch list. statistics pitch sets shown table experiments omitted velocity reasons reduce vocabulary size tractable amount incorporation velocity would scatter word distribution severely would yield good estimation results given amount data points nottingham dataset. tokenization done scattering every possible combinations musical information separate words. duration octave note pitch note octave chord pitch chord time step combined single integer. including durations preprocessing pipeline able tokenize time step different lengths. notes whose lengths different corresponding chords inserted dummy notes length note chord sequence would same. rest dummy notes designated special ‘rest’ token. excluded music tokens occurred less times total dataset keep size vocabulary tractable. tokenized integer sequences used inputs seqgan. based generated output sequence tokens seqgan model postprocessing performed convert sequences midi ﬁles. loading constructed vocabulary token sequence token sequence converted musical symbols note chord reverse process preprocessing. symbols appended melody stream chord stream. processing tokens streams combined midi ﬁle. unlike models ﬁxed time steps introduced related work preprocessing method distinguish case single note played long time case single note played multiple times. method represented variable duration single word token processed recurrent networks. dynamic timing representation also beneﬁt generative model rnns learn time-dependent structure musical sequence beyond ﬁxed time steps. proposed preprocessing method designed minimal human-designed reformulation possible since wanted model fully observe underlying data distribution polyphonic symbolic midi data model could leverage learning. however method also drawback tokenizing naive hashing-like approach. naive hashing make vocabulary space expand necessary. difﬁcult learn chords octave appear times dataset even chords octaves abundant dataset. example tonic triad different octaves actually related vocabulary maps different tokens. describe core details seqgan model modiﬁcations stabilized training model customized polyphonic midi dataset. seqgan generator rnns discriminator cnns pretrained regular negative log-likelihood loss tuned adversarial training policy gradient outputs discriminator cnns ranging reward signals. followed training scheme original work. experienced instabilities adversarial training hyperparameters original work. instability persisted original sequence length setting customized setting main obstacle came discriminator vastly outperforming generator. even pretraining generator achieve saturated performance generator failed fool thus lowered representational power discriminator reducing number convolutional layers also increased receptive ﬁeld convolution ﬁlters since wanted discriminator capture periodic structure musical sequences effectively. note large receptive ﬁeld approach shown effective related work handles waveform audio furthermore found hyperparameters policy gradients needed careful optimization. used monte carlo search rollouts calculating rewards policy gradient ensure lower variance reward signals. prevented generator learning unnecessary noise would lead divergence critically impact performance model. adjusted reward discount factor compensate longer sequence length also applied conservative target generator network update rate observed higher update rate stabilized adversarial training reward signals constrained divergence generator. instead feeding mixture minibatch containing real fake samples discriminator original work used minibatch discrimination technique minibatches contained real fake samples. technique used several works gans empirically improved adversarial training model. trained seqgan hyperparameter optimization resulted larger version original model. polyphonic word representation midi vocabulary size embedded word randomly initialized -dimensional vectors. created sequences length training. length also applies sequence generation trained model. generator rnns lstm cells. discriminator cnns convolutional layers feature maps receptive ﬁeld pretrained generator rnns discriminator cnns epochs regular negative loglikelihood loss. tendency discriminator dominate ﬁrst pretrained generator discriminator learning rates learning rate generator higher used batch size experiments. compared strategies unconditional method sampled sequences always started predeﬁned zero token conditional method trained model generated sequences trained model ﬁrst word real sequence start token. strategy additionally compared formulations loss discriminator original softmax reward cross entropy loss sigmoid reward least squares loss known stabilize training gans generator followed policy gradient method given scalar reward time step. generated sequences showed musically coherent structure long-term harmonics. measured results quantitative perceptive qualitative perspectives. quantitative analysis calculated bleu score measures similarity validation generated samples largely used evaluate quality machine translation speciﬁc bleu score calculated comparing entire corpus validation sequence generated model. higher bleu score means samples generator follow underlying real data distribution closely. conditional method used start token randomly sampled batch traintable performance comparison bleu- scores validation set. seqgan original softmax output discriminator cross entropy loss. lsseqgan sigmoid output discriminator least squares loss. table mean opinion score results. uniform random sample generated uniform random probability time step vocabulary. adversarial samples adversarial training progressively increasing bleu- score. mode collapse sample failure case adversarial training bleu- score table showed bleu score generator saturated pretraining improved adversarial training. generator trained loss showed peak performance bleu score reached approximately adversarial training could generally improve score best conﬁguration bleu score note improvements similar magnitude reported original paper. however could reproduce results original network conﬁgurations instant divergence generator. unconditional method performed relatively better conditional method especially adversarial training phase. possible explanation unconditional method estimate manifolds embedded space better ﬁxed zero start token model observe many trajectories real data manifold single starting point compared smaller number trajectories many starting points pretraining phase conditional method. impacts potential beneﬁts unsupervised adversarial training reinforcement learning signals model pretrained conditional method tends fall local minimum higher probability model pretrained unconducted qualitative analysis human perceptive performance generated midi sequences using user study. experiment asked participants rate seven different sequences responding three questions pleasant song? realistic sequence? interesting song? questions constructed given inspiration midinet. seven sequences included sample real dataset sequence sampled uniform random probability time step vocabulary sample failure case adversarial training bleu score remove bias notiﬁed participants seven sequences generated model. table showed sequences adversarial training sounded like real ones sequences pretrained model consistent quantitative analysis. samples model pretrained sounded relatively repetitive focused shortterm harmonics. expected since pretraining phase targets next token real training dataset. samples adversarial training tended show longer harmonics consistent chord progressions possibly since model successfully explored policies received high reward keeping chord progression. although experiments showed adversarial training boosted performance music language modeling drawbacks nature gans. firstly gans often suffer mode collapsing problem generator fools discriminator creating artifacts rather realistic samples also noticed problem generated samples played note constantly broke musical coherence. phenomenon also observed decrease bleu score implies divergence pretrained model. recent works gans introduce earth-mover distance loss function overcome issue thus incorporating idea discrete gans could alleviate problem recent improvements original work based rank-based loss directly applicable task. secondly training gans computationally efﬁcient training generator rnns. example stabilized hyperparameters gans require roughly times computing time training epoch relatively small improvement performance. computational cost also scales number monte carlo policy rollouts gives trade-off accuracy variance. thirdly policy gradient method monte carlo rollout highly stochastic. although adversarial training provide extra performance improvement method cannot reinforcement learning signal showed high variance relatively reproducibility. means even hyperparameter settings would need multiple training trials achieve improvements adversarial training. leaves room improvements minimizing variance reinforcement learning signals notably monte carlo tree search experience replay examples. restriction vocabulary pre-deﬁned words observed dataset limitation model cannot create chords melodies outside dataset. terms creativity model would compose novel music outside boundaries learned data harder train unconstrained models capable processing arbitrary polyphonic input output crucial creativity. mentioned related work observed reinforcement learning using reward signals direct inject prior knowledge musical structure model. suggests could leverage reinforcement learning signals incorporating critic model evaluates musical consonance based music theory. indeed rl-tuner deep q-networks based model uses scores music theory rules auxiliary reward signals plan implement idea future work. albeit proposed word embedding method polyphonic midi data simple efﬁcient word embedding random projection effectively capture relative harmony consonance word. modular networks consider relative information midi data could improve performance music language model. cnns viable choice purpose plan cnn-rnn hybrid model future work. objective structured experiments automatic music generation need robust quantitative measures evaluate perceptive quality machinegenerated music experiments quantitative bleu score analysis consistent qualitative user study certain degree exactly reﬂect perceptive performance. development structured quantitative metric would improve objectivity reproducibility research automatic music generation. dario amodei sundaram ananthanarayanan rishita anubhai jingliang eric battenberg carl case jared casper bryan catanzaro qiang cheng guoliang chen deep speech end-to-end speech recognition english mandarin. international conference machine learning pages cameron browne edward powley daniel whitehouse simon lucas peter cowling philipp rohlfshagen stephen tavener diego perez spyridon samothrakis simon colton. survey monte carlo tree search methods. ieee transactions computational intelligence games kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. kratarth goel raunaq vohra sahoo. polyphonic music generation modeling temporal dependencies using rnn-dbn. international conference artiﬁcial neural networks pages springer goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems pages kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition pages daniel johnson. generating polyphonic music using tied parallel networks. international conference evolutionary biologically inspired music pages springer kevin dianqi xiaodong zhengyou zhang ming-ting sun. adversarial ranking language generation. advances neural information processing systems pages xudong qing haoran raymond zhen wang stephen paul smolley. least squares generative adversarial networks. ieee international conference computer vision pages ieee volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature kishore papineni salim roukos todd ward weijing zhu. bleu method automatic evaluation machine translation. proceedings annual meeting association computational linguistics pages association computational linguistics richard sutton david mcallester satinder singh yishay mansour. policy gradient methods reinforcement learning function approximation. advances neural information processing systems pages li-chia yang szu-yu chou yi-hsuan yang. midinet convolutional generative adversarial network symbolic-domain music generation. proceedings international society music information retrieval conference suzhou china", "year": "2017"}