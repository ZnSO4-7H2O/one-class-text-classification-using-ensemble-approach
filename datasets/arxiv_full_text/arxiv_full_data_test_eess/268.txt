{"title": "Semi-automated Annotation of Signal Events in Clinical EEG Data", "tag": "eess", "abstract": " To be effective, state of the art machine learning technology needs large amounts of annotated data. There are numerous compelling applications in healthcare that can benefit from high performance automated decision support systems provided by deep learning technology, but they lack the comprehensive data resources required to apply sophisticated machine learning models. Further, for economic reasons, it is very difficult to justify the creation of large annotated corpora for these applications. Hence, automated annotation techniques become increasingly important. In this study, we investigated the effectiveness of using an active learning algorithm to automatically annotate a large EEG corpus. The algorithm is designed to annotate six types of EEG events. Two model training schemes, namely threshold-based and volume-based, are evaluated. In the threshold-based scheme the threshold of confidence scores is optimized in the initial training iteration, whereas for the volume-based scheme only a certain amount of data is preserved after each iteration. Recognition performance is improved 2% absolute and the system is capable of automatically annotating previously unlabeled data. Given that the interpretation of clinical EEG data is an exceedingly difficult task, this study provides some evidence that the proposed method is a viable alternative to expensive manual annotation. ", "text": "abstract— effective state machine learning technology needs large amounts annotated data. numerous compelling applications healthcare benefit high performance automated decision support systems provided deep learning technology lack comprehensive data resources required apply sophisticated machine learning models. further economic reasons difficult justify creation large annotated corpora applications. hence automated annotation techniques become increasingly important. study investigated effectiveness using active learning algorithm automatically annotate large corpus. algorithm designed annotate types events. model training schemes namely threshold-based volume-based evaluated. threshold-based scheme threshold confidence scores optimized initial training iteration whereas volume-based scheme certain amount data preserved iteration. recognition performance improved absolute system capable automatically annotating previously unlabeled data. given interpretation clinical data exceedingly difficult task study provides evidence proposed method viable alternative expensive manual annotation. electroencephalogram nonintrusive clinical tool widely used neurologists clinicians diagnose brain-related illnesses epilepsy seizures however manual annotation clinical data time-consuming expensive requiring well-trained board-certified neurologists short supply. interpretation signals quite subtle challenging. example inter-rater agreement identification electrographic seizures kappa statistic periodic discharges kappa statistic subtle events spikes sharp waves onset non-convulsive seizures degree inter-rater agreement even lower. further virtually impossible generate vast amount data needed train advanced deep learning systems manual annotation. systems often show effectiveness large amount data. therefore automatic annotation process highly desirable cost-effective generate large amounts data. would allow niche problems bioengineering field addressed powerful machine learning technology. fortunately machine learning techniques make possible automatically annotate data. semi-supervised learning methods self-training active learning attractive scenario. approaches require small amount transcribed data iteratively re-train models improved classification. however compared selftraining algorithm conventional active learning still requires constant cooperation expert interactively annotate informative data. self-training several semi-supervised approaches active learning selects confident features automatic model re-training. study developed self-training approach iteratively annotate large corpus. figure shows generic flow diagram self-training process. self-training employed successfully applications face recognition word classification object detection gait recognition applied self-training eeg-based brain computer interface spelling system. baseline system trained support vector machine panicker reported recognition system using two-classifier cotraining approach improve classification visual evoked potentials. studies claimed substantial recognition improvements. however clinical applications large amounts data exists positive result reported literature. methodology -second epochs processed standard feature extraction process described detail baseline system also described designed identify classes events spike and/or sharp waves periodic lateralized epileptiform discharges generalized periodic epileptiform discharges artifacts movements background first three events associated signal events clinical interest. last three events associated background channel simply used improve classification performance. work self-training employed automatically annotate unlabeled data temple university hospital corpus world’s largest publicly available database clinical data comprising records patients. though reports data signal data fully annotated. order make data learning particularly signal event classification need annotated data. self-training becomes attractive option. self-training usually employs small amount labeled data train classifier uses trained classifier predict unlabeled data. effectiveness self-training data mixed example panicker found addition unlabeled/auto-labelled data improve system performance certain limit addition unlabeled data degrade performance degradations spotted analysis data five subjects participated experiments. effect also reported cohen using different datasets semisupervised learning human-computer interactions. experience results extend large amounts clinical data described here hence motivation work. proposed annotation system begins small amount manually annotated data. used bootstrapping process create seed data. initially expert transcribe small amount data three signal classes remaining classes relatively easy identify able train team undergraduates annotate classes. trained background model large portion remaining data since vast majority data background. built pilot models used locate events interest. events manually reviewed. iterated process decent baseline models. performance baseline system six-way classification. model trained manually labelled data patients contained hours pled hours gped minutes spsw hours bckg hours artf segments minutes eyem. baseline system stabilized attempted models self-training method label remaining data. control flow process shown figure described follows proposed algorithm selects best candidates given class re-training original models. original hidden markov model training contained eight mixtures five states long quality selected features decent additional iterations retraining process expect recognition performance continuously increase. course optimal identify patterns force system improve semi-supervised learning approaches. proposed self-training method need human intervention expert iterations. separates approach many published results obtained conventional active learning. algorithm selects highly-ranked events adds existing training re-training round. since truth markings data used event-specific likelihood threshold determine suitable patterns added training set. worth mentioning confidence scores measured experiments normalized probability densities derived standard forward-backward algorithm classification performance evaluated manually-labeled evaluation determine performance improving performance and/or converging. models used different thresholds adjusted automatically iteration. clinical data significantly challenging data collected controlled conditions many artifacts channel variations. artifacts pose serious problems semi-supervised classification schemes used active learning. example movement artifact often confused spsw event making promising misleading candidate active learning. self-training algorithms tend sensitive outliers lead entire cluster converging wrong label following section present pilot results approaches eeg. series experiments conducted investigate using proposed algorithm automatically annotate eeg. since spsw event fairly rare difficult annotate type event expect benefit significantly process. therefore pilot studies used data class analyze optimize parameters. effectiveness re-training class investigated effectiveness self-training first iteration classes separately. class half events original training selected re-training. example original training contained gped epochs first re-training decoding process top-ranked gped epochs selected augment training set. main purpose investigation analyze recognition performance class self-training applied. table shows sensitivity performance classes. sensitivity improved absolute spsw class first round re-training. sensitivity gped pled classes also improved around absolute respectively. however self-training algorithm seems less effective eyem. further bckg artf classes also suffered small degradation performance. since background events need active learning classes critical since large amount background data available corpus. detailed exploration volume data preserved retraining presented next section. number preseved events re-training mentioned section step self-training process select decoded epochs high confidence. pilot study investigated impact different rankings epochs. focused spsw class parameter analysis. figure shows trend recognition performance reduce number included spsw events re-training. analysis controlled amount highly ranked epochs reducing preserved features re-training increasing recognition performance observed. began augmenting training decoded features. inclusion thresholds recognition performance increased. given original training contained spsw epochs added roughly half top-ranked newly predicted spsw epochs training model able achieve sensitivity purpose analysis find effective scheme adding decoded events original training re-training iterations. also investigated number selected epochs parameter iteration given threshold. analysis unlabeled files employed speed analysis. figure shows increase number selected spsw epochs function re-training iteration. first iteration events highest confidence scores added original training set. second iteration events added. observe increased number iterations many spsw epochs preserved recognition performance increased. however recognition accuracy begins drop third iteration. figure figure indicate considering volume unlabeled data size original labeled data influential. last column figure second column figure suggest better grow size re-training database approximately half re-training iteration. threshold analysis intuitively expect increase re-training threshold eliminate low-quality events. conducted series experiments different observations section optimized threshold spsw initial iteration. chose thresholds based results section figure clearly shows best performance achieved threshold resulted adding spsw decoded events original training set. interestingly recognition performance begins decrease peak even though theoretically selected events higher thresholds allow better re-training models. threshold spsw events selected. small amount additional high confidence data significant impact model re-training. note despite degradation recognition performance performance still slightly better baseline accuracy. comparision experimental schemes previous sections investigated number factors affect proposed algorithm applied eeg. section data selection schemes analyzed volume-based scheme optimization first iteration subsequent iterations roughly half previously selected epochs added; thresholdbased scheme threshold optimized first iteration threshold unchanged following iterations. scheme lead increase threshold iteration whereas scheme keeps optimized threshold fixed. figure shows performance schemes. seven iterations conducted performance proposed self-training method. schemes demonstrated using data spsw class. first iteration optimized based findings section using parameters optimized section overall performance increased absolute. kept initial threshold unchanged performance continuously degraded. suggests threshold optimized initial iteration longer optimal subsequent iterations. paper investigated using self-training semisupervised algorithm automatically annotate corpus. proposed method based limited amount manually annotated data updates training models iteratively re-training re-decoding. pilot study shows iterations able label signals also improve recognition accuracy though scale improvements small first time approach worked large amounts clinical data. example authors reported eeg-based recognition systems tested pools subjects respectively. improvements made employing techniques integrate variety learning algorithms active learning. also worthwhile explore autolabelling algorithms co-training potentially improve recognition performance. material based part upon work supported national science foundation grant iip. opinions findings conclusions recommendations expressed material author necessarily reflect views nsf. research reported publication also supported national institutes health award number uhg. corpus effort funded defense advanced research projects agency auspices doug weber contract temple university’s college engineering temple university’s office senior vice-provost research. halford shiau desrochers kolls dean waters azar haas kutluay martz sinha kern kelly sackellares laroche inter-rater agreement identification electrographic seizures periodic discharges recordings clin. neurophysiol. vol. roli marcialis semi-supervised pca-based face recognition using self-training proceedings joint iapr international conference structural syntactic statistical pattern recognition mihalcea co-training self-training word sense disambiguation proceedings conference computational natural language learning rosenberg hebert schneiderman semisupervised self-training object detection models proceedings ieee workshop applications computer vision approach panicker member puthusserypady member adaptation brain– computer interfaces two-classifier cotraining approach ieee trans. biomed. eng. vol. cohen cozman sebe cirelo huang semi-supervised learning classifiers theory algorithms human-computer interactions ieee trans. pattern anal. mach. intell. vol. dec. harati golmohammadi lopez obeid improved event classification using picone differential energy proceedings ieee signal processing medicine biology symposium harati lopez obeid jacobson tobochnik picone corpus data resource automated interpretation proceedings ieee signal processing medicine biology symposium", "year": "2018"}