{"title": "A Review of Convolutional Neural Networks for Inverse Problems in  Imaging", "tag": "eess", "abstract": " In this survey paper, we review recent uses of convolution neural networks (CNNs) to solve inverse problems in imaging. It has recently become feasible to train deep CNNs on large databases of images, and they have shown outstanding performance on object classification and segmentation tasks. Motivated by these successes, researchers have begun to apply CNNs to the resolution of inverse problems such as denoising, deconvolution, super-resolution, and medical image reconstruction, and they have started to report improvements over state-of-the-art methods, including sparsity-based techniques such as compressed sensing. Here, we review the recent experimental work in these areas, with a focus on the critical design decisions: Where does the training data come from? What is the architecture of the CNN? and How is the learning problem formulated and solved? We also bring together a few key theoretical papers that offer perspective on why CNNs are appropriate for inverse problems and point to some next steps in the field. ", "text": "abstract—in survey paper review recent uses convolution neural networks solve inverse problems imaging. recently become feasible train deep cnns large databases images shown outstanding performance object classiﬁcation segmentation tasks. motivated successes researchers begun apply cnns resolution inverse problems denoising deconvolution super-resolution medical image reconstruction started report improvements state-of-the-art methods including sparsity-based techniques compressed sensing. here review recent experimental work areas focus critical design decisions training data come from? architecture cnn? learning problem formulated solved? also bring together theoretical papers offer perspective cnns appropriate inverse problems point next steps ﬁeld. purpose review summarize recent works using cnns inverse problems imaging; i.e. problems naturally formulated recovering image noisy measurements; criterion excludes detection segmentation classiﬁcation quality assessment etc. also focus cnns avoiding architectures recurrent neural networks fully-connected networks stacked denoising autoencoders. organized literature search application looking topics broad interest could least three peer-reviewed papers last years. resulting applications references summarized table constrained scope allow draw meaningful generalizations surveyed works. basic ideas underlying convolutional neural networks inverse problems new. here give condensed history cnns give context follows. historical perspective accessible introduction deep neural networks summary recent history architecture proposed neural networks developed solving inverse imaging problems early approaches used networks parameters always include learning largely superseded compressed sensing approaches computer hardware improved became feasible train larger larger neural networks until krizhevsky achieved signiﬁcant improvement state imagenet classiﬁcation challenge using train convolutional layers million parameters million images. work spurred resurgence interest neural networks speciﬁcally cnns computer vision tasks also inverse problems more. k.h. acknowledges support epfl fellows fellowship program co-funded marie curie european unions horizon framework programme research innovation grant agreement manuscript organized follows. begin section brief background inverse imaging problems formulated learning problems. continue section summarizes recent results obtained using cnns variety image reconstruction applications. survey recent cnn-based reconstruction methods detail section focus design decisions involving training network architecture formulation learning problem optimization procedure itself. brieﬂy cover theoretical perspectives good performance cnns inverse problems section discuss critiques cnn-based approach section conclude section view future directions ﬁeld. begin introducing inverse problems contrasting traditional approach solving learningbased approach. textbook treatment inverse problems throughout section x-ray running example figure shows images various mathematical quantities mention. much work theory practice cnns posted preprint server arxiv.org eventually appearing traditional journal. lack peer review arxiv.org preferred cite papers except cases trying illustrate recent trend future direction ﬁeld. citing paper arxiv follow inline citation asterisk e.g. fig. block diagram image reconstruction methods using images x-ray examples. image creates measurements used estimate variety ways. traditional approach apply direct inversion artifact-prone sparse-measurement case current state regularized reconstruction rreg written general several recent works apply cnns result direct inversion iterative reconstruction might also reasonable input measurements back projected measurements. space acceptable images even d+time values representing physical quantity interest x-ray attenuation concentration ﬂuorophores; space measurement vectors depends imaging operator could include images fourier samples line integrals etc. typically consider continuous object usually discrete example x-ray image representing x-ray attenuations represents physics x-ray source detector measured sinogram inverse imaging problem develop reconstruction algorithm order recover original image measurements dominant approach reconstruction call objective function approach model recover estimate system model usually linear appropriate measure error. continuing example would discretization x-ray transform could euclidean distance h{x} many applications decades engineering gone developing fast reasonably accurate inverse operator easily solved robj{y} ˜h−{y}; ﬁltered back projection algorithm. important direct inverses begin show signiﬁcant artifacts number quality measurements decreases either underlying discretization breaks down inversion becomes ill-posed unfortunately many realworld problems measurements costly motivates collect possible. order reconstruct sparse noisy measurements often better regularized formulation regularization functional promotes solutions match prior knowledge simultaneously makes problem well-posed. could total variation regularization penalizes large gradients perspective challenge solving inverse problem designing implementing speciﬁc application. much effort gone designing generalpurpose regularizers minimization algorithms. example compressed sensing provides sparsity-promoting regularizers. nonetheless worst case application necessitates developing accurate efﬁcient along minimization algorithm. alternative objective function approach learning approach training ground truth images corresponding measurements known. parametric reconstruction algorithm rlearn substantially winner achieving error rate expect large gains inverse problems? expect denoising results improve order magnitude next years? next section answer question surveying results reported recent cnn-based approaches image reconstruction. inverse problems review here denoising provides best look recent trends results standard experiments appear papers. work cnn-based denoising showed average psnr berkeley segmentation dataset less improvement contemporary wavelet markov random ﬁeld-based approaches. comparison recent denoising work reported improvement similar experiment remains less better contemporary non-cnn methods another point reference approach reported average psnr standard test images less better comparisons another reported average experiment. recent achieves average conditions. important perspective denoising results learning distribution natural images could reused inside iterative optimization proximal operator enforce learned regularization inverse problem. trends similar deblurring super-resolution though experiments varied therefore harder compare. deblurring showed around psnr improvement comparison methods showed improvement around super-resolution work reported less improvement psnr comparisons. next years reported psnr increase baseline. even recent work improves work around psnr. video super-resolution improves non-cnn-based methods psnr improves upon result another inverse problems medical imaging direct comparison works impossible wide variety experimental setups. cnn-based work shows improvement limited-view reconstruction direct methods unregularized iterative methods compare regularized iterative methods. showed fullview improvement several direct reconstruction around improvement regularized iterative reconstruction. recently showed improvement psnr tv-regularized reconstruction showed larger improvement different tv-regularized method demonstrates performance equal state-of-the-art advantages running time. possible parameters measure error regularizer parameters avoiding overﬁtting. learning step complete rlearn used reconstruct image measurements. summarize objective function approach reconstruction function regularized minimization problem learning approach solution regularized minimization problem parametric function used solve inverse problem. learning formulation attractive overcomes many limitations objective function approach need handcraft forward model cost function regularizer optimizer hand learning approach requires training minimization typically difﬁcult requires problem-dependant choice class functions described finally note learning objective function approaches describe spectrum rather dichotomy. fact learning formulation strictly general including objective function formulation special case. discuss section iv-b aspects objective formulation approach retain critical choice design learning-based approaches inverse problems imaging. focus formulation using cnns. using means roughly ﬁxing functions sequence ﬁltering operations alternating simple nonlinear operations. class functions parametrized values ﬁlters used ﬁlter weights parameters minimization occurs. illustration figure shows typical architecture. describe theoretical motivations using cnns learning architecture inverse problems section mention practical advantages here. first forward operation consists convolutions simple pointwise nonlinear functions. means training complete execution rlearn fast amenable hardware acceleration gpus. second gradient computable chain rule gradients involve small convolutions meaning parameters learned efﬁciently gradient descent. ﬁrst cnn-based method entered imagenet large scale visual recognition challenge error rate object localization classiﬁcation task compared error rate next closest method winner subsequent competitions majority entries cnn-based continued improve illustration typical architecture images including objective function used training. relu function fig. denotes convolution. convolutions layer described tensor representing stack ﬁlters. improvements matter? cnn-based methods profound impact inverse problems object classiﬁcation. indeed difference impossible eye. hand improvements occur heavily studied ﬁelds denoising lena image since further cnns offer unique advantages many traditional methods. design architecture less decoupled application hand reused problem problem. also expanded straightforward ways computer memory grows evidence larger networks lead better performance. finally trained running model fast means cnn-based methods attractive terms running time even improve upon state-of-the-art performance. section survey design decisions needed develop cnn-based approaches inverse problems imaging. organize section around learning equation summarized figure ﬁrst describing training created network architecture designed learning problem formulated solved. learning requires suitable training i.e. pairs learn. typical learning problem training outputs provided oracle labeling inputs. example object classiﬁcation human graders might view large number images provide annotations each. inverse problem setting considerably difﬁcult oracle exists. example x-ray generate training would need image large number physical phantoms exact models feasible practice. choice training also constrains network architecture input output network must match dimensions respectively. generating training data cases generating training data straightforward forward model invert known exactly easily computable. denoising training data generated corrupting images noise; noisy image serves training input clean image training output e.g. noise serve oracle output scheme called residual learning super-resolution follows pattern training pairs easily generated downsampling e.g. true deblurring training pairs generated blurring medical imaging focus reconstructing real measurements corresponding ground truth usually known. emerging paradigm learn reconstruct sparse measurements using reconstructions fully-sampled measurements train. example reconstruction trains using under-sampled k-space data inputs reconstructions fully-sampled k-space data outputs. likewise uses low-view reconstruction input high-view reconstruction output. learn low-dose measurements preprocessing another aspect training data preparation whether training inputs measurements themselves whether preprocessing occurs. denoising natural measurements dimensions reconstruction. applications trend direct inverse operator preprocess network input. following notation section ii-a viewed combination objective function fig. example x-ray reconstructions. ground truth comes reconstruction using views. next three columns show reconstructions views using regularized reconstruction cnn-based approach cnn-based reconstruction preserves texture present ground truth results signiﬁcant increase snr. learning approach instead rlearn composition direct inverse ˜h−. example super-resolution ﬁrst upsample interpolate low-resolution input images; preprocess also preprocesses iterative reconstruction; preprocesses inverse fourier transform. without preprocessing must learn underlying physics inverse problem. even clear possible cnns preprocessing also leverage signiﬁcant engineering effort gone designing direct inverses past decades. superﬁcially type preprocessing appears inversion followed denoising standard approach inverse problems. unique artifacts caused direct inversion especially sparse measurement case usually highly structured therefore good candidates generic denoising approaches. instead allowed learn speciﬁc character artifacts. practical aspect preprocessing controlling dynamic range input. typically problem working natural images standardized datasets huge ﬂuctuations intensity contrast measurements certain inverse problems. avoid small images dominating error training best scale dynamic range training similarly advantageous discard training patches without sufﬁcient contrast. training size cnns typically least thousands parameters train; thus number pairs training important practical concern. number training pairs varied among papers surveyed. biomedical imaging papers tended fewest samples images papers natural images connected end-to-end creating modular hierarchical designs. example four-times super-resolution architecture created connecting two-times super-resolution networks distinct training two-times superresolution network applying twice modules trained unit. second approach begin iterative optimization algorithm unroll turning iteration layer network. scheme ﬁlters normally ﬁxed iterative minimization instead learned. approach pioneered sparse coding; results showed learned algorithms could achieve given error fewer iterations standard ones. many iterative optimization algorithms alternate ﬁltering steps pointwise nonlinear steps resulting network often cnn. approach authors unrolled admm algorithm design reconstruction state-of-the-art results improvements running time. networks designed original algorithm speciﬁc case therefore performance network cannot worse original algorithm training successful. concept unrolling also applied coarser scale modules network mimic steps typical blind deconvolution pipeline extract features estimate kernel estimate image repeat. another promising design approach similar unrolling learn part existing iterative method. example given modular nature popular iterative optimization schemes admm employed proximal operator rest algorithm remains unchanged *.this design combines many good aspects objective function learning-based approaches allows single used several different inverse problems without retraining. understanding learning minimization problem statistical inference provide useful insight selection cost regularization functions. perspective formulate goal learning maximizing conditional likelihood training output given corresponding training input parameters conditional likelihood. likelihood follows gaussian distribution optimization equivalent introduction euclidean distance regularization. another learning standard euclidean cost regularization implicitly chosen patch size numerous patches created small training set. patch size also important ramiﬁcations performance network linked architecture larger ﬁlters deeper networks requiring larger training patches large small training overﬁtting must avoided regularization learning and/or validation strategies necessary produce generalizes overcome fact performance limited size variety training set. strategy increase training size data augmentation pairs generated transforming existing ones. example augmented training pairs scaling space time turning pairs pairs. augmentation must application-speciﬁc trained network approximately invariant transforms used. another strategy effectively increase training size pretrained network. example ﬁrst trains image super-resolution large image dataset retrains videos. network architecture mean choice family cnns parameterized notation represents speciﬁc architecture weights learned training. great variety among cnn-based methods regarding architecture many convolutional layers ﬁlter sizes nonlinearities etc. example uses parameters uses order hundred thousand. section survey recent approaches architecture design inverse problems. simplest approach architecture design simply stack series convolutional layers non-linear functions figure provides baseline check feasibility network given application. straightforward adjust size network either changing number layers number channels layer size ﬁlters layer. example keeping ﬁlters small allows network deeper given number parameters constraining ﬁlters separable reduces number parameters. give experimenter sense training time required hardware well effects network size performance. simple starting point architecture tweaked greater performance; example adding downsampling upsampling operations simply adding layers instead using architecture design adapt successful architecture another application. example adapts network designed biomedical image segmentation reconstruction changing number output layers architectures also used validation. training performance network validation monitored training terminated performance validation begins drop. another method dropout individual units network randomly deleted training. motivation dropout idea network regularized forming weighted average possible parameter settings weights determined performance. regularization feasible removing units training provides reasonable approximation performs well practice. excellent performance cnns various applications undisputed question remains mostly unanswered. here bring together different theoretical perspectives begin explain cnns good solving inverse problems imaging. universal approximation know neural networks universal approximators. speciﬁcally fullyconnected neural network hidden layer approximate continuous function arbitrarily well provided hidden layer large enough result directly apply cnns fully connected consider network patch patch input patch mapped corresponding output patch fully connected network. thus cnns universal approximators shift-invariant functions. perspective statements cnns work well generalize algorithm vacuously true cnns generalize shiftinvariant algorithms. hand notion universal approximation tells network learn learn comparison established algorithms help guide understanding cnns practice. unrolling concrete perspective cnns generalizations established algorithms comes idea unrolling discussed section iv-b. idea originated authors unrolled ista algorithm sparse coding neural network. network typical includes recurrent connections share alternating linear/nonlinear motif. general perspective nearly state-of-the-art iterative reconstruction algorithms alternate linear steps pointwise nonlinear steps follows cnns able perform similarly well given appropriate training. reﬁnement idea comes establishes conditions forward model ensure linear step iterative method convolution. inverse problems surveyed meet conditions theory predicts certain inverse problems e.g. structured illumination microscopy amenable reconstruction cnns. another reﬁnement concerns popular rectiﬁed linear unit employed nonlinearity cnns results spline theory adapted show combinations relus approximate continuous function. suggests combinations relus usually employed cnns able closely assumes gaussian noise model; well-known fact inverse problems general. formulation used works surveyed despite fact several raise questions whether best choice formulation explicitly allows prior information desired parameters used. gaussian model weights well noise formulation results euclidean cost function euclidean regularization weights examples regularizations cnns total generalized variation norm sparsity coefﬁcients. regularized approaches taken objective function learning ﬁxed remains actually minimize crucial deep topic practical perspective treated black availability several high-quality software libraries perform efﬁcient training userdeﬁned architectures. comparison libraries refer here provide basic overview. popular approaches learning variations gradient descent. common stochastic gradient descent used where iteration gradient cost function computed using random subsets available training. reduces overall computation compared computing true gradient still providing good approximation. process tuned adding momentum i.e. combining gradients previous iterations clever ways using higher order gradient information bfgs initial weights zero chosen random distribution learning nonconvex initialization potentially change minimum network converges much difference observed practice. however good initializations improve speed convergence. explains popularity taking pretrained networks case unrolled architecture initializing network weights based corresponding known ﬁlters. recently procedure called batch normalization inputs layer network normalized proposed increase learning speed reduce sensitivity initialization mentioned section iv-a overﬁtting serious risk training networks potentially millions parameters. addition augmenting training steps taking training reduce overﬁtting. simplest split training data used optimization invariance another perspective comes work scattering transforms cascades linear operations nonlinearities combinations formed different channels. simpliﬁed model shows invariance translation importantly small deformations input cnns generalize scattering transform giving potential additional invariances e.g. rigid transformations frequency shifts etc. invariances attractive image classiﬁcation work needed connect results inverse problems. papers surveyed present many reasons optimistic cnns inverse problems also want mention general critiques approach. hope useful points think writing reviewing manuscripts area well jumping-off points future research. algorithm descriptions reproducibility planning survey aimed measure quantitative trends literature e.g. plot number training samples versus number parameters network. quickly discovered nearly impossible. manuscripts clearly noted number parameters training provided clear-enough description network calculate value. many included ﬁgure network architecture along lines figure without clear statement dimensions layer. similar problems exist description training evaluation procedures always clear whether evaluation data comes simulation real dataset. ﬁeld matures hope papers converge standard describe network architecture training evaluation. lack clarity presents barrier reproducibility work. another barrier fact training often requires specialized expensive hardware. gpus become ubiquitous largest cnns remain difﬁcult small research groups train. example imagenet large-scale visual recognition challenge took between days train robustness learning success cnnbased algorithm hinges ﬁnding reasonable solution learning problem stated before nonconvex problem best solution hope many local minima cost. raises questions robustness learning changes initialization parameters speciﬁcs optimization method employed. contrast typical convex formulations inverse problems speciﬁcs initialization optimization scheme provably affect quality result. superior architecture simply optimization fell superior local minimum? example confusion cause shows context denoising super-resolution jpeg deblocking network trained cost function outperform network trained cost function even regard cost. authors’ analysis highly disturbing result attribute learning stuck local optimum. regardless vast majority work relies cost computationally convenient provides excellent results. indication large networks trained lots data overcome problem. authors show larger networks local minima local minima equivalent terms testing performance. also identify global minima likely correspond parameter settings overﬁt training set. work stability learning process important step towards wider acceptance cnns inverse problem community. generally sensitive results given experiment small changes training network architecture optimization procedure? possible experimenter overﬁt testing iteratively tweaking network architecture state-of-the-art results achieved? combat this cnn-based approaches provide carefully-constructed experiments results reported large number testing images. even better competition datasets testing data hidden algorithm development complete. trust results? trained cnns remain non-linear highly complex. trust reconstructions generated systems? look evaluate sensitivity network noise ideally small changes input cause small changes output; data augmentation training help achieve this. similarly demonstrating generalization datasets help improve conﬁdence results showing performance network dependent systematic bias dataset. related question measure quality results. even robust improvement demonstrated practitioners inevitably want know e.g. whether resulting images reliably used diagnosis. much possible methods accessed respect ultimate application reconstruction rather intermediate measure ssim. critique made approach inverse problems especially relevant cnns often treated black boxes reconstructions generate plausible-looking design hiding areas algorithm less sure result. problems imaging. cnns powerful ﬂexible believe remains plenty room create even better systems. ﬁnal section suggest directions future research might take. biomedical imaging cnns applied inverse problems measurements take form image measurement model simple less relatively complicated models. search arxiv.org reveals dozens papers submitted within last months suggesting many incoming contributions areas. expect diffusion modalities spect optical tomography ultrasound superresolution microscopy etc. follow. combine cnns knowledge underlying physics well direct iterative inversion techniques. surveyed works involve using correct artifacts created direct iterative methods remains open question best prereconstruction method. creative approach build inverse operator network architecture network compute inverse fourier transform. another would back projected measurements least take form image could reduce burden learn physics forward model. cnns could deployed variety ways here e.g. using approximate high quality extremely slow reconstruction method. enough computing power training could generated running slow method real data trained resulting network could provide fast accurate reconstructions. cross-task learning cross-task learning algorithm trained dataset deployed different related task. attractive inverse problem setting avoids costly retraining network imaging parameters change occur often. could imagine network transfers completely different imaging modalities especially training data target modality scarce; e.g. network could train denoising natural images used reconstruct images. recent work made progress direction learning cnn-based proximal operator used inside iterative optimization method inverse problem multidimensional signals modern inverse problems imaging increasing involve reconstruction d+time images. however cnn-based approaches problems involve inputs outputs. likely much work deep neural networks general practical considerations. speciﬁcally learning strongly relies computation current gpus maximally physical memory. limitation makes training large network inputs outputs infeasible. large model partitioned onto separable computers. another data parallelism data split. used together large computational gains achieved approaches tackling multidimensional imaging problems. generative adversarial networks perceptual loss cnn-based approaches inverse problems also stand beneﬁt developments neural network research. development generative adversarial network offer break current limits supervised learning. basically networks trained competition generator tries learn mapping training samples discriminator attempts distinguish output generator real data. setup e.g. produce generator capable creating plausible natural images noise. essentially revises learning formulation replacing cost function another neural network. contrast designed cost function suboptimal assumed noise model incorrect discriminator network learned cost function correctly models probability density function real data from. gans already begun used inverse problems e.g. super-resolution deblurring related approach perceptual loss network trained compute loss function matches human perception. method already used style transfer super-resolution compared standard euclidean loss networks trained perceptual loss give better looking results typically improve snr. remains seen whether ideas gain acceptance applications medical imaging results must quantitatively accurate. rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations microstructure cognition vol. cambridge press burger schuler harmeling image denoising plain neural networks compete bmd? ieee conference computer vision pattern recognition jun. chen image denoising inpainting deep neural networks advances neural information processing systems chen pock trainable nonlinear reaction diffusion flexible framework fast effective image restoration ieee trans. pattern anal. mach. intell. vol. schawinski zhang zhang fowler santhanam generative adversarial networks recover features astrophysical images galaxies beyond deconvolution limit monthly notices royal astronomical society letters vol. oktay guerrero kamnitsas caballero marvao cook oregan rueckert multi-input cardiac image super-resolution using convolutional neural networks medical image computing computer-assisted intervention miccai springer cham oct. candes romberg robust uncertainty principles exact signal reconstruction highly incomplete frequency information ieee trans. inf. theory vol. feb. ledig theis huszar caballero cunningham acosta aitken tejani totz wang photo-realistic single image super-resolution using generative adversarial network arxiv. sep. ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets advances neural information processing systems ghahramani welling cortes lawrence weinberger eds. curran associates inc. johnson alahi fei-fei perceptual losses real-time style transfer super-resolution computer vision eccv ser. lecture notes computer science. cham switzerland springer oct. michael mccann received b.s.e. biomedical engineering university michigan ph.d. degree biomedical engineering carnegie mellon university currently scientist laboratoire d’imagerie biom´edicale centre d’imagerie biom´edicale ´ecole polytechnique f´ed´erale lausanne works x-ray reconstruction. research interest centers developing signal processing tools answer biomedical questions. kyong hwan received b.s. integrated m.s. ph.d. degrees department brain engineering kaist korea advanced institute science technology daejeon south korea respectively. post doctoral scholar kaist currently post doctoral scholar biomedical imaging group ´ecole polytechnique f´ed´erale lausanne switzerland. research interests include rank matrix completion sparsity promoted signal recovery sampling theory michael unser professor director epfl’s biomedical imaging group lausanne switzerland. primary area investigation biomedical image processing. internationally recognized research contributions sampling theory wavelets splines image processing stochastic processes computational bioimaging. published journal papers topics. author tafti book introduction sparse stochastic processes cambridge university press biomedical engineering instrumentation program national institutes health bethesda conducting research bioimaging. unser held position associate editor-in-chief ieee transactions medical imaging. currently member editorial boards siam imaging sciences ieee selected topics signal processing foundations trends signal processing. founding chair technical committee imaging signal processing ieee signal processing society. prof. unser fellow ieee eurasip fellow member swiss academy engineering sciences. recipient several international prizes including three ieee-sps best paper awards technical achievement awards ieee", "year": "2017"}