{"title": "Waveform Modeling and Generation Using Hierarchical Recurrent Neural  Networks for Speech Bandwidth Extension", "tag": "eess", "abstract": " This paper presents a waveform modeling and generation method using hierarchical recurrent neural networks (HRNN) for speech bandwidth extension (BWE). Different from conventional BWE methods which predict spectral parameters for reconstructing wideband speech waveforms, this BWE method models and predicts waveform samples directly without using vocoders. Inspired by SampleRNN which is an unconditional neural audio generator, the HRNN model represents the distribution of each wideband or high-frequency waveform sample conditioned on the input narrowband waveform samples using a neural network composed of long short-term memory (LSTM) layers and feed-forward (FF) layers. The LSTM layers form a hierarchical structure and each layer operates at a specific temporal resolution to efficiently capture long-span dependencies between temporal sequences. Furthermore, additional conditions, such as the bottleneck (BN) features derived from narrowband speech using a deep neural network (DNN)-based state classifier, are employed as auxiliary input to further improve the quality of generated wideband speech. The experimental results of comparing several waveform modeling methods show that the HRNN-based method can achieve better speech quality and run-time efficiency than the dilated convolutional neural network (DCNN)-based method and the plain sample-level recurrent neural network (SRNN)-based method. Our proposed method also outperforms the conventional vocoder-based BWE method using LSTM-RNNs in terms of the subjective quality of the reconstructed wideband speech. ", "text": "difﬁculty distinguishing fricatives similar voices. therefore speech bandwidth extension aims restore missing high-frequency components narrowband speech using correlations exist high-frequency components wideband speech signal attracted attentions many researchers. methods applied real-time voice communication also beneﬁt speech signal processing areas text-tospeech synthesis speech recognition speech enhancement many researchers made efforts ﬁeld bwe. early studies adopted source-ﬁlter model speech production attempted restore high-frequency residual signals spectral envelopes respectively input narrowband signals. high-frequency residual signals usually estimated narrowband residual signals spectral folding estimate high-frequency spectral envelopes narrowband signals always difﬁcult task. achieve goal simple methods codebook mapping linear mapping statistical methods using gaussian mixture models hidden markov models proposed. statistical methods acoustic models build represent mapping relationship narrowband spectral parameters high-frequency spectral parameters. although statistical methods achieved better performance simple mapping methods inadequate modeling ability gmms hmms lead over-smoothed spectral parameters constraints quality reconstructed speech signals recent years deep learning become emerging ﬁeld machine learning research. deep learning techniques successfully applied many signal processing tasks. speech signal processing neural networks deep structures introduced speech generation tasks including speech synthesis voice conversion speech enhancement ﬁeld neural networks also adopted predict either spectral parameters representing vocal-tract ﬁlter properties original log-magnitude spectra derived short-time fourier transform studied model architectures included deep neural networks recurrent temporal restricted boltzmann machines recurrent neural networks long short-term memory cells methods achieved better performance using abstract—this paper presents waveform modeling generation method using hierarchical recurrent neural networks speech bandwidth extension different conventional methods predict spectral parameters reconstructing wideband speech waveforms method models predicts waveform samples directly without using vocoders. inspired samplernn unconditional neural audio generator hrnn model represents distribution wideband high-frequency waveform sample conditioned input narrowband waveform samples using neural network composed long short-term memory layers feed-forward layers. lstm layers form hierarchical structure layer operates speciﬁc temporal resolution efﬁciently capture long-span dependencies temporal sequences. furthermore additional conditions bottleneck features derived narrowband speech using deep neural network -based state classiﬁer employed auxiliary input improve quality generated wideband speech. experimental results comparing several waveform modeling methods show hrnn-based method achieve better speech quality runtime efﬁciency dilated convolutional neural network -based method plain sample-level recurrent neural network -based method. proposed method also outperforms conventional vocoder-based method using lstm-rnns terms subjective quality reconstructed wideband speech. however limitation transmission channels restriction speech acquisition equipments bandwidth speech signal usually limited narrowband frequencies. example bandwidth speech signal public switching telephone network less khz. missing high-frequency components speech signal usually leads naturalness intelligibility baidu speech department baidu technology park beijing china work done graduate student national engineering laboratory speech language information processing university science technology china. conventional statistical models like gmms hmms since deep-structured neural networks capable modeling complicated nonlinear mapping relationship input output acoustic parameters. however existing methods vocoder-based ones means vocoders used extract spectral parameters narrowband waveforms reconstruct waveforms predicted wideband high-frequency spectral parameters. lead deﬁciencies. first parameterization process vocoders usually degrades speech quality. example spectral details always lost reconstructed waveforms low-dimensional spectral parameters mel-cepstra line spectral pairs adopted represent spectral envelopes vocoders. spectral shapes noise components voiced frames always ignored values binary voiced/unvoiced ﬂags used describe excitation. second difﬁcult parameterize predict phase spectra phase-warpping issue. thus simple estimation methods mirror inversion popularly used predict high-frequency phase spectra existing methods also constraints quality reconstructed wideband speech. recently neural network-based speech waveform synthesizers wavenet samplernn presented. wavenet distribution waveform sample conditioned previous samples additional conditions represented using neural network dilated convolutional neural layers residual architectures. samplernn adopted recurrent neural layers hierarchical structure unconditional audio generation. inspired wavenet waveform modeling generation method using stacked dilated cnns proposed previous work achieved better subjective performance vocoder-based approach utilizing lstmrnns. hand methods applying rnns directly model generate speech waveforms investigated. therefore paper proposes waveform modeling generation method using rnns bwe. discussed above direct waveform modeling generation help avoid spectral representation phase modeling issues vocoderbased methods. considering sequence memory modeling ability rnns lstm units paper adopts lstm-rnns model generate wideband highfrequency waveform samples directly given input narrowband waveforms. inspired samplernn hierarchical structure presented task. multiple recurrent layers hrnn layer operates speciﬁc temporal resolution. compared plain samplelevel deep rnns hrnns capable efﬁcient capturing long-span dependencies temporal sequences. furthermore additional conditions bottleneck features extracted narrowband speech using dnn-based state classiﬁer introduced hrnn modeling improve performance bwe. paper makes ﬁrst successful attempt model generate speech waveforms directly sample-level using rnns task. second various architectures waveform-based including plain sample-level lstmrnns hrnns hrnns additional conditions implemented evaluated paper. experimental results comparing several waveform modeling methods show hrnn-based method achieves better speech quality run-time efﬁciency stacked dilated cnnbased method plain sample-level rnn-based method. proposed method also outperforms conventional vocoder-based method using lstm-rnns terms subjective quality reconstructed wideband speech. paper organized follows. section brieﬂy review previous methods including vocoder-based ones dilated cnn-based one. section details proposed method presented. section reports experimental results conclusions given section vocoder-based methods using dnns rnns proposed recent years methods spectral parameters logarithmic magnitude spectra ﬁrst extracted short time fourier transform then dnns lstm-rnns trained minimum mean square error criterion establish mapping relationship narrowband speech high-frequency components wideband speech. additional features extracted narrowband speech bottleneck features used auxiliary inputs improve performance networks stage reconstruction wideband speech reconstructed concatenating input narrowband speech high-frequency components predicted trained lstm-rnn. phase spectra wideband speech usually generated simple mapping algorithms mirror inversion finally inverse overlap-add algorithm carried reconstruct wideband waveforms predicted phase spectra. experimental results previous work showed lstm-rnns achieve better performance dnns vocoder-based nevertheless still issues vocoder-based approach discussed section quality degradation caused parameterization vocoders inadequacy restoring phase spectra. recently novel waveform generation model named wavenet proposed successfully applied speech synthesis task model utilizes stacked dilated cnns describe autoregressive generation process audio waveforms without using frequency analysis vocoders. stacked dilated consists many convolutional layers different dilation lstm-rnns speech generation usually built acoustic parameters frame-level extracted vocoders ﬁxed frame shift straightforward model generate speech waveforms sample-level using similar lstm-rnn framework. structure sample-level recurrent neural networks shown fig. composed cascade lstm layers feed-forward layers. input waveform samples output waveform samples quantized discrete values µ-law. embedding layer maps discrete sample value real-valued vector lstm layers model sequence embedding vectors recurrent manner. lstm layer calculation process formulated output lstm layers time step represents activation function lstm units. multiple lstm layers output calculated layerby-layer. then passes layers. activation function last layer softmax function generates probability distribution output sample conditioned previous current input samples given training parallel input output waveform sequences model parameters lstm layers estimated using cross-entropy cost function. generation time output sample obtained maximizing conditional probability distribution preliminary informal listening test showed generation criterion achieve better subjective performance generating random samples distribution. random sampling necessary conventional wavenet samplernn models autoregressive architecture. however model structure shown fig. autoregressive one. input waveforms provide necessary randomness synthesize output speech especially unvoiced segments. motivated idea waveform modeling generation method proposed described conditional distribution output wideband high-frequency waveform sequence conditioned input narrowband waveform sequence using stacked dilated cnns similar wavenet samples discretized -bit µ-law quantization softmax output layer adopted. residual parameterized skip connections together gated activation functions also employed capacitate training deep networks accelerate convergence model estimation. different wavenet method modeled mapping relationship waveform sequences autoregressive generation process output waveform sequence. causal non-causal model structures implemented experimental results showed noncausal structure achieved better performance causal stacked dilated non-causal illustrated fig. described conditional distribution extension stage given input narrowband speech output sample obtained selecting quantization level maximum posterior probability. finally generated waveforms processed high-pass ﬁlter added input narrowband waveforms reconstruct ﬁnal wideband waveforms. experimental results showed method achieved better subjective performance vocoder-based method using lstm-rnns inspired samplernn unconditional audio generator containing recurrent neural layers hierarchical structure paper proposes waveform modeling generation methods using rnns bwe. section ﬁrst introduce plain sample-level rnns waveform modeling. structures hierarchical rnns conditional hrnns explained detail. finally ﬂowchart using rnns introduced. assume hrnn tiers total tier works sample-level tiers frame-level tiers since operate temporal resolution lower samples. frame-level tiers k-th tier operates frames composed samples. range time step k-th tier determined denoting quantized input waveforms assuming represents sequence length zero-padding divisible represents operation rounding observed time step n-th tier corresponds time steps m-th tier. k-th tier frame inputs t-th time step written framing concatenation operations shown fig. frame-level ties composed lstm layers. tier lstm units update hidden states based hidden states previous time step input current time step lstm layer k-th tier calculation process formulated different temporal resolution different tiers tier generates conditioning vectors tier time step implemented producing separate linear projections time step. intermediate tiers processing generating conditioning vectors tier. thus describe conditioning vectors uniformly srnn generation output sample depends previous current plain lstm-rnn architecture still deﬁciencies waveform modeling generation. first sample-level modeling makes difﬁcult model long-span dependencies input output speech signals signiﬁcantly increased sequence length compared frame-level modeling. second srnns suffer inefﬁciency waveform generation point-by-point calculation layers dimension expansion embedding layer. therefore inspired samplernn hierarchical structure proposed next subsection alleviate problems. structure hrnns illustrated fig. similar srnns mentioned section iii-a hrnns also composed lstm layers layers. different plain lstm-rnn structure srnns lstm layers hrnns form hierarchical structure multiple tiers tier operates speciﬁc temporal resolution. bottom tier deals individual samples outputs sample-level predictions. higher tier operates lower temporal resolution tier conditions tier except tier. model structure similar samplernn main difference original samplernn model unconditional audio generator employs history output waveforms network input generates output waveforms autoregressive way. while hrnn model shown fig. describes mapping relationship waveform sequences directly without considering autoregressive property output waveforms. hrnn structure speciﬁcally designed because narrowband waveforms used inputs task. removing autoregressive connections help reduce computation complexity facilitate parallel computing generation time. although conditional samplernns developed used neural vocoders reconstruct speech waveforms acoustic parameters still follow autoregressive framework different hrnns. tier gives probability distribution output sample conditioned current input sample together conditioning vector passed tier encodes history information since input sequence individual samples convenient model correlation among using memoryless structure layers. first mapped real-valued vector embedding layer. embedding vectors form input time step sample-level tier i.e. t+c−] finally obtain conditional probability distribution output sample passing layers. activation function last layer softmax function. output layers describes conditional distribution worth mentioning structure shown fig. non-casual utilizes future input samples together current previous input samples predict current output sample predict fig. generally speaking input samples current time step necessary order predict current output sample accroding also difference hrnn model samplernn causal autoregressive structure. similar srnns parameters hrnns estimated using cross-entropy cost function given training parallel input output sample sequences. generation time predicted using conditional probability distribution vocoder-based order combine auxiliary inputs hrnn model introduced section iii-b conditional hrnn structure designed shown fig. compared hrnns conditional hrnns additional tier named conditional tier top. input features conditional tier frame-level auxiliary feature vectors extracted input waveforms rather waveform samples. assume total number tiers conditional hrnn donate frame shift auxiliary input features. equations section iii-b still works here. similar introductions section iii-b frame inputs conditional tier written represents d-th dimension auxiliary feature vector time calculations hrnns followed. finally conditional probability distribution generating written ﬂowchart using srnns hrnns illustrated fig. mapping strategies. narrowband waveforms towards corresponding wideband counterparts narrowband waveforms towards waveforms high-frequency component wideband speech database wideband speech recordings used model training. training stage input narrowband waveforms obtained downsampling wideband waveforms. guarantee length consistency input timit corpus contained english speech multi-speakers sampling rate bits resolution adopted experiments. chose utterances construct training validation respectively. another utterances speakers included training validation used test evaluate performance different methods. experiments narrowband speech waveforms sampled obtained downsampling wideband speech khz. five systems constructed comparison experiments. descriptions systems follows. vrnn vocoder-based method using lstm-rnns introduced section ii-a. drnn-bn system used comparison predicted high-frequency components using deep lstm-rnn auxiliary features. backpropagation time algorithm used train lstm-rnn model based minimum mean square error criterion. system dnnbased state classiﬁer built extract features. -frames -dimensional narrowband mfccs used input classiﬁer posterior probabilities states monophones regarded output classiﬁer. classiﬁer adopt hidden layers hidden units layer hidden units hidden layers. layer ﬁfth hidden layer extractor could capture linguistic information. feature extractor also used chrnn system. dcnn waveform-based method using stacked dilated cnns introduced section ii-b. cnnhf system used comparison predicted high-frequency waveforms using non-causal cnns performed better conﬁgurations. srnn waveform-based method using samplelevel rnns introduced section iii-a. built model lstm layers layers. lstm layers layers hidden units embedding size model trained stochastic gradient decent minibatch size minimize cross entropy predicted real probability distribution. zeropadding applied make sequences minibatch length cost values added zero samples ignored computing gradients. adam optimizer used update parameters initial learning rate truncated backpropagation time algorithm employed improve efﬁciency model training truncated length output sequences narrowband waveforms upsampled sampling rate wideband speech zero high-frequency components. upsampled narrowband waveforms used model input. output waveforms either unﬁltered wideband waveforms high-frequency waveforms high-frequency waveforms obtained sending wideband speech high-pass ﬁlter ampliﬁer reducing quantization noise dotted lines fig. waveforms used model training input output waveform samples discretized -bit µ-law quantization. model parameters srnns hrnns trained cross-entropy criterion optimizes classiﬁcation accuracy discrete output samples training set. extension stage upsampled quantized narrowband waveforms trained srnns hrnns generate probability distributions output samples. output sample obtained selecting quantization level maximum posterior probability. later quantized output samples decoded continuous values using inverse mapping µ-law quantization. deampliﬁcation process conducted strategy order compensate effect ampliﬁcation training time. finally generated waveforms high-pass ﬁltered added input narrowband waveforms generate ﬁnal wideband waveforms. particularly conditional hrnns features used auxiliary input implementation shown gray lines fig. features regarded compact representation linguistic acoustic information here features extracted dnn-based state classiﬁer bottleneck layer smaller number hidden units hidden layers. inputs mel-frequency cepstral coefﬁcients extracted narrowband speech outputs posterior probability states. trained cross-entropy criterion used feature extractor extension time. hrnn waveform-based method using hrnns introduced section iii-b. hrnn composed tiers layers tier lstm layer tier therefore lstm layers layers total srnn system. number experiments tuning validation set. setups dimension hidden units training method srnn system mentioned above. frame size conﬁgurations hrnn model discussed section iv-b. chrnn waveform-based method using conditional hrnns introduced section iii-c. features extracted state classiﬁer used vrnn system adopted auxiliary conditions. model composed tiers. conditional tier lstm layer hidden units three tiers hrnn system. basic setups training method hrnn system. setup conditional tier introduced detail section iv-e. experiments ﬁrst investigated inﬂuence frame sizes mapping strategies performance hrnn system. then comparison different waveform-based methods including dcnn srnn hrnn systems carried out. later effect introducing features hrnns studied comparing hrnn system chrnn system. finally proposed waveform-based method compared conventional vocoder-based one. introduced section iii-b frame sizes parameters makes hrnn model different conventional sample-level rnn. experiment studied effect performance hrnnbased bwe. hrnn models several conﬁgurations trained accuracy efﬁciency compared shown fig. here classiﬁcation accuracy predicting discrete waveform samples validation used measure accuracy different models. total time generating utterances validation mini-batch size single tesla used measure run-time efﬁciency. mapping strategies considered experiment. results shown fig. existed conﬂict accuracy efﬁciency trained hrnn models. using smaller frame sizes improved accuracy sample prediction increased computational complexity extension stage strategies. finally chose trade-off used conﬁguration building hrnn system following experiments. strategy achieved much lower classiﬁcation accuracy strategy. reasonable since difﬁcult predict aperiodic noise-like high-frequency waveforms predict wideband waveforms. objective subjective evaluations conducted investigate strategy achieve better performance hrnn-based bwe. since improper compare classiﬁcation accuracy strategies directly score perceptual evaluation speech quality wideband speech adopted objective measurement here. utilized clean wideband speech reference calculated pesq scores utterances test generated using strategies respectively. comparison pesq scores upsampled narrowband utterances also calculated. average pesq scores conﬁdence intervals shown table differences three systems signiﬁcant according results paired t-tests table strategy achieved higher pesq score strategy. average pesq hrnn-wb system even lower upsampled narrowband speech. attributed model hrnn-wb system aimed reconstruct whole wideband waveforms incapable generating high-frequency components accurately hrnn-hf system. fig. average scores comparing system pairs including hrnn-hf hrnn-wb hrnn dcnn hrnn srnn chrnn hrnn chrnn vrnn. error bars represent conﬁdence intervals numerical values parentheses represent p-value one-sample t-test different system pairs. conducted amazon mechanical turk crowdsourcing platform compare subjective performance hrnn-wb hrnn-hf systems. wideband waveforms utterances randomly selected test reconstructed hrnnwb hrnn-hf systems. pair generated wideband speech evaluated random order native english listeners rejecting improper listeners based anticheating considerations listeners asked judge utterance pair better speech quality preference. here hrnn-wb system used reference system. scores denoted wideband utterance reconstructed evaluated system i.e. hrnn-hf system sounded better than worse than equal sample generated reference system pair. calculated average score conﬁdence interval pairs utterances listened listeners. besides one-sample t-test also conducted judge whether signiﬁcant difference average score examining p-value. results shown ﬁrst system pair fig. suggests hrnnhf system outperformed hrnn-wb system signiﬁcantly. consistent results comparing strategies dilated cnns used model waveforms task therefore strategy adopted following experiments building waveform-based systems. performance three waveform-based systems i.e. dcnn srnn hrnn systems compared objective subjective evaluations. accuracy efﬁciency metrics used section iv-b pesq score used section iv-c adopted objective measurements. besides extra metrics adopted here including signalto-noise ratio measured distortion waveforms spectral distance reﬂected distortion frequency domain. voiced frames unvoiced frames also calculated separately system. fairness efﬁciency comparison mini-batch size three systems generating utterances test set. time generating second speech using tesla recorded measurement efﬁciency experiment. table shows objective performance three systems test set. conﬁdence intervals also calculated metrics except generation time. results paired t-tests indicated differences three systems metrics signiﬁcant accuracy pesq score dcnn system good systems. hrnn system achieved best performance accuracy pesq score. hrnn system dcnn system achieved best performance voiced segments unvoiced segments respectively. hrnn system achieved lowest overall lowest unvoiced segments. hand dcnn system achieved lowest voiced frames among three systems. considering lsds calculated using amplitude spectra snrs inﬂuenced amplitude phase spectra reconstructed waveforms inferred hrnn system better restoring phase spectra voiced frames dcnn system according snr-v lsd-v results systems shown table terms efﬁciency generation time srnn system times longer hrnn system sampleby-sample calculation layers srnn structure discussed section iii-a. also efﬁciency dcnn system slightly worse hrnn system. results reveal hrnns help improve accuracy efﬁciency srnns modeling long-span dependencies among sequences using hierarchical structure. objective measurements used section iv-d adopted compare hrnn chrnn systems. results shown table iii. chrnn system outperformed hrnn system pesq score prediction accuracy good hrnn system. systems achieved similar performance. results show chrnn system better reconstructing voiced frames hrnn system contrary. terms efﬁciency generation time chrnn system higher hrnn system extra conditional tier. -point test also conducted evaluate subjective performance chrnn system using hrnn system reference system following evaluation conﬁgurations introduced section iv-c. results shown fourth system pairs fig. reveal utilizing features additional conditions hrnn-based improve subjective quality reconstructed wideband speech signiﬁcantly. fig. also shows spectrogram wideband speech generated chrnn system example sentence. comparing spectrograms produced hrnn system chrnn system observe high-frequency components generated chrnn system stronger hrnn system. lead better speech quality shown fig. finally compared performance vocoder-based waveform-based methods conducting objective subjective evaluations vrnn system chrnn system since systems adopted features auxiliary input. objective results including pesq shown table chrnn system achieved signiﬁcantly better vrnn system suggested proposed waveform-based method restore phase spectra accurately conventional vocoder-based method. pesq chrnn system good vrnn system. reasonable considering vrnn system modeled fig. observed high-frequency energy unvoiced segments generated dcnn system much weaker natural speech outputs srnn hrnn systems. compared srnn hrnn systems dcnn system better reconstructing high-frequency harmonic structures voiced segments. observations line results discussed earlier. furthermore -point tests carried evaluate subjective performance hrnn system using dcnn system srnn system reference system respectively. conﬁgurations tests ones introduced section iv-c. results shown second third system pairs fig. proposed hrnn-based method generated speech signiﬁcantly better quality dilated cnn-based method. compared srnn system hrnn system slightly better superiority insigniﬁcant signiﬁcance level. however hrnn system much efﬁcient srnn system generation time shown table compared hrnn system chrnn system objective subjective evaluations explore effects additional conditions hrnn-based bwe. introduced section iv-a features used additional conditions chrnn system since provide linguistic-related information besides acoustic waveforms. chrnn system adopted conditional hrnn structure introduced section iii-c tiers. dimension predicted directly used calculation pesq lsd. -point test also conducted evaluate subjective performance chrnn system using vrnn system reference system following evaluation conﬁguratioins introduced section iv-c. results shown ﬁfth system pairs fig. score high signiﬁcantly indicates chrnn system achieve signiﬁcantly higher quality reconstructed wideband speech vrnn system. comparing spectrograms produced vrnn system chrnn system fig. observed chrnn system performed better vrnn system generating high-frequency harmonics voiced sounds. besides high-frequency components generated chrnn system less over-smoothed natural vrnn system unvoiced segments. furthermore discontinuity lowfrequency high-frequency spectra speech generated vrnn system also found vocoderbased method shown fig. waveformbased systems alleviated discontinuity effectively. experimental results indicate superiority modeling generating speech waveforms directly utilizing vocoders feature extraction waveform reconstruction task. maximal latency different systems application scenarios strict requirement latency algorithm. compared maximal latency systems listed section iv-a results shown table here latency refers duration future input samples necessary predicting current output sample. maximal latencies vrnn system chrnn system determined window size stft extracting mfcc parameters implementation. maximal latencies three systems depended structures. srnn system processed input waveforms generate output waveforms sample-by-sample without latency according non-causal structure shown fig. adopted dcnn system receptive ﬁeld length made highest latency among systems. latency hrnn system relatively short number concatenated frames frame size tier small run-time efﬁciency waveform-based deﬁciency waveform-based methods time-consuming generation time. shown table table hrnn system achieved best run-time efﬁciency among four waveform-based systems still took seconds generate second speech current implementation. therefore accelerate computation hrnns important task future work. shown fig. using longer frame sizes help reduce computational complexity hrnns. another possible reduce number hidden units model parameters similar attempt accelerating wavenet speech synthesis paper proposed novel waveform modeling generation method using hierarchical recurrent neural networks fulﬁll speech bandwidth extension task. hrnns adopt hierarchy recurrent modules capture long-span dependencies input output waveform sequences. compared plain sample-level stacked dilated proposed hrnn model achieves better accuracy efﬁciency predicting high-frequency waveform samples. besides additional conditions bottleneck features extracted narrowband speech improve subjective quality reconstructed wideband speech. experimental results show proposed hrnn-based method achieves higher subjective preference scores conventional vocoder-based method using lstm-rnns. evaluate performance proposed methods using practical band-limited speech data improve efﬁciency waveform generation using hrnns utilize types additional conditions tasks future work. nakamura hashimoto oura nankaku tokuda mel-cepstral analysis technique restoring high frequency components low-sampling-rate speech proc. interspeech mehri kumar gulrajani kumar jain sotelo courville bengio samplernn unconditional end-toend neural audio generation model arxiv preprint arxiv. garofolo lamel fisher fiscus pallett darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc nasa sti/recon technical report vol. zhang speech wideband extension based gaussian mixture model chinese journal acoustics g.-b. song martynovich study hmm-based bandwidth extension speech signals signal processing vol. z.-h. ling s.-y. kang senior schuster x.-j. qian meng deng deep learning acoustic modeling parametric speech generation systematic review existing techniques future trends ieee signal processing magazine vol. z.-h. ling deng modeling spectral envelopes using restricted boltzmann machines deep belief networks statistical parametric speech synthesis ieee transactions audio speech language processing vol. l.-h. chen z.-h. ling l.-j. l.-r. voice conversion using deep neural networks layer-wise generative training ieee/acm transactions audio speech language processing vol. tsao matsuda hori speech enhancement based deep denoising autoencoder. proc. interspeech l.-r. c.-h. regression approach speech enhancement based deep neural networks ieee/acm transactions audio speech language processing vol. pulakka alku bandwidth extension telephone speech using neural network ﬁlter bank implementation highband spectrum ieee transactions audio speech language processing vol.", "year": "2018"}