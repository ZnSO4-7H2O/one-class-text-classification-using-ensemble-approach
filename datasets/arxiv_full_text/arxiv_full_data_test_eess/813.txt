{"title": "On the difficulty of a distributional semantics of spoken language", "tag": "eess", "abstract": " In the domain of unsupervised learning most work on speech has focused on discovering low-level constructs such as phoneme inventories or word-like units. In contrast, for written language, where there is a large body of work on unsupervised induction of semantic representations of words, whole sentences and longer texts. In this study we examine the challenges of adapting these approaches from written to spoken language. We conjecture that unsupervised learning of the semantics of spoken language becomes feasible if we abstract from the surface variability. We simulate this setting with a dataset of utterances spoken by a realistic but uniform synthetic voice. We evaluate two simple unsupervised models which, to varying degrees of success, learn semantic representations of speech fragments. Finally we present inconclusive results on human speech, and discuss the challenges inherent in learning distributional semantic representations on unrestricted natural spoken language. ", "text": "domain unsupervised learning work speech focused discovering low-level constructs phoneme inventories word-like units. contrast written language large body work unsupervised induction semantic representations words whole sentences longer texts. study examine challenges adapting approaches written spoken language. conjecture unsupervised learning semantics spoken language becomes feasible abstract surface variability. simulate setting dataset utterances spoken realistic uniform synthetic voice. evaluate simple unsupervised models which varying degrees success learn semantic representations speech fragments. finally present inconclusive results human speech discuss challenges inherent learning distributional semantic representations unrestricted natural spoken language. realm written language unsupervised approaches inducing semantic representations words long pedigree history substantial success core idea behind models build word representations predict surrounding context. search similarly generic versatile representations whole sentences various composition operators applied word representations alternatively sentence representations induced objective predict surrounding sentences representations capture aspects meaning encoded sentences used variety tasks semantic entailment text understanding. case spoken language unsupervised methods usually focus discovering relatively low-level constructs phoneme inventories word-like units. mainly fact insight distributional semantics shall know word company keeps hopelessly confounded case spoken language. text words considered semantically similar co-occur similar neighbors. however speech segments occur utterance situation often many features addition similar meaning uttered speaker accompanied similar ambient noise. study show abstract away speaker background noise effectively capture semantic characteristics spoken utterances unsupervised way. present segmatch model trained match segments utterance. segmatch utterance encodings compared audiovec trained decode context surrounds utterance. investigate whether representations capture semantics evaluate speech vision datasets photographic images paired spoken descriptions. experiments show single synthetic voice simple model trained image captions capture pairwise similarities correlate visual space. chung glass sequence-to-sequence audiovec model learns semantic embeddings audio segments corresponding words predicting audio segments around chung glass experiment model rename speechvec. chen train semantic word embeddings word-segmented speech part method training system non-aligned speech text. works closely related current study crucially unlike assume speech already segmented discrete words. models section encoder architecture. encoder loosely based architecture chrupała i.e. consists -dimensional convolutional layer subsamples input followed stack recurrent layers followed self-attention operator. unlike chrupała layers instead layers implement residual connections. modiﬁcations made order exploit fast native cudnn implementation stack thus speed experimentation exploratory stage research. encoder deﬁned follows conv convolutional layer length channels stride stack layers attn self-attention unit lnormalization. self-attention operator computes weighted activations timesteps furthermore discuss factors preventing effective learning datasets multiple human speakers include confounds semantic situational factors well artifacts datasets. studies unsupervised learning speech typically discover phonemic lexical building blocks language signal. park glass show words phrase units continuous speech discovered using algorithms based dynamic time warping. oord introduce vector quantisedvariational autoencoder model convolutional encoder trained audio data gives discrete encodings closely related phonemes. recently several unsupervised speech recognition methods proposed segment speech cluster resulting word-like segments encode segment embeddings containing phonetic information scharenborg show word phrase units arise by-product end-to-end tasks speech-tospeech translation. current work directly extract semantic rather word form information speech. semantic information encoded speech used studies ground speech visual context. datasets images paired spoken captions used train multimodal models extract visually salient semantic information speech without access textual information form semantic supervision contextual information another modality limits help learn understand speech describing now. hand success word embeddings derived distributional semantic principles shown rich semantic information within structure language semantic representations words obtained latent semantic analysis proven closely resemble human semantic knowledge wordvec models produce semantically rich word embeddings learning predict surrounding words text encoder projection matrices beginning segment respectively. single shared encoder types speech segments projections separate. decoding rather model learns match encoded segments utterance distinguish encoded segments different utterances within minibatch. loss function similar matching spoken utterances images chrupała difference matching utterance segments other beginning segments utterance beginning segments different utterances within batch cosine distance encoded segments. loss function thus attempts make cosine distance between encodings matching segments less distance encodings mismatching segment pairs margin. model chung glass works word-segmented speech encoder encodes middle word word sequence decoder decodes surrounding words. similarly skip-thought model works sequence three sentences encoding middle decoding previous next one. fully unsupervised setup access word segmentation thus audiovec models work arbitrary speech segments. split utterance three equal sized chunks model encodes middle decodes ﬁrst third one. decoder predicts mfcc features time based state hidden layer time reading chung glass clear whether addition hidden state decoder also receives mfcc frame input. thus implemented versions without input. audiovec-c decoder receives output encoder initial state hidden layer frame input predicts next frame xﬁrst mfcc features previous chunk time ˆxﬁrst predicted features next time step learned projection matrix single step recurrence xmiddle sequence mfcc features input. decoder third chunk xthird deﬁned way. audiovec-u decoder receives output encoder input time step access frame note speciﬁc segment speech crucial component either models mostly driven fact experiments speech vision datasets speech consists isolated utterances. data consisting longer narratives dialogs could different segmentation schemes. datasets order facilitate evaluation semantic aspect learned representations work speech vision datasets couple photographic images spoken descriptions. thanks structure data evaluation metrics detailed section synthetically spoken coco dataset created chrupała based original coco dataset using google api. captions spoken single synthetic voice realistic simpler human speakers lacking variability ambient noise. images captions. five thousand images held validation test. flickrk audio caption corpus dataset contains captions original flickrk corpus read aloud crowdworkers. images image descriptions. thousand images held validation another thousand test set. places audio caption corpus dataset collected using crowdworkers. image described single spontaneously spoken caption. training images validation images evaluation metrics evaluate quality learned semantic speech representations according following criteria. paraphrase retrieval synthetically spoken coco dataset well flickrk audio caption corpus image described independent spoken captions. thus captions describing image effectively paraphrases other. structure data allows paraphrasing retrieval task measure semantic quality learned speech embeddings. encode spoken utterances validation data rank others according cosine similarity. measure median rank top-ranked paraphrase; recallk proportion paraphrases among top-ranked utterances image representational space similarity analysis evaluating pairwise similarities objects correlated object representation spaces compare cosine similarities among encoded utterances versus cosine similarities among vector representations images. speciﬁcally create pairwise similarity matrices among encoded utterances validation data among images corresponding utterance note since descriptions image image replicated times matrix take upper triangulars matrices compute pearson’s correlation coefﬁcient them. image features evaluation obtained ﬁnal fully connected layer vgg- pre-trained imagenet consist dimensions. preprocess audio extracting dimensional mel-frequency cepstral coefﬁcients plus total energy. milisecond windows sampled every miliseconds. audiovec segmatch models trained maximum epochs adam learning rate gradient clipping segmatch uses margin encoder layers units. convolutional layer channels size stride hidden layer attention audiovec decoder hidden units; size output projections segmatch also units. segmatch size erased center portion utterance frames. apply early stopping report results model epoch scored best recall. applying segmatch human data synthetic speech table shows evaluation results synthetic speech. representations learned audiovec segmatch compared performance random vectors mean mfcc vectors well visually supervised representations audiovec works better chance mean mfcc paraphrase retrieval correlate visual space. segmatch works much better audiovec according criteria. come close paraphrase retrieval correlate visual modality even better. flickrk initial experiments flickrk similarly unsuccessful. analysis learned segmatch representations revealed spite partitioning data speaker training speaker identity decoded them. enforcing speaker invariance thus implemented version segmatch auxiliary speaker classiﬁer connected encoder gradient reversal operator architecture optimizes main loss time pushing encoder remove information speaker identity representation outputs. preliminary experiments addition able prevent speaker identity encoded representations ﬁrst epochs training. evaluating speaker-invariant representation gave contradictory results shown table good scores paraphrase retrieval zero correlation visual space. analysis showed seems artifact flickrk data spoken captions belonging consecutively numbered images share characteristics even though images not. side effect causes captions belonging image also share features independent semantic content leading high paraphrasing scores. artifact changes data collection procedure affected aspect captions ways correlate sequential ordering dataset. treat image number regression target ﬁrst principal components segmatch representation captions predictors account holdout variance using non-linear model effect disappears arbitrarily relabel images. synthetic speech segmatch approach inducing utterance embeddings shows promising performance. likewise previous work shown success word-segmented speech. remain challenges carrying results natural unsegmented speech. word segmentation highly non-trivial research problem variability spoken language serious intractable confounding factor. even controlling speaker identity still superﬁcial features speech signal make easy model ignore semantic content. artifacts datasets thus care needed evaluating unsupervised models spoken language example multiple evaluation criteria help spot spurious results. spite challenges future want explore effectiveness enforcing desired invariances auxiliary classiﬁers gradient reversal. references afra alishahi marie barking grzegorz chrupała. encoding phonology recurrent neuproceedings model grounded speech. conference computational natural language learning pages association computational linguistics. yi-chen chen chia-hao shen sung-feng huang hung-yi lee. towards unsupervised automatic speech recognition trained unarxiv preprint aligned speech text only. arxiv.. grzegorz chrupała lieke gelderloos afra alishahi. representations language model visually grounded speech signal. proceedings annual meeting association computational linguistics. junyoung chung caglar gulcehre kyunghyun yoshua bengio. empirical evaluation gated recurrent neural networks sequence modeling. nips deep learning representation learning workshop. yaroslav ganin victor lempitsky. unsupervised domain adaptation backpropagation. proceedings international conference machine learning volume proceedings machine learning research pages lille france. pmlr. david harwath antonio torralba james glass. unsupervised learning spoken language advances neural inforvisual context. mation processing systems pages micah hodosh peter young julia hockenmaier. framing image description ranking task data models evaluation metrics. journal artiﬁcial intelligence research yacine jernite samuel bowman david sontag. discourse-based objectives fast unsupervised sentence representation learning. arxiv preprint arxiv.. richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treeproceedings conference bank. empirical methods natural language processing pages yu-hsuan wang hung-yi lin-shan lee. segmental audio wordvec representing utterances sequences vectors applications spoken term detection. ieee international conference acoustics speech signal processing. proceedings. julian georg zilly rupesh kumar srivastava koutn´ık j¨urgen schmidhuber. recurrent proceedings inhighway networks. ternational conference machine learning volume proceedings machine learning research pages international convention centre sydney australia. pmlr. nikolaus kriegeskorte marieke peter bandettini. representational similarity analysisconnecting branches systems neuroscience. frontiers systems neuroscience tsung-yi michael maire serge belongie james hays pietro perona deva ramanan piotr doll´ar lawrence zitnick. microsoft coco computer vision– common objects context. eccv pages springer. tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems pages olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. odette scharenborg laurent besacier alan black mark hasegawa-johnson florian metze graham neubig sebastian st¨uker pierre godard markus m¨uller lucas ondel linguistic unit discovery multi-modal inputs unwritten languages summary speaking rosetta jsalt workshop. arxiv preprint arxiv..", "year": "2018"}