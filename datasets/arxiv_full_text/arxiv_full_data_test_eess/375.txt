{"title": "Phonetic and Graphemic Systems for Multi-Genre Broadcast Transcription", "tag": "eess", "abstract": " State-of-the-art English automatic speech recognition systems typically use phonetic rather than graphemic lexicons. Graphemic systems are known to perform less well for English as the mapping from the written form to the spoken form is complicated. However, in recent years the representational power of deep-learning based acoustic models has improved, raising interest in graphemic acoustic models for English, due to the simplicity of generating the lexicon. In this paper, phonetic and graphemic models are compared for an English Multi-Genre Broadcast transcription task. A range of acoustic models based on lattice-free MMI training are constructed using phonetic and graphemic lexicons. For this task, it is found that having a long-span temporal history reduces the difference in performance between the two forms of models. In addition, system combination is examined, using parameter smoothing and hypothesis combination. As the combination approaches become more complicated the difference between the phonetic and graphemic systems further decreases. Finally, for all configurations examined the combination of phonetic and graphemic systems yields consistent gains. ", "text": "state-of-the-art english automatic speech recognition systems typically phonetic rather graphemic lexicons. graphemic systems known perform less well english mapping written form spoken form complicated. however recent years representational power deep-learning based acoustic models improved raising interest graphemic acoustic models english simplicity generating lexicon. paper phonetic graphemic models compared english multi-genre broadcast transcription task. range acoustic models based lattice-free training constructed using phonetic graphemic lexicons. task found long-span temporal history reduces difference performance forms models. addition system combination examined using parameter smoothing hypothesis combination. combination approaches become complicated difference phonetic graphemic systems decreases. finally conﬁgurations examined combination phonetic graphemic systems yields consistent gains. hidden markov model based automatic speech recognition systems typically built using sub-words units phones graphemes. system performance depends appropriate deﬁnition sub-word units accuracy consistency decomposing words sub-word units. phonetic lexicons provide mapping orthographic representation word sequence letters sequence phones. however generation lexicons requires linguistic knowledge target language time-consuming expensive. hand graphemic lexicons attractive graphemes directly used. moreover graphemic lexicons easily expanded include out-of-vocabulary words unlike phonetic lexicons. languages close grapheme-to-phone mapping graphemic hmm-based systems shown perform similarly phonetic systems however languages irregular grapheme-to-phone mappings english graphemic hmm-based systems normally perform signiﬁcantly worse phonetic counterparts surprising system relies acoustic model implicitly capture irregularities graphemic acoustic realisation. powerful deep learning based acoustic models used connectionist temporal classiﬁcation model long-span temporal information graphemic systems phonetic systems small read english task paper aims whether recent deep-learning based acoustic models also model long-span temporal information allow hmm-based graphemic systems perform level accuracy phonetic systems english. range models available including long short-term memory networks convolutional neural networks time-delay neural networks bidirectional lstm networks additionally various layer-wise combination schemes allow advantages several models leveraged models also offer ﬂexibility terms span temporal information capture. instance interleaved tdnn-lstm model extends temporal span lstm model wide window future. models also efﬁciently trained directly random initialisation using approaches lattice-free maximum mutual information estimation. often results improved performance state-level minimum bayes’ risk trained models complex models likely have possibly signiﬁcant variations performance depending choice training hyper-parameters. variation system performance taken advantage using system combination paper examine forms system combination different complexities costs. ﬁrst random ensemble method utilises multiple training runs different random seeds produce slightly different complimentary systems. second model smoothing interpolates number intermediate model parameters using weights estimated subset training data. finally graphemic systems competitive phonetic system complimentary phonetic systems. rest paper organized follows. sections describe graphemic acoustic models model combination approaches respectively. section details experiments conducted english multi-genre broadcast transcription task phonetic graphemic models well using different combination approaches. finally conclusions given section core graphemic system graphemic lexicon. english straightforward form alphabet letters /a-z/. addition base graphemes also useful mark additional attributes apostrophes abbreviations excerpts phonetic graphemic lexicons b.b.c.’s information moon ﬁrst entry abbreviation apostrophe attributes potentially allows graphemic system handle discrepancy pronounced written forms. three examples illustrate situations graphemic systems struggle model letter omission vowel consonant vowel-consonant recombination. though issues handled using context dependent models e.g. bi-graphemes trigraphemes others length context necessary disambiguation prohibitively large. example phonetic lexicon used section associates three phones /dh/ /th/ sound corresponding grapheme sequence ’th’. problem compounded fact following grapheme depending neighbor represented different vowel/consonant phones. examples given section suggest graphemic systems context modelling even important phonetic systems rather solely relying acoustic modelling units handle intricate grapheme-to-phone rules also possible examine acoustic models capable modelling long-span temporal information. deep neural network acoustic model small number preceding succeeding frames typically used predict current state shown left right context window sizes typically less tdnn complex structure enables cover signiﬁcantly larger number preceding succeeding frames without signiﬁcantly increasing number model parameters. example model considered section uses past future frames. recurrent units lstm network described equation allows even longer-span temporal information modelled. note practice past information typically truncated ﬁxed large number frames furthermore tdnn-lstm model obtained interleaving tdnn layers lstm layers increases context window frames past frames future. addition powerful classiﬁers advanced deep-learning based acoustic models thus utilise signiﬁcantly longer span temporal information used previous work gaussian mixture models. thus large variations behaviours intermediate models iteration iteration ﬁnal models originating different starting points. latter likely larger models trained different random initialisations using lf-mmi criterion cross-entropy initialisation stage common targets systems. variation typically results models making different predictions. depending level useful variation diverse predictions help resolve confusions. serves basis various system combination approaches combination ensemble diverse individually accurate systems often result signiﬁcant gains common methods introduce diversity include random parameter initialisation bagging random decision trees using different random initialisations shown simple efﬁcient approach introducing diversity method able provide signiﬁcant diversity keeping similar performance across systems. thus combining systems ensemble could yield strong gains. less common method ensemble generation take number intermediate models training interpolate parameters number models represents parameters model represents combination weight. idea behind model smoothing designed reduce unwanted variations training. models normally selected later stages training using ﬁxed iteration interval between selected models combination weights associated individual layers optimised subset training examples. combination weights constrained though generally hard ensure combined model would improve ﬁnal trained model paper shows large performance improvements possible. represents -best hypothesis utterance using model total number utterances. cwer measures different -best hypotheses models found correlated combination gains standard deviation wers possible model smoothing equation combining iterations model training run. general system combination approach hypothesis-level combination. examples form approach rover confusion network combination minimum bayes risk combination work combination used ﬁnds word sequence attempts minimise expected across combination weights posterior probability word sequence given observation sequence acoustic model hypotheses represents levenshtein distance word sequences though computationally expensive combination shown perform better rover combination cnc. experiments conducted using data english multi-genre broadcast challenge. data supplied british broadcasting corporation consists audio television programmes. data contains wide range genres comedy drama sports shows. total hours audio data associated subtitles available acoustic model training. lightly supervised decoding selection used extract hours training hours development devb also supplied. acoustic model features -dimensional mel-ﬁlter bank features normalised using utterance level mean normalisation show-segment level variance normalisation around left bi-phone dependent states used targets. results based automatic audio segmentation using based segmenter trained mgb- data. examine impact acoustic model complexity phonetic graphemic system performance range acoustic models different topology spans temporal information built. include feed-forward sub-sampled tdnn unidirectional lstm interleaved tdnn-lstm models. models hidden layers -dimensional sigmoid units input context window spanning frames past frames future. tdnn models layers -dimensional rectiﬁed linear units wider input context window spanning frames past frames future. lstm model lstmp layers -dimensional cells -dimensional recurrent nonrecurrent projections. effective temporal information window lstm spans frames past frames future. interleaved tdnn-lstm models layers dimensional relu units. tdnn-lstm model widest temporal information window starting frames past ending frames future. models trained using lf-mmi criterion single using kaldi toolkit work speaker-independent systems used. ﬁrst pass decoding language model -gram language model words lexicon used. trained audio subtitles words supplied subtitles. addition recurrent neural network language model also used reﬁne result ﬁrst pass decoding. cued-rnnlm toolkit used train rnnlm using layer -dimensional units. given vocabulary size quantity training data noise contrastive estimation adopted speed training evaluation test time -gram approximation rnnlm used rescore -gram lattices. rnnlm trained unnormalized output layer probabilities used rescoring provided large speed decoding/combination used produce ﬁnal output. unless stated otherwise performance -gram model quoted. impact acoustic model performance difference phonetic graphemic systems illustrated table second column shows relative degradation performance graphemic system. complexity model span available temporal information increases difference phonetic graphemic system wers drops relative. largest drop happens lstm units used model longer history information. implies graphemic systems sensitive shorter histories phonetic systems. third column table shows graphemic system gets competitive gain combining phonetic system increases relative. graphemic systems also expected sensitive choice acoustic modelling context. wider contexts suitable graphemic systems better account mismatch orthographic spoken form. however shorter contexts appealing simplicity speed training well decoding. table shows phonetic systems signiﬁcantly robust bi-phone units replaced mono-phone units. though mono-grapheme units yield twice large degradation mono-phone units simplicity graphemic lexicons offers interesting compromise. rather combining phonetic graphemic systems also possible combine systems diverse ensemble discussed section simple produce additional system utilise alternative training criterion smbr training. table shows smbr training yields competitive model combination gains systems criteria larger phonetic graphemic systems table partly attributed larger performance differences phonetic graphemic systems combined. additional systems also generated using simpler approaches. example model smoothing require another model trained. work models iteration interval taken ﬁnal epoch lf-mmi training combination weights estimated subset training data discussed section table shows model smoothing effective improve system performance graphemic phonetic systems. additionally performing model smoothing difference phonetic graphemic systems reduced though gains combining phonetic graphemic systems decrease model smoothing dropping relative still large gain performance combined systems model smoothing yielding better performance modifying training criterion. alternatively random ensembles built changing random seed used initialise models lf-mmi training. expensive model smoothing allows additional diversity introduced. lf-mmi training beneﬁt random ensemble generation avoids cross-entropy initialisation stage approaches smbr training targets normally used system possibly reducing diversity ﬁnal systems sequence training. work ensemble tdnn-lstm models created building additional models using different seeds random parameter initialisation. table shows although standard deviation across systems small cwer large suggesting systems complementary. cwer number context ensemble smbr trained models task mean cwer last block table shows ensemble combination multiple single models yields large gains approaches examined work. given large gains model smoothing interesting examine ensembles smoothed models. also shown table expected cwers ensembles reduced model smoothing reduces diversity precise stopping points. however still large gains relative ensemble combination. additionally difference phonetic graphemic smoothed unsmoothed systems combining random ensembles reduced relative. given small difference phonetic graphemic ensembles additional gains combining systems might expected. however extensive combination techniques means diversity ensembles already signiﬁcantly reduced. table shows combining phonetic graphemic ensembles yields absolute relative reduction point interesting improved language modelling approaches yield beneﬁts. last column table shows -gram rescoring reduces rnnlm gave additional improvement yielding ﬁnal error rate task. paper investigated whether recent advances deep learning based approaches enabled graphemic english systems reach performance level traditionally used phonetic systems. found combination long-span temporal history future information context-dependent graphemic units important obtain competitive performance graphemic english systems. relative difference phonetic graphemic systems reduced employing system combination approaches model smoothing random ensemble methods found effective. combination methods yielded graphemic english system multigenre broadcast transcription relatively worse equivalent phonetic english system complementary. worth noting performance combining ensemble phonetic systems thus simply enlarging size phonetic ensemble expected match graphemic/phonetic ensemble performance. hinton deng dahl mohamed jaitly senior deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine bell gales hain kilgour lanchantin mcparland renals wester challenge evaluating multi-genre broadcast media recognition. proc. asru workshop pages woodland qian zhang gales karanasou lanchantin wang. cambridge university transcription systems multi-genre broadcast challenge. proc. asru workshop pages povey peddinti galvez ghahremani manohar wang khudanpur. purely sequence-trained neural networks based lattice-free mmi. proc. interspeech pages chen wang gales woodland. efﬁcient training evaluation recurrent neural network language models automatic speech recognition. ieee/acm transactions audio speech language processing chen wang gales woodland. efﬁcient lattice rescoring methods using recurrent neural network language models. ieee/acm transactions audio speech language processing", "year": "2018"}