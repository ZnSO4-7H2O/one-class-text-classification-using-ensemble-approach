{"title": "Framework for evaluation of sound event detection in web videos", "tag": "eess", "abstract": " The largest source of sound events is web videos. Most videos lack sound event labels at segment level, however, a significant number of them do respond to text queries, from a match found using metadata by search engines. In this paper we explore the extent to which a search query can be used as the true label for detection of sound events in videos. We present a framework for large-scale sound event recognition on web videos. The framework crawls videos using search queries corresponding to 78 sound event labels drawn from three datasets. The datasets are used to train three classifiers, and we obtain a prediction on 3.7 million web video segments. We evaluated performance using the search query as true label and compare it with human labeling. Both types of ground truth exhibited close performance, to within 10%, and similar performance trend with increasing number of evaluated segments. Hence, our experiments show potential for using search query as a preliminary true label for sound event recognition in web videos. ", "text": "largest source sound events videos. videos lack sound event labels segment level however signiﬁcant number respond text queries match found using metadata search engines. paper explore extent search query used true label detection sound events videos. present framework large-scale sound event recognition videos. framework crawls videos using search queries corresponding sound event labels drawn three datasets. datasets used train three classiﬁers obtain prediction million video segments. evaluated performance using search query true label compare human labeling. types ground truth exhibited close performance within similar performance trend increasing number evaluated segments. hence experiments show potential using search query preliminary true label sound event recognition videos. internet ﬂooded massive amount multimedia data mostly comprising videos containing sound events often critical understand video content. hence necessary automatically recognize sound events within audio e.g. police siren dishwasher birds singing. sound event recognition applied multiple forms conjunction modalities retrieve index consumer-generated videos based content video surveillance human-robot interaction wildlife monitoring context-aware systems benchmark performances. although necessary literature tends focus dcase-like datasets audio-only recordings smaller scale. leaves primary source sound events intrinsic problems less explored makes unclear state sound event recognition systems work videos. although youtube based audioset recently released containing weak labels sound events work explores mismatch conditions existing audio-only research datasets applied youtube videos. sound event recognition large-scale videos poses several challenges mainly lack annotated audio recordings sound events train evaluate systems. order exploit recordings unsupervised solutions explored clustering sound diarization semi-supervised approaches visual domain video analysis learn combination labeled unlabeled data sources. another technique relies weak labels learning presence absence sounds recording known. videos primary idea associated metadata used assign weak labels used training classiﬁcation models however metadata title keywords description noisy often related visual information rather audio content hence remains seen reliably used true label ground truth train evaluate sound recognition systems. analysis forms major contribution framework proposed paper. paper ﬁrst exploration identify extent search query relates textual metadata used true label sound events segment level youtube videos. study best knowledge unavailable literature. study developed framework large-scale sound event recognition videos consisting three modules crawl hear feedback. crawl youtube videos crawled using search queries corresponding sound event labels keyword sound drawn three datasets. hear datasets used train three multi-class classiﬁers used obtain sound event label prediction million video segments. evaluated performance using search query true label compare subset human labeling collected feedback module. types ground truth exhibit similar performance trend. hence show search query provides reasonable ground truth largescale sound event detection videos. purpose framework sound event labels search queries crawl videos lack true labels segment level; train classiﬁers using labeled audio recognize sound events unlabeled crawled video segments; evaluate system performance using types ground truth search query human labeling collected website. framework described following sections consists three modules illustrated figure hear dataset aggregator organizes different annotated sound event datasets. audio preprocessed acoustic features extracted feature extractor module. sound event classiﬁer features used train classiﬁers unlabeled segments crawled videos. per.. feedback module displays classiﬁer predictions along corresponding audio segments website nels.cs.cmu.edu. using website human feedback collected evaluate classiﬁer performance compare performance search query true label. section explain framework study relation search query presence sound events video segments. achieve objective trained sound event classiﬁers using labeled recordings sourced three different audio-only datasets trained detectors unlabeled crawled youtube video segments. performance evaluated using types true labels search query used retrieve videos collected human inspection. crawl contrast audio-only recordings collecting audio videos poses several challenges. youtube contains massive amount videos proper formulation search query necessary ﬁlter videos higher chances containing desired sound event. typing query composed noun conditioner necessarily fetch video containing sound event associated metadata often corresponds visual content; contrary audio-only websites freesounds.org. therefore modiﬁed query combination keywords <sound event label> sound exampleair conditioner sound. although results empirically improved sound event always found occurring even present sometimes present within short duration. discarded videos longer minutes shorter three seconds either likely contain unrelated sounds short processed. deﬁned search query used crawl videos corresponding sound event labels described following section around hours video processed equally distributed audio event corresponds million video segments seconds each. segments converted -bit encoding mono-channel sampling rate ﬁles. note that acoustic content videos unstructured target sound often overlapping audio noise speech music. dataset aggregator sound events come publicly available annotated datasets tut. partitioned dataset training validation testing sets avoid dealing costly process cross-fold validation testing million segments. classes categories animals natural soundscapes water sounds human non-speech sounds interior/domestic sounds exterior sounds. esc- consists audio segments average duration seconds. urbansoundsk classes conditioner horn children playing bark street music shot drilling engine idling siren jackhammer. urbansoundsk consists audio segments average duration seconds. classes like passing bird singing door banging major sound contexts namely home context residential area. dataset consists audio segments average duration seconds. feature extractor extracted features audio recordings datasets based work provided near stateof-the-art performance time developing experimental results. pipeline agnostic classiﬁer used. audio recordings re-sampled -bit encoding mono channel sampling rate standard format experiments. feed channels learning model. ﬁrst channel comprises log-scaled melspectrograms mel-bands window size size second channel comprises delta coefﬁcients mel-spectrograms. sound event classiﬁers used multi-class classiﬁers using convolutional neural networks datasets based work thus trained models classiﬁcation sound events esc- respectively. used different models dataset using single model audio events presented many challenges like dealing unbalanced classes inconsistency feature normalization resulted performance architecture consisted following layer parameters optimizations done using validation set. input used melﬁlters number frames channels mel-spectra delta features melspectrograms. input window length frames moved frames hence trained predicted audio segments approximately secs. ﬁrst convolutional relu layer consisted ﬁlters rectangular shape allowing slight frequency invariance. pooling applied pool shape stride second convolutional relu layer consisted ﬁlters pooling processing applied fully connected hidden layers neurons relu non-linearity. ﬁnal output layer softmax layer. training performed using keras implementation mini batch stochastic gradient descent even shufﬂed sequential batches nestrov momentum used weight decay layer dropout probability layers. evaluation classiﬁers performance youtube videos segment level lack true labels sound events. hence evaluated classiﬁcation performance types references ground truth search query used retrieve videos human inspection collected website described feedback module. evaluation assuming search query ground truth evaluation process segments retrieved video using given search query barking sound labeled contain barking even might necessarily true. motivation search query reﬂection accumulated metadata tags title description keywords hence wanted degree query relates acoustic content segments. evaluation using human feedback ground truth human inspection needed provide reliable ground truth hence million predicted segments sorted based classiﬁer conﬁdence evaluated group experts tasks related sound recognition. segments classes distributed randomly among human evaluators least people evaluated segment reduce human bias decide based majority vote. segments displayed using similar interface main page nels.cs.cmu.edu difference audio displayed lieu video order avoid revealing cues images title. evaluators choose options correct incorrect whether evaluator claims system’s predicted class present within segment not. results crawled youtube videos main takeaway study exhibited correlation presence sound events video segments corresponding search query illustrated figures note human inspection reliable ground truth search query assumption true class based metadata based visual content. thus precision human feedback expected higher query uncertain could query-based performance better expected considering uncertainty audio content videos. moreover performance follows similar trend human feedback relatively close precision less absolute shows potential search query used class label lieu human annotations. performance three classiﬁers video segments evaluated types ground truth search query human feedback shown figures y-axis performance terms precisionk precision highconﬁdence ranked segments. x-axis high-conﬁdence segments yielded systems. ﬁgures results unstable number audio segments small could vary depending selected audio segments however performance stabilizes grows. stopped results youtube user example tend focus home page results translates equals further evaluated million segments using search query ground truth obtained precision scores esc- datasets. future work involves using crowd-sourcing collect human feedback determine whether performance based human inspection would remain within precision. fig. three classiﬁers search query-based performance follows similar close trend based human feedback. better corresponding random performance shown figure videos established true sound event labels segment level. thus studied relation search queries based sound event labels presence corresponding sound event. developed framework crawl videos using search queries trained classiﬁers audio-only datasets video segments evaluate performance types ground truth search query collected human inspection. showed correlation search query presence sound events video segments. results encourage exploration search query preliminary label true class evaluate sound event classiﬁcation large-scale. results datasets classiﬁcation accuracy three systems corresponding testing sets shown order establish reliable performance match conditions. although three datasets well explored ﬁeld split different manner avoid cross-fold experiments million segments. classiﬁcation accuracy esc- considerably shoou-i jiang zexi xiaojun chang xingzhong chuang zhenzhong zhongwen xuanchong yang informedia trecvid nist trecvid video retrieval evaluation workshop vol. yu-gang jiang xiaohong zeng guangnan ellis shih-fu chang subhabrata bhattacharya mubarak shah columbia-ucf trecvid multimedia event detection combining multiple modalities contextual concepts temporal matching. trecvid vol. zhen-zhong shoou-i alexander hauptmann double fusion multimedia event detection advances multimedia modeling cheng jingen saad omar javed qian amir tamrakar ajay divakaran harpreet sawhney manmatha james allan sri-sarnoff aurora system trecvid multimedia event detection recounting proceedings trecvid pradeep atrey namunu maddage mohan kankanhalli audio based event detection multimedia surveilacoustics speech signal processing lance icassp proceedings. ieee international conference ieee vol. janvier maxime xavier alameda-pineda laurent girin radu horaud sound representation classiﬁcation benchmark domestic robots ieee international conference robotics automation ieee maxime janvier xavier alameda-pineda laurent girinz radu horaud sound-event recognition companion humanoid ieee-ras international conference humanoid robots ieee jose ruiz-mu˜noz mauricio orozco alzate germ´an castellanos-dom´ınguez multiple instance learning-based birdsong classiﬁcation using unsupervised recording segmentation international joint conference artiﬁcial intelligence antti eronen vesa peltonen juha tuomi anssi klapuri seppo fagerlund timo sorsa ga¨etan lorho jyri huopaniemi audio-based context recognition ieee transactions audio speech language processing vol. tuomas virtanen annamaria mesaros toni heittola mark plumbley peter foster emmanouil benetos mathieu lagrange proceedings detection classiﬁcation acoustic scenes events workshop tampere university technology. department signal processing dimitrios giannoulis emmanouil benetos stowell mathias rossignol mathieu lagrange mark plumbley detection classiﬁcation acoustic scenes events ieee aasp challenge ieee waspaa. ieee jort gemmeke daniel ellis dylan freedman jansen wade lawrence channing moore manoj plakal marvin ritter audio ontology human-labeled dataset audio events proc. ieee icassp orleans benjamin elizalde gerald friedland howard ajay divakaran there data like less data percepts video concept detection consumer-produced media proceedings international workshop audio multimedia methods large-scale video analysis. wenjing eduardo coutinho huabin ruan haifeng bj¨orn schuller xiaojie xuan semi-supervised active learning sound classiﬁcation hybrid learning environments plos vol. yang yang lead exceed labor-free video concept learning jointly exploiting videos images ieee conference computer vision pattern recognition june chuang chen lixin duan boqing gong webly-supervised video recognition mutually voting relevant images video frames european conference computer vision shawn hershey sourish chaudhuri daniel ellis jort gemmeke jansen channing moore manoj plakal devin platt saurous bryan seybold architectures large-scale audio classiﬁcation acoustics speech signal processing ieee international conference ieee annamaria mesaros toni heittola tuomas virtanen database acoustic scene classiﬁcation sound event detection european signal processing conference budapest hungary", "year": "2017"}