{"title": "Variational image compression with a scale hyperprior", "tag": "eess", "abstract": " We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder. We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular MS-SSIM index, and yields rate-distortion performance surpassing published ANN-based methods when evaluated using a more traditional metric based on squared error (PSNR). Furthermore, we provide a qualitative comparison of models trained for different distortion metrics. ", "text": "describe end-to-end trainable model image compression based variational autoencoders. model incorporates hyperprior effectively capture spatial dependencies latent representation. hyperprior relates side information concept universal virtually modern image codecs largely unexplored image compression using artiﬁcial neural networks unlike existing autoencoder compression methods model trains complex prior jointly underlying autoencoder. demonstrate model leads state-of-the-art image compression measuring visual quality using popular ms-ssim index yields rate–distortion performance surpassing published ann-based methods evaluated using traditional metric based squared error furthermore provide qualitative comparison models trained different distortion metrics. recent machine learning methods lossy image compression generated signiﬁcant interest machine learning image processing communities like lossy compression methods operate simple principle image typically modeled vector pixel intensities quantized reducing amount information required store transmit introducing error time. typically pixel intensitites quantized directly. rather alternative representation image found vector space quantization takes place representation yielding discrete-valued vector discrete losslessly compressed using entropy coding methods arithmetic coding create bitstream sent channel. entropy coding relies prior probability model quantized representation known encoder decoder class ann-based methods image compression mentioned above entropy model used compress latent representation typically represented joint even fully factorized distribution note need distinguish actual marginal distribution latent representation entropy model entropy model typically assumed parametric form parameters ﬁtted data marginal unknown distribution arising distribution images encoded method used infer alternative representation smallest average code length encoder–decoder pair achieve using shared entropy model given shannon cross entropy distributions note entropy minimized model distribution identical marginal. implies that instance using fully factorized entropy model statistical dependencies exist actual distribution latent representation lead suboptimal compression performance. conventional compression methods increase compression performance transmitting side information additional bits information sent encoder decoder figure left representation transform coding model generative bayesian model corresponding variational inference model. nodes represent random variables parameters arrows indicate conditional dependence them. right diagram showing operational structure compression model. arrows indicate data boxes represent transformations data. boxes labeled represent either addition uniform noise applied training quantization arithmetic coding/decoding testing signal modiﬁcations entropy model intended reduce mismatch. feasible marginal particular image typically varies signiﬁcantly marginal ensemble images compression model designed for. scheme hope amount side information sent smaller average reduction code length achieved matching closely marginal particular image. instance jpeg models images independent ﬁxed-size blocks pixels. however image structure large homogeneous regions efﬁciently represented considering larger blocks time. reason recent methods hevc partition image variablesize blocks convey partition structure decoder side information compress block representations using partitioning. entropy model jpeg always factorized groups elements whereas factorization variable hevc. hevc decoder needs decode side information ﬁrst correct entropy model decode block representations. since encoder free select partitioning optimizes entropy model image scheme used achieve efﬁcient compression. conventional compression methods structure side information hand-designed. contrast model present paper essentially learns latent representation entropy model underlying compression model learns representation image. model optimized end-to-end minimizes total expected code length learning balance amount side information expected improvement entropy model. done expressing problem formally terms variational autoencoders probabilistic generative models augmented approximate inference models ballé theis previously noted autoencoder-based compression methods formally equivalent vaes entropy model described above corresponds prior latent representation. here formalism show side information viewed prior parameters entropy model making hyperpriors latent representation. speciﬁcally extend model presented ballé fully factorized prior hyperprior captures fact spatially neighboring elements latent representation tend vary together scales. demonstrate extended model leads state-ofthe-art image compression performance measured using ms-ssim quality index furthermore provides signiﬁcantly better rate–distortion performance compared ann-based methods measured using peak signal-to-noise ratio metric based mean squared error. finally present qualitative comparison effects training model class using different distortion losses. figure left image kodak dataset. middle left visualization subset latent representation image learned factorized-prior model. note clearly visible structure around edges textured regions indicating dependency structure exists marginal represented factorized prior. middle right standard deviations latents predicted model augmented hyperprior. right latents divided elementwise standard deviation. note reduces apparent structure indicating structure captured prior. quantized form discrete-valued losslessly compressed using entropy coding techniques arithmetic coding transmitted sequence bits. side decoder recovers compressed signal subjects parametric synthesis transform recover reconstructed image context paper think transforms generic parameterized functions artiﬁcial neural networks rather linear transforms traditional compression methods. parameters encapsulate weights neurons etc. quantization introduces error tolerated context lossy compression giving rise rate–distortion optimization problem. rate expected code length compressed representation assuming entropy coding technique operating efﬁciently written cross entropy represents quantization function entropy model described introduction. context marginal distribution latent representation arises image distribution properties analysis transform. distortion expected difference reconstruction original image measured norm perceptual metric. coarseness quantization alternatively warping representation implied analysis synthesis transforms affects rate distortion leading trade-off higher rate allows lower distortion vice versa. various compression methods viewed minimizing weighted quantities. formally parameterize problem weight distortion term. different applications require different trade-offs hence different values order able gradient descent methods optimize performance model parameters transforms problem needs relaxed quantization gradients respect zero almost everywhere. approximations investigated include substituting gradient quantizer substituting additive uniform noise quantizer training here follow latter method switches back actual quantization applying model compression method. denote quantities derived approximation tilde opposed hat; instance represents noisy representation quantized representation. inference model synthesis transform linked generative model analysis transform inference model variational inference goal approximate true posterior ˜y|x assumed intractable parametric variational density minimizing expectation kullback–leibler divergence data distribution matching parametric density functions transform coding framework appreciate minimization divergence equivalent optimizing compression model rate–distortion performance. indicated ﬁrst term evaluate zero second third term correspond weighted distortion rate respectively. let’s take closer look terms. denotes uniform distribution centered since width uniform distribution constant ﬁrst term divergence technically evaluates zero dropped loss function. likelihood works squared difference output synthesis transform weighted minimizing second term divergence thus equivalent minimizing expected distortion reconstructed image. squared error loss equivalent choosing gaussian distribution; distortion metrics equivalent distribution guaranteed metrics necessarily correspond normalized density function. third term divergence easily seen identical cross entropy marginal ex∼px prior reﬂects cost encoding produced inference model assuming entropy model. note term represents differential cross entropy opposed shannon entropy uniform noise approximation. given assumptions however close approximations similarly ballé model prior using non-parametric fully factorized density model vectors encapsulate parameters univariate distribution pyi|ψ note convolve non-parametric density standard uniform density. enable better match prior marginal details appendix shorthand refer case factorized-prior model. center panel ﬁgure visualizes subset quantized responses compression model trained way. visually clear choice factorized distribution stark simpliﬁcation non-zero responses highly clustered areas high contrast; i.e. around edges within textured regions. implies probabilistic coupling responses represented models fully factorized prior. would expect better model consequently better compression performance model captured dependencies. introducing hyperprior elegant achieving this. evident center panel ﬁgure signiﬁcant spatial dependencies among elements notably scales appear coupled spatially. standard model dependencies target variables introduce latent variables conditioned target variables assumed independent introduce additional random variables capture spatial dependencies propose extend model follows element modeled zero-mean gaussian standard deviation standard deviations predicted applying parametric transform extend inference model simply stacking another parametric transform effectively creating single joint factorized variational posterior follows follows intuition responses sufﬁcient estimate spatial distribution standard deviations. prior beliefs hyperprior model using non-parametric fully factorized density model previously used again ﬁrst term zero since product uniform densities unit width. second term encapsulates distortion before. third fourth term represent cross entropies encoding respectively. analogy traditional transform coding fourth term seen representing side information. right-hand panel ﬁgure illustrates model used compression method. encoder subjects input image yielding responses spatially varying standard deviations. responses summarizing distribution standard deviations figure network architecture hyperprior model. left side shows image autoencoder architecture right side corresponds autoencoder implementing hyperprior. factorized-prior model uses identical architecture analysis synthesis transforms represents quantization represent arithmetic encoder arithmetic decoder respectively. convolution parameters denoted number ﬁlters kernel support height kernel support width downupsampling stride indicates upsampling downsampling. chosen dependent lower values higher values. quantized compressed transmitted side information. encoder uses quantized vector estimate spatial distribution standard deviations uses compress transmit quantized image representation decoder ﬁrst recovers compressed signal. uses obtain provides correct probability estimates successfully recover well. feeds obtain reconstructed image. transforms alternating compositions linear nonlinear functions common artiﬁcial neural networks speciﬁcally composed convolutions gdn/igdn nonlinearities implement local divisive normalization type transformation shown particularly suitable density modeling compression images composed convolutions rectiﬁers make hyperprior model factorized-prior model comparable chose identical architectures shown ﬁgure maintain translation invariance across model elements channel index assumed follow univariate distribution. allows model used arbitrary image sizes. arithmetic coding implemented using simple non-adaptive binary arithmetic coder. element independently converted representation binary integer arithmetically encoded signiﬁcant least signiﬁcant bit. since spatial distribution standard deviations known decoder time decoding attempted arithmetic coder need handle conditional dependencies. also need separately trained since binary probabilities needed encoding direct function probability mass functions probability mass functions turn direct functions models trained body color jpeg images heights/widths pixels comprising approximately million images scraped world wide web. images excessive saturation screened reduce number non-photographic images. reduce existing compression artifacts images downsampled randomized factor minimum height width equaled pixels. then randomly placed pixel crops downsampled images extracted. minibatches crops time used perform stochastic gradient descent using adam algorithm learning rate common machine learning techniques batch normalization learning rate decay found beneﬁcial effect setup trained total separate models half models hyperprior half without; half models mean squared error distortion metric half ms-ssim distortion index ﬁnally combinations different values order cover range rate– distortion tradeoffs. evaluate compression performance models publicly available kodak dataset summarized rate–distortion curves shown ﬁgure results individual images well summarized comparisons wider range existing methods provided appendices quantify image distortion using peak signal-to-noise ratio ms-ssim. curve represents rate–distortion tradeoffs given models across different values since ms-ssim yields values compared methods achieve values well converted quantity decibels order improve legibility. interestingly maybe surprisingly results differ substantially depending distortion metric used loss function training. measuring distortion psnr models perform poorly optimized ms-ssim. however optimized squared error model factorized prior outperforms existing conventional codecs jpeg well ann-based methods trained squared error note published ann-based methods shown underperform compared ones shown made data available factorized prior model outperform encapsulation hevc targeted still image compression. training hyperprior model squared error close performance better results higher rates lower ones still substantially outperforming published ann-based methods. measuring distortion using ms-ssim conventional codecs jpeg lower performance ranking. surprising since methods optimized squared error best knowledge state compression performance terms ms-ssim rippel bourdev surprisingly matched factorized prior model even though model conceptually much complex hyperprior model adds gains across rate–distortion tradeoffs consistently surpassing state art. figure rate–distortion curves aggregated kodak dataset. plot shows peak signalto-noise ratios function rate bottom plot shows ms-ssim values converted decibels msssim value range zero one). observe matching training loss metric used evaluation crucial optimize performance. hyperprior model trained squared error outperforms ann-based methods terms psnr approximates hevc performance. terms ms-ssim hyperprior model consistently outperforms conventional codecs well rippel bourdev current state-of-the-art model metric. note psnr plot aggregates curves equal values ms-ssim plot aggregates equal rates order provide fair comparison stateof-the-art methods. refer ﬁgures appendix full-page curves include wider range compression methods. figure visual artifacts generated rates depend training loss. ﬁgure generated hyperprior model using ms-ssim loss bottom ﬁgure trained using squared loss. figure amount side information function total rate hyperprior model optimized squared error averaged kodak normalized pixel. small fraction total rate used encoding pressed similar rates models optimized ms-ssim distortion loss compared squared loss overall ﬁdelity terms much detail preserved appears similar. however spatial distribution detail changes substantially. ms-ssim like predecessor ssim metric designed model human visual contrast perception. compared squared loss effect attenuating error image regions high contrast boosting error regions contrast human visibility threshold varies local contrast. behavior yields good results images containing textures different local contrast however frequently expected also produce results inconsistent human expectations image show ﬁgure compression model trained ms-ssim assigns detail grass removes detail text side airplane semantic relevance often assigned high-contrast areas squared-error optimized models produce subjectively better reconstructions cases. important note neither distortion metric sophisticated enough capture image semantics makes choice distortion loss difﬁcult one. prior work ann-based image compression shown extending transform coding concept linear nonlinear transforms fundamentally improves qualitative nature compression artifacts appears nonlinear transforms higher computational capacity adapt better statistics natural images imitating properties data distribution better linear transforms. comparing image reconstructions visually models without hyperprior changes qualitative nature artifacts. rather hyperprior model simply tends produce image reconstructions improved detail lower rate corresponding model factorized prior. figure shows much total rate hyperprior model uses side information. amount side information grows total rate stays even highest total rates. still resulting improvement prior enables performance gains factorized-prior model shown ﬁgure note architecture models explicitly constrain rates way. illustrated trade-off allocating bits encoding simply result optimizing loss function given implement variational image compression model conceptually identical model presented ballé augment powerful entropy model introducing hyperprior local scale parameters latent representation. hyperprior trained end-to-end rest model. like recent image compression methods based anns method directly optimized distortion losses complex pixel-wise losses mean squared error. ﬁrst studies emerging ﬁeld examine effect optimizing popular perceptual metrics ms-ssim compare optimizing squared loss. note ballé compare models trained different metrics results limited choice transforms. figure demonstrates results show signiﬁcant variation terms visual quality depending image content implies unless human rating santurkar formulate compression method hybrid vae-gan framework adopting stepwise training scheme decoder ﬁrst trained using adversarial loss. ﬁxed encoder trained minimize reconstruction error. rippel bourdev also employ adversarial approach weighted combination ms-ssim adversarial loss. baig torresani propose compression scheme based colorization color channels predicted luminance channel making model speciﬁc side information. luminance channel compressed using traditional method. proposed method exhibits signiﬁcant color distortions rates limited compression method used luminance channel. early exploration hierarchical generative models compression small images found gregor however aspect quantization thoroughly considered hence actual compression method designed. theis approach problem generating gradient descent directions quantization functions replacing gradient identity function derive differentiable upper bound discrete rate term. ballé instead replace quantizer additive uniform noise training discrete rate term differential entropy. method doesn’t offer bound approximation establishes direct relationship discrete continuous prior distributions enables direct evaluation discrete prior function latents hence makes hyperprior feasible practice. quality approximation veriﬁed empirically ballé wainwright simoncelli observe linear ﬁlter responses follow heavy-tailed marginal distributions represented conditionally gaussian groups neighboring coefﬁcients linked common scale multiplier. distributions ﬁlter responses modeled gaussian scale mixtures. simoncelli extend model spatially localized groups wavelet coefﬁcients global image model. model seen extension this ﬁlter responses replaced responses nonlinear transform approximate inference model added. theis directly gaussian scale mixtures form fully factorized prior. presented form variational model perhaps closely related ladder vaes however choose different parametric forms accommodate approximation quantization entropy coding process. classical transform coding methods compression researchers exploited statistical dependency latent variables carefully hand-engineering entropy codes modeling dependencies quantized regime presents much difﬁcult engineering problem relying fully factorized entropy model; transitioning nonlinear transforms whose parameters determined training complicates problem. toderici model images directly binarized latent representation technically removes need separate entropy coding step. however corresponds inﬂexible entropy model model apparently compensates using higher capacity transforms johnston improve method designing adaptive entropy model. however entropy model included rate term training transforms hence feedback returned entropy model back transforms training. breaks paradigm end-to-end optimization stand better compression performance. similarly rippel bourdev hand-designed energy function without trainable parameters prior training autoencoder design adaptive entropy model post hoc. fact factorized prior model matches performance method optimized metric point towards disconnect. ágústsson extend fully-factorized prior model proposing vector quantization small subtensors latent representation effectively relaxes factorization. train method end-to-end. models presented make type nonlinearity implementing local normalization. part gaussianizing transformation shown efﬁcient terms number parameters removing statistical dependencies image data pointwise nonlinearities furthermore long history generative models starting independent component analysis successfully recover factorized representations maximizing likelihood assuming fully factorized prior. despite facts observe signiﬁcant dependencies neighboring elements remain latent representation compression models even though took care impose constraints transforms might reduce capacity factorize representation attribute fact rate–distortion loss unlike maximum likelihood loss trades rate term expected distortion. easy increasing values rate term containing factorized prior becomes less less important. hence questionable whether rate–distortion optimality implies full independence representation least arbitrary values regardless this fact hyperprior models consistently outperform models factorized prior illustrate important compression method reduce mismatch prior marginal model trained appropriate loss capacity surpass state ms-ssim quite reach performance heavily optimized traditional method psnr discrepancy indicate methods based anns reached expressive power traditional methods. such introduction hyperprior traditional terms side information elegant introducing ﬂexible priors step right direction. ágústsson eiríkur soft-to-hard vector quantization end-to-end learning compressible representations. advances neural information processing systems asuni giachetti testimages large-scale archive testing visual devices basic image processing algorithms. proc. stag smart tools apps graphics. baig mohammad haris lorenzo torresani multiple hypothesis colorization application image compression. computer vision image understanding ./j.cviu.... ballé johannes valero laparra eero simoncelli density modeling images using generalized normalization transformation. arxiv e-prints. presented int. conf. learning representations. arxiv siwei eero simoncelli modeling multiscale subbands photographic images fields gaussian scale mixtures. ieee transactions pattern analysis machine intelligence ./tpami... wang zhou eero simoncelli alan conrad bovik multi-scale structural similarity image quality assessment. conf. rec. asilomar conf. signals systems computers. ./acssc... figure non-parametric model gaussian mixture distribution. gray plots illustrate convergence model. non-parametric model able produce good ground truth density. ballé non-parametric piecewise linear density model represent factor fully factorized prior. increasing number samples unit interval principle used model univariate density arbitrary precision. however practical problems range values non-zero probability must ﬁnite known ahead time implementation non-trivial existing automatic differentiation frameworks numerical issues normalizing density fact typically relies discrete operations array indexing. compression models presented paper instead following model based cumulative. deﬁne density using cumulative satisfying following constraints note monotonicity constraint cumulative established requiring density function non-negative. suppose cumulative composition functions. density written using chain rule calculus general univariate domain range need dimensional guarantee density need range ensure that require jacobian elements non-negative. matrix product computing non-negative well deﬁned valid density. effective choice following vector denotes elementwise multiplication. rationale behind particular nonlinearity allows expand contract space near controls rate expansion contraction ﬁxed positive value peaks density would become easier model troughs. seem deﬁne density function explicit derivative; however automatic differentiation framework operation easy implement resulting density function normalized construction. found model well arbitrary densities perform well piecewise linear model context compression models. experiments paper used dimensionalities univariate density model associated parameters model prior ˜y|˜z hyperprior using densities convolved standard uniform density function. ensure priors enough ﬂexibility match variational posterior this consider cases beneﬁcial terms rate– distortion performance model disable part latent representation leading lower effective dimensionality model architecture for. simplicity notation let’s assume variational posterior prior dimension collapsed. case converges always producing constant value corresponding dimensions entropy evaluate zero bits quantized representation deterministic cross entropy evaluate zero however prior needs ﬂexible enough assume shape posterior unit-width uniform density. figure fitting density model described previous section uniform distribution without convolving model uniform density. gray plots illustrate convergence model. assumes smoothness thus fails adequate uniform steep edges augmented model almost perfectly. inﬁnitely steep edges uniform distribution corner case gaussian density model also non-parametric model described appendix this incorporate added noise directly prior/hyperprior convolving underlying density model standard uniform cumulative underlying density model. whatever underlying density letting scale towards zero makes approach unit-width uniform density. since non-parametric model deﬁned cumulative cumulative gaussian available computational frameworks solution easy implement practice. results seem indicate certain degree statistical dependency latent image representation preferred rate–distortion objective hyperprior model performs better embracing this. however possible dependencies remain simply analysis figure rate–distortion curves factorized-prior models differing transform capacity note performance gains increased number ﬁlters stagnates λ-dependent saturation point reached. example moving ﬁlters makes signiﬁcant difference moving yields negligible gain beneﬁt going synthesis transforms enough capacity factorize image representation training algorithm succeed ﬁnding global optimum. although impossible fully control this attempted minimize chances capacity limitations transforms lead wrong conclusions carefully selecting number ﬁlters across layers transforms established previous experiments that given exist certain number ﬁlters layer performance saturates gains achieved increasing optimal number ﬁlters increases indicating models higher rates require higher transform capacities. based previous experiments attempted choose values close point saturation little higher order control capacity limitations minimizing training time. additionally found allowing somewhat wider bottleneck helps achieve comparable performance overall lower used choosing model architectures. table lists encoding decoding times method python tensorflow implementation well different number ﬁlters layer averaged kodak tecnick datasets. note performance optimization attempted. particular optimize metaparameter choices computational complexity. rather chose number ﬁlters high enough rule bottlenecks transforms described previous section. arithmetic coding implemented customized operator c++. thus measurements represent proof method feasible utility meaningful comparisons methods limited. average increase runtime hyperprior model compared factorized-prior model plots ﬁgures show results ﬁgure provide comparisons wider array compression methods. note method aggregate rate–distortion points across images differs psnr ms-ssim plots latter interpolate curves image using cubic splines predeﬁned rates average across equal rates. former interpolation used averaging rate distortion measurements across equal values noted ballé directly comparing curves different methods aggregation give misleading results. this match aggregation method data available current state ms-ssim). ultimately comparison based individual images provided section considered reliable; however data individual images rippel bourdev available. figure rate–distortion curves psnr covering wide range conventional ann-based compression methods. hyperprior model outperforms conventional codecs well ann-based methods wide margin. figure rate–distortion curves ms-ssim covering wide range conventional annbased compression methods. trained ms-ssim hyperprior model outperforms rippel bourdev current state consistently across rates. note even trained using squared loss hyperprior model yields higher ms-ssim scores conventional methods. figure rate–distortion curves psnr covering wide range conventional ann-based compression methods. results qualitatively similar results kodak. figure rate–distortion curves ms-ssim covering wide range conventional annbased compression methods. results qualitatively similar results kodak.", "year": "2018"}