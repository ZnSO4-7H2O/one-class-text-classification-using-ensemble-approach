{"title": "Human and Machine Speaker Recognition Based on Short Trivial Events", "tag": "eess", "abstract": " Trivial events are ubiquitous in human to human conversations, e.g., cough, laugh and sniff. Compared to regular speech, these trivial events are usually short and unclear, thus generally regarded as not speaker discriminative and so are largely ignored by present speaker recognition research. However, these trivial events are highly valuable in some particular circumstances such as forensic examination, as they are less subjected to intentional change, so can be used to discover the genuine speaker from disguised speech. In this paper, we collect a trivial event speech database that involves 75 speakers and 6 types of events, and report preliminary speaker recognition results on this database, by both human listeners and machines. Particularly, the deep feature learning technique recently proposed by our group is utilized to analyze and recognize the trivial events, which leads to acceptable equal error rates (EERs) despite the extremely short durations (0.2-0.5 seconds) of these events. Comparing different types of events, 'hmm' seems more speaker discriminative. ", "text": "proposed perform kind speech including statistical model approach gained popularity neural model approach emerged recently attracted much interest. despite signiﬁcant progress achieved regular speech research non-linguistic part speech signals still limited. example cough laugh talking others ‘tsk-tsk’ ‘hmm’ listening others. events produced different personal habits contain little linguistic information. however convey information speakers. example recognize person even laugh familiar him/her. non-linguistic non-regular events occur ubiquitously conversations call ‘trivial events’. typical trivial events include cough laugh ‘ahem’ etc. value trivial events events resistant potential disguise. forensic examination example suspects intentionally change voices counteract voiceprint testing largely fool human listeners cause failures existing system. however trivial events much harder counterfeited speaker makes possible events discover true speaker disguised speech. show disguised speech deceives humans state-of-theart techniques section interesting question type trivial event conveys speaker information? moreover identify speakers events human machine? previous work studied three trivial events cough laugh ‘wei’ found convolutional time-delay deep neural network unexpected high recognition accuracy obtained equal error rate reaches cough seconds. good performance largely attributed deep speaker feature learning technique proposed recently. paper extend previous work several aspects extend study types trivial events i.e. cough laugh ‘hmm’ ‘tsk-tsk’ ‘ahem’ sniff; collect trivial event speech database release public human speech often events call trivial events e.g. cough laugh sniff. compared regular speech trivial events usually short variable thus generally regarded speaker discriminative largely ignored present speaker recognition research. however trivial events highly valuable particular circumstances forensic examination less subjected intentional change used discover genuine speaker disguised speech. paper collect trivial event speech database involves speakers types events report preliminary speaker recognition results database human listeners machines. particularly deep feature learning technique recently proposed group utilized analyze recognize trivial events leading acceptable equal error rates ranging despite extremely short durations events. comparing different types events ‘hmm’ seems speaker discriminative. biometric authentication highly important security reality cyberspace. among various biometrics iris palmprint ﬁngerprint face voiceprint received much attention recently partly convenience non-intrusiveness. decades research speaker recognition voiceprint achieved remarkable improvement. present research works ‘regular speech’ i.e. speech intentionally produced people involving clear linguistic content. type speech rich speaker information obtained vocal fold vibration vocal tract modulation speaker identiﬁablility generally acceptable. many algorithms work supported part national natural science foundation china projects part national basic research program china grant miao zhang xiaofei kang joint ﬁrst authors. corresponding author dong wang organization paper follows deep feature learning approach brieﬂy described section trivial event speech database cslt-trivial-i presented section performance human machine tests reported section conclusions discussions presented section speaker recognition trivial events still limited. relevant work noticed hansen analyzed acoustic properties scream speech studied performance type speech using recognition system based gaussian mixture models-universal background model. signiﬁcant performance reduction reported compared performance regular speech. studies don’t focus trivial speech events deﬁned still related work. example investigated impact whisper speech hanilc¸i investigated impact loud speech. existing speaker recognition techniques based statistical models e.g. gaussian mixture model-universal background model framework subsequent subspace models joint factor analysis approach i-vector model. additional gains obtained discriminative models various normalization techniques plda shared property statistical methods acoustic features e.g. popular frequency cepstral coefﬁcients feature rely long speech segments discover distributional patterns individual speakers. since trivial events short statistical models suitable represent them. neural model approach gained much attention recently. compared statistical model approach neural approach focuses learning frame-level speaker features hence suitable dealing short speech segments e.g. trivial events. approach ﬁrst proposed ehsan regular deep neural network trained discriminate speakers training data conditioned input speech frames. frame-level features extracted last hidden layer utterance-based representation called ‘d-vector’ derived averaging frame-level features. recently proposed convolutional time-delay structure quality learned speaker features signiﬁcantly improved. particularly found features achieve remarkable performance short speech segments. property employed recognize trivial events previous study appropriate speech corpus ﬁrst concern analysis conducted trivial speech events. unfortunately trivial event databases publicly available present. exception ut-nonspeech corpus collected scream detection recognition corpus contains screams coughs whistles. interested ubiquitous events easy changed speakers intentionally complicated database required. therefore decided construct database release public usage. database denoted cslt-trivial-i. collect data designed mobile application distributed people agreed participate. application asked participants utter types trivial events random order event occurred times randomly. random order ensures reasonable variance recordings event. sampling rate recordings precision samples bits. received recordings participants. participants ranges recordings manually checked recordings clear channel effect deleted. finally speech segments purged single event retained segment. manual check recordings persons remained segments event person. table presents data proﬁle purged database. application used collecting cslt-trivialused collect recordings disguise database. recording participants instructed best counterfeit voices recording disguise speech. recording application asked participants pronounce sentences involving words. sentence spoken twice time normal style time intentional disguise. manual check segments much channel effect removed. manual check recordings speakers remained. database denoted cslt-disguise-i. table presents data proﬁle details. section reports experiments. ﬁrst present details systems built investigation based i-vector model based deep speaker feature learning furthermore performance systems cslttrivial-i reported compared performance human listeners. finally disguise detection experiment conducted cslt-disguise-i reported demonstrates speech disguise fools humans existing systems. purpose comparison build systems i-vector system d-vector system. i-vector system input feature involves -dimensional mfccs plus energy augmented ﬁrst second order derivatives. composed gaussian components dimensionality i-vector space three scoring methods used cosine distance cosine distance projection plda. dimensionality projection space plda used scoring i-vectors length-normalized. system trained using kaldi recipe. frames total. number output units corresponding number speakers training data. frame-level speaker features extracted last hidden layer d-vector utterance derived averaging frame-level speaker features. scoring methods used i-vector system also used dvector system test including cosine distance plda. speech-ocean datatang database used training recorded telephone sampling rate khz. database consists speakers chinese utterances. training used train matrix lda/plda models ivector system well ct-dnn model d-vector system. ﬁrst experiment evaluate performance trivial events human listeners systems. cslt-trivial-i database used conduct test. consists speakers types trivial events type speaker involving segments. original data recording matches speech-ocean datatang database. listener presented yes/no questions questions event type. question listener asked listen speech segments randomly sampled event type probability speaker. listeners allowed perform test multiple times. collected test sessions amounting trials total. performance evaluated terms detection error rate proportion incorrect answers within whole trials including false alarms false rejections. results shown table seen humans tell speaker short trivial event particularly nasal sound ‘hmm’. cough laugh ‘ahem’ humans obtain speaker information performance lower. ‘tsk-tsk’ sniff performance answers given listeners almost random. expected extent types events sound rather weak producing much vocal fold vocal tract. system large margin conﬁrming deep speaker feature learning approach suitable statistical model approach recognizing short speech segments. comparing different events found ‘hmm’ conveys speaker information cough laugh ‘ahem’ less informative. ‘tsk-tsk’ sniff least discriminative. observations consistent results human test. moreover found d-vector systems discriminative normalization approaches plda provide clear advantage ’hmm’ sniff. possible reason little intra-speaker variances involved types events statistical based discrimination helpful. comparing humans machines best machine system i.e. d-vector system highly competitive. although values directly comparable results still show roughly almost types trivial events d-vector system makes fewer mistakes humans. particularly events humans perform worst i.e. ‘tsk-tsk’ sniff machines work much better. although listeners invited professional speech scientists results affected audio devices human listeners used results still provide strong evidence machines potentially better human beings listening trivial events. second experiment examine humans machines discriminate disguised speech. human test listener presented trials containing samples speaker sample disguised version. listener asked tell samples speaker. avoid bias listeners informed speech samples disguised. trials also involve imposter speech trials used inject noise test counted ﬁnal result. collected trails total result indicates human listeners largely fail discriminating disguised speech. results systems reported table found machines better humans discriminating disguised speech error rates still high. again d-vector system performs better fig. deep speaker features normal speech disguised speech speaker sentence plotted t-sne. picture represents single person normal disguised speech represented darker lighter curves respectively. observe impact speech disguise intuitively plot deep speaker features produced d-vector system -dimensional space using t-sne. results shown fig. discrepancy normal disguised speech highly speaker-dependent speakers good voice counterfeiters speakers well. paper studied compared performance human listeners machines speaker recognition task trivial speech events. experiments types trivial events demonstrated humans machines discriminate speakers extent trivial events particularly events involving clear vocal tract activities e.g. ‘hmm’. additionally deep speaker feature learning approach works much better conventional statistical model approach task cases outperforms human listeners. also tested performance humans machines disguised speech found speech disguise place serious challenge them. john hansen mahesh kumar nandwana navid shokouhi analysis human scream impact text-independent speaker veriﬁcation journal acoustical society america vol. cemal hanilc¸i tomi kinnunen rahim saeidi jouni pohjalainen paavo alku figen ertas speaker identiﬁcation shouted speech analysis compensation acoustics speech signal processing ieee international conference ieee nicolas scheffer luciana ferrer mitchell novel scheme speaker recognition mclaren using phonetically-aware deep neural network acoustics speech signal processing ieee international conference ieee william campbell douglas sturim douglas reynolds support vector machines using supervectors speaker veriﬁcation ieee signal processing letters vol. daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannemann petr motlicek yanmin qian petr schwarz kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. ieee signal processing society number epfl-conf-. patrick kenny gilles boulianne pierre ouellet pierre dumouchel joint factor analysis versus eigenchannels speaker recognition ieee transactions audio speech language processing vol. najim dehak patrick kenny r´eda dehak pierre dumouchel pierre ouellet front-end factor analysis speaker veriﬁcation ieee transactions audio speech language processing vol. ehsan variani erik mcdermott ignacio lopez moreno javier gonzalez-dominguez deep neural networks small footprint text-dependent speaker veriﬁcation acoustics speech signal processing ieee international conference ieee georg heigold ignacio moreno samy bengio end-to-end text-dependent speaker noam shazeer veriﬁcation acoustics speech signal processing ieee international conference ieee", "year": "2017"}