{"title": "Neural Voice Cloning with a Few Samples", "tag": "eess", "abstract": " Voice cloning is a highly desired feature for personalized speech interfaces. Neural network based speech synthesis has been shown to generate high quality speech for a large number of speakers. In this paper, we introduce a neural voice cloning system that takes a few audio samples as input. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model with a few cloning samples. Speaker encoding is based on training a separate model to directly infer a new speaker embedding from cloning audios and to be used with a multi-speaker generative model. In terms of naturalness of the speech and its similarity to original speaker, both approaches can achieve good performance, even with very few cloning audios. While speaker adaptation can achieve better naturalness and similarity, the cloning time or required memory for the speaker encoding approach is significantly less, making it favorable for low-resource deployment. ", "text": "voice cloning highly desired feature personalized speech interfaces. introduce neural voice cloning system learns synthesize person’s voice audio samples. study approaches speaker adaptation speaker encoding. speaker adaptation based ﬁne-tuning multi-speaker generative model. speaker encoding based training separate model directly infer speaker embedding applied multi-speaker generative model. terms naturalness speech similarity original speaker approaches achieve good performance even cloning audios. speaker adaptation achieve slightly better naturalness similarity cloning time required memory speaker encoding approach signiﬁcantly less making favorable low-resource deployment. generative models based deep neural networks successfully applied many domains image generation speech synthesis language modeling deep neural networks capable modeling complex data distributions conditioned external inputs control content style generated samples. speech synthesis generative models conditioned text speaker identity text carries linguistic information controls content generated speech speaker identity captures characteristics pitch speech rate accent. approach multi-speaker speech synthesis jointly train generative model speaker embeddings triplets text audio speaker identity idea encode speaker-dependent information low-dimensional embeddings sharing majority model parameters across speakers. limitation methods generate speech observed speakers training. intriguing task learn voice unseen speaker speech samples a.k.a. voice cloning corresponds few-shot generative modeling speech conditioned speaker identity. generative model trained scratch large amount audio samples focus voice cloning speaker minutes even seconds data. challenging model learn speaker characteristics limited amount data still generalize unseen texts. ∗equal contribution cloned audio samples found https//audiodemos.github.io single speaker model require hours training data multi-speaker model speakers requires minutes data speaker. demonstrate analyze strength speaker adaption approaches voice cloning based ﬁne-tuning pre-trained multi-speaker model unseen speaker using samples. propose novel speaker encoding approach provides comparable naturalness similarity subjective evaluations yielding signiﬁcantly less cloning time computational resource requirements. work builds upon state-of-the-art neural speech synthesis few-shot generative modeling. neural speech synthesis recently surge interest speech synthesis neural networks including deep voice deep voice deep voice wavenet samplernn charwav tacotron voiceloop among methods sequence-to-sequence models attention mechanism much simpler pipeline produce natural speech work deep voice baseline multi-speaker model simple convolutional architecture high efﬁciency training fast model adaptation. noted techniques seamlessly applied neural speech synthesis models. few-shot generative modeling humans learn generative tasks examples motivates research few-shot generative models. early studies mostly focus bayesian methods. example hierarchical bayesian models used exploit compositionality causality few-shot generation characters words speech recently deep neural networks achieve great successes few-shot density estimation conditional image generation great potential composition learned representation. work investigate few-shot generative modeling speech conditioned particular speaker. train separate speaker encoding network directly predict parameters multi-speaker generative model taking unsubscribed audio samples inputs. speaker-dependent speech processing speaker-dependent modeling widely studied automatic speech recognition goal improving performance exploiting speaker characteristics. particular groups methods neural alignment voice cloning approaches. ﬁrst group speaker adaptation whole-model portion model merely speaker embedding speaker adaptation voice cloning vein approaches differences arise text-to-speech speech-to-text considered second group based training models jointly embeddings. extraction embeddings based i-vectors bottleneck layers neural networks trained classiﬁcation loss although general idea speaker encoding also based extracting embeddings directly major distinction speaker encoder models trained objective function directly related speech synthesis. lastly speaker-dependent modeling essential multi-speaker speech synthesis. using i-vectors represent speaker-dependent characteristics approach however limitation separately trained objective directly related speech synthesis. also accurately extracted small amount audio another approach multi-speaker speech synthesis using trainable speaker embeddings randomly initialized jointly optimized generative loss function. voice conversion closely related task voice cloning voice conversion. goal voice conversion modify utterance source speaker make sound like target speaker keeping linguistic contents unchanged. unlike voice cloning voice conversion systems need generalize unseen texts. common approach dynamic frequency warping align spectra different speakers. agiomyrgiannakis roupakia proposes dynamic programming algorithm simultaneously estimates optimal frequency warping weighting transform matching source target speakers using matching-minimization algorithm. uses spectral conversion approach integrated locally linear embeddings manifold learning. also approaches model spectral conversion using neural networks models typically trained large amount audio pairs target source speakers. consider multi-speaker generative model takes text speaker identity trainable parameters model parameterized esi. latter denotes trainable speaker embedding corresponding optimized minimizing loss function penalizes difference generated ground-truth audios embeddings shown effectively capture speaker characteristics low-dimensional vectors despite training generative loss discriminative properties observed speaker embedding space voice cloning extract speaker characteristics unseen speaker cloning audios generate audio given text speaker. performance metrics generated audio speech naturalness speaker similarity approaches neural voice cloning summarized fig. explained following sections. idea speaker adaptation ﬁne-tune trained multi-speaker model unseen speaker using audio-text pairs. fine-tuning applied either speaker embedding whole model. embedding-only adaptation following objective although entire model provides degrees freedom speaker adaptation optimization challenging small amount cloning data. early stopping required avoid overﬁtting. propose speaker encoding method directly estimate speaker embedding audio samples unseen speaker. model require ﬁne-tuning voice cloning. thus model used unseen speakers. speaker encoder takes cloning audio samples estimates speaker model parametrized ideally speaker encoder jointly trained multi-speaker generative model scratch loss function deﬁned generated audio note speaker encoder trained speakers multi-speaker generative model. training cloning audio samples randomly sampled training speaker inference audio samples target speaker used compute observed optimization challenges joint training scratch speaker encoder tends estimate average voice minimize overall generative loss. possible solution introduce discriminative loss functions intermediate embeddings generated audios. case however approaches slightly improve speaker differences. instead propose multi-speaker generative model then speaker encoder trained loss predict embeddings sampled cloning audios initialization. fine-tuning encourages generative model compensate embedding estimation errors reduce attention problems. however generative loss still dominates learning speaker differences generated audios slightly reduced well experimented classiﬁcation loss mapping embeddings labels softmax layer. experimented integrating pre-trained classiﬁer encourage discrimination generated audios. cloning sample attention considering different cloning audios contain different amount speaker information multi-head self-attention mechanism compute weights different audios aggregated embeddings. speaker classiﬁer determines speaker audio sample belongs voice cloning evaluation speaker classiﬁer trained speakers used cloning. high-quality voice cloning would result high classiﬁcation accuracy. architecture composed similar spectral temporal processing layers fig. additional embedding layer softmax function. speaker veriﬁcation task authenticating claimed identity speaker based test audio enrolled audios speaker. particular performs binary classiﬁcation identify whether test audio enrolled audios speaker consider end-to-end text-independent speaker veriﬁcation model speaker veriﬁcation model trained multi-speaker dataset used verify cloned audio ground-truth audio speaker. unlike speaker classiﬁcation approach speaker veriﬁcation model require training audios target speaker cloning hence used unseen speakers samples. quantitative performance metric equal error-rate used measure close cloned audios ground truth audios. ﬁrst experiments multi-speaker generative model speaker encoder trained using librispeech dataset contains audios speakers totalling hours. librispeech dataset automatic speech recognition audio quality lower compared speech synthesis datasets. voice cloning performed vctk dataset vctk consists audios sampled native speakers english various accents. consistent librispeech dataset vctk audios downsampled khz. chosen speaker cloning audios randomly sampled experiment. sentences presented appendix used generate audios evaluation. second experiments investigate impact training dataset. split vctk dataset training testing speakers used training multi-speaker model speakers validation speakers cloning. multi-speaker generative model based convolutional sequence-to-sequence architecture proposed ping similar hyperparameters grifﬁn-lim vocoder. better performance increase time-resolution reducing length window size parameters quadratic loss term penalize large amplitude components superlinearly. speaker adaptation experiments reduce embedding dimensionality yields less overﬁtting problems. overall baseline multi-speaker generative model around trainable parameters trained librispeech dataset. second experiments hyperparameters vctk model used ping train multi-speaker model speakers vctk grifﬁn-lim vocoder. train speaker encoders different number cloning audios separately. initially cloning audios converted log-mel spectrograms frequency bands length window size log-mel spectrograms spectral processing layers composed -layer prenet size then temporal processing applied convolutional layers ﬁlter width finally multi-head attention applied heads unit size keys queries values. ﬁnal embedding size validation consists held-out speakers. batch size used initial learning rate annealing rate applied every iterations. mean absolute error validation shown fig. appendix cloning audios leads accurate speaker embedding estimation especially attention mechanism train speaker classiﬁer using vctk dataset classify speakers audio sample belongs speaker classiﬁer fully-connected layer size convolutional layers ﬁlters width ﬁnal embedding layer size model achieves accuracy validation size train speaker veriﬁcation model using librispeech dataset. validation sets consists held-out speakers librispeech. eers estimated randomly pairing utterances different speakers test set. perform trials test set. describe details speaker veriﬁcation model appendix speaker adaptation approach pick optimal number iterations using speaker classiﬁcation accuracy. speaker encoding consider voice cloning without joint ﬁne-tuning speaker encoder multi-speaker generative model. table summarizes approaches lists requirements training data cloning time memory footprint. speaker adaptation fig. shows speaker classiﬁcation accuracy number iterations. both classiﬁcation accuracy signiﬁcantly increases samples samples. sample count regime adapting speaker embedding less likely overﬁt samples figure speaker classiﬁcation accuracy different numbers cloning samples. different numbers cloning samples. librispeech vctk represent eers estimated random pairing utterances ground-truth datasets. adapting whole model. methods also require different numbers iterations converge. compared whole-model adaptation embedding adaptation takes signiﬁcantly iterations converge thus results much longer cloning time. figs. show classiﬁcation accuracy obtained speaker classiﬁcation speaker veriﬁcation models. speaker adaptation speaker encoding beneﬁt cloning audios. number cloning audio samples exceed whole-model adaptation outperforms techniques. speaker encoding yields lower classiﬁcation accuracy compared embedding adaptation achieve similar speaker veriﬁcation performance. besides evaluations discriminative models conduct subject tests amazon mechanical turk framework. assessment naturalness -scale mean opinion score assessment similar generated audios ground-truth audios target speakers -scale similarity score question categories tables show results human evaluations. higher number cloning audios improve metrics. improvement signiﬁcant whole model adaptation degrees freedom provided unseen speaker. indeed high sample counts naturalness signiﬁcantly exceeds baseline model dominance better quality adaptation samples training data. speaker encoding achieves naturalness similar better baseline model. naturalness even improved ﬁne-tuning since allows generative model learn compensate errors speaker encoder. similarity scores slightly improve higher sample counts speaker encoding match scores speaker embedding adaptation. shown fig. speaker encoder maps speakers meaningful latent space. inspired word embedding manipulation apply algebraic operations inferred embeddings transform speech characteristics. transform gender estimate averaged speaker embeddings gender difference particular speaker. example britishmale averagedfemale averagedmale yields british female speaker. similarly consider region accent transformation britishmale averagedamerican averagedbritish obtain american male speaker. results demonstrate high quality audios speciﬁc gender accent characteristics. figure visualization estimated speaker embeddings speaker encoder. ﬁrst principal components speaker embeddings british north american regional accents shown constitute majority labeled speakers vctk dataset. please appendix detailed analysis. speaker adaptation. speaker veriﬁcation results given appendix hand compared librispeech cleaner vctk data improves multi-speaker generative model leading better whole-model adaptation results. hand embedding-only adaptation signiﬁcantly underperforms whole-model adaptation limited speaker diversity vctk dataset. study approaches neural voice cloning speaker adaptation speaker encoding. demonstrate approaches achieve good cloning quality even cloning audios. naturalness show speaker adaptation speaker encoding achieve similar baseline multi-speaker generative model. thus proposed techniques potentially improved better multi-speaker models future similarity demonstrate approaches beneﬁt larger number cloning audios. performance whole-model embedding-only adaptation indicates discriminative speaker information still exists generative model besides speaker embeddings. beneﬁt compact representation embeddings fast cloning small footprint speaker. observe drawbacks training multi-speaker generative model using speech recognition dataset low-quality audios limited speaker diversity. improvements quality dataset would result higher naturalness. expect techniques beneﬁt signiﬁcantly large-scale high-quality multi-speaker dataset. amodei ananthanarayanan anubhai battenberg case casper catanzaro cheng chen deep speech end-to-end speech recognition english mandarin. international conference machine learning pages chen ling dai. voice conversion using deep neural networks layer-wise generative training. ieee/acm transactions audio speech language processing desai black yegnanarayana prahallad. spectral mapping using artiﬁcial neural networks voice conversion. ieee transactions audio speech language processing hwang tsao wang wang chen. probabilistic interpretation artiﬁcial neural network-based voice conversion. asia-paciﬁc signal information processing association annual summit conference mehri kumar gulrajani kumar jain sotelo courville bengio. samplernn unconditional end-to-end neural audio generation model. arxiv preprint arxiv. miao metze. speaker adaptation long short-term memory recurrent neural networks. sixteenth annual conference international speech communication association miao zhang metze. speaker adaptive training deep neural network acoustic models using i-vectors. ieee/acm transactions audio speech language processing shen pang weiss schuster jaitly yang chen zhang wang skerryryan natural synthesis conditioning wavenet spectrogram predictions. arxiv preprint arxiv. snyder ghahremani povey garcia-romero carmiel khudanpur. deep neural network-based speaker embeddings end-to-end speaker veriﬁcation. ieee spoken language technology workshop pages wang skerry-ryan stanton weiss jaitly yang xiao chen bengio agiomyrgiannakis clark saurous. tacotron fully end-to-end text-to-speech synthesis model. corr abs/. abdel-hamid jiang liu. fast adaptation deep neural network based discriminant codes speech recognition. ieee/acm transactions audio speech language processing yamagishi kobayashi nakano ogata isogai. analysis speaker adaptation algorithms hmm-based speech synthesis constrained smaplr adaptation algorithm. ieee transactions audio speech language processing figure speaker encoder architecture intermediate state dimensions. multiplication operation last layer represents inner product along dimension cloning samples. speaker veriﬁcation model given enrollment audios test audio speaker veriﬁcation model performs binary classiﬁcation tells whether enrollment test audios speaker. although using speaker veriﬁcation models would also sufﬁce choose create speaker veriﬁcation models using convolutional-recurrent architecture note equal-error-rate results test unseen speakers state-of-the-art speaker veriﬁcation models. architecture model illustrated figure compute mel-scaled spectrogram enrollment audios test audio resampling input constant sampling frequency. then apply two-dimensional convolutional layers convolving time frequency bands batch normalization relu non-linearity convolution layer. output last convolution layer feed recurrent layer mean-pool time apply fully connected layer obtain speaker encodings enrollment audios test audio. probabilistic linear discriminant analysis scoring similarity encodings plda score deﬁned speaker encodings enrollment test audios respectively fully-connected layer scalar parameters symmetric matrix. then feed sigmoid unit obtain probability speaker. model trained using cross-entropy loss. table lists hyperparameters speaker veriﬁcation model librispeech dataset. addition speaker veriﬁcation test results presented main text also include result using enrollment audio multi-speaker generative model trained librispeech. multi-speaker generative model trained vctk results figure noted that cloned audios could potentially better ground truth vctk speaker veriﬁcation model trained librispeech dataset. parameter audio resampling freq. bands mel-spectrogram length convolution layers channels ﬁlter strides recurrent layer size fully connected size dropout probability learning rate gradient norm gradient clipping max. value figure speaker veriﬁcation number cloning audio samples. multi-speaker generative model speaker veriﬁcation model trained using librispeech dataset. voice cloing performed using vctk dataset. figure speaker veriﬁcation using enrollment audio enrollment audios number cloning audio samples. multi-speaker generative model trained subset vctk dataset including speakers voice cloning performed speakers. speaker veriﬁcation model trained using librispeech dataset. trained speaker encoder model fig. exempliﬁes attention distributions different audio lengths. attention mechanism yield highly non-uniformly distributed coefﬁcients combining information different cloning samples especially assigns higher coefﬁcients longer audios intuitively expected potential information content them. figure mean absolute error embedding estimation number cloning audios validation speakers shown attention mechanism without attention mechanism figure inferred attention coefﬁcients speaker encoder model nsamples lengths cloning audio samples. dashed line corresponds case averaging cloning audio samples. analyze speaker embedding space learned trained speaker encoders apply principal component analysis space inferred embeddings consider ground truth labels gender region accent vctk dataset. fig. shows visualization ﬁrst principal components. observe speaker encoder maps cloning audios latent space highly meaningful discriminative patterns. particular gender dimensional linear transformation learned speaker embeddings achieve high discriminative accuracy although models never ground truth gender label training.", "year": "2018"}