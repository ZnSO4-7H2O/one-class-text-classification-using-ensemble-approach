{"title": "Blind Source Separation with Optimal Transport Non-negative Matrix  Factorization", "tag": "eess", "abstract": " Optimal transport as a loss for machine learning optimization problems has recently gained a lot of attention. Building upon recent advances in computational optimal transport, we develop an optimal transport non-negative matrix factorization (NMF) algorithm for supervised speech blind source separation (BSS). Optimal transport allows us to design and leverage a cost between short-time Fourier transform (STFT) spectrogram frequencies, which takes into account how humans perceive sound. We give empirical evidence that using our proposed optimal transport NMF leads to perceptually better results than Euclidean NMF, for both isolated voice reconstruction and BSS tasks. Finally, we demonstrate how to use optimal transport for cross domain sound processing tasks, where frequencies represented in the input spectrograms may be different from one spectrogram to another. ", "text": "optimal transport loss machine learning optimization problems recently gained attention. building upon recent advances computational optimal transport develop optimal transport non-negative matrix factorization algorithm supervised speech blind source separation optimal transport allows design leverage cost short-time fourier transform spectrogram frequencies takes account humans perceive sound. give empirical evidence using proposed optimal transport leads perceptually better results euclidean isolated voice reconstruction tasks. finally demonstrate optimal transport cross domain sound processing tasks frequencies represented input spectrograms diﬀerent spectrogram another. blind source separation task separating mixed signal diﬀerent components usually referred sources. context sound processing used separate speakers whose voices recorded simultaneously. common address task decompose signal spectrogram non-negative matrix factorization proposed example schmidt olsson well mysore denoting ˜xji short-time fourier transform coeﬃcient input signal frequency time frame magnitude spectrogram deﬁned |˜xji| problem tackled solving problem number sources number time windows column loss function. dictionary matrix weight matrix related single source. supervised setting source training data learned advance training phase. test time given signal separated spectrograms recovered corresponding signals reconstructed suitable post-processing. several loss functions considered literature squared euclidean distance kullback-leibler divergence itakura-saito divergence present article propose optimal transport loss between spectrograms perform supervised speech nmf. optimal transport deﬁned minimum cost moving mass histogram another. taking account transportation cost frequencies provides powerful metric compare stft spectrograms. main advantage using optimal transport loss quantify amplitude frequency shift noise coming example quantization tuning musical instrument. metrics euclidean distance kullback-leibler divergence compare spectrograms element-wise almost blind type noise another advantage element-wise metrics optimal transport enables diﬀerent quantizations i.e. frequency supports training test times. indeed frequencies represented spectrogram depend sampling rate signal time-windows used computation change training test times. optimal transport need re-quantize training testing data share frequency support optimal transport well-deﬁned spectrograms distinct supports long deﬁne transportation cost frequencies. finally optimal transport framework enables generalize wiener ﬁlter common post-processing source separation using optimal transport plans figure comparison euclidean distance optimal transport losses. synthetic musical notes generated putting weight fundamental exponentially decreasing weights harmonics subharmonics ﬁnally convoluting gaussian. left examples spectrograms notes. right optimal transport loss euclidean distance note fundamental .khz note fundamental .khz+σ functions euclidean distance varies sharply whereas optimal transport loss captures smoothly change fundamental. variations optimal transport loss regularized version similar although regularized become negative. optimal transport loss ﬁrst proposed sandler lindenbaum solved problem using bi-convex formulation relied approximation optimal transport based wavelets recently rolet proposed fast algorithms compute entropy-regularized optimal transport loss ﬂexible sense require assumption frequency quantization cost function used. using optimal transport loss spectrograms also proposed flamary name optimal spectral transportation. developed novel method unsupervised music transcription achieves state-of-the-art performance. method relies cost matrix designed speciﬁcally musical instruments allowing diracs dictionary columns. dictionary column vector single non-zero entry learn corresponding coeﬃcients. trivial structure dictionary results eﬃcient coeﬃcient computation. however approach cannot applied speech separation since relies assumption musical note represented fundamental. also requires designing cost moving fundamental harmonics neighboring frequencies. human voices intrinsically complex therefore necessary learn dictionary coeﬃcients i.e. solve full problems. paper extend optimal transport rolet case columns input matrix normalized order propose algorithm suitable spectrogram data. normalizing time frames total weight desirable sound processing tasks would amplify noise. deﬁne cost frequencies optimal transport objective spectrograms provides relevant metric them. apply framework single voice reconstruction blind source separation show optimal transport loss provides better results usual squared euclidean loss. finally show framework cross domain frequencies represented test spectrograms diﬀerent ones dictionary. happen example train test data recorded diﬀerent equipment stft computed diﬀerent parameters. denote matrices upper-case vectors bold lower-case scalars lower-case. matrix transpose column row. denotes all-ones vector dimension deduced context simply write matrices start introducing optimal transport entropy regularization loss previous works optimal transport nmf. comprehensive overview optimal transport computational perspective peyr´e cuturi cost matrix p-th power distance matrix i.e. metric space ot/p distance vectors norm vectors features quantization weights data onto features. sound processing applications vectors real numbers corresponding frequencies spectrogram corresponding magnitude. computing minimal transportation cost frequencies spectrograms optimal transport exhibits variations accordance frequency noise involved signal generative process results instance tuning musical instruments subject’s condition speech processing. unnormalized optimal transport. work wish deﬁne optimal transport non-negative necessarily normalized. note transportation polytope empty long value kbk. hence deﬁne optimal transport possibly unnormalized vectors computing optimal transport cost amounts solve linear program done specialized versions simplex algorithm worst-case complexity considering loss histograms supported hundred bins computation becomes quickly intractable. moreover using loss involves diﬀerentiating diﬀerentiable everywhere. hence would resort subgradient methods. would prohibitively slow since iteration would require obtain subgradient current iterate requires solve entropy regularized optimal transport. remedy limitations cuturi proposed entropy-regularization term optimal transport objective thus making loss diﬀerentiable everystrictly convex. entropy-regularized optimal transport since used numerous works loss diverse tasks optimized train time ﬁxed test time. problem convex separately jointly. solved alternating full optimization respect resulting sub-problem high dimensional linear program many constraints intractable standard solvers even short sound signals. addition convergence proofs alternate minimization methods typically assume strictly convex sub-problems case using non-regularized loss. shown sub-problem either ﬁxed smooth fenchel-rockafellar dual solved eﬃciently leading fast overall algorithm. however deﬁnition optimal transport requires inputs reconstructions norm equal achieved normalizing input beforehand restricting columns simplex using regularizers negative entropies deﬁned simplex solve problem accelerated gradient descent recover optimal weight matrix primal-dual relationship value gradient convex conjugate respect second variable likewise solve problem accelerated gradient descent recover optimal dictionary matrix primal-dual relationship duality results allow constrained primal problem evaluation objective gradient requires solving optimal transport problem non-constrained dual problem whose objective gradient evaluated closed form. primal constraints kxik kdwik enforced primal-dual relationship. moreover entropy regularization makes smooth respect second variable. present approach optimal transport bss. first introduce changes rolet necessary computing optimal transport stft spectrograms sound data. deﬁne transportation cost frequencies. finally show reconstruct sound signals separated spectrograms. however deﬁnition optimal transport require inputs simplex norm. deﬁnition convex conjugate gradient still value cuturi peyr´e simply relax condition problem keep simplex constraint columns dictionary update guaranteed stay compact set. negative entropy deﬁned non-negative orthant coeﬃcient matrix regularizer keep non-negative order compute optimal transport spectrogams perform need cost matrix represents cost moving weight frequencies original spectrogram frequencies reconstructed spectrogram. schmidt olsson scale quantize spectrograms relying fact perceptual diﬀerence frequencies smaller high frequency frequency domain. following intuition propose frequencies log-domain apply cost function domain. frequency j-th input data spectrogram ˆfˆj frequency ˆj-th reconstruction spectrogram deﬁne cost matrix rm×n figure parameter cost matrix. inﬂuence parameter cost matrix. left cost matrix; center sample lines cost matrix; right dictionary learned validation data. center bottom figure shows eﬀect learned dictionaries. using yields cost spiked leading dictionary elements several spikes frequency bands whereas tends produce smoother dictionary elements. figure power cost matrix. inﬂuence power cost matrix. left cost matrix; center sample lines cost matrix; right dictionary learned validation data. center bottom wiener filter. case reconstruction frequency domain original signal classical recover voice time domain apply wiener ﬁlter. original fourier spectrum separated spectra wiener ﬁlter builds before applying original spectra’s phase performing inverse stft. generalized filter. propose extend ﬁltering case domain happen example test data recorded using diﬀerent sample frequency stft performed diﬀerent time-window train data. case domain train data diﬀerent domain coeﬃcients correspond diﬀerent sound frequencies. such cannot wiener ﬁltering. heuristic mapping. alternative generalized ﬁlter propose simply reconstructed signal domain assigning weight spectrogram closest neighbor according distance deﬁned cost matrix separated signal reconstruction. separated sounds reconstructed inverse stft applying wiener ﬁlter generalized ﬁlter section present main empirical ﬁndings paper. start describing dataset used pre-processing applied show optimal transport loss allows perceptually good reconstructions single voices even dictionary elements. finally show optimal transport loss improves upon euclidean loss model single-domain cross-domain settings. evaluate method english part multi-lingual speech database telephonometry dataset. data consists recordings voice four males four females pronouncing diﬀerent english sentences. split person’s audio time-wise train-test data. ﬁles re-sampled treated mono signal. initialization performed setting dictionary column optimal transport barycenter time frames training data added gaussian noise barycenters computed using algorithm benamou ﬁrst show using optimal transport loss leads better perceptual reconstruction voice data. evaluated pemo-q score isolated test voices. dictionaries learned isolated voices train dataset following separation experiment. figure shows mean standard deviation scores optimal transport euclidean nmf. pemo-q score optimal transport signiﬁcantly higher value found empirically scores tend better euclidean even though reconstructed voices clearly worse listening optimal transport reconstruct clear intelligible voices dictionary elements. evaluate blind source separation using peass score proposed emiya claim closer humans would score sdr. consider mixtures voices mixture simply addition sound signals. single-domain blind source separation. ﬁrst show using optimal transport improves euclidean using frequencies spectrogram train test data. experiment training test data processed exactly train test time euclidean-based reconstruct signal using wiener ﬁlter applying inverse stft. optimal transport-based source separation evaluate separation using either wiener ﬁlter generalized ﬁlter. figure shows comparison pair mixed voices selected validation shows peass score better optimal transport loss almost ﬁles. case single domain wiener ﬁlter generalized wiener ﬁlter yields similar results. experiment keep cross-domain blind source separation. dictionaries trained single domain experiment re-process test data diﬀerent time-window stft. although still compute optimal transport spectrograms thanks cost matrix. figure shows resuts train set. score euclidean computed ﬁrst mapping test data domain train data using heuristic mapping performing same-domain separation. heuristice mapping generalized ﬁlter improve upon using euclidean achieve similar results. still generalized ﬁlter allows exact processing whether performing single domain cross domain separation diﬀerence cost matrix heuristic mapping requires additional post-processing also requires choose rules mapping. regularization transport plan. work considered entropy-regularized optimal transport introduced cuturi allows easy-to-solve dual problem since convex conjugate smooth computed closed form. however convex regularizer would yield duality results could considered long conjugate computable. instance squared norm regularization considered several recent works shown desirable properties better numerical stability sparsity optimal transport plan. moreover similarly entropic regularization shown convex conjugate gradient computed closed form figure single domain separation score. comparison optimal transport euclidean optimal transport generalized wiener ﬁlter data-point represents peass scores mixed another coordinate optimal transport wiener ﬁlter’s score coordinate score compared method. figure cross domain separation score. comparison optimal transport generalized wiener ﬁlter euclidean optimal transport heuristic mapping cross domain speech separation task. data-point represents peass scores mixed another coordinate optimal transport generalized wiener ﬁlter’s score coordinate score compared method. learning procedure. following work rolet solved problem alternating minimization approach iteration complete optimization performed either dictionary coeﬃcients. seems work well experiments would interesting compare smaller steps approach like seung unfortunately updates exist knowledge gradient methods primal would prohibitively slow since involve solving large optimal transport problems iteration. showed using optimal transport based loss improve performance nmf-based models voice reconstruction separation tasks. believe ﬁrst step towards using optimal transport loss speech processing possibly using complicated models neural networks. versatility optimal transport compare spectrograms diﬀerent frequency domains lets dictionaries sounds recorded processed training set. property could also beneﬁcial learn common representations diﬀerent datasets. contains reconstructed signal test sentences male validation voice optimal transport dictionary rank dictionary learnt training sentences voice.", "year": "2018"}