{"title": "Structured-based Curriculum Learning for End-to-end English-Japanese  Speech Translation", "tag": "eess", "abstract": " Sequence-to-sequence attentional-based neural network architectures have been shown to provide a powerful model for machine translation and speech recognition. Recently, several works have attempted to extend the models for end-to-end speech translation task. However, the usefulness of these models were only investigated on language pairs with similar syntax and word order (e.g., English-French or English-Spanish). In this work, we focus on end-to-end speech translation tasks on syntactically distant language pairs (e.g., English-Japanese) that require distant word reordering.  To guide the encoder-decoder attentional model to learn this difficult problem, we propose a structured-based curriculum learning strategy.  Unlike conventional curriculum learning that gradually emphasizes difficult data examples, we formalize learning strategies from easier network structures to more difficult network structures. Here, we start the training with end-to-end encoder-decoder for speech recognition or text-based machine translation task then gradually move to end-to-end speech translation task. The experiment results show that the proposed approach could provide significant improvements in comparison with the one without curriculum learning. ", "text": "sequence-to-sequence attentional-based neural network architectures shown provide powerful model machine translation speech recognition. recently several works attempted extend models end-to-end speech translation task. however usefulness models investigated language pairs similar syntax word order work focus end-to-end speech translation tasks syntactically distant language pairs require distant word reordering. guide encoder-decoder attentional model learn difﬁcult problem propose structured-based curriculum learning strategy. unlike conventional curriculum learning gradually emphasizes difﬁcult data examples formalize learning strategies easier network structures difﬁcult network structures. here start training end-to-end encoder-decoder speech recognition text-based machine translation task gradually move end-to-end speech translation task. experiment results show proposed approach could provide signiﬁcant improvements comparison without curriculum learning. index terms speech recognition human-computer interaction computational paralinguistics translating spoken language words recognizing speech automatically ones words translated anlanguage extremely complex. traditional approach speech-to-text translation systems must construct automatic speech recognition machine translation system independently trained tuned. given speech input system processes transforms speech text source language transforms text source language corresponding text target language basic unit information sharing components words text level. even though signiﬁcant progress made various commercial speech translation systems introduced approach continues suffer several major limitations. drawbacks speech acoustics might involve linguistic paralinguistic information paralinguistic information factor written communication much cannot even expressed words. consequently words output lost paralinguistic information linguistic parts translated system. studies proposed including additional component handle paralinguistic translation step introduces complexity delay another noted problem half world’s languages actually written form spoken. another solution translate directly phoneme-based transcription. however performance phoneme-based usually errors stage propagate throughout translation process therefore would useful ways beyond current conventional approach directly translate speech source language text target language. recently deep learning shown much promise many tasks. sequence-to-sequence attention-based neural network architecture provides powerful model machine translation speech recognition recently several works extended models end-to-end speech translation tasks. duong directly trained attentional models parallel speech data. work applicable spanish-english language pairs similar syntax word order furthermore focused alignment performance. attempt build full-ﬂedged end-to-end attentional-based speech-to-text translation system b´erard work done small french-english synthetic corpus language share similar word order languages local movements sufﬁcient translation. paper proposes ﬁrst attempt build end-to-end attention-based system syntactically distant language pairs suffers long-distance reordering phenomena. train attentional model english-japanese language pairs versus word order. guide encoderdecoder attentional model learn difﬁcult problem proposed structured-based curriculum learning strategy. unlike conventional curriculum learning gradually emphasize difﬁcult data examples formalize strategies start training end-to-end encoder-decoder speech recognition text-based machine translation tasks gradually train network end-to-end speech translation tasks adapting decoder encoder parts. start training end-to-end encoder-decoder speech recognition text-based machine translation task gradually move end-to-end speech translation task. curriculum learning learning paradigm inspired learning processes humans animals learn easier aspects gradually increase difﬁcult ones. although application training strategies machine learning discussed machine learning cognitive science researchers going back elman cl’s ﬁrst formulation context machine learning introduced bengio step done assist decoder relevant information encoder side based current decoder hidden states. several variations calculate align. simply product encoder decoder hidden states training process attention-based encoder-decoder model basically difﬁcult standard neural network model attention-based model needs jointly optimize three different modules simultaneously. utilizing attention-based encoderdecoder architecture constructing direct task obviously difﬁcult model needs solve complex problems learning process long speech sequence corresponding words similar issues focused ﬁeld learning make good alignment rules source target languages similar issues discussed ﬁeld furthermore utilize attention-based encoder-decoder architecture construct system syntactically distance language pairs suffer long-distance reordering phenomena train attentional model english-japanese language pairs versus word order. therefore assist encoder-decoder model learn difﬁcult problem proposed structured-based curriculum learning strategy. strategy attentional-based neural network trained directly speech translation tasks using similar difﬁcult speech translation data instead formalize structured-based strategies start training end-to-end encoder-decoder text-based tasks gradually train network end-to-end tasks. words train attentional encoder-decoder architecture starting simpler task switch certain part structure training phase difﬁcult target task. difﬁculty problems increases gradually training phase strategies. including shape recognition object classiﬁcation language modeling tasks however studies focused organize sequence learning data examples context single task learning. bengio proposed curriculum learning multiple tasks. again tasks still belonged type problem object classiﬁcation tasks shared input output spaces. contrast previous studies utilize strategy simple recognition/classiﬁcation problems sequence-to-sequence based neural network learning problems speech translation tasks; attentional-based neural network trained directly speech translation task using similar difﬁcult speech translation data. instead formalize strategies start training end-to-end encoder-decoder speech recognition textbased machine translation tasks gradually train network end-to-end speech translation tasks adapting decoder encoder parts respectively; different tasks speech recognition text-based machine translation speech translation used structured-based share input output spaces multiple tasks. built end-to-end speech translation system upon standard attention-based encoder-decoder neural networks architecture consists encoder decoder attention modules. given input sequence length encoder produces sequence vector representation henc used bidirectional recurrent neural network long short-term memory units consist forward backward lstms. forward lstm reads input sequence henc backward lstm estimates forward reads input sequence reverse order henc. thus input obtain decoder hand predicts target sequence length estimating conditional probability here uni-directional lstm conditional probability estimated based whole sequence previous output phase train attentional-based encoderdecoder neural network standard task predicts corresponding transcription input speech sequence source language. phase next replace decoder decoder retrain match decoder’s output. model predicts corresponding word sequence target language given input speech sequence source language. phase before train attentional-based encoder-decoder neural network standard task predicts corresponding transcription input speech sequence source language. phase replace decoder decoder retrain match encoder’s output work asr-mt transcoder. model’s objective predict word representation good corresponding word sequence source language given input speech sequence source language. here loss function calculate mean squared error output decoder ouput encoder. phase finally combine attention decoder modules perform speech translation task source speech sequence target word sequence train whole architecture using softmax crossentropy function. type start attention-based system similar type construct attentional-based system fast slow tracks instead starting system start system. case model gradually adapts encoder part encoder closely resemble encoder. conducted experiments using basic travel expression corpus btec english-japanese parallel corpus consists training sentences sentences test set. since corresponding speech utterances text corpus unavailable used google text-to-speech synthesis generate speech corpus source language. speech utterances segmented multiple frames window size step size. extracted -dimension ﬁlter bank features using kaldi’s feature extractor normalized zero mean unit variance. text corpus using one-hot vectors results large sparse vectors large vocabulary. study incorporated word embedding learns dense representation words low-dimensional vector space. used data build attention-based system direct system cl-based st-system. table summarizes network parameters. systems used learning rate adopted adam models. applied attentional encoder-decoder architecture described section train direct systems. also constructed asr+mt cascade system. proposed models also applied cl-based attentional encoder-decoder architecture described section train type type unfortunately type failed converge. might large divergence encoder text input space encoder speech input space. successfully trained systems listed below. performance system achieved word error rate remaining systems evaluated based translation quality using standard automatic evaluation metric bleu+ first show proposed methods work training steps. fig. illustrates softmax cross-entropy epochs. system easiest task translating text source language corresponding target language. loss decreased quite fast. hand direct training hard therefore gave worst performance using cl-based proposed method decrease loss. speciﬁcally trained type slow track successfully outperformed text-based system. next investigated translation quality models summarized fig.. results also reveal direct attentional system difﬁcult. direct enc-dec model seems over-ﬁtting language model could handle input speech utterances. results also demonstrated proposed enc-dec model signiﬁcantly improved baseline. best performance achieved proposed enc-dec model even surpassed text-based cascade asr+mt systems. system constructed combination second third parts actually resembles conventional system. therefore system viewpoint additional components ﬁrst part introduced noise input system might function denoising encoder-decoder prevents over-ﬁtting. paper achieved english-japanese end-to-end speech text translation without affected error. proposals utilized structured-based strategies training attentional-based systems start training attentional gradually train network end-toend tasks adapting decoder part. experimental results demonstrated learning model stable translation quality outperformed standard system. best performance achieved proposed model. current results however still rely synthetic data. future investigate effectiveness proposed method using natural speech data investigate various possible language pairs paralinguistic information expand speech-to-text translation task speech-to-speech translation task. nakamura markov nakaiwa andhisashi kawai jitsuhiro j.-s. zhang yamamoto sumita yamamoto multilingual speech-to-speech translation system ieee transactions audio speech language processing vol. kano takamichi sakti neubig toda nakamura generalizing continuous-space translation paralinguistic information interspeech annual conference international speech communication association lyon france august sakti neubig nakamura transferring emphasis speech translation using hard-attentional neural network models annual conference international speech communication association september aguero adell bonafonte prosody generation speech-to-speech translation ieee international conference acoustics speech signal processing proceedings vol. i–i. povey ghoshal boulianne burget glembek goel hannemann motlicek qian schwarz silovsky stemmer vesely kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. ieee signal processing society ieee catalog cfpsrw-usb. c.-y. orange method evaluating automatic evaluation metrics proceedings coling geneva switzerland coling –aug available http//www.aclweb.org/anthology/c- chan jaitly vinyals listen attend spell neural network large vocabulary conversational speech recognition acoustics speech signal processing ieee international conference ieee koehn marcu statistical phrase-based translation proceedings north american chapter association computational linguistics human language technology volume ser. naacl stroudsburg association computational linguistics available http//dx.doi.org/./.", "year": "2018"}