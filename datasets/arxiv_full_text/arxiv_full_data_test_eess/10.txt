{"title": "A Pessimistic Approximation for the Fisher Information Measure", "tag": "eess", "abstract": " The problem of determining the intrinsic quality of a signal processing system with respect to the inference of an unknown deterministic parameter $\\theta$ is considered. While the Fisher information measure $F(\\theta)$ forms a classical tool for such a problem, direct computation of the information measure can become difficult in various situations. For the estimation theoretic performance analysis of nonlinear measurement systems, the form of the likelihood function can make the calculation of the information measure $F(\\theta)$ challenging. In situations where no closed-form expression of the statistical system model is available, the analytical derivation of $F(\\theta)$ is not possible at all. Based on the Cauchy-Schwarz inequality, we derive an alternative information measure $S(\\theta)$. It provides a lower bound on the Fisher information $F(\\theta)$ and has the property of being evaluated with the mean, the variance, the skewness and the kurtosis of the system model at hand. These entities usually exhibit good mathematical tractability or can be determined at low-complexity by real-world measurements in a calibrated setup. With various examples, we show that $S(\\theta)$ provides a good conservative approximation for $F(\\theta)$ and outline different estimation theoretic problems where the presented information bound turns out to be useful. ", "text": "layout measurement sensors signiﬁcantly inﬂuence technical properties computational complexity power consumption production cost reliability processing delay system performance. therefore given ability modify data gathering system alternative design orous method required order draw precise conclusion achievable performance systems operating optimum estimation procedures note denotes collection independent abstract—the problem determining intrinsic quality signal processing system respect inference unknown deterministic parameter considered. fisher information measure forms classical tool problem direct computation information measure become difﬁcult various situations. estimation theoretic performance analysis nonlinear measurement systems form likelihood function make calculation information measure challenging. situations closed-form expression statistical system model available analytical derivation possible all. based cauchy-schwarz inequality derive alternative information measure provides lower bound fisher information property evaluated mean variance skewness kurtosis system model hand. entities usually exhibit good mathematical tractability determined low-complexity real-world measurements calibrated setup. various examples show provides good conservative approximation outline different estimation theoretic problems presented information bound turns useful. index terms—cram´er-rao lower bound estimation theory fisher information lower bound smooth limiter minimum fisher information nonlinear systems squaring loss worst-case noise. situation estimation theory provides variety useful tools. hand guidelines design estimation algorithms hand corresponding performance bounds latter originally derived benchmark estimation algorithms identify potential improvements establish efﬁciency error bounds also become popular ﬁgure merit design optimization measurement system problem frequently arises ﬁeld signal processing efﬁcient extraction information noisy data interest engineers also design physical measurement system itself. note stein digital mathematics group mathematics department vrije universiteit brussel belgium nossek department teleinformatics engineering universidade federal cear´a brasil department electrical computer engineering technische universit¨at m¨unchen germany obtained. note estimator asymptotically attains equality respect called asymptotically efﬁcient. estimators designed along principle maximum-likelihood known exhibit efﬁciency asymptotic regime mild conditions. consequently sufﬁciently large fisher information inequality holds estimation procedures satisfying relative estimation theoretic quality modiﬁcation respect reference system assessed information ratio using information ratio design optimization measurement system requires computing benchmark experiment modiﬁcations interest. alteration probability distribution takes complicated form become difﬁcult. situation parametric probabilistic model deﬁnes statistical behavior random output unknown direct analytical formulation information measure becomes impossible. however mean example hard-limited gaussian model given information bound holds equality simple counter example immediately constructed. consider system output follow generic parametric gaussian distribution measure amount intrinsic information unknown deterministic parameter contained average within observation random output note fisher information measure also plays important role performance bounds bayesian setting parameter considered random variable. comprehensive overview topic scope article found motivated insight obtained preceding section improve lower bound achieve utilizing cauchy-schwarz inequality general approach subsequently maximizing resulting expression. leads alternative information measure forms pessimistic approximation exclusively contains mean variance skewness kurtosis system output model parametric form. discussion situations derivative variance vanishes shows inequality contained presented result special case. using various examples continuous discrete system outputs verify quality alternative information measure order demonstrate possible applications result provide insights approximately determine estimation theoretic information loss squaring standard gaussian input distribution advance discussion concerning minimum fisher information finally mimic situation practical relevance. measuring output moments smooth limiting device standard gaussian input demonstrate conservatively establish intrinsic inference capability nonlinear signal processing system information measure analytic form parametric output model available. note refered skewness indicator asymmetry output distribution called kurtosis characterization shape output distribution moments stand relation pearson’s inequality approximation obtains compact form inequality states derived information measure always dominated fisher information measure therefore gives cautious approximation fisher information requires integrating squared score function contrast alternative information measure exclusively needs mean variance skewness kurtosis parametric form. entities usually analytically tractable determined simple measurements calibrated setup well studied various probability laws. note based moments cumulants alternative bound found becomes clear binary system output following parametric bernoulli distribution derived expression tight approximation original inference capability given finally want outline possible applications presented result opportunities provided information bound like present three problems provides interesting useful insights. problems discussed cover theoretic well practical aspects statistical signal processing. important question signal processing specify worst-case noise distribution considered system model common assumption ﬁeld noise affects technical receive systems additive way. therefore model high practical relevance leads minimum fisher information therefore estimation theoretic perspective gaussian noise worst-case assumption additive system like constant second output moment presented bounding approach allows generalize statements. system output exhibits characteristic another interesting problem statistical signal processing characterize estimation theoretic quality nonlinear receive measurement systems. fisher information measure rigorous tool allows draw precise conclusions. however depending nature nonlinearity exact calculation information measure become complicated. example scenario consider problem analyzing intrinsic capability system squaring output unit variance. case system output follows non-central chi-squared distribution single degree freedom parameterized analytical description associated probability density function includes bessel function characterization fisher information compact analytical form trivial. short-cut information loss squaring random input variable note fig. indicates small values squaring operation results strong degradation estimation capability. comparison corresponding visualized fig. note hard-limiting gaussian model shown forms tight lower bound fisher information measure. observed small values information algebraic sign system input conveys information input mean amplitude hard-limiter output vary slower parameter therefore squaring receiver outperforms hardlimiter comes estimating mean system input samples system output note squaring device fig. depicts conservative approximation exact squaring loss output mapping model depicted different setups input consider gaussian distribution unit variance like output mean variance skewness kurtosis measured independent monte-carlo simulations nonlinear system output considered value input mean result shown fig. numerically approximating required derivatives depicted fig. approximation calculated. fig. measured information loss smooth limiter model shown dotted line indicates exact information loss hard-limiter difﬁcult. appropriate system model unknown direct consultation analytical tool like fisher information measure becomes impossible. however situation information bound like allows numerically approximate information measure low-complexity. moments system output measured calibrated setup parameter controlled determined monte-carlo simulations. demonstrate validation technique using smooth limiter model i.e. system input transformed established generic compact lower bound fisher information measure. various examples shown derived expression potential provide good approximation broad number cases. makes presented information bound versatile mathematical tool variety problems encountered design optimization signal processing systems. further pessimistic nature attained alternative information measure allows strengthen insights worst-case noise generalize classical results gaussian system models exhibit minimum fisher information. finally outlined presented information bound order benchmark estimation capability physical measurement systems output statistics unknown analytical form. fisher mathematical foundations theoretical statistics philosoph. trans. roy. soc. london vol. jan. fisher theory statistical estimation proc. cambridge ˇzivojnovi´c minimum fisher information moment-constrained distributions application robust blind identiﬁcation elsevier signal process. vol. oct.", "year": "2015"}