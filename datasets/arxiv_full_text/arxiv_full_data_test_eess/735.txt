{"title": "Benchmark 3D eye-tracking dataset for visual saliency prediction on  stereoscopic 3D video", "tag": "eess", "abstract": " Visual Attention Models (VAMs) predict the location of an image or video regions that are most likely to attract human attention. Although saliency detection is well explored for 2D image and video content, there are only few attempts made to design 3D saliency prediction models. Newly proposed 3D visual attention models have to be validated over large-scale video saliency prediction datasets, which also contain results of eye-tracking information. There are several publicly available eye-tracking datasets for 2D image and video content. In the case of 3D, however, there is still a need for large-scale video saliency datasets for the research community for validating different 3D-VAMs. In this paper, we introduce a large-scale dataset containing eye-tracking data collected from 61 stereoscopic 3D videos (and also 2D versions of those) and 24 subjects participated in a free-viewing test. We evaluate the performance of the existing saliency detection methods over the proposed dataset. In addition, we created an online benchmark for validating the performance of the existing 2D and 3D visual attention models and facilitate addition of new VAMs to the benchmark. Our benchmark currently contains 50 different VAMs. ", "text": "nasiopoulosa electrical engineering department icics university british columbia vancouver canada department psychology university british columbia vancouver canada icics university british columbia telus communications inc. canada bstract. visual attention models predict location image video regions likely attract human attention. although saliency detection well explored image video content attempts made design saliency prediction models. newly proposed visual attention models validated large-scale video saliency prediction datasets also contain results eye-tracking information. several publicly available eye-tracking datasets image video content. case however still need large-scale video saliency datasets research community validating different d-vams. paper introduce large-scale dataset containing eye-tracking data collected stereoscopic videos subjects participated free-viewing test. evaluate performance existing saliency detection methods proposed dataset. addition created online benchmark validating performance existing visual attention models facilitate addition vams benchmark. benchmark currently contains different vams. fig. example showing inaccuracy saliency prediction methods applied stereoscopic image pair left view image depth human fixation subjective tests generated saliency maps using different methods context aware gbvs hdct hsaliency rare self-resemblance static itti-video kocberber self-resemblance dynamic rahtu yubing. chair salient object stands depth. compactness measured combined generate region-level saliency top-down rois detected using dvam refined disparity zhang bottom-up combine depth existing motion saliency fig. statistics stereoscopic video dataset temporal spatial information measures depth temporal spatial indices stars correspond training videos squares refer validation videos histograms average intensity values train validation histograms average disparity bracket train validation table performance evaluation different vams using eye-tracking dataset stereoscopic videos performance calculated fdms respectively. p-values rounded digits floating point. table performance evaluation different vams saliency prediction stereoscopic videos integrated depth maps values table show improvements achieved depth maps used.*", "year": "2018"}