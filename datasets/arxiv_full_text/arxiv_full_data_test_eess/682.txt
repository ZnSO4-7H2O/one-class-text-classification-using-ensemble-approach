{"title": "Extracting Domain Invariant Features by Unsupervised Learning for Robust  Automatic Speech Recognition", "tag": "eess", "abstract": " The performance of automatic speech recognition (ASR) systems can be significantly compromised by previously unseen conditions, which is typically due to a mismatch between training and testing distributions. In this paper, we address robustness by studying domain invariant features, such that domain information becomes transparent to ASR systems, resolving the mismatch problem. Specifically, we investigate a recent model, called the Factorized Hierarchical Variational Autoencoder (FHVAE). FHVAEs learn to factorize sequence-level and segment-level attributes into different latent variables without supervision. We argue that the set of latent variables that contain segment-level information is our desired domain invariant feature for ASR. Experiments are conducted on Aurora-4 and CHiME-4, which demonstrate 41% and 27% absolute word error rate reductions respectively on mismatched domains. ", "text": "performance automatic speech recognition systems signiﬁcantly compromised previously unseen conditions typically mismatch training testing distributions. paper address robustness studying domain invariant features domain information becomes transparent resolving mismatch problem. speciﬁcally investigate recent model called factorized hierarchical variational autoencoder fhvaes learn factorize sequence-level segment-level attributes different latent variables without supervision. argue latent variables contain segment-level information desired domain invariant feature asr. experiments conducted aurora- chime- demonstrate absolute word error rate reductions respectively mismatched domains. recently neural network-based acoustic models greatly improved performance automatic speech recognition systems. unfortunately well known performance degrade signiﬁcantly testing domain mismatched training. major reason speech data complex distributions contain information linguistic content also speaker identity background noise room characteristics etc. among sources variability subset relevant rest considered nuisance therefore hurt performance distributions attributes mismatched training testing. mapping out-of-domain data in-domain data using enhancementbased methods generally requires parallel data domains. another popular strategy train system large diverse dataset possible however strategy feasible labeled data available domains. alternatively robustness also achieved training using features domain invariant case would domain mismatch issues domain information transparent system. paper consider highly adverse scenario clean noisy speech available transcripts available clean speech. study recently proposed model called factorized hierarchical variational autoencoder learning domain invariant features without supervision. fhvae models learn factorize sequence-level attributes segment-level attributes different latent variables. training system latent variables encode segment-level attributes testing mismatched domains demonstrate latent variables contain linguistic information domain invariant. comprehensive experiments study effect different fhvae architectures training strategies derived domain features robustness systems. proposed method evaluated aurora- chime- datasets contain artiﬁcially corrupted noisy speech real noisy speech respectively. proposed fhvae-based feature reduces absolute word error rate compared ﬁlter bank features compared variational autoencoder-based features. released code fhvaes described paper. rest paper organized follows. section introduce fhvae model method extract domain invariant features. section describes experimental setup section presents results discussion. conclude work section mentioned above generation speech data often involves many independent factors however unseen unsupervised setting. therefore natural describe generative process using latent variable model latent variable ﬁrst sampled prior distribution speech segment sampled distribution conditioned convolutional variational autoencoder proposed model process; assuming prior diagonal gaussian shown automatically learns model independent attributes regarding generation speaker identity linguistic content using orthogonal latent subspaces. result provided mechanism potentially learning domain invariant features discovering latent variables contain domain information. generation sequential data often involves multiple independent factors operating different scales. instance speaker identity affects fundamental frequency utterance level phonetic content affects spectral characteristics segment level. result sequence-level attributes generative models fhvaes maximize marginal likelihood observed dataset; intractability exact posterior fhvaes optimize segment variational lower bound formulated follows notice utterances fhvae would degenerate vanilla vae. prevent collapsing additional discriminative objective encourages discriminability regard utterance segment drawn from. speciﬁcally deﬁne |gµµ combining objectives weighting parameter obtain discriminative segment variational lower bound volume tends smaller amount variation within utterance compared utterances attributes spectral contours tend similar amounts variation within utterances. based observation fhvaes formulate generative process sequential data factorized hierarchical graphical model imposes sequence-dependent priors sequenceindependent priors different sets latent variables. speciﬁcally given dataset {x}m consisting i.i.d. sequences {x}n sequence segments sequence segments assumed generated random process involves latent variables follows s-vector drawn prior distribution latent sequence i.i.d. variables drawn sequence-independent prior sequence-dependent prior respectively; i.i.d. speech segments {x}n drawn condition distribution diag)) whose mean diagonal variance parameterized neural networks. joint probability sequence formulated evaluate effectiveness proposed method extracting domain invariant features consider domain mismatched scenarios. speciﬁcally train system using clean test system clean noisy set. idea would observe smaller performance discrepancy different domains feature representation domain invariant. next introduce datasets well model architectures training conﬁgurations experiments. aurora- primary dataset experiments. aurora- broadband corpus designed noisy speech recognition tasks based wall street journal corpus microphone types clean/channel included noise types artiﬁcially added microphone types results four conditions clean channel noisy channel+noisy. multi-condition development training fhvae models development contains noise labels speaker labels utterance used exp. index training contains speaker labels. system trained clean train clean evaluated multi-condition test eval set. verify proposed method non-artiﬁcial dataset repeat experiments chime- dataset contains real distant-talking recordings noisy environments. original clean utterances single channel real noisy utterances training partition train fhvae models. system trained original clean training evaluated chime- development set. trained stochastic gradient descent using minibatch size without clipping minimize negative variational lower bound plus l-regularization weight based formulation regarded summarization sequence-level attributes sequence encouraged encode sequence-level attributes segment similar within utterance. consequently encodes residual segmentlevel attributes segment together provide sufﬁcient information generating segment. observe inference depends corresponding segment posteriors diag diag modeled isotropic gaussian hand trainable lookup table posterior mean training sequence. estimation testing sequences found pointed nuisance attributes regarding speaker identity room geometry background noise generally consistent within utterance. treat utterance sequence attributes become sequence-level attributes would encoded result encodes residual linguistic information invariant nuisance attributes desired domain invariant feature. adam optimizer used initial learning rate training terminated lower bound development improve epochs. fhvae trained conﬁguration optimization method except loss function replaced negative discriminative segment variational lower bound. seqseq-vae seqseq-fhvae architectures lstm units used experiments. latent space vaes contain dimensions. since fhvae models latent spaces dimensional. hyperparameters explored experiments. inputs vae/fhvae chunks consecutive speech frames randomly drawn utterances frame represented dimensional ﬁlter bank energies. extract features fhvae training utterance compute concatenate posterior mean variance chunks shifted frame generates sequence features frames shorter original sequence. ﬁrst frame last frame match original length. kaldi used feature extraction decoding forced alignment training initial hmm-gmm model original clean utterances. recipe provided chime- challenge kaldi aurora- recipe adapted changing training data used. computational network toolkit used neural network-based acoustic model training. experiments lstm acoustic model architecture proposed applied memory cells -node projection layer lstm layer lstm layers total. bptt segment contains frames mini-batch contains utterances since empirically utterances similar performance utterances. momentum used starting second epoch percent training data held validation control learning rate. learning rate halved gain observed epoch. language model used decoding experiments. section report experimental results datasets provide insights outcome. table summarize results aurora- chime- respectively. tables different experiments separated double horizontal lines indexed exp. index ﬁrst column. second column feature refers frame representations used training models. third sixth column explains model conﬁguration discriminative training weight fhvae models. separate encoder decoder parameters third fourth column. averaged by-condition word error rate shown rest columns. start establishing aurora- baseline results trained different types feature representations including fbank latent variable extracted latent segment variable extracted fhvae. fhvae model encoders fair comparison fhvae models also consider model hidden units encoder layer. results shown table exp. index mentioned condition matched domain conditions mismatched domains. hand fhvae models improve performance mismatched domains large margin slight degradation matched domain. particular features learned fhvae consistently outperform features mismatched conditions absolute reduction. believe experiment veriﬁes fhvaes successfully retain domain invariant linguistic features encode domain related information contrast results suggests vaes encode information single latent variables still contain domain related information hurt performance mismatched domains. next explore optimal fhvae architectures extracting domain invariant features. particular study effect number hidden units layer number layers. results variant listed table exp. index exp. index respectively. regarding averaged model hidden units layer total three layers achieves lowest interestingly break condition observed increasing fhvae model capacity helps reducing noisy condition deteriorates channelmismatching condition hidden units layers. speaker veriﬁcation experiments suggest discriminative training facilitates factorizing segment-level attributes sequencelevel attributes sets latent variables. study effect discriminative training learning robust features show results table exp. index model trained discriminative object. increasing discriminative weight observe consistent improvement conditions better factorization segment sequence information; however increasing weight performance starts degrade. discriminative object inversely affects modeling capacity constraining expressibility latent sequence variables. ﬁrst seem surprising utilizing supervised information fashion improve performance. believe concatenating utterances actually discards useful information respect learning domain invariant features. fhvaes latent segment variables encode attributes consistent within sequence. concatenating speaker utterances noise information longer consistent within sequences would thus encoded latent segment variables; similarly latent segment variables would speaker invariant case. lastly study s-vectors derived fhvae model seen summarization sequence-level attributes utterance. apply procedure i-vector based speaker adaptation utterance ﬁrst estimate s-vector concatenate s-vectors feature representation frame generate feature sequence. results shown table exp. index observe signiﬁcant degradation similar models. reasonable combination actually contains similar information latent variable models degradation mismatch distributions training testing sets. section repeat baseline layer experiments chime- dataset order verify effectiveness fhvae optimality fhvae architecture non-artiﬁcial dataset. results shown table exp. index trend applies chime- dataset latent segment variables fhvae outperform latent variable representations outperform fbank features. fhvae architectures absolute decrease achieved increasing number encoder/decoder layers also consistent trends aurora-. core idea fhvae learn sequence-speciﬁc priors model generation sequence-level attributes smaller amount variation within sequence. suppose treat utterance sequence speaker noise information belongs sequence-level attributes consistent within utterance. alternatively consider fhvae models learn speaker-speciﬁc priors noise-speciﬁc priors respectively. paper conduct comprehensive experiments studying fhvae models domain invariant features extractors. feature demonstrates superior robustness mismatched domains compared fbank vae-based features achieving absolute reduction aurora- chime- respectively. future plan study fhvae-based augmentation methods similar tara sainath brian kingsbury george saon hagen soltau abdel-rahman mohamed george dahl bhuvana ramabhadran deep convolutional neural networks large-scale speech tasks neural networks vol. has¸im f´elix chaumont quitry tara sainath kanishka acoustic modelling cd-ctc-smbr lstm rnns automatic speech recognition understanding ieee workshop ieee wei-ning zhang james glass unsupervised domain adaptation robust speech recognition variational autoencoder-based data augmentation automatic speech recognition understanding ieee workshop ieee arun narayanan deliang wang ideal ratio mask estimation using deep neural networks robust speech recognition acoustics speech signal processing ieee international conference ieee yusuf isik jonathan roux zhuo chen shinji watanabe john hershey single-channel multi-speaker separation using deep clustering arxiv preprint arxiv. feng yaodong zhang james glass speech feature denoising dereverberation deep autoencoders noisy reverberant speech recognition acoustics speech signal processing ieee international conference ieee jinyu dong jui-ting huang yifan gong improving wideband speech recognition using mixed-bandwidth training data cd-dnn-hmm spoken language technology workshop ieee. ieee michael seltzer dong yongqiang wang investigation deep neural networks noise robust speech recognition acoustics speech signal processing ieee international conference ieee oriol vinyals suman ravuri comparing multilayer perceptron deep belief network tandem features robust acoustics speech signal processing ieee international conference ieee tara sainath brian kingsbury bhuvana ramabhadran auto-encoder bottleneck features using deep belief networks acoustics speech signal processing ieee international conference ieee wei-ning zhang james glass unsupervised learning disentangled interpretable representations sequential data advances neural information processing systems emmanuel vincent shinji watanabe aditya arie nugraha barker ricard marxer analysis environment microphone data simulation mismatches robust speech recognition computer speech language daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannemann petr motlicek yanmin qian petr schwarz kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. ieee signal processing society number epfl-conf-. dong adam eversole mike seltzer kaisheng zhiheng huang brian guenter oleksii kuchaiev zhang frank seide huaming wang introduction computational networks computational network toolkit tech. rep. tech. rep. microsoft research http//codebox/cntk hasim andrew senior franc¸oise beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling. interspeech zhang guoguo chen dong kaisheng yaco sanjeev khudanpur james glass highway long short-term memory rnns distant speech recognition ieee international conference acoustics speech signal processing ieee", "year": "2018"}