{"title": "Efficient Nonlinear Transforms for Lossy Image Compression", "tag": "eess", "abstract": " We assess the performance of two techniques in the context of nonlinear transform coding with artificial neural networks, Sadam and GDN. Both techniques have been successfully used in state-of-the-art image compression methods, but their performance has not been individually assessed to this point. Together, the techniques stabilize the training procedure of nonlinear image transforms and increase their capacity to approximate the (unknown) rate-distortion optimal transform functions. Besides comparing their performance to established alternatives, we detail the implementation of both methods and provide open-source code along with the paper. ", "text": "parameters reconstructed image vector. parameters functions obtained minimizing empirical rate–distortion loss function taking expectations image data distribution left-hand rate term contains entropy model parameters jointly optimized right-hand term represents expected distortion measured distortion metric lagrange multiplier determining trade-off rate distortion terms. note invertibility transforms theoretically guaranteed minimization loss ensures reconstruction ﬁdelity. image distribution unknown expectation typically replaced average number training images. approximation relax discrete quantizer transfer curve loss function made suitable minimization anns generally composite functions alternating linear nonlinear components quality solutions offer vary depending optimization algorithm constraints linear components nonlinearities used. exactly details affect performance poorly understood makes necessary resort empirical evaluation. rest paper concerned evaluating techniques used several recent compression models whose efﬁcacy individually evaluated ﬁrst extension adam popular variant call spectral adam sadam detailed following section; second nonlinearity introduced called generalized divisive normalization described section improves efﬁciency transforms compared popular nonlinearities. techniques make reparameterization i.e. performing descent transform parameters invertible functions them. compare performance established techniques provide detailed explanation implemented them. abstract—we assess performance techniques context nonlinear transform coding artiﬁcial neural networks sadam gdn. techniques successfully used state-of-the-art image compression methods performance individually assessed point. together techniques stabilize training procedure nonlinear image transforms increase capacity approximate rate–distortion optimal transform functions. besides comparing performance established alternatives detail implementation methods provide open-source code along paper. hallmark artiﬁcial neural networks that given right parameters approximate arbitrary functions appears ﬁnding sufﬁciently good parameters longer problem many applications. efforts utilize techniques machine learning anns stochastic gradient descent context image compression recently garnered substantial interest even though models developed scratch rapidly become competitive modern conventional compression methods published years earliest publications appeared whereas conventional methods hevc culmination decades engineering efforts. properly utilized anns continue enable rapid development image compression models fewer constraints hinder engineering process before. constraint context transform coding linearity. arguably well-known transform discrete cosine transform however closeness optimality rate–distortion sense established assumption transform linear data distribution gaussian although vast majority contemporary image compression methods made nonlinear extensions improve performance empirical data distributions rely linear transforms core. nonlinear transform coding ann-based methods discard constraint. analysis transform replaced generic parametric function implemented neural network image vector parameters analogously synthesis transform function represents quantized image representation another matrix consisting ﬁlters ﬁlters subset parameters optimized. ﬁlters update rule gradient descent dictates subtraction gradient loss function respect multiplied step size element corresponds ﬁlter. updates consist scalar mixtures data vectors projected onto corresponding ﬁlter hence inherit much covariance structure data. consequently drawn natural images power spectrum roughly inversely proportional spatial frequency effective step size several orders magnitude higher low-frequency components high-frequency components leading inbalance convergence speeds even stability problems. white thus effective step size equalized across frequencies. however works applied directly data anns typically consist several layers linear–nonlinear functions. hope decorrelated inputs lead less correlated intermediates general covariance structure higher layers unknown priori still ill-conditioned. running average derivative diagonal matrix representing running estimate covariance. since constrained diagonal however cannot effectively represent covariance structure natural images. solve this apply algorithm real-input discrete fourier transform reparameterizing running estimates derivative before consists rescaled inverse square root covariance estimate. however covariance estimate ⊤cgf diagonal fourier domain rather diagonal terms ﬁlter coefﬁcients. long shift-invariance property like virtually spatiotemporal data fourier basis guaranteed good approximation eigenvectors true covariance structure implies modiﬁed algorithm model true covariance structure derivatives near-optimal fashion using nothing fourier transforms applied ﬁlter coefﬁcients. call technique sadam spectral adam. like adam unlike pre-whitening adapts data used online training settings applied ﬁlters layer long neural network convolutional local normalization known occur ubiquitously throughout biological sensory systems including human visual system shown factorizing probability distribution natural images reduction statistical dependencies thought essential ingredient transform coding. such surprising local normalization implemented generalized divisive normalization recently successfully applied image compression nonlinear transforms. represents vector normalized responses vectors matrices represent parameters transformation contexts values ﬁxed make denominator resemble weighted ℓ-norm context convolutional neural networks often also constrained operate locally space indexes across responses different ﬁlters across different spatial positions tensor modiﬁcations serve simplify implementation retaining ﬂexibility transformation. generally parametric forms simple enough implement given values parameters. however optimizing parameters using gradient descent pose practical caveats address below. derivatives fig. rate–distortion performance descent steps compression model nonlinearity ﬁlters layer. sadam reduces loss function faster adam step size. fig. rate–distortion performance descent steps compression model nonlinearity ﬁlters layer. setup step size large adam leading instabilities cause unpredictable outcomes make experimentation difﬁcult. evaluate performance methods introduced previous sections conducted experiments tensorflow framework using compression models similar ones baseline comparison. transforms constructed follows details). analysis transform consists three convolutional layers ﬁlters bias term ﬁlter zero boundary handling. conducted sets experiments respectively. kernel support downsampling factors layer order. analogously synthesis transform three layers ﬁlters kernel support upsampling factors mirroring analysis transform problems remain solution ﬁrst parameter happen identical zero point optimization next update given would zero well regardless loss function. implies parameters risk stuck zero. second negative values parameters need prevented. solve both reparameterization small constant βmin desired minimum value found work well empirically. prevent denominator become zero βmin elementwise reparameterization γmin order). sandwiched convolutional layers nonlinear layers popular nonlinearities comparison relu leaky relu softplus tanh nonlinearity makes entire network linear. experiment involving used inverse synthesis transform entropy model used separate non-parametric scalar density model ﬁlter described detail trained model random patches pixels taken large dataset images preprocessing applied described avoid overﬁtting issues evaluated compression performance averaging well-known kodak dataset. reported rates average bitstream lengths cross entropy measurements. ﬁrst three ﬁgures compare training algorithms. generally convergence behavior differ depending hyperparameters number ﬁlters sadam outperforms adam experiments comparing identical number descent steps note step size setups appears large adam result optimization unstable lowering step size ﬁxes problem slows convergence. sadam achieves faster convergence cases improves conditioning optimization problem allowing step size used across variety setups. next ﬁgures compare models trained sadam differ nonlinearities better nonlinearities approximating rate–distortion optimal transforms across values different network architectures improvement approximately .–.db psnr highest rate point. note models approximately number parameters given computationally convolutions tend eclipse nonlinearities observe differences nonlinearities manifest mostly higher bitrates moreover many ﬁnal-layer ﬁlters analysis transforms utilized lower bitrate models converging produce constant outputs zero entropy interpreted context approximation capacity. higher number ﬁlters increases network’s degrees freedom hence capacity approximate rate–distortion optimal transform across different values higher bitrate implies optimal transforms complex higher-dimensional approximation capacity models driven limit there. lower bitrates approximation capacity models sufﬁcient rate–distortion performance converges making simply different approximations fig. rate–distortion performance descent steps different training algorithms. algorithms converge similar results eventually sadam remains better comparing number steps. fig. rate–distortion performance descent steps different nonlinearities ﬁlters layer. increases approximation capacity network leading better performance. fig. rate–distortion performance descent steps different nonlinearities comparing networks different number ﬁlters higher bitrates require greater approximation capacity making differences nonlinearities visible. equivalent transforms. note network achieves similar performance leaky relu network approximately bits/pixel. able approximate optimal image transforms using fewer ﬁlters nonlinearities parametric form suitable redundancy reduction images. note experimented batch normalization well found lead substantially worse results given setup regardless nonlinearity used even increasing batch size values high close feasible limit training single gpu. higher tolerance internal noise anns trained classiﬁcation opposed compression. theis cunningham huszár lossy image compression compressive autoencoders arxiv e-prints presented int. conf. learning representations. arxiv toderici vincent johnston hwang minnen shor covell full resolution image compression recurrent neural networks ieee conf. computer vision pattern recognition ./cvpr... arxiv present techniques sadam improve training representation efﬁciency ann-based image transforms context nonlinear transform coding provide detailed comparison performance popular training algorithms nonlinearities. sadam stabilizes speeds training procedure experiments. appears increase approximation capacity transforms suggesting networks fewer ﬁlters used compared nonlinearities without hurting performance. provide implementations sadam entropy model https//github.com/tensorﬂow/compression. given encouraging results topic future work explore whether increased efﬁciency terms number ﬁlters also translates computational efﬁciency advantage depending hardware platform implemented explore relationship batch normalization gdn. toderici o’malley hwang vincent minnen baluja covell sukthankar variable rate image compression recurrent neural networks arxiv e-prints presented int. conf. learning representations. arxiv ballé laparra simoncelli end-toend optimization nonlinear transform codes perceptual quality picture coding symposium ./pcs... arxiv ágústsson mentzer tschannen cavigelli timofte benini gool soft-tohard vector quantization end-to-end learning compressible representations advances neural information processing systems ballé minnen singh hwang johnston variational image compression scale hyperprior arxiv e-prints presented int. conf. learning representations. arxiv itu-r rec. iso/iec high efﬁciency video coding ahmed natarajan discrete cosine transform ieee transactions computers vol. ./t-c... kingma adam method stochastic optimization arxiv e-prints presented int. conf. learning representations. arxiv ballé laparra simoncelli density modeling images using generalized normalization transformation arxiv e-prints presented int. conf. learning representations. arxiv carandini heeger normalization canonical neural computation nature reviews neuroscience vol. ./nrn. malo laparra psychophysically tuned divisive normalization approximately factorizes natural images neural computation vol. ./neco_a_.", "year": "2018"}