{"title": "Hello Edge: Keyword Spotting on Microcontrollers", "tag": "eess", "abstract": " Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4%, which is ~10% higher than the DNN model with similar number of parameters. ", "text": "keyword spotting critical component enabling speech based user interactions smart devices. requires real-time response high accuracy good user experience. recently neural networks become attractive choice architecture superior accuracy compared traditional speech processing algorithms. always-on nature application highly constrained power budget typically runs tiny microcontrollers limited memory compute capability. design neural network architecture must consider constraints. work perform neural network architecture evaluation exploration running resource-constrained microcontrollers. train various neural network architectures keyword spotting published literature compare accuracy memory/compute requirements. show possible optimize neural network architectures within memory compute constraints microcontrollers without sacriﬁcing accuracy. explore depthwise separable convolutional neural network compare neural network architectures. ds-cnn achieves accuracy higher model similar number parameters. deep learning algorithms evolved stage surpassed human accuracies variety cognitive tasks including image classiﬁcation conversational speech recognition motivated recent breakthroughs deep learning based speech recognition technologies speech increasingly becoming natural interact consumer electronic devices example amazon echo google home smart phones. however always-on speech recognition energy-efﬁcient also cause network congestion transmit continuous audio stream billions devices cloud. furthermore cloud based solution adds latency application hurts user experience. also privacy concerns audio continuously transmitted cloud. mitigate concerns devices ﬁrst detect predeﬁned keyword \"alexa\" google\" \"hey siri\" etc. commonly known keyword spotting detection keyword wakes device activates full scale speech recognition either device cloud. applications sequence keywords used voice commands smart device voice-enabled light bulb. since system always-on power consumption maximize battery life. hand system detect keywords high accuracy latency best user experience. conﬂicting system requirements make active area research ever since inception years recently renaissance artiﬁcial neural networks form deep learning algorithms neural network based become popular power consumption requirement keyword spotting systems make microcontrollers obvious choice deploying always-on system. microcontrollers low-cost energy-efﬁcient processors ubiquitous everyday life presence variety devices ranging home appliances automobiles consumer electronics wearables. however deployment neural network based microcontrollers comes following challenges limited memory footprint typical microcontroller systems tens hundred memory available. entire neural network model including input/output weights activations within small memory budget. limited compute resources since always-on real-time requirement limits total number operations neural network inference. microcontroller resource constraints conjunction high accuracy latency requirements call resource-constrained neural network architecture exploration lean neural network structures suitable primary focus work. main contributions work follows ﬁrst train popular neural models literature google speech commands dataset compare terms accuracy memory footprint number operations inference. addition implement model using depth-wise separable convolutions point-wise convolutions inspired success resource-efﬁcient mobilenet computer vision. model outperforms prior models aspects accuracy model size number operations. finally perform resource-constrained neural network architecture exploration present comprehensive comparison different network architectures within compute memory constraints typical microcontrollers. code model deﬁnitions pretrained models available https//github.com/arm-software/ml-kws-for-mcu. typical system consists feature extractor neural network based classiﬁer shown fig. first input speech signal length framed overlapping frames length stride giving total frames. frame speech features extracted generating total features entire input speech signal length log-mel ﬁlter bank energies mel-frequency cepstral coefﬁcients commonly used human-engineered speech features deep learning based speech-recognition adapted traditional speech processing techniques. feature extraction using lfbe mfcc involves translating time-domain speech signal frequency-domain spectral coefﬁcients enables dimensionality compression input signal. extracted speech feature matrix classiﬁer module generates probabilities output classes. real-world scenario keywords need identiﬁed continuous audio stream posterior handling module averages output probabilities output class period time improving overall conﬁdence prediction. traditional speech recognition technologies hidden markov models viterbi decoding approaches achieve reasonable accuracies hard train computationally expensive inference. techniques explored include discriminative models adopting large-margin problem formulation recurrent neural networks although methods signiﬁcantly outperform based terms accuracy suffer large detection latency. models using deep neural networks based fully-connected layers rectiﬁed linear unit activation functions introduced outperforms models small detection latency. furthermore low-rank approximation techniques used compress model weights achieving similar accuracy less hardware resources main drawback dnns ignore local temporal spectral correlation input speech features. order exploit correlations different variants convolutional neural network based explored demonstrate higher accuracy dnns. drawback cnns modeling time varying signals ignore long term temporal dependencies. combining strengths cnns rnns convolutional recurrent neural network based investigated demonstrate robustness model noise. prior neural networks trained cross entropy loss function max-pooling based loss function training model long short-term memory proposed achieves better accuracy dnns lstms trained cross entropy loss. although many neural network models presented literature difﬁcult make fair comparison trained evaluated different proprietary datasets \"alexa\" dataset etc.) different input speech features audio duration. also primary focus prior research maximize accuracy small memory footprint model without explicit constraints underlying hardware limits number operations inference. contrast work hardware-centric targeted towards neural network architectures maximize accuracy microcontroller devices. constraints memory compute signiﬁcantly limit neural network parameters number operations. typical microcontroller system consists processor core on-chip sram block on-chip embedded ﬂash. table shows commercially available microcontroller development boards cortex-m processor cores different compute capabilities running different frequencies consisting wide range on-chip memory program binary usually preloaded non-volatile ﬂash loaded sram startup processor runs program sram main data memory. therefore size sram limits size memory software use. memory footprint performance also constraining factor running neural networks microcontrollers. microcontrollers designed embedded applications cost high energy-efﬁciency primary targets high throughput compute-intensive workloads neural networks. microcontrollers integrated instructions useful running neural network workloads. example cortex-m cortex-m integrated simd instructions used accelerate low-precision computation neural networks. section gives overview different neural network architectures explored work including deep neural network convolutional neural network recurrent neural network convolutional recurrent neural network depthwise separable convolutional neural network standard feed-forward neural network made stack fully-connected layers non-linear activation layers. input ﬂattened feature matrix feeds stack hidden fully-connected layers neurons. typically fully-connected layer followed rectiﬁed linear unit based activation function. output linear layer followed softmax layer generating output probabilities keywords used posterior handling. main drawback based fail efﬁciently model local temporal spectral correlation speech features. cnns exploit correlation treating input time-domain spectral-domain features image performing convolution operations convolution layers typically followed batch normalization relu based activation functions optional max/average pooling layers reduce dimensionality features. inference parameters batch normalization folded weights convolution layers. cases linear low-rank layer simply fully-connected layer without non-linear activation added convolution layers dense layers purpose reducing parameters accelerating training rnns shown superior performance many sequence modeling tasks especially speech recognition language modeling translation etc. rnns exploit temporal relation input signal also capture long-term dependencies using \"gating\" mechanism. unlike cnns input features treated image rnns operate time steps time step corresponding spectral feature vector concatenated previous time step output used input rnn. figure shows model architecture typical model cell could lstm cell gated recurrent unit cell since weights reused across time steps models tend less number parameters compared cnns. similar batch normalization cnns research show applying layer normalization beneﬁcial training rnns hidden states normalized time step. convolution recurrent neural network hybrid takes advantages both. exploits local temporal/spatial correlation using convolution layers global temporal dependencies speech features using recurrent layers. shown fig. crnn model starts convolution layer followed encode signal dense fully-connected layer information. here recurrent layer bi-directional multiple stages increasing network learning capability. gated recurrent units used base cell recurrent layers uses fewer parameters lstms gave better convergence experiments. recently depthwise separable convolution proposed efﬁcient alternative standard convolution operation used achieve compact network architectures area computer vision ds-cnn ﬁrst convolves channel input feature separate ﬁlter uses pointwise convolutions combine outputs depth dimension. decomposing standard convolutions convolutions followed convolutions depthwise separable convolutions efﬁcient number parameters operations makes deeper wider architecture possible even resource-constrained microcontroller devices. work adopt depthwise separable based implementation mobilenet shown fig. average pooling followed fully-connected layer used provide global interaction reduce total number parameters ﬁnal layer. google speech commands dataset neural network architecture exploration experiments. dataset consists -second long audio clips keywords thousands different people clip consisting keyword. neural network models trained classify incoming audio keywords \"yes\" \"no\" \"up\" \"down\" \"left\" \"right\" \"on\" \"off\" \"stop\" \"go\" along \"silence\" \"unknown\" word remaining keywords dataset. dataset split training validation test ratio making sure audio clips person stays set. models trained google tensorﬂow framework using standard cross-entropy loss adam optimizer batch size models trained iterations initial learning rate reduced ﬁrst iterations. training data augmented background noise random time shift trained models evaluated based classiﬁcation accuracy test set. table summarizes accuracy memory requirement operations inference network architectures literature trained google speech commands dataset models mfcc features extracted speech frame length stride gives features second audio. accuracy shown table accuracy test set. memory shown table assumes -bit weights activations sufﬁcient achieve accuracy full-precision network. also assume memory activations reused across different layers hence memory requirement activations uses maximum consecutive layers. operations table counts total number multiplications additions matrix-multiplication operations layer network representative execution time entire network. models existing literature optimized different datasets different memory/compute resources hence direct comparison accuracy unfair. said results still provide useful insights different neural network architectures although dnns achieve best accuracy tend memory intensive less number operations/inference hence suit well systems limited compute capability discussed section memory footprint execution time important considerations able keyword spotting microcontrollers. considered designing optimizing neural networks running keyword spotting. based typical microcontroller system conﬁgurations derive three sets constraints neural networks table targeting small medium large microcontroller systems. memory compute limit derived assumptions amount resources allocated running tasks network communication etc. operations inference limit assumes system running inferences second. figure shows number operations inference memory requirement test accuracy neural network models prior work trained google speech commands dataset overlayed memory compute bounding boxes neural network classes section ideal model would high accuracy small memory footprint lower number computations i.e. close origin fig. apart lstm model models memory/compute resource heavy bounding kb/mops memory/compute limits. cnn- crnn models bounding boxes lower accuracies compared cnn- model boxes all. rest section discusses different hyperparameters feature extraction neural network architectures tuned order bring models close origin still achieve high accuracy. figure number operations memory test accuracy models prior work trained speech commands dataset shown fig. input speech signal features extracted number features impact model size number operations accuracy. parameters feature extraction step impact model size number operations accuracy number mfcc features frame frame stride number mfcc features audio frame impacts number weights fully-connected recurrent layers convolution layers weights reused convolution layers. frame stride determines number frames processed inference impacts number weights fully-connected layers recurrent convolution layers weight reuse. impact number operations inference. efﬁcient model would maximize accuracy using small i.e. small and/or large neural network architectures corresponding hyperparameters explored work summarized table lstm model mentioned table includes peephole connections output projection layer similar whereas basic lstm model include those. crnn uses convolution layer followed multi-layer recurrent layers. also batch normalization convolutional/fully-connected layers layer normalization recurrent layers. inference parameters batch normalization layer normalization folded weights convolution recurrent layers hence layers ignored memory/ops computation. number memory cells projection layer size number memory cells conv features/kernel size/stride number memory cells layer size number ds-conv layers ds-conv features/kernel size/stride iteratively perform exhaustive search feature extraction hyperparameters model hyperparameters followed manual selection narrow search space. ﬁnal best performing models neural network architecture along memory requirements operations summarized table fig. hyperparameters networks summarized appendix results dnns memory-bound achieve less accuracies saturate even model scaled cnns achieve better accuracies limited weights ﬁnal fully-connected layers. models achieve better accuracies cnns yield even smaller models less cases demonstrating exploiting temporal dependencies maximizes accuracy within resource budget. crnn models combine best properties cnns rnns achieve better accuracies cnns rnns even less ops. crnn architecture also scales well memory/compute resources available. ds-cnn achieves best accuracies demonstrate good scalability owing deeper architecture enabled depthwise separable convolution layers less compute/memory intensive. study scalability models smaller microcontroller systems memory expand search space ds-cnn models. figure shows accuracy memory/ops requirements ds-cnn models targeted constrained devices. shows scaled-down ds-cnn models achieve better accuracies models similar number reduction memory requirement. neural networks typically trained ﬂoating point weights activations. previous research shown ﬁxed-point weights sufﬁcient neural networks minimal loss accuracy. microcontroller systems limited memory motivates quantization -bit ﬂoating point weights -bit ﬁxed point weights deployment thus reducing model size moreover ﬁxed-point integer operations much faster ﬂoating point operations typical microcontrollers another reason executing quantized model deployment. work quantization described using -bits represent weights activations. given signed complement -bit ﬁxed-point number value bi.i−n fractional length also negative. ﬁxed given layer different layers. example represent range step represent range step represent range step weights quantized -bits progressively layer time ﬁnding optimal layer minimizes loss accuracy quantization. weights quantized activations also quantized similar appropriate fractional length layer. table shows accuracies representative -bit networks quantized using method compared original full-precision networks. table shows accuracy quantized network either marginally better full-precision network possibly better regularization quantization. believe conclusion hold neural network models explored work. deployed application cortex-m based stmfg-disco development board using cmsis-nn kernels picture board performing shown fig. deployed model model -bit weights -bit activations running inferences second. inference including memory copying mfcc feature extraction execution takes microcontroller wait-for-interrupt mode remaining time power saving. entire application occupies memory including weights activations audio mfcc features. hardware optimized neural network architecture efﬁcient results memory compute constrained microcontrollers. trained various neural network architectures keyword spotting published literature google speech commands dataset compare accuracy memory requirements operations inference perspective deployment microcontroller systems. quantized representative trained -bit ﬂoating-point models -bit ﬁxed-point versions demonstrating models easily quantized deployment without loss accuracy even without retraining. furthermore trained model using depthwise separable convolution layers inspired mobilenet. based typical microcontroller systems derived three sets memory/compute constraints neural networks performed resource constrained neural network architecture exploration best networks achieving maximum accuracy within constraints. three sets memory/compute constraints depthwise separable model achieves best accuracies compared model architectures within constraints shows good scalability ds-cnn model. code model deﬁnitions pretrained models available https//github.com/armsoftware/ml-kws-for-mcu. would like thank matt mattina research bratt technology group help support. would also like thank pete warden google’s tensorflow team valuable inputs feedback project.", "year": "2017"}