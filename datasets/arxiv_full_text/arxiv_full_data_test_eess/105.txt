{"title": "Attention-Based Models for Text-Dependent Speaker Verification", "tag": "eess", "abstract": " Attention-based models have recently shown great performance on a range of tasks, such as speech recognition, machine translation, and image captioning due to their ability to summarize relevant information that expands through the entire length of an input sequence. In this paper, we analyze the usage of attention mechanisms to the problem of sequence summarization in our end-to-end text-dependent speaker recognition system. We explore different topologies and their variants of the attention layer, and compare different pooling methods on the attention weights. Ultimately, we show that attention-based models can improves the Equal Error Rate (EER) of our speaker verification system by relatively 14% compared to our non-attention LSTM baseline model. ", "text": "attention-based models recently shown great performance range tasks speech recognition machine translation image captioning ability summarize relevant information expands entire length input sequence. paper analyze usage attention mechanisms problem sequence summarization end-to-end textdependent speaker recognition system. explore different topologies variants attention layer compare different pooling methods attention weights. ultimately show attention-based models improves equal error rate speaker veriﬁcation system relatively compared non-attention lstm baseline model. however challenge architecture introduced that silence background noise well captured. though speaker veriﬁcation runs short window segmented keyword detector phonemes usually surrounded frames silence background noise. ideally speaker embedding built using frames corresponding phonemes. thus propose attention layer soft mechanism emphasize relevant elements input sequence. paper organized follows. sec. ﬁrst brieﬂy review lstm-based d-vector baseline approach trained end-to-end architecture sec. introduce attention mechanism baseline architecture covering different scoring functions layer variants weights pooling methods. sec. setup experiments compare attention-based models baseline model present results testing set. conclusions made sec. end-to-end training architecture described fig. training step tuple evaluation utterance enrollment utterances lstm network {xj∼ represents features ﬁxed-length segment represent speakers utterances equal tuple includes single utterance speaker different utterance speaker call tuple positive enrollment utterances speaker i.e. negative otherwise. generate positive negative tuples alternatively. utterance output lstm’s last layer frame ﬁxed dimensional vector take last frame output d-vector build tuple {ωj∼ centroid tuple represents voiceprint built utterances deﬁned follows speaker veriﬁcation process verifying based reference enrollment utterances whether veriﬁcation utterance belongs known speaker. subtask global password text-dependent speaker veriﬁcation refers problems transcripts reference enrollment veriﬁcation utterances constrained speciﬁc phrase. study focus google google global passwords relate voice match feature google home i-vector based systems combination veriﬁcation back-ends probabilistic linear discriminant analysis dominating paradigm previous years. recently rising deep learning various machine learning applications efforts focusing using neural networks speaker veriﬁcation. currently promising approaches end-to-end integrated architectures simulate enrollment-veriﬁcation two-stage process training. example authors propose architectures resemble components i-vector plda system. architecture allowed bootstrap network parameters pretrained i-vector plda models better performance. however initialization stage also constrained type network architectures could used deep neural networks initialized classical i-vector plda models. shown long short-term memory networks achieve better performance dnns integrated end-to-end architectures td-sv scenarios. standard sigmoid function equals otherwise equals end-to-end loss function encourages larger value smaller value consider update positive negative tuples loss function similar triplet loss facenet training dataset collection anonymized user voice queries mixture google google. around utterances around speakers. testing dataset manual collection consisting speakers. it’s divided enrollment sets veriﬁcation sets google google. enrollment evaluation dataset contains respectively average evaluation utterances speaker. baseline model -layer lstm layer dimension projection layer dimension lstm linear layer dimension acoustic parametrization consists -dimensional log-mel-ﬁlterbank coefﬁcients computed window overlap. acoustic features used keyword detection speaker veriﬁcation. keyword spotting system isolates segments length frames contain global password segments form tuples mentioned above. keywords mixed together using multireader technique introduced first compare baseline model basic attention layer using different scoring function results shown table bias-only linear attention bring little improvement non-linear attention improves performance signiﬁcantly especially shared parameters. divided-layer attention double dimension last layer lstm output equally divide dimension parts part-a part-a build d-vector using part-b learn scores another variation basic attention layer that instead directly using normalized weights average lstm outputs optionally perform maxpooling attention weights. additional pooling mechanism potentially make network robust temporal variations input signals. experimented maxpooling methods compare basic attention layer variants scoring function performs best previous experiment shared-parameter non-linear scoring function fsnl. results table divided-layer attention performs slightly better basic attention cross-layer attention cost dimension last lstm layer doubled. compare different pooling methods attention weights introduced sec. divided-layer attention sharedparameter non-linear scoring function. sliding window maxpooling experimented different window sizes steps found window size frames step frames perform best evaluations. also global top-k maxpooling found performance best results shown table sliding window maxpooling improves eer. also visualize attention weights training batch different pooling methods fig. interesting observation that there’s pooling clear -strand -strand pattern batch. pattern corresponds o-kay-google -phoneme hey-goo-gle -phoneme structure keywords. fig. visualized attention weights different pooling methods. image x-axis time y-axis different utterances training batch. pooling; sliding window maxpooling window size step global top-k maxpooling apply sliding window maxpooling global top-k maxpooling attention weights much larger near-end utterance easy understand lstm accumulated information near-end beginning thus conﬁdent produce d-vector. paper experimented different attention mechanisms keyword-based text-dependent speaker veriﬁcation system experimental results best practice shared-parameter non-linear scoring function; dividedlayer attention connection last layer output lstm; apply sliding window maxpooling attention weights. combining best practices improved baseline lstm model relative improvement. attention mechanisms especially ones using shared-parameter scoring functions could potentially used improve text-independent speaker veriﬁcation models speaker diarization systems has¸im andrew senior franc¸oise beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling fifteenth annual conference international speech communication association quan wang alan papir ignacio lopez moreno generalized end-to-end loss speaker veriﬁcation arxiv preprint arxiv. https//www.androidheadlines.com///voice-matchwill-allow-google-home-to-recognize-your-voice.html najim dehak patrick kenny r´eda dehak pierre dumouchel pierre ouellet front-end factor analysis speaker veriﬁcation ieee transactions audio speech language processing vol. georg heigold ignacio moreno samy bengio noam shazeer end-to-end text-dependent speaker veriﬁcation acoustics speech signal processing ieee international conference ieee guoguo chen carolina parada georg heigold smallfootprint keyword spotting using deep neural networks acoustics speech signal processing ieee international conference ieee rohit prabhavalkar raziel alvarez carolina parada preetum nakkiran tara sainath automatic gain control multi-style training robust small-footprint keyword spotting deep neural networks acoustics speech signal processing ieee international conference ieee chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun yoshua bengio attention-based models speech recognition advances neural information processing systems kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhudinov rich zemel yoshua bengio show attend tell neural image caption generation visual attention international conference machine learning florian schroff dmitry kalenichenko james philbin facenet uniﬁed embedding face recognition clustering proceedings ieee conference computer vision pattern recognition", "year": "2017"}