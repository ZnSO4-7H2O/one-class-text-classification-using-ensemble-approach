{"title": "Light Gated Recurrent Units for Speech Recognition", "tag": "eess", "abstract": " A field that has directly benefited from the recent advances in deep learning is Automatic Speech Recognition (ASR). Despite the great achievements of the past decades, however, a natural and robust human-machine speech interaction still appears to be out of reach, especially in challenging environments characterized by significant noise and reverberation. To improve robustness, modern speech recognizers often employ acoustic models based on Recurrent Neural Networks (RNNs), that are naturally able to exploit large time contexts and long-term speech modulations. It is thus of great interest to continue the study of proper techniques for improving the effectiveness of RNNs in processing speech signals.  In this paper, we revise one of the most popular RNN models, namely Gated Recurrent Units (GRUs), and propose a simplified architecture that turned out to be very effective for ASR. The contribution of this work is two-fold: First, we analyze the role played by the reset gate, showing that a significant redundancy with the update gate occurs. As a result, we propose to remove the former from the GRU design, leading to a more efficient and compact single-gate model. Second, we propose to replace hyperbolic tangent with ReLU activations. This variation couples well with batch normalization and could help the model learn long-term dependencies without numerical issues.  Results show that the proposed architecture, called Light GRU (Li-GRU), not only reduces the per-epoch training time by more than 30% over a standard GRU, but also consistently improves the recognition accuracy across different tasks, input features, noisy conditions, as well as across different ASR paradigms, ranging from standard DNN-HMM speech recognizers to end-to-end CTC models. ", "text": "systems natural language processing name few. another ﬁeld transformed technology automatic speech recognition thanks modern deep neural networks current speech recognizers able signiﬁcantly outperform previous gmm-hmm systems allowing applied several contexts web-search intelligent personal assistants control radiological reporting. despite progress last decade state-of-the-art speech recognizers still away reaching satisfactory robustness ﬂexibility. lack robustness typically happens facing challenging acoustic conditions characterized considerable levels non-stationary noise acoustic reverberation development robust recently fostered great success international challenges chime reverb aspire also extremely useful establish common evaluation frameworks among researchers. dominant approach automatic speech recognition relies combination discriminative generative hidden markov model normally employed acoustic modeling purposes predict context-dependent phone targets. acoustic-level predictions later embedded hmm-based framework also integrates phone-transitions lexicon language model information retrieve ﬁnal sequence words. emerging alternative end-to-end speech recognition aims drastically simplify current pipeline using fully discriminative systems learn everything data without additional human effort. popular end-to-end techniques attention models connectionist temporal classiﬁcation attention models based encoder-decoder architecture coupled attention mechanism decides input information analyze decoding step. based predicting symbols predeﬁned alphabet extra unit emits labels added. similarly hmms likelihood computed dynamic programming summing paths possible realizations ground-truth label sequence. allows optimize likelihood desired output sequence directly without need explicit label alignment. aforementioned frameworks recurrent neural networks represent valid alternative standard feed-forward dnns. rnns fact often employed speech recognition capabilities abstract—a ﬁeld directly beneﬁted recent advances deep learning automatic speech recognition despite great achievements past decades however natural robust human-machine speech interaction still appears reach especially challenging environments characterized signiﬁcant noise reverberation. improve robustness modern speech recognizers often employ acoustic models based recurrent neural networks naturally able exploit large time contexts long-term speech modulations. thus great interest continue study proper techniques improving effectiveness rnns processing speech signals. paper revise popular models namely gated recurrent units propose simpliﬁed architecture turned effective asr. contribution work two-fold first analyze role played reset gate showing signiﬁcant redundancy update gate occurs. result propose remove former design leading efﬁcient compact single-gate model. second propose replace hyperbolic tangent relu activations. variation couples well batch normalization could help model learn long-term dependencies without numerical issues. results show proposed architecture called light reduces per-epoch training time standard also consistently improves recognition accuracy across different tasks input features noisy conditions well across different paradigms ranging standard dnn-hmm speech recognizers endto-end models. deep learning emerging technology considered promising directions reaching higher levels artiﬁcial intelligence paradigm rapidly evolving noteworthy achievements last years include among others development effective regularization methods improved optimization algorithms better architectures exploration generative models deep reinforcement learning well evolution sequence sequence paradigms also represent important milestones ﬁeld. deep learning deployed wide range domains including bioinformatics computer vision machine translation dialogue ieee. personal material permitted. permission ieee must obtained uses current future media including reprinting/republishing material advertising promotional purposes creating collective works resale redistribution servers lists reuse copyrighted component work works. ./tetci.. http//ieeexplore.ieee.org/ document// machine learning community research novel powerful models active research topic. general-purpose rnns long short term memories subject several studies modiﬁcations past years evolution recently novel architecture called gated recurrent unit simpliﬁes complex lstm cell design. work continues efforts revising grus. differently previous efforts primary goal derive general-purpose modify standard design order better address speech recognition. particular major contribution paper twofold first propose remove reset gate network design. similarly found removing signiﬁcantly affect system performance also certain redundancy observed update reset gates. second propose replace hyperbolic tangent rectiﬁed linear unit activations state update equation. relu units shown effective sigmoid non-linearities feed-forward dnns despite recent success non-linearity largely avoided rnns numerical instabilities caused unboundedness relu activations composing many times layers sufﬁciently large weights lead arbitrarily large state values. however coupling relu-based batch normalization experience numerical issues. allows take advantage techniques proven effective mitigate vanishing gradient problem well speed network training. evaluated proposed architecture different tasks datasets input features noisy conditions well different frameworks results show revised architecture reduces per-epoch training wall-clock time improving recognition accuracy. moreover proposed solution leads compact model arguably easier interpret understand implement simpliﬁed design based single gate. rest paper organized follows. sec. recalls standard architecture sec. illustrates detail proposed model related work. sec. description adopted corpora experimental setup provided. results reported sec. finally conclusions drawn sec. suitable architecture able learn short long-term speech dependencies represented rnns rnns indeed potentially capture temporal information dynamic fashion allowing network freely decide amount contextual information time step. several works already highlighted effectiveness rnns various speech processing tasks speech recognition speech enhancement speech separation well speech activity detection training rnns however complicated vanishing exploding gradients impair learning longterm dependencies although exploding gradients tackled simple clipping strategies vanishing gradient problem requires special architectures properly addressed. common approach relies so-called gated rnns whose core idea introduce gating mechanism better controlling information various time-steps. within family architectures vanishing gradient issues mitigated creating effective shortcuts gradients bypass multiple temporal steps. popular gated rnns lstms often achieve state-of-the-art performance several machine learning tasks including speech recognition lstms rely memory cells controlled forget input output gates. despite effectiveness sophisticated gating mechanism might result overly complex model. hand computational efﬁciency crucial issue rnns considerable research efforts recently devoted development alternative architectures noteworthy attempt simplify lstms recently novel model called gated recurrent unit based multiplicative gates. particular standard architecture deﬁned following equations vectors corresponding update reset gates respectively represents state vector current time frame element-wise multiplications denoted activations gates logistic sigmoid functions constrain take values hyperbolic tangent. network current input vector parameters model matrices architecture ﬁnally includes trainable bias vectors added non-linearities applied. shown current state vector linear interpolation previous activation update gate decides much units update activations. linear interpolation component learning long-term dependencies. close fact previous state kept unaltered remain unchanged arbitrary number time steps. hand close zero network tends favor previous introduction grus follows reset gate useful signiﬁcant discontinuities occur sequence. language modeling happen moving text another semantically related. situation convenient reset stored memory avoid taking decision biased unrelated history. nevertheless believe speciﬁc tasks like speech recognition functionality might useful. fact speech signal sequence evolves rather slowly past history virtually always helpful. even presence strong discontinuities instance observable boundary vowel fricative completely resetting past memory harmful. hand helpful memorize phonotactic features since phone transitions likely others. also argue certain redundancy activations reset update gates might occur processing speech sequences. instance necessary give importance current information model small values similar effect achieved update gate only small values assigned latter solution tends weight candidate state depends heavily current input. similarly high value assigned either order place importance past states. redundancy also highlighted fig. temporal correlation average activations update reset gates readily appreciated trained timit. degree standard tanh activations less used feedforward networks work well piecewise-linear activations training deeper networks adoption relu-based neurons shown effective improving limitations common past rnns. numerical instabilities originating unbounded relu functions applied long time series. however coupling activation function batch normalization turned helpful taking advantage relu neurons without numerical issues discussed next sub-section. batch normalization recently proposed machine learning community addresses so-called internal covariate shift problem normalizing mean variance layer’s pre-activations training minibatch. several works already shown technique effective improve system performance speed-up training procedure batch normalization applied rnns different ways. authors suggest apply feed-forward connections only normalization step extended recurrent connections using separate statistics time-step. work tried approaches observe substantial beneﬁts extending batch normalization recurrent parameters reason applied technique feed-forward connections obtaining compact model almost equally performing signiﬁcantly less computationally expensive. batch normalization limited feed-forward connections indeed related computations become independent time step performed parallel. offers possibility apply reduced computational efforts. outlined previous sub-section coupling proposed model batchnormalization could also help limiting numerical issues relu rnns. batch normalization fact rescales neuron pre-activations inherently bounding values relu neurons. model concurrently takes advantage well-known beneﬁts relu activation batch normalization. experiments found latter technique helps numerical issues also limited feed-forward connections only. minibatch mean variance respectively. small constant added numerical stability. variables trainable scaling shifting parameters introduced restore network capacity. note presence makes biases redundant. therefore omitted ﬁrst attempt remove grus recently single-gate architecture called minimal gated recurrent unit achieves performance comparable obtained standard grus handwritten digit recognition well sentiment classiﬁcation task. best knowledge contribution ﬁrst attempt explores architectural variation speech recognition. recently attempts also done embedding relu units framework. instance authors replaced tanh activations relu neurons vanilla showing capability model learn long-term dependencies proper orthogonal initialization adopted. work extend relu architecture. summary novelty approach consists integration three design aspects single model turned particularly suitable speech recognition. potential beneﬁts li-grus preliminarily observed part work speech recognition described study extends previous effort several ways. first better analyze correlation arising reset update gates. analyze gradient statistics better study impact batch normalization. moreover assess approach larger variety speech recognition tasks considering several different datasets well noisy reverberant conditions. finally extend experimental validation end-to-end model. ﬁrst experiments timit corpus performed test proposed model close-talking scenario. experiments based standard phoneme recognition task aligned proposed kaldi recipe validate model realistic scenario experiments also conducted distant-talking conditions dirha-english corpus reference context domestic environment characterized presence non-stationary noise acoustic reverberation training based original wsjk corpus contaminated impulse responses measured real apartment. test phase carried real simulated datasets consisting sentences uttered native american speakers. development sentences uttered different speakers also used hyperparameter tuning. test approach different reverberation conditions contaminated versions latter training test data generated different impulse responses. simulations based image method correspond four different reverberation times details realistic impulse responses adopted corpus found additional experiments conducted chime dataset based real simulated data recorded four noisy environments training composed noisy sentences recored microphones uttered total speakers. development based sentences uttered four speakers test based real utterances simulated sentences four speakers. experiments reported paper based single channel setting test phase carried single microphone information chime data found evaluate proposed model larger scale task additional experiments performed tedtalk dataset released context iwslt evaluation campaigns training composed talks total hours speech. development test composed talks test sets based talks talks respectively. architecture adopted experiments consisted multiple recurrent stacked together prior ﬁnal softmax classiﬁer. recurrent layers bidirectional rnns obtained concatenating forward hidden states backward hidden states recurrent dropout used regularization technique. since extending standard dropout recurrent connections hinders learning long-term dependencies followed approach introduced tackles issue sharing dropout mask across time steps. moreover batch normalization adopted exploiting method suggested discussed sec. feedforward connections architecture initialized according glorot’s scheme recurrent weights initialized orthogonal matrices similarly gain factor batch normalization initialized shift parameter initialized sentences sorted ascending order according lengths starting shortest utterance minibatches sentences progressively processed training algorithm. sorting approach minimizes need zero-paddings forming minibatches resulting helpful avoid possible biases batch normalization statistics. moreover sorting approach exploits curriculum learning strategy shown slightly improve performance ensure numerical stability gradients. optimization done using adaptive moment estimation algorithm running epochs performance development monitored epoch learning rate halved performance improvement went certain threshold gradient truncation applied allowing system learn arbitrarily long time dependencies. learning rate number hidden layers hidden neurons layer dropout factor) optimized development data. particular guessed initial values according experience starting performed grid search progressively explore better conﬁgurations. total experiments conducted various models. dnn-hmm experiments trained predict context-dependent phone targets. feature extraction based blocking signal frames overlap experimental activity conducted considering different acoustic features i.e. mfccs log-mel ﬁlter-bank features well fmllr features labels derived performing forced alignment procedure original training datasets. standard recipe kaldi details test posterior probabilities generated frame normalized prior probabilities. obtained likelihoods processed hmm-based decoder that integrating acoustic lexicon language model information single search graph ﬁnally estimates sequence words uttered speaker. part system implemented theano coupled kaldi decoder form context-dependent rnn-hmm speech recognizer. models used experiments consisted layers bidirectional rnns either units. unlike experiments weight noise used regularization. application weight noise simpliﬁcation adaptive weight noise successfully used before regularize ctc-lstm models weight noise applied weight matrices sampled zero mean normal distribution standard deviation batch normalization used initialization settings experiments. glorot’s scheme used initialize weights input features experiments dimensional fbank features features also used original work ctc-lstm models speech recognition layer trained label set. decoding done using best-path method without adding external phone-based language model. decoding labels mapped ﬁnal label set. models trained epochs using batches utterances. experiments reported following based hybrid dnn-hmm speech recognizers since latter paradigm typically reaches state-of-the-art performance. however sake comparison also extended experimental validation end-to-end model. precisely sub-section ﬁrst quantitatively analyze correlations update reset gates standard gru. sub-section extend study analysis gradient statistics. role batch normalization experiments described sub-sections respectively. speech recognition performance reported timit dirha-english chime well ted-talk corpus sub-sections respectively. computational beneﬁts li-gru ﬁnally discussed sub-section v-i. correlation update reset gates preliminarily discussed sec. sub-section take step analyzing quantitative using cross-correlation function deﬁned particular cross-correlation average activations update reset gates shown fig. gate activations computed input frames time step average hidden neurons considered. cross-correlation displayed along autocorrelation represents upper-bound limit former function. fig. clearly shows high peak revealing update reset gates redundant. peak maximum centered indicating almost no-delay occurs gate activations. result obtained single-layer bidirectional neurons mfcc features trained timit. training-step cross-correlation averaged development sentences. peak training epochs showing attributes rather quickly similar role update reset gates. fact epochs correlation peak reaches maximum value almost maintained subsequent training iterations. goal table reports norm gradient main parameters considered models. results reveal reset gate weight matrices smaller gradient norm compared parameters. result somewhat expected since reset gate parameters processed different nonlinearities attenuate gradients. would anyway indicate that average reset gate less impact ﬁnal cost function supporting removal design. avoiding norm gradient tends increase suggests functionalities reset gate performed model parameters. norm increases case li-grus adoption relu units. non-linearity indeed improves back-propagation gradient time-steps hidden layers making long-term dependencies easier learn. results obtained used subsection preliminary analysis correlation gradients done previous sub-sections compare models terms ﬁnal speech recognition performance. highlight importance batch normalization table compares phone error rate achieved without technique. results show batch normalization helpful improve performance leading relative improvement m-gru proposed ligru. latter improvement conﬁrms model couples particularly well technique adopted relu activations. without batch normalization relu activations li-gru unbounded tend cause numerical instabilities. according experience convergence li-gru without batch normalization achieved setting rather small learning rate values. latter setting however lead poor performance clearly emerged experiment coupling li-gru technique strongly recommended. table summarizes results timit data set. experiments li-gru clearly outperforms standard showing effectiveness proposed model even end-to-end framework. improvement obtained without batch normalization similarly observed hybrid systems latter technique leads better performance coupled ligru. however smaller performance gain observed batch normalization applied ctc. result could also related different choice regularizer weight noise used instead recurrent dropout. general pers higher hybrid systems. end-to-end methods fact relatively young models currently still less competitive complex dnn-hmm approaches. believe hybrid speech recognizers could partially reduced experiments careful setting hyperparameters introduction external phone-based language model. main focus paper however show effectiveness proposed ligru model fair comparison hybrid systems scope work. results table highlighted proposed li-gru model outperforms architectures. sub-section extend study performing detailed comparison popular architectures. provide fair comparison batch normalization hereinafter applied considered models. moreover least experiments varying initialization seeds conducted architecture. results thus reported average corresponding standard deviation. table presents performance obtained timit dataset. ﬁrst reports results achieved simple relu activations although architecture recently shown promising results machine learning tasks results conﬁrm gated recurrent networks outperform traditional rnns. also observe grus tend slightly outperform lstm model. expected mgru achieves performance similar obtained standard grus supporting speculation redundant role played reset gate speech recognition application. last reports performance achieved proposed model which besides removing reset gate relu activations used. li-gru performance indicates architecture consistently outperforms rnns considered input features. remarkable achievement average obtained fmllr features. best knowledge result yields best published performance timit test-set. previous results obtained optimizing main hyperparameters model development set. table reports outcome optimization process corresponding best architectures obtained architecture. models best performance achieved hidden layers neurons. also worth noting ﬁrst experiments timit sub-section assess model challenging realistic distant-talking task using dirha english corpus. challenging aspect dataset acoustic mismatch training testing conditions. training fact performed reverberated version test characterized non-stationary noises reverberation. tables viii summarize results obtained results exhibit trend comparable observed timit conﬁrming li-gru still outperform even challenging scenario. results consistent real simulated data well across different features considered study. performance reported table highlights comparable error rates standard m-gru distanttalking case even observe small performance gain removing reset gate. suppose behaviour reverberation implicitly introduces redundancy signal multiple delayed replicas sample. results forward memory effect make reset gate ineffective. fig. extend previous experiments generating simulated data different reverberation times ranging outlined sec. iv-a. order simulate realistic situation different impulse responses used training testing purposes. additive noise considered experiments. expected performance degrades reverberation time increases. similarly previous achievements still observe li-gru outperform considered reverberation conditions. sub-section extend results chime corpus important benchmark ﬁeld thanks success chime challenges tab. comparison across various architectures presented. sake comparison results obtained ofﬁcial chime also reported ﬁrst rows. results obtained section directly comparable best systems chime competition. purpose work indeed techniques multi-microphone processing data-augmentation system combination well lattice rescoring used here. results conﬁrm trend previously observed highlighting signiﬁcant relative improvement achieved passing proposed li-gru. similarly ﬁndings previous section small beneﬁts observed removing reset gate. largest performance however reached adopting relu units conﬁrming effectiveness architectural variation. note also systems signiﬁcantly outperform baseline even latter based sequence discriminative training tab. splits performance real test four noisy categories. li-gru outperforms considered environments performance gain higher challenging acoustic conditions met. instance obtain relative improvement environment relative improvement observed street recordings. tab. reports comparison li-gru ted-talks corpus. experiments performed standard mfcc features four-gram language model considered decoding step details). results test sets consistently shows performance gain achieved proposed architecture. conﬁrms effectiveness li-gru even larger scale task. particular relative improvement achieved. improvement statistically signiﬁcant according matched pairs sentence segment word error test conducted nist sclite stat tool p-value previous subsections reported several speech recognition results showing li-gru outperforms rnns. sub-section ﬁnally focus another aspect proposed architecture namely improved computational efﬁciency. table xiii compare perepoch wall-clock training time li-gru models. training time reduction achieved proposed architecture datasets. reduction reﬂects amount parameters saved li-gru also around reduction computational complexity originated compact model also arises testing purposes making model potentially suitable smallfootprint studies dnns designed portable devices small computational capabilities. paper revised standard grus speech recognition purposes. proposed li-gru architecture simpliﬁed version standard reset gate removed relu activations considered. batch normalization also used improve system performance well limit numerical instabilities originated relu non-linearities. experiments conducted different paradigms tasks features environmental conditions conﬁrmed effectiveness proposed model. li-gru fact yields better recognition performance also reduces computational complexity reduction training time standard gru. authors would like thank piergiorgio svaizer insightful suggestions earlier version paper. would also thank anonymous reviewers careful reading manuscript helpful comments. gratefully acknowledge support nvidia corporation donation tesla used research. computations also made helios supercomputer university montreal managed calcul qubec compute canada. srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol. abdel-hamid mohamed jiang deng penn convolutional neural networks speech recognition ieee/acm transactions audio speech language processing vol. silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis mastering game deep neural networks tree search nature vol. jan. weninger watanabe roux hershey tachioka geiger schuller rigoll merl/melco/tum system reverb challenge using deep recurrent neural network feature enhancement ieee reverb workshop schwarz huemmer maas kellermann spatial diffuseness features dnn-based speech recognition noisy reverberant environments proc. icassp ravanelli brakel omologo bengio batch-normalized joint training dnn-based distant speech recognition proc. graves fern´andez gomez schmidhuber connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks proc. icml dahl deng acero context-dependent pretrained deep neural networks large-vocabulary speech recognition ieee transactions audio speech language processing vol. chen watanabe erdogan hershey speech enhancement recognition using multi-task learning long short-term memory recurrent neural networks proc. interspeech menne heymann alexandridis irie zeyer kitza golik kulikov drude schl¨uter haeb-umbach mouchtaris rwth/upb/forth system combination chime challenge evaluation chime challenge weninger erdogan watanabe vincent roux hershey schuller speech enhancement lstm recurrent neural networks application noise-robust proc. lva/ica eyben weninger squartini schuller real-life voice activity detection lstm recurrent neural networks application hollywood movies proc. icassp bengio simard frasconi learning long-term dependencies gradient descent difﬁcult ieee transaction neural networks vol. mar. glorot bengio understanding difﬁculty training deep feedforward neural networks proc. aistats laurent pereyra brakel zhang bengio batch normalized recurrent neural networks proc. icassp povey ghoshal boulianne burget glembek goel hannemann motlicek qian schwarz silovsky stemmer vesely kaldi speech recognition toolkit proc. asru graves mohamed hinton speech recognition deep recurrent neural networks proc. icassp barker vincent christensen green pascal chime speech separation recognition challenge. computer speech language vol. gillick some statistical issues comparison speech recognition algorithms proc. icassp renals small-footprint deep neural networks highway connections speech recognition proc. interspeech chen parada heigold small-footprint keyword spotting using deep neural networks proc. icassp sainath parada convolutional neural networks smallfootprint keyword spotting proc. interspeech hori araki yoshioka fujimoto watanabe ogawa otsuka mikami kinoshita nakatani nakamura yamato low-latency real-time meeting recognition understanding using distant microphones omni-directional camera ieee transactions audio speech language processing vol.", "year": "2018"}