{"title": "Invertible Autoencoder for domain adaptation", "tag": "eess", "abstract": " The unsupervised image-to-image translation aims at finding a mapping between the source ($A$) and target ($B$) image domains, where in many applications aligned image pairs are not available at training. This is an ill-posed learning problem since it requires inferring the joint probability distribution from marginals. Joint learning of coupled mappings $F_{AB}: A \\rightarrow B$ and $F_{BA}: B \\rightarrow A$ is commonly used by the state-of-the-art methods, like CycleGAN [Zhu et al., 2017], to learn this translation by introducing cycle consistency requirement to the learning problem, i.e. $F_{AB}(F_{BA}(B)) \\approx B$ and $F_{BA}(F_{AB}(A)) \\approx A$. Cycle consistency enforces the preservation of the mutual information between input and translated images. However, it does not explicitly enforce $F_{BA}$ to be an inverse operation to $F_{AB}$. We propose a new deep architecture that we call invertible autoencoder (InvAuto) to explicitly enforce this relation. This is done by forcing an encoder to be an inverted version of the decoder, where corresponding layers perform opposite mappings and share parameters. The mappings are constrained to be orthonormal. The resulting architecture leads to the reduction of the number of trainable parameters (up to $2$ times). We present image translation results on benchmark data sets and demonstrate state-of-the art performance of our approach. Finally, we test the proposed domain adaptation method on the task of road video conversion. We demonstrate that the videos converted with InvAuto have high quality and show that the NVIDIA neural-network-based end-to-end learning system for autonomous driving, known as PilotNet, trained on real road videos performs well when tested on the converted ones. ", "text": "similarly arbitrary learning model trained translated samples perform well samples target domain. training framework however much computationally expensive. unsupervised image-to-image translation aims ﬁnding mapping source target image domains many applications aligned image pairs available training. ill-posed learning problem since requires inferring joint probability distribution marginals. joint learning coupled mappings commonly used state-of-the-art methods like cyclegan learn translation introducing cycle consistency requirement learni.e. fab) problem fba) cycle consistency enforces preservation mutual information input translated images. however explicitly enforce inverse operation fab. propose deep architecture call invertible autoencoder explicitly enforce relation. done forcing encoder inverted version decoder corresponding layers perform opposite mappings share parameters. mappings constrained orthonormal. resulting architecture leads reduction number trainable parameters present image translation results benchmark data sets demonstrate state-of-the performance approach. finally test proposed domain adaptation method task road video conversion. demonstrate videos converted invauto high quality show nvidia neural-network-based end-toend learning system autonomous driving known pilotnet trained real road videos performs well tested converted ones. inter-domain translation problem converting instance e.g. image video domain another applicable wide variety learning tasks including object detection recognition image categorization sentiment analysis action recognition speech recognition more. high-quality domain translators ensure arbitrary learning model trained samples source domain perform well tested translated samples. translation problem posed supervised learning framework e.g. learner access corresponding pairs instances domains unsupervised learning framework e.g. paired instances available. paper focuses latter case difﬁcult time realistic acquiring data paired images often impossible practice. unsupervised domain adaptation typically solved using generative adversarial networks framework generator performs domain translation trained learn mapping source target domain discriminator trained discriminate original images target domain provided generator. setting generator usually structure autoencoder. common state-of-theart domain adaptation approaches cyclegan unit built basic approach. cyclegan addresses problem adaptation domain domain training translation networks realizes mapping realizes fba. cycle consistency loss ensures correlation input image corresponding translation. particular achieve cycle consistency cyclegan trains autoencoders minimizes adversarial loss jointly minimize figure heatmap values matrix invauto auto cycle resnet architectures mnist cifar data sets. matrices constructed multiplying weight matrices consecutive layers encoder decoder respectively. case invauto closest identity matrix. cycle consistency loss also incorporated recent implementations unit. implicitly assumed model learn mappings f−ba however explicitly imposed. consider simple example. assume ﬁrst autonecoder -layer linear multi-layer perceptron weight matrix ﬁrst layer denoted weight matrix second layer denoted thus input xa∈a outputs dexa. second autoencoder -layer encoder weight matrix decoder weight matrix input data point produce output dexb. satisfy cycle consistency requirement following hold conditions equivalent dede dede holds example contrast approach implicitly require f−ba. thus context given simple example correlate encoders decoders satisfy inversion conditions avoid performing prohibitive inversions large matrices instead guarantee conditions hold steps introducing shared parametrization encoder decoder appropriate training achieve orthonormality i.e. train autoencoder satisfy dexb arbitrary input autoencoder satisfy dexa arbitrary input since encoder decoder coupled given training leads satisfying inversion conditions. practical networks contain linear non-linear transformations. therefore propose speciﬁc architectures invertible. figure comparison invauto auto cycle convolutional resnet architectures mnist cifar data sets. matrices constructed multiplying weight matrices consecutive layers encoder decoder respectively. figure illustrate basic idea behind invauto. plots obtained training single autoencoder reconstruct input. invauto shared weights satisfying inverted non-linearities clearly obtains matrix closest identity compared methods i.e. vanilla autoencoder autoencoder cycle consistency variational autoencoder note also time invauto requires half number trainable parameters. paper organized follows section reviews literature section explains invauto details section explains apply invauto domain adaptation section demonstrates experimental veriﬁcation proposed approach section provides conclusions. unsupervised image-to-image translation models developed tackle domain adaptation problem unpaired data sets. plethora existing approaches utilize autoencoders trained framework autoencoder serves generator learning problem. includes approaches based conditional methods introducing additional components loss function forcing partial cycle consistency another approach introduces coupled gans generator autoencoder coupling obtained sharing subset weights autoencoders well discriminators. technique later extended utilize variational autoencoders generators resulting approach commonly known unit. cyclegan presents another addressing image-to-image translation speciﬁc training scheme preserves mutual information input translated images unit cyclegan constitute popular choices performing image-to-image translation. also exist learning tasks viewed instances image-to-image translation problem. among them notable approaches focus style transfer preserving content input image altering style mimic style images target domain. goal achieved introducing content style loss functions jointly optimized. finally inverse problems super-resolution also fall category image-toimage translation problems denote subsequent transformations signal propagated network denote inversions. thus architecture inverted layer layer layer encoder mirror inverted counterpart decoder. autoencoder trained reconstruct input. explain invert different types layers deep model. consider transformation input signal performed arbitrary fully-connected layer encoder parametrized weight matrix denote layer’s input denote output. thus parametrize counterpart layer decoder transpose thus considered encoder decoder layers share parametrization. therefore enforce counterpart decoder’s layer perform transformation consider transformation input image performed arbitrary convolutional layer encoder denote layer’s vectorized input image denote corresponding output. convolution implemented using matrix multiplication involving toeplitz matrix toeplitz matrix obtained kernels convolutional ﬁlters thus transformation inverse explained equations ones used before equations however toeplitz matrix. parametrize counterpart layer decoder transpose toeplitz matrix transpose toeplitz matrix practice obtained copying weights considered convolutional layer counterpart decoder’s layer implemented transposed convolutional layer parametrize counterpart residual block decoder transpose matrix given figure therefore enforce counterpart decoder’s residual block perform transformation consider bias separate layer network. then handling biases straightforward. particular layer encoder perform bias addition counterpart layer decoder bias subtracted. section validate concept invauto. goal section show proposed shared parametrization training enforce orthonormality time orthonormality property organically achieved standard architectures. compare invauto previously mentioned vanilla autoencoder autoencoder cycle consistency variational autoencoder. experimented various data sets architectures resnet). networks designed down-sampling layers up-sampling layers. encoder’s matrix decoder’s matrix constructed multiplying weight matrices consecutive layers encoder decoder respectively. test orthonormality reporting histograms cosine similarity pair rows matrix methods along mean standard deviation expect cosine similarity close invauto. show -norm rows expect rows invauto closeto-unit norm invauto enforces encoder consequently decoder orthonormal. methods explicitly demand thus orthonormality encoders weaker. observation invertible activation function bijection. paper consider modiﬁed leakyrelu activation function non-linearity model. consider transformation input signal performed non-linearity applied encoder nonlinearity deﬁned consider transformation input signal performed residual block encoder modify residual block remove internal nonlinearity given figure residual block parametrized weight matrices toeplitz matrices corresponding convolutional transposed convolutional layers residual block. denote block’s vectorized input denote corresponding output. thus transformation deﬁned conﬁrmed figures shown introduction. supplement provide three ﬁgures complement figure show diagonal off-diagonal well ratio off-diagonal diagonal various methods. reconstruction loss obtained methods also shown section supplement purpose performing domain adaptation construct dedicated architecture similar cyclegan invauto feature level generators. invauto contains encoder decoder form autoencoders. internal autoencoders used conversion between features corresponding different domains. thus encoder performs conversion features corresponding domain features corresponding domain decoder hand performs conversion features corresponding domain features corresponding domain since form invauto realizes inversion shares parameters introduces strong correlations generators reduces number trainable parameters distinguishes approach cyclegan. proposed architecture illustrated figure details architecture training provided section supplement. figure architecture domain translator invauto inputs translator. converted image domain converted image domain. invertible autoencoder built encoder decoder autoencoder. enca encb feature extractors deca decb ﬁnal layers generators genb i.e. gena i.e. respectively. discriminators disa disb discriminate whether input comes generator original data next demonstrate experiments domain adaptation problems. compare model unit cyclegan. used publicly available implementations methods available https//github.com/mingyuliutw/ unit/ https//github.com/junyanz/ pytorch-cyclegan-and-pixpix/.the details architecture training process summarized section supplement. data sets last tasks i.e. originally paired however randomly permuted train model unsupervised fashion. training testing images furthermore resized resolution. visual results image conversion presented figures invauto visually performs comparably state-of-the-art methods. task trained autoencoders domains i.e. trained reconstruct well images domain reconstruct badly images domain. autoencoders evaluate quality converted images high reconstruction loss autoencoder images converted resemble corresponding domain implies low-quality image translation. table contains results numerical evaluation shows performance invauto similar state-of-the-art techniques compare invauto furthermore contained within performance range established cyclegan unit test quality image-to-image translations obtained invauto nvidia evaluation system autonomous driving described details system evaluates performance already trained nvidia neural-network-based end-to-end learning platform autonomous driving test video using simulator autonomous driving. system uses following performance metrics evaluation autonomy position precision comfort. describe metrics described well mentioned paper. emphasize metrics expressed percentage corresponds best performance. collected high-resolution videos road night camera inside car. video frames. pictures resized resolution conversion resized back original size used domain translator well cyclegan convert collected video night video also collected night video video evaluate model used aforementioned nvidia evaluation system converted videos used testing sets system. report results table pilotnet model used testing trained mostly videos thus expected perform worse night videos. therefore performance original night video worse video converted video terms autonomy position precision. comfort deteriorates inconsistency consecutive frames converted video i.e. videos converted frame-byframe apply post-processing ensure smooth transition frames. results invauto cyclegan comparable. proposed novel architecture call invertible autoencoder which opposed common deep learning architectures allows layers model performing opposite operations share weights. achieved enforcing orthonormal mappings layers model. demonstrate applicability proposed architecture problem domain adaptation evaluate benchmark data sets autonomous driving task. performance proposed approach matches state-of-the-art methods requires less trainable parameters. table test reconstruction loss invauto auto cycle convolutional resnet architectures mnist cifar data sets. signiﬁcantly higher reconstruction loss construction. figure heatmap values matrix invauto auto cycle convolutional resnet architectures mnist cifar data sets. matrices constructed multiplying weight matrices consecutive layers encoder decoder respectively. case invauto closest identity matrix. generator architecture implementation invauto contains invertible residual blocks images blocks used encoder remaining decoder. layers decoder inverted versions encoder’s layers. furthermore down-sampling layers upsampling layers model trained images three down-sampling layers three up-sampling layers model trained images. details generator’s architecture listed table table convenience conv denote convolutional layer convnormrelu denote convolutionalinstancenorm-leakyrelu layer invres denote invertible residual block tanh denote hyperbolic tangent activation function. negative slope leakyrelu function ﬁlters square following notations represents ﬁlter size represents number output feature maps. paddings added correspondingly. discriminator architecture similar discriminator architecture patchgan described table architecture training images. criterion optimization training loss cycle consistency equation adam optimizer learning rate also penalty weight", "year": "2018"}