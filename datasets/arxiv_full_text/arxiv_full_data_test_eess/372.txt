{"title": "Compressed Anomaly Detection with Multiple Mixed Observations", "tag": "eess", "abstract": " We consider a collection of independent random variables that are identically distributed, except for a small subset which follows a different, anomalous distribution. We study the problem of detecting which random variables in the collection are governed by the anomalous distribution. Recent work proposes to solve this problem by conducting hypothesis tests based on mixed observations (e.g. linear combinations) of the random variables. Recognizing the connection between taking mixed observations and compressed sensing, we view the problem as recovering the \"support\" (index set) of the anomalous random variables from multiple measurement vectors (MMVs). Many algorithms have been developed for recovering jointly sparse signals and their support from MMVs. We establish the theoretical and empirical effectiveness of these algorithms at detecting anomalies. We also extend the LASSO algorithm to an MMV version for our purpose. Further, we perform experiments on synthetic data, consisting of samples from the random variables, to explore the trade-off between the number of mixed observations per sample and the number of samples required to detect anomalies. ", "text": "abstract consider collection independent random variables identically distributed except small subset follows different anomalous distribution. study problem detecting random variables collection governed anomalous distribution. recent work proposes solve problem conducting hypothesis tests based mixed observations random variables. recognizing connection taking mixed observations compressed sensing view problem recovering support anomalous random variables multiple measurement vectors many algorithms developed recovering jointly sparse signals support mmvs. establish theoretical empirical effectiveness algorithms detecting anomalies. also extend lasso algorithm version purpose. further perform experiments synthetic data consisting samples random variables explore trade-off number mixed observations sample number samples required detect anomalies. problem anomaly detection focus interest many ﬁelds science engineering including network tomography cognitive radio radar paper study problem identifying small number anomalously distributed random variables within much larger collection independent otherwise identically distributed random variables. call random variables following anomalous distribution anomalous random variables. conventional approach detecting anomalous random variables sample random variable individually apply hypothesis testing techniques recent paper proposes perform hypothesis testing mixed observations random variables instead samples individual random variables. call technique compressed hypothesis testing. approach motivated recent development compressed sensing signal processing paradigm shows small number random linear measurements signal sufﬁcient accurate reconstruction. large body work area shows optimization-based iterative methods reconstruct signal accurately efﬁciently samples taken sensing matrix satisfying certain incoherence properties compressed sensing also studied bayesian framework signals assumed obey prior distribution results presented show mixed measurement approach achieves better detection accuracy fewer samples compared conventional un-mixed approach. however compressed hypothesis testing requires distributions random variables known priori available practice. further authors pointed proposed approach requires conducting large number hypothesis tests especially number random variables collection large rendering approach computationally prohibitive. efﬁcient algorithms proposed alternatives analytical study performance provided. propose methods detecting anomalous random variables require minimal knowledge distributions computationally efﬁcient whose performance easy characterize. begin generalizing compressed hypothesis testing method posing problem multiple measurement vector problem compressed sensing setting collection signals recovered simultaneously assumption commonalities sharing support. related vein work involves signals smoothly varying support consistent changes slowly time compressed hypothesis testing certainly motivated compressed sensing techniques authors formally frame anomaly detection problem compressed sensing setting. also focus compressed sensing algorithms might eliminate need prior knowledge distributions might lead efﬁcient detection large collections random variables. following view collection random variables random vector identify indices anomalous random variables within random vector. also draw analogy collection independent samples random vector ensemble signals practice signals often become available time. speciﬁcally consider random vector xn’s independent random variables. assume follows distributions call prevalent distribution anomalous distribution. denote index random variables denote index random variables follow anomalous distribution. denote independent realization random vector time time-step obtain mixed observations applying sensing matrix rm×n signals formulation necessarily sparse different supports since samples random vector changing time. nevertheless still close connection formulation recovering common sparse support collection signals mmvs. index anomalous random variables corresponds index anomalies signals shared signals. index thus viewed common support anomalies signals motivates consider applicability many algorithms designed signal reconstruction. further analytical studies many algorithms readily available. therefore investigate algorithms applied adapted anomaly detection problem consideration analyze performance detection accuracy theory numerical experiments. focus algorithms presented paper extending deﬁnitions so-called joint sparsity models introduce signal models jsm-r jsm-r problem anomaly detection. jsm-r jsm-r signals adapt several signal reconstruction algorithms anomaly detection. additionally develop algorithm jsm-r model extends least absolute shrinkage selection operator algorithm framework. show theoretically numerically algorithms accurately detect anomalous random variables. also provide numerical results demonstrate trade-off number time-steps number mixed observations time-step needed detect anomalies. section introduce models jsm-r jsm-r four algorithms repurposed signal recovery anomaly detection well lasso algorithm. also provide theoretical guarantees section. section explore performance algorithms conducting numerical experiments strategic choices parameters involved. finally conclude section help keep track notation provide handy reference table section adopt convention random variables upper case realizations lower case. matrix entries subscripted indices. ﬁrst index indicate position second indicate column position. section introduce signal models anomaly detection problem describe algorithms detecting anomalous random variables signal models. also provide theoretical guarantees algorithms. recall consider problem detecting anomalous random variables collection random variables anomalous random variables different probability distribution remaining random variables. seek identify anomalous random variables independent realizations random variables. emphasize framing random variable problem compressed sensing problem refer independent realizations signals. signals important commonality share indices anomalous entries commonality among signals already explored ﬁeld distributed compressed sensing recovering signals speciﬁc correlation among them. three joint sparsity models introduced characterize different correlation structures. utilize commonality signals anomaly detection propose signal models motivated jsms deﬁned namely jsm- jsm-. since signals consideration realizations random variables term models jsm-r jsm-r respectively appended indicates random variable version existing jsms. deﬁne models ﬁrst brieﬂy describe jsm- jsm-. jsm- signals jointly sparse signals share support jsm- signals consist components non-sparse common component shared signals sparse innovation component different signal. innovation components jsm- signals share support. next extend deﬁnitions signals anomaly detection setting. jsm-r jsm-r models deﬁned follows deﬁnition random variable anomalous indices. signal ensemble rn×t entries denotes realization time jsm-r signal ensemble when small large small common component shared jsm-r signal model assumes small amplitude variables generated prevalent distribution large amplitude variables generated anomalous distribution. model characterizes scenario anomalies exhibit large spikes. model relates sparse signal model support sparse signal corresponds indices anomalous random variables. fact jsm-r signal sparse signal additive gaussian noise. example anomalies following jsm-r model network sensors completely malfunction produce signals vastly different amplitudes rest sensors. different jsm-r signals jsm-r signal model introduced constraints amplitude signal entries rather signals different time-steps assumed share unknown common component signals different time-steps. note common component prevalent distribution anomalous distribution. further innovation component assumed follow jsm-r signal model. model characterizes scenario exists background signal change time anomalies exhibit large spikes background signal. common component jsm-r signals longer correspond sparse signal model. jsm-r model applications geophysical monitoring constant background signal present anomalies appear large spikes erratic behavior. figure provides visual illustration model nuances. brieﬂy describe algorithms study paper among three jsm-r signals jsm-r signals. algorithms jsm-r signals originally proposed recovering jsm- signals including one-step greedy algorithm multiple measurement vector simultaneous orthogonal matching pursuit algorithm. propose version lasso algorithm detecting anomalies jsm-r signals investigate performance numerical experiments. algorithms jsm-r also proposed recovering jsm- signals including transpose estimation common component algorithm alternating common innovation estimation algorithm. presented algorithms goal identify indices anomalous random variables mixed measurements φtxt) number anomalies assumed known priori. ﬁrst describe three algorithms applied anomaly detection jsm-r signals. osga algorithm non-iterative greedy algorithm introduced recover support jsm- signals based inner products measurement columns sensing matrix show theorem condition prevalent anomalous distributions osga algorithm able recover anomaly indices jsm-r model using small number measurements time-step. although osga algorithm shown work asymptotically perform well small number time-steps available. empirical evidence conﬁrmed conjecture osga algorithm used reconstruct jsm- signals thus consider approaches like matching pursuit problem. next describe version orthogonal matching pursuit algorithm proposed fig. depiction existing joint sparsity models models developed anomaly detection distributions used generate example ones used numerical experiments section table index anomalies mmv-somp algorithm iterative greedy pursuit algorithm recovery jointly sparse signals. somp ﬁrst proposed adapted framework since focus signal recovery detecting anomalous entries adapt algorithm jsm-r signal model. adapted algorithm presented algorithm identiﬁes anomaly indices time. iteration column index sensing matrices accounts largest residual across signals time-steps selected. remaining columns sensing matrix orthogonalized. algorithm stops iterations number anomalous random variables. show numerical experiments section adapted mmv-somp algorithm performs better osga algorithm small number time-steps. lasso algorithm aims sparse solution regression problem constraining norm solution lasso algorithm also considered efﬁcient algorithm anomaly detection mixed observations. however authors considered lasso algorithm using measurement time-step. paper extend lasso algorithm general setting term mmv-lasso algorithm. mmv-lasso algorithm described algorithm measurements time-steps concatenated vertically become vector sensing matrices rm×n also concatenated vertically become r×n. concatenated measurements sensing matrices regular lasso algorithm anomaly indices found taking indices corresponding largest amplitudes estimate. lasso problem step algorithm tackled various approaches scope paper. difference jsm-r jsm-r signals jsm-r signals share common component unknown. thus algorithms jsmr signals ﬁrst estimate common component mixed measurement subtract contribution component measurement. tecc algorithm proposed recovering jsm- signals. also adapt algorithm focus detecting anomalous indices jsm-r signals adapted algorithm found algorithm ﬁrst step tecc algorithm estimates common component jsm-r signals. using estimate contribution remaining innovation component measurement estimated. algorithms jsm-r signals applied identify anomaly indices. show theorem tecc algorithm able identify anomalous variables conditions prevalent anomalous distributions. similar osga algorithm theorem guarantees success tecc algorithm asymptotic case goes inﬁnity perform well small next describe alternative algorithm also proposed cases small acie algorithm extension tecc algorithm also introduced based observation initial estimate common component sufﬁciently accurate subsequent steps. instead one-time estimation tecc algorithm acie algorithm iteratively reﬁnes estimates common component innovation components. acie algorithm also easily adapted jsm-r signals anomaly detection. acie algorithm described algorithm ﬁrst obtain initial estimate anomaly index setk using tecc algorithm. iteration build basis number measurements time-step subset basis vectors corresponding indices orthonormal columns spans orthogonal complement φtk. recall algorithm designed jsm-r signals variables generated prevalent distribution much smaller amplitude anomalous distribution. following theorem shows jsm-r signals osga algorithm able identify indices anomalous variables asymptotically measurements time-step. theorem theorem sensing matrix contain entries i.i.d time-step suppose random variables distributed assuming measurements time-step osga recovers probability approaching diving proof theorem ﬁrst observe signals correspond jsm-r signals zero mean potentially small variance prevalent distribution signal entry expected small amplitude. contrast non-zero mean similar possibly larger variance anomalous distribution amplitude expected much larger. proof. assume convenience without loss generality anomalous random variables indexed prevalent random variables indexed consider test statistic t=yt φtn) sample mean random variable yφn) large numbers case. ﬁnal step compare expected values establish distinguishable general conditions. without loss generality select prevalent case anomalous case. note refers cases respectively statistics good statistics setting. them reﬂects incorrect estimate sparse support good reﬂects correct estimate sparse support. prevalent case substituting yφk+) rearranging obtain yφk+) last step follows independence independence xn’s other. claim cross-terms zero. this φk+) entries vectors i.i.d. note mutually distinct mutually independent. case have eabcb] examining expected value individually recall distributed thus recalling rest distributed subsequent cases. establish eφk+)] eφn)φk+)] results without argument make assumptions finally substituting expected values calculated grows large statistic converges eyφk+)] overwhelming probability. next theorem shows asymptotically algorithm able detect anomalous variables measurements time-step jsm-r signals. recall jsm-r signals unknown common component shared signals time-steps signal different innovation component follows jsm-r model. following theorem proof assume algorithm implemented step algorithm results like theorem exist algorithms jsm-r algorithm could used step theorem would still hold. theorem theorem sensing matrix time-step contain entries i.i.d. random variables distributed recovers probability approaching section evaluate numerically performance algorithms anomaly detection. speciﬁcally examine success rate determining anomalous index signal matrix rn×t whose columns signals obtained time-step share anomalous indices. performance assessed various settings varying number anomalies number columns number mixed measurement time-step. focus trade-off number measurements number time-steps required identify varying numbers anomalies. experiments measurement matrices rm×n comprise independent entries measurement vectors calculated φtxt) obtain estimate algorithm’s recovery success rate high conﬁdence instead using ﬁxed number random trials across different parameter combinations adaptively determine necessary number trials jeffreys interval bayesian two-tailed binomial proportion conﬁdence interval. conﬁdence interval around true success rate shrinks width smaller report current proportion successes recovery accuracy algorithm. signals generated models corresponding jsm-r jsm-r signal deﬁnitions introduced section algorithms applied jsm-r signals algorithms applied jsm-r signals. experiments summarized table jsm-r experiments assume mean zero prevalent distribution much larger mean anomalous distribution letting variance small. shown previous section signals generated distributions satisfy deﬁnitions jsm-r. jsm-r experiments explore settings first prevalent anomalous distributions assumed different means; second distributions mean. recall previous section show means distributions common components jsm- signals generated distributions. note algorithms jsm-r signals knowledge mean prevalent anomalous distributions. chose distributions table numerical simulations remain consistent observe jsm-r experiments distributions means separated three standard deviations each additional standard deviation good measure. ensures distributions statistically distinct other. explored detection accuracy affected vary proportion overlap distributions. present results recovering anomalous index jsmr signals. signal length ﬁxed results anomalies presented. value random variables follow distribution random variables follow another distribution goal recover index random variables. figure shows success rate identifying three values using osga algorithm. ﬁgure denotes success rate speciﬁc speciﬁc estimated number trials value indicated color take values figure plot success rate mmv-somp mmv-lasso algorithms respectively. three algorithms success rate anomaly identiﬁcation increases number measurements increases and/or number time-steps increases. success identiﬁcation obtained sufﬁciently large number measurements time-steps. important differences performance among three algorithms. quired anomaly detection increases number anomalies present. mmv-lasso performance seems less affected varying number anomalies performance algorithms. secondly comparing figure reveals mmv-somp requires fewer time-steps osga algorithm reach success given number measurements. thirdly mmv-lasso algorithm requires signiﬁcantly fewer measurements time-steps success compared osga mmv-somp. finally asymmetry effect increasing number measurements versus increasing number time-steps performance osga mmv-somp. algorithms increasing number measurements effective increasing number time-steps improving performance. obvious asymmetry recovery performance found mmv-lasso algorithm. near symmetry phenomenon mmv-lasso expected since doubling either doubles number rows matrix algorithm providing similar amounts information algorithm. comparison benchmark note authors propose lasso efﬁcient algorithm detect anomalies. performance proposed method shown ﬁrst phase diagrams figure here expand application lasso allowing trade-off number measurements time-step number time-steps measurements taken. applications ability store multiple measurements time-step seeking minimize time needed accumulate data might prefer mmv-lasso algorithm detect anomalies. experiments assumed know number anomalies explore possibility estimating number anomalies detect them consider following experiments. osga calculate test statistics algorithm sort descending order; determine whether amplitudes used estimate case ensure recovery possible known results shown figure demonstrate potential methods estimate theoretical justiﬁcation methods left future work. next present results recovering anomalous index jsmr signals. similar jsm-r signals length signal number anomalies takes values unlike jsmr signals random variables follow distribution anomalous random variables follow distribution order fair comparison algorithms implement osga algorithm step tecc algorithm step acie algorithm. iteration acie algorithm performance tecc acie algorithms varying numbers measurements time-steps anomalous distribution follows presented figures range performance setting anomalous variables distributed similar figures thus omitted. sufﬁciently large number measurements time-steps algorithms able achieve success recovery anomalous index set. ﬁxed number time-steps minimum number measurements required identiﬁcation increases number anomalies increases algorithms. improvement performance acie algorithm tecc algorithm. acie algorithm requires fewer time-steps reach recovery success given number measurements; similarly requires fewer measurements recovery success given number time-steps. thus assumed prevalent anomalous distributions different variances experiments. investigate performance algorithms ratio variance changes experiment setting figure shows phase transition tecc algorithm vary ratio variances figure shows phase transition acie algorithm vary ratio variances. cases algorithms behaving might expect. smaller ratio variances measurements time-steps takes detect anomalies. paper formally posed problem detecting anomalously distributed random variables problem drawing analogy samples random variables ensembles signals. established signal models characterizing possible correlation structures among signals contain anomalous entries. based signal models showed theoretical numerical analysis many algorithms sparse signal recovery adapted anomaly detection problem. algorithms provided theoretical guarantees anomaly detection asymptotic case. experimental results synthetic data show good performance signals conforming either model sufﬁciently large number time-steps available. algorithms succeed detecting anomalies still room optimizing performance. currently algorithms require storing sensing matrices time-step memory. future work would like explore optimal ways design sensing matrices reduce memory burden. provided asymptotic anomaly detection guarantees algorithms interested providing guarantees algorithms presented. additionally interested characterizing performance bounds algorithm’s ﬁnite sample case. theorem shows variances anomalous prevalent distributions distinct anomalies detected algorithm. additional information means distributions perhaps algorithms could extended identify differences means detect anomalies even identical variances. finally theoretical results presented rely gaussian distributions. interested expanding algorithms distributions might distinguishable current approach. distributions heavy tails variance longer ﬁnite theorem assuming large numbers might incorrect convergence expected value might slow. would interesting investigate kinds heavy-tailed distributions algorithms start fail. acknowledgements initial research effort conducted research collaboration workshop women data science mathematics july held icerm. funding workshop provided icerm dimacs supported career grant ccf−. partially supported alfred sloan foundation career bigdata supported faculty start-up fund montana state university. number random variables random variables indices number anomalous random variables anomalous random variable indices |k|= number measurements time-step measurement index number time-steps measured time-step index prevalent distribution anomalous distribution random vector comprising independent random variables -dimensional matrix independent realizations time-steps n-dimensional sensing matrix i.i.d. entries n-dimensional realization time n-dimensional vertical concatenation time -dimensional vertical concatenation m-dimensional random vector deﬁned adopt convention random variables upper case realizations lower case. matrix entries subscripted indices. ﬁrst index indicate position second indicate column position. indicate column vectors substituting respective index.", "year": "2018"}