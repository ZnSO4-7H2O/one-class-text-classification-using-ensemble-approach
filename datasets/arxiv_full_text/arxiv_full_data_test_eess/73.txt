{"title": "End-to-end DNN Based Speaker Recognition Inspired by i-vector and PLDA", "tag": "eess", "abstract": " Recently several end-to-end speaker verification systems based on deep neural networks (DNNs) have been proposed. These systems have been proven to be competitive for text-dependent tasks as well as for text-independent tasks with short utterances. However, for text-independent tasks with longer utterances, end-to-end systems are still outperformed by standard i-vector + PLDA systems. In this work, we develop an end-to-end speaker verification system that is initialized to mimic an i-vector + PLDA baseline. The system is then further trained in an end-to-end manner but regularized so that it does not deviate too far from the initial system. In this way we mitigate overfitting which normally limits the performance of end-to-end systems. The proposed system outperforms the i-vector + PLDA baseline on both long and short duration utterances. ", "text": "reason end-to-end training effective long utterances text-independent speaker veriﬁcation could overﬁtting training data. second reason could previous works trained short segments even long segments used testing. reduces memory requirements training reduces risk overﬁtting introduces mismatch training test conditions. work develop end-to-end speaker veriﬁcation system initialized mimic i-vector plda baseline. system consists module extraction sufﬁcient statistics module extraction i-vectors ﬁnally discriminative plda model producing scores. three modules ﬁrst developed individually mimic corresponding part i-vector plda baseline. after modules trained individually combined system trained end-to-end manner long short utterances. end-to-end training regularize model parameters towards initial parameters deviate them. system prevented becoming different original i-vector plda baseline reduces risk overﬁtting. additionally ﬁrst developing three modules individually easily optimal architectures well detect difﬁculties aware end-to-end training. evaluate system three different data sets derived previous nist sres. three test sets contain speech various languages designed test performance long short utterances. achieved results show proposed system outperforms generatively discriminatively trained i-vector plda baselines. followed design prism dataset sense splitting data training test sets. prism contains data following sources nist fisher english switchboard. training end-to-end system initialization used female portion nist sre’ telephone condition independently tune performance blocks figure recently several end-to-end speaker veriﬁcation systems based deep neural networks proposed. systems proven competitive text-dependent tasks well text-independent tasks short utterances. however text-independent tasks longer utterances end-to-end systems still outperformed standard i-vector plda systems. work develop end-to-end speaker veriﬁcation system initialized mimic i-vector plda baseline. system trained end-to-end manner regularized deviate initial system. mitigate overﬁtting normally limits performance endto-end systems. proposed system outperforms i-vector plda baseline long short duration utterances. index terms— speaker veriﬁcation end-to-end recent years many attempts take advantage neural networks speaker veriﬁcation. attempts replaced improved components i-vector plda system neural network. example using bottleneck features instead conventional mfcc features acoustic models instead gaussian mixture models extraction sufﬁcient statistics either complementing plda replacing ambitiously take frame level features utterance input directly produce utterance level representation usually referred embedding recently proposed embedding obtained means pooling mechanism example taking mean framewise outputs layers recurrent effective approach train classifying training speakers i.e. using multiclass training order speaker veriﬁcation embeddings extracted used standard backend e.g. plda. ideally however trained directly speaker veriﬁcation task i.e. binary classiﬁcation utterances target non-target trial systems known end-to-end systems proven competitive text-dependent tasks well text-independent tasks short test utterances abundance training data however text-independent tasks longer utterances end-to-end project received funding european unions horizon research innovation programme marie sklodowska-curie grant agreement marie sklodowska-curie coﬁnanced south moravian region grant agreement project also supported google faculty research award program czech science foundation project gj-y. short lang condition derived prism language condition taking multiple short cuts original recording. durations speech cuts reﬂect evaluation plan nist sre’ precisely based cuts actual detected speech sre’ labeled development data. chose cuts follow uniform distribution split resulting equally large disjoint sets speakers overlap. used part tuning performance dplda end-to-end system. part used evaluation only. noted that simplicity test single-enrollment trials unlike sre’ system description include multi-enrollment trials plda dplda based i-vectors extracted means diagonal covariance components. i-vector extractor dimensions trained training set. training generative discriminative baseline systems used telephone data training also included short cuts derived portion training data comes non-english nonnative-english speakers. duration speech cuts follows uniform distribution seconds. cuts comprise segments total finally augmented training data labeled development data nist sre’. plda used standard plda recipe i-vectors mean length normalized. linear discriminant analysis applied prior plda training decreasing dimensions i-vectors perform additional domain adaptation score normalization. also ﬁltered training data speaker least utterances reduces total training utterances. discriminative plda dplda baseline model trained full batch i-vectors means lbfgs optimizing binary cross-entropy training data. used tune single constant used regularization imposed parameters except constant i-vectors mean length normalized. mean normalization performed decreasing dimensionality vectors initialization dplda training used corresponding plda model. dplda training prior probability target trials reﬂect sre’ evaluation operating point fig. block diagram end-to-end system. part corresponds converts features responsibilities. adding next blocks obtain ﬁrst order statistics part simulates i-vector extraction followed length normalization. parameters solid line blocks meant trained outputs dashed blocks directly computed. section describe proposed end-to-end architecture. system depicted figure ﬁrst describe modules features statistics statistics i-vectors dplda details) complete end-to-end system. system implemented using theano library ﬁrst module end-to-end system converts sequence feature vectors sufﬁcient statistics. denote module module consists network predicts vector responsibilities frame input utterance followed layer pooling frames sufﬁcient statistics. network predicts responsibilities consists four hidden layers sigmoid activation functions softmax output layer. hidden layers neurons output layer elements corresponds number components baseline gmm-ubm. train network stochastic gradient descent optimize categorical cross-entropy gmm-ubm posteriors targets. input network acoustic features described section preprocessed follows. frame window frames around current frame considered. window temporal trajectory feature coefﬁcient weighted hamming window projected ﬁrst bases results dimensional input network frame. network predicting responsibilities trained layer produces sufﬁcient statistics. input layer matrix frame-by-frame responsibilities coming previous softmax layer matrix original acoustic features without preprocessing. layer trained designed exactly reproduces standard calculation sufﬁcient statistics used i-vector extraction. noted that principle expanding features necessary order predict gmm-ubm posteriors since calculated original features. however using expanded features hope gain improvements end-to-end training. second module end-to-end system trained mimic i-vector extraction sufﬁcient statistics denote module input sufﬁcient statistics ﬁrst converted adapted supervectors overcome computational problems would arise using dimensional supervector input supervectors projected dimensional space. consists dimensional hidden layers hyperbolic tangent activation functions. last layer designed produce length normalized dimensional i-vectors. training objective average cosine distance outputs reduced lengthnormalized reference i-vectors. trained regularization. normally dplda trained iteratively using full batches i.e. update model parameters calculated based training data. whenever dplda model trained individually experiments train way. however end-to-end system would require much memory computational time. common neural networks therefore calculate update model parameters based minibatch i.e. randomly selected subset training data. approach limits number utterances speaker minibatch. utterances speaker batch would give target trials trials would statistically dependent affect training negatively individual components described previous subsections trained individually combined end-toend system. unfortunately combining modules leads large memory requirements end-to-end system. happens mainly reasons. first contrary individual training modules projection needs part network order modules connected. matrix parameters uses approximately memory. second needs process frames many different utterances batch obtain sufﬁcient number trials dplda module. mitigate problem large matrix complete end-to-end training train dplda model jointly. individual training precalculated input includes projection since input ﬁxed long updated. training minibatches pairs mitigate large memory requirements module modify training procedure keep less intermediate results memory. speciﬁcally usual training input ﬁrst forward propagated network output layer. outputs stored memory used backpropagation obtain derivative loss respect model parameter. part calculates responsibilities results variables store memory total number frames. much subsequent modules layer outputs utterance. thus order reduce memory usage calculate sufﬁcient statistics utterance time discard layer outputs block sufﬁcient statistics utterance calculated. sufﬁcient statistics utterances obtained continue forward propagation normal keeping outputs memory. backpropagation recalculate outputs block needed. achieved similar scan checkpoints. trick allows minibatches pairs instead approximately unlike individual training adam optimizer training since robust different learning rate requirements different modules compared standard sgd. halved learning rate whenever immin development epoch training dplda. report results equal error rate well average minimum detection cost function operating points namely probability target trials equal table shows results baselines end-toend system well systems stages baseline replaced show results plda dplda baseline respectively. dplda performs better generatively trained plda sets. consistent previous ﬁndings nist sre’ shows results replaced i-vector extractor plda model trained baseline output noticeable performs better supposed mimic. reason seems capable learning robust model generalizes better unseen data mainly uses larger context. experiments development phase showed using dimensional features input layer gave similar performance whereas large context features gave substantial improvement shows results train module output module instead statistics ubm. again observe small degradation compared using standard i-vector extractor interestingly change generative trained plda dplda model performs better baselines. suggests output well discriminate speakers well fulﬁll plda model assumptions generative training work well. individual training blocks proceed joint training modules using regularization towards parameters initial models. batch size pairs. seen table joint training modules improves performance data sets. finally last shows performance modules trained jointly. training discussed section seen performance almost unchanged previous row. three possible reasons this. first minibatches might small stable training. second well initialized subsequent modules already trained output model stuck local minimum. third current design quite constrained. estimates responsibilities cannot modify features used calculate statistics. issues studied future work. work developed end-to-end speaker veriﬁcation system outperforms i-vector+plda baseline three different datasets utterances many different languages long short durations. system constrained besimilar i-vector plda system. mitigated overﬁtting normally limits performance end-to-end systems. conservative approach future work explore less constrained system perform better particular complement i-vector+plda systems. found joint training modules three submodules system effective joint training three modules effective. future work therefore want develop effective strategies joint training three modules. proposed system designed using single enrollment sessions extending deal multiple enrollment sessions also important future work. ferrer bratt burget cernocky glembek graciarena lawson matejka promoting robustness speaker plchot modeling community prism evaluation https//code.google.com/p/prism-set/ plchot matˇejka silnova novotn´y diez rohdin glembek br¨ummer swart jorr´ın-prieto garc´ıa buera kenny alam bhattacharya analysis description submission nist interspeech stockholm sweden karaﬁ´at gr´ezl vesel´y hannemann sz˝oke ˇcernock´y babel system analysis adaptation based systems proceedings interspeech international speech communication association. lozano-diez silnova matˇejka glembek plchot peˇs´an burget gonzalez-rodriguez analysis optimization bottleneck features speaker recognition proceedings odyssey vol. international speech communication association. novel scheme speaker recognition using phonetically-aware deep neural network ieee international conference acoustics speech signal processing novoselov pekhovsky kudashev mendelev prudnikov non-linear plda i-vector speaker veriﬁcation ieee international conference acoustics speech signal processing sept bhattacharya alam kenny gupta modelling speaker channel variability using deep neural networks robust speaker veriﬁcation ieee spoken language technology workshop diego december moreno deep neural networks small gonzalez-dominguez footprint text-dependent speaker veriﬁcation ieee international conference acoustics speech signal processing heigold moreno bengio shazeer end-to-end text-dependent speaker veriﬁcation ieee international conference acoustics speech signal processing march snyder ghahremani povey garcia-romero carmiel khudanpur deep neural network-based speaker embeddings end-to-end speaker veriﬁcation ieee spoken language technology workshop burget plchot cumani glembek matˇejka br¨ummer discriminatively trained probabilistic linear discriminant analysis speaker veriﬁcation proc. international conference acoustics speech signal processing prague", "year": "2017"}