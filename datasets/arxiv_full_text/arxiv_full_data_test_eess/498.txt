{"title": "Improved TDNNs using Deep Kernels and Frequency Dependent Grid-RNNs", "tag": "eess", "abstract": " Time delay neural networks (TDNNs) are an effective acoustic model for large vocabulary speech recognition. The strength of the model can be attributed to its ability to effectively model long temporal contexts. However, current TDNN models are relatively shallow, which limits the modelling capability. This paper proposes a method of increasing the network depth by deepening the kernel used in the TDNN temporal convolutions. The best performing kernel consists of three fully connected layers with a residual (ResNet) connection from the output of the first to the output of the third. The addition of spectro-temporal processing as the input to the TDNN in the form of a convolutional neural network (CNN) and a newly designed Grid-RNN was investigated. The Grid-RNN strongly outperforms a CNN if different sets of parameters for different frequency bands are used and can be further enhanced by using a bi-directional Grid-RNN. Experiments using the multi-genre broadcast (MGB3) English data (275h) show that deep kernel TDNNs reduces the word error rate (WER) by 6% relative and when combined with the frequency dependent Grid-RNN gives a relative WER reduction of 9%. ", "text": "time delay neural networks effective acoustic model large vocabulary speech recognition. strength model attributed ability effectively model long temporal contexts. however current tdnn models relatively shallow limits modelling capability. paper proposes method increasing network depth deepening kernel used tdnn temporal convolutions. best performing kernel consists three fully connected layers residual connection output ﬁrst output third. addition spectro-temporal processing input tdnn form convolutional neural network newly designed gridrnn investigated. grid-rnn strongly outperforms different sets parameters different frequency bands used enhanced using bi-directional grid-rnn. experiments using multi-genre broadcast english data show deep kernel tdnns reduces word error rate relative combined frequency dependent grid-rnn gives relative reduction artiﬁcial neural networks become dominant approach acoustic modelling achieving dramatic improvements range tasks. commonly used neural network architecture timedelay neural network originally proposed often used sub-sampled form tdnn consists identical fully-connected layers repeated different time-steps. thus seen forerunner convolutional neural network applies layer speciﬁed regular shifts. tdnn restriction formulation extends easily multiple axes e.g. time frequency. shifting layer incorporates assumption important features occur various time-steps. incorporating knowledge reduces computation number parameters required. also known difference speaker express small shifts frequency. knowledge incorporated model applying cnns along frequency dimension extension tdnn work evaluates. deepening neural networks continues yield improvements performance. striking example computer vision improvement imagenet classiﬁcation task transitioning alexnet resnet based cnns increase depth came stacking convolutional layers. work proposes method deepening tdnn similar proposed make kernel used temporal convolution deep. results potentially much complex kernel. dimensional recurrent neural networks gaining popularity cnns modelling spectro-temporal variations here efﬁcient grid-rnn designed used input tdnn architecture uses separate parameters different frequency bands. experiments multi-genre broadcast english challenge task used evaluate different changes architecture demonstrate efﬁcacy proposed improvements. remainder paper organised follows. section outlines proposed extensions sub-sampled tdnn architecture discusses related work. sections present experimental setup results sec. gives conclusions. tdnn starts layer takes stack frames input replicated across different time-steps. following layer also takes input stack different time-steps preceding layer also replicated across different time-steps. initial layers thus learn detect features within narrow temporal contexts later layers operate much larger temporal context. original tdnn formulation uses shifts frame time layers expensive terms computation numbers parameters operating large temporal contexts. shown neither frame shifts uniform time shifts necessary. proposed sub-sampled tdnn illustrated fig. constructed moving ﬁrst layer across window temporal context frames shifts frames thus splitting total input context time-bins. followed binary tree combination outputs layer. tdnn baseline acoustic model paper. back-propagation gradients accumulated instantiations layers normalised. sub-sampled tdnn effective well-studied structure. however structure sub-sampled tdnns generally limits depth turn limits modelling strength. hence order keep underlying structure increase modelling capability kernels. image classiﬁcation proposed structure convolutional ﬁlter replaced stack layers. means ﬁlter learn represent abstract features. cnns shown promising results range speech tasks layer ﬁlters convolved input results multiple output-maps ﬁlter. followed application element-wise activation function function. operation layer performs input axes spectrogram written dimensionality time-axis dimensionality frequency-axis size ﬁlters index ﬁlter number ﬁlters. wlmk learned parameters cnn. generally followed pooling operation summarises patches output either computing average maximum value allows invariance shifts location feature. description ﬁlter would applied across entire input space known full weight sharing assumes feature occur across entire input space. valid assumption temporal axis hence done tdnn architecture. however generally case frequency axis characteristics different higher lower frequencies. incorporate model design limited weight sharing introduced speciﬁc ﬁlter used speciﬁc part frequency axis outputs max-pooled single scalar. initial experiments method give signiﬁcant improvements possibly strong loss information large pooling sizes. hence introduce convolution strategy strikes balance lws. frequency axis divided different overlapping frequency bands convolution followed max-pooling performed within frequency bands outputs concatenated. comparison pooling operation span entire frequency band. convolution described index frequency band shift frequency bands. frequency bands size overlap giving input -dimensional along frequency axis resulting frequency bands. ﬁlter size resulting size output reduced max-pooling layer pooling size rnns widely used speech recognition often form long short-term memory architecture recently standard time-domain lstms extended model time frequency dimensions done unfolding two-dimensional rnns along time frequency. gives advantage cnns able model correlations features time frequency. gridlstms shown outperform cnns well tf-lstms input lstm layers gridlstms tf-lstms unfolded time time-step time used computationally expensive lstm. three types temporal kernels shown fig. evaluated experiments. ﬁrst kernel standard-kernel simple layer used baseline structure. second kernel double-kernel built using layers instead one. finally resnet-kernel constructed appending layer layers bypassed residual connection. using resnet-kernel increases depth tdnn layers even though deeper networks generally improve performance neural network architectures usually harder slower train. residual connections identity mappings output initial layers input later layers described input output block layers skipped. direct connection means back-propagation gradients deep structures effective since effective minimum depth terms layers reduced. resnet-kernel effective minimum depth reduced three layers single layer. furthermore hypothesised simpler optimise residual function combined mapping. residual function resnetkernel layer sigmoid activation function followed layer linear activation function. activation function cannot used last layer resnet-kernel since non-negative output range could increase input signal. hence linear activation function used. using reasoning sec. beneﬁcial untie parameters σ-rnn along frequency axis denoted frequency dependent grid-rnn shown colour-coding fig. potentially frequency-bandspeciﬁc weights eqn. grid-rnn inherent directionality past future lower frequencies higher frequencies. recurrent weight-matrices rnns likely spectral radius especially lregularisation. means information provided model vanishes exponentially fast thus hypothesised model improved using bi-directional fd-grid-rnn constructed training fd-gridrnns parallel directionality fig. directions along time frequency axes reversed. outputs concatenated. bi-directionality increase inherent latency model unfolded structure. proposed architectures evaluated using multi-genre broadcast data speech recognition challenge task hour training selected episodes sub-titles phone matched error rate compared lightly supervised output used training supervision. hour subset sampled utterance level set. word vocabulary used trigram word level language model estimated acoustic transcripts separate million word subtitle archive. test devb contains hours audio data manually segmented utterances episodes shows. system outputs evaluated confusion network decoding well -best viterbi decoding. experiments conducted extended version log-mel ﬁlterbank analysis used without delta coefﬁcients. inputs normalised utterance level mean show-segment level variance models trained using cross-entropy criterion frame-level shufﬂing used. decision tree clustered triphone tied-states along gmm-hmm/dnn-hmm system training alignments used training sets. newbob+ learning rate scheduler used train models setup previous systems initial learning rate used models frame minibatch. l-regularisation used tuned systems tuned systems. give context data results compared layer projected lstm followed layer size output layer. lstmps implemented following data width hidden layers projected vector size dataset increased respectively. paper proposes efﬁcient grid-rnn architecture shown fig. uses vanilla-rnn groups input window seven time-bins described sec. thus neatly combined previously discussed tdnn architectures. frequency axis split different bins size shown fig. grid-rnn structure composed rnns activation function performs feature linear activation. σ-rnn called combination matrix fig. models information instantiations σ-rnn. linear activation function used improved information activation function. structure trained unfolded form shown similar work done grid-rnn described following equations eqn. seen tf-rnn equivalent proposed structure linear-rnn eqn. replaced concatenation inputs t−k. structure would much longer series non-linear mappings leading potentially strong loss information moves network. comparison linear-rnn provides linear path time network. partly analogous skip connections time within tf-lstm frequency time grid-lstm provided memory cell. system tdnn double-tdnn resnet-tdnn tdnn tdnn tdnn+deep resnet-tdnn+deep cnn-tdnn cnn-resnet-tdnn grid-rnn-tdnn grid-rnn-resnet-tdnn fd-grid-rnn-resnet-tdnn bd-fd-grid-rnn-resnet-tdnn l-lstmp wers structures given ﬁrst section table number parameters models kept roughly constant parameters adjusting layer width. therefore layer widths tdnns using standard-kernel resnet-kernel respectively. double kernel gives reduction much resnet kernel. might lower minimum path network resnet-tdnn. given relative reduction confusion network decoding less inferred deep kernels also sharpen network output distributions. validate improvement adding deep kernels tdnn model deepened simpler fashion. number layers inserted kernel output layer equivalent deepening kernel layers added kernel improvement keeping overall number parameters network constant second section table shows tdnn architectures improve increased depth kernel standard tdnn kernel four layers deep similar resnet-tdnn. however changes complimentary replacing kernel resnet-tdnn layers without residual connection gives improvement. reduction resnet-kernels increases dataset shown sec. optimal number additional layers depend task data set. hence adding additional hidden layers tdnn using grid-rnn investigated. section four table shows improvements adding grid-rnn tdnn standard-kernel resnet-kernel grid-rnn width σ-rnn linear-rnn. standard-kernel rel. werr achieved. resnet-kernel relative reduction increases parameters across frequency domain untied bidirectional fd-grid-rnn-resnet-tdnn overall rel. improvement baseline tdnn experiments show strength fdgrid-rnn cnn. besides ability model correlations features time frequency discussed previous work fd-grid-rnn many parameters designated spectro-temporal modelling. independent parameters used convolution small fraction total number parameters opposed bd-fd-gridrnn parameters. time input ﬁrst tdnn-kernel larger cnn. shows issue uses small number parameters input ﬁrst tdnn-kernel kept reasonable size. performance modelling approaches also tested using larger training results shown table models hidden layer width except σ-rnn grid-rnns size deep kernels continue give considerable improvement larger dataset. further improvement deep kernels scales better larger dataset simply adding hidden layers resnet-tdnn+deep relative lower tdnn+deep comparison smaller dataset resnet-kernel frequency convolution used ﬁlters frequency band. larger output size layer results paper presented different extensions sub-sampled tdnn architecture. deep kernels abstract feature extraction well cnns d-rnn reduce spectro-temporal variation input feature. propose d-rnn architecture rely lstms complimentary tdnn architecture. found using deep kernels d-rnn offers results best performance. overall combined structure yields relative reduction baseline tdnn architecture. sainath narayanan caroselli bacchiani misra shafran pundak chin k.c. r.j. weiss wilson variani siohan weintraub mcdermott rose shannon acoustic modeling google home proc. interspeech stockholm abdel-hamid mohamed jiang deng penn convolutional neural networks speech recognition ieee/acm transactions audio speech language processing vol. bell m.j.f. gales hain kilgour lanchantin mcparland renals wester p.c. woodland challenge evaluating multi-genre broadcast media transcription proc. asru scottsdale http//www.mgb-challenge.org lanchantin m.j.f. gales karanasou qian wang p.c. woodland zhang selection multigenre broadcast data training automatic speech recognition systems proc. interspeech francisco richmond clark fitt generating combilex pronunciations morphological analysis proc. interspeech mangu brill stolcke finding consensus speech recognition word error minimization applications confusion networks computer speech language vol. p.c. woodland qian zhang karanasou m.j.f. gales lanchantin wang cambridge university transcription systems multi-genre broadcast challenge proc. asru scottsdale", "year": "2018"}