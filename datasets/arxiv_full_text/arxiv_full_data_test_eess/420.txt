{"title": "Joint Modeling of Accents and Acoustics for Multi-Accent Speech  Recognition", "tag": "eess", "abstract": " The performance of automatic speech recognition systems degrades with increasing mismatch between the training and testing scenarios. Differences in speaker accents are a significant source of such mismatch. The traditional approach to deal with multiple accents involves pooling data from several accents during training and building a single model in multi-task fashion, where tasks correspond to individual accents. In this paper, we explore an alternate model where we jointly learn an accent classifier and a multi-task acoustic model. Experiments on the American English Wall Street Journal and British English Cambridge corpora demonstrate that our joint model outperforms the strong multi-task acoustic model baseline. We obtain a 5.94% relative improvement in word error rate on British English, and 9.47% relative improvement on American English. This illustrates that jointly modeling with accent information improves acoustic model performance. ", "text": "performance automatic speech recognition systems degrades increasing mismatch training testing scenarios. differences speaker accents signiﬁcant source mismatch. traditional approach deal multiple accents involves pooling data several accents training building single model multi-task fashion tasks correspond individual accents. paper explore alternate model jointly learn accent classiﬁer multi-task acoustic model. experiments american english wall street journal british english cambridge corpora demonstrate joint model outperforms strong multi-task acoustic model baseline. obtain relative improvement word error rate british english relative improvement american english. illustrates jointly modeling accent information improves acoustic model performance. recent breakthroughs automatic speech recognition resulted word error rate human transcribers english switchboard benchmark. however dealing acoustic condition mismatch training testing data signiﬁcant challenge still remains unsolved. well-known performance systems degrades signiﬁcantly presented speech speakers different accents dialects speaking styles encountered system training paper speciﬁcally focus acoustic modeling multi-accent asr. dialects deﬁned variations within language differ geographical regions social groups distinguished traits phonology grammar vocabulary speciﬁcally dialects associated residence ethnicity social class native language speakers. example british american english words different spellings like favour favor; different pronunciations \"sedjul en\"skedzul english word schedule; spanglish vocabulary evolve differently dialects like phrase cell phone castilian spanish uses m´ovil latin ameriuse celular english phoneme realized differently phoneme dress pronounced england wales; arabic dialects also differ intonation rhythm cues paper focus issue differing pronunciations eschewing considerations grammatical vocabulary differences. acoustic modeling across multiple accents explored many years various approaches summarized three categories uniﬁed models adaptive models ensemble models. uniﬁed model trained limited number accents generalized accent adaptive model ﬁne-tunes uniﬁed model accent-speciﬁc data assuming accent known ensemble model aggregates accent-speciﬁc recognizers produces optimal model selection combination recognition experiments revealed uniﬁed model usually underperforms adaptive model turn underperforms ensemble model note prior approaches explicitly include accent information training indirectly example different target phoneme sets various accents. contrasts sharply humans memorize phonological phonetic forms accented speech mental representations phonological forms extremely detailed include traces individual voices types voices paper propose link training acoustic models accent identiﬁcation models manner similar linking learning processes human speech perception. show joint model performs well also accent identiﬁcation compared separately-trained models. given recent success end-to-end models bidirectional long short-term memory recurrent neural network acoustic model trained connectionist temporal classiﬁcation loss function acoustic modeling. accent identiﬁcation network also blstm includes average pooling layer compute utterance-level accent embedding. also introduce joint architecture lower layers network trained using auxiliary task multi-accent acoustic modeling remains primary task network. next network hard switch accent-speciﬁc output layers preliminary experiments wall street journal american english cambridge british english corpora demonstrate joint model aid-based hard-switch achieves lower compared state-of-the-art multi-task also show model also beneﬁts joint training. remainder paper organized follows section reviews relevant literature. section introduces model multi-accent acoustic model switching strategy. section shows experiments analysis followed conclusion section closely related work illustrated hierarchical grapheme-based auxiliary phoneme-based four english dialects trained signiﬁcantly outperformed accent-speciﬁc grapheme-based respectively achieving competitive phoneme-based multi-accent similarly also trained multi-accent phonemebased loss instead adapted accent-speciﬁc output layer using target accent. relevant work compared performance training accent dialect speciﬁc acoustic models joint models. approaches predicted context-dependent triphone states using dnns used weighted ﬁnite state transducer -based decoder. example senones accents chinese predicted assuming accents within language share common state inventory elfeky implemented dialectal multi-task learning framework three dialects arabic using prediction uniﬁed states across dialects prediction primary task dialect identiﬁcation secondary task. dmtl model deviated directly predicted states using convolutional-lstm-dnns trained either cross-entropy state-level minimum bayes risk ignoring secondary dialect identiﬁcation output recognition time. dmtl model trained dialectal data underperformed dialect-speciﬁc model. dialectal knowledge distilled model also designed achieved results competitive below dialect-speciﬁc models. effectiveness dialect-speciﬁc models motivated investigations ensemble methods multiple dialect-speciﬁc acoustic models recognition. soto explored approaches selecting combining best decoded hypothesis pool dialectal recognizers. work still different make selection directly using predicted dialect. huang used similar strategy identifying accent ﬁrst followed acoustic model selection however work considered gmms classiﬁer. proposed system consists multiple accent-speciﬁc acoustic models accent identiﬁcation model. describe components joint model section. acoustic model selection based hard-switch accent-speciﬁc models illustrated section accurate identiﬁcation speaker’s accent essential pipelined systems since accent identiﬁcation errors cause large mismatch acoustic models. given hypothesis accents discriminated spectral features researchers attempted model spectral distribution accent using gmms. recently dnns explored much expressive model compared gmms especially modeling probability distributions. implemented independent summarizes low-level acoustic features utterance stack bidirectional lstms projection layers. average-pooling layer applied transformed acoustic features acoustic realization speaker’s accent observable frame. applying average-pooling gives robust estimate figure depicts details model. single sigmoidal neuron used output layer classiﬁcation classifying accents english trained network using cross-entropy loss function. recently end-to-end systems achieved comparable performance traditional pipelined systems hybrid dnnhmm systems. systems come beneﬁt avoiding time-consuming iterations alignment model building. rnns using loss function popular approach systems loss computes total likelihood output label sequence given input acoustics possible alignments. achieves introducing special blank symbol augments label sequence make length equal length input sequence. clearly multiple augmented sequences uses forward-backward algorithms efﬁciently likelihoods sequences. loss output label sequence input acoustic sequence blank-augmented sequence sequences. decoding target label sequences obtained either greedy search wfst-based decoder. multi-accent acoustic model combines ctc-based accent. applied multiple blstm layers shared accents capture accent-independent acoustic features placed separate dnns extract accentspeciﬁc features. figure describes structure multi-accent acoustic model. jointly trained average accent-speciﬁc losses. test time multi-accent model requires knowledge speaker’s accent pick accent-speciﬁc targets. experimented oracle accent label using trained network make decision. previous multi-accent model assumes multi-tasking between phone sets accents sufﬁcient make network learn accent-speciﬁc acoustic information. alternate approach explicitly supervise network accent information. leads joint model multi-accent acoustic modeling primary tasks higher layers auxiliary task lower level layers shown figure joint model aggregates modules structures forementioned models section jointly trained end-to-end fashion objective function interpolation weight balancing loss multi-accent cross-entropy loss model parameters. loss sums probabilities possible paths corresponding equation classiﬁcation loss laid cross-entropy. losses different scales optimal value needs tuned development data. perform experiments dialects english corpora–wall street journal- american english cambridge british english. contain overlapping distinct phone sets phones respectively. corpora contain approximately hours audio. held-out training data development set. window size speech frame frame shift extracted -dimensional log-mel scale ﬁlter banks performed per-utterance cepstral mean subtraction. vocal tract length normalization. stacked neighboring frames picked every alternate frame -dimensional acoustic feature stream half frame rate. various models compared terms phone error rate word error rate particularly obtain simple frame-wise greedy decoding projection outputs removing repeated phones blank symbol. attila toolkit used report applying wfst-based decoding. evaluation performed eval american english british english. joint model uses four blstm layers lowest layer attached network highest single layer connects accent-speciﬁc softmax layers. single layer hidden units used task. weights models initialized uniformly adam optimizer initial learning rate used gradients clipped range discard training utterances longer frames. new-bob annealing held-out data used early stopping learning rate half whenever held-out loss decrease. purpose fair comparison used four layer blstm baseline acoustic models well. given trained ctc-based multi-accent acoustic model classiﬁer apply maximum likelihood estimation switch between accent-speciﬁc output layers yuk. paid denote probability accent estimated aid. joint model sensitive interpolation weight cross-entropy losses. tuned development data. figure depicts relationship overall accents different values. goes larger overall increases small ﬂuctuations especially tends largest expected since weights neural networks updated using errors. found optimal value table oracle performance word error rates assumes true accent known advance. word error rates calculated decoding wfst-graph incorporating relative improvement model aspec reported parenthesis. results. given well-trained independent joint model still signiﬁcantly outperforms baseline models mtlp achieves better aspec. comparison oracle wers models british wers relatively constant without distortion however american english wers deteriorate accordingly. independent recall british english utterances test data. interesting note biggest improvement aspec comes using joint model instead mtlp model independent model. improvement upon using joint model still larger indicates joint model already learned sufﬁcient accent-speciﬁc information accent supervision lower layers. table wers hard-switch using distorted aid. rel. shows relative improvement aspec; ind. applies independent neural trained separately. proposed model applies jointly learn multi-accent ams. achieved minimum figure illustrates trend accuracy different values. weights perform well accuracies greater tail values lead even worse performance. best performance achieved accuracy. ﬁrst evaluate oracle performance various models table results assume correct accent utterance provided models. words acoustic model corresponds correct accent i.e. relevant target accentspeciﬁc softmax layer used. seen proposed joint model signiﬁcantly outperforms accent-speciﬁc model relative improvement overall multi-task accent model observation indicates deep blstm layers shared multiple accent learn expressive accent-independent features reﬁne accent-speciﬁc ams. auxiliary task accent identiﬁcation also helps introducing extra accent-speciﬁc information. advantage augmenting general acoustic features speciﬁc information implicitly learned joint model observed natural language processing tasks well. value implicit feature augmentation rich area future investigation. paper studies state-of-the-art approaches acoustic modeling across multiple accents. note prior approaches explicitly include accent information training indirectly example different phone inventories various accents. propose end-to-end multi-accent acoustic modeling approach jointly trained accent identiﬁcation. blstm-rnns design acoustic models trained apply average pooling compute utterance-level accent embedding. experiments show multi-accent acoustic models accent identiﬁcation beneﬁt other joint model using hard-switch outperforms stateof-the-art multi-accent acoustic model baseline separatelytrained network. obtain relative improvement british english american english. oracle experiments section demonstrate value proposed joint model mtlp model classiﬁer operates perfectly. section demonstrates impact imperfect performance using hard-switch. table shows wayne xiong jasha droppo xuedong huang frank seide mike seltzer andreas stolcke dong geoffrey zweig achieving human parity conversational speech recognition arxiv preprint arxiv. george saon gakuto kurata sercu kartik audhkhasi samuel thomas dimitrios dimitriadis xiaodong bhuvana ramabhadran michael picheny lynn-li english conversational telephone speech recognition humans machines arxiv preprint arxiv. chao huang chen eric chang accent issues large vocabulary continuous speech recognition international journal speech technology vol. janet holmes introduction sociolinguistics routledge victor soto olivier siohan mohamed elfeky pedro moreno selection combination hypotheses dialectal speech recognition acoustics speech signal processing ieee international conference ieee fadi biadsy julia hirschberg using prosody phonotactics arabic dialect identiﬁcation tenth annual conference international speech communication association mohamed elfeky meysam bastani xavier velez pedro moreno austin waters towards acoustic model uniﬁcation across dialects spoken language technology workshop ieee. ieee kanishka has¸im multi-accent speech recognition hierarchical grapheme based models acoustics speech signal processing ieee international conference ieee huang dong chaojun yifan gong multiaccent deep neural network acoustic model accentspeciﬁc layer using kld-regularized model adaptation fifteenth annual conference international speech communication association mingming chen zhanlei yang jizhong liang yanpeng wenju improving deep neural networks based multiaccent mandarin speech recognition using i-vectors accentspeciﬁc layer sixteenth annual conference international speech communication association jiangyan zhengqi jianhua regularized model adaptation improving lstm based multi-accent mandarin speech recognition chinese spoken language processing international symposium ieee yanli zheng richard sproat liang izhak shafran haolang zhou daniel jurafsky rebecca starr su-youn yoon accent detection speech recognition shanghai-accented mandarin. interspeech mohamed elfeky pedro moreno victor soto multidialectical languages effect speech recognition much choice hurt international conference natural language speech processing alex graves navdeep jaitly towards end-to-end speech recognition recurrent neural networks proceedings international conference machine learning dario amodei sundaram ananthanarayanan rishita anubhai jingliang eric battenberg carl case jared casper bryan catanzaro qiang cheng guoliang chen deep speech end-to-end speech recognition english maninternational conference machine learning darin has¸im andrew senior kanishka franc¸oise beaufays fast accurate recurrent neural network acoustic proceedings intermodels speech recognition speech yajie miao mohammad gowayyed florian metze eesen end-to-end speech recognition using deep models wfst-based decoding automatic speech recognition understanding ieee workshop ieee awni hannun carl case jared casper bryan catanzaro greg diamos erich elsen ryan prenger sanjeev satheesh shubho sengupta adam coates deep speech scaling endto-end speech recognition arxiv preprint arxiv. andrew maas ziang jurafsky andrew lexicon-free conversational speech recognition neural networks proceedings conference north american chapter association computational linguistics human language technologies chorowski dzmitry bahdanau kyunghyun yoshua bengio end-to-end continuous speech recognition using attention-based recurrent ﬁrst results arxiv preprint arxiv. dzmitry bahdanau chorowski dmitriy serdyuk philemon brakel yoshua bengio end-to-end attention-based large vocabulary speech recognition acoustics speech signal processing ieee international conference ieee william chan navdeep jaitly quoc oriol vinyals listen attend spell neural network large vocabulary conversational speech recognition acoustics speech signal processing ieee international conference ieee", "year": "2018"}