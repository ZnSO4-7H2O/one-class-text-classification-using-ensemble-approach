{"title": "Randomness and isometries in echo state networks and compressed sensing", "tag": "eess", "abstract": " Although largely different concepts, echo state networks and compressed sensing models both rely on collections of random weights; as the reservoir dynamics for echo state networks, and the sensing coefficients in compressed sensing. Several methods for generating the random matrices and metrics to indicate desirable performance are well-studied in compressed sensing, but less so for echo state networks. This work explores any overlap in these compressed sensing methods and metrics for application to echo state networks. Several methods for generating the random reservoir weights are considered, and a new metric, inspired by the restricted isometry property for compressed sensing, is proposed for echo state networks. The methods and metrics are investigated theoretically and experimentally, with results suggesting that the same types of random matrices work well for both echo state network and compressed sensing scenarios, and that echo state network classification accuracy is improved when the proposed restricted isometry-like constants are close to 1. ", "text": "although largely diﬀerent concepts echo state networks compressed sensing models rely collections random weights; reservoir dynamics echo state networks sensing coeﬃcients compressed sensing. several methods generating random matrices metrics indicate desirable performance well-studied compressed sensing less echo state networks. work explores overlap compressed sensing methods metrics application echo state networks. several methods generating random reservoir weights considered metric inspired restricted isometry property compressed sensing proposed echo state networks. methods metrics investigated theoretically experimentally results suggesting types random matrices work well echo state network compressed sensing scenarios echo state network classiﬁcation accuracy improved proposed restricted isometry-like constants close paper considers similarities random matrices used compressed sensing echo state networks well metrics used concepts predict good performance. noisy linear measurements information sensing matrix generally compressed sensing yielding overdetermined system equation admits inﬁnitely many solutions. however suﬃciently sparse satisﬁes certain properties unique solution approximated convex optimization scheme several strategies setting sensing matrix used including random deterministic methods. common thread among strategies must satisfy restricted isometry property explored section guarantee recoverability observations high probability. hand esns special types recurrent neural networks whose hidden layer weights trained particular dataset task ﬁxed. weights deterministic delay line reservoirs randomly assigned classical esns work classical esns considered. properties randomly assigned hidden layer weights contribute good performance esns fully understood. generally scales matrix hidden layer weights spectral radius ashley.prater.us.af.mil. cleared public release wpafb public aﬀairs case number abw-. opinions ﬁndings conclusions recommendations expressed work author necessarily reﬂect view united states force. paper explore strategies metrics designed random matrices ﬁeld research apply toward other. methods metrics explored theoretically sections experimentally section metric suggest desirable performance based random weight matrix inspired compressed sensing proposed section work uses following notation. matrices denoted capital roman letters e.g. vectors lowercase roman letters matrices ‘matlab’ notation j-th matrix given k-th column j-th element vector given either subscripts function notation scalars denoted lowercase greek letters e.g. except indices arrays denoted throughout standard vector p-norm represents spectral radius i.e. largest absolute eigenvalue square matrix argument. section dynamics esns explored. suppose spatiotemporal input signals given matrix rl×t spatial dimension temporal dimension inputs. hidden layer sometimes also referred reservoir nodes. values nodes timesteps stored matrix updated according dynamics leaking rate scaling factors bias term nonlinear activation function. matrices input reservoir weights respectively. hidden layer ﬁxed output layer trained speciﬁc dataset desired task. output weights wout rk×n found collection input desired output pairs training wouta approximation usually obtained using regularization regularization constant note single output weight matrix found applied hidden layer states time steps. alternate approaches proposed linear output weights matrices approach used work tends yield accurate output results. temporal prediction tasks desired output ‘future input’ temporal oﬀset classiﬁcation tasks desired output incidator current class membership. equal number classes choose column vector zeros except k-th corresponding training input belongs k-th class time desirable determine metrics used indicate whether particular instance random matrix perform well compressed sensing model deploying particular dataset. section types measures quality discussed namely closely random dynamics represent isometry well clases separated hidden layer esn. study isometric behavior hidden layer inspired so-called restricted isometry property exhibited random matrices arise compressed sensing research. used compressed sensing help indicate sparse vector recovered observations equation using low-rank recovery scheme. matrix typically ‘shortand-fat’ random matrix therefore equation general diﬃcult solve since deﬁnes overdetermined system. sparsity well properties enable exact recovery. satisﬁes exist small constants course esns completely diﬀerent setting compressed sensing. esns interested recovering sparse vector rather embed inputs higher dimensional space using rich dynamics reservoir. moreover hidden layer weights matrix square concerned sparsity inputs. however isometry-like properties still inﬂuence quality reservoir. setting propose near-isometry property expressed chosen tightest bounds satisfy intuitive larger near-isometric interval niiρw would lead better separation data within reservoir. however near-isometry constants large reservoir saturated give poor results. classiﬁcation tasks useful measure well reservoir separates classes. separation ratio measures separation classes reservoir time step. deﬁne center mass reservoir nodes time inputs k-th class implementations esns compressed sensing models often collection random weights. however randomizations achieved several ways give better results others. explore methods determining random matrices methods often used generate sensing matrices compressed sensing. methods found strategies literature. matrices generating using satisfy equation high probability provided constant sparsity input signal. since condition reduces generated satisfying near-isometry property close choice scaling factor large impact behavior random dynamics esn. shown figure scaling factor chosen matrices generated using methods exhibit similar behavior ﬁgure displays lower upper bounds near-isometry figure estimates upper lower bounds near-isometry interval plotted various colored lines correspond diﬀerent methods determine scaling factor determined using left plots right plots. intervals reservoirs size generated using methods color lower curve values upper curve values left plot chosen throughout. right plot chosen inverse median value near-isometry interval determined experimentally. addition strategies discussed above consider three methods determining section random matrix methods discussed section quality measures discussed section explored experimentally used echo state networks compressed sensing models. random matrices used reservoir weights classiﬁcation task used sensing matrix compressed sensing task. along output accuracy types tasks near-isometry interval separation ratio measured esns restricted isometry constants measured compressed sensing. simulations perform classiﬁcation noisy sine versus square wave dataset. data consist sine square wave segments period repeated times contaminated gaussian noise illustration inputs shown figure types signals input sequentially using methods generate nodes scaling factor. input weights rl×n chosen matrix ones. parameters model chosen tanh methods chosen entries nonzero. results simulations summarized table several measures quality results given. ‘acc’ overall percent accuracy pointwise classiﬁcation inputs ‘acc sine’ ‘acc square’ percent pointwise classiﬁcation accuracy types inputs. spectral radius reservoir scaling measured average separation ratio reservoir given ﬁnal column approximate near-isomorphism interval equation entry average simulations. good classiﬁcation accuracy achieved using method generate single method appears advantage others task. however scaling factor large impact accuracy results. across selecting method consistently gives good accuracy high class separation ratio tight near-isometry interval centered note choosing method gives decent best accuracy results across also gives less favorable nearisometry intervals. approach often used experiments achieve good results. experimental results suggest better approach would optimize near-isometry interval instead. sparse vectors generated recovered noisy random linear observations. sparse vectors generated randomly selecting indices sampling {γi} i.i.d. randm variables sampled uniform distribution {ai} i.i.d. random variables sampled standard normal distribution. sparse vectors observed according sensing matrix generated using methods scaling factor generated using methods gaussian white noise sampled i.i.d. note methods used generate apply non-square matrices. table results classiﬁcation simulations using sine square wave dataset. ﬁrst columns indicate strategy used generate wres remaining columns giving mean several measures quality simulations. entries highlighted correspond highest separation symmetric isometry intervals close results simulations shown table figure ﬁrst columns table indicate methods used generate third column average value simulations pair third column average simulations mean squared error ideal estimator computed smaller indicates accurate results. ﬁnal column interval estimates restricted isometry constants equation approximated similar restricting sampling -sparse vectors. good results obtained using method generate restricted isometry interval relatively tight centered small. expected good performance guaranteed high probability matrices satisfying rip. evidenced results scaled behave proven perform well compressed sensing tasks. however best accuracy results achieved figure instance exact sparse vector recovery using diﬀerent methods generate ﬁgure indicates diﬀerent method generate within ﬁgure symbols correspond diﬀerent methods generating values found using using using original values table results sparse vector recovery simulations. values highlighted correspond accuracy restricted isometry intervals lowest among simulations. restricted isometry property gives guarantees high probability good performance compressed sensing scheme input data agnostic. adapted random matrices arising hidden layer echo state networks experimental results suggesting matrices good near-isometry constants also give good results classiﬁcation tasks. speciﬁc method used generate random behavior less important deﬁnition scaling factor seems perform consistently well using method random better performance attained using approaches illustrated compressed sensing results table however approaches theoretical performance guarantees. research methods warranted. results also suggest guideline selecting satisfy good performance necessarily good strategy. propose better consistent approach would select achieve tightest near-isometry interval centered arash farokh. deterministic construction binary bipolar ternary compressed sensing matrices. ieee transactions information theory ./tit... bertschinger natschlager. real-time computation edge chaos recurrent neural networks.", "year": "2018"}