{"title": "Resolution limits on visual speech recognition", "tag": "eess", "abstract": " Visual-only speech recognition is dependent upon a number of factors that can be difficult to control, such as: lighting; identity; motion; emotion and expression. But some factors, such as video resolution are controllable, so it is surprising that there is not yet a systematic study of the effect of resolution on lip-reading. Here we use a new data set, the Rosetta Raven data, to train and test recognizers so we can measure the affect of video resolution on recognition accuracy. We conclude that, contrary to common practice, resolution need not be that great for automatic lip-reading. However it is highly unlikely that automatic lip-reading can work reliably when the distance between the bottom of the lower lip and the top of the upper lip is less than four pixels at rest. ", "text": "visual-only speech recognition dependent upon number factors difﬁcult control lighting; identity; motion; emotion expression. factors video resolution controllable surprising systematic study effect resolution lip-reading. data rosetta raven data train test recognizers measure affect video resolution recognition accuracy. conclude that contrary common practice resolution need great automatic lip-reading. however highly unlikely automatic lip-reading work reliably distance bottom lower upper less four pixels rest. pixels images aligned. compute mean eigenvectors covariance. concatenating shape appearance features forms feature vector training testing. built model frames ﬁtted unseen data using inverse compositional ﬁtting rosetta raven data four videos north american talkers reciting edgar allen poe’s ‘the raven’. poem published recited properly poem trochaic octameter appear followed talkers dataset. figure shows example frames highdeﬁnition video talkers. database summarised table recorded non-interlaced resolution frames second. talkers wore make-up. typical lip-reading system number stages ﬁrst data pre-processed normalised; second face lips tracked; third visual features extracted classiﬁed. practice many systems tracking challenging affects overall recognition performance. however tracking problem insurmountable realistic track talking heads outdoor scenes ﬁlmed shaky hand-held cameras focus feature extraction using active appearance models select aams since shown robust performance number datasets example) perform feature types combined model shape appearance trained whole video sequence training creates mean model modes varied create shape appearance changes. training small number frames identiﬁed manually landmarked. models procrustes-aligned mean covariance shape computed. eigenvectors covariance matrix give modes variation used deform mean shape. appearance mesh shapenormalizes images piecewise afﬁne transform four videos converted images ffmpeg using image encoding full highdeﬁnition resolution build initial model select ﬁrst frame nine others randomly. frames hand-labelled model face lips. preliminary model ﬁtted inverse compositional ﬁtting remaining frames stage therefore tracked ﬁtted full face talker dependent aams full resolution lossless frame images figure models decomposed sub-models eyes eyebrows nose face outline lips figure shows talker’s lips sub-model. next video frames used high-resolution ﬁtting down-sampled required resolutions nearest neighbor sampling up-sampled bilinear sampling provide sets frames. frames physical size origiproduce ground truth listen recitation poem produced ground truth text word transcript converted american english phone level transcript using pronunciation dictionary however phones visible lips select mapping phones visemes here viseme mapping based upon walden’s trained consonants montgomery al’s vowel classiﬁcations illustrated table viseme recognition selected phoneme recognition small data beneﬁts reducing number classes needed increasing training data available viseme classiﬁer. note visemes equally represented data shown viseme counts figures talker test fold randomly selected lines poem. remaining lines used training folds. repeating times gives ﬁve-fold crossvalidation. visemes cannot equally represented folds. recognition hidden markov models implemented hidden markov toolkit initialised using ‘ﬂat start’ method using prototype states mixture components information training samples. choose states mixtures deﬁne viseme plus silence short-pause labels re-estimate parameters four times pruning. tool hhed together short-pause silence models states three re-estimating hmms times. hvite used force-align data using word transcript interested affect resolution loss lip-reading information rather than affect would also tracker consequently shape features experiment unaffected downsample whereas appearance features vary number substitution errors number deletion errors number insertion errors total number labels reference transcriptions accuracy measure rather correctness since accounts errors including insertion errors notoriously common reading. insertion error occurs recognizer output extra words/visemes missing original transcript example could once upon midnight dreary recognizer outputs once upon upon midnight dreary dreary. recognizer inserted words never present deleted one. figure shows acurracy versus resolution uwn. x-axis calibrated vertical height lips talker rest position. example maximum resolution talker lip-height approximately pixels rest position whereas lip-height approximately pixels. worst performance talker using shape-only features. note shape features vary resolution variation curve cross-fold validation error nevertheless variation within error bar. poor performance usual lip-reading standard error dominated insertion errors usual explanation effect shape data contains characteristic shapes indistinct shapes easier recogniser insert garbage symbols learn duration symbol indistinct start shapes co-articulation. talker distinctive shapes scores better shape feature. hmms re-estimated twice more however force-aligned viseme transcript rather original viseme transcript used previous reestimations. complete recognition using hmms require word network. hlstats hbuild make unigram word-level network bi-gram word-level network finally hvite used different network support recognition task plots appearance never becomes poor shape performance even resolutions. accuracies clear inﬂection point around four pixels pixels performance declined noticeably. table error rates insertions deletions substitutions pixels four covering lips rest less four pixels recognition performance falls. values averaged folds. table shows deletion insertion substitution error rates recognition performance resolutions four pixels rest. insertion errors signiﬁcantly lower deletions substitutions conﬁdent accuracy scores accurate insertions despite negative accuracy scores being achieved unigram word network support figure shown performance simple visual speech recognizers threshold effect resolution. successful lip-reading needs minimum four pixels across closed lips. however surprising result remarkable resilience computer lip-reading shows resolution. given modern experiments lip-reading usually take place high-resolution video example) disparity measured performance assumed performance striking. course higher resolution beneﬁcial tracking previous work able show factors believed highly detrimental lip-reading off-axis views actually ability improve performance rather degrade also noted previous shibboleths outdoor video poor lighting agile motion affecting performance overcome seems lip-reading better trust data conventional wisdom. however appearance interest since varies downsample. resolutions lower four pixels difﬁcult conﬁdent shape information effective. however basic problem error rate therefore adopt supportive word model. figure shows recognition accuracy versus resolution bwn. also includes sub-plots zoom rightpart graph. shape models perform worse appearance models looking zoomed bowden harvey e.-j. owen b.-j. theobald. recent developments automated lip-reading. spie security+ defence pages j–j. international society optics photonics bowden harvey e.-j. owen theobald. automated conversion video text reality? lewis burgess editors optics photonics counterterrorism crime fighting defence viii volume spie pages u–u–. spie zhou hong zhao pietik¨ainen. compact representation visual speech data using latent variables. pattern analysis machine intelligence ieee transactions b.-j. theobald harvey e.-j. bowden. improving visual features lip-reading. proceedings international conference auditoryvisual speech processing volume theobald harvey bowden. robust facial feature tracking using selected multiresolution linear predictors. computer vision ieee international conference pages ieee", "year": "2017"}