{"title": "Learning Spontaneity to Improve Emotion Recognition In Speech", "tag": "eess", "abstract": " We investigate the effect and usefulness of spontaneity (i.e. whether a given speech is spontaneous or not) in speech in the context of emotion recognition. We hypothesize that emotional content in speech is interrelated with its spontaneity, and use spontaneity classification as an auxiliary task to the problem of emotion recognition. We propose two supervised learning settings that utilize spontaneity to improve speech emotion recognition: a hierarchical model that performs spontaneity detection before performing emotion recognition, and a multitask learning model that jointly learns to recognize both spontaneity and emotion. Through various experiments on the well known IEMOCAP database, we show that by using spontaneity detection as an additional task, significant improvement can be achieved over emotion recognition systems that are unaware of spontaneity. We achieve state-of-the-art emotion recognition accuracy (4-class, 69.1%) on the IEMOCAP database outperforming several relevant and competitive baselines. ", "text": "classiﬁcation detecting target phonemes dufour also shown spontaneity useful identifying speakers’ role utilizing spontaneity information automatic speech recognition system. recent work tian established emotional content essentially different spontaneous acted speech compare emotion recognition types speech observe different sets features contribute success emotion classiﬁcation spontaneous acted speech. another study emotion recognition using convolutional neural network found type data affect emotion recognition results however work spontaneity information emotion recognition task. recent work used gender spontaneity information explicitely long short term memory network effective speech emotion recognition aggregated data corpus work differs work providing detailed analysis insight towards effect spontaneity emotion recognition speech proposing svm-based hierarchical multitask learning framework. work investigate usefulness spontaneity speech context emotion recognition. hypothesize emotional content interrelated spontaneity speech propose spontaneity classiﬁcation auxiliary task problem emotion recognition speech. investigate supervised learning settings multilabel hierarchical model performs spontaneity detection followed emotion classiﬁcation multitask learning model jointly learns recognize spontaneity emotion speech returns labels. construct proposed models standard acoustic prosodic features conjunction support vector machine classiﬁers. choose shown produce results comparable long short term memory networks training dataset sufﬁciently large experiments iemocap database observe recognizing emotion easier spontaneous speech scripted speech longer context useful spontaneity classiﬁcation signiﬁcant improvement emotion recognition achieved using spontaneity additional information spontaneity-unaware systems. rest paper organized follows section describes feature extraction process supervised classiﬁcation methods used using spontaneity emotion classiﬁcation. section provides details experimental setup results followed conclusion section emotion recognition using spontaneity section propose models utilize spontaneity information speech improve emotion recognition multilabel hierarchical model performs spontaneity detection followed emotion recognition multitask learninvestigate effect usefulness spontaneity speech context emotion recognition. hypothesize emotional content speech interrelated spontaneity spontaneity classiﬁcation auxiliary task problem emotion recognition. propose supervised learning settings utilize spontaneity improve speech emotion recognition hierarchical model performs spontaneity detection performing emotion recognition multitask learning model jointly learns recognize spontaneity emotion. various experiments wellknown iemocap database show using spontaneity detection additional task signiﬁcant improvement achieved emotion recognition systems unaware spontaneity. achieve state-of-the-art emotion recognition accuracy iemocap database outperforming several relevant competitive baselines. index terms emotion recognition spontaneous speech multitask learning. recognizing human emotion critical human-centric system involving human-human human-machine interaction. emotion expressed perceived various verbal non-verbal cues speech facial expressions. recent years speech emotion recognition studied extensively independent modality combination others majority work speech emotion recognition follows step approach. first acoustic prosodic features extracted machine learning system employed recognize emotion labels although acoustic prosodic features common lexical features emotional vector also shown useful recognition various methods proposed starting traditional hidden markov models ensemble classiﬁers recently deep neural networks recently abdelwahab busso proposed ensemble feature selection method addresses problem training test data arising different distributions. zong introduced domain-adaptive least squares regression technique problem. owing latest trends machine learning autoencoders recurrent neural networks also used speech emotion recognition. efforts improve speech emotion recognition primarily concentrated building better machine learning system. although spontaneity ﬂuency nativity speech well studied literature effect emotion recognition tasks well studied. work addressed problem distinguishing spontaneous scripted speech include acoustic prosodic feature-based context needed recognize spontaneity known help emotion recognition later section investigate role spontaneity detection. note spontaneity classiﬁer uses sequence length emotion recognition performed utterance level. according hypothesis spontaneity emotional information speech interrelated. perform tasks spontaneity detection emotion recognition together multitask learning framework. instead focusing single learning task multitask learning paradigm shares representations among related tasks learning simultaneously enables better generalization following idea jointly learn classify spontaneity emotion. posed multilabel multioutput classiﬁcation problem. basic idea presented fig. train single classiﬁer learns optimize joint loss function pertaining tasks. deﬁne weight matrix containing weight vectors w{ysye} classifying possible label tuples denotes cardinality set. order jointly model spontaneity emotion intend minimize loss function deﬁned follows. loss function regularization loss term w{ysye} soft-margin loss term i.e. parameter controls relative balance cost terms. term allows misclassiﬁcation near-margin training samples penalizing imposing loss term varies degree misclassiﬁcation. optimal classiﬁer weights learned minimizing joint loss function classiﬁer trained i.e. learned using entire ωtrain using features described earlier. since emotion vary consecutive recordings joint model uses sequence length perform detailed experiments iemocap database demonstrate importance spontaneity context emotion recognition validate proposed classiﬁcation models. extract speech features following interspeech emotion challenge feature includes four level descriptors mel-frequency cepstral coefﬁcients zero-crossing rate voice probability computed using autocorrelation function fundamental frequency speech sample sliding window length stride length extract llds. generates dimensional local feature vector windowed segment. descriptor smoothed using moving average ﬁlter smoothed version used compute respective ﬁrst order delta coefﬁcients. appending delta features obtain local feature vector dimension every windowed segment. create global feature entire speech sample local features pooled temporally computing different statistics along dimensions generating global feature vector data sample. consider training samples corresponding feai= trainture representations {fj}n sample feature vector associated labels represents debinary spontaneity labels notes emotion labels. note four emotion labels considered paper. denote entire label space order spontaneity information speech propose simple system ﬁrst recognizes speech sample spontaneous not. emotion classiﬁer chosen based decision made spontaneity classiﬁer. divide entire training ωtrain samples subtrain contains spontaneous speech samples sets train contains scripted non-spontaneous samples. shown figure train separate support vector machine classiﬁers recognizing emotion using train. additionally train another spontaneity detection sequence length using entire ωtrain. sequence length used account role context spontaneity performed spontaneity classiﬁcation understand role various parameters. investigate role context contribution various features spontaneity detection. train classiﬁer kernel distinguish spontaneous scripted speech using features described section utterance level i.e. system achieves average accuracy order study effect context spontaneity classiﬁcation vary sequence length account longer context increase concatenating consecutive utterances. consequently concatenate corresponding global features. yields feature vector variation classiﬁcation accuracy different values shown fig. general trend classiﬁcation accuracy improves longer context achieves accuracy result intuitively explained fact longer parts conversation used classiﬁcation becomes easier detect spontaneity. result also highlights spontaneity detected fairly high accuracy hence assures additional spontaneity detection module would harm overall performance speech processing pipleline incorrect detection spontaneity. role features investigate importance feature individually spontaneity classiﬁcation performing ablation study. exclude features time record corresponding spontaneity classiﬁcation accuracy. results presented table observe mfcc features important all. single feature provide accuracy indicating feature well suited task spontaneity classiﬁcation. moreover comparing accuracies achieved removing delta actual features removing actual features retaining delta features along motion capture recordings face text transcriptions. data collected different sessions session contains several dyadic conversations. altogether conversations labeled either improvised scripted. serves spontaneity label almost equal number scripted spontaneous conversations database. conversation broken separate samples utterances organized speaker-wise turn-by-turn fashion. samples labeled multiple annotators following categories neutral sadness anger frustration excitement. single sample multiple labels owing different annotators. cases ﬁnal label chosen label noted annotators randomly leading labels case tie. used four emotion categories anger neutral sadness. parameter settings features described section computed using sliding window length stride yields local feature vector dimension global feature vector dimension sample. features normalized values svms radial basis function kernel. results reported average statistics computed -fold cross validation. sanity check started hypothesis emotional content different spontaneous scripted speech. order check experimentally trained ωtrain iemocap database discriminate among anger neutral sadness. test phase computed recognition accuracy spontaneous scripted speech separately. observe overall accuracy emotion recognition recognition accuracy higher speech samples labeled spontaneous i.e. scripted speech basic result supports assumption emotional content different spontaneous scripted speech. observation also consistent results reported cnn-based recent work compare gain using spontaneity information construct baselines svm-based emotion classiﬁer random forest -based emotion classiﬁer. classiﬁers trained recognize emotion without using information spontaneity labels. also compare recent work emotion recognition cnn-based emotion recognition representation learning-based emotion recognition additionally compare results recent lstm-based framework uses gender spontaneity information emotion classiﬁcation. performances proposed spontaneity-aware emotion recognition methods along baselines existing methods presented table table proposed hierarchical outperforms baselines competing methods achieving overall recognition accuracy joint model achieves accuracy comparing performance baseline proposed spontaneity aware methods observe even features classiﬁer improvement hierarchical method overall emotion recognition accuracy achieved adding spontaneity information looking improvements individual classes class anger beneﬁts spontaneity information. evident notable improvement recognition accuracy using hierarchical model baseline emotions sadness. table shows recognition accuracy always lower scripted speech irrespective classiﬁcation method used. indicates emotion easier detect spontaneous speech result consistent observations made earlier work proposed hierarchical classiﬁer performs slightly better joint classiﬁer possibly owing accurate spontaneity classiﬁcation. recall spontaneity classiﬁer hierarchical model used longer context joint model uses nevertheless joint classiﬁer still practical scenario temporal sequence recording unknown hence sequence length spontaneity necessarily constrained. clearly spontaneity information helps emotion recognition. svm-based methods could achieve better result competing methods explicitly detecting using spontaneity information speech. reason behind svm-based methods outperforming deep learning-based methods rep. learning possibly spontaneity longer context case hierarchical method. lstm-based spontaneity aware method though uses four classes ours aggregated corpus training lstm network. training different experimental setting. paper studied spontaneity information speech inform improve emotion recognition system. primary goal work study aspects data inform emotion recognition system also gain insights relationship spontaneous speech task emotion recognition. investigated supervised schemes utilize spontaneity improve emotion classiﬁcation multilabel hierarchical model performs spontaneity classiﬁcation emotion recognition multitask learning model jointly learns classify spontaneity emotion. various experiments showed spontaneity useful information speech emotion recognition signiﬁcantly improve recognition rate. method achieves state-of-the-art recognition accuracy iemocap database. future work could directed towards understanding effect meta information gender. busso deng yildirim bulut kazemzadeh neumann narayanan analysis emotion recognition using facial expressions speech multimodal information proceedings international conference multimodal interfaces. chen speech emotion recognition ieee zong zheng zhang huang cross-corpus speech emotion recognition based domain-adaptive leastsquares regression ieee signal processing letters vol. jang speech emotion recognition using convolutional recurrent neural networks signal information processing association annual summit conference asia-paciﬁc. neumann attentive convolutional neural network based speech emotion recognition study impact input features signal length acted speech interspeech englebienne truong evers towards speech emotion recognition wild using aggregated corpora deep multi-task learning. international speech communication association gupta malandrakis xiao guha segbroeck black potamianos narayanan multimodal prediction affective dimensions depression human-computer interactions proceedings international workshop audio/visual emotion challenge.", "year": "2017"}