{"title": "Distributed Adaptive Learning with Multiple Kernels in Diffusion  Networks", "tag": "eess", "abstract": " We propose an adaptive scheme for distributed learning of nonlinear functions by a network of nodes. The proposed algorithm consists of a local adaptation stage utilizing multiple kernels with projections onto hyperslabs and a diffusion stage to achieve consensus on the estimates over the whole network. Multiple kernels are incorporated to enhance the approximation of functions with several high and low frequency components common in practical scenarios. We provide a thorough convergence analysis of the proposed scheme based on the metric of the Cartesian product of multiple reproducing kernel Hilbert spaces. To this end, we introduce a modified consensus matrix considering this specific metric and prove its equivalence to the ordinary consensus matrix. Besides, the use of hyperslabs enables a significant reduction of the computational demand with only a minor loss in the performance. Numerical evaluations with synthetic and real data are conducted showing the efficacy of the proposed algorithm compared to the state of the art schemes. ", "text": "kernel methods used solve e.g. nonlinear regression tasks based problem formulation reproducing kernel hilbert space linear techniques applied approximate unknown nonlinear function. function modeled element rkhs corresponding kernel functions utilized approximation. methodology exploited derive variety kernel adaptive ﬁlters particular naive online regularized risk minimization kernel normalized least-mean-squares kernel afﬁne projection hyperplane projection along afﬁne subspace enjoy signiﬁcant attention limited complexity applicability online learning scenarios. hypass algorithm derived functional space approach based adaptive projected subgradient method set-theoretic estimation framework exploits metric regard kernel gram matrix showing faster convergence improved steady-state performance. kernel gram matrix determines metric rkhs decisive convergence behavior gradient-descent algorithms kernel adaptive ﬁlters extended multiple kernels increase degree freedom estimation process. this accurate approximation functions several high frequency components possible smaller number dictionary samples compared using single kernel only. regarding distributed kernel-based estimation algorithms several schemes derived distributed consensus-based regression algorithm based kernel least squares proposed extended multiple kernels schemes utilize alternating direction method multipliers distributed consensusbased processing. recent works apply diffusionbased schemes kernel least-mean-squares derive distributed kernel adaptive ﬁlters nodes process information parallel. functional adapt-then-combine klms proposed kernelized version algorithm derived random fourier features diffusion klms proposed uses random fourier features achieve ﬁxed-size coefﬁcient vector avoid priori design dictionary set. however achievable performance strongly depends number utilized fourier features. besides aforementioned schemes incorporate update equations ordinary euclidean space thus exploit metric induced kernel gram matrix. furthermore majority schemes consider multiple kernels adaptation mechanism. abstract—we propose adaptive scheme distributed learning nonlinear functions network nodes. proposed algorithm consists local adaptation stage utilizing multiple kernels projections onto hyperslabs diffusion stage achieve consensus estimates whole network. multiple kernels incorporated enhance approximation functions several high frequency components common practical scenarios. provide thorough convergence analysis proposed scheme based metric cartesian product multiple reproducing kernel hilbert spaces. introduce modiﬁed consensus matrix considering speciﬁc metric prove equivalence ordinary consensus matrix. besides hyperslabs enables signiﬁcant reduction computational demand minor loss performance. numerical evaluations synthetic real data conducted showing efﬁcacy proposed algorithm compared state schemes. distributed learning within networks topic high importance applicability various areas environmental monitoring social networks data applications observed data usually spread nodes thus unavailable central entity. environmental monitoring applications instance nodes observe common physical quantity interest temperature humidity speciﬁc location. spatial reconstruction physical quantity area covered network non-cooperative strategies deliver satisfactory performance. rather distributed learning algorithms relying information exchanges among neighboring nodes required fully exploit observations available network. distributed learning linear functions addressed variety algorithms past decade e.g. contrast works address problem distributed learning nonlinear functions/systems. exploit adaptive ﬁlter input space dimension output space. ﬁlter/function employs positive deﬁnite kernels kernel induces rkhs uses corresponding dictionaries cardinality here dictionary {κq)}r contains kernel functions centered samples simplicity assume dictionary uses centers {¯x}r although assumption required. multikernel adaptive ﬁlter given derivation proposed algorithm rely previous work however considers distributed learning linear functions euclidean space speciﬁcally derive kernel-based learning scheme rkhs isomorphic euclidean space respectively. speciﬁcally propose distributed algorithm completely operating cartesian product space multiple rkhss. cartesian product space exploited cartesian hypass algorithm adaptive learning multiple kernels proposed operating corresponding euclidean parameter space metric based kernel gram matrix employed kernel needs considered. metric determined block diagonal matrix block diagonals given kernel gram matrices. derive distributed learning scheme rely average consensus coefﬁcient vectors kernel. idea proposed scheme fully conduct distributed learning euclidean space considering metric cartesian product space. metric responsible enhanced convergence speed adaptive algorithm. operating metric implies consensus matrix used diffusion information within network needs adapted introduce modiﬁed consensus matrix operating metric product space. fact show modiﬁed consensus matrix coincides consensus matrix operating ordinary euclidean space used ﬁnding actually implies metric product space alter convergence properties average consensus scheme. particularly important proving monotone approximation property proposed scheme. provide thorough convergence analysis considering metric product space. speciﬁcally prove monotone approximation asymptotic optimization asymptotic consensus convergence characterization limit point within framework apsm. practical implication demonstrate projecting current estimate onto hyperslab instead ordinary hyperplane signiﬁcantly reduce computational demand node. varying hyperslab thickness trade-off error performance complexity node adjusted. corroborate ﬁndings extensive numerical evaluations synthetic well real data mathematical proofs given appendices. clidean space ||·||rm respectively rkhs ||·||h respectively. given positive deﬁnite matrix rm×m xtky deﬁnes inner product norm norm matrix rm×m ||x||k induced vector norm ||·||k deﬁned ||x||k maxy= ||xy||k/||y||k. spectral norm matrix denoted ||x|| choose identity matrix said convex estimated output k−κk close function output arbitrary input samples achieved distributed fashion node network based acquired data pairs {}j∈j thus equip node multikernel adaptive ﬁlter parameterized individual coefﬁcient vector furthermore node assumed rely dictionaries i.e. globally known common nodes. specify coefﬁcient vectors result estimate close node’s measurement introduce closed convex node time index design parameter. hyperslab containing vectors provide maximum distance desired output parameter controls thickness hyperslab introduced consider uncertainty caused measurement noise njk. issue optimal sjk. deﬁne local cost function time node metric distance coefﬁcient vector hyperslab k-norm sense cost function gives residual kprojection onto sjk. distance metric non-negative convex function minimum value deﬁne global cost network time local costs individual cost time-varying. objective minimize sequence global costs nodes network convexity global cost also convex. simultaneously coefﬁcient vectors nodes converge solution guarantees consensus network. consider following optimization problem time understanding. however emphasize formulation originates considerations isomorphic functional space. interested reader referred appendix problem formulation functional space. continuous nonlinear function network nodes. function assumed space rkhss deﬁned label node index time index node observes nonlinear function noise sample. based nodes’ observations time index acquired input-output samples {}j∈j available within network. describe connections among nodes network employ graph nodes edges edge network represents connection nodes given node connected itself i.e. assume graph undirected i.e. edges equivalent other. neighbors node given containing nodes connected node furthermore consider graph connected i.e. node reached node multiple hops. objective nodes learn nonlinear function based acquired inputoutput samples {}j∈j distributed fashion. nodes able exchange information neighboring nodes enhance individual estimate unknown function deﬁnition local cost directly minimizers given points hyperslab sjk. since local cost metric distance minimum value zero minimizers global cost time sjk. points minimize local cost therefore also θjk. thus point minimizing local cost θjk∀j also minimizer global cost consider arbitrary many time instants deﬁne optimal solution problem step ensures local cost reduced hence global cost reduced well. step seeks consensus among coefﬁcient vectors {wj}j∈j information exchange among neighboring nodes satisfy constraint exchange node inherently obtains property sets neighbors exploited improve convergence behavior learning algorithm. apsm asymptotically minimizes sequence nonnegative convex functions thus used minimize local cost function node coefﬁcient vector node time particular case apsm update k-norm reads difference vector wjk−p ksjk used move coefﬁcient vector direction hyperslab controlled step size µjk. note update solely relies local information i.e. information neighboring nodes needed. projection ksjk calculated points minimize global cost time instant node therefore call point ideal estimate. however ﬁnding challenging task particularly practical considerations. limited memory instance measurements stored time node. hence information unavailable thus ideal estimate cannot acquired. alternative feasible task minimization ﬁnitely many global costs approach stems intuition good estimate minimize many costs possible. acquire estimate nodes agree point contained overbar gives closure set. finding point clearly less restrictive task ﬁnding since global costs excluding ﬁnitely many ones need minimized. therefore proposed algorithm achieve estimates shown apsm converges points remark considerations need assume enable hyperslab threshold chosen sufﬁciently large depending noise distribution variance. examples choose noisy environments proposed impulsive noise occurring ﬁnitely many times regard time instant ﬁnal impulse guarantee however impulsive noise occurs inﬁnitely many times measurements straightforward ensure convergence apsm introduced later nevertheless whenever impulsive noise occurs error signal apsm abruptly change. based change noisy measurements detected discarded practice satisﬁed. satisfy constraint reach consensus coefﬁcient vectors node fuses vector neighbors i}i∈nj employ symmetric matrix rj×j assigning weights edges network. -entry denoted gives weight edge nodes obviously connection present among nodes entry zero. fusion step node time follows vector ones. ﬁrst condition guarantees convergence average states network second condition keeps network stable state consensus reached. matrices vastly applied literature consensus averaging problems e.g. proposed algorithm solve given following update equations node time index projection ksjk given iteration node performs local apsm update transmits intermediate coefﬁcient vector neighbors receiving intermediate coefﬁcient vectors neighbors node fuses vector fact comprises projection cartesian product rkhss used chypass algorithm therefore call proposed scheme diffusion-based chypass distributed implementation chypass. remark diffusion stage d-chypass omitted algorithm reduces local adaptation noncooperative scheme node individually approximates based node-speciﬁc measurement data. however case node access individual property time instant contrast diffusing coefﬁcient vectors among neighboring nodes node inherently obtains information property sets {sik}i∈nj neighbors. simply observed inserting therefore compared local adaptation d-chypass show faster convergence speed lower steady-state error cooperation within network. several works shown beneﬁt distributed approaches non-cooperative strategies context diffusion-based adaptive learning references therein. lemma unit vector n-th entry rrqj. further deﬁne consensus subspace span{b brq} stacked vector coefﬁcient vectors network rrqj. then following properties. consensus matrix decomposed consensus matrix constructed matrix identity matrix. matrix said compatible graph since equivalently calculated wjk+ i∈nj gjiwik deﬁnition consensus matrix know ||p|| holds. however analysis d-chypass algorithm need know norm w.r.t. matrix since d-chypass operates kmetric. therefore introduce modiﬁed consensus matrix proof proofs theorem directly deduced corresponding proofs theorem a)-c) consideration ||p|| ||p||k nonnegative convex function. note proof theorem needs derived considering k-metric ordinary euclidean metric proofs theorem given appendix following section evaluate performance d-chypass applying spatial reconstruction multiple gaussian functions real altitude data tracking time-varying nonlinear function network nodes. nodes distributed unit-square area node uses cartesian position vector regressor. assume positions nodes stay ﬁxed i.e. change time. necessary d-chypass applicable e.g. applied mobile network positions change time investigated time index nodes take measurement function position hence network constantly monitors function experiments assume model since zero-mean white gaussian noise variance scenario measurements function spatially spread nodes collaboration among nodes inevitable good regression performance. thus appropriate application example beneﬁt distributed learning becomes clear. compare performance d-chypass rff-dklms fatc-klms multikernel distributed consensus-based estimation state algorithms distributed kernelbased estimation. rff-dklms fatc-klms single kernel approaches based diffusion mechanism. assuming fatc-klms considers local data adaptation step schemes exhibit number transmissions node d-chypass. enable fair comparison restrict adaptation step fatc-klms local data extend algorithm multiple kernels d-chypass. call scheme diffusionbased multikernel least-mean-squares update equation node given show convergence properties d-chypass ﬁxed deterministic network topologies. although space study k-metric space unlike still prove properties lemmas theorem sequence generated satisﬁes following. rff-dklms approximates kernel evaluations random fourier features design speciﬁc dictionary necessary. however performance highly dependent number utilized fourier features determines dimension vectors exchanged. mkdice distributed regression scheme based kernel least squares multiple kernels using admm distributed mechanism. number transmissions iteration higher compared d-chypass rffdklms dmklms. naturally adaptive scheme included reference purposes. benchmark performance consider central chypass given |nj| denotes degree node algorithms coherence threshold average dictionary size utilized. single kernel approaches arithmetic average bandwidths chosen multikernel schemes kernel bandwidth. evaluate d-chypass hyperplane projection i.e. d-chypass hyperslab projection chosen parameter values considered algorithms listed table figure compares nmse learning curves dchypass d-chypass local adaptation central chypass. clearly local adaptation completely fails approximate d-chypass dchypass perform close central chypass. figure compares performance d-chypass state schemes. d-chypass signiﬁcantly outperforms compared algorithms terms convergence speed steady-state error. regarding monokernel approaches fatccentral chypass requires node positions measurements {}j∈j time index single node perform projection ksjk onto sjk. regarding dictionaries assume uses samples {¯x}r samples subset node positions {xj}j∈j network selected following coherence criterion node position compared included dictionary every dictionary entry {¯x}r sample ¯xr+ satisﬁes here coherence threshold controlling cardinality dictionary generated priori node positions algorithm iterates. stays ﬁxed throughout reconstruction process speciﬁc algorithm. cartesian coordinate vector nodes randomly placed following uniform distribution nodes share connection distance satisﬁes assume noise variance nodes average performance trials network realization trial. regarding kernel choice gaussian kernels bandwidths diffusion-based algorithms klms outperforms rff-dklms steady-state error although uses dictionary samples compared rrff random fourier features. increasing number fourier features rrff performance signiﬁcantly improved rff-dklms nevertheless improvement comes huge increase communication overhead since number fourier features equal dimension coefﬁcient vectors exchanged. dmklms exchanges vectors entries only coefﬁcient vectors rff-dklms rrff entries. thus relying priori designed dictionary dmklms d-chypass huge savings communication overhead computational complexity achieved. enhanced performance d-chypass compared multikernel approaches better metric form k-norm normalization factor projection psjk adapts step size µjk. exploiting projection w.r.t. knorm shape cost function changed convergence speed improved figure d-chypass show similar performance negligible loss d-chypass however minor loss comes huge reduction complexity node since d-chypass projects onto hyperslab higher probability contained d-chypass vector updated saving signiﬁcant amount computations. contrast using hyperplane vector updated iteration. figure shows number local apsm updates node logarithmic scale hyperslab threshold additionally nmse averaged last iterations relation threshold depicted. thresholds step size used. observe using hyperslab thresholds saves huge amount complexity keeping error performance constant. e.g. d-chypass average updates executed node. compared updates d-chypass reduction approximately computations achieved. crucial especially sensors computational capability limited battery life. however figure also clear compufig. contour plots true reconstruction node using d-chypass steady state. green circles show node positions ﬁlled circles chosen dictionary entries. figure depict contour plot true function together exemplary node positions reconstructed function d-chypass reconstruction shown node network steady state. virtue consensus averaging step node network reconstruction. observe gaussian functions approximated good accuracy. peaks functions clearly distinguished. however outer regions inaccuracies still seen. expected reduced increasing dictionary size. figure shows error performance algorithms averaged dictionary size trials. nmse values calculated average last iterations iterations algorithm. observe d-chypass outperforms competitors growing dictionary size. particular dmklms mkdice lose performance dictionary sizes d-chypass steadily improves reconstruction. reason dmklms mkdice step size adjusted growing dictionary size avoid increasing steady-state error. dmklms step size normalized squared norm kernel vector dchypass lead divergence. regarding fatcklms similar effect expected appear higher dictionary sizes since uses kernel only. therefore range dictionary samples performs better dmklms mkdice. show performance mkdice dmklms improved figure depict nmse performance adapted step size iteration coherence threshold results average dictionary size point mkdice dmklms show degrading performance according figure step sizes chosen dmklms mkdice respectively. observe adjusting step size dictionary size steady-state performance improved compared figure mkdice dmklms outperform fatc-klms. remark regarding d-chypass noted divergence observed. caused inversion ill-conditioned kernel gram matrix occurs dictionary employs node positions close leading linear dependency higher coherence threshold probability case increases. numerically stabilize inversion scaled identity matrix added matrix regularization. matrix substituted thresholds regularization parameter used experiment achieve stable performance. apply d-chypass reconstruction real altitude data node measures altitude position data etopo global relief model provided national oceanic atmospheric administration exhibits several low/high frequency components. original data position given longitude latitude corresponding altitude delivered position. choose area points longitudes latitudes however easier handling longitudes latitudes cartesian coordinates unit-square area consider randomly placed described area. nodes distance share connection. assume noise coherence threshold algorithm employs dictionary average size rff-dklms uses rrff fourier features. performances averaged independent trials. table lists chosen parameter values considered algorithms. figure depicts nmse performance iteration. d-chypass outperforms algorithms terms convergence speed steady-state error. although dmklms performs close d-chypass observed convergence speed d-chypass faster. fatc-klms rff-dklms perform worst since reconstruction capability limited kernel only. rff-dklms converges faster fatc-klms noted produces higher communication overhead rrff fourier features compared dictionary samples fatc-klms. contour plots multikernel approaches steadystate node shown figure d-chypass observe good reconstruction original although details area around missing. reconstructions dmklms mkdice show less accurate approximation especially areas around valley function contains gaussian shapes whose bandwidths expanding shrinking time apply d-chypass reconstruction time-varying function compare mkdice dmklms. network nodes randomly distributed unitsquare area average performance trials network realization trial. noise variance considered algorithms average dictionary size samples achieved. figure shows nmse iteration number ﬂuctuations error curves time-varying bandwidths algorithms ﬂuctuations stay speciﬁc error range illustrating function tracked within certain range accuracy. observe d-chypass signiﬁcantly outperform remaining algorithms. additionally range ﬂuctuations error lower d-chypass compared algorithms. also visible utilizing kernels d-chypass improves tracking performance compared using kernel d-chypass nevertheless worth noting even kernel d-chypass outperforms multikernel approaches dmklms mkdice illustrating signiﬁcant gain employing k-norm algorithm. computational complexity communication overhead analyze complexities communication overhead algorithms iteration network. complexities consider number multiplications assume gaussian kernels used furthermore dictionary designed priori stays ﬁxed time common nodes. therefore kernel gram matrix computed ofﬂine iterative process d-chypass avoiding inversion iteration. note block diagonal inversions matrices computed. results complexity order network d-chypass starts iterating. reduce complexity d-chypass selective-update strategy applied selects coherent dictionary samples entries coefﬁcient vector updated usually iteration inverse matrix computed complexity multiplications heavily reduced. overhead count number transmitted scalars among nodes. algorithms except mkdice consensus averaging step produces broadcast transmissions. beside broadcasts mkdice comprises also unicast transmissions vectors depend receiving node increase overhead signiﬁcantly. table lists complexities overhead algorithms complexity inversion matrix denoted vinv figure depicts complexity overhead dictionary size network nodes edges. rff-dklms rrff included reference. clearly seen complexity overhead admm-based mkdice highest among algorithms inversion matrix iteration transmission unicast vectors respectively. furthermore dictionary sizes d-chypass lower complexity rff-dklms. including selective-update strategy complexity d-chypass signiﬁcantly reduced even lower single kernel fatc-klms. d-chypass dmklms exhibit overhead iteration lower compared rff-dklms dictionary sizes real data. note restricted scenario applied general distributed nonlinear system identiﬁcation task. compared state algorithms could observe signiﬁcant gains error performance convergence speed stability employed dictionary size. particular proposed apsm-based algorithm signiﬁcantly outperformed admm-based multikernel scheme terms error performance highly decreased complexity communication overhead. embedding hyperslab projection computational demand node could drastically reduced certain range thresholds keeping error performance constant. equivalently formulate problem cartesian product space rkhss. furthermore derive local apsm update exploiting isomorphism product space euclidean space using k-metric. proposed adaptive learning algorithm exploiting multiple kernels projections onto hyperslabs regression nonlinear functions diffusion networks. provided thorough convergence analysis regarding monotone approximation asymptotic minimization consensus limit point algorithm. introduced novel modiﬁed consensus matrix proved identical ordinary consensus matrix. application example investigated proposed scheme reconstruction spatial distributions network nodes synthetic since lies space rkhss thus estimate approach minimize metric distance estimate true function space estimate multikernel adaptive ﬁlter expressed decomposable wqκq). problem therefore formulated following functional optimization problem however space norm inner product closed-form expressions functions might uniquely decomposable depending choice kernel functions. alternative approach formulate problem functions cartesian product space uniquely decomposable independent underlying kernel functions norm closed-form expression pre]. parameter space inner product served equivalent respective coefﬁcient vector rrq. then correspondence problem equivalent holds. problem formulation product space )q∈q consider q-tuple )q∈q. furthermore monokernel ﬁlter fact element dictionary subspace span{κq)}r thus lies cartesian product dictionary subspaces solution problem directly given projection onto dictionary subspace projection fact best approximation ×-norm sense. monokernel case shown certain assumptions approximately equals minimum mean square error estimate exact equality mmse estimate achieved operating learning procedure space square-integrable functions probability measure djk| node containing bounded local instantaneous error time instant here ))q∈q )q∈q estimated output given using reproducing property kernel. reasonable assume enforces consensus among estimates {φj}j∈j network. regarding relation product parameter space note ﬁnite-dimensional isomorphic dictionary subspace p||k show modiﬁed consensus matrix identical assume compatible graph connected undirected network matrix deﬁnition rj×j. then holds examining k−/pk/ k−/k/ gk−/i rqk/ mimic proof show convergence k∈n. theorem know sequence ||k)k∈n converges every )t]t thus sequence bounded every subsequence accumulation point. then according bolzanoweierstrass theorem bounded real sequence vaerenbergh lazaro-gredilla santamaria kernel recursive least-squares tracker time-varying regression ieee trans. neural netw. learn. syst. vol. august yukawa r.-i. ishii efﬁcient kernel adaptive ﬁltering algorithm using hyperplane projection along afﬁne subspace eusipco takizawa yukawa adaptive nonlinear estimation based parallel projection along afﬁne subspaces reproducing kernel hilbert space ieee trans. signal process. vol. yamada ogura adaptive projected subgradient method asymptotic minimization sequence nonnegative convex functions numer. funct. anal. optim. vol. bouboulis chouvardas theodoridis online distributed learning networks spaces using random fourier features ieee transactions signal processing vol. honeine richard bermudez snoussi essoloh vincent functional estimation hilbert space distributed learning wireless sensor networks ieee icassp honeine richard bermudez chen snoussi decentralized approach nonlinear prediction time series data sensor networks eurasip wirel commun netw vol. boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers foundations trends machine learning vol. yamada slavakis yamada efﬁcient robust adaptive ﬁltering algorithm based parallel subgradient projection techniques ieee trans. signal process. vol. cavalcante stanczak distributed subgradient method dynamic convex optimization problems noisy information exchange ieee sel. topics signal process. vol. cavalcante rogers jennings yamada distributed asymptotic minimization sequences convex functions broadcast adaptive subgradient method ieee sel. topics signal process. vol. renato lu´ıs garrido cavalcante received electronics engineering degree instituto tecnol´ogico aeron´autica brazil m.e. ph.d. degrees communications integrated systems tokyo institute technology japan respectively. april april recipient japanese government scholarship. currently research fellow fraunhofer institute telecommunications heinrich hertz institute berlin germany lecturer technical university berlin. previously held appointments research fellow university southampton southampton u.k. research associate university edinburgh edinburgh u.k. cavalcante received excellent paper award ieice ieee signal processing society student paper award also co-authored study received best student paper award ieee international workshop signal processing advances wireless communications current interests signal processing distributed systems multiagent systems convex analysis machine learning wireless communications. armin dekorsy received b.sc. degree fachhochschule konstanz konstanz germany m.sc. degree university paderborn paderborn germany ph.d. degree university bremen bremen germany communications engineering. research engineer deutsche telekom distinguished member technical staff bell labs europe lucent technologies. joined qualcomm gmbh european research coordinator conducting qualcomms internal external european research activities. currently head department communications engineering university bremen. authored coauthored journal conference publications holder patents area wireless communications. long-term expertise research wireless communication systems baseband algorithms signal processing. amante eakins etopo arc-minute global relief model procedures data sources analysis noaa technical memorandum nesdis ngdc-. national geophysical data center noaa toda yukawa online model-selection learning nonlinear estimation based multikernel adaptive ﬁltering ieice transactions fundamentals electronics communications computer sciences vol. slavakis yamada ogura adaptive projected subgradient method ﬁxed point strongly attracting nonexpansive mappings numerical functional analysis optimization vol. ban-sok shin received dipl.-ing. degree university bremen since then research assistant department communications engineering university bremen currently working towards ph.d. degree. research interests include distributed inference/estimation adaptive signal processing machine learning application sensor networks. masahiro yukawa received b.e. m.e. ph.d. degrees tokyo institute technology respectively. studied visiting researcher/professor university york u.k. technical university munich germany technical university berlin germany worked riken japan special postdoctoral researcher niigata university japan associate professor currently associate professor department electronics electrical engineering keio university japan. since july visiting scientist center riken japan. associate editor ieee transactions signal processing multidimensional systems signal processing ieice transactions fundamentals electronics communications computer sciences research interests include mathematical adaptive signal processing convex/sparse optimization machine learning. yukawa recipient research fellowship japan society promotion science april march received excellent paper award young researcher award ieice respectively yasujiro niwa outstanding paper award ericsson young scientist award telecom system technology award young scientists’ prize commendation science technology minister education culture sports science technology kddi foundation research award ffit academic award member ieice.", "year": "2018"}