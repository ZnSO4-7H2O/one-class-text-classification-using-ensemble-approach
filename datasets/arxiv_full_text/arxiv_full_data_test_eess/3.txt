{"title": "Sparsity-Promoting Sensor Selection for Non-linear Measurement Models", "tag": "eess", "abstract": " Sensor selection is an important design problem in large-scale sensor networks. Sensor selection can be interpreted as the problem of selecting the best subset of sensors that guarantees a certain estimation performance. We focus on observations that are related to a general non-linear model. The proposed framework is valid as long as the observations are independent, and its likelihood satisfies the regularity conditions. We use several functions of the Cram\\'er-Rao bound (CRB) as a performance measure. We formulate the sensor selection problem as the design of a selection vector, which in its original form is a nonconvex l0-(quasi) norm optimization problem. We present relaxed sensor selection solvers that can be efficiently solved in polynomial time. We also propose a projected subgradient algorithm that is attractive for large-scale problems and also show how the algorithm can be easily distributed. The proposed framework is illustrated with a number of examples related to sensor placement design for localization. ", "text": "detection probability. sensor placement problem also interpreted sensor selection problem best subset available sensor locations selected subject speciﬁc performance constraint. sensor selection pertinent various diverse ﬁelds especially applications dealing large-scale networks like network monitoring location-aware services like target localization tracking ﬁeld estimation environment monitoring general. fundamental questions interest deploy limited sensors available? need process acquired measurements? focus processing informative sensors general non-linear statistical inference problem. large volume literature exists sensor selection sensor selection problem often formulated optimization problem based wellknown performance measures experimental design sensor selection problem expressed following optimization problem selection vector length scalar cost function related mean squared error covariance matrix covariance matrix optimized select best subset sensors available sensors different functions used typical choices related a-optimality minimizes eigenvalues tr{e}. e-optimality minimizes maximum eigenvalue λmax{e}. d-optimality minimizes determinant det{e}. combinatorial optimization problem involving searches clearly intractable even small-scale problems simplify problem nonconvex boolean constraint relaxed convex constraint relaxed optimization problem studied additive gaussian linear models matrix available closed form importantly listed performance measures independent unknown parameter. moreover practice exact number sensors abstract—sensor selection important design problem large-scale sensor networks. sensor selection interpreted problem selecting best subset sensors guarantees certain estimation performance. focus observations related general non-linear model. proposed framework valid long observations independent likelihood satisﬁes regularity conditions. several functions cram´er-rao bound performance measure. formulate sensor selection problem design selection vector original form nonconvex norm optimization problem. present relaxed sensor selection solvers efﬁciently solved polynomial time. also propose projected subgradient algorithm attractive large-scale problems also show algorithm easily distributed. proposed framework illustrated number examples related sensor placement design localization. index terms—sensor selection sensor placement cram´er-rao bound selection vector sparsity non-linear models statistical inference projected subgradient algorithm convex optimization sensor networks. trum applications services related safety security surveillance environmental climate monitoring list few. sensor nodes spatially deployed operate network sensor node capable sensing processing communicating nodes central processing unit. network fundamental task distributed data sampling seek extract relevant information. sensors provide prohibitively large dataset usually gathered fusion center. gathered data optimally processed rejecting redundant identical faulty measurements. sensor selection fundamental design task sensor networks. number sensors often limited either economical constraints availability physical storage space. order reduce hardware costs well resulting communications processing overhead would like smartly deploy sensors. sensor selection also enables design spatio-temporal sensing patterns guarantee certain performance measure energy-efﬁciency information measure estimation accuracy authors faculty electrical engineering mathematics computer science delft university technology netherlands. email {s.p.chepuri;g.j.t.leus}tudelft.nl. selection problem applied sensor placement power grid monitoring alternative approaches exploiting submodularity objective function heuristics based genetic algorithms greedy algorithms also proposed solve sensor selection problem. sensor selection dynamical systems often referred sensor polling scheduling studied sensor placement problem linear models addressed design sensing matrix optimizes measure related orthogonality rows. literature deals measurements related additive gaussian linear models. experimental design non-linear models within bayesian sequential design frameworks discussed sensor selection target tracking based extended kalman ﬁltering proposed selection performed designing appropriate gain matrix. although non-linear measurement model additive gaussian noise used past state estimate used compute error covariance matrix leading suboptimal solution. sensor selection detection problems studied reliable sensor selection based actual measurements identify outliers presented. different problem related sensor selection problem identifying source-informative sensors studied sensor selection problem interpreted problem select best sensors available sensors. selected sensors deemed best subset sensors guarantee certain speciﬁed estimation accuracy. consider general scenarios measurements unknown parameter follow non-linear model instance). non-linear measurement models frequently encountered applications like source localization tracking ﬁeld estimation phase retrieval list few. error covariance matrix non-linear models always available closed form importantly depends unknown parameter. ﬁrst contribution context sensor selection cram´er-rao bound performance measure. rigorous performance measure optimality generalizes well non-linear measurement models moreover need actual measurements hence framework also well-suited solving ofﬂine design problems. addition this number sensors selected i.e. generally known practice. hence instead ﬁxing pose sensor selection cardinality minimization problem provides number selected sensors byproduct. order this different thresholds specify required accuracy. formulated design selection vector norm nonconvex boolean optimization problem. requires brute-force evaluation choices. example available potential sensors order possible choices whose direct enumeration clearly impossible. nonconvex sensor selection problem relaxed using standard convex relaxation techniques efﬁciently solved polynomial time. cope large-scale problems present projected subgradient algorithm. worth mentioning projected subgradient algorithm allows easy distributed implementation. sparsity-enhancing concave surrogate norm also proposed sensor selection alternative traditional best convex relaxation. particularly advantageous multiple identical sensor measurements. illustrate sensor selection problem using examples sensor placement source localization. remainder paper organized follows. section present non-linear measurement model. section show problem formulation present algorithms solve relaxed optimization problem section section derive dual problem provide extensions. section proposed framework applied number different models related sensor selection localization. paper ﬁnally concludes section vii. notations used paper described follows. upper bold face letters used matrices denotes transposition. diag refers block diagonal matrix elements argument main diagonal. denotes vector ones identity matrix size e{·} denotes expectation operation. tr{·} matrix trace operator. det{·} matrix determinant. λmin{a} denotes minimum eigenvalue symmetric matrix means positive semideﬁnite matrix. denotes symmetric matrices size denotes cardinality spatial temporal sensor measurement unknown parameter noise process regressors non-linear functionals. vector collect measurements. likelihood measurements probability density function parameterized unknown vector goal select best subset available sensor measurements certain accuracy estimate guaranteed. next mathematically formulate sensor selection problem. known functions observations only depend unknown parameter. regularity conditions general hold observations belong family exponential pdfs already includes large number distributions. trace constraint sufﬁcient condition resulting sensor selection problem computationally less attractive compared minimum eigenvalue constraint moreover lmis used also represent trace constraint. reasons focus minimum eigenvalue constraints however without loss generality either performance constraints used. performance measures depend unknown parameter. practice unknown parameter physical meaning takes values within certain domain denoted example case direction-of-arrival estimation sector source expected target localization surveillance area target resides. since non-linear models depends unknown propose constrain every point within domain remark bayesian setting prior information unknown parameter available additional knowledge typically yields lower related information matrix often called bayesian information matrix given prior information matrix order reduce hardware costs storage processing communication overhead minimize number selected sensors. achieved minimizing cardinality selection vector i.e. minimizing number non-zero entries selection vector. mathematically sensor selection problem formulated design selection vector expressed following optimization problem measure identiﬁability problem speciﬁcally non-singular implies solvability unique estimate however converse necessarily true. sensor selection problem presented paper seeks subset sensors full rank domain solvability problem domain always ensured. denotes probability values deﬁne accuracy required assumed known. higher accuracy level obtained reducing and/or increasing metric used several occasions accuracy measure next discuss popular performance measures satisfy requirement. words lower bound eigenvalue matrix solution satisfying convex λeigin trace constraint larger feasible compared minimum eigenvalue constraint. however although typically based newton’s method using approximating barrier function. brief description projected newton’s method provided appendix used analyze computational complexity relaxed sensor selection problem. remark computational cost involved iteration follows matrices blockdiagonal structure blocks. forming matrix wmfm λeigidn costs ﬂops; computing s−fi cholesky factorization costs ﬂops; matrices s−fi s−fj costs finally newton step computed cholesky factorization costing ﬂops projection costs ﬂops. assuming overall computational complexity iteration projected newton’s algorithm second-order newton’s method typically intractable number sensors large circumvent problem propose subgradient based algorithm. projected subgradient algorithm ﬁrst-order method attractive large-scale problems iteration much cheaper process. subgradient method typically used optimizations involving non-differentiable functions subgradient method generalization gradient method non-smooth non-differentiable functions ℓnorm minimum eigenvalue constraint functions. next derive projected subgradient algorithm. wmfm} constraint function denotes constraints afﬁne subgradient objective all-one vector ∂feig denote subgradient constraint function feig here ∂feig denotes subdifferential feig evaluated compute express constraint function feig norm refers number non-zero entries i.e. threshold λeig imposes accuracy requirement. threshold λeig also sparsity-inducing parameter λeig implies sparser solution. suppose domain consists points obtained gridding entire domain certain resolution. resulting multiple constraints stacked together single constraint. consider domain constraints equivalently expressed single wmfm λeigidn diag note gridding independent denote simply well known norm optimization np-hard nonconvex. speciﬁcally original sensor selection problem np-hard. boolean constraint non-convex incurs combinatorial complexity. next present number solvers relaxed convex problem solved efﬁciently polynomial time. computationally tractable solution traditional best convex surrogate norm namely ℓ-norm heuristic. ℓ-norm known represent efﬁcient heuristic norm optimization convex constraints especially solution sparse relaxations well-studied problems linear constraints context compressed sensing sparse signal recovery non-convex boolean constraint relaxed convex constraint |wm| denotes ℓ-norm. positivity constraint objective function simply afﬁne function optimization problem standard problem inequality form efﬁciently solved polynomial time using interiorpoint methods implementation interior-point method solving problems inequality form remark ﬁrst form matrix wmfm costs ﬂops. minimum eigenvalue corresponding eigenvector computed using power method cost ﬂops forming vector costs ﬂops computing norm costs ﬂops update projection together cost ﬂops. assuming earlier computational cost projected subgradient algorithm much lower complexity projected newton’s method. distributed implementation projected subgradient algorithm easy. simple distributed averaging algorithm used compute wmfm. minimum eigenvalue corresponding eigenvector computed using power iterations node independently. update equation subgradient vector projection computed coordinatewise already distributed. subgradient methods typically slow compared interior-point method involving newton iterations subgradient methods typically require hundred iterations. newton’s method typically requires order steps. hand unlike projected subgradient method newton’s method cannot easily distributed requires relatively high complexity iteration computation storage second-order derivatives. depending scale problem resources available processing could choose subgradient newton’s algorithm. concave surrogate sparsity-enhancing iterative algorithm ℓ-norm customarily used best convex relaxation ℓ-norm. however intersection ℓ-norm ball positive semi-deﬁnite cone always unique point shown following theorem. proof proof follows fact ℓ-norm strictly convex linearity constraint set. consider example λeigidn words observations identical. case extreme points ℓ-norm ball i.e. example solutions. moreover since solution convex minimization problem convex also solution gives inﬁnite number solutions relaxed optimization problem cases ℓnorm relaxation typically result sparse solution. min]t ∂feig eigenvector corresponding minimum mfm}. minimum eigenvalue corresponding eigenvector computed using low-complexity iterative algorithm called power method using standard eigenvalue decomposition projection point onto denoted expressed elementwise words current iterate feasible λeig) update direction negative objective subgradient constraints absent; current iterate infeasible λeig) update direction subgradient associated constraints. update computed iterate projected onto constraint using iterate feasible diminishing nonsummable step size used. iterate feasible polyak’s step size feig+ǫ used adopt optimal value known best known approximate algorithm terminated speciﬁed maximum number iterations kmax. finally estimate denoted wkmax. convergence results subgradient method constrained optimization derived since projection onto convex non-expansive affect convergence. projected subgradient algorithm summarized algorithm improve upon ℓ-norm solution nonuniqueness following theorem propose alternative relaxation original sensor selection problem also results fewer selected sensors. instead relaxing norm ℓ-norm using nonconvex surrogate function yield better approximation. motivated logarithm geometric mean elements used alternative surrogate function linear inverse problems adapting sensor selection problem arrive optimization problem solution relaxed optimization problem used compute suboptimal boolean solution selection problem. straightforward technique often used simple rounding technique boolean estimate given round deﬁne round operator rounds arguments towards nearest integer. however guarantee boolean estimates obtained rounding technique always satisfy constraint. hence propose randomized rounding technique suboptimal boolean estimates computed based random experiments guided solution problem iterative version randomized rounding technique summarized algorithm small constant prevents cost tending cost concave since smooth w.r.t. iterative linearization performed obtain local minimum ﬁrst-order approximation around results instead minimizing original cost majorizing cost optimized attain local minima. speciﬁcally optimization problem iteratively driven local minimum using iterations iterative algorithm summarized algorithm iteration solves weighted ℓ-norm optimization problem. weight updates force small entries vector zero avoid inappropriate suppression larger entries. parameter provides stability guarantees zero-valued entries strictly prohibit nonzero estimate next step. finally estimate given imax speciﬁed maximum number iterations. remark projected subgradient algorithm adapted sparsity-enhancing iterative algorithm well. optimization problem replaced following update equations threshold λdet speciﬁes mean radius conﬁdence ellipsoid words although indication performance estimator sufﬁcient condition log-determinant constraint concave function relaxed sensor selection problem scalar constraints solved either proposed cost functions i.e. ℓ-norm log-based concave surrogate. examples sensor placement localization localization important extensively studied topic wireless sensor networks target localization performed using plethora algorithms exploit inter-sensor measurements like time-of-arrival time-difference-of-arrival angle-of-arrival received signal strength performance location estimator depends algorithm also placement anchors sensor placement challenge localization system design certain sensor constellations deteriorate performance also result ambiguity identiﬁability issues sensor placement problem interpreted problem divide speciﬁc sensor area grid points select best subset grid points. here selected sensors deemed best guarantee certain minimal accuracy location estimates within speciﬁc target area consider two-dimensional network target located target area possible sensors located grid points. absolute positions sensor grid points known hence considered sensors commonly referred anchor nodes. coordinates target anchor denoted vectors respectively assumed unknown known within next illustrate proposed framework number examples related localization. pairwise distance target anchor denoted amk. practice pairwise distances obtained ranging generally noisy. range measurements generally follow additive gaussian non-linear model given dual variz mzsm} tr{fmz} ables e{st detailed derivation dual problem appendix dual problem interpreted problem maximizing diameter conﬁdence ellipsoid. optimal solution problem also solution dual e-optimal design problem maximizes diameter conﬁdence ellipsoid centered around origin. dual formulation often easier solve inequality constraints. dual problem solved using yalmip sedumi earlier. suppose dual feasible primal feasible dual problem yields following bound primal problem λeig tr{z}− variable unit vector optimization problem constraints every point inequality constraints optimization problem constraint every point hence solving computationally intense solving determinant constraint another popular scalar performance measure quality estimate determinant constraint. measure related d-optimality. relaxed sensor selection voltage measured sensor’s received signal strength indicator circuit. often reported measured power. ensemble mean received power sensor expressed remark sensor selection problem also formulated active sensing. active sensing sensors transmit probing signals selection parameter active sensing soft parameter used joint selection resource allocanormalized maximum prescribed value hence dimensionless. relaxed active sensor selection problem takes form fact minimizing ℓnorm active sensor selection minimizes overall network resources received power reference distance however shadowing difference measured received power ensemble average random. randomness shadowing typically modeled log-normal process gaussian expressed decibels speciﬁcally received power sensor follows gaussian distribution i.e. rdb). related radar sonar estimate location point source emits reﬂects energy. suppose sensors measure energy generated point source. measurements given known energy emitted reﬂected source known propagation function gain modeled isotropic exponential attenuation noise. related internally calls sedumi matlab implementation second-order interior-point methods. consider scenario shown fig. sensors illustrate sensor selection problem. recall problem choose best sensor positions available ones certain speciﬁed localization accuracy achieved. domain example target area target resides avoid inﬁnitely many constraints area consists grid points certain resolution. grid target area uniformly resolution along horizontal vertical directions shown fig. original non-convex sensor selection problem relaxed ℓ-norm optimization problem. alternatively concave surrogate function used enhance sparsity. optimization problem concave surrogate cost function iteratively solved afﬁnely scaling objective based solution previous iteration. sparsityenhancing iterative algorithm imax number candidates used randomized rounding algorithm observed simulations solution typically found ﬁrst batch itself tens candidate entries sufﬁcient. following parameters simulations square-degrees σrdb fig. shows sensor selection distance measurement model. thresholds computed selection shown fig. based algorithm randomized rounding recover approximate boolean solution. selection results based ℓ-norm cost minimum eigenvalue constraint shown fig. fig. also shows solution based concave surrogate cost function minimum eigenvalue constraint leads sparser solution. selection results based trace constraint obtained solving illustrated fig. sensors region selected constraints. fig. shows zero-duality selection based sparsity-enhancing iterations minimum eigenvalue constrains. boolean solution recovered using randomized rounding. minimum eigenvalue constraints ℓ-norm concave surrogate based relaxations. randomized rounding applied concave surrogate based solution. ℓ-norm based selection trace constraints. ℓ-norm cost function different cost primal problem dual problem different values larger values result larger subsequently sensors selected. sufﬁcient trace constraint larger feasible compared stronger sufﬁcient minimum eigenvalue constraint. result considered scenario minimum eigenvalue constraint leads slightly larger ℓ-norm compared trace constraint. optimization problem also solved using projected subgradient method summarized algorithm kmax iterations. solution projected subgradient shown fig. performance projected subgradient algorithm compared solution interior-point methods denoted fopt i.e. /fopt shown fig. even though convergence projected subgradient algorithm slow estimated support hundred iterations used along randomized rounding reﬁne solution. computation time computer projected practical estimator meet cases therefore sensors obtained speciﬁc would lead underestimate desired mse. account choosing appropriately. give entire solution path selected sensors different values fig. solution path efﬁciently computed increasing sensors corresponding used meet desired requirement. non-linear model solved least-squares sense iteratively using gauss-newton’s method iterations maximum root-mse maximum root-crb average rmse average location estimates target within target area using selected sensors different values shown fig. considered scenario maximum average root-crb satisfy performance constraint fig. solution path sensors selected different values maximum rmse based selected sensors seen plot. maximum average rmse location estimates based gauss-newton’s method corresponding maximum average root-crb performance constraint different values given inequality performance constraint shown solid line fig. maximum rmse satisfy accuracy requirement speciﬁed certain corrected using appropriate moreover considered scenario average rmse performance constraint still reasonable. also show maximum rmse fig. proposed framework general applied variety data models long valid. illustrate next consider measurement models. sensor selection based bearing measurements illustrated fig. here selection results based measurement model shown fig. sensor selection results based energy measurements shown fig. considered measurement models common structure decreases distance increases. however rate decreases different different models. anyway result decrease optimization problem leads sensor selection close target area considered models. sensor selection important design problem sensor networks. sensor selection problem described problem selecting best subset sensors guarantees certain speciﬁed performance measure. sensor selection enables deployment sensors guarantees fig. sensor selection based bearing measurements available sensors. sensor selection solved minimum eigenvalue constraints square-degrees. using ℓ-norm log-based heuristics. thresholds computed using noise variance fig. sensor selection based available sensors. sensor selection solved minimum eigenvalue constraints using ℓ-norm log-based heuristics. thresholds computed using σrdb resulting estimation accuracy. also minimizes hardware communications resulting processing costs largescale networks. focus observations follow nonlinear model. proposed framework valid long observations independent pdfs regular. number functions related performance measure. original nonconvex optimization problem relaxed using convex relaxation techniques efﬁciently solved polynomial time. handle largescale problems also presented projected subgradient algorithm. also enables easy distributed implementations. proposed framework applied sensor placement design number different models related localization. fig. sensor selection based energy measurements. illustration ﬁeld generated unit amplitude point source location according available sensors sensors indicated selected. source domain indicated sensor selection solved minimum eigenvalue constraints using ℓ-norm log-based heuristics. thresholds computed using specify required accuracy assumed known. typical choice constant chisquared values i.e. cumulax tive distribution function chi-squared random variable degrees freedom. performance measure related d-optimality. gustafsson gunnarsson mobile positioning using wireless networks possibilities fundamental limitations based available wireless network measurements ieee signal process. mag. vol. july gezici tian giannakis kobayashi molisch poor sahinoglu localization ultra-wideband radios look positioning aspects future sensor networks ieee signal process. mag. vol. july patwari kyperountas hero r.l. moses n.s. correal locating nodes cooperative localization wireless sensor networks ieee signal process. mag. vol. jul. waterschoot leus static ﬁeld estimation using wireless sensor network based ﬁnite element method proc. ieee international workshop computational advances multi-sensor adaptive processing haotian zhang j.m.f. moura krogh dynamic ﬁeld estimation using wireless sensor networks tradeoffs estimation error communication cost ieee trans. signal process. vol. krause singh guestrin near-optimal sensor placements gaussian processes theory efﬁcient algorithms empirical studies journal machine learning research vol. masazade fardad p.k. varshney sparsity-promoting extended kalman ﬁltering target tracking wireless sensor networks ieee signal process. lett. vol. converge eigenvector corresponding maximum eigenvalue vmax maximum eigenvalue λmax respectively here forming matrix λmaxin dominant eigenvalue λmax λmin apply power iterations compute λmax λmin vmin thus minimum eigenvalue it’s corresponding eigenvector. chepuri leus a.-j. veen sparsity-exploiting anchor placement localization sensor networks proc. european signal processing conference sept. mateos rajawat dynamic network cartography advances network health monitoring ieee signal process. mag. vol.", "year": "2013"}