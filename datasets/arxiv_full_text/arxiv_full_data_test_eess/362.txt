{"title": "Compressive Sensing: Performance Comparison Of Sparse Recovery  Algorithms", "tag": "eess", "abstract": " Spectrum sensing is an important process in cognitive radio. A number of sensing techniques that have been proposed suffer from high processing time, hardware cost and computational complexity. To address these problems, compressive sensing has been proposed to decrease the processing time and expedite the scanning process of the radio spectrum. Selection of a suitable sparse recovery algorithm is necessary to achieve this goal. A number of sparse recovery algorithms have been proposed. This paper surveys the sparse recovery algorithms, classify them into categories, and compares their performances. For the comparison, we used several metrics such as recovery error, recovery time, covariance, and phase transition diagram. The results show that techniques under Greedy category are faster, techniques of Convex and Relaxation category perform better in term of recovery error, and Bayesian based techniques are observed to have an advantageous balance of small recovery error and a short recovery time. ", "text": "said 𝐾-sparse elements entries nonzero. mathematically written ∑‖𝑥‖%≤𝐾 ‖.‖% 𝑙-norm sparsity level signal. measurements sparse signal 𝑥∈𝑅/. signal matrix 𝜙∈𝑅.×/ matrix reduction compression must preserve information stored 𝐾-sparse signal necessary recover original signal measurements. measurement matrices classified categories random deterministic matrices. random matrices present drawbacks implementation. deterministic matrices proposed alternative reduce randomness. examples deterministic matrices include toeplitz circulant matrices sparse recovery problem underdetermined system linear equations needs solved using sparse prior. system underdetermined existence uniqueness solution guaranteed soon signal sufficiently sparse measurement matrix satisfies restricted isometry property certain level last decades several sparse recovery algorithms proposed algorithms classified three main categories convex relaxation greedy bayesian category. several papers surveys related compressive sensing application context cognitive radio published. instance describes comparison greedy algorithms. authors provided survey greedy recovery algorithms. another survey compressive abstract—spectrum sensing important process cognitive radio. number sensing techniques proposed suffer high processing time hardware cost computational problems compressive sensing proposed decrease processing time expedite scanning process radio spectrum. selection suitable sparse recovery algorithm necessary achieve goal. number sparse recovery algorithms proposed. paper surveys sparse recovery algorithms classify categories compares performances. comparison used several metrics recovery error recovery time covariance phase transition diagram. results show techniques greedy category faster techniques convex relaxation category perform better term recovery error bayesian based techniques observed advantageous balance small recovery error short recovery time. next generation communication systems expected smart fast sensing wideband spectrum identifying free channels last decades number sensing techniques proposed including energy detection autocorrelation based euclidean distance bayesian inference method techniques based measurements sampled nyquist rate analog/digital converter result high processing time hardware cost computational complexity order overcome limitations compressive sensing proposed solution decrease processing time speed spectrum scanning process compressive sensing theory asserts certain signals recovered accurately using fewer measurements nyquist/shannon sampling principle use. shown fig. compressive sensing involves three main processes sparse representation measurement sparse recovery first process sparse representation consists representing signal number projections suitable basis. instance wavelet transform fast fourier transform discrete cosine transform smallest 𝑙-norm satisfies equation 𝜙𝑥=𝑦 using convex optimization observations measurement matrix 𝑥∈𝑅/ unknown sparse 𝑚𝑖𝑛‖𝑥‖% 𝑠𝑢𝑏𝑗𝑒𝑐𝑡 𝜙𝑥=𝑦 𝑙-norm 𝑚𝑖𝑛‖𝑥‖% represent ‖𝑥‖%=∑ minimum l-norm 𝑚𝑖𝑛𝑐b𝑥 𝑠𝑢𝑏𝑗𝑒𝑐𝑡 𝐴𝑥=𝑏 objective function bounds maxbb𝑦 subject ab𝑦+𝑧=c called dual variable transpose fundamental theorem linear programming states solution linear program primal infeasibility ‖𝑏−𝐴𝑥‖y dual infeasibility ‖𝑐−𝑧−ab𝑦‖y duality cb𝑥−bb𝑦 equal zeros. sensing application published aforementioned papers focus either sparse recovery category concept compressive sensing. authors provided survey compressive sensing cognitive radio. survey covers compressive sensing processes sparse representation measurement sparse recovery algorithms. best knowledge performance comparison sparse recovery algorithms categories published before. thus need detailed review papers compare analyze current sparse recovery algorithms categories using different performance metrics. therefore paper propose classification sparse recovery algorithms according approach belongs compare performances. remainder article organized follows. first categorize existing sparse recovery algorithms review techniques category. mathematical background algorithm provided. performance comparison algorithms investigated concluding remarks given. shown fig. sparse recovery algorithms classified three main categories convex relaxation greedy bayesian category. techniques convex relaxation category solve sparse signal recovery problem convex relaxation algorithms. examples techniques include basis pursuit gradient projection sparse reconstruction gradient descent greedy algorithms second category recover sparse signal iterative process. examples algorithms orthogonal matching pursuit compressive sampling matching pursuit a*orthogonal matching pursuit stagewise orthogonal matching pursuit generalized orthogonal matching pursuit iterative hard thresholding third category bayesian framework solves sparse recovery problem taking account prior knowledge sparse signal distribution. bayesian techniques classified types estimation hierarchical bayesian framework. estimation framework underlines mentioned previous section recovery algorithms classified three categories convex relaxation greedy bayesian category. category implemented algorithms. convex relaxation category implemented basic pursuit gradient descent algorithms. greedy category implemented orthogonal matching pursuit iterative hard thresholding algorithms. bayesian category linear equation 𝑦=𝜙𝑥 𝑦∈𝑅. m-dimensional vector measurements 𝑥∈𝑅/ unknown signal recovered 𝜙∈𝑅.×/ measurement matrix. updates iteratively using largest elements term magnitude zero represent value iteration transpose measurement matrix initialization iteration calculates calculating 𝑧z=𝑥z+𝜙b applying operator iterating reaching joint distribution hierarchical model inverse noise variance hyperparameters vector observations gaussian distribution zero mean variance equal gamma prior placed follows 𝑝𝑏)=γ𝑏) coefficients 𝑝=𝛾/exp−𝛾 ‖𝑥‖% 𝑝=𝑝𝑝 since distribution multivariate gaussian distribution parameters 𝜇=∑𝛽𝜙b𝑦 ∑=x% ∧=𝑑𝑖𝑎𝑔 𝑝=/𝑝 used matrix satisfies restricted isometric property isometric constant δy\\</. algorithm calculates iteratively sparse signal 𝑥∈𝑅/ measurements using 𝑥=h\\ δy\\+/ transpose measurement matrix residue whose expression given r=y− operator keeps largest magnitude gradient descent algorithm starts initializing residue zero weight updates column measurement matrix largest correlation residue r=y−ϕ𝑥 taking higher 𝑘=𝑎𝑟𝑔𝑚𝑎𝑥jkl𝜙b𝑟jlm transpose measurement matrix then selected column appended 𝑆=𝑆∪{𝑘}. components zero using following formula /s=u.𝑦; /sv= selected columns denotes complement denotes pseudo-inverse matrix u=x%𝜙/sb pseudo-inverse matrix restricted algorithm updates residue 𝑟=𝑦−𝜙𝑥r iterates components relevance vector machines posterior variance defines zeromean gaussian prior element instead using inverse noise variance models prior using precision gaussian density function denotes gaussian distribution mean equal zero variance 𝛼x%. addition gamma prior considered denotes gamma distribution. similarly gamma prior considered 𝛼~=/𝜎y 𝑝=log𝑝 =log˜𝑝𝑝𝑑𝑥 =−+𝑙𝑜𝑔|𝐶|+𝑦b𝐶x%𝑦] 𝐶=𝜎y𝐼+𝜙∧x%𝜙b ∧=𝑑𝑖𝑎𝑔. becomes search hyperparameters 𝛼z˙¨=𝛾𝜇 𝜇=−𝛼σ diagonal element {…n}. 𝛼~z˙¨ =‖¸x˝‖ ˛x∑ﬁ work consider sparse signal length contains spikes randomly chosen. measurement matrix used toeplitz matrix whose size 𝜎=.. sparsity level number measurements consider monte carlo draws number measurements length sparse signal. toeplitz matrix reduces randomness memory usage allows also fast acquisition recovery. sparse signal considered noisy. generation gaussian noise done random number generator standard deviation also estimate maximizing respect babacan proposed update single element instead updating whole vector proposition isn’t 𝑙=𝑙=−%y«𝑙𝑜𝑔|𝐶x|−%y𝑦b𝐶xx%𝑦+y∑ %{ﬁy+𝜆𝛾›= y«𝑙𝑜𝑔 %%{ﬁﬂ+ ℎ=%y«𝑙𝑜𝑔 %%{ﬁﬂ+ %{ﬁy+𝜆𝛾› 𝑠=𝜙b𝐶xx%𝜙 𝑞=𝜙b𝐶xx%𝑦 ﬁ=‡ﬁ= satisfied 𝛾=⎩⎪⎨⎪⎧−𝑠+𝑠‚y−𝜆 𝑖𝑓𝑞y−𝑠>𝜆 algorithm starts initializing zero tests 𝑞y−𝑠>𝜆 added model. otherwise 𝑞y−𝑠>𝜆 algorithm re-estimates otherwise 𝑞y−𝑠<𝜆 algorithm prunes model setting zero. finally algorithm updates using using higher bayesian techniques orthogonal matching pursuit show better performance term recovery error decreases reach number measurements larger fig. shows recovery error algorithms behaves sparsity sparse signal length changing fixed number measurements according fig. sparsity signal increases recovery error algorithms increases. fig. also compares recovery error sparse recovery algorithms. seen figure bayesian techniques orthogonal matching pursuit basis pursuit sparsity level consistently achieve recovery error. sparsity increases recovery error algorithms increases still show better performance compared gradient descent iterative hard thresholding techniques. fig. shows behavior bayesian techniques opposite iterative hard thresholding basis pursuit techniques. number measurements increases recovery time bayesian techniques decreases. however recovery time basis pursuit iterative hard thresholding techniques number measurements increases. phase transition diagram determine given recovery algorithm provide good recovery capabilities. also seen representation success sparse recovery algorithm terms probability success plotted phase space pair δ=m/n corresponds number samples acquired ρ=k/m ratio sparsity level signal number samples acquired. instance restricted isometric property gives relationship number measurements sparsity level size signal. order present conditions clear phase transition diagram translate requirements signal sparsity level size signal number measurements plot separates phase success failure recovery sparse signal. explicit expression curve given donoho consider face numbers properties projected cross-polytope order define formula order measure asymptotic phase point chose sequence δ=m/n 𝐾/𝑀. each generate toeplitz matrix size random 𝐾-sparse signal attempted recover 𝑦=𝜙𝑥 using algorithms. experiment fig. shows recovery error algorithms respect number measurements fixed sparsity level signal. number measurement basis pursuit shows better performance techniques. however number measurements fig. shows phase transition diagram sparse recovery algorithms. seen basis pursuit shows best performance followed bayesian greedy techniques. fig. shows orthogonal matching pursuit gradient descent techniques faster techniques number measurements sparsity levels. shown figure level sparsity bayesian techniques faster techniques except orthogonal matching pursuit. however sparsity higher behavior recovery time bayesian technique laplace prior increase significantly become higher techniques. however bayesian relevance vector machine technique recovers signal average time sec. fig. shows number measurements increases covariance increases reaching number measurements high bayesian basis pursuit orthogonal matching pursuit techniques. hand iterative hard thresholding gradient pursuit show lower correlation. fig. shows covariance around sparsity bayesian techniques basis pursuit orthogonal matching pursuit techniques. covariance decreases sparsity level signal increases. table summarizes performances implemented sparse recovery algorithms. table orthogonal matching pursuit bayesian relevance vector machine perform better sparse recovery algorithms. basis pursuit gradient descent orthogonal matching pursuit iterative hard thresholding bayesian fast laplace bayesian relevance vector machine paper performed comparison sparse recovery algorithms convex relaxation greedy bayesian category. results show techniques greedy category faster techniques. however techniques convex relaxation category perform better term finding solution sparse recovery problem small errors. techniques bayesian category balance small recovery error short recovery time. future work includes application compressive sensing scanning wideband spectrum develop model fast data acquisition. model implemented using universal software radio peripheral device radio software. salahdine ghazi kaabouch fihri \"matched filter detection dynamic threshold cognitive radio networks\" international conference wireless networks mobile communications reyes subramaniam kaabouch spectrum sensing technique based autocorrelation euclidean distance comparison energy detection cognitive radio networks elsevier comput. elect. eng. vol. pp-- subramaniam reyes kaabouch \"spectrum occupancy measurement autocorrelation based scanning technique using usrp ieee wireless microwave technology conf reyes subramanian kaabouch bayesian inference method scanning radio spectrum estimating channel occupancy\" ieee annual ubiquitous computing electronics mobile communication conference press. f.salahdine n.kaabouch ghazi \"bayesian compressive sensing circulant matrix spectrum sensing cognitive radio networks\" ieee annual ubiquitous computing electronics mobile communication conference press. morgan yang zhang \"practical compressive sensing toeplitz circulant matrices\" international society optics photonics visual communications image processing vol. foucart sparse recovery algorithms sufficient conditions terms restricted isometry constants\" approximation theory xiii antonio springer york nowak wright \"gradient projection sparse reconstruction application compressed sensing inverse problems\" ieee journal selected topics signal processing vol. garg khandekar \"gradient descent sparsification iterative algorithm sparse recovery restricted isometry property\" inproceedings annual international conference machine learning. donoho tsaig drori starck \"sparse solution underdetermined systems linear equations stagewise orthogonal matching pursuit\" ieee transactions information theory vol. budhiraja survey compressive sensing based greedy pursuit reconstruction algorithms\" international journal image graphics signal processing vol. pp.-", "year": "2018"}