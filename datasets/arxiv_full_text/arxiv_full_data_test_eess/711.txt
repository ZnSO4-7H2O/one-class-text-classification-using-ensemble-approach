{"title": "Extreme Learning Machine for Graph Signal Processing", "tag": "eess", "abstract": " In this article, we improve extreme learning machines for regression tasks using a graph signal processing based regularization. We assume that the target signal for prediction or regression is a graph signal. With this assumption, we use the regularization to enforce that the output of an extreme learning machine is smooth over a given graph. Simulation results with real data confirm that such regularization helps significantly when the available training data is limited in size and corrupted by noise. ", "text": "abstract—in article improve extreme learning machines regression tasks using graph signal processing based regularization. assume target signal prediction regression graph signal. assumption regularization enforce output extreme learning machine smooth given graph. simulation results real data conﬁrm regularization helps signiﬁcantly available training data limited size corrupted noise. consider target output smooth signals graph propose graph signal processing hypothesis approach results improved prediction performance. experiments realworld data show validity hypothesis. elms graph signal processing emerged directions much interest respective communities believe work step towards bringing together. extreme learning machines emerged active area research within machine learning community elms differ traditional approaches neural networks suppor vector machines important respect parameters hidden nodes randomly generated learning takes place output layer extreme layer solving regularized least-squares problem result suffer computational issues often affect traditional approaches making fast effective learning approach. despite simplicity shown high quality performances classiﬁcation regression tasks often giving similar better results comparison svms orders magnitude faster fact enjoys universal approximation properties given mild conditions activation functions shown approximate continuous piecewise continuous target function number neurons hidden layer tends inﬁnity though traditionally elms developed single-layer feed forward networks multi-layer distributed incremental extensions also developed machine learning paradigm performance depends nature training testing data. abundant reliable training data better classiﬁcation regression performance however many applications training data scarce corrupted noise. cases incorporate additional structures training phase. article propose emerging notion graph signal processing enhance prediction performance regression tasks. particular consider graph nodes denoted denotes node edge adjacency matrix since consider undirected graphs symmetric vector y··· said graph signal denotes value signal node graph smoothness graph signal measured terms quadratic form graph laplacian matrix diagonal degree matrix diagonal given aij. measure variation across connected nodes smaller value implies smoother signal consider observations input target pairs extreme learning machine consists input layer output layer hidden layer neurons shown figure neuron implements nonlinear operation input parameterized variables parametric scalar function example sigmoid function functions referred activation functions. parameters drawn randomly known probability distribution. weights corresponds regression coefﬁcient relating output neuron next propose graph signal processing. assume target signal graph signal corrupted noise. goal learn predicted output smooth graph achieve training elm. words learn optimal regression coefﬁcients minimizer following optimization problem column vectors expresses prediction output projections along principal directions given case component along effectively eliminated. principal components corresponding largest correspond informative directions. components corresponding smaller usually noise high-frequency components. eigenvectors corresponding smaller eigenvalues smooth graph observe condition achieved corresponding small and/ large. corresponds effectively retaining components vary smoothly across observation inputs {··· and/ smoothly varying across nodes graph. extend smoothing achieved depends regularization parameters thus elmg output corresponds smoothing denoising operation training targets. consider application proposed graphs real-world graph signal datasets. datasets true targets ton’s smooth graph signals speciﬁed graph. assume true target values observed access noisy target tn’s corresponding inputs xn’s. order simulate situation deliberately corrupt true graph signal targets additive white gaussian noise noisy targets training elmg. trained models used predict targets inputs test dataset. consider following three different activation functions popular literature entries parameters neurons drawn independently standard normal distribution. parameters found exhaustive grid search. compare prediction performance strategies terms normalized mean-square error test data averaged different dataset next discuss elmg output training data interpreted smoothing action across observations graph nodes. show elmg performs shrinkage along principal components graph laplacian kernel matrix. vectorizing using denotes rank equal number nonzero singular values reduced singular value matrix submatrix non-zero diagonal full singular value matrix. then βl⊗hh partitions noise realizations. discussed earlier hypothesis graph signal structure helps improve prediction elm. experiments show indeed case. consider temperature measurements largest cities sweden period october december consider geodesic graph whose adjacency matrix given geodesic distance cities. consider target vector temperature measurements cities given corresponding measurements previous input construct noisy training targets adding white gaussian noise signal-to-noise level nmse function sigmoid activation function plotted figure observe elmg outperforms signﬁcant margin particularly small sample sizes. increased performance elmg almost coincide. nmse obtained different values three activation functions listed table next consider functional magnetic resonance imaging data obtained cerebellum region brain used data consists intensity values different voxels obtained fmri cerebellum region. graph obtained mapping cerebellum voxels anatomically following atlas template refer details graph construction associated signal extraction. consider ﬁrst nodes analysis. goal intensity values ﬁrst vertices input make predictions remaining vertices forms output total graph signals corresponding different measurements single subject. half data training half testing. earlier experiment construct noisy training targets signalto-noise level nmse prediction mean testing data averaged different random partitions dataset different realizations noise three activation functions listed table also seen figure trend remains like case temperature data elmg outperforms bring together extreme learning machines graph signal processing. using assumption signal predicted smooth graph relevant regression problem uses graph-laplacian matrix well kernel observed inputs. resulting solution interpretation providing simultaneous smoothness across training samples across graph nodes. hypothesis graph knowledge improve performance extreme learning machine veriﬁed experiments real data. a.a. mohammed minhas q.m. jonathan m.a. sid-ahmed human face recognition based multidimensional extreme learning machine pattern recognition vol. semi-supervised learning visual content analysis understanding. g.-b. huang chen c.-k. siew universal approximation using incremental constructive feedforward networks random hidden nodes trans. neur. netw. vol. july distributed extreme learning machine alternating direction method multiplier neurocomputing vol. supplement advances extreme learning machines shuman s.k. narang frossard ortega vandergheynst emerging ﬁeld signal processing graphs extending highdimensional data analysis networks irregular domains ieee signal process. mag. vol. sandryhaila moura data analysis signal processing graphs representation processing massive data sets irregular structure ieee signal process. mag. vol.", "year": "2018"}