{"title": "Deep Learning Based Speech Beamforming", "tag": "eess", "abstract": " Multi-channel speech enhancement with ad-hoc sensors has been a challenging task. Speech model guided beamforming algorithms are able to recover natural sounding speech, but the speech models tend to be oversimplified or the inference would otherwise be too complicated. On the other hand, deep learning based enhancement approaches are able to learn complicated speech distributions and perform efficient inference, but they are unable to deal with variable number of input channels. Also, deep learning approaches introduce a lot of errors, particularly in the presence of unseen noise types and settings. We have therefore proposed an enhancement framework called DEEPBEAM, which combines the two complementary classes of algorithms. DEEPBEAM introduces a beamforming filter to produce natural sounding speech, but the filter coefficients are determined with the help of a monaural speech enhancement neural network. Experiments on synthetic and real-world data show that DEEPBEAM is able to produce clean, dry and natural sounding speech, and is robust against unseen noise. ", "text": "speech enhancement tasks unfortunately directly applying deep enhancement networks multi-channel enhancement suffers difﬁculties. first deep enhancement techniques often produce artifacts nonlinear distortions perceptually undesirable. second neural networks often generalize poorly unseen noise conﬁgurations whereas speech enhancement ad-hoc sensors variability large. turns problems turn resolved traditional beamforming. therefore several algorithms proposed applies deep learning predict time-frequency masks beamforming produce enhanced speech. however methods conﬁned frequency domain suffers problems application. first work well ad-hoc microphones spatial correlation estimation errors. second application human consumption frequency-domain methods suffer phase distortions discontinuities impede perceptual quality. motivated observation proposed enhancement framework ad-hoc microphones called deepbeam combines deep learning beamforming directly works waveform. deepbeam introduces time-domain beamforming ﬁlter produce natural sounding speech ﬁlter coefﬁcients iteratively determined help wavenet shown despite error-prone enhancement network deepbeam able converge approximately optimal beamformer assumptions. experiments simulated real-world data show deepbeam able produce clean natural sounding speech generalize well various settings. denotes discrete convolution denotes additive noise. impulse responses signal reverberation noise reverberation k-th channel respectively. goal design τ-tap beamformer whose output deﬁned multi-channel speech enhancement ad-hoc sensors challenging task. speech model guided beamforming algorithms able recover natural sounding speech speech models tend oversimpliﬁed inference would otherwise complicated. hand deep learning based enhancement approaches able learn complicated speech distributions perform efﬁcient inference unable deal variable number input channels. also deep learning approaches introduce errors particularly presence unseen noise types settings. therefore proposed enhancement framework called deepbeam combines complementary classes algorithms. deepbeam introduces beamforming ﬁlter produce natural sounding speech ﬁlter coefﬁcients determined help monaural speech enhancement neural network. experiments synthetic real-world data show deepbeam able produce clean natural sounding speech robust unseen noise. multi-channel speech enhancement ad-hoc sensors long challenging task traditional benchmark multichannel enhancement tasks beamforming algorithms work well ad-hoc microphones. beamformers need calibrate speaker location well interference characteristics turn beam toward speaker suppressing interference. however neither vital information accurately measured missing sensor position information microphone heterogeneity another class beamforming algorithms avoid measuring speaker position interference. instead introduce prior knowledge speech optimal beamformer maximizing speechness criteria sample kurtosis negentropy speech prior distributions ﬁtting glottal residual etc. particular grab algorithm able outperform closest microphone strategy even adverse real-world scenarios. despite success algorithms bottlenecked oversimpliﬁed prior knowledge. example grab models glottal energy resulting vocal tract ambiguity. fact projection matrix onto beamforming output space. essentially projecting onto space representable beamforming ﬁlter. shown solving wiener ﬁltering problem requires computing which complex probabilistic dependencies would like introduce deep neural network learn. however discussed training neural network directly predict multi-channel input suffers inﬂexible input dimensions artifacts poor generalization. deepbeam tries resolve problems approximate solution. section describe deepbeam algorithm. ﬁrst outline algorithm describe neural network structure applies. finally convergence analysis introduced. alg. essentially alternates posterior expectation projection iteratively. shown section long error term large iteration approximately converge optimal beamformer output. elegance deepbeam regarded noisy observation shares statistical structures true noisy observations this notice output beamformer therefore shown also takes form speech noise source different impulse response. justiﬁes monaural enhancement network take care mentioned deepbeam introduces deep enhancement network learn posterior expectation addressing limitations. first deepbeam regularized beamformer generalize well unseen noise microphone conﬁgurations. second tolerates distortions artifacts generated neural network. formally neural network outputs inaccurate prediction posterior expectation single-channel noisy observation prediction error. goal deepbeam approximate optimal beamformer given inaccurate enhancement network. alg. shows description deepbeam algorithm. graph deepbeam framework shown fig. enhancement network applied similar inspired wavenet formally denote quantized speech samples samples enhancement network predicts posterior probability mass function contains stack dilated convolutional layers residual connections skip outputs. second module called post processing module sums skip outputs feeds stack fully connected layers producing ﬁnal output. major differences standard wavenet structure. first input enhancement network noisy observation waveform instead clean speech. second account future dependencies convolutional layers noncausal instead causal posterior distribution predicted posterior moments var|y] computed moments predicted pmf. constant. assumption actually quite stringent bound weighted norm itself projected value fact projection drastically reduce weighted norm error term. example artifacts nonlinear distortions enhancement network introduces cannot possibly generated beamforming therefore removed projection. errors likely remain residual noise reverberations. advantage combining beamforming ﬁlter neural network. assumption also intuitive. means projected output error always smaller input error. implies mean square convergence optimal beamformer output. actuality nonzero tends small. ﬁrst term measures distance optimal beamformer output true speech. according empirical study number channel sufﬁcient optimal beamformer able recover true speech well ﬁrst term small. second term measures distance posterior expectations former conditional single-channel noisy speech latter multiple-channel noisy speech. considering speech sample space highly structured noisy speech relatively clean already posterior expectations close true speech thereby close other. nutshell small deepbeam prediction highly accurate. section verify convergence behavior deepbeam empirically. section ﬁrst introduces enhancement network conﬁgured trained presents results experiments simulated real-world data. audio samples found http//tiny.cc/aqjoy enhancement network hyperparameter conﬁgurations follow blocks dilated convolution layers. post processing layers. hidden node dimension skip node dimension clean speech quantized level µ-law companding thus output dimension activation function dilated convolutional layers gated activation unit; post processing layers relu function. output activation softmax. enhancement network trained simulated data only generated speech source noise source eight microphones randomly placed randomly sized cubic room. impulse response source microphone generated using image-source method noisy observations generated according reverberation time uniformly randomly drawn energy ratio speech source noise source uniformly randomly drawn speech content drawn vctk contains speakers. noise content contains minutes audio drawn total duration training audio hours. enhancement network trained using adam optimizer iterations. simulated data evaluation generated training data except differences. first source energy ratio four levels second speaker noise either seen unseen training leading four different scenarios test generalizability. worth highlighting unseen speaker utterances unseen noise drawn different corpora training timit freesfx respectively. utterance seconds length. total length dataset minutes. signal-to-noise ratio energy ratio processed clean speech processed noise direct-to-reverberant ratio ratio energy direct path speech processed output reverberation direct path reverberation deﬁned clean speech convolved peak portion tail portion processed room impulse response. peak portion deﬁned within highest peak; tail portion deﬁned beyond. table shows results. expected deepbeam’s performance drops noise speaker seen training neither seen. however terms even deepbeam signiﬁcantly outperforms mvdr benchmark noise suppression. terms deepbeam matches surpasses closest except grab performs poorer utterance reduced seconds seconds realistic challenging. short cleanness dryness algorithms achieve deepbeam achieve superior performance. deepbeam baselines also evaluated real-world dataset introduced consists utterances speakers mixed types noises recorded real conference room using eight randomly positioned microphones. source energy ratio closest microphone utterance scenario around minute long total length dataset minutes. besides subjective test similar performed amazon mechanical turk. utterance broken sentences. test unit called subject presented sentence processed algorithms asked assign them. assigned subjects. table shows results. seen deepbeam outperforms algorithms large margin. particular deepbeam achieves noise types. results impressive deepbeam trained simulated data. real-world data differ signiﬁcantly simulated data terms speakers noise types recording environment. what’s more microphones contaminated strong electric noise accounted still deepbeam manages conquer unexpected. neural network used vulnerable unseen scenarios deepbeam made robust. order empirically test whether deepbeam good convergence property sets eight-channel simulated data generated setting study different number channels sub-test channels randomly drawn data deepbeam prediction resulting convergence curves sets averaged. runs fig. shows averaged convergence curves. seen deepbeam converges well sub-tests supports convergence discussions section also channels deepbeam higher convergence level reach shows deepbeam able accommodate different numbers channels using monaural network. also marginal beneﬁt channel diminishes. proposed deepbeam solution multi-channel speech enhancement ad-hoc sensors. deepbeam combines complementary beamforming deep learning techniques exhibited superior performance generalizability terms noise suppression reverberation cancellation perceptual quality. deepbeam made step closer resolving long lasting crux perceptual quality poor generalizability deep enhancement networks demonstrates power bridging signal processing deep learning areas. shmulik markovich-golan alexander bertrand marc moonen sharon gannot optimal distributed minimumvariance beamforming approaches speech enhancement wireless acoustic sensor networks signal processing vol. bradford gillespie henrique malvar dinei florˆencio speech dereverberation maximum-kurtosis subband adaptive ﬁltering ieee international conference acoustics speech signal processing ieee vol. kenichi kumatani john mcdonough barbara rauch dietrich klakow philip garner weifeng beamforming maximum negentropy criterion ieee transactions audio speech language processing vol. taesu hagai attias soo-young te-won blind source separation exploiting higher-order frequency dependencies ieee transactions audio speech language processing vol. daichi kitamura nobutaka hiroshi sawada hirokazu kameoka hiroshi saruwatari determined blind source separation unifying independent vector analysis nonnegieee transactions audio ative matrix factorization speech language processing vol. po-sen huang minje mark hasegawa-johnson paris smaragdis deep learning monaural speech separation ieee international conference acoustics speech signal processing felix weninger john hershey jonathan roux bj¨orn schuller discriminatively trained recurrent neural networks single-channel speech separation ieee global conference signal information processing jahn heymann lukas drude reinhold haeb-umbach neural network based spectral mask estimation acoustic beamforming acoustics speech signal processing ieee international conference ieee hakan erdogan john hershey shinji watanabe michael mandel jonathan roux improved mvdr beamforming using single-channel mask prediction networks. interspeech xiong xiao shengkui zhao douglas jones siong chng haizhou time-frequency mask estimation mvdr beamforming application robust speech recognition acoustics speech signal processing ieee international conference ieee xueliang zhang zhong-qiu wang deliang wang speech enhancement algorithm iterating single-and multimicrophone processing application robust acoustics speech signal processing ieee international conference ieee lukas pfeifenberger matthias z¨ohrer franz pernkopf dnn-based speech mask estimation eigenvector beamacoustics speech signal processing forming ieee international conference ieee aaron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu wavenet generative model audio arxiv preprint arxiv. eric lehmann anders johansson diffuse reverberation model efﬁcient image-source simulation room impulse responses ieee transactions audio speech language processing vol. john garofolo lori lamel william fisher jonathon fiscus david pallett darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc nasa sti/recon technical report vol. freesfx http//www.freesfx.co.uk/ lloyd grifﬁths alternative approach linearly constrained adaptive beamforming ieee transactions antennas propagation vol. fl´avio ribeiro dinei florˆencio zhang michael seltzer crowdmos approach crowdsourcing mean ieee international conference opinion score studies acoustics speech signal processing ieee", "year": "2018"}