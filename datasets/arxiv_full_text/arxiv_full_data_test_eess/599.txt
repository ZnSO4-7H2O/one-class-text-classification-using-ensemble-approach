{"title": "Deep factorization for speech signal", "tag": "eess", "abstract": " Various informative factors mixed in speech signals, leading to great difficulty when decoding any of the factors. An intuitive idea is to factorize each speech frame into individual informative factors, though it turns out to be highly difficult. Recently, we found that speaker traits, which were assumed to be long-term distributional properties, are actually short-time patterns, and can be learned by a carefully designed deep neural network (DNN). This discovery motivated a cascade deep factorization (CDF) framework that will be presented in this paper. The proposed framework infers speech factors in a sequential way, where factors previously inferred are used as conditional variables when inferring other factors. We will show that this approach can effectively factorize speech signals, and using these factors, the original speech spectrum can be recovered with a high accuracy. This factorization and reconstruction approach provides potential values for many speech processing tasks, e.g., speaker recognition and emotion recognition, as will be demonstrated in the paper. ", "text": "reasons firstly factors mixed unclear seems highly complex; secondly perhaps fundamentally major factors particularly speaker trait behaves long-term distributional properties rather short-time patterns. partly demonstrated fact successful speaker recognition approaches i-vector rely statistical models retrieve speaker vectors based multiple frames therefore wide suspicion speech signals short-time factorizable. fortunately recent study showed speaker traits largely short-time spectral patterns carefully designed deep neural network learn extract patterns frame level following studies demonstrated frame-level deep speaker features highly generalizable work well voices trivial events laugh cough short seconds robust language mismatch shorttime property speaker traits suggests speech signals possibly short-time factorizable known anmajor speech factor linguistic content also shorttime identiﬁable paper present cascaded deep factorization approach obtain factorization. approach signiﬁcant factors inferred ﬁrstly less signiﬁcant factors inferred subsequently condition factors already inferred. experiments speaker recognition task emotion recognition task demonstrated cdf-based factorization highly effective. furthermore show original speech signal reconstructed cdf-derived factors pretty well. previous study presented ct-dnn structure learn speaker features frame level. network consists convolutional component timedelay component. component comprises layers followed max-pooling layer. component comprises layers followed pnorm layer. output second p-norm layer projected feature layer. activations units layer length normalization form speaker feature input speech frame. model training feature layer fully connected output layer whose units correspond speaker identities training data. trainvarious informative factors mixed speech signals leading great difﬁculty decoding factors. intuitive idea factorize speech frame individual informative factors though turns highly difﬁcult. recently found speaker traits assumed long-term distributional properties actually short-time patterns learned carefully designed deep neural network discovery motivated cascade deep factorization framework presented paper. proposed framework infers speech factors sequential factors previously inferred used conditional variables inferring factors. show approach effectively factorize speech signals using factors original speech spectrum recovered high accuracy. factorization reconstruction approach provides potential values many speech processing tasks e.g. speaker recognition emotion recognition demonstrated paper. speech signals involve rich information including linguistic content speaker trait emotion channel background noise etc. researchers worked several decades decode information leading multitude speech information processing tasks including automatic speech recognition speaker recognition longterm research tasks addressed pretty well least large amount data available e.g. sre; others remain difﬁcult e.g. automatic emotion recognition major difﬁculty speech processing resides fact multiple informative factors intermingled together whenever decode particular factor factors contribute uncertainties. intuitive idea deal information blending factorize speech signal individual informative factors frame level. however turns highly difﬁcult least work supported national natural science foundation china grant national basic research program china grant no.cb. pre-print version published arxiv.. dong wang corresponding author performed optimize cross-entropy objective aims discriminate training speakers based input frames. demonstrated speaker feature inferred ct-dnn structure highly speaker-discriminative speaker traits largely short-time spectral patterns identiﬁed frame level. research demonstrated linguistic content individually inferred frame level deep speaker feature learning method described previous section demonstrated speaker traits also identiﬁed short segment. denote single factor inference method based deep neural models individual deep factorization rationality method two-fold firstly target factor sufﬁciently signiﬁcant speech signals; secondly large amount training data available. large-scale supervised learning picks task-relevant factors leveraging power dnns feature learning. factors less signiﬁcant and/or without much training data however applicable. fortunately successful inference linguistic speaker factors signiﬁcantly simplify inference ‘not prominent’ factors. motivated cascaded deep factorization approach ﬁrstly infer particular factor factor conditional variable infer second factor finally speech signal factorized individual factors corresponding particular task. demonstrate concept apply approach factorize emotional speech signals three factors linguistic speaker emotion. fig. illustrates architecture. firstly system trained using word-labelled speech data. frame-level linguistic factor form phone posteriors study produced system concatenated feature train system. system used produce frame-level speaker factor discussed previous section. linguistic factor speaker factor ﬁnally concatenated feature train system emotion factor produced last hidden layer. approach fundamentally different conventional factorization approach e.g. frame-level conventional methods segment-level; relies discriminative training conventional factorization methods rely maximum likelihood estimation; infers factors sequentially data partial labels conventional approaches infer factors jointly fulllabelled data; based dnns deep nonlinear non-gaussian conventional approaches based models shallow linear gaussian. highlight complex model structures possible conduct factorization e.g. collaborative learning architecture however framework consistent cascaded convolution view speech interesting property cdf-inferred factors used recover original speech. deﬁne linguistic factor speaker factor emotion factor speech frame three factors recover spectrum according cascaded convolution view reconstruction written form non-linear recovery function respectively implemented dnn. represents residual assumed gaussian. reconstruction illustrated fig. spectra domain. note inferred fbanks rather original spectra still recover original signal pretty well seen section approach shared similar motivation phonetic i-vector approach utilize phonetic factor support inference factors. difference neural model retrieves framelevel features phonetic i-vector probabilistic model retrieves utterance-level representations. also related multi-task learning transfer learning multiple tasks used regularize training compared methods approach focuses explaining variabilities speech signals rather learning models. finally approach also related auto-encoder regarded factor following experiments. kaldi toolkit used train model following kaldi nnet recipe. structure consists hidden layers containing units. input feature fbanks output layer discriminates pdfs. ofﬁcial -gram language model word error rate system linguistic factor represented -dimensional phone posteriors derived output dnn. build three systems i-vector/plda system represent conventional statistical model approach; d-vector system follows ct-dnn architecture features comprise input; d-vector system follows spirit linguistic factors produced system used additional input ct-dnn architecture. i-vector system composed gaussian components i-vector dimension system trained following kaldi recipe. d-vector systems frame-level speaker features dimensions utterance-level d-vector derived average frame-level features within utterance. details d-vector systems found report results identiﬁcation task though similar observations obtained veriﬁcation task. identiﬁcation task matched speaker identiﬁed given test utterance. i-vector system enrolled speaker represented i-vector enrolled speech i-vector test speech compared i-vectors enrolled speakers ﬁnding speaker whose enrollment i-vector nearest test speech. i-vector system popular plda model used measure similarity i-vectors; d-vector system simple cosine distance used. results terms top- identiﬁcation rate shown table table means test condition duration enrollment speech seconds test speech frames. note frames length effective context window speaker ct-dnn single frame speaker feature used condition. results observed d-vector system performs much better i-vector baseline particularly short speech segments. comparing results seen approach involves phonetic knowledge conditional variable greatly improves d-vector system short speech segment condition. section applies approach emotion recognition task. purpose ﬁrst build dnn-based baseline. model consists hidden layers containing units. layer p-norm layer reduces dimensionality output layer database database used train system. training data ofﬁcial train dataset composed speakers utterances. test contains three datasets including speakers utterances total. database fisher database used train systems. training consists male female speakers utterances total. speaker seconds speech signals. used training t-matrix plda models i-vector baseline system model described section evaluation consists male female speakers utterances total. speakers training evaluation overlapped. speaker utterances used enrollment rest test. database cheavd database used train systems. database selected chinese movies programs used standard database multimodal emotion recognition challenge emotions total happy angry surprise disgust neutral worried anxious sad. training contains utterances evaluation contains utterances. comprises units corresponding number emotion classes cheavd database. model produces frame-level emotion posteriors. utterance-level posteriors obtained averaging frame-level posteriors utterance-level emotion decision achieved. three conﬁgurations investigated according factor used conditional linguistic factor speaker factor results evaluated metrics identiﬁcation accuracy ratio correct identiﬁcation emotion categories; macro average precision average emotion category. last experiment linguistic factor speaker factor emotion factor reconstruct original speech signal using convolutional model shown fig. model trained using cheavd database. training processing averaged frame-level reconstruction loss validation reduced loss evaluation trained model fig. shows reconstruction example utterance selected test seen three cheavd database. factors reconstruct original spectrum pretty well. listening tests show reconstruction quality rather good hard human listeners tell difference reconstructed speech original speech. examples found project site http//project.cslt.org. success ‘deep reconstruction’ indicates approach factorizes speech signals task-oriented informative factors also preserves information speech factorization. moreover demonstrates cascaded convolution view largely correct. essentially provides vocoder decomposes speech signals sequential convolution task-oriented factors fundamentally different classical source-ﬁlter model. results training shown table values frame-level utterance-level reported. seen conditional factors involved values training signiﬁcantly improved speaker factor seems provide contribution. improvement training accuracy demonstrates conditional factors considered speech signal explained much better. results evaluation also shown table again observe clear advantage training. note involving factors improve utterance-level results. attributed fact models trained using frame-level data fully consistent metric utterancelevel test. nevertheless superiority multiple conditional factors seen clearly frame-level metrics. note discrepancy results training evaluation over-ﬁtting caused approach; simply reﬂects mismatch training evaluation hence difﬁpaper presented cascaded deep factorization approach factorize speech signals individual taskoriented informative factors. experiments demonstrated speech signals well factorized frame level approach speech signals largely reconstructed using deep neural models cdf-derived factors. moreover results emotion recognition task demonstrated approach particularly valuable learning inferring less signiﬁcant factors speech signals. patrick kenny gilles boulianne pierre ouellet pierre dumouchel joint factor analysis versus eigenchannels speaker recognition ieee transactions audio speech language processing vol. najim dehak patrick kenny r´eda dehak pierre dumouchel pierre ouellet front-end factor analysis speaker veriﬁcation ieee transactions audio speech language processing vol. geoffrey hinton deng dong george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. zhiyuan tang lantian dong wang ravichander vipperla collaborative joint training multitask recurrent model speech speaker recognition ieee/acm transactions audio speech language processing vol. hiroya fujisaki communication minds ultimate goal speech communication target research next half-century journal acoustical society america vol. yoshinori sagisaka nick campbell norio higuchi computing prosody computational models processing spontaneous speech springer science business media nicolas scheffer luciana ferrer mitchell novel scheme speaker recognition mclaren using phonetically-aware deep neural network acoustics speech signal processing ieee international conference ieee patrick kenny vishwa gupta themos stafylakis ouellet alam deep neural networks extracting baum-welch statistics speaker recognition proc. odyssey dong wang thomas fang zheng transfer learning speech language processing signal information processing association annual summit conference asia-paciﬁc. ieee yanmin qian tian dong neural network based multi-factor aware joint training robust speech recognition ieee/acm transactions audio speech language processing vol. modeling speaker variability using long short-term memory networks speech recognition sixteenth annual conference international speech communication association mingliang minghao yang linlin chao jianhua building chinese natural emotional audio-visual database signal processing international conference ieee jianhua bj¨orn schuller shiguang shan dongmei jiang multimodal emotion recognition challenge ccpr chinese conference pattern recognition. springer", "year": "2018"}