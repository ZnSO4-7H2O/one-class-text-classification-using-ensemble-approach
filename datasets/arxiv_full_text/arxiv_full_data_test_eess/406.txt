{"title": "Weakly-supervised Dictionary Learning", "tag": "eess", "abstract": " We present a probabilistic modeling and inference framework for discriminative analysis dictionary learning under a weak supervision setting. Dictionary learning approaches have been widely used for tasks such as low-level signal denoising and restoration as well as high-level classification tasks, which can be applied to audio and image analysis. Synthesis dictionary learning aims at jointly learning a dictionary and corresponding sparse coefficients to provide accurate data representation. This approach is useful for denoising and signal restoration, but may lead to sub-optimal classification performance. By contrast, analysis dictionary learning provides a transform that maps data to a sparse discriminative representation suitable for classification. We consider the problem of analysis dictionary learning for time-series data under a weak supervision setting in which signals are assigned with a global label instead of an instantaneous label signal. We propose a discriminative probabilistic model that incorporates both label information and sparsity constraints on the underlying latent instantaneous label signal using cardinality control. We present the expectation maximization (EM) procedure for maximum likelihood estimation (MLE) of the proposed model. To facilitate a computationally efficient E-step, we propose both a chain and a novel tree graph reformulation of the graphical model. The performance of the proposed model is demonstrated on both synthetic and real-world data. ", "text": "abstract—we present probabilistic modeling inference framework discriminative analysis dictionary learning weak supervision setting. dictionary learning approaches widely used tasks low-level signal denoising restoration well high-level classiﬁcation tasks applied audio image analysis. synthesis dictionary learning aims jointly learning dictionary corresponding sparse coefﬁcients provide accurate data representation. approach useful denoising signal restoration lead sub-optimal classiﬁcation performance. contrast analysis dictionary learning provides transform maps data sparse discriminative representation suitable classiﬁcation. consider problem analysis dictionary learning timeseries data weak supervision setting signals assigned global instead instantaneous label signal. propose discriminative probabilistic model incorporates label information sparsity constraints underlying latent instantaneous label signal using cardinality control. present expectation maximization procedure maximum likelihood estimation proposed model. facilitate computationally efﬁcient e-step propose chain novel tree graph reformulation graphical model. performance proposed model demonstrated synthetic real-world data. synthesis dictionary learning dictionary corresponding sparse representation jointly learned data aims minimizing reconstruction error. overcomplete synthesis dictionary learning sparse coding introduced various state-of-the-art approaches introduced solve dictionary learning problem including k-svd matrix factorization lagrangian dual gradient descent feature-sign search audio spectral image analysis convolutive dictionary learning proposed analysis dictionary learning transform learning offer alternative dictionary learning produces sparsiﬁed outcome applying analysis dictionary original data. stated analysis approach shows signiﬁcant advantage synthesis denoising approach terms signal recovery random piecewise-constant natural signal data. linear dependencies sets rows dictionary improves recovery quality pursuit algorithm. linear dependencies incorporated forcing sparse ∗all authors school electrical engineering computer science oregon state university corvallis e-mail{youzraichxfernkimjinsu}oregonstate.edu. work partially supported national science foundation grants ccf- iis-. zero-mean rows training. synthesis analysis dictionary learning used solving problems reconstruction denoising sparse coding. however without additional supervision component methods reported perform sub-optimally classiﬁcation tasks fact approaches reconstruction rather classiﬁcation. nevertheless approaches applied classiﬁcation modifying objective include label term renders learned dictionary discriminative possible. detailed discussion supervised learning approaches provided subsection ii-c. paper consider weak-supervision setting analysis dictionary learning suitable classiﬁcation. weak-supervision setting data portion containing multiple data points label describing classes present absent label instance remains unavailable. motivating example setting problem in-situ bio-acoustic monitoring audio recordings varying signal-to-noise ratio collected unattended microphones. even fairly short intervals common multiple simultaneous vocalizations multiple species addition noise sources wind rain stream nearby vehicles. consequently intervals audio data associated multiple class labels. since audio signals multi-labeled interval level precise location class pattern contained signal unknown. hence isolating individual pattern assigning appropriate label training example traditional supervised learning setting trivial. discriminative dictionary learning approaches considered full label information available i.e. data example associated label weakly-supervised setting learning dictionary given observed signals/images binary labels considered non-convolutive synthesis dictionary learning approaches introduced best knowledge approaches cannot easily extended convolutive analysis dictionary learning setting. work focuses dictionary learning weak supervision learn dictionary given signals label sets. focusing time-series propose algorithm weakly-supervised convolutive analysis dictionary learning. contributions follows. develop novel discriminative probabilistic model analysis dictionary learning weak-supervision setting. present alternative approach cardinality constraints sparse. noisy signal model analysis dictionary learning formulated minimizing quadratic error xi’s subject resulting analyzed signals sparse. convolutive analysis dictionary learning convolves analysis dictionary estimated noiseless signals resulting signals sparse discriminative dictionary learning several approaches proposed dictionary learning presences labels. supervised dictionary learning following approaches proposed learn dictionary class prune large dictionaries jointly learn dictionary classiﬁer embed class labels learning sparse coefﬁcients learn histogram dictionary elements signal constituents weakly supervised dictionary learning several margin based non-convolutive synthesis dictionary learning approaches proposed approaches propose learn discriminative synthesis dictionary fully exploiting visual attribute correlations rather label priors preliminary work convolutive analysis dictionary learning approach weak-supervision proposed. here expand approach simplify previous graphical model re-derive formulations. addition propose novel inference approach tree reformulation graphical model allowing near linear processing time probabilistic calculations. iii. problem formulation solution approach throughout paper denote signals lower case e.g. similarly lower case letters represent indexes simplicity omit dependence time signals e.g. also denote signal cases time-evaluation operator denote evaluation signal time e.g. denote vectors boldfaced lower case e.g. upper case letters denote sets e.g. constants signals paper assumed discrete time. consequently convolution operator denote implicit observations graphical model opposed commonly used norm regularization. approach allows localization patterns-of-interest. introduce novel framework efﬁcient message passing using reformulation proposed graphical model chain tree. reformulation yields near-linear exact probability calculation alleviates need approximate inference. preliminary work topic introduced synthesis dictionary learning goal simultaneously dictionary corresponding coefﬁcients represent signals dictionary collection atoms dictionary atom signal approximated linear combination dictionary coefﬁcient vector associated signal. synthesis dictionary learning typically formulated optimization problem goal minimize sparse coefﬁcients {αi}n reconstruction error. convolutive dictionary learning often considered time invariant signals speech audio. convolutive dictionary learning signal assumed formed combining convolution dictionary words corresponding sparse activation signals approach signal contain multiple time-shifted copies dictionary signal convolutive dictionary learning eliminates need additional dictionary words model multiple shifts dictionary word. joint recovery dictionary sparse activation signals achieved minimizing reconstruction error subject sparsity constraints activation signals. model paper provides simpliﬁcation model make model elegant clear. present derivations model detail paper. additionally novel tree reformulation graphical model provided paper. thirdly modeling variation derivations redone. results synthetic dataset real-world datasets well. model assumptions develop model introduce additional assumptions aforementioned setting explain link analysis result signal label. speciﬁcally assume convolutive instance labeler signal hidden discrete-value label signal produced given analysis result time i.e. probability c|xn) depends signal analysis result evaluated time analysis signal sparse without loss generality assume support included i.e. fully-supervised setting potentially sparse time-instance label signal provided observed data form signal viewed ﬁne-grain label indicating presence particular class patterns point time. setting location pattern given class used extract examples class train classiﬁer. weak supervision setting unknown interested learning discriminative version convolutive analysis dictionary given observed data. setting every signal accompanied single label containing classes present signal e.g. contains patterns classes hence data provided setting form goal setting two-fold develop time-instance-level classiﬁer predicts latent label signal value previously unseen signal based training data develop signal-level classiﬁer predicts label previously unseen signal based training data proceed proposed formulation problem using probabilistic model approach. solve weakly-supervised dictionary learning problem present probabilistic model learning discriminative convolutional dictionary. begin introducing convolution used model proceed graphical representation proposed discriminative convolutional dictionary learning model. convolutional model simplify exposition model using vectors instead signals convert signal vectors vector width windowed portion signal. simplicity assume denote notation allows replace convolution −∆−∆ aforementioned one-dimensional signal model extended two-dimensional signal model convolution analysis dictionary applied time dimension. two-dimensional setting denotes analysis dictionary word signal denotes using note assumption makes possible empty signal label case instantaneous labels zero positive class associated time instance. simplicity remove braces change represent union time instances. calculating incomplete data likelihood involves enumerating possible instance labels computationally intractable especially number instance signal large. expectation maximization exact inference computes likelihood brute force manner marginalizing instance value combinations intractable. resolve this consider expectation maximization approach proposed approach similar speciﬁcally algorithm alternated expectation hidden variable maximization auxiliary function following steps derivation auxiliary function model explained appendix maximization auxiliary function provides update rule dictionary words bias terms learning rate model description probabilistic graphical model weakly-supervised convolutive analysis dictionary learning shown figure which target estimate model parameters explained earlier latent label signal time given depends entire signal convolution evaluated time hence signal window xnt. probabilistic model follows multinomial logistic regression model given encode notion sparsity instance label introduce class following novel class concept integrate constraint number non-zero instances signal probability model introduce latent random variable indicator takes value number nonzero less equal sparsity upper bound zero otherwise. treat tuning parameter graphical model. smaller value sparser label signal probability model sparsity indicator label signal given since class represented signal label obtain signal label consider possibilities. first label signal contain zeros expect ∪t{yn}. alternatively label signal contains zeros expect ∪t{yn}. consequently corresponding probabilistic model signal label given given parametric representation proposed model natural consider estimating model parameter using maximum likelihood estimation since model contains hidden variables adopt expectation-maximization framework facilitate estimator. continue derivation complete incomplete data likelihood. challenge computation gradient involves computation posterior probability c|yn signal. term presents challenges inference proposed model. brute-force calculation requires marginalization instance level labels i.e. marginalization exponential number instances signal hence makes brute-form calculation prohibitive. following present proposed efﬁcient approach calculating posterior instance level label probability. graphical model reformulation e-step goal e-step obtain posterior probability c|yn ﬁrst requires calculation prior probability. efﬁciently compute prior probability |xn; function signal convolutions form performed obtain values −∆−∆ settings fast fourier transform inverse used computationally efﬁcient implementation convolution. proceed efﬁcient procedures calculating posterior probability given prior probabilities. term c|xn; calculated using regarded prior probability instance label i.e. without information signal label sparsity constraint. term c|yn viewed posterior instance label probability takes account signal label sparsity constraint. denote difference posterior probability prior c|yn c|xn; gradient calculation w.r.t. performed convolution time-reversed signal consider label portion original graphical model figure enumerating possible values compute posterior exponential respect number time instances. v-structure graph figure offer immediate efﬁcient approach computing posterior. hence propose reformulation model follows. denote =))p c|xn bi). properties understand range used computing joint probability examine values forward backward messages non-zero. forward backward messages following properties sparsity deﬁnition constraint deﬁnition step joint probability. finally numerator computed using forward messages backward messages |xn; ¯nn) c|xn; =))αt−) reformulation gives rise chain model figure proceed linear time procedure calculating posterior probabilities takes advantage reformulation model chain. forward backward message passing chain compute joint probabilities pntc using dynamic programming approach summarized following three steps step forward message passing. step goal n|xn; compute joint state probability note efﬁciently calculate store forward backward messages consider following results obtained recursive formula initialization obtained sparsity constraint deﬁnition based summary effective calculation cardinality constraints size signal large chain model reformulation become computational-wise inefﬁcient since complexity time space grows increases. instead propose complete full binary tree depth indicates tree level node variable index level log) graph structure reformulation original graphical model figure make e-step calculation efﬁcient. complete full binary tree structure node tree nt). denote label ancestors node level tree number non-zero class instances ancestors node level tree. present recursive relation leaf’s level assign values tree inference target chain inference compute posterior probability using dynamic programming approach joint probabilities in|xn; computed efﬁciently following three steps c|xn; bi). convolutive model tree based update equation forward message treat message particular value discrete signal update forward message performing upconvolution date backward message update convolution backward message signal update convolution backward message signal complexity analysis divided three parts prior calculation posterior calculation e-step gradient calculation m-step. evaluate prior probability gradient update forming number convolutions time domain signal. therefore time complexity tntw). space complexity respectively. posterior calculation e-step chain forward backward messages require possible values previous state values ¯nn. therefore overall time space complexity |yn||yn| ¯nn) ¯nn) respectively. formulate tree forward backward messages need possible values previous parents’ states. therefore resulting time nn)) section ﬁrst present runtime comparison chain tree model reformulations e-step inference. continue evaluating performance proposed approach using synthetic datasets real world datasets. computational complexity three main calculations namely prior calculation posterior calculation gradient calculation m-step. since posterior calculation dominates computational complexity since focused developing efﬁcient computation step following results runtime analysis posterior calculation e-step based chain tree reformulations model. used randomly generated prior probability input posterior calculation. illustrate relationships e-step posterior calculation time number classes signal |yn| number time instances signal sparsity regularization signal vary |yn| nn/tn figure shows posterior calculation time signal based tree reformulation grows nearly-linear rate respect setting sparsity level .tn. addition shows chain based inference time grows quadratically however chain reformulation efﬁcient tree approach small |yn| large. even though figure shows posterior calculation time exponential respect |yn| number classes signal usually small number practice figure exhibits nearconstant runtime respect sparsity factor ¯nn/tn tree inference. runtime values models shown table designing synthetic datasets goal test performance proposed algorithms different types data terms dimension data whether generative discriminative approach taken data generation mechanism. data generated follows. first generate label sets using ﬁxed proportion contains single label contain contain three contain label pure noise. labels label generated sampling uniformly replacement nine classes target size reached. given non-empty label generate signal ﬁrst decide number active time instances signal contain true templates randomly choosing |yn| |yn| |yn| sampling uniformly |yn| |yn| active time instance exact location sampled uniformly without replacement class label uniformly sampled scaling factor sampled generate lastly generate ﬁnal adding white gaussian noise whose variance average signal energy ˜xns snrdb control signal noise ratio ﬁnal data. binary patterns dataset dataset work signals generate data following discriminative assumption. first randomly generated binary sequences size synthetic signals generated signal determined label time instance matching sub-window starting three pre-deﬁned class-speciﬁc templates shown figure label sub-window matched template class respectively otherwise. time instance labels created signal label union corresponding instance labels. experimental setting demonstrate performance proposed approach used random splits signals trained split data tested rest data. random split denote monte-carlo run. evaluated performance test data runs kernel size regularization term cardinality constraints gabor basis dataset ﬁrst tuned model parameters evaluating average signal label prediction accuracy iteration number using cross-validation prediction accuracy found optimal prediction performance used present cardinality constraint parameter function snrdb kernel size regularization term cardinality constraint benchmark competing algorithm generative dictionary learning followed logistic regression approach best knowledge unaware weak-supervision methods convolutive dictionary learning. order provide benchmark considered two-step approach generative convolutive dictionary learning method followed classiﬁer. implementation generative dictionary learning method chose constructed generative dictionary dk}. used matched ﬁlter approach compute test statistic dictionary works maxt xtrain time reversed version dk). combined test statistics feature vector trained logistic regression classiﬁers based feature vectors corresponding binary labels indicating presence absence class resulting classiﬁers performance evaluation instance level classiﬁcation signal classiﬁcation. using runs evaluated proposed gdllr approach trained ﬁxed number outer iterations vary dictionary window size sparsity regularization number dictionary words gabor basis dataset. binary patterns dataset vary sparsity regularization evaluation metric computing instance level detection area-under-the-curve calculate aucc. class obtain ground truth based presence absence given class time stamp c|xtest signal level detection obtained based aucc class aucc obtained based signal level ground truth corresponding test score deﬁned results synthetic datasets gabor basis dataset based highest prediction accuracy hyper-parameters wscadl approach aforementioned cross-validation. hyper-parameters gdl-lr approach optimal window size learned close ground truth gabor basis length. believe kernel size least cover length signal patterns obtain good performance. kernel size large over-ﬁtting occur. proposed wscadl approach competing gdl-lr framework observe prediction performance increases snrdb increases figure methods perform similarly snrdb values medium high values proposed wscadl approach outperforms competing gdl-lr approach. show importance cardinality constraints present signal label accuracy proposed approach average instance-level signal-level detection function cardinality parameter figure figure shows signal label accuracy signal-level drops signiﬁcantly. suspect setting forces non-zero instance labels predicted zero training test. performance drops gracefully. setting allow zero instance labels predicted non-zero. figure optimal terms signal label accuracy. also observe average instance-level reaches peak ground truth maximum cardinality data. however average signal-level signal label accuracy reaches peak slightly higher ground truth. evaluate performance terms ﬁxed snrdb present detection performance methods table comparing instance level signal detection performance class observe proposed wscadl approach outperforms gdl-lr approach. class gdl-lr approach detection aucs comparable wscadl sometimes aucs gdl-lr approach slightly higher approach. variance detection aucs gdl-lr approach mostly higher wscadl approach. suspect since gdl-lr approach performs unsupervised dictionary learning followed classiﬁer training separate fashion resulting words large variability. believe ﬁxed combining steps one. however weaksupervision setting combined approach non-trivial extension best knowledge unavailable. hence provide results two-step approach only. binary patterns dataset hyper-parameters aforementioned cross validation. optimal kernel size slightly higher ground truth window size gdl-lr approach optimal dictionary window size sparsity constraint number dictionary words learned wscadl words figure learned gdl-lr words show wscadl able recover true patterns gdl-lr approach fails. figure also shows wscadl localize corresponding class patterns ideally gdl-lr approach failing task. resulting detection aucs wscadl gdl-lr approaches shown table iii. niﬁcantly. since data constructed linear combination dictionary words gdl-lr approach able recover dictionary could reconstruct data accurately. discriminative data generation setting gdl-lr approach reproduce original data sparsity constraint relaxed. however regardless sparsity gdl-lr approach seems perform poorly classiﬁcation. suspect lack discriminative power gdl-lr dictionary words obtained. world datasets. aasp challenge ofﬁce live scene dataset dataset consists audio recordings sounds taken ofﬁce environment training dataset consists individual events recording varying time various class. test dataset contains seven roughly three minutes long ofﬁce live sound recordings single recording multiple labeled. task detect presence absence events test set. bioacoustic dataset dataset contains labeled -second recordings different bird species. audio recordings bird song collected andrews experimental forest using unattended microphones recording contain multiple species. data preprocessing aasp challenge ofﬁce live training dataset compared proposed approach supervised dictionary learning approaches. since competing supervised dictionary learning algorithms ﬁxed size feature vector created ﬁxed duration training signals various duration training data. fair comparison used modiﬁed short duration training data algorithms. ﬁxed short duration training data selected duration single occurrence sound event lasts less second recordings around duration. recordings longer chunked duration signals. recordings shorter extended using last sample value. note proposed wscadl algorithm require aforementioned preprocessing handle varying signal length. perform detection task test audio minutes long chunk test recordings apply following procedures. datasets audio recording applied spectrogram generation applied windowed signal window size overlap ratio number bins twice window samples; noise whitening column spectrogram divided noise spectrum spectrogram down-sampling matlab built-in imresize function applied experimental setting. ofﬁce live experimental setting considered experiments. ﬁrst experiment trained training dataset consists duration training examples tested sec-long recording test set. second experiment sec-long recordings training test. experiment cross-validated parameter tuning trained original labeled data validated independent data. parameter tuning performed dictionary learning approaches parameters yielded highest prediction accuracy selected. tuning approach dictionary window size cardinality constraint along regularization term using learned wstuning parameter value cadl words optimal evaluated signal instance level detection performance experiment trained sub-sampled test along signal labels generated union event ground truth labels. choosing optimal model parameters considered range experiment evaluated detection performance remaining bioacoustic experimental setting cross-validated parameter tuning trained training data evaluated performance independent data. tunning parameters considered were window size cardinality constraint regularization term learned analysis words optimal tuning parameter value used dictionary predict signal label test set. ofﬁce live event detection compared wscadl approach discriminative dictionary learning approaches sparse representation-based classiﬁcation label consistent k-svd dictionary learning structured incoherence shared features fisher discrimination dictionary learning dictionary learning separating particularity commonality fast low-rank shared dictionary learning object classiﬁcation proposed approach optimal tuning parameters found window size regularization term cardinality parameter shown figure presents performance proposed table signal evaluation metrics various methods dataset. next metric indicates performance improves metric decreased results column m-nn extracted table wscadl approach varying cardinality parameter aasp dataset. setting cardinality parameter less larger optimal value reduces accuracy. discriminative dictionary learning algorithms parameters values tuned cross-validation. algorithm uses training examples dictionary. lcksvd lcksvd dlsi fddl dictionary words class used total number dictionary atoms copar lrsdl algorithms dictionary words class used shared dictionary atoms. however proposed wscadl algorithm assign total dictionary words therefore dictionary word learned predict class. proposed model limited words total since model uses single word class. potential extensions allow words class considered future work. figure shows proposed method outperforms discriminative dictionary learning approaches except src. additionally approach outperforms others predicting whether true class among ranked three classes shown figure instance signal label detection receiver operating curves proposed method shown figure resulting aucs shown table iv). average maximum detection aucs across classes instance signal slightly higher experiment experiment minimum aucs lower. detection best performing class experiment close indicates wscadl able discover class perfectly test recording. moreover potential proposed approach demonstrated using experiment weak-supervision provided. despite limiting setting average aucs experiment comparable higher average aucs reported experiment single label example provided. illustrates potential label-economic weak-supervision setting potential proposed approach setting. bioacoustic classiﬁcation compared proposed wscadl approach gdl-lr approach dictionary learning based approaches methods perform segmentation multi-instance multi-label leaning approaches mimlfast lsb-cmm mimlsvm mimlnn evaluated approaches using multi-label evaluation metrics results indicate proposed wscadl approach outperforms gdl-lr metrics considered. additionally proposed approach shows slight figure classiﬁcation accuracy ofﬁce live training data mean standard deviation runs selecting class selecting classes. detection rocs time instance level signal experiments. advantage terms rank loss average precision miml algorithms. error hamming loss proposed approach comparable performance miml approaches. approach outperformed alternative miml approaches terms coverage. results column m-nn table directly extracted table note alternative miml approaches m-nn involve process converting spectrograms bag-of-words using segmentation feature extraction segment proposed wscadl approach gdl-lr directly applied spectrograms. suspect disadvantage observed alternative miml approaches error propagation segmentation feature extraction steps jointly optimized miml classiﬁcation. inference forward backward message passing update forward backward message requires running possible values therefore computational complexity o|yn| min). since time step depend previous time step overall computational complexity nn)tn)) tree inference forward backward message passing update forward backward message requires running possible values therefore computational complexity however updates forward backward messages tree controlling cardinality parameter operating convolutive nature. large rely inverse speedup convolution convolution complexity become since current instance tree level depend previous parents’ recursive formula overall computational complexity aharon elad bruckstein k-svd algorithm designing overcomplete dictionaries sparse representation ieee transactions signal processing vol. mairal bach ponce sapiro online learning matrix factorization sparse coding journal machine learning research vol. vipperla bozonnet wang evans robust speech recognition multi-source noise environments using convolutive nonnegative matrix factorization proc. chime weak-supervision setting. incorporated cardinality constraints observations enforce sparsity signal label determine location patterns-of-interest given class. model parameter estimation developed update rules introduced novel chain tree reformulations proposed graphical model facilitate efﬁcient probability calculations inference. particular cardinality constraints expressed fraction signal length showed computational complexity chain reformulation quadratic signal length nearly-linear tree reformulation veriﬁed numerical runtime comparison. sanity check demonstrated proposed discriminative approach performs comparably generative alternative data follows generative paradigm. however data follows discriminative model approach outperforms generative approach. additionally showed proposed approach yields competitive sometimes superior performance terms accuracy real-world datasets compared either state-of-the-art approaches dictionary learning alternative solutions weak-supervision setting. algorithm auxiliary function given expectation complete data log-likelihood hidden variables conditioned observed data. therefore auxiliary function formulated ravishankar bresler learning sparsifying transforms ieee transactions signal processing vol. gangeh farahat ghodsi kamel supervised dictionary learning sparse representation-a review arxiv preprint arxiv. nguyen torresani torre rother weakly supervised discriminative localization classiﬁcation joint learning process ieee international conference computer vision. ieee ramirez sprechmann sapiro classiﬁcation clustering dictionary learning structured incoherence shared features ieee conference computer vision pattern recognition winn criminisi minka object categorization learned universal visual dictionary tenth ieee international conference computer vision volume vol. ieee moosmann triggs jurie fast discriminative visual codebooks using randomized clustering forests twentieth annual conference neural information processing systems press zhang zhang huang simultaneous discriminative projection dictionary learning sparse representation based classiﬁcation pattern recognition vol. lazebnik raginsky supervised learning quantizer codebooks information loss minimization ieee transactions pattern analysis machine intelligence vol. yang zhang feng zhang fisher discrimination dictionary learning sparse representation international conference computer vision. julesz textons pham raich fern arriaga multi-instance multilabel learning presence novel class instances proceedings international conference machine learning pham raich fern dynamic programming instance annotation multi-instance multi-label learning ieee transactions pattern analysis machine intelligence ruiz-muñoz raich fern dictionary extraction collection spectrograms bioacoustics monitoring ieee international workshop machine learning signal processing giannoulis benetos stowell rossignol lagrange plumbley detection classiﬁcation acoustic scenes events ieee aasp challenge ieee workshop applications signal processing audio acoustics ieee jiang davis label consistent k-svd learning discriminative dictionary recognition ieee transactions pattern analysis machine intelligence vol. kong wang dictionary learning approach classiﬁcation separating particularity commonality european conference computer vision. springer jinsub jinsub received ph.d. electrical computer engineering cornell university. september june jinsub postdoctoral associate school cornell university. joined school eecs oregon state university august research interest spans statistical signal processing statistical learning optimization methods applications security smart energy systems. briggs lakshminarayanan neal fern raich hadley hadley betts acoustic classiﬁcation multiple simultaneous bird species multi-instance multi-label approach journal acoustical society america vol. zeyu zeyu received b.s. electronics information technology huazhong university science tech m.s. electrical computer engineering oregon state university currently ph.d. candidate electrical computer engineering oregon state university corvallis current research interests include motif discovering analysis dictionary learning multi-instance multi-label learning applications bioacoustics. raviv raich raviv raich received b.sc. m.sc. degrees aviv university tel-aviv israel respectively ph.d. degree georgia institute technology atlanta electrical engineering. researcher communications team industrial research ltd. wellington zealand. postdoctoral fellow university michigan arbor. raich school electrical engineering computer science oregon state university corvallis assistant professor currently associate professor research interests statistical signal processing machine learning. raich served associate editor ieee transactions signal processing currently serves chair machine learning signal processing technical committee ieee signal processing society. xiaoli fern xiaoli fern received ph.d. degree computer engineering purdue university west lafayette m.s. degree shanghai jiao tong university shanghai china since school electrical engineering computer science oregon state university corvallis currently associate professor. received career award fern currently serving action editor machine learning journal regularly serves program committee top-tier international conferences machine learning data mining including nips icml ecml aaai icdm siam sdm. general research interest areas machine learning data mining. forward message passing update rule formulated marginalizing previous state variables current time instance rely v-structure chain structure fig. update step given current time joint state node observed node independent previous joint state node current time instance ¯nn) combining applying deﬁnition forgiven current time joint state node observed node independent previous joint state node current time instance ¯nn) combining apply conditional rule |xn; ¯nn) ¯nn)p c|xn; bi). time instance label known observed state node independent observed signal parameter ¯nn) ¯nn). since ¯nn) obtained marginalizing", "year": "2018"}