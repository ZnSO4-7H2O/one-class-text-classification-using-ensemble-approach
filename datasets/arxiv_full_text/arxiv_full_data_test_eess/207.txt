{"title": "Variational Autoencoders for Learning Latent Representations of Speech  Emotion: A Preliminary Study", "tag": "eess", "abstract": " Learning the latent representation of data in unsupervised fashion is a very interesting process that provides relevant features for enhancing the performance of a classifier. For speech emotion recognition tasks, generating effective features is crucial. Currently, handcrafted features are mostly used for speech emotion recognition, however, features learned automatically using deep learning have shown strong success in many problems, especially in image processing. In particular, deep generative models such as Variational Autoencoders (VAEs) have gained enormous success for generating features for natural images. Inspired by this, we propose VAEs for deriving the latent representation of speech signals and use this representation to classify emotions. To the best of our knowledge, we are the first to propose VAEs for speech emotion classification. Evaluations on the IEMOCAP dataset demonstrate that features learned by VAEs can produce state-of-the-art results for speech emotion classification. ", "text": "tempts model natural images using generative models research conducted learning latent representations speech generation conversion speaker identiﬁcation importantly feasibility vaes speech emotion recognition largely unexplored. paper conduct preliminary study understand feasibility learning latent representation speech emotion. also investigate performance variant known conditional variational autoencoder learning latent representation speech emotion. objectively measure performance latent representation long short term memory classify speech emotion using latent representation features. simultaneously offers opportunity validate performance learning latent representation delivers vae-lstm classiﬁcation framework. given autoencoders widely used speech emotion implement ae-lstm model compare classiﬁcation performance vae-lstm. also compare classiﬁcation performance vae-lstm recent results literature. comparisons show latent representation learned variant cvae help achieve stateof-the-art speech emotion classiﬁcation performance. autoencoders extensively used emotion recognition however date variational autoencoders mainly used natural image generation vaes speech processing recognition limited. speech audio domain vaes mainly used speech generation transformation also used learn phonetic content speaker identity speech segments without supervisory data moreover framework based used learn frame-level utterance-level robust representations. authors used salient features along speech features robust speech recognition. proposed based framework modelling spectral conversion unaligned corpora. study encoder learned phonetic representation speaker decoder reconstructed designated speaker removing demand parallel corpora model training spectral conversion. finally blaauw used fully-connected model frame-level spectral envelopes speech signal. based experiments authors found achieve similar comparatively better reconstruction erlearning latent representation data unsupervised fashion interesting process provides relevant features enhancing performance classiﬁer. speech emotion recognition tasks generating effective features crucial. currently handcrafted features mostly used speech emotion recognition however features learned automatically using deep learning shown strong success many problems especially image processing. particular deep generative models variational autoencoders gained enormous success generating features natural images. inspired this propose vaes deriving latent representation speech signals representation classify emotions. best knowledge ﬁrst propose vaes speech emotion classiﬁcation. evaluations iemocap dataset demonstrate features learned vaes produce state-of-the-art results speech emotion classiﬁcation. index terms speech emotion classiﬁcation variational autoencoders deep learning feature learning recently speech emotion recognition received signiﬁcant attention industry academia. various applications human-computer interaction analysis humanhuman interactions. speech signal complex distributions high variance various factors speaking style gender linguistic content environmental channel effects emotional state. understanding inﬂuence factors speech signal crucial problem speech emotion recognition. although considerable attempts focused handcrafting features capture factors automatic learning features sensitive emotion needs exploration. deep generative models recently becoming immensely popular deep learning community fact unlike discriminative approaches learn true distribution training data generate data points paper focused generating data capitalising capacity generative models learn true distribution data hence create powerful features automatically. commonly used efﬁcient generative models currently generative adversarial nets variational autoencoders gans optimised generative tasks vaes probabilistic graphical models optimised latent modelling. therefore focus vaes. many atmany researchers used lstms speech emotion recognition many scenarios lstms effective conventionally-employed support vector machines researchers also used lstm networks iemocap speech corpus shown perform better powerful methods like hidden markov models chernykh used connectionist temporal classiﬁcation loss function lstm networks emotion classiﬁcation evaluated iemocap dataset. worth mentioning work emily also employed iemocap database speech emotion recognition. however authors used transfer learning leverage information another database improve speech emotion accuracy. transfer learning scope paper future would investigate transfer learning enhance accuracy achieved approach. variational autoencoder combination graphical models neural networks. similar structure autoencoder functions differently. learns compressed representation input reconstructs input compressed representation. hand learns parameters probability distribution representing input latent space. done making latent distribution close possible prior latent variable. advantages prior allows injection domain knowledge enabling estimation uncertainty prediction making suitable speech emotion recognition. however quantities unknown. idea infer using determined using variational inference inferred upon minimising divergence known distribution becomes seen eventually reduce reconstruction error train encoder produces parameters probability distribution latent space based known distribution choice. minimise divergence example assume latent space normal distribution need train encoder generate mean covariance. samples generated using parameters decoder generate approximation conditional variational autoencoder conventional generate speciﬁc data example picture elephant user inputs elephant image. models latent variable image directly. eliminate problem conditional variational autoencoder models latent variables emotion data conditioned random variables encoder therefore conditioned variables decoder also conditioned variables many possibilities conditional variable could categorical distribution expressing label even could distribution data. despite capabilities particularly interested generating speech emotion however distance original generated emotion becomes smaller predeﬁned threshold parameters probability distribution features emotion imposing conditions simply concatenate speech frame representation logmel particular emotion emotion class label pass encoder. lstm model long range contexts presence special structure called memory cell. emotions speech context-dependent therefore ability model contextual information makes lstm suitable speech emotion recognition lstm memory cell built memory block constitutes hidden layers lstm. three gate units memory cell input output forget gate used perform reading writing resetting information respectively. feature representations input lstm input gate enables memory block selectively control incoming information store internal memory. output gate decides part information output forget gate selectively clears speech emotional contents memory cell. lstm emotion classiﬁcation output vector projected onto vector length number emotion classes. projection done using simple functions lstm output vector rm×n weight vector vector length number classes vector mapped onto probability vector values probabilities equates highest probability indicates identiﬁed class. overall classiﬁcation framework shown figure previous studies concluded performance lstm model enhanced using predictive knowledge-inspired features despite limited training examples therefore lstm natural choice features generated vaes. experimentation selected interactive emotional dyadic motion capture dataset widely used speech emotion recognition. iemocap multimodal corpus containing recordings actors sessions. session contains female male speaker. data includes types dialogues scripted non-scripted. non-scripted dialogue speakers instructed without pre-written scripts. scripted dialogue data actors followed pre-written script. annotation values various parameters used adam optimiser follows learning rate values chosen iterative manner obtain minimum reconstruction loss autoencoder networks. used reparameterization trick approximate latent space normally distributed setting denotes element-wise multiplication cvae conditioned categorical emotion labels. benchmark performance also used conventional autoencoder architecture except gaussian layer replaced fully connected layer. lstm model consisted consecutive lstm layers activation hyperbolic tangent. hidden states second lstm layer connected dense layer outputs dense layer softmax layer classiﬁcation categorical dimensional class labels. network parameters chosen cross-validation experiments. common setup used adam optimiser default learning rate following avoid overﬁtting used early stopping criteria maximum number epochs equal experiments performed using nvidia quadro memory. latent representations generated vaes input lstm network classiﬁcation. segmentlevel latent representations obtained autoencoder networks merged whole utterance-level features classiﬁcation emotions iemocap corpus split training testing data investigated performance model training speaker-independent manner. also allowed compare results previous studies. adopted leave-onesession-out cross-validation approach evaluated models weighted accuracy unweighted accuracy categorical dimensions. dimensional annotations followed evaluations strategies able compare studies. report f-measures scores test dataset. models trained using data testing performed remaining unseen data. table shows ﬁve-fold classiﬁcation results different subsets iemocap data. noted features learned produces better classiﬁcation performance compared conventional autoencoder. representations learned cvae highly predictive outperform learned vae. performed assessors based video audio streams. utterance annotated using categories neutral happiness sadness anger surprise fear disgust frustration excited other. better compare results related work computed results improvised scripted complete data considered four emotions neutral happiness sadness anger combining happiness excited emotion following state-of-the-art studies corpus iemocap data also annotated three continuous dimensions arousal power valence comparison classiﬁcation results state-of-the-art approaches also consider emotion dimensions. however maintain classiﬁcation problem like within dimension created three categories high consider logmel speech frame representation used following studies hamming window length frame-shift applied speech signal discrete fourier transform coefﬁcients computed. computed mel-frequency ﬁlterbanks. feature formulated taking logarithmic power mel-frequency band energy. input speech segments length latent representation data. speech segment features represented latent space used encoding layers hidden units respectively. number hidden units chosen based intuition prior work autoencoders speech recognition using vaes used adam optimiser stochastic optimisation algorithm widely used update network weights iteratively based training data figure results using different number latent features categorical dimensional annotations. figure shows effect different number features categorical classiﬁcation accuracy presents corresponding trend mean score dimensional annotation. model using bidirectional-lstm model achieved accuracy. authors used low-level acoustic features mfcc along derivatives feature model. authors used different types features evaluated single view well multi-view attentive iemocap data using four emotions mention best results table. chernykh used three different type features report accuracy using blstm. using cvae derived features achieve accuracy competitive respect literature. table presents -fold cross-validation results dimensional annotation using iemocap data mean represents arithmetic mean three emotional dimensions arousal power valence results calculated basis classifying three subcategories high within emotion dimension. compare performance proposed methods autoencoder model also recent studies literature. vae-lstm cvae-lstm signiﬁcantly outperform ae-lstm model cvae-lstm producing best performance. studies compared table used different types features knowledge-inspired disﬂuency nonverbal vocalization features statistical low-level descriptor features input lstm model. highest score achieve closely outperform using proposed cvae-lstm model results reported above used latent space size essentially means used mean variances normal distribution latent features. however also investigate impact higher lower number latent features. figure show trend results using different number latent features categorical dimensional emotions respectively. across vaes small number features perform poorly. however large number features produce best performance well. within lower higher bound insigniﬁcant improvement observed increase number features. based results conclude suitable number latent features needs determined empirically avoid selecting small large number features. paper demonstrate vaes effectively learn latent representation speech emotion offers great potential learning powerful features automatically. show helps achieve high classiﬁcation accuracy combined classiﬁer natural choice lstm lstm intrinsic capacity model contextual information like speech emotion also lstm model enhanced using predictive knowledge-inspired features. analyse categorical dimensional emotions comparing emotion classiﬁcation results widely used ae-lstm model show vaes offer great promise producing state-of-the-art results. also analyse impact number latent features classiﬁcation accuracy view determining optimal number features. however conclude suitable number features needs determined empirically. overall preliminary results presented paper demonstrate highly feasible automatically learn features speech emotion classiﬁcation using deep learning techniques potentially motivate researchers innovate space. w¨ollmer metallinou katsamanis schuller narayanan analyzing memory blstm neural networks enhanced emotion classiﬁcation dyadic spoken interactions acoustics speech signal processing ieee international conference ieee w¨ollmer metallinou eyben schuller narayanan context-sensitive multimodal emotion recognition speech facial expression using bidirectional lstm modeling eleventh annual conference international speech communication association chao yang long short term memory recurrent neural network based multimodal dimensional emotion recognition proceedings international workshop audio/visual emotion challenge. neumann attentive convolutional neural network based speech emotion recognition study impact input features signal length acted speech arxiv preprint arxiv. busso bulut c.-c. kazemzadeh mower chang narayanan iemocap interactive emotional dyadic motion capture database language resources evaluation vol. abdel-hamid a.-r. mohamed jiang deng penn convolutional neural networks speech recognition ieee/acm transactions audio speech language processing vol. tashev speech emotion recognition using deep neural network extreme learning machine fifteenth annual conference international speech communication association eyben scherer schuller sundberg andr´e busso devillers epps laukka narayanan geneva minimalistic acoustic parameter voice research affective computing ieee transactions affective computing vol. denton chintala fergus deep generative image models using laplacian pyramid adversarial networks advances neural information processing systems c.-c. h.-t. hwang y.-c. tsao h.-m. wang voice conversion non-parallel corpora using variational auto-encoder signal information processing association annual summit conference asia-paciﬁc. ieee deng zhang marchi schuller sparse autoencoder-based feature transfer learning speech emotion recognition affective computing intelligent interaction humaine association conference ieee deng zhang fr¨uhholz schuller semisupervised autoencoders speech emotion recognition ieee/acm transactions audio speech language processing vol. learning utterance-level normalisation using variational autoencoders robust automatic speech recognition spoken language technology workshop ieee.", "year": "2017"}