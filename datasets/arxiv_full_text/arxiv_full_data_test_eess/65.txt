{"title": "Understanding the visual speech signal", "tag": "eess", "abstract": " For machines to lipread, or understand speech from lip movement, they decode lip-motions (known as visemes) into the spoken sounds. We investigate the visual speech channel to further our understanding of visemes. This has applications beyond machine lipreading; speech therapists, animators, and psychologists can benefit from this work. We explain the influence of speaker individuality, and demonstrate how one can use visemes to boost lipreading. ", "text": "machines lipread understand speech movement decode lip-motions spoken sounds. investigate visual speech channel understanding visemes. applications beyond machine lipreading; speech therapists animators psychologists beneﬁt work. explain inﬂuence speaker individuality demonstrate visemes boost lipreading. machine lipreading speech recognition withaudio input e.g. silent video. research interest computer vision engineers speech researchers. current complimentary challenges are; develop end-to system understand visual speech signal apply knowledge domains speech therapy animation. work addresses latter challenge. phonemes smallest sounds make viseme visual equivalent current knowledge visemes limited proven function visemes phonemes. work focuses understanding visemes order recognise right phoneme. conventional lipreading machines conventional lipreading process high level adopted audio recognition systems. track faces extract features train model classify ﬁlter output language network. debates optimal tracking methods features classiﬁer method remain pre-deep learning classic choices accurate results active appearance model features hidden markov model classiﬁers e.g. data begin play-off measure effect using different phoneme-to-viseme maps prior work. tested conventional system talkers. results displayed figure heatmap consonant x-axis vowel y-axis. combination disney vowels woodward consonants perform best. contrasts concluded lee’s visemes achieved accurate lipreading isolated words suggests utterance duration affects visemes. figure critical difference plots maps. critical difference measure conﬁdence intervals between different algorithms overlapping bars join maps critically different. comparing figures consonant visemes vary less vowel sets. observation supported lipreading practitioners advocate shapes articulator sounds gestures formed motion shapes motions determined consonants. erarchical training method used viseme classiﬁers initialisation models phoneme classiﬁers viseme sizes. talker mean results figure phoneme hmms initialised visemes achieve higher accuracy. also tested language network unit. figure show phoneme network better word network. however using phoneme network means ﬁnal output phoneme string requires processing understand effect signiﬁcant. comparison previous mappings little difference disney’s outperforms others continuous speech lee’s marginally outperforms others isolated words. means visemes vary speaker utterance. suggest speaker individuality visual speech variability different people visual gestures whilst talking. speaker-dependent recognition choices selecting visemes containing fewer classes phoneme sets outperform phoneme labelled classiﬁers. phoneme classiﬁers desirable cross-speaker consistent mapping similarities visemes right visemes out-perform phoneme classiﬁers used help train phoneme classiﬁers lipread signiﬁcantly better also. best results achieved units match classiﬁers language network signiﬁcantly purposes decoding phonemes words spoken preferred network unit words end-to-end systems perform well data deep learning still fully understand visual speech signal. understanding visual speech mean improve adaptation talkers future. results show speaker-dependent visemes improve lipreading accuracy. figure conclusion reinforced equivalent experiments continuous speech talkers. plots show visemes blue plots multi-speaker visemes orange speakerindependent visemes. speaker independence ability lipread previously unseen talkers obstacle lipreading machines. learn limitation useful visemes within towards recognition accuracy. badly trained viseme worse viseme. however visemes visemes increase accuracy. whilst training data detrimental classiﬁcation less right knowledge visual gestures need data reduced accurate lipreading. present experiment showed viseme sets visemes negatively affected homophone confusions. sets large differentiate sufﬁciently accurate lipreading. means range optimum sizes varies talker. this designed b.-j. theobald harvey e.-j. bowden. improving visual features lip-reading. proceedings international conference audio-visual speech processing yook. audio-to-visual conversion using hidden markov models. proceedings paciﬁc international conference artiﬁcial intelligence pages springer", "year": "2017"}