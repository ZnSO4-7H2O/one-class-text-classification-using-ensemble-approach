{"title": "Voice Impersonation using Generative Adversarial Networks", "tag": "eess", "abstract": " Voice impersonation is not the same as voice transformation, although the latter is an essential element of it. In voice impersonation, the resultant voice must convincingly convey the impression of having been naturally produced by the target speaker, mimicking not only the pitch and other perceivable signal qualities, but also the style of the target speaker. In this paper, we propose a novel neural network based speech quality- and style- mimicry framework for the synthesis of impersonated voices. The framework is built upon a fast and accurate generative adversarial network model. Given spectrographic representations of source and target speakers' voices, the model learns to mimic the target speaker's voice quality and style, regardless of the linguistic content of either's voice, generating a synthetic spectrogram from which the time domain signal is reconstructed using the Griffin-Lim method. In effect, this model reframes the well-known problem of style-transfer for images as the problem of style-transfer for speech signals, while intrinsically addressing the problem of durational variability of speech sounds. Experiments demonstrate that the model can generate extremely convincing samples of impersonated speech. It is even able to impersonate voices across different genders effectively. Results are qualitatively evaluated using standard procedures for evaluating synthesized voices. ", "text": "voice impersonation voice transformation although latter essential element voice impersonation resultant voice must convincingly convey impression naturally produced target speaker mimicking pitch perceivable signal qualities also style target speaker. paper propose novel neuralnetwork based speech qualitystyle-mimicry framework synthesis impersonated voices. framework built upon fast accurate generative adversarial network model. given spectrographic representations source target speakers’ voices model learns mimic target speaker’s voice quality style regardless linguistic content either’s voice generating synthetic spectrogram time-domain signal reconstructed using grifﬁn-lim method. effect model reframes well-known problem style-transfer images problem style-transfer speech signals intrinsically addressing problem durational variability speech sounds. experiments demonstrate model generate extremely convincing samples impersonated speech. even able impersonate voices across different genders effectively. results qualitatively evaluated using standard procedures evaluating synthesized voices. voice impersonation person attempts mimic voice another sound like person complex phenomenon. often performed impersonator intuitively trying match prominent aspects voice person mimicked. common sets characteristics voice varied impersonator process elements voice quality elements style. voice quality however hard-to-describe entity. aspects nasality roughness breathiness etc. identiﬁed literature quantiﬁable manner permits meaningful comparison across speakers. elements style include temporal pitch energy patterns characteristic target speaker’s cadence speaking rate prosody etc. similarly ambiguous terms exact deﬁnition quantiﬁability comparability. addition quality style idiosyncrasies language variants grammar choice vocabulary usage words etc. also mimicked impersonator create impression target speaker’s voice; however currently focus paper. paper attempt address problem automatic generation impersonations transferring stylistic content speech target speaker impersonator. objective convert speciﬁc aspects voice without modifying content. ideally would able identify isolate explicitly measure aspects voice characterize stylistic features modify target voice. however generally difﬁcult features subjectively even objectively identiﬁable quantiﬁable. misquote hon. potter stewart know describe them know occur. objective develop mechanism transfer identiﬁable aspects style. absence mechanisms isolate quantify measure these generative approaches source-ﬁlter models within explicit components source ﬁlter modiﬁed spectral-transformation models psola etc. become inappropriate. instead must alternate approach ability transform voice must learned solely identiﬁability target characteristics. prior research greatest relevance context relates voice transformation deals speciﬁc problem converting source voice target one. voice transformation long history surface addresses issues mention. conventionally voice transformation modiﬁes instantaneous characteristics source signal pitch spectral envelope. strategies used range simple codebookbased conversion minimum-mean-squared error linear estimators sophisticated neural network models methods frequently quite effective transforming instantaneous characteristics signal even prosodic cues generally insufﬁcient capture unmeasurable unquantiﬁable style general sense word. trained heavily reliant availability parallel recordings source target speaker saying utterances providing exact examples considered ideal conversion. cases order learn voice conversion effectively recordings must also perfectly time aligned requirement generally satisﬁed time-warping recordings align another. realization hard targets required learn conversion unrealistic alignment required also fundamentally inappropriate objective learn perform wholesale conversion voice transform style. context recent advances science learning generative models provide directions. rather attempting learn mapping parallel signals models attempt discriminate instead data desired stylistic feature not. generators attempt produce data speciﬁc characteristic must learn fool discriminator. since features unquantiﬁable discriminator must fact also learned. generators discriminators modeled deep neural networks known able model transformation appropriate design sufﬁcient training data. since primary driver learning process discrimination parallel data needed conventional voice-conversion methods required. generative adversarial networks gans successfully applied variety problems image generation learning feature representations style transfer wherein algorithms involved result fast vivid generation images different artistic styles ranging simple photographs painting styles selected artists. work harness power models problem style transfer speech. outset note speech signals several problems inherent images. unlike images speech sounds ﬁxed size lose much stylistic characteristics scaled generation time-series data speech also challenging problem compared images. naive implementations process result generation data lost linguistic stylistic even intelligible content. work propose multiple models problem voice transformation. models corresponding learning algorithms designed consider speciﬁc challenges inherent speech. speciﬁcally show appropriate choice model structure learning algorithm introducing appropriate discriminators framework speciﬁc characteristics voice might retained without modifying others losing linguistic content order emulate different aspects impersonation voice mimicry. section brieﬂy outline concept gans. section describe designs gans voice modiﬁcations. section present experimental evaluations proposed models conclude discussions section generative adversarial network generative model which foundation generative model data variable. model intended generate samples closely match draws actual distribution data. models differ conventional generative models fundamental manner learned. conventional generative models trained likelihood maximization criteria divergence measure synthetic distribution encoded generative models true distribution data minimized. contrast gans trained discriminatively samples generated model cannot distinguished actual draws true distribution data. consider random variable probability distribution unknown samples drawn. instance represent images particular class samples readily available actual distribution unknown. attempts generate samples indistinguishable actual samples drawn true distribution. original model comprises generator discriminator generator takes input random variable drawn standard probability distribution function e.g. standard normal distribution produces output vector discriminator attempts discriminate samples drawn true distribution model samples produced generator represent event vector drawn discriminator attempts compute posteriori probability i.e. train attempt learn score output discriminator response productions maximized time attempt learn minimized also maximizing objectives concurrently achieved following optimization basic extended number ways literature particularly context style transfer among images e.g. figure common underlying denominator models input data instance drawn distribution transformed instance generator gab. transformer convert style variable natively occurs distribution discriminator attempts distinguish genuine draws instances obtained transforming draws actual optimization achieved follows. deﬁne exa∼pa −exb∼pb exa∼pa train components alternately updated minimizing losses equation generator updated minimizing generator loss discriminator updated minimize discriminator loss work however directly based discogan model shown figure discogan symmetric model attempts transform categories data other. discogan includes generators gba. attempts transform draw distribution indistinguishable draws distribution reverse. since objective modify speciﬁc aspects speech e.g. style must extra components model achieve this. call model incorporates modiﬁcations voicegan. here term attempts retain structure even converted xab. careful choice ensures both accurate reconversion retention linguistic information conversion xab. fig. visualization generator discriminator architectures counterparts similar structure. number convolutional layers larger actual implementation. account fact unlike images speech signals variable length cannot scaled down must make modiﬁcations generators discriminators. modiﬁed structures shown figure figure shows structure original generator discogan. based fully convolutional structure handle variable length inputs. figure shows architectural details proposed discriminator voicegan. this adaptive pooling layer added layers fully connected layer. includes channel-wise pooling channel’s feature pooled single element. converts variable-sized feature vector ﬁxed number dimensions many components number channels. addition discriminator distinguishes generated data real data second type discriminator model extract target style information input data make sure generated data still style information embedded achieve this include discriminator similar architecture figure also includes discriminators attempts discriminate actual draws draws transformed performs analogous operations draws generators discriminators must jointly trained. training process discogan similar model figure signiﬁcant modiﬁcation addition losses emphasize competition generators discriminators include requirement must inverses extent possible i.e. xaba gba) must close original similarly xbab gab) must close original requirement encoded reconstruction losses lcon lcon deﬁned equation deﬁne generator loss symmetric manner. overall generator loss lganab lganba. discriminator loss deﬁned deﬁned equation finally implementation discogan feature loss also added compare feature similarity generated data real data. before generators discriminators trained alternate minimization generator discriminator losses. discogan originally designed transform style images. order apply model speech ﬁrst convert invertible picture-like representation namely spectrogram. operate primarily magnitude spectrogram retaining phase input signals transformed recreate transformed signals transformed magnitude spectrogram. ﬁrst must make several modiﬁcations discogan model. original discogan designed operate images ﬁxed size. work inherently variable-sized speech signals constraint must relaxed design. secondly important ensure linguistic information speech signal lost even signal modiﬁed. sufﬁcient constraints must added model this. finally tidigits dataset. dataset comprises total speakers women boys girls. speaker reads digit sentences. sampling rate audio chose database relatively simple linguistic content. purpose demonstration choose unquantiﬁable identiﬁable characteristic gender. goal show data used learn convert gender speaker’s voice. discussion below therefore style refers gender. note characteristic similarly chosen. model architecture voicegan described above. generator network model comprises -layer encoder -layer transposed decoder. discriminator network comprises -layer adaptive pooling. employ batch normalization leaky relu activations networks. number ﬁlters layer increasing power training networks smoothness constraint comprising cumulative ﬁrst order difference adjacent columns spectrogram added loss enhance temporal continuity generated spectrogram. results available evaluate quality generated speech signal also conduct signal-to-noise ratio test using standard nist stnr method wada method results shown table data class randomly select samples test dataset compute mean variance generated results. wada test results around since generated noise well-modeled gaussian noise. stnr test results show generated data good quality. evaluation time-domain signal reconstructed generated spectrogram using grifﬁn-lim method based iterative procedure minimizes mean square error modiﬁed magnitude spectrogram actual signals spectrogram. details method explained grifﬁn-lim method reduce voice quality signiﬁcant degree. voicegan model observably able transfer style speaker another. proposed however model remains vanilla many extensions possible. method easily extended stylistic features identiﬁed. principle longer-term prosodic-level style features also transferred simple binary discriminators longer useful characteristics. continuous-valued discrimination required. veriﬁed multiple style aspects concurrently modiﬁed. remain areas ongoing research. preliminary experiments veriﬁed even linguistic content modiﬁed choose; however measurable controlled manner challenge remains addressed. future versions voicegan model continue incorporate relevant innovations area adversarial modeling. ming keith jellyman john mason nicholas evans assessment objective quality meaacoustics sures speech intelligibility estimation speech signal processing icassp proceedings. ieee international conference ieee vol. i–i. chanwoo richard stern robust signal-to-noise ratio estimation based waveform amplitude distribution ninth annual conference international analysis speech communication association daniel erro navas inma hernaez parametric voice conversion based bilinear frequency warping plus amplitude scaling ieee transactions audio speech language processing vol. tomoki toda alan black keiichi tokuda voice conversion based maximum-likelihood estimation spectral parameter trajectory ieee transactions audio speech language processing vol. ling-hui chen zhen-hua ling li-juan li-rong voice conversion using deep neural networks layerwise generative training ieee/acm transactions audio speech language processing vol. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio generative adversarial nets advances neural information processing systems alec radford luke metz soumith chintala unsupervised representation learning deep convolutional generative adversarial networks arxiv preprint arxiv. jun-yan taesung park phillip isola alexei efros unpaired image-to-image translation using cycle-consistent adversarial networks arxiv preprint arxiv. ahmed elgammal bingchen mohamed elhoseiny marian mazzone creative adversarial networks generating learning styles deviating style norms arxiv preprint arxiv. leon gatys alexander ecker matthias bethge image style transfer using convolutional neural networks proceedings ieee conference computer vision pattern recognition sergey ioffe christian szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning", "year": "2018"}