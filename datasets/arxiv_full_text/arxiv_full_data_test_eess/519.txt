{"title": "Predicting Natural Hazards with Neuronal Networks", "tag": "eess", "abstract": " Gravitational mass flows, such as avalanches, debris flows and rockfalls are common events in alpine regions with high impact on transport routes. Within the last few decades, hazard zone maps have been developed to systematically approach this threat. These maps mark vulnerable zones in habitable areas to allow effective planning of hazard mitigation measures and development of settlements. Hazard zone maps have shown to be an effective tool to reduce fatalities during extreme events. They are created in a complex process, based on experience, empirical models, physical simulations and historical data. The generation of such maps is therefore expensive and limited to crucially important regions, e.g. permanently inhabited areas.  In this work we interpret the task of hazard zone mapping as a classification problem. Every point in a specific area has to be classified according to its vulnerability. On a regional scale this leads to a segmentation problem, where the total area has to be divided in the respective hazard zones. The recent developments in artificial intelligence, namely convolutional neuronal networks, have led to major improvement in a very similar task, image classification and semantic segmentation, i.e. computer vision. We use a convolutional neuronal network to identify terrain formations with the potential for catastrophic snow avalanches and label points in their reach as vulnerable. Repeating this procedure for all points allows us to generate an artificial hazard zone map. We demonstrate that the approach is feasible and promising based on the hazard zone map of the Tirolean Oberland. However, more training data and further improvement of the method is required before such techniques can be applied reliably. ", "text": "gravitational mass ﬂows avalanches debris ﬂows rockfalls common events alpine regions high impact transport routes. within last decades hazard zone maps developed systematically approach threat. maps mark vulnerable zones habitable areas allow effective planning hazard mitigation measures development settlements. hazard zone maps shown effective tool reduce fatalities extreme events. created complex process based experience empirical models physical simulations historical data. generation maps therefore expensive limited crucially important regions e.g. permanently inhabited areas. work interpret task hazard zone mapping classiﬁcation problem. every point speciﬁc area classiﬁed according vulnerability. regional scale leads segmentation problem total area divided respective hazard zones. recent developments artiﬁcial intelligence namely convolutional neuronal networks major improvement similar task image classiﬁcation semantic segmentation i.e. computer vision. convolutional neuronal network identify terrain formations potential catastrophic snow avalanches label points reach vulnerable. repeating procedure points allows generate artiﬁcial hazard zone map. demonstrate approach feasible promising based hazard zone tirolean oberland. however training data improvement method required techniques applied reliably. hazard zone maps natural hazards particularly gravitational mass ﬂows constant threats settlement infrastructure alpine regions. beside fatalities destroyed buildings events lead blocked destroyed transport routes. mitigate impact natural hazards transport routes settlement areas protected various artiﬁcial barriers dams avalanche galleries snow fences. essential basis planning protection measures settlement development hazard zone maps. introduced austria south tirol austria maps mark every point habitable areas vulnerable highly vulnerable categories colour codes slightly different countries principle same. additional colour mark areas identiﬁed safe figure uncoloured areas subject detailed analysis show terrain hazard zone maps developed complex expensive process. therefore many areas detailed analysis missing. although understanding mitigation strategies improved signiﬁcantly within last decades still struggle handle natural disasters regularly. continuously reminded fact disasters like snow avalanche farindola debris puster valley name events year highlights demand improved models procedures improve predictions reduce uncertainties extend investigated area. artiﬁcial intelligence neuronal networks neuronal networks common approaches artiﬁcial intelligence machine learning. technique applied task complex develop algorithm solution. principal idea develop application learns solve problem own. often simpler solving problem directly. approach shown outstanding performance notoriously difﬁcult tasks image speech recognition work explore possibilities neuronal networks management natural hazards. neuronal networks used develop temporal spatial models learned autonomously historical data fact approach makes possible process historical data human expertise make suggestions future decisions. words neuronal networks learn past catastrophes experienced engineers. point view neuronal network statistical-empirical model similar well established αβ-model both α-β-model neuronal network processing topographic features. however neuronal network extract features terrain data without human help. advantage approach choose speciﬁc terrain features beforehand network chooses based statistical considerations. allows neuronal network work mostly autonomously. obviously human supervision goal reduce human effort essential tasks. hope neuronal networks help improve natural hazard mitigation therefore safety standards alpine regions. also neuronal networks reduce costs make detailed studies hazard zone maps available regions permanently inhabited areas attempts process geographical data like hazard zones neuronal networks. groups performed landslide susceptibility assessments neuronal networks encouraging results. however approaches directly comparable network important features slope inclination curvature extracted manually elevation maps. comparable study geographical scale outstanding results shown isola automatically transforming satellite images street maps neuronal networks. idea similar transforming terrain maps hazard zone maps. work focus snow avalanches. snow avalanches extremely complex notoriously difﬁcult predict. high demand improved models. moreover focus learning generating hazard zone maps. hazard zone maps change time contrast momentary reports like avalanche bulletin. therefore focus recognition spatial features decisive catastrophic snow avalanches. train neuronal networks needs examples input data corresponding output data. usually humans create databases examples allow neuronal network learn. terrain data combination ofﬁcial hazard zone maps human generated training data purpose. using maps neuronal network learn experience history inputs used hazard zone maps. expect approach also applicable natural hazards ﬂoods debris ﬂows. although required adapt chosen network architecture input data speciﬁc problem work blueprint hazards. paper organised follows section outline state neuronal section present implementation specialised predicting snow networks. avalanches. section describe training phase show results training validation samples. finally summarise work section highlight plans future. origin neuronal networks traced back early work mcculloc pits ﬁrst introduced mathematical model biological neuron. neuron stimulated dendrites signal processed stimulus passed axon dendrites neurons mcculloc-pits-cell mathematical description process. processing signal implemented different ways. standard implementation consists linear weighting adding bias nonlinear activation function. mechanism illustrated figure also expressed simple function output neuron input. activation function usually rectiﬁed linear unit sigmoid function. function introduces non-linearity network increasing complexity behaviour. weights determine stimulus cell reacts bias acts threshold overcome cell emits stimulus own. weights bias mutable allow cell change behaviour learning phase. substantially. example deep network shown figure example consists input nodes processed hidden layers neurons readout layer three neurons. network connects input features three output features. output usually probability input features match certain category. neurons layer connected neurons following layer. architecture called densely connected network. figure example simple neuronal network input values three output values. circle represents neuron including weights bias activation function arrows represent stimuli. vectors neuron inputs outputs weight tensor bias vector. full network consists nested layers thus create complex non-linear function model expanded ﬁrst layer network figure contains neurons inputs neuron. therefore mathematical description requires weights biases layer. whole network contains weights biases summarised network parameters bi}. network parameters represent networks degree freedom. high degree freedom means model describe complex processes learning result network depends parameters. neuronal network worthless without appropriate parameters. finding optimal parameters speciﬁc network speciﬁc task called learning training. stick simplest training method called supervised learning train network need three additional components training data contains examples inoutput pairs cost loss function describing ﬁtness network training data optimiser modiﬁes parameters minimise cost function. based input vector step called forward pass. goal minimise difference correct answer networks answer pairs difference vectors deﬁned cost function. many deﬁnitions function common called cross entropy deﬁned single data pair parameters mean loss loss function network function steady differentiable respect special kind functions gradient descent optimiser well suited. methods work differentiating loss respect parameters. resulting gradient points towards steepest growth loss. therefore stepping opposite direction gradient leads reduction loss towards optimum parameters. step optimiser written gradient loss respect parameters learning rate chosen carefully developer. gradient efﬁciently calculated applying chain rule starting network. step called backward propagation. complete learning step consists forward propagation yielding loss followed backward propagation yielding improved parameter set. classic gradient descent optimiser improved signiﬁcantly last decade leading statistical gradient descent momentum methods adagrad rmsprop current state adam method optimisation neuronal networks ill-posed problem meaning unique solution solution changes drastically minimal variation input. latter revealed overﬁtting network training data. case network generalise data seen training. major problem solved last decade powerful efﬁcient regularisation methods dropout however also best methods limits since sufﬁcient data identify statistically signiﬁcant coherence. usually part available training data used training reserved validation ﬁnal network. identify overﬁtting estimate realistic performance. convolutional neuronal networks rapid progress recent years attributed availability cheap fast hardware development specialised network architectures especially convolutional neuronal networks. context specialisation means create copies neurons multiple times speciﬁc layout share weights. increases complexity network without increasing degree freedom thus learning difﬁculties. convolutional neuronal networks inspired visual cortex mammalians. networks consider spatial relations also include spatial invariance. makes well suited spatial problems major tasks computer vision image recognition segmentation. major attribution given hubel wiesel experiments visual cortex. discovered hierarchical structure visual cortex consisting simple complex hypercomplex cells limited receptive ﬁeld cell. simple cells ﬁrst hierarchical layer able recognise edges different orientations image. information detected edges processed complex cells recognise primitive shapes like circles corners. hierarchy continues complex objects recognised upper layers. large amount simple cells mammalian brain. many identify edge different area image. words many cells identical different receptive ﬁeld. terms mcculloc-pits model means many neurons share weights bias linked different input cells. leads mathematical description similar convolution thus name convolutional neuronal network. mathematical description simple convolutional layer used image recognition ﬁlter size weight matrix context called ﬁlter bias. result neuron position input position result convolutional layer called feature since describes position speciﬁc feature found image. although mathematical description complex concept quite simple imagine convolutional layer behave like ﬁlter moving image emitting stimulus ﬁlter matches underlying structure. match structures different types make many layers single hierarchical step creating many feature maps step. going hierarchy consecutive layers ﬁnding structures three dimensional feature maps output feature maps own. examples ﬁlters ﬁrst layer detecting edges different orientation shown figure convolutional layers often followed pooling layers increase information density. layers simply forward maximum stimulus group neurons. finally fully connected layers known simple neuronal networks output probability image match speciﬁc category. output plausible probabilities softmax function applied last layer. simple dimensional convolutional neuronal network shown figure consists amount neurons fully connected network figure contains half parameters last fully connected layer. convolutional network ﬁrst used lecun classify hand written digits reaching error rate year nobody thought approach could transferred large scale image recognition. decisive breakthrough achieved krizhevsky using gpus convolutional neuronal network million parameters. achieved top- error rate top- error rate yearly imagenet competition. groundbreaking time. following competitions evolutions krizhevskys network state cnns consist convolutional layers figure visualisation ﬁlters ﬁrst layer alexnet scanning underlying picture edges different orientations. clearly represented values ﬁlter visualised colours. note ﬁlters learned network preset. figure example simple dimensional convolutional network single hidden convolutional layer depth three. coloured neurons share parameters although amount complexity before number parameters reduced. connections neurons position drawn black grey reduce confusion. feature maps layer millions parameters. networks trained multiple high performance gpus weeks millions training samples reach top- error rates task chose work strongly related semantic segmentation task network mark position form objects images. fact semantic segmentation gave inspiration work. instead marking positions recognised objects images mark vulnerable areas map. indicators input data vulnerability point chose geographic maps surrounding point terrain maximum expected snowfall within period three days although indicators avalanches identiﬁed important ones sufﬁcient proof concept. indicator eliminated exposition slope. seem irritating experienced alpinist. however catastrophic avalanches usually happen long time without sunshine exposition matter. used fact apply rotation invariant convolutional neuronal network. idea already successfully employed dieleman classify galaxies. desired output network hazard zone shown figure train network repeatedly feed input maps enforce network yield hazard zone adapting parameters. rotation invariant neuronal network assembled convolutional neuronal networks different receptive ﬁelds viewports. viewports arranged around point interest looking different direction shown figure results sub-networks combined fully connected network ﬁnal result vulnerability investigated point. describe architecture network yields best results. plenty space optimisation achieved systematically changing called meta-parameters describing architecture. used viewports physical dimension resolution m/pixel meaning network look point investigation. viewports processed four convolutional layers ﬁlter size number feature maps layer process. ﬁrst layer takes viewports terrain outputs eight feature maps second layer takes outputs figure create rotation invariant neuronal network extract viewports forming circle around point interest extracted patches feed different sub-networks. extraction ﬁrst three viewports shown. similar procedure done snow scaled resolution. feature maps third layer outputs fourth snow attached feature maps third layer passed fourth convolutional layer. chose approach information density snow much lower terrain map. fourth convolutional layer followed densely connected layers neurons fully connected layer three neurons. densely connected layers followed dropout layers reduce overﬁtting. output last three neurons ﬁnal result sub-network. sub-networks share weights therefore completely identical. sub-network results merged processed ﬁnal fully connected readout layer yielding probabilities three categories. architecture shown figure overall network contains approximately parameters fully connected layers. neuronal network implemented based googles machine learning toolkit tensorflow efﬁcient implementations neuronal networks. viewport extraction running parallel processed viewports continuously feed neuronal network running gpu. could utilise available hardware efﬁciently. training results training data shown figures vectors) k-vectors). training data unbalanced meaning green zone much bigger yellow zones. case network achieve best result simply predicting green zone time undesired behaviour. address issue randomly subsample mini-batches training data yellow green zones represented equally. maps randomly rotated ﬂipped increase entropy training data set. used adam optimiser exponentially decaying learning rate. training phase picked training validation samples random continuously calculated top- top- accuracy. here top- accuracy deﬁned probability network yield second best result top- accuracy start corresponds guessing. training top- accuracy constantly raising approximately reach peak top- validation accuracy approximately balanced data set. details accuracy evolution training phase shown figure large training validation accuracy unusual convolutional networks dropout regularisation assume behaviour attributed figure architecture rotation invariant neuronal network. numbers feature maps vectors show dimensions. snow shown reduce complexity appended feature maps third convolutional layer. result vector contains probabilities point interest green yellow zone. probabilities optimised match ofﬁcial hazard zone training. small training data contains avalanches data points zone determine parameters. training stopped manually steps took machine nvidia intel cpu. figure top- top- accuracy calculated balanced training balanced validation graphs smoothed savitzky-gloay ﬁlter. validation accuracy peaks optimisation steps approximately accuracy. point training validation diverge clear sign overﬁtting. training phase network used generate hazard zone maps arbitrary regions terrain expected snow cover available. automatic generated hazard zone maps validation regions shown figuresto alongside ofﬁcial hazard zone maps. reduced predictions areas ofﬁcial hazard zone available judge results. maps created ﬁnal version network. expect better results network peak accuracy. generation theses maps takes minutes machine used training. used validation regions substantially different topographies. top- accuracy generated hazard zone maps approximately l¨angenfeld stanzer valley. values calculated unbalanced data therefore considerably higher. reason green zones easier predict dominating real case scenario especially l¨angenfeld. generated hazard zone maps show interesting details. looking l¨angenfeld observe avalanches identiﬁed neuronal network. however shape avalanches runout represented poorly. looking stanzer valley green zones preserved quite well also steep terrain. figure hazard zone l¨angenfeld alongside prediction neuronal network black lines right picture show outlines documented avalanches. note network never ofﬁcial hazard zone documented avalanches. work shows application rotation invariant convolutional network hazard zone mapping. network learns existing hazard zone maps generate ones regions. process allows transfer knowledge region another. accuracy validation regions signiﬁcantly plain coincidence showing feasibility idea. however serious application approach needs improved. several methods suggested literature improve results batch normalisation untied biases also pooling feature maps inappropriate task related translation invariance corrupt estimation runout distances. overﬁtting network early training phase highlights necessity training data. demand high amounts training data probably biggest disadvantage method. however training methods improve results small data sets unsupervised learning imperative understand processes within neuronal network practical application. several techniques visualise neuronal network features determining outcome interesting machine learning point view also lead insights avalanche formation since neuronal network encodes statistical information training data. overall conﬁdent artiﬁcial intelligence neuronal networks play important role many areas everyday life. potential augment creation process interregional standardization hazard zone maps crucial instrument safety considerations settlement development tourism transit alpine regions. figure hazard zone stanzer valley alongside prediction neuronal network black lines bottom picture show outlines documented avalanches. note network never ofﬁcial hazard zone documented avalanches. thank prof. harders prof. haltmeier supervision project research cluster deep learning doctoral programme computational interdisciplinary modelling valuable discussions. gratefully acknowledge support granig ¨osterle doctoral advisors prof. fellin prof. rauch.", "year": "2018"}