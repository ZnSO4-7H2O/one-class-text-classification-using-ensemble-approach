{"title": "Cross-modal Embeddings for Video and Audio Retrieval", "tag": "eess", "abstract": " The increasing amount of online videos brings several opportunities for training self-supervised neural networks. The creation of large scale datasets of videos such as the YouTube-8M allows us to deal with this large amount of data in manageable way. In this work, we find new ways of exploiting this dataset by taking advantage of the multi-modal information it provides. By means of a neural network, we are able to create links between audio and visual documents, by projecting them into a common region of the feature space, obtaining joint audio-visual embeddings. These links are used to retrieve audio samples that fit well to a given silent video, and also to retrieve images that match a given a query audio. The results in terms of Recall@K obtained over a subset of YouTube-8M videos show the potential of this unsupervised approach for cross-modal feature learning. We train embeddings for both scales and assess their quality in a retrieval problem, formulated as using the feature extracted from one modality to retrieve the most similar videos based on the features computed in the other modality. ", "text": "increasing amount online videos brings several opportunities training self-supervised neural networks. creation large scale datasets videos youtubem allows deal large amount data manageable way. work ways exploiting dataset taking advantage multi-modal information provides. means neural network able create links audio visual documents projecting common region feature space obtaining joint audio-visual embeddings. links used retrieve audio samples well given silent video also retrieve images match given query audio. results terms recallk obtained subset youtube-m videos show potential unsupervised approach cross-modal feature learning. train embeddings scales assess quality retrieval problem formulated using feature extracted modality retrieve similar videos based features computed modality. index terms— sonorization videos become next frontier artiﬁcial intelligence. rich semantics contained make challenging data type posing several challenges perceptual reasoning even computational level. mimicking learning process knowledge extraction humans develop visual audio perception remains open research question video contain information format manageable science research. videos used work main reasons. firstly naturally integrate visual audio data providing weak labeling modality respect other. secondly high volume visual audio data allows training machine learning algorithms whose models governed high amount parameters. huge scale video archives available online increasing number video cameras constantly monitor world offer data computation power available process them. popularization deep neural networks among computer vision audio communities deﬁned common framework boosting multimodal research. tasks like video sonorization speaker impersonation self-supervised feature learning exploited opportunities offered artiﬁcial neurons project images text audio feature space bridges across modalities built. work exploits relation visual audio contents video clip learn joint embedding space deep neural networks. multilayer perceptrons visual features second audio features trained mapped cross-modal representation. adopt self-supervised approach exploit unsupervised correspondence audio visual tracks video clip. propose joint audiovisual space address retrieval task formulating query modalities. depicted figure whether video audio clip used query search matching pair large collection videos. example animated could sonorized ﬁnding adequate audio track audio recording illustrated related video. paper present simple effective model retrieving documents fast light search. address exact alignment modalities would require much higher computation effort. paper structured follows. section introduces related work learned audiovisual embeddings neural networks. section presents architecture model section trained. experiments reported section ﬁnal conclusions drawn section source code trained model used paper publicly available https//github.com/surisdi/ youtube-m. past years relationship audio visual content videos researched several contexts. overall conventional approaches divided four categories according task generation classiﬁcation matching retrieval. become increasingly popular research done relationship music album covers also music videos visual modality explore multimodal information present types data. recent study also explored cross-modal relations modalities using images people talking speech. done canonical correlation analysis cross-modal factor analysis. also applying uses visual sound features common subspace features aiding clustering image-audio datasets. work presented idea greedy layer-wise training restricted boltzmann machines vision sound. present work focused using information present modality create joint embedding space perform cross-modal retrieval. idea exploited especially using text image joint embeddings also kinds data example creating visual-semantic embedding using synchronous data learn discriminative representations shared across vision sound text joint representations images video audio fully exploited work explored option knowledge authors. paper seek joint embedding space using music videos obtain closest farthest video given query video based either image audio. main idea current work borrowed baseline understand approach. there authors create joint embedding space recipes images. retrieve recipes food image looking recipe closest embedding. apart retrieval results also perform experiments studying localized unit activations arithmetics images. images video vector features representing audio. features already precomputed provided youtube-m dataset particular video-level features represent whole video clip vectors audio another video. feature representations result average pooling local audio features computed windows second local visual features computed frames sampled main objective system transform different features features laying joint space. means video ideally image features audio features transformed joint features space. call features embeddings represent image embeddings audio embeddings. idea joint space represent concept video image audio generalization consequence videos similar concepts closer embeddings videos different concepts embeddings apart joint space. example representation tennis match video close football match maths lesson. thus fully connected layers different sizes stacked other going original features embeddings. audio image network completely separated. fully connected layers perform non-linear transformation input features mapping embeddings parameters non-linear mapping learned optimization process. that classiﬁcation embeddings done also using fully connected layer different classes using sigmoid activation function. insight step section number hidden layers necessarily ﬁxed well number neurons layer since experimented different conﬁgurations. hidden layer uses relu activation function weights layer regularized using norm. objective work embeddings video close possible keeping embeddings different videos possible. formally given video represented audio visual features provide additional information system incorporating video labels provided youtube-m dataset. information added regularization term seeks solve high-level classiﬁcation problem audio video embeddings sharing weights branches. idea classiﬁcation weights embeddings labels shared modalities. loss optimized together previously explained similarity loss serving regularization term. basically system learns classify audio images video different classes labels provided dataset. limit effect using regularization parameter incorporate previously explained regularization joint embedding single fully connected layer shown figure formally obtain label probabilities softmax softmax represents learned weights shared branches. softmax activation used order obtain probabilities output. objective make similar possible similar possible category labels video represented image features audio features respectively. positive pairs same. time however prevent embeddings different videos close joint space. words want similarity. however objective force opposite other. instead forcing similarity equal zero allow margin similarity small enough force embeddings clearly place joint space. call margin training positive negative pairs used positive pairs ones correspond video negative pairs ones correspond video proportion negative samples pnegative. negative pairs selected random pairs common label order help network learn distinguish different videos embedding space. notion similarity closeness mathematically translated cosine similarity embeddings cosine similarity deﬁned margin percentage negative samples pnegative hidden layers network branch number neurons layer being features embedding image branch audio branch. experiments presented section developed subset video clips youtube-m dataset dataset contain video ﬁles representations precomputed features audio video. audio features computed using method explained audio windows second visual features computed frames sampled inception model provided tensorflow dataset provides video-level features represent video using single vector thus maintain temporal information; also provides frame-level features consist single vector representing second audio single vector representing frame video sampled frame second. main goal dataset provide enough data reach state results video classiﬁcation. nevertheless huge dataset also permits approaching tasks related videos cross-modal tasks obtain quantitative results recallk metric. deﬁne recallk recall rate retrieval experiments percentage queries corresponding video retrieved hence higher better. experiments performed different dimension feature vector. table shows results recall audio video. words audio embedding video many times retrieve embedding corresponding images video. table shows recall video audio. reference random guess result would k/number elements. obtained results show clear correspondence embeddings coming audio features ones coming video features. also interesting notice results audio video video audio similar system trained bidirectionally. addition objective results performed insightful qualitative experiments. consisted generating embeddings audio video list different videos. then randomly chose video image embedding retrieved video closest audio embedding around closest embedding corresponded video took second ordered list. figure shows experiments. left results given video query getting closest audio; right input query audio. examples depicting real videos audio available online shows results going image audio going audio image. four different random examples shown case. result query also show youtube-m labels completeness. results show starting image features video retrieved audio represents accurate images. subjectively negligible cases retrieved audio actually better video original example original video artiﬁcially introduced music cases background commentator explaining video foreign language. analysis also done similarly around audio colorization approach providing images given audio. presented effective method retrieve audio samples correctly given video. qualitative results show already existing online videos variety represent good source audio videos even case retrieving small subset large amount videos. existing difﬁculty create audio scratch believe retrieval approach path follow order give audio videos. information provided individual image audio features used current work. promising future work implies using temporal information match audio images making implicit synchronization audio images video have without needing supervised control. thus next step research introducing recurrent neural network allow create accurate representations video also retrieve different audio samples image creating fully synchronized system. also would interesting study behavior system depending class input. observing dataset clear classes degree correspondence audio image example videos artiﬁcially added music related images. short believe youtube-m dataset allows promising research future ﬁeld video sonorization audio retrieval huge amount samples capturing multi-modal information highly compact way. work partially supported spanish ministry economy competitivity european regional development fund contract tec-r. amanda duarte funded mobility grant severo ochoa program barcelona supercomputing center jiansong chao haofen wang wenlei zhou weinan zhang yong tunesensor semantic-driven music recommendation service digital photo albums international semantic conference alexander schindler andreas rauber audiovisual approach music genre classiﬁcation affective color features european conference information retrieval. springer esra acar frank hopfgartner sahin albayrak understanding affective content music videos learned representations international conference multimedia modeling. springer olivier gillet slim essid richard correlation automatic audio visual segmentations music videos ieee transactions circuits systems video technology vol. dongge nevenka dimitrova mingkun ishwar sethi multimedia content processing cross-modal association proceedings eleventh international conference multimedia. amaia salvador nicholas hynes yusuf aytar javier marin ferda ingmar weber antonio torralba learning cross-modal embeddings cooking recipes food images cvpr andrea frome greg corrado jonathon shlens samy bengio jeffrey dean marc’aurelio ranzato tomas mikolov devise deep visual-semantic embedding model neural information processing systems sami abu-el-haija nisarg kothari joonseok paul natsev george toderici balakrishnan varadarajan sudheendra vijayanarasimhan youtube-m largecorr vol. scale video classiﬁcation benchmark abs/. mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems arxiv preprint arxiv. hershey chaudhuri ellis gemmeke jansen moore plakal platt saurous seybold slaney weiss wilson architectures large-scale audio classiﬁcation ieee international conference acoustics speech signal processing march", "year": "2018"}