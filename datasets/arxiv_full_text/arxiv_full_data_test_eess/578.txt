{"title": "Pop Music Highlighter: Marking the Emotion Keypoints", "tag": "eess", "abstract": " The goal of music highlight extraction is to get a short consecutive segment of a piece of music that provides an effective representation of the whole piece. In a previous work, we introduced an attention-based convolutional recurrent neural network that uses music emotion classification as a surrogate task for music highlight extraction, for Pop songs. The rationale behind that approach is that the highlight of a song is usually the most emotional part. This paper extends our previous work in the following two aspects. First, methodology-wise we experiment with a new architecture that does not need any recurrent layers, making the training process faster. Moreover, we compare a late-fusion variant and an early-fusion variant to study which one better exploits the attention mechanism. Second, we conduct and report an extensive set of experiments comparing the proposed attention-based methods against a heuristic energy-based method, a structural repetition-based method, and a few other simple feature-based methods for this task. Due to the lack of public-domain labeled data for highlight extraction, following our previous work we use the RWC POP 100-song data set to evaluate how the detected highlights overlap with any chorus sections of the songs. The experiments demonstrate the effectiveness of our methods over competing methods. For reproducibility, we open source the code and pre-trained model at https://github.com/remyhuang/pop-music-highlighter/. ", "text": "huang y.-s. music highlighter marking emotion keypoints. ansactions international society music information retrieval https//doi.org/./tismir. goal music highlight extraction thumbnailing extract short consecutive segment piece music somehow representative whole piece. previous work introduced attention-based convolutional recurrent neural network uses music emotion classification surrogate task music highlight extraction assuming emotional part song usually corresponds highlight. paper extends previous work following aspects. first methodology-wise experiment architecture need recurrent layers making training process faster. moreover compare late-fusion variant early-fusion variant study better exploits attention mechanism. second conduct report extensive experiments comparing proposed attention-based methods heuristic energy-based method structural repetition-based method three simple feature-based methods respectively. lack public-domain labeled data highlight extraction following previous work rwc-pop -song data evaluate detected highlights overlap chorus sections songs. experiments demonstrate superior effectiveness methods competing methods. reproducibility share code pre-trained model https//github. com/remyhuang/pop-music-highlighter/. introduction growing amount multimedia data available internet ability efficiently browse data important. music highlight extraction music thumbnailing task aims find short continuous segment piece music nicely represent whole piece. understood audio preview chosen machines. successful algorithms music highlight extraction useful many music information retrieval tasks indexing retrieval trial listening radio podcasts djing example highlight extraction pre-processing step pick representative segment song facilitate subsequent labeling processing analysis instead dealing whole song taking random segment song agreement chorus section song representative sample whole song. common practice play chorus introducing music chart using music commercials user study conducted meintanis shipman showed that getting familiar song chorus section regarded important part song compared parts intro verse. researchers view chorus sections memorable emotional part song extracting features chorus wang demonstrated improved accuracy recognizing song level arousaldominance-related emotions. related work absence well-defined public domain labeled data music highlights important stream related work focuses chorus detection instead. many previous researchers assumed most-repeated patterns melody motifs correspond chorus sections. firstly used methods selfsimilarity matrix hidden markov model segmenting music piece several parts analyzed resulting segments identify frequent ones. commonly used feature representations include time-frequency representations spectrogram chromagram chorus repeats several times song still necessary employ methods pick highlight. simple solution randomly pick heuristics used. limitations approach. first most-repeated part song always correspond chorus; also verse even short melodies. second still variations different chorus sections song better give sections different highlight scores distinguish them. words classifier regression model needed. however little work performed regarding this best knowledge lack labeled data training supervised model estimates highlight scores. another important stream related work attempts detect highlights leveraging traces user behavior logged online music services. example working electronic dance music yadati aimed detect drops specific moments drastic change song. drops usually associated strong emotional release. drop point often implies interesting part song take segment starting drop point highlight. yadati achieved drop detection feeding spectrogram mel-frequency cepstral coefficients features related rhythm support vector machine labeled data needed train mined soundcloud allows users leave timed comments annotating specific moments song. idea seems work music genres unclear whether timed comments available. another approach utilizing user logs presented bittner pointed people scrub playhead music player start song specific moment moments usually occur best parts song. therefore access scrubbing behavior data bittner highlight extraction performed simply running peak picking algorithm scrubbing data find moments. crowdsourced method classifiers needed. streaming companies spotify anonymously scrubbing behavior data. however knowledge data open research community. moreover classifier still needed want highlight unpopular songs took different approach previous work utilizing possible connections emotions song highlights. idea firstly data emotion labels train neural network model music emotion classification then adds network so-called attention mechanism weigh contribution different short time chunks song predicting song-level emotion labels song. attention mechanism widely used many natural language processing problems learn attention specific parts input. used attention mechanism assign attention scores audio chunks facilitate weighted aggregation emotion prediction different chunks making final song-level prediction illustrated figure model trained using emotion labels investigated whether peaks attention scores help detect chorus sections songs. preliminary experiment showed idea promising parallel work independently proposed similar attention-based neural network model music highlight extraction. three notable differences model ours. first used music genre classification surrogate task used emotion classification. attention mechanism assessed audio chunk contributes predicting genre song. second made emotion prediction audio chunk first used attention arrive song-level estimate used attention linearly combine features extracted chunk performed songlevel prediction based aggregated feature vector depicted figure lastly used attention scores alone highlight extraction additionally considered energy curve input audio. makes sense found chorus-like sections dramatic moments song tend louder parts song however evaluated result model highlight extraction using private data report result without using energy. moreover combining energy curve attention scores actually weighed energy curve therefore argued highlights determined mainly energy. public-domain chorus detection data rwc-pop database evaluate effect following main design choices attention-based approach highlight extraction using emotion classification genre classification surrogate task; using late fusion early fusion incorporate attention scores. facets represent main difference model model proposed shown possible speed model training process using convolutional layers rather recurrent layers attention mechanism moderate performance drop highlight extraction. motivated work vaswani study paper advanced design uses so-called positional encodings model temporal ordering audio chunks uses fully-connected layers attention mechanism. observe slight boost accuracy chorus detection design. compared repetition-based method previous work consider three simple unsupervised feature-based methods performance comparison covering spectral energy spectral centroid spectral roll-off. moreover motivated investigate performance fusing energy-based method attentionbased methods. music highlight extraction received much academic attention literature. promote future research facilitate application model problems share research community code emotion-based highlight extraction model built https//github.com/remyhuang/pop-musichighlighter/. methods recurrent neural attention modeling late fusion firstly introduce model proposed previous work model architecture illustrated figure model uses multi-class music emotion classification surrogate task addition incorporates recurrent attention mechanism late-fusion fashion weighing chunk-level predictions. therefore call emotion-based recurrent neural attention modeling late fusion emotionbased rnam-lf short. specifically given song arbitrary length transform audio mel-spectrogram split whole input several chunks fixed short length. leads chunks mel-spectrograms considered image. chunk stack convolutional layers high-level feature extraction followed global pooling layer temporal dimension temporal aggregation convert chunk input feature vector that pass intermediate features different branches. prediction outputs uses fully-connected layers learn mapping intermediate features target emotion labels. result chunk chunklevel label prediction significant characteristic music sequential meaning musical event likely certain dependency upon preceding events dependency tends stronger musical events nearby. hence sensible exploit temporal information estimating attention scores. rnam-lf attention mechanism realized recurrent layer bi-directional long-short term attention scores used information chunk used make label prediction call late-fusion model. training stage estimate compared ground truth label compute binary cross-entropy loss backpropagated update network parameters. expect attention mechanism allow model weigh different chunks differently based output memory cells lstm. therefore final estimate model likely boost contribution relevant chunks suppress unimportant ones. non-recurrent neural attention modeling late fusion field vaswani recently proposed network architecture known transformer contains attention mechanisms uses convolutional recurrent layers all. besides better exploit temporal information added positional encoding input features represent relative position entire sentence. design makes model training inference faster exhibits strong performance machine translation tasks. motivated idea experiment design recurrent layers attention mechanism. specifically positional embeddings intermediate feature embeddings feeding attention mechanism. positional embeddings dimension following sine cosine functions different frequencies positional encoding similarly attention scores late fusion. model different rnam-lf details regarding attention mechanism block figure nam-lf refer model. moreover nam-lf refer variant uses instead input fully-connected layers study effect positional encoding. nam-lf nam-lf computationally lighter rnam-lf. moreover nam-lf explicitly takes account position chunk calculating attention scores. hand helps model extract sequential information song. hand information regarding position chunks useful right. example intuitively highlight song tends appear beginning song. early-fusion variant discussed earlier also illustrated figure consider early-fusion variant uses attention scores combine feature embeddings uses aggregated feature vector make song-level estimate stack fully-connected layers. ‘ef’ denote model adopts early-fusion method. shown figure either late fusion early fusion attention mechanism viewed supplementary network addition main network making predictions. however attention score used whole network whereas fully-connected layers fusion done. therefore attention mechanism stress finding importance chunks learnable parameters accordingly design perform better want estimated attention scores highlight extraction. intend verify experiment. genre-based variant also multi-label multi-class music genre classification surrogate task train models. amounts changing labeled data; exactly network architectures introduced before. music highlight extraction generate music highlight pre-defined length based attention scores. listening experience extracted highlight consecutive segment. hence simply running window pick consecutive collection attention curve chunks highest aggregated sum. although mplementation details show details network architecture nam-lf example table attention-based models implemented evaluated experiments employ almost architecture input data converted audio song mel-scale spectrograms components using librosa sampling song using hamming window samples -sample size. moreover following common practice table network architecture proposed namlf model. convolutional layers values represent number filters kernel size strides activation functions. fully-connected layers values represent number hidden units dropout rate activation functions. layers batch normalization. also show size input output intermediate output training stage. emotion labels used in-house data collection compiled yang consisting clips western music song-level emotion labels crawled music service website allmusic. songs among possible emotion classes popular emotion associated songs least popular emotion songs average number songs emotion class instead access full songs -second audio preview crawled digital without knowing starting point audio preview extracted song. randomly sample songs training network songs validation set. clip uniformly segmented chunks overlaps seconds. accordingly size chunk input shown table three convolution layers feature extraction fully-connected layers chunk-level prediction four fully-connected layers attention mechanism. rnam-lf layer bi-directional lstm attention mechanism hidden units. matter model used output song vector length genre-based variant collect data beatport worldwide principal source music sub-genre classification. contains total possible sub-genres ‘house’ ‘dubstep’. songs released class collect songs training songs validation leading training data songs validation songs. song associated sub-genre treat multi-class classification problem. audio file acquire beatport -minute sample version song take first seconds network architecture emotion-based counterpart. mini-batch songs model training amounts chunks mini-batch. find useful apply batch normalization every layer network stabilize training process. inference stage process song time. however divide song -second chunks also view chunks mini-batch apply batch normalization normalize chunks based shared statistics. find empirically improves results highlight extraction. note test song arbitrary length number chunks test song varies. experiment lack public-domain annotations music highlights chorus detection proxy evaluate result highlight extraction performance study reported here. evaluation protocol chorus detection -song popular music subset database evaluate correspondence music highlights chorus sections. rwc-pop songs english style western music typical songs american charts rest songs japanese style japanese music typical songs japanese charts publicly available widely used mir. accordingly researchers easily obtain copy data evaluate highlight extraction model fairly compare results ours. rwc-pop data comes manual annotation start times chorus section song. experiment length music highlights seconds evaluate degree overlap -second highlight chorus section song largest overlap highlight. quantified recall rate precision rate defined follows length overlap represents temporal overlap extracted highlight nearest chorus section. also compute f-measure harmonic mean highlight overlap chorus sections song zero. highlight subset nearest chorus section nearest chorus section subset highlight close one. calculate values song rwc-pop report average results. consider emotion-based genre-based variants models. emotion-based rnam-lf principal method proposed previous work simply previous implementation. genre-based nam-ef setting closest proposed three differences used either recurrent convolution layers attention mechanism fully-connected layers plus positional encoding; genre classification data set; settings regarding details network facilitate fair comparison implementation models. differences note results genre-based nam-ef reported cannot reflect performance model despite similarity main ideas. upper bound length extracted highlight fixed seconds length chorus sections variable possible songs. upper bound performance obtained oracle method selects -second segment song leads highest spectral energy uses librosa compute rootmean-square energy frame spectrogram create energy curve chooses segment highest aggregated highlight. repetition unsupervised method uses structural features proposed serra music segmentation dfourier magnitude coefficient-based clustering method grouping resulting musical segments. assume largest cluster corresponds chorus choose segment cluster closest middle song highlight. fair comparison trim extend segment make seconds long. implementation based music structure analysis framework proposed nieto bello results tabulate results table dividing three groups unsupervised methods emotion-based attentional modeling methods genre-based counterparts. results first group energy-based method performs best reaching f-measure. moreover large performance method methods. suggests chorus sections tend louder parts useful feature. results first group also show taking middle segment repeated work well finding chorus sections. comparing results first second groups table appears attention-based method using emotion classification surrogate task outperforms energy-based method. particular nam-lf leads highest f-measure encouraging implies chunks contributing prediction music emotions correspond song highlights connections leveraged proposed methods. works better positional encoding useful positional encoding slightly outperforms rnam. validates previous choice shows attention mechanism simplified using fully-connected layers. addition performing better highlight extraction discarding recurrent layers also speeds training procedure. implementation training time epoch three times shorter rnam. figure ground truth chorus sections different colors indicate different chorus sections song. second energy curve. last four rows attention curves estimated four different emotion-based models three songs rwc-pop. left right ‘disc/.mp’ ‘disc/. ‘disc/.mp’. rnam-lf attention score -second audio chunk following previous work three attention-based methods attention score -second audio chunk. regions mark resulting -second highlights. examples found github page. figure shows energy curve attention curves computed different methods three randomly selected songs rwc-pop resulting highlights correspond chorus sections. different attention-based methods sometimes attend different choruses song nam-based method tends select chorus energybased method. moreover comparing attention curves nam-lf nam-ef former fewer peaks peaks nicely correspond different choruses song. confirms better choice comparing results second third groups table shows genre-based models much inferior emotion-based models. genrebased settings rnam-lf outperforms nam-lf many unsupervised methods best results cannot beat even energy-based method. possible genre data sets improve performance current implementation shows emotions exhibit stronger connection choruses genres figure lastly shows results fuse energy curves attention curves did. different fusion weights running emotionbased method nam-lf fusion slightly improves results especially precision rate. best result obtained fusion weight possible reason performance gain energy curve frame-based provides better resolution attention curve helps fine-tune result. contrast fusing energy genre-based attention figure best result obtained weighing energy curve similar weight used evidence better emotion rather genre surrogate task attention-based models. discussion section discuss issues limited performance models. first note training data -second previews songs. possible preview contain highlight chorus sections song. would better training data consisted full songs instead previews. second current implementation emotion labels crawled all-music website ground truth training emotion classifier. possible labeled data noisy. addition considering emotion classes makes difficult carefully investigate dependency emotions song highlights. future hope validate emotion labels human validation did) reduce number emotion classes facilitate in-depth analysis. example depending emotion song tries express highlight song chorus somewhere else. moreover japanese songs rwcpop training data emotion classification mainly composed western music. hand hope test highlight extraction music genres evaluate generalizability. hand also important include songs genres training set. input features models consider mel-spectrogram far. feature representations chromagram tempogram added exploit information regarding chord progressions rhythmic patterns. finally note main goal work extract continuous short segment full-length song highlight. therefore need recall chorus sections song. however chorus sections song similar another possible recall finding segments acoustically similar selected highlight. performed example integration model msaf conclusion paper presented modification previous attention-based neural network models leverage emotions extract music highlights. time non-recurrent attention mechanism inspired vaswani design reduces training time third contributes improved results highlight extraction. moreover presented performance study provides insights design attention-based method verifying advantage using emotion rather genre surrogate task using latefusion design rather early-fusion design. evaluating rwc-pop chorus detection best method emotion-based nam-lf achieves f-measure better energy-based method several unsupervised baselines. results shown table indicate still large performance oracle method ‘upper bound’ find results encouraging proposed method involve labels chorus sections music highlights. future work interested mainly following directions. first emotions highlights want explore aspects aesthetics novelty highlight extraction. second hope build collaborate companies benchmark data evaluating music highlight extraction avoid possible discrepancy choruses highlights help move forward research topic. http//music.naver.com/. would better could compare attention-based method presented bittner however possible without access spotify internal data. https//www.allmusic.com/moods. https//www.digital.com/. prepare test data beatport data mentioned later paper focus evaluating performance highlight extraction emotion genre classification. https//www.beatport.com/. beatport data sub-genre classification part ongoing dj-related project access genre data used complete list subgenres ‘breaks’ ‘dance’ ‘hard dance’ ‘house’ ‘deep house’ ‘electro house’ ‘progressive house’ ‘dubstep’ ‘dub techno’ ‘electro room’ ‘electronica/downtempo’ ‘future house’ ‘garage/bassline/grime’ ‘hardcore/ hard techno’ ‘hip-hop/r&b’ ‘jump dnb’ ‘jungle dnb’ ‘leftfield bass’ ‘leftfield house&techno’ ‘liquid dnb’ ‘minimal/deep tech’ ‘progressive room’ emotion surrogate task classification accuracy achieved nam-lf validation random guess genre surrogate task classification accuracy achieved nam-lf validation random guess msaf contains implementation various algorithms music structural analysis. selection specific algorithms structural features d-fourier magnitude coefficient-based clustering based performance study reported nieto bello personal communication nieto learned unfortunately msaf provide good solution chorus detection. however still include baseline represents state-of-the-art unsupervised structure analysis method available community. bittner hernandez humphrey jehan mccurry montecchio automatic playlist sequencing transitions. proceedings international society music information retrieval conference cooper foote summarizing popular music structural similarity analysis. proceedings ieee international workshop applications signal processing audio acoustics https//doi.org/./aspaa.. dieleman schrauwen end-to-end learning music audio. proceedings ieee international conference acoustics speech signal processing https//doi. org/./icassp.. goto chorus section detection method musical audio signals application music listening station. ieee transactions audio speech language processing https//doi.org/./tsa.. goto hashiguchi nishimura music database popular classical jazz music databases. proceedings international conference music information retrieval grosche müller kurth cyclic tempogram mid-level tempo representation music signals. ieee international conference acoustics speech signal processing huang y.-s. chou s.-y. yang y.-h. djnet dream making automatic international society music information retrieval conference late-breaking paper huang y.-s. chou s.-y. yang y.-h. music thumbnailing neural attention modeling music emotion. proceedings asia paciﬁc signal information processing association annual summit conference ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning j.-y. j.-y. h.-g. music emotion classification based music highlight detection. proceedings ieee international conference information science applications j.-y. yang y.-h. event localization music auto-tagging. proceedings multimedia https//doi.org/./. logan music summarization using phrases. proceedings ieee international conference acoustics signal müller ewert chroma toolbox matlab implementations extracting variants chroma-based audio features. proceedings international society music information retrieval conference nieto bello systematic exploration computational music structure research. proceedings international society music information retrieval conference peeters burthe rodet toward automatic music audio summary generation signal analysis. proceedings international conference music information retrieval series structure features segment similarity. ieee transactions multimedia https//doi.org/./tmm.. shen zhou long jiang zhang disan directional self-attention network rnn/cnn-free language understanding. arxiv preprint arxiv.. balen burgoyne wiering veltkamp analysis chorus features popular song. proceedings international society music information retrieval conference wang chen yang enhance popular music emotion regression importing structure information. proceedings asiapaciﬁc signal information processing association annual summit conference yadati larson liem hanjalic detecting drops electronic dance music content based approaches socially significant music event. proceedings international society music information retrieval conference cite article huang y.-s. chou s.-y. yang y.-h. music highlighter marking emotion keypoints. ansactions international society music information retrieval https//doi.org/./tismir. copyright author. open-access article distributed terms creative commons attribution international license permits unrestricted distribution reproduction medium provided original author source credited. http//creativecommons.org/licenses/by/./.", "year": "2018"}