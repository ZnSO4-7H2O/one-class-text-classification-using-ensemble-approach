{"title": "Deep Long Short-Term Memory Adaptive Beamforming Networks For  Multichannel Robust Speech Recognition", "tag": "eess", "abstract": " Far-field speech recognition in noisy and reverberant conditions remains a challenging problem despite recent deep learning breakthroughs. This problem is commonly addressed by acquiring a speech signal from multiple microphones and performing beamforming over them. In this paper, we propose to use a recurrent neural network with long short-term memory (LSTM) architecture to adaptively estimate real-time beamforming filter coefficients to cope with non-stationary environmental noise and dynamic nature of source and microphones positions which results in a set of timevarying room impulse responses. The LSTM adaptive beamformer is jointly trained with a deep LSTM acoustic model to predict senone labels. Further, we use hidden units in the deep LSTM acoustic model to assist in predicting the beamforming filter coefficients. The proposed system achieves 7.97% absolute gain over baseline systems with no beamforming on CHiME-3 real evaluation set. ", "text": "far-ﬁeld speech recognition noisy reverberant conditions remains challenging problem despite recent deep learning breakthroughs. problem commonly addressed acquiring speech signal multiple microphones performing beamforming them. paper propose recurrent neural network long short-term memory architecture adaptively estimate real-time beamforming ﬁlter coefﬁcients cope non-stationary environmental noise dynamic nature source microphones positions results timevarying room impulse responses. lstm adaptive beamformer jointly trained deep lstm acoustic model predict senone labels. further hidden units deep lstm acoustic model assist predicting beamforming ﬁlter coefﬁcients. proposed system achieves absolute gain baseline systems beamforming chime- real evaluation set. although extraordinary performance achieved automatic speech recognition advent deep neural networks performance still degrades dramatically noisy far-ﬁeld situations achieve robust speech recognition multiple microphones used enhance speech signal reduce effects noise reverberation improve performance. scenario essential step front-end processing multichannel ﬁltering beamforming steers spatial sensitivity region beam direction target source inserts spatial suppression regions nulls directions corresponding noise interference. delay-and-sum beamforming widely used multichannel signal processing multichannel inputs microphone array delayed aligned time summed single channel signal. signal target direction enhanced noises interferences coming directions attenuated. filter-and-sum beamforming applies ﬁlters input channels summing minimum variance distortionless response generalized eigenvalue ﬁlter-and-sum beamforming methods solve ﬁlter coefﬁcients using different derivations. although methods achieved good performance beamforming goal optimize signal-level objective order achieve robust speech recognition important jointly optimize beamforming acoustic model objective maximizing performance. parameters frequency-domain beamformer ﬁrst estimated based generalized cross correlation microphones. conventional features extracted beamformed signal passing second acoustic modeling. instead ﬁltering frequency domain performs spatial spectral ﬁltering time-domain convolution waveform. output feature passed convolutional lstm acoustic model predict context-dependent state output targets. beamforming frequency decomposition factorized separate layers network. approaches assume speaker position environment ﬁxed estimate constant ﬁlter coefﬁcients either beamforming spatial spectral ﬁltering. however real noisy far-ﬁeld scenarios position source noise room impulse response keep changing time-invariant ﬁlter coefﬁcients estimated neural networks fail robustly enhance target signal. therefore propose adaptively estimate beamforming ﬁlter coefﬁcients time frame using lstm deal possible changes source noise channel conditions. enhanced signal generated applying time-variant ﬁlter coefﬁcients short-time fourier transform array signals. ﬁlterbank like features obtained enhanced signal passed deep lstm acoustic model predict senone posterior. lstm beamforming network lstm acoustic model jointly trained using truncated back-propagation time cross-entropy objective. stft coefﬁcients array signals used input beamforming network. systems speech signal enhanced lstm acoustic model. speech enhancement module acoustic model jointly optimized minimize input single channel signal. previous work shown speech separation performance improved incorporating speech recognition alignment information within speech enhancement framework. inspired this feed units hidden layer lstm acoustic model previous time step back auxiliary input beamforming network predict current ﬁlter coefﬁcients. note work different that perform adaptive beamforming different input channels system works input channels; adaptive lstm beamformer predicts frequency domain ﬁlter coefm= sophisticated features extracted beamformed stft coefﬁcients passed lstm acoustic model predict senone posterior. experiments ﬁlterbank like feature generated operation matrix multiplication dimensional real-value vector power spectrum beamformed signal time global mean variance normalization applied ﬁlterbank like feature. note operations section performed real-value computation easily represented differentiable computational graph. recently lstms shown effective dnns conventional rnns acoustic modeling able model temporal sequences long-range dependencies accurately others especially amount training data large. lstm successfully applied rnn-hmm hybrid systems end-to-end system work deep lstm-hmm hybrid system utilized acoustic modeling. forced alignment ﬁrst generated gmm-hmm system used frame-level acoustic targets lstm attempts classify. lstm trained cross-entropy objective function using truncated bptt. paper connect deep lstm adaptive lstm beamformer compute ﬁlterbank beamformed stft coefﬁcients. ﬁcients performs frequency domain ﬁlter-and-sum stft coefﬁcients work majorly focuses time-domain ﬁltering waveforms input; ﬁlter bank like features generated ﬁxed transform beamformed stft coefﬁcients acoustic modeling work time/frequency domain convolution performed trainable parameters beamformed features work; additional gate modulation applied feedback reduce system complexity much smaller dataset. experiments show feedback captures high-level knowledge acoustic states increases performance. experiments conducted chime dataset. joint training lstm adaptive beamforming network deep lstm acoustic model achieves absolute gain single channel signal real test data. acoustic model feedback provides extra gain generalization delay-and-sum beamforming ﬁlter-andsum beamformer processes signal microphone using ﬁnite impulse response ﬁlter summing frequency domain operation written xtfm complex stft coefﬁcient timefrequency index signal channel beamforming ﬁlter coefﬁcient ˆxtf complex stft coefﬁcient enhanced signal. numbers microphones time frames frequencies. cope time-variant source position room impulse response make ﬁlter coefﬁcients timedependent propose adaptive ﬁlter-and-sum beamforming lstm network special kind recurrent neural network purpose-built memory cells store information lstm successfully applied many different tasks strong capability learning long-term dependencies. lstm takes input sequence computes hidden vector sequence iterating equation implement lstm peep hole connections. work apply real-value lstm adaptive ﬁlterand-sum beamformer predict real imaginary parts complex ﬁlter coefﬁcients time channel introduce following real-value vectors complex values gtfm xtfm deep lstm acoustic model section need jointly optimized objective maximizing performance. words beamforming lstm needs concatenated lstm acoustic model form integrated network takes multichannel stft coefﬁcients input produces senone posteriors illustrated fig. deep lstm three hidden layers experiments shown simplicity. train integrated lstm network connect beamforming network ﬁltering acoustic model single feed forward network backpropagate gradient cross-entropy objective function network adaptive beamformer acoustic model optimized task using multichannel training data. fig. unfolded integrated network lstm adaptive beamformer lstm acoustic model. acoustic feedback introduced allow hidden units lstm acoustic model assist predicting ﬁlter coefﬁcient current time. that feed hidden units hidden layer deep lstm acoustic model back input lstm beamformer auxiliary feature predict ﬁlter coefﬁcients next time. introducing acoustic model feedback re-written direct training integrated network easily falls local optimum gradients lstm beamformer deep lstm acoustic model different dynamic ranges. robust estimation model parameters training performed sequence shown algorithm chime- dataset released chime speech separation recognition challenge incorporates wall street journal corpus sentences spoken talkers situated challenging noisy environment recorded using -channel tablet based microphone array. chime- dataset consists real simulated data. real data recorded speech spoken actual talkers four real noisy environments generate simulated data clean speech ﬁrst convoluted estimated impulse response environment mixed background noise separately recorded environment training consists real noisy utterances speakers simulated noisy utterances speakers training recorded noisy environments. utterances development including real simulated utterances environments. utterances test including real simulated utterances environments. speakers training development test mutually different training development test data recorded different channels. text corpus also used train language model. baseline system built chainer kaldi toolkits. -dimensional ﬁlterbank features extracted kaldi channels used train deep lstm acoustic model using chainer. lstm layers hidden layer units. output layer units corresponds senone target. input feature ﬁrst projected dimensional space lstm. forced alignment generated gmm-hmm system trained data channels used target lstm training. evaluation development test data channel used testing lstm trained using bptt truncation size learning rate batch size stochastic gradient descent performance baseline system shown table -dimensional complex stft coefﬁcients extracted speech channels real imaginary parts stft coefﬁcients channels concatenated together form dimensional input beamforming lstm. input projected dimensional space lstm. beamforming lstm hidden layer units. hidden units vector projected sets dimensional ﬁlter coefﬁcients adaptively beamtable performance baseline lstm acoustic model beamformit-enhanced signal input joint training lstm beamformer lstm acoustic model without acoustic feedback. forming signals channels using objective computed beamformed signal beamformit beamforming lstm trained using bptt truncation size batch size learning rate baseline lstm acoustic model trained section lstm adaptive beamformer trained section concatenated together initialization integrated network. feature extraction layer inserted lstms extract dimensional ﬁlterbank features integrated network trained described steps section bptt truncation size batch size learning rate used training. data channels development test used evaluating integrated network. performance different cases shown table table best system integrated network lstm adaptive beamformer deep lstm acoustic model acoustic feedback achieves wers simulated development real development simulated test real test chime- dataset respectively. joint training integrated network without updating deep lstm acoustic model achieves absolute gains baseline system simulated development real development real test respectively. joint training integrated network parameters updated achieves absolute gains respectively baseline systems simulated development real development simulated test real test respectively. large performance improvement justiﬁes lstm adaptive beamformer able estimate real-time ﬁlter coefﬁcients adaptively response changing source position environmental noise room impulse response lstm acoustic model jointly trained optimize objective. further absolute gains achieved introduction acoustic feedback indicates high-level acoustic information also helpful predicting ﬁlter coefﬁcients next time step. note although proposed system acoustic feedback achieves absolute gains beamformed signal generated beamformit simulated development test sets work well beamformit real development test sets. beamformit implementation two-step time delay arrival viterbi postprocessing makes past future information predicting best alignment multiple channels current time system history past utilized estimate current ﬁlter coefﬁcients. explain differences performance alleviated using bidirectional lstm part future work. lstm beamformer adaptively predicts time-variant beamforming coefﬁcients performs ﬁlter-and-sum beamforming input channels. ﬁlter bank feature obtained stft coefﬁcients. fig. ﬁlter bank feature obtained lstm adaptive beamformer quite similar ﬁlter bank feature extracted stft coefﬁcients beamformed beamformit utterance. high matches lstm acoustic model well maximizing performance. work lstm adaptive beamforming proposed adaptively predict real-time beamforming ﬁlter coefﬁcients deal time-variant source location environmental noise room impulse response inherent multichannel speech signal. achieve robust lstm adaptive beamformer jointly trained deep lstm acoustic model optimize objective. framework achieves absolute gains baseline system chime- dataset. improvement achieved introducing acoustic feedback assist predicting ﬁlter coefﬁcients. however approach work well beamformit real data look future. hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. dahl deng acero context-dependent pre-trained deep neural networks large-vocabulary speech recognition ieee transactions audio speech language processing vol. marc delcroix takuya yoshioka atsunori ogawa yotaro kubo masakiyo fujimoto nobutaka keisuke kinoshita miquel espi shoko araki takaaki hori tomohiro nakatani strategies distant speech recognitionin reverberant environments eurasip journal advances signal processing vol. hori chen erdogan hershey roux mitra watanabe merl/sri system chime challenge using beamforming robust feature extraction advanced speech recognition ieee workshop automatic speech recognition understanding warsitz haeb-umbach blind acoustic beamforming based generalized eigenvalue decomposition ieee transactions audio speech language processing vol. july xiao watanabe erdogan hershey seltzer chen zhang mandel deep beamforming networks multi-channel speech recognition ieee international conference acoustics speech signal processing march sainath weiss wilson narayanan bacchiani andrew speaker location microphone spacing invariant acoustic modeling multichannel waveforms ieee workshop automatic speech recognition understanding sainath weiss wilson narayanan bacchiani factored spatial spectral multichannel ieee international conference waveform cldnns acoustics speech signal processing march felix weninger jrgen geiger martin wllmer bjrn schuller gerhard rigoll feature enhancement deep {lstm} networks {asr} reverberant multisource environments computer speech language vol. erdogan hershey watanabe roux phase-sensitive recognition-boosted speech separation using deep recurrent neural networks ieee international conference acoustics speech signal processing april tara sainath weiss kevin wilson michiel bacchiani neural network adaptive beamforming robust multichannel speech recognition proc. interspeech hochreiter schmidhuber long short-term memory neural computation vol. graves mohamed hinton speech recognition deep recurrent neural networks ieee international conference acoustics speech signal processing weng watanabe juang recurrent deep neural networks robust speech recognition ieee international conference acoustics speech signal processing hasim andrew senior franc¸oise beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling. interspeech chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun yoshua bengio attention-based models speech recognition advances neural information processing systems barker marxer vincent watanabe third chime speech separation recognition challenge dataset task baselines ieee workshop automatic speech recognition understanding geiger weninger gemmeke wllmer schuller rigoll memory-enhanced neural networks robust ieee/acm transactions audio speech language processing vol. june seiya tokui kenta oono shohei hido justin clayton chainer next-generation open source framework deep learning proceedings workshop machine learning systems twenty-ninth annual conference neural information processing systems daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannemann petr motlicek yanmin qian petr schwarz silovsky georg stemmer karel vesely kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. dec. ieee signal processing society ieee catalog cfpsrw-usb.", "year": "2017"}