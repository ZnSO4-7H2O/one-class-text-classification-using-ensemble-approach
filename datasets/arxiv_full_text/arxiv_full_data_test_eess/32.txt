{"title": "First and Second Order Methods for Online Convolutional Dictionary  Learning", "tag": "eess", "abstract": " Convolutional sparse representations are a form of sparse representation with a structured, translation invariant dictionary. Most convolutional dictionary learning algorithms to date operate in batch mode, requiring simultaneous access to all training images during the learning process, which results in very high memory usage and severely limits the training data that can be used. Very recently, however, a number of authors have considered the design of online convolutional dictionary learning algorithms that offer far better scaling of memory and computational cost with training set size than batch methods. This paper extends our prior work, improving a number of aspects of our previous algorithm; proposing an entirely new one, with better performance, and that supports the inclusion of a spatial mask for learning from incomplete data; and providing a rigorous theoretical analysis of these methods. ", "text": "abstract. convolutional sparse representations form sparse representation structured translation invariant dictionary. convolutional dictionary learning algorithms date operate batch mode requiring simultaneous access training images learning process results high memory usage severely limits training data size used. recently however number authors considered design online convolutional dictionary learning algorithms oﬀer better scaling memory computational cost training size batch methods. paper extends prior work improving number aspects previous algorithm; proposing entirely better performance supports inclusion spatial mask learning incomplete data; providing rigorous theoretical analysis methods. sparse representations dictionary learning. sparse signal representation aims represent given signal linear combination elements ﬁxed signal components example approximate sparse representations used wide variety applications including denoising super-resolution classiﬁcation face recognition issue solving sparse coding problems choose dictionary early work sparse representations used ﬁxed basis wavelets discrete cosine transform learned dictionaries provide better performance †department mathematics ucla angeles ‡ccs division alamos national laboratory alamos division alamos national laboratory alamos ¶department mathematics ucla angeles methods require simultaneous access training samples training. contrast online dictionary learning methods process training samples streaming fashion. speciﬁcally chosen sample training step. framework online dictionary learning denotes sparse coding instance d-update computes outer iteration batch dictionary learning algorithm involves computing coeﬃcient maps training samples online learning methods compute coeﬃcient small number training sample iteration coeﬃcient maps {x}t− used d-update computed previous iterations. thus algorithms implemented large sets training data dynamically generated data. online d-update methods corresponding online dictionary learning algorithms divided classes class second-order algorithms. algorithms inspired recursive least squares iterative reweighted least squares kernel second-order stochastic approximation etc. previous information {d}t− construct surrogate function estimate true loss function update minimizing surrogate function. surrogate functions involve ﬁrstorder second-order information i.e. gradient hessian loss function respectively. signiﬁcant diﬀerence classes class algorithms need access information current step i.e. class algorithms entire history step i.e. {d}t− {s}t however discuss sec. below possible store informa.. convolutional form. convolutional sparse coding highly structured sparse representation model recently attracted increasing attention variety imaging inverse problems coeﬃcient maps size signal since implement convolutions frequency domain computational eﬃciency convenient adopt circular boundary conditions convolution operation. masking matrix usually }-valued matrix masks unknown unreliable pixels operator denotes pointwise multiplication. batch learning methods alternatively minimize {xkm} {dm} dealing entire training iteration. large update subproblem computationally expensive e.g. single step complexity memory usage current state-of-the computationally contribution article. goal present work develop online convolutional dictionary learning methods training data sets much larger presently feasible. develop online methods directions ﬁrst-order method second-order method. contribution article includes analysis forgetting factor used algorithm analysis stopping condition d-update. analysis eﬀects circular boundary conditions dictionary learnrelationship works. recently works online appeared. study second-order methods. framework diﬀerent methods update uses projected coordinate descent uses iterated sherman-morrison update previous work uses frequency-domain fista update forgetting factor technique inspired correct surrogate function uses region-sampling reduce memory cost. paper second-order algorithm algorithm improves algorithm introducing additional techniques improved stopping condition fista image-splitting. former technique greatly reduces number inner-loop iterations d-update. latter compared region-sampling fully utilizes information training set. techniques algorithm converges faster algorithm method designed diﬀerent problem therefore directly comparable methods. recent paper online appeared completing work proposed algorithm uses framework previous work therefore expected oﬀer similar performance initial method. online methods standard dictionary learning problems propose online frameworks problem ﬁrst order method based projected stochastic gradient descent direct application methods problem computationally expensive propose number techniques reduce time memory usage. details discussed sections respectively. techniques calculate operator introducing algorithms consider basic problem computational techniques used section well sections ﬁxed basic problem computing sparsity property. ﬁrst option utilize sparsity speciﬁcally saved triple array records indices values non-zero elements nonzero entries contribute computational time. triple array commonly referred coordinate list standard representing sparse matrix. column circular shift dimension dictionary kernel. thus density same. assuming density vector number nonzero entries operator giving single step complexity computing computing frequency domain. another option utilize structure well known convolving signals size corresponds pointwise multiplication frequency representations. method takes advantage property. first zero-pad match size basic problem written modiﬁed method iterate transformed frequency spatial domains gradient cheaper compute frequency domain projection cheaper compute spatial domain. means conjugate cogradient equivalent gradient thus modiﬁed coincides standard using conjugate cogradient. proof given appendix similar result also given parameter step size given deﬁnition partial derivative respect optimal i.e. deﬁned thus compute gradient ﬁrst compute coeﬃcient maps training signal dictionary given compute gradient complexity analysis algorithm list single-step complexity memory usage diﬀerent options table frequency-domain update sparse matrix technique reduce single-step complexities. comparison computational techniques depends sparsity dictionary kernel size section numerically compare methods. convergence algorithm algorithm equivalent standard projected sgd. thus properly choosing step sizes algorithm converges stationary point diminishing step size rule used dictionary learning works convergence performance diﬀerent step sizes numerically tested section second-order method algorithm section ﬁrst introduce details directly applying second order stochastic approximation method problems discuss issues resolutions. section computationally tractable. update eﬃciently introduce surrogate function given computed cbpdn using latest dictionary surrogate given inaccurate loss function surrogate function involves loss functions contain information x··· example computed using large single step complexity memory usage handling whole image fista slow solving subproblem fista takes many steps reach improvement forgetting factor. time dictionary result accumulation past coeﬃcient maps computed then-available dictionaries. balance accumulated past contributions information provided training samples compute weighted combination contributions combination gives eﬀect forgetting exponent small tends lead stable algorithm since training signals given nearly equal weights stochastic approximation small variance. propositions give theoretical explanations phenomenon. however small leads inaccurate surrogate loss function since gives large weights information. extreme case modiﬁed surrogate function reduces standard previous work sample small regions whole signals limited memory algorithm performs worse algorithm training whole signals. claimed performance sacriﬁce caused circular boundary condition. fact caused sampling. paper sample small regions random center position ﬁxed size. sample small regions parts image sampled sampled several times. consequently present paper propose imagesplitting technique algorithm avoids issue. shows worse performance splitting size smaller threshold actually caused boundary condition. boundary issues. circular boundary conditions signals periodic potential introduce boundary artifacts representation therefore also learned dictionary size training images much larger kernels evidence eﬀect learned dictionary negligible reasonable expect eﬀects become pronounced smaller training images regions obtain using small splitting size possibility severe artifacts image size approaches kernel size illustrated fig. sec. study eﬀect show using splitting size twice kernel size dimension suﬃcient avoid artifacts expected argument illustrated fig. improvement stopping fista early. another issue surrogate function method stopping condition fista. small ﬁxed tolerance result many inner-loop iterations initial steps. another strategy used spams ﬁxed number inner-loop iterations theoretical convergence guarantee. article propose diminishing tolerance scheme subproblem solved inexactly online learning algorithm still theoretically guaranteed converge. stopping accuracy increasing increases. speciﬁcally stopping tolerance decreased increases. moreover warm start initial solution step) number inner-loop iterations stays moderate increases validated results fig. reasons. simplicity; fista used solve metric computed directly directly operator linear operator thus complexity computing hessian matrix memory cost computing reduced density sparse matrix memory cost still sparse although comparison solve frequency domain frequency-domain operator linear operator seems lead larger complexity compute hessian ﬂops memory cost. however since component diagonal frequency-domain product non-zero values. number ﬂops memory cost complexities listed single step complexity memory usage algorithm signal dimension; number dictionary kernels; size kernel; average density coeﬃcient maps; average loops fista step. numerically tested table assumption easily guaranteed normalizing training signal. assumption common assumption dictionary learning linear regression papers practically must guaranteed choosing suﬃciently large penalty parameter larger penalty parameter leads sparser compared lemma shows convergence rate surrogate function method exact d-update proposition shows inexact d-update shares rate. since inexact version stops fista earlier faster. proof proposition given appendix numerical results learning clean data set. experiments computed using matlab running workstation intel xeon cpus clocked .ghz. implementations algorithms available matlab version sporco software library included values obtained computing cbpdn test set. smaller functional value indicates better dictionary. similar methods evaluate dictionary also used dictionary learning works regularization parameter chosen training consists images selected mirflickr-m dataset test consists diﬀerent images source. images used originally size accelerate experiments crop part yield training testing images pre-processed dividing rescale pixel values range highpass ﬁltering. work solve convolutional sparse coding step using admm algorithm adaptive penalty parameter scheme stopping condition primal dual normalized residuals less relaxation parameter fig. large ﬁxed step size used functional value decreases fast initially becomes unstable later smaller step size causes opposite. diminishing step size balances accuracy convergence rate. second test computational techniques table shows. techniques reduce complexity updating option better memory cost option better calculation time. fig. shows objective values versus training time. actual image data contained dataset resolution since dataset primarily targeted image classiﬁcation tasks. original images used derived obtained downloading original images flickr used derive mirflickr-m images. pre-processing applied inability standard model eﬀectively represent low-frequency/large-scale image components case highpass component computed diﬀerence input signal lowpass component computed tikhonov regularization gradient term regularization parameter fig. tuning step size modiﬁed learning epoch pass whole training set. large ﬁxed algorithm converges fast ﬁrst becomes unstable later small ﬁxed algorithm converges slowly. diminishing step size provides good balance. validation algorithm algorithm test four techniques separately forgetting exponent image splitting size stopping tolerance fista computational techniques give accurate solution. fig. shows that curve monotonic small oscillation converges higher functional value. larger algorithm converges lower functional values. large validation improvement image splitting size boundary artifacts. section convergence comparisons shown fig. dictionaries obtained diﬀerent displayed fig. experiments consider square signals square dictionary kernels algorithm converges good functional value without image-splitting. however smaller threshold algorithm converges higher functional value implies worse dictionaries. thus conclude splitting size least twice dictionary kernel size dimension. otherwise lead boundary artifacts. phenomenon consistent discussion section artifacts speciﬁcally displayed fig. smaller threshold features learned incomplete. fig. eﬀect technique algorithm learning epoch pass whole training set. boundary artifacts become signiﬁcant splitting region size smaller twice dictionary kernel size. kernel size threshold section studies eﬀect boundary artifacts image-splitting objective functional values. table shows also helps reducing computing time memory cost numerically validated section validation improvement stopping tolerance fista section fig. shows eﬀect using diﬀerent using small stopping tolerance leads good functional value large number fista iterations large tolerance leads large functional value small number fista iterations. consider proposed diminishing tolerance rule ./t. algorithm starts algorithm based results fig. diminishing tolerance avoids large number fista loops especially initial steps losing little accuracy ﬁnal objective close validation improvement computational techniques. section compare calculation time memory usage spatial-domain update frequency-domain update. table illustrates image-splitting helps reduce single-step complexity memory usage option option option advantage smaller splitting size signiﬁcant option option much better option single step time option comparable option reason that option reducing helps reduce single-step time cost cbpdn updating hessian matrix loops fista help reduce time cost single-step time cost fista. however option image-splitting reduces three complexities also reduces single-step complexity fista. furthermore option uses much less memory fig. fig. compare objective functional value versus time. fig. indicates reducing help option table shows smaller reduces single step complexity also reduces gain step smaller splitting size leads less information used training. trade-oﬀ. option contrast beneﬁts smaller seen fig. table although splitting training image reduces gain step beneﬁt overwhelms loss. thus option smaller splitting size better long larger threshold boundary artifacts. main result convergence speed. section study convergence speeds methods clean data without masking operator. compare methods leading batch learning algorithms method papyan uses k-svd updates dictionary spatial domain algorithm uses admm consensus dictionary update computed frequency domain. batch learning algorithms test subsets images selected training set. online learnmethods frequency-domain dictionary update scheme. plot prev. online refers algorithm onlinesamp proposed online algorithms converge faster batch methods algorithms since scalable size training test methods whole training images. parameters tuned follows. batch learning algorithm software released batch learning algorithm adaptive penalty parameter scheme modiﬁed performance comparison batch online methods presented fig. advantage online learning signiﬁcant obtain functional value test batch learning takes hours previous method takes around hours algorithm option takes around hour algorithm algorithm option takes less hour. conclude that modiﬁed surrogate-splitting converge faster batch learning algorithms previous method. main result dictionaries obtained diﬀerent algorithms. fig. display dictionaries obtained algorithms section small training images leads random kernels dictionaries. training containing images works much better. algorithms learn comparable dictionaries within much less time much less memory usage numerical results learning noisy data set. section learn dictionaries noisy images. test algorithms training salt-and-pepper impulse noise known pixel locations. apply noise pixels fig. shows. data section images pre-processed applying highpass ﬁlter computed diﬀerence input non-linear lowpass ﬁlter. number noisy pixels without masking still learn features fig. demonstrates. number noisy pixels fig. learning noisy training speed test. algorithms option batch refers iterated sherman-morrison dictionary update mask decoupling technique incorporating mask mask decoupling technique additive mask simulation solve sparse coding step masked objective function. parameters algorithms chosen similarly section algorithm choose option algorithm choose option comparison functional values noise-free test shown fig. online algorithms converge much faster stably. algorithms take around hour converge mask-decoupling scheme requires hours. unlike experiments section algorithms epoch i.e. true online learning. results fig. demonstrate algorithm feasible large data set. ﬁrst order method algorithm cheaper single step learns less number iterations. second order method algorithm converse behavior achieveing slightly smaller functional value number iterations. finally fig. shows algorithms similar performance terms functional value test respect training time. result consistent section good performance time memory usage. compared recent online works framework diﬀerent d-update algorithms second-order method improves framework several practical techniques. ﬁrst-order method best knowledge ﬁrst attempt ﬁrst order methods online cdl. shows better performance time memory usage requires fewer parameters tune. moreover based methods also proposed online dictionary learning method able learn meaningful dictionaries partially masked training set. although single-channel images considered article online methods easily extended multichannel case appendix frequency-domain fista. solve propose frequency-domain fista algorithm calculates gradient frequency domain projection extrapolation spatial domain. mathematically speaking illustrates frequency-domain fista actually equivalent standard fista. however calculating convolutional operator frequency domain reduces computing time. thus algorithm faster. although studies standard sparse coding uniqueness condition applied convolutional case condition proof convex function minimum assumption convexity function proof. deﬁne sequence random variables ipzi. expectations variances respectively. apply lyapunov central limit theorem stochastic sequence {yi}. first check lyapunov implies generated algorithm quasi-martingale. thus results converges almost surely. proofs using results proposition paper following proof line proposition obtain results aharon elad bruckstein k-svd algorithm designing overcomplete dictionaries sparse representation ieee transactions signal processing doi./tsp... bertsekas nonlinear programming athena scientiﬁc billingsley probability measure john wiley sons bottou on-line learning stochastic approximations degraux kamilov boufounos online convolutional dictionary learning multimodal proceedings ieee international conference image processing beijing china sept. doi./icip... engan aase husøy method optimal directions frame design proceedings ieee international conference acoustics speech signal processing vol. doi./icassp... garcia-cardona wohlberg subproblem coupling convolutional dictionary learning proceedings ieee international conference image processing beijing china sept. doi./icip... heide heidrich wetzstein fast ﬂexible convolutional sparse coding proceedings ieee conference computer vision pattern recognition doi./cvpr... huiskes thomee trends ideas visual concept detection flickr retrieval evaluation initiative proceedings international conference multimedia information retrieval doi./.. jenatton mairal bach obozinski proximal methods sparse hierarchical dictionary learning proceedings international conference machine learning kasiviswanathan wang banerjee melville online l-dictionary learning application novel document detection advances neural information processing systems lewicki sejnowski coding time-varying signals using sparse shift-invariant representations advances neural information processing systems kearns solla cohn eds. vol. garcia-cardona wohlberg online convolutional dictionary learning proceedings ieee international conference image processing beijing china sept. doi./icip... papyan sulam elad working locally thinking globally theoretical guarantees convolutional sparse coding ieee transactions signal processing doi./tsp... slavakis giannakis online dictionary learning data using accelerated stochastic approximation algorithms proceedings ieee international conference acoustics speech signal processing doi./icassp... wang yang huang gong locality-constrained linear coding image classiﬁcation proceedings ieee conference computer vision pattern recognition doi./cvpr... wang wang d.-y. yeung online robust non-negative dictionary learning visual tracking proceedings ieee international conference computer vision doi./iccv... wohlberg convolutional sparse representation color images proceedings ieee southwest symposium image analysis interpretation santa mar. doi./ssiai... wohlberg sporco python package standard convolutional sparse representations proceedings python science conference austin july doi./shinma-fce-. wright yang ganesh sastry robust face recognition sparse representation ieee transactions pattern analysis machine intelligence doi./tpami... zhang jiang davis online semi-supervised discriminative dictionary learning sparse representation proceedings asian conference computer vision doi./---- zhang patel convolutional sparse low-rank coding-based rain streak removal proceedings ieee winter conference applications computer vision mar. doi./wacv... zhang kasiviswanathan yuen harandi online dictionary learning symmetric positive deﬁnite manifolds vision applications proceedings aaai conference artiﬁcial intelligence", "year": "2017"}