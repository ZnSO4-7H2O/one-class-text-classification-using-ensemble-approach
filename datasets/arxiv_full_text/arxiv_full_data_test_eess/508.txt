{"title": "Learning Hidden Markov Models from Pairwise Co-occurrences with  Application to Topic Modeling", "tag": "eess", "abstract": " We present a new algorithm for identifying the transition and emission probabilities of a hidden Markov model (HMM) from the emitted data. Expectation-maximization becomes computationally prohibitive for long observation records, which are often required for identification. The new algorithm is particularly suitable for cases where the available sample size is large enough to accurately estimate second-order output probabilities, but not higher-order ones. We show that if one is only able to obtain a reliable estimate of the pairwise co-occurrence probabilities of the emissions, it is still possible to uniquely identify the HMM if the emission probability is \\emph{sufficiently scattered}. We apply our method to hidden topic Markov modeling, and demonstrate that we can learn topics with higher quality if documents are modeled as observations of HMMs sharing the same emission (topic) probability, compared to the simple but widely used bag-of-words model. ", "text": "present algorithm identifying transition emission probabilities hidden markov model emitted data. expectation-maximization becomes computationally prohibitive long observation records often required identiﬁcation. algorithm particularly suitable cases available sample size large enough accurately estimate second-order output probabilities higher-order ones. show able obtain reliable estimate pairwise co-occurrence probabilities emissions still possible uniquely identify emission probability sufﬁciently scattered. apply method hidden topic markov modeling demonstrate learn topics higher quality documents modeled observations hmms sharing emission probability compared simple widely used bag-of-words model. hidden markov models widely used machine learning data samples time dependent example speech recognition language processing video analysis. graphical model shown figure models sequence data {yt}t indirect observations underlying markov chain {xt}t available homogeneous hmms parsimonious models sense fully characterized transition probability emission probability even though size given data {yt}t assuming know underlying transition emission probabilities calculate observation likelihood using forward algorithm estimate likely hidden sequence using viterbi algorithm compute posterior probability hidden states using forward-backward algorithm. natural problem setting however neither hidden state sequence underlying probabilities known us—we access sequence observations reveal structure expectation-maximization rabiner juang expectation step performed calling forward-backward algorithm. speciﬁc instance also called baum-welch algorithm baum ghahramani however complexity baumem converges slowly forward-backward algorithm must called many times. critical issue learned high accuracy number observation samples large enough. designing scalable algorithms learning hmms work sufﬁcient statistics—a summary given observation sequence whose size grow throughout paper assume process stationary true almost surely underlying markov process ergodic process going reasonable amount time. large enough accurately estimate co-occurrence probability consecutive emissions according graphical model shown figure easy given value conditionally independent variables leading factorization noticing nonnegative matrix tri-factorization number inconsequential constraints properly represent probabilities vanluyten lakshminarayanan raich cybenko crespi proposed using nonnegative matrix factorization estimate probabilities. however nmfbased methods serious shortcoming context tri-factorization general unique number works propose tensor methods overcome identiﬁability issue. instead working pairwise co-occurrence probabilities start estimating joint probabilities three consecutive observations rank harshman context hmms equivalent assuming linearly independent columns relatively mild condition. known unique much relaxed conditions sidiropoulos order uniquely retrieve transition probability using relationship actually best achieve using triple-occurrences without making assumptions. salient feature case triple-occurrence probability exactly given efﬁciently calculated using generalized eigendecomposition related algebraic methods sanchez kowalski leurgans lathauwer methods work well however low-rank tensor perturbed; e.g. insufﬁcient mixing sample averaging triple occurrence probabilities. also possible handle cases observation that given conditionally independent yt−τ yt+τ then grouping yt−τ single categorical variable taking possible outcomes yt+τ another construct much bigger tensor size uniquely identify underlying structure long certain linear independence requirements satisﬁed conditional distribution grouped variables allman bhaskara huang sharan intuitively clear ﬁxed need much larger realization length order accurately estimate -occurrence probabilities grows price need learning larger number hidden states. large enough accurately estimate triple higher-order occurrence probabilities. prove actually possible recover latent structure pairwise co-occurrence probabilities provided underlying emission probability sufﬁciently scattered. compared existing nmf-based learning approaches formulation employs different criterion ensure identiﬁability parameters. matrix factorization approach resolves cases cannot handled tensor methods namely insufﬁcient estimate third-order probabilities additional condition quite mild emission probability matrix must sufﬁciently scattered rather simply full column-rank. apply method hidden topic markov modeling gruber case number hidden states indeed much smaller number ambient states htmm goes beyond simple widely used bag-of-words model assuming words document emitted hidden topic sequence evolves according markov model. show improved performance real data using simple intuitive model take word ordering account learning topics also beneﬁts identiﬁability guaranteed matrix factorization method. illustrative example showcase inferred topic word news article figure taken reuters data obtained mimaroglu htmm gets much consistent smooth inferred topics compared obtained bag-of-words model result agrees human understanding. supplementary material prove emission probability generic transition probability sparse uniquely identiﬁed triple-occurrence probability using latest tensor identiﬁability result chiantini ottaviani china daily vermin grain stocks survey provinces cities showed vermin consume china grain stocks china daily year tonnes china fruit output left tonnes vegetables paper blamed waste inadequate storage preservation methods government launched national programme reduce waste calling improved technology storage preservation greater production additives paper gave details china daily vermin grain stocks survey provinces cities showed vermin consume china grain stocks china daily year tonnes china fruit output left tonnes vegetables paper blamed waste inadequate storage preservation methods government launched national programme reduce waste calling improved technology storage preservation greater production additives paper gave details triple-occurrences taking averages change expectation. however individual terms summation independent other making hard determine fast estimates converge expectation. state-of-the-art concentration result hmms kontorovich states -lipschitz function arguments made previous section motivate going back matrix factorization methods learning realization length large enough obtain accurate estimates triple co-occurrence probabilities. explained co-occurrence probability matrix admits nonnegative matrix tri-factorization model number additional equality constraints. columns represent conditional distributions matrix represents joint distribution consecutive markovian variables therefore furthermore represent respectively since assume markov chain stationary same i.e. notice imply symmetric fact often symmetric. deﬁnition cone∗ denote polyhedral cone denote elliptical cone ⊤x}. matrix called sufﬁciently scattered satisﬁes that cone∗ cone∗ {λek denotes boundary ⊤x}. sufﬁciently scattered condition ﬁrst proposed huang establish uniqueness conditions widely used nonnegative matrix factorization model sufﬁciently scattered nonnegative decomposition unique column permutation scaling. follow work strengthened extended identiﬁability results based geometry inspired condition huang similar tri-factorization model considered huang context bag-of-words topic modeling shown among feasible solutions notice huang constraints core matrix terms identiﬁability easy ground-truth satisﬁes solving even without produce solution satisﬁes thanks uniqueness. practice given less accurate redundant information help improve estimation error goes beyond identiﬁability consederations. proof theorem referred huang provide insights geometryinspired sufﬁciently scattered condition discuss reasonable assumption. notation cone∗ comes convention convex analysis dual cone conical hull vectors i.e. cone {m⊤α similarly derive dual cone ⊤x/√k useful property dual cone convex cones therefore ﬁrst requirement sufﬁciently scattered deﬁnition equivalently means give geometric illustration sufﬁciently scattered condition figure focus -dimensional plane intersection plane nonnegative orthant probability intersecting plane respectively. rows scaled represented black dots figure conical hull represented shaded region. polygon dashed lines represents also establish uniqueness nmf. case coordinate vectors appear rows therefore cone equals nonnegative orthant. makes sense condition makes identiﬁcation problem easier also restrictive assumption. sufﬁciently scattered condition hand requires shaded region contains inner circle shown figure intuitively requires rows well scattered probability simplex extent separable. separability-based identiﬁcation considered barlier glaude however construct second-order statistics different ours. figure shows case sufﬁciently scattered also happens case identiﬁable. sufﬁciently scattered necessary enough rows boundary nonnegative orthant i.e. relatively sparse. speciﬁcally sufﬁciently scattered column contains proof given supplementary material. grows larger volume ratio goes zero superexponential decay rate. implies volume inner sphere quickly becomes negligible compared volume probability simplex becomes moderately large. take home point that practical identiﬁability analysis based sufﬁciently scattered condition poses interesting non-convex optimization problem first given co-occurrence probability exact therefore good idea loss function ﬁrst term kullback-leibler distance empirical probability parameterized version second term identiﬁability-driven regularization. need tune parameter yield good estimation results. however intuitively relatively small value. suppose sufﬁciently accurate priority minimize difference θm⊤; exist equally good second term comes play helps pick solution sufﬁciently scattered. noticing constraints convex loss function propose design iterative algorithm solve using successive convex approximation. iteration updates deﬁne update simple closed form expression still obtained efﬁciently. noticing nonnegativity constraint implicitly implied individual functions loss function propose solve using newton’s method equality constraints although newton’s method requires solving linear system equations number variables iteration special structure exploit reduce per-iteration complexity hessian loss function diagonal linear equality constraints highly structured; using block elimination ultimately need solve positive deﬁnite linear system variables. together desired accuracy update. noticing complexity naive implementation newton’s method would difference moderately large in-line implementation tailored newton’s entire proposed algorithm solve problem summarized algorithm notice additional line-search step ensure decrease loss function. constraint convex line-search step incur infeasibility. computationally operation involves nℓkj carried succinctly since optimizing non-convex problem propose method huang obtain initialization best start feasible point simple choice scaling matrix one. finally show algorithm converges stationary point problem proof relegated supplementary material based razaviyayn section validate identiﬁability performance synthetic data. case underlying transition emission probabilities generated synthetically compare estimated ones evaluate performance. simulations conducted matlab using toolbox includes functions generate observation sequences given transition emission probabilities well implementation baum-welch algorithm baum i.e. algorithm estimate transition emission probabilities using observations. unfortunately even moderate problem sizes considered streamlined matlab implementation baum-welch algorithm able execute within reasonable amount time performance included here. baselines compare plain approach using multiplicative update figure total variation difference ground truth estimated transition probability emission probability total variation difference emission probabilities calculated since column matrices indicates probability total variation difference equal half l-norm; similarly transition probabilities rescaling rows one. result averaged random problem instances. vanluyten tensor approach sharan using simultaneous diagonalization tensorlab vervliet since work empirical distributions instead exact probabilities result simultaneous diagonalization going optimal. therefore initialize algorithm ﬁtting nonnegative tensor factorization divergence loss shashanka reﬁnement. focus cases number hidden states smaller number observed states explained introduction even seemingly easier case known guarantee unique recovery parameters pair-wise co-occurrence probability. known tensor approach able guarantee identiﬁability given exact triple-occurrence probability. demonstrate section much harder obtain accurate triple-occurrence probability comparing co-occurrence probability. result sufﬁciently scattered assumption holds emission probability estimated parameters obtained method always accurate obtained tensor cpd. matrix size followed row-normalization; emission probabilities approximately entries random exponential matrices zero normalizing columns shown satisfy sufﬁciently scattered condition high probability huang number realizations compare estimation error transition matrix emission matrix aforementioned methods. show total variation distance ground truth probabilities proposed method indeed works best obtaining almost perfect recovery sample size based method work well since cannot obtain accurate estimates third-order statistics needs. initialized improves performance still away proposed method. working well since identiﬁability case. analyzing text data core application domains machine learning. prevailing approaches model text data. classical bag-of-words model assumes word independently drawn certain multinomial distributions. distributions different across documents efﬁciently summarized small number topics mathematically modeled distributions words; task widely known topic modeling hofmann blei however obvious bag-of-words representation oversimpliﬁed. n-gram model hand assumes words conditionally dependent window-length seems much realistic model although choice totally unclear often dictated memory computational limitations practice—since size joint distribution grows exponentially more somewhat difﬁcult extract topics model despite preliminary attempts wallach wang propose model document realization topics hidden states emitting words states evolving according markov chain hence name hidden topic markov model documents means working collection hmms. similar topic modeling works assume topic matrix shared among documents meaning given hmms share emission probability. bag-of-words model document speciﬁc topic distribution whereas model document topic transition probability previous discussion row-sum column-sum same also topic probability speciﬁc document. difference markovian assumption topics rather over-simplifying independence assumption. immediate advantages htmm. since markovian assumption imposed topics exposed observations independent other agrees intuition. hand understand although word dependencies exist wide neighborhood need work pair-wise co-occurrence probabilities -grams. releases picking window length n-gram model maintaining dependencies words well beyond neighborhood words. also includes bag-of-words assumption special case topics words indeed independent means transition probability special form p⊤d. closest work gruber also termed hidden topic markov model. however make simplifying assumption transition illustrate performance htmm comparing three popular bag-of-words topic modeling approaches plsa hofmann blei fastanchor arora guarantees identiﬁability every topic characteristic anchor word. htmm model guarantees identiﬁability topic matrix sufﬁciently scattered relaxed condition anchor word one. reuters data obtained mimaroglu document construct word co-occurrence statistics well bag-of-words representations document baseline algorithms. version stop-words removed makes htmm model plausible since syntactic dependencies removed leaving semantic dependencies. vocabulary size reuters around making method relying triple-occurrences impossible implement tensor-based methods compared here. page limitations show quality topics learned various methods terms coherence. simply higher coherence means meaningful topics concrete deﬁnition found arora supplementary material. figure different number topics tried entire dataset htmm consistently produces topics highest coherence. additional evaluations found supplementary material. presented algorithm learning hidden markov models unsupervised setting i.e. using sequence observations. approach guaranteed uniquely recover ground-truth structure using pairwise cooccurrence probabilities observations assumption emission probability sufﬁciently scattered. unlike complexity proposed algorithm grow length observation sequence. compared tensor-based methods learning approach requires reliable estimates pairwise co-occurrence probabilities easier obtain. applied method topic modeling assuming document realization rather simpler bag-of-words model obtained improved topic coherence results. refer reader supplementary material detailed proofs propositions additional experimental results. assuming full rank system solved elimination described suppose rm×n diagonal cost calculating ∆ntx dominated forming inverting matrix ada⊤ diagonal. matrix full rank last implied rest. therefore discard last equality constraint. keep calculating matrix multiplications simpler expression discard corresponding entry column/row operations. form algorithm falls exactly framework block successive convex approximation algorithm proposed razaviyayn block variables. invoking every limit point algorithm stationary point problem additionally since constraint problem compact sub-sequence limit point also stationary point. proves q.e.d. algorithm converges stationary point problem section conduct similar synthetic experiment identify parameters much smaller problem size include classical baum-welch algorithm another baseline. fixing transition probabilities synthetically generated random exponential matrix size followed row-normalization; emission probabilities part random exponential matrices identity matrix column normalization guaranteed sufﬁciently scattered. number realizations compare estimation error transition matrix emission matrix aforementioned methods. show total variation distance similar experiment shown main paper proposed method works best terms estimating parameters without sacriﬁcing much computational times. much one’s surprise baum-welch algorithm working well terms estimation error. possibly limit maximum number iterations enough convergence. expected computational time baum-welch grows linearly respect length observations methods independent means problem size considered main paper iteration baum-welch takes approximately minutes hours depending realization length. clearly feasible practice. also present elapsed time four algorithms excluding baum-welch algorithm case considered main paper i.e. similar timing result shown figure proposed method takes longest time compared three signiﬁcantly; also recall propose method works considerably better terms estimation accuracy. main body paper showed htmm able learn topics higher quality using pairwise word cooccurrences. quality topics evaluated using coherence deﬁned follows. topic figure total variation difference ground truth estimated transition probability emission probability elapsed time total variation difference emission probabilities calculated since column matrices indicates probability total variation difference equal half l-norm; similarly transition probabilities rescaling rows one. result averaged random problem instances. intuition high probability appearing topic high probability co-occurring document well; hence higher value coherence indicates indicative topic. coherence individual topics averaged coherence entire topic matrix. smaller perplexity means probability model data better. seen figure htmm gives smallest perplexity. notice since htmm takes word ordering account fair methods take bag-of-words representation documents. bag-of-words model essentially multinomial whose n...nk different combinations observation orderings. case factor includes scaling factor included since know word ordering document. htmm log-likelihood calculated efﬁciently using forward algorithm. result surprising. even using topic matrix bag-of-words model tries k-dimensional representation document whereas htmm looks -dimension representation. wonder causing over-ﬁtting argue not. first terms coherence htmm learns topic matrix higher quality. learning feature representations document showcase following result. topic-word probabilities topic weights topic transition probability infer underlying topic word. bag-of-words models word probable topic document matter appears. htmm learn transition emission probability topic word optimally estimated using viterbi algorithm. speciﬁc news article reuters data topic inference given plsa china daily vermin grain stocks survey provinces cities showed vermin consume china grain stocks china daily year tonnes china fruit output left tonnes vegetables paper blamed waste inadequate storage preservation methods government launched national programme reduce waste calling improved technology storage preservation greater production additives paper gave details china daily vermin grain stocks survey provinces cities showed vermin consume china grain stocks china daily year tonnes china fruit output left tonnes vegetables paper blamed waste inadequate storage preservation methods government launched national programme reduce waste calling improved technology storage preservation greater production additives paper gave details generic transition probabilities linearly independent other conditional probability contains nonzeros. uniquely identiﬁed tripleoccurrence probability permutation hidden states proof. clear identiﬁability holds focus case", "year": "2018"}