{"title": "Learning to Optimize: Training Deep Neural Networks for Wireless  Resource Management", "tag": "eess", "abstract": " For the past couple of decades, numerical optimization has played a central role in addressing wireless resource management problems such as power control and beamformer design. However, optimization algorithms often entail considerable complexity, which creates a serious gap between theoretical design/analysis and real-time processing. To address this challenge, we propose a new learning-based approach. The key idea is to treat the input and output of a resource allocation algorithm as an unknown non-linear mapping and use a deep neural network (DNN) to approximate it. If the non-linear mapping can be learned accurately by a DNN of moderate size, then resource allocation can be done in almost real time -- since passing the input through a DNN only requires a small number of simple operations.  In this work, we address both the thereotical and practical aspects of DNN-based algorithm approximation with applications to wireless resource management. We first pin down a class of optimization algorithms that are `learnable' in theory by a fully connected DNN. Then, we focus on DNN-based approximation to a popular power allocation algorithm named WMMSE (Shi {\\it et al} 2011). We show that using a DNN to approximate WMMSE can be fairly accurate -- the approximation error $\\epsilon$ depends mildly [in the order of $\\log(1/\\epsilon)$] on the numbers of neurons and layers of the DNN. On the implementation side, we use extensive numerical simulations to demonstrate that DNNs can achieve orders of magnitude speedup in computational time compared to state-of-the-art power allocation algorithms based on optimization. ", "text": "notation. denote indicator function function takes value takes value denote normal distribution mean variance rectiﬁed linear unite deﬁned relu max{ binary unit denoted binary returns evaluates true returns otherwise. denote nonnegative numbers. consider following basic interference channel power control problem wireless network consisting single-antenna transceivers pairs. denote direct channel transmitter receiver denote interference channel transmitter receiver channels assumed constant resource allocation slot. furthermore assume based schemes modify algorithm work real domain. speciﬁcally ﬁrst observe rate function remains unchanged replaced |hkj|. using observation show problem equivalent following given training data points simple three layer neural network approximate relationship characterizes behavior unfortunately network learns model outputs zero regardless inputs are; figure fig. illustration different models learned iteration solving problem blue curves ﬁgures represent input output relationship iterations curves represent learned neural networks. chosen generating training samples. left model learned approximating relationship right model learned approximating relationship continuous mapping representing iteration; problem parameter; represent parameter space feasible region problem respectively. following result extension well-known universal approximation maximization problem initialization input feature necessary since without mapping well deﬁned alternatively also possible learn mapping ﬁxed initialization optimization variable lies compact set. therefore assuming channel realizations {hij} lies compact using theorem conclude wmmse approximated arbitrarily well feedforward network single hidden layer. main steps analysis follows construct simple neural networks consist relus binary units approximate multiplication division operations; compose small neural networks approximate rational function representing iteration algorithm; concatenate rational functions approximate entire algorithm; bounding error propagated using four lemmas obtain main theoretical result section. concisely state result make following deﬁnitions. given input channel vector {hij} denote variable iteration generated wmmse. also hmin hmax remark admissible deﬁned mainly ensure channels compact set. admittedly conditions channel realizations lower bounded hmin vmin somewhat artiﬁcial needed bound error propagation. worth noting problem isolated local maximum solution fig. structure used work. fully connected neural network input layer multiple hidden layers output layer. hidden layers relu activation function output layer pmax) incorporate power constraint. approximation error size network rather minor however want point numbers predicted theorem represent upper bounds size network. practice much smaller networks often used achieve multiple hidden layers output layer shown fig. input network magnitude channel coefﬁcients {|hkj|} output network power allocation {pk}. further relu activation function hidden layers activation. additionally enforce used denote index training sample. simplicity pmax then generate corresponding optimized power vectors tuple pmax initialization objnew objold running wmmse referred number iterations termination criteria. tuple training sample. repeat process multiple times generate entire training data selection early stopping training stage. typically size validation data small compared training set. collect indices training validation sets respectively. training stage. entire training data })i∈t optimize weights neural network. cost function mean squared error label output network. optimization algorithm efﬁcient implementation mini-batch model widely used simulate performance various resource allocation algorithms. experiment consider three different network setups model imac. practical consideration multi-cell interfering model considered fig. network conﬁguration imac users. triangles represent black hexagons represent boundaries cell colored circles represent user locations; shows case users located uniformly fig. parameter selection gaussian case mses evaluated validation set. larger batch size leads slower convergence smaller batch size incurs unstable convergence behavior. larger learning rate leads following schemes wmmse; random power allocation strategy simply generates power allocation uniform maximum power allocation pmax latter schemes serve heuristic baselines. simulation results shown fig. gaussian imac. curve ﬁgure represents result obtained averaging fig. cumulative distribution function describes rates achieved different algorithms randomly generated testing data points shows gaussian shows imac relative computational performance imac fact that number users imac different input channel coefﬁcients therefore built imac smaller size compared contrary computation respectively. randomly generate direct validation data denoted interfering channels form training data sample. repeating process generate training samples", "year": "2017"}