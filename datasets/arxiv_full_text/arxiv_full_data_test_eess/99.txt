{"title": "BridgeNets: Student-Teacher Transfer Learning Based on Recursive Neural  Networks and its Application to Distant Speech Recognition", "tag": "eess", "abstract": " Despite the remarkable progress achieved on automatic speech recognition, recognizing far-field speeches mixed with various noise sources is still a challenging task. In this paper, we introduce novel student-teacher transfer learning, BridgeNet which can provide a solution to improve distant speech recognition. There are two key features in BridgeNet. First, BridgeNet extends traditional student-teacher frameworks by providing multiple hints from a teacher network. Hints are not limited to the soft labels from a teacher network. Teacher's intermediate feature representations can better guide a student network to learn how to denoise or dereverberate noisy input. Second, the proposed recursive architecture in the BridgeNet can iteratively improve denoising and recognition performance. The experimental results of BridgeNet showed significant improvements in tackling the distant speech recognition problem, where it achieved up to 13.24% relative WER reductions on AMI corpus compared to a baseline neural network without teacher's hints. ", "text": "many end-to-end methods proposed overcome issue front-end approaches jointly optimizing multiple components uniﬁed framework. among them discuss popular approaches relevant method. multi-task denoising jointly optimizes denoising recognition sub-networks using synchronized clean data. minimizes weighted loss functions crossentropy loss recognition sub-network output mean square error loss denoising sub-network output clean data. although multi-task denoising showed improvements acoustic models minimizing acoustic data high-level abstracted features often unsuccessful. performance depends heavily underlying acoustic models. knowledge distillation transfers generalization ability bigger teacher network typically much smaller student network. provides soft-target information computed teacher network addition hard-targets student network learn generalize similarly. generalized distillation extends distillation methods training teacher network separate clean data. student network trained noisy data time guided soft-labels teacher access synchronized clean speech. generalized distillation methods showed decent performance chime aurora corpora. bridgenet provides multiple hints teacher network. methods utilize teacher’s soft labels. bridgenet provides teacher’s intermediate feature representations additional hints properly regularize student network learn signal denoising. despite remarkable progress achieved automatic speech recognition recognizing far-ﬁeld speeches mixed various noise sources still challenging task. paper introduce novel student-teacher transfer learning bridgenet provide solution improve distant speech recognition. features bridgenet. first bridgenet extends traditional student-teacher frameworks providing multiple hints teacher network. hints limited soft labels teacher network. teacher’s intermediate feature representations better guide student network learn denoise dereverberate noisy input. second proposed recursive architecture bridgenet iteratively improve denoising recognition performance. experimental results bridgenet showed signiﬁcant improvements tackling distant speech recognition problem achieved relative reductions corpus compared baseline neural network without teacher’s hints. index terms— distant speech recognition distant speech recognition recognize human speeches presence noise reverberation interference caused mainly large distance speakers microphones. challenging task especially unavoidable mismatches signal quality normal close-talking far-ﬁeld speech signals. traditional speech recognizers trained speech samples closetalking microphones show signiﬁcant performance drops recognizing far-ﬁeld signals. great efforts improve performance. traditional front-end approaches interconnect multiple independent components speech enhancer acoustic speech detector speaker identiﬁcation many blocks speech recognition module. interconnected components denoise dereverberate far-ﬁeld speeches generate enhanced data. major issue since student teacher networks multiple recursions knowledge bridges repeatedly connected every recursion. however knowledge bridge added intermediate recursion always degraded performance. bridgenet adds knowledge bridges last recursion shown figure section present recursive architecture. recursive neural network popularly used sentence parsing sentimental analysis sentence paraphrase many areas. applies weights recursively structure. concept similar recurrent network clear difference recursive neural network traverse given structure topological order. figure shows building blocks proposed recursive architecture. composed four sub-blocks take acoustic features feedback states input merges outputs produces recognized phone states. block type network. represents output corresponding sub-blocks. indicates recursion number. linit zero vector used input zero recursion. advantage sub-block division enables network recurse heterogeneous input output types. example typical acoustic model context-dependent phones network output. output cannot input next recursion network input acoustic signal totally different phone states. proposed architecture provides different input paths. processed independently merged later figure presents unroll proposed recursive network depth direction. implies number recursion. input applied network denoising signal denoising also improved reference output. proposed recursive architecture enables bi-directional information ﬂows between signal denoising speech recognition functions simple network cascading. experimental results conﬁrm effectiveness bridgenet showing bridgenet multiple hints presented accuracy improvements distant speech corpus. recursive architecture bridgenet achieved improvements. bridgenet provides novel student-teacher transfer learning based recursive architecture deploy learningfrom-hints paradigm figure presents high-level block diagram bridgenet. student teacher networks constructed recursive network. don’t need recursion number. typically teacher network recursions complexity matters training stage. bridgenet uses collection triplets training data yt). model differences. first model recurse homogeneous input output. second global shortcut path always added output prior recursion model allows ﬂexibly combine heterogeneous inputs. simple addition special case figure shows bridgenet concept applied recursive network figure four components layers ﬁrst lstm layers second lstm layers dimension reduction layer since feedback phone states acoustic input don’t correlations frequency time directions cannot instead feedback phones separately procnn layers. cessed controlled gate network formulation referred feedback state recursion output recursion weights learned. input paths combined later dimension reduction layer. dimension reduction layer fully-connected merge reduce dimensions second lstm block corpus provides hours meeting conversations recorded individual headset microphones single distant microphones data cleanly recorded high noise speaker’s interferences. improved beamforming multiple channels becomes data. since corpora synchronously recorded alignment label generated corpus type used train network corpus. bridgenet trained clean alignment ihm. kaldi microsoft cognitive toolkit used train decode bridgenet. ﬁlterbank amplitudes dimensions generated feature vectors. stacked frames bridgenet. residual lstms bridgenet memory cells hidden nodes. ﬁnal softmax output contextdependent phone classes. layers kernels feature maps respectively. since corpus meeting conversation multiple speakers provide types word error rates all-speakers main-speaker wers. all-speakers decode concurrent speeches challenge considering training procedure focuses main speaker. main-speaker decode single main speaker time frame table provides evaluation multi-task denoising corpus. multi-task denoising shows main-speaker reduction cnnlstm contrary bigger improvement observed trained layers layer neurons except bottleneck layers model main difference model showed signiﬁcantly lower wers. conjectured gain multi-task denoising decreases better acoustic model. next cnn-lstm trained clean alignment corpus. main-speaker cnn-lstm improved simply changing alignment labels. however multi-task denoising improved cnnlstm degraded table presented bridgenet results corpus. cnn-lstm baseline network. lstm knowledge bridges shown figure table means bridgenet knowledge distillation connection. likewise kd+dr kd+dr+lstm imply bridgenets corresponding knowledge bridges. bridgenets recursion student teacher networks. bridgenets student network recursion teacher network recursions knowledge bridges added last recursion. bridgenet non-recursive network lstm showed relative reduction cnn-lstm bridgenet respectively. results showed knowledge bridges intermediate layers improved student network guiding student’s feature representations. kd+dr+lstm showed relative reduction kd+dr+lstm without recursion. compared cnn-lstm kd+dr+lstm provided improvements respectively. recursive architecture signiﬁcantly boosted performance student network. table presented bridgenet results data. data formed beamforming channel data using beamformit student network trained beamformed training data also evaluated beamformed evaluation data. similar results bridgenet provides signiﬁcant improvements. kd+dr+lstm showed relative reduction cnn-lstm recursions gain increased compared baseline paper proposes novel student-teacher transfer learning bridgenet. bridgenet introduces knowledge bridges provide student network enhanced feature representations different abstraction levels. bridgenet also based proposed recursive architecture enables iteratively improve signal denoising recognition. experimental results conﬁrmed training multiple knowledge bridges recursive architectures signiﬁcantly improved distant speech recognition. javier ramırez jos´e segura carmen benıtez angel torre antonio rubio efﬁcient voice activity detection algorithms using long-term speech information speech communication vol. yanmin qian tian dong investigation using parallel data far-ﬁeld speech recognition acoustics speech signal processing ieee international conference ieee mirco ravanelli philemon brakel maurizio omologo yoshua bengio batch-normalized joint training spodnn-based distant speech recognition language technology workshop ieee. ieee adriana romero nicolas ballas samira ebrahimi kahou antoine chassang carlo gatta yoshua bengio fitnets hints thin deep nets arxiv preprint arxiv. ying jian yang xiaoming image superresolution deep recursive residual network ieee conference computer vision pattern recognition workshops. ieee jean carletta simone ashby sebastien bourban mike flynn mael guillemot thomas hain jaroslav kadlec vasilis karaiskos wessel kraaij melissa kronenthal meeting corpus preannouncement international workshop machine learning multimodal interaction. springer daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannemann petr motlicek yanmin qian petr schwarz silovsky georg stemmer karel vesely kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding. dec. ieee signal processing society ieee catalog cfpsrw-usb. dong adam eversole mike seltzer kaisheng zhiheng huang brian guenter oleksii kuchaiev zhang frank seide huaming wang introduction computational networks computational network toolkit tech. rep. technical report tech. rep. microsoft research research. microsoft. com/apps/pubs xavier anguera chuck wooters javier hernando acoustic beamforming speaker diarization meetings ieee transactions audio speech language processing vol.", "year": "2017"}