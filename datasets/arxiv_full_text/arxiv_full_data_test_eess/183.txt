{"title": "State-of-the-art Speech Recognition With Sequence-to-Sequence Models", "tag": "eess", "abstract": " Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-theart ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2% to 5.6%, while the best conventional system achieves 6.7%; on a dictation task our model achieves a WER of 4.1% compared to 5% for the conventional system. ", "text": "attention-based encoder-decoder architectures listen attend spell subsume acoustic pronunciation language model components traditional automatic speech recognition system single neural network. previous work shown architectures comparable state-of-theart systems dictation tasks clear architectures would practical challenging tasks voice search. work explore variety structural optimization improvements model signiﬁcantly improve performance. structural side show word piece models used instead graphemes. also introduce multi-head attention architecture offers improvements commonly-used single-head attention. optimization side explore synchronous training scheduled sampling label smoothing minimum word error rate optimization shown improve accuracy. present results unidirectional lstm encoder streaming recognition. hour voice search task proposed changes improve best conventional system achieves dictation task model achieves compared conventional system. sequence-to-sequence models gaining popularity automatic speech recognition community folding separate acoustic pronunciation language models conventional system single neural network. variety sequence-to-sequence models explored literature including recurrent neural network transducer listen attend spell neural transducer monotonic alignments recurrent neural aligner models shown promising results thus clear approaches would practical unseat current state-of-the-art hmm-based neural network acoustic models combined separate conventional system. sequence-to-sequence models fully neural without ﬁnite state transducers lexicon text normalization modules. training models simpler conventional systems require bootstrapping decision trees time alignments generated separate system. date however none models able outperform state-of-the system large vocabulary continuous speech recognition task. goal paper explore various structure optimization improvements allow sequence-to-sequence models signiﬁcantly outperform conventional system voice search task. since previous work showed offered improvements sequence-to-sequence models focus improvements model work. model single neural network includes encoder analogous conventional acoustic model attender acts alignment model decoder analogous language model conventional system. consider modiﬁcations model structure well optimization process. structure side ﬁrst explore word piece models applied machine translation recently speech rnn-t compare graphemes modest improvement wpm. next explore incorporating multi-head attention allows model learn attend multiple locations encoded features. overall relative improvement structure improvements. optimization side explore variety strategies well. conventional systems beneﬁt discriminative sequence training optimizes criteria closely related therefore present work explore training models minimize number expected word errors signiﬁcantly improves performance. second include scheduled sampling feeds previous label prediction training rather ground truth. third label smoothing helps make model less conﬁdent predictions regularization mechanism successfully applied vision speech tasks fourth many models trained asynchronous synchronous training recently shown improve neural systems four optimization strategies allow additional relative improvement structure improvements. finally incorporate language model rescore n-best lists second pass results relative improvement wer. taken together improvements model structure optimization along second-pass rescoring allow improve single-head attention grapheme system voice search task. provides relative reduction compared strong conventional model baseline achieves also observe similar sections show language models integrated. section extends model multi-head attention. explore discriminative training sections synchronous training regimes section unidirectional encoders low-latency streaming decoding. basic model used experiments work consists modules shown figure listener encoder module similar standard acoustic model takes input features maps higher-level feature representation henc. output encoder passed attender determines encoder features henc attended order predict next output symbol similar dynamic time warping alignment module. finally output attention module passed speller takes attention context generated attender well embedding previous prediction order produce probability distribution current sub-word unit given previous units {yi− input traditionally sequence-to-sequence models used graphemes output units folds neural network side-steps problem out-of-vocabulary words alternatively could longer units word pieces shorter units context-independent phonemes disadvantages using phonemes requires having additional found improve graphemes experiments motivation looking word piece models follows. typically word-level much lower perplexity compared grapheme-level thus feel modeling word pieces allows much stronger decoder compared graphemes. addition modeling longer units improves effective memory decoder lstms allows model potentially memorize pronunciations frequently occurring words. furthermore longer units require fewer decoding steps; speeds inference models signiﬁcantly. finally wpms shown word piece models used paper sub-word units ranging graphemes entire words. thus out-of-vocabulary words word piece models. word piece models trained maximize language model likelihood training set. word pieces positiondependent special word separator marker used denote word boundaries. words segmented deterministically independent context using greedy algorithm. multi-head attention ﬁrst explored machine translation extend work explore value speech. speciﬁcally shown figure extends conventional attention mechanism multiple heads head generate different attention distribution. allows head different role attending encoder output hypothesize makes easier decoder learn retrieve information encoder. conventional singleheaded architecture model relies encoder provide clearer signals utterances decoder pickup information attention. observed tends allocate attention head beginning utterance contains mostly background noise thus hypothesize better distinguish speech noise encoded representation less ideal example degraded acoustic conditions. conventional systems often trained optimize sequencelevel criterion addition training. although loss function optimize attention-based systems sequence-level loss function closely related metric actually care about namely word error rate. variety methods explored literature address issue context sequence-to-sequence models work focus minimum expected word error rate training proposed mwer strategy objective minimize expected number word errors. loss functon given equation denotes number word errors hypothesis compared ground-truth label sequence ﬁrst term interpolated standard cross-entropy based loss important order stabilize training λlce decoder topology neural language model function language model; exposed training transcripts. external hand leverage large amounts additional data text address potentially weak learned decoder incorporate external inference only. external large -gram trained text data variety domains. since domains different predictive value lvcsr task domain-speciﬁc ﬁrst trained combined together using bayesian-interpolation incorporate second-pass means log-linear interpolation. particular given n-best hypotheses produced model beam search determine ﬁnal transcript where provided number words tuned development set. using criterion transcripts language model probability demoted ﬁnal ranked list. additionally last term addresses common observation incorporation leads higher rate deletions. experiments conducted hour training consisting million english utterances. training utterances anonymized hand-transcribed representative google’s voice search trafﬁc. data created artiﬁcially corrupting clean utterances using room simulator adding varying degrees noise reverberation overall average noise sources youtube daily life noisy environmental recordings. report results utterances extracted google trafﬁc also evaluate resulting model trained voice search data dictation utterances longer sentences voice search utterances. experiments -dimensional log-mel features computed window shifted every similar current frame features stacked frames left downsampled frame rate. encoder network architecture consists long short-term memory layers. explored unidirectional bidirectional lstms unidirectional lstms hidden units bidirectional lstms hidden units direction unless otherwise stated experiments reported unidirectional encoders. additive attention used single-headed multi-headed attention experiments. multi-headed attention experiments heads. decoder network -layer lstm hidden units layer. expectation equation approximated either sampling restricting summation n-best list decoded hypotheses commonly used sequence training latter found effective experiments denote nbest {y··· n-best hypotheses computed using beam-search decoding input utterance loss function approximated shown equation weights normalized word errors hypothesis explore scheduled sampling training decoder. feeding ground-truth label previous prediction helps decoder learn quickly beginning introduces mismatch training inference. scheduled sampling process hand samples probability distribution previous prediction uses resulting token feed previous token predicting next label. process helps reduce training inference behavior. training process uses teacher forcing beginning training steps training proceeds linearly ramp probability sampling model’s prediction speciﬁed step keep constant training. step ramp probability million steps steps asynchronous synchronous training respectively compare asynchronous synchronous training shown synchronous training potentially provide faster convergence rates better model quality also requires effort order stabilize network training. approaches high gradient variance beginning training using multiple replicas explore different techniques reduce variance. asynchronous training replica ramp system start training replicas once instead start gradually. synchronous training techniques learning rate ramp gradient norm tracker. learning rate ramp starts learning rate gradually increases learning rate providing similar effect replica ramp gradient norm tracker keeps track moving average gradient norm discards gradients signiﬁcantly higher variance moving average. approaches crucial making synchronous training stable. label smoothing regularization mechanism prevent model making over-conﬁdent predictions. encourages model higher entropy prediction therefore makes model adaptable. followed design ﬁrst experiments explore different structure improvements model. table compares performance models given graphemes table indicates perform slightly better graphemes. consistent ﬁnding provides stronger decoder compared graphemes resulting roughly relative improvement second compare performance shown experiment table. provides around improvement. indicates model focus multiple points attention input signal similar spirit language model passed encoder helps signiﬁcantly. since models perform best explore proposed optimization methods model rest paper. explore performance various optimization improvements discussed section table shows including synchronous training wpm+mha model provides improvement. furthermore including scheduled sampling gives additional relative improvement wer; label smoothing gives additional relative improvement. finally mwer training provides overall gain optimizations around moving synchronous training conﬁguration yields better converged optimum similar wall clock time. interestingly scheduled sampling minimum word error rate discriminative methods observed combination continues yield additive improvements. finally regularization label smoothing even large amounts data proven beneﬁcial. unidirectional bidirectional systems. table shows proposed changes give relative reduction unidirectional system slightly smaller improvement bidirectional system. illustrates proposed methods offer improvements independent model topology. finally compare proposed model state-ofthe-art discriminatively sequence-trained frame rate system terms wer. table shows proposed sequence-tosequence model offers relative improvement production system voice search dictation task respectively. furthermore comparing size ﬁrst-pass models model around times smaller conventional model. important note second pass model still dominates model size. designed attention-based model sequence-to-sequence speech recognition. model integrates acoustic pronunciation language models single neural network require lexicon separate text normalization component. explored various structure optimization mechanisms improving model. cumulatively structure improvements yielded improvement optimization improvements yielded improvement language model rescoring yielded another improvement. applied google voice search task achieve hybrid hmm-lstm system achieves wer. tested models dictation task model achieves hybrid system achieves wer. note however unidirectional system limitation entire utterance must seen encoder labels decoded therefore important next step revise model streaming attention-based model neural transducer t.n. sainath prabhavalkar kumar rybach kannan schogol nguyen chen chiu need lexicon? evaluating value pronunciation lexica end-to-end models proc. icassp hochreiter schmidhuber long short-term memory neural computation vol. schuster paliwal bidirectional recurrent neural networks ieee transactions signal processing vol. abadi tensorflow large-scale machine learning heterogeneous distributed systems available online http//download.tensorﬂow.org/paper/whitepaper.pdf szegedy vanhoucke ioffe shlens wojna rethinking inception architecture computer vision proc. ieee conference computer vision pattern recognition", "year": "2017"}