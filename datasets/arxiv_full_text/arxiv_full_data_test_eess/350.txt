{"title": "Fast binary embeddings, and quantized compressed sensing with structured  matrices", "tag": "eess", "abstract": " This paper deals with two related problems, namely distance-preserving binary embeddings and quantization for compressed sensing . First, we propose fast methods to replace points from a subset $\\mathcal{X} \\subset \\mathbb{R}^n$, associated with the Euclidean metric, with points in the cube $\\{\\pm 1\\}^m$ and we associate the cube with a pseudo-metric that approximates Euclidean distance among points in $\\mathcal{X}$. Our methods rely on quantizing fast Johnson-Lindenstrauss embeddings based on bounded orthonormal systems and partial circulant ensembles, both of which admit fast transforms. Our quantization methods utilize noise-shaping, and include Sigma-Delta schemes and distributed noise-shaping schemes. The resulting approximation errors decay polynomially and exponentially fast in $m$, depending on the embedding method. This dramatically outperforms the current decay rates associated with binary embeddings and Hamming distances. Additionally, it is the first such binary embedding result that applies to fast Johnson-Lindenstrauss maps while preserving $\\ell_2$ norms.  Second, we again consider noise-shaping schemes, albeit this time to quantize compressed sensing measurements arising from bounded orthonormal ensembles and partial circulant matrices. We show that these methods yield a reconstruction error that again decays with the number of measurements (and bits), when using convex optimization for reconstruction. Specifically, for Sigma-Delta schemes, the error decays polynomially in the number of measurements, and it decays exponentially for distributed noise-shaping schemes based on beta encoding. These results are near optimal and the first of their kind dealing with bounded orthonormal systems. ", "text": "paper deals related problems namely distance-preserving binary embeddings quantization compressed sensing. first propose fast methods replace points subset associated euclidean metric points cube {±}m associate cube pseudo-metric approximates euclidean distance among points methods rely quantizing fast johnson-lindenstrauss embeddings based bounded orthonormal systems partial circulant ensembles admit fast transforms. quantization methods utilize noise-shaping include sigma-delta schemes distributed noise-shaping schemes. resulting approximation errors decay polynomially exponentially fast depending embedding method. dramatically outperforms current decay rates associated binary embeddings hamming distances. additionally ﬁrst binary embedding result applies fast johnson-lindenstrauss maps preserving norms. second consider noise-shaping schemes albeit time quantize compressed sensing measurements arising bounded orthonormal ensembles partial circulant matrices. show methods yield reconstruction error decays number measurements using convex optimization reconstruction. speciﬁcally sigma-delta schemes error decays polynomially number measurements decays exponentially distributed noise-shaping schemes based beta encoding. results near optimal ﬁrst kind dealing bounded orthonormal systems. signal matrix cm×n would often like quantize digitize i.e. entry ﬁnite discrete hand viewing measurement operator quantization step allows move analog world continuous measurements digital world bit-streams. thereby allows computer processing signals well storage transmission. hand view linear embedding taking vectors image vector quantized version viewed non-linear embedding therefore used expedite tasks information ∗department mathematics university california diego e-mail tlhucsd.edu †department mathematics university california diego e-mail rsaabucsd.edu. known measurement vector denotes unknown noise. sparse compressible recovered measurements even indeed study recovery algorithms reconstruction guarantees measurements purview compressed sensing popular reconstruction methods setting ℓ-minimization. here vector approximated solution earliest results compressed sensing show certain class subgaussian random measurement matrices including i.i.d. standard gaussian bernoulli entries high probability draw matrix uniformly results subgaussian random matrices mathematically important techniques introduce proof concept compressed sensing viable. nevertheless physical reasons full design control measurement matrix practice. moreover practical reasons require measurement matrices admit fast multiplications. reasons motivated study structured random matrices compressed sensing context perhaps popular classes structured random measurement matrices drawn bounded orthogonal ensemble partial circulant ensemble indeed studying interplay structured random matrices quantization focus paper. examples include discrete fourier matrix hadamard matrix. physical reasons boes arise naturally various important applications compressed sensing including involving magnetic resonance imaging additional practical beneﬁt boes e.g. based fourier hadamard transforms implementation speed. viewed linear operators matrices admit fast implementations number additions multiplications scales like contrast cost standard matrix vector multiplication scales like such boes also appeared various results involving fast johnson-lindenstrauss embeddings deﬁnition circulant matrix cn×n given action denotes convolution. choosing vector uniformly random setting rows rows indexed partial circulant ensembles also appear naturally various applications compressed sensing including involving radar wireless channel estimation mainly fact action circulant matrix vector equivalent convolution circulant matrix. refer discussion applications. boes pces also admit fast transformations convolution essentially diagonalized fourier transform admits fast implementation. such pces also admit implementation number addition multiplications scales opposed reason pces like boes feature prominently constructions fast johnson-lindenstrauss embeddings. additional beneﬁt applications memory required storing scales like candés ﬁrst study structured random matrices drawn compressed sensing context. show matrices satisfy restricted isometry property high probability provided number measurements log. later many researchers improved dependence number measurements i.e. improved logarithmic factors. knowledge current best result haviv regev achieves lower bound log. similarly many researchers studied recovery guarantees compressed sensing random measurement matrices drawn partial circulant ensembles eventually showing similar dependence i.e. results apply boes pces added caveat randomizing signs rows. therefore useful introduce following construction. consider linear measurements signal quantization process measurement vector replaced vector elements ﬁnite known quantization alphabet. ﬁniteness allows elements represented ﬁnite binary strings turn allows digital storage processing. indeed digital processing necessary ﬁeld binary embedding compressed sensing careful selection yield faster memory-eﬃcient algorithms. binary embedding wishes design quantization approximately preserves distance signals hand compressed sensing typically requires reconstruction algorithm given quantized measurements ensures small reconstruction error ˆxk. various choices quantization maps reconstruction algorithms proposed context binary embedding compressed sensing. ranged intuitive quantization approaches namely memoryless scalar quantization sophisticated approaches based noise shaping quantization including quantization distributed noise shaping quantization well others indeed afore-mentioned noise shaping quantization techniques combine computational simplicity ability yield favorable error guarantees function number measurements. obtain results thus provide necessary detailed overview section distortion embeddings played important role recently signal processing machine learning transform high dimensional signals low-dimensional ones preserving geometric properties. beneﬁts embeddings direct consequences dimensionality reduction include potential reduced storage space computational time associated signals. perhaps important embeddings given johnson-lindenstrauss lemma shows embed points o-dimensional space simultaneously preserve pairwise euclidean distance δ-lipschitz distortion. recently binary embeddings also attracted attention signal processing machine learning communities part theoretical interest part motivated potential beneﬁts reductions memory requirements computational time. roughly speaking nonlinear embeddings high dimensional signals discrete cube lower-dimensional space. signal represented short binary code memory needed storing entire data reduced considerably cost computation. matrix whose entries independent standard gaussian random variables. probability exceeding α-binary embedding associated normalized hamming distance log. drawback execute embedding must incur memory cost order store must also incur full computational cost dense matrix-vector multiplication time point embedded. address ﬁrst point above researchers tried design binary embeddings implemented eﬃciently. example embeddings include circulant binary embedding structured hashed projections binary embeddings based walshhadamard matrices partial gaussian toeplitz matrices random column ﬂips knowledge results address second point above; sign function quantize measurements. simple quantization method cannot yield much better distance preservation measurements i.e. increases discussed section roughly speaking compressible vector close k-sparse vector. precisely best k-term approximation error denoted minv∈σn kx−vk measure close k-sparse. notation refer unit euclidean ball unit ball respectively. follows means universal constant deﬁned analogously. notation indicate expectation taken respect random variable operator norm matrix kakp→q deﬁned kakp→q rest paper organized follows. section highlight main contributions paper section review necessary technical deﬁnitions tools compressed sensing embedding theory probability. section dedicated review quantization theory particular noise-shaping techniques instrumental results. sections dedicated main theorems proofs binary embeddings quantized compressed sensing. section provides proof critical technical result namely certain matrices associated methods satisfy restricted isometry property. finally section provides proofs technical lemmas require along way. below describe main results pertain binary embeddings quantized compressed sensing summarize main technical contribution allows obtain results. diagonal matrix random signs matrix construction noise-shaping quantizer i.e. either quantization above decays polynomially fast exponentially fast fact prove general version result applies inﬁnite sets arbitrary show high probability prescribed distortion denote gaussian width euclidean radius deﬁned section remark results even general sense embed ﬁnite quantization alphabet used noise-shaping quantization scheme particular means choose alphabet contains elements obtain improved constants additive part error bounds. thus eﬀect results amount fast quantized johnsonlindenstrauss embeddings. provide ﬁrst non-trivial quantization results optimal dependence sparsity setting compressed sensing bounded orthonormal systems. results also apply partial circulant ensembles approach summarized algorithm show noise-shaping quantization approaches namely quantization distributed noise-shaping quantization signiﬁcantly outperform naive quantization approach memoryless scalar quantization. yield polynomial exponential error decay respectively function number measurements. contrast error decay memoryless scalar quantization better linear noise shaping quantization techniques produce vectors whose entries belong ﬁnite set. equally importantly related vector measurements noise-shaping relation form so-called noise shaping operator vector bounded entries. recalling compressed sensing wishes recover sparse vector reconstruction approach solve optimization problem choice critical success reconstruction require satisfy restricted isometry property section satisﬁes property reconsensing results thus main technical challenge construct satisﬁes restricted isometry property. non-trivial dependencies implicit random model generates bounded orthonormal systems partial circulant matrices. simultaneously scalar choose must small enough yield desired polynomial exponential decay rates number measurements choice achieves goals recall restricted isometry property matrices along implications compressed sensing well theorems showing boes pces satisfy high probability polylog indeed several ensembles random matrices satisfy paper particularly interested bounded orthonormal ensembles partial circulant matrices recall following results. remark relax deﬁnition requiring pseudo-metric require require respect triangle inequality allow deﬁnition discrete cube rp×m deﬁne pseudo-metric need following multiresolution version restricted isometry property used oymak prove embedding results arbitrary i.e. necessarily ﬁnite sets. deﬁnition ⌈log given number sequence distortion sparsity levels. matrix rm×n satisﬁes multiresolution restricted isometry property distortion sparsity -rip holds. theorem suppose matrix rm×n obeys multiresolution previously mentioned various choices quantization maps reconstruction algorithms proposed context binary embedding compressed sensing. below ﬁrst review lower bounds associated optimal quantization decoding scheme outperform. review basics memoryless scalar quantization well noise shaping quantization need results. measured tradeoﬀ number bits reconstruction error describe examine optimal rate-distortion relationship. given desired worst-case approximation error measured norm cover class balls radius associated euclidean norm smallest number ǫ-balls called covering number denoted optimal scheme consists encoding signal class using bits mapping center ǫ-ball lies. simple volume argument yields optimal lower bound approximation error term rate which case gaussian matrices exist schemes approach optimal error bounds albeit various tradeoﬀs. among these results apply gaussian random matrices rely noiseshaping quantization techniques apply general class subgaussians. quantization direction work extends noise-shaping results case structured random matrices selected according construction describing necessary technical details pertaining noise-shaping quickly describe basic approach quantization i.e. memoryless scalar quantization widely assumed approaches binary embeddings quantization compressed sensing measurements words round measurement nearest element quantization alphabet instance case binary embedding i.e. sign. sign otherwise. quantization alphabet step size used bound δ√m. thus setting binary embedding deﬁnes quasi-isometric note optimal scheme described herein impractical requires direct measurements available requires ﬁnding ball belongs. number balls scales exponentially ambient dimension task becomes prohibitively expensive. spite simplicity optimal oversamples i.e. particular error bounds improve measurements needed. increasing number measurements fact ignores correlations measurements acts component-wise. principle still reduction error using ﬁner quantization alphabet often feasible practice quantization alphabet ﬁxed hardware built desirable prefer simple e.g. -bit embedding. order address problem noise-shaping quantization methods sigma-delta modulation alternative decoding methods proposed settings quantization bandlimited functions ﬁnite frame expansions compressed sensing measurements. however methods studied framework binary embedding problems. stated before main contributions work extend noise-shaping approaches embedding problems compressed sensing structured random matrices. noise-shaping quantizers example modulation ﬁrst proposed analog-to-digital conversion bandlimited functions success essentially fact push quantization error toward nullspace associated reconstruction operators. methods successfully extended frameworks ﬁnite frames compressed sensing fact approaches based quantization beta encoders achieve near-optimal bounds sub-gaussian measurements. ialδ complex quantization alphabet. remark mention complex case sake completeness remark results apply complex setting discussion restrict attention real valued matrices vectors. called state vector kuk∞ constant independent here lower triangular toeplitz matrix unit diagonal. noise-shaping quantizers exist unconditionally requirement kuk∞ uniformly bounded however certain suitable assumptions exist simply implemented recursive algorithms example following lemma provides conditions schemes stable. proof simply induction found example algorithm inspired approach reconstruction noise-shaping quantized measurements used ﬁrst introduce so-called condensation goal design matrix matrix norm exponentially small quantization algorithm stable. turn helps achieve exponentially polynomially small errors settings binary embedding compressed sensing. present candidates pair based quantization distributed noise shaping. constant independent dimensions nontrivial ﬁxed quantization alphabet. nevertheless exist several constructions exactly boundedness property albeit diﬀerent constants results herein assume ﬁxed alphabet stable schemes relevant result following proposition summarizing results previous works using schemes compressed sensing achieve polynomially small error bound however approaches assume gaussian subgaussian random matrices proof techniques easily extended case structured random matrices. part role applied plays associated proofs; essentially requires d−rφ restricted isometry property. dependencies among rows case boes pces property diﬃcult prove. nevertheless authors ﬁrst study pces prove error bounds decay polynomially approach proposed uses diﬀerent sampling scheme generate diﬀerent reconstruction method requires diﬀerent proof technique. chou güntürk achieved exponentially small error bound quantization unstructured random ﬁnite frame expansions using so-called distributed noise-shaping approach. work paper extends approach handle practically important cases compressed sensing binary embeddings structured random matrices drawn pce. review distributed noise-shaping. ready present main results binary embeddings showing algorithm yields quasi-isometric embeddings preserve euclidean distances. start case ﬁnite sets present analysis methods inﬁnite sets. proof. proof follows theorem main technical result theorem respectively showing matrices randomized column signs provide johnson-lindenstrauss remark theorem modiﬁed apply ﬁnite subsets indeed condition unit euclidean ball instead ball. satisﬁes -rip theorems considered event theorem matrix √mkφdǫzk αkzk theorem implies hence kφdǫzk∞ α√m. scaling quantization alphabet carefully e.g. considering {±pm} place ensures quantization schemes stable stability constant yields error bound transformation price extension small avoid lower bound bernstein’s inequality union bound rows followed union bound would yield bound kφdǫxk∞ high probability provided polynomial turn scaling alphabet would imply bound theorem transformation remark note requirement proof eveφ satisﬁes appropriate restricted isometry property. hard matrices independent entries drawn appropriately normalized gaussian subgaussian herein present results compressed sensing acquisition reconstruction paradigm outlined algorithm first cover case acquired measurements quantized using scheme show polynomial error decay quantization error. second present analogous result distributed noise shaping quantization show exponential error decay quantization error function number measurements. remark note modifying reconstruction technique slightly non-quantization noise handled robust analogous manner method leave details interested reader. main technical theorem shows scaled version satisﬁes restricted isometry property. proof theorem based technique in-turn relies heavily rough architecture proof follows. ﬁrst show desired property holds expectation leverage expectation result generalized bernstein bound obtain property high probability. proof holds expectation turn comprises several steps. triangle inequality shows expected value constant bounded expected constant matrix expected supremum chaos process. bound chaos process theorem requires controlling dudley integral hence certain covering numbers. again technique adapted works nelson rudelson vershynin", "year": "2018"}