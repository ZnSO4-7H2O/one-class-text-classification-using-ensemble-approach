{"title": "Multichannel Sound Event Detection Using 3D Convolutional Neural  Networks for Learning Inter-channel Features", "tag": "eess", "abstract": " In this paper, we propose a stacked convolutional and recurrent neural network (CRNN) with a 3D convolutional neural network (CNN) in the first layer for the multichannel sound event detection (SED) task. The 3D CNN enables the network to simultaneously learn the inter- and intra-channel features from the input multichannel audio. In order to evaluate the proposed method, multichannel audio datasets with different number of overlapping sound sources are synthesized. Each of this dataset has a four-channel first-order Ambisonic, binaural, and single-channel versions, on which the performance of SED using the proposed method are compared to study the potential of SED using multichannel audio. A similar study is also done with the binaural and single-channel versions of the real-life recording TUT-SED 2017 development dataset. The proposed method learns to recognize overlapping sound events from multichannel features faster and performs better SED with a fewer number of training epochs. The results show that on using multichannel Ambisonic audio in place of single-channel audio we improve the overall F-score by 7.5%, overall error rate by 10% and recognize 15.6% more sound events in time frames with four overlapping sound sources. ", "text": "abstract—in paper propose stacked convolutional recurrent neural network convolutional neural network ﬁrst layer multichannel sound event detection task. enables network simultaneously learn interintra-channel features input multichannel audio. order evaluate proposed method multichannel audio datasets different number overlapping sound sources synthesized. dataset four-channel ﬁrst-order ambisonic binaural single-channel versions performance using proposed method compared study potential using multichannel audio. similar study also done binaural single-channel versions real-life recording tut-sed development dataset. proposed method learns recognize overlapping sound events multichannel features faster performs better fewer number training epochs. results show using multichannel ambisonic audio place single-channel audio improve overall fscore overall error rate recognize sound events time frames four overlapping sound sources. sound event detection task recognizing sound events respective temporal start time audio recording. sound events real life always occur isolation tend considerably overlap other. recognizing overlapping sound events referred polyphonic sed. applications polyphonic numerous. recognizing sound events like alarm glass breaking used surveillance automatic detection road accidents ensure quick intervention emergency teams environmental sound event detection used monitoring biodiversity further used automatically annotating audio datasets sound events recognized used query similar content retrieval. research leading results received funding european research council european unions framework programme grant agreement everysound. authors also wish acknowledge csc-it center science finland computational resources using supervised classiﬁers like gaussian mixture model hidden markov model fully-connected networks convolutional neural networks recurrent neural networks recently state method polyphonic proposed mel-band energy feature used convolutional recurrent neural network architecture. recognizing overlapping sound events using singlechannel audio challenging task. overlapping sound events potentially recognized better multichannel audio. ﬁrst methods multichannel audio proposed performed audio channels separately combined likelihoods across channels used ﬁnal prediction. recently state crnn network single-channel extended multichannel features multiple feature classes shown performance improves using binaural audio instead single-channel audio version dataset. regard also proposed binaural audio features exploiting inter-aural intensity time differences. network proposed cnns used feature extractors learned intra-channel information input multichannel audio features rnns followed cnns learning inter-channel information. paper propose learn interintra-channel information within layer. implement using ﬁrst layer network. enables method learn interintra-channel information input multichannel audio within layers additional parameters comparison hardware devices smart homes virtual reality content creation modern hearing aids surveillance sensors microphone them. using multichannel audio available devices potentially improve polyphonic improvement additionally enhance overall performance devices. although showed using binaural audio place single-channel audio improves performance conclusive work studies potential two-channel audio. besides order carry study publicly available data. moreover collecting annotating dataset tedious expensive time-consuming task. order assess necessity collecting dataset paper synthesize three multichannel audio datasets three temporally overlapping sound sources. multichannel audio datasets four-channel ﬁrst-order ambisonic audio. additionally perform binauralization real head related transfer function obtain binaural version audio used omnidirectional channel single-channel version. experiments carried datasets understand extent improvement achieve using multichannel audio current state methods using single-channel binaural audio. based results obtained decide invest collection real-life multichannel dataset. furthermore order compare consistency results obtained synthetic dataset perform similar experiments reallife recordings tut-sed dataset consists single-channel binaural audio. paper organized follows section describes feature extraction details proposed neural network. datasets used metric evaluation baseline method evaluation procedure explained section iii. finally results discussion presented section proposed multichannel method shown figure input method either single-channel multichannel audio. single-channel audio input mel-band energy feature. case multichannel input mel-band energy feature extracted channels; additionally generalized crosscorrelation phase transform feature extracted channel pair multichannel audio. audio features multichannel neural network architecture maps activities sound event classes dataset. output neural network continuous range sound event classes corresponds probability particular sound class active frame. continuous range output thresholded obtain ﬁnal binary decision sound event class active absent frame. general proposed method takes sequence framewise audio features input predicts activity target sound event classes input frames. detailed description feature extraction neural network presented below. audio extracted channels proposed motivated inter-aural intensity difference used human auditory system localize recognize overlapping sound events. neural network capable performing linear operations obtain information similar binaural mbe. recently binaural shown improve performance even larger binaural datasets motivated this continue feature extracted input channels paper. case single-channel audio input extract windows overlap refer mbe-mono. mel-bands frequency range multichannel audio extract channels refer mbe-bin binaural mbe-ambi fourchannel audio. sequence length frames feature general dimension number channels mbe-mono mbe-bin mbe-ambi. generalized cross correlation phase transform case binaural audio proposed represent similar information inter-aural time difference humans using generalized cross correlation phase transform shown methods beneﬁt overlapping sound events. motivated this continue paper. similar extract three resolutions where coefﬁcients two-channels calculated. coefﬁcient time frame frequency total bins. frame given extracted delays range τmax maximum sample delay sound wave travel pair microphones recording audio. order factorisable feature length pooling neural network values chosen range three multi-resolution. sequence length frames number possible pair-of-two combinations channels audio number resolutions extracted. case binaural audio results ambisonic audio amounts neural network features along channel-time-frequency enables network learn interintra-channel features simultaneously within ﬁrst layer. followed sequence layers receptive ﬁlters size output activation layers padded zeros keep dimension output input. batch normalization max-pooling performed every layer along frequency axis reduce ﬁnal dimension number ﬁlters ﬁnal layer respective branches. activations branches concatenated along feature axis layers bi-directional gated recurrent units learn long-term temporal activity patterns. followed layer time-distributed fully-connected network. ﬁnal prediction layer many sigmoid units number sound event labels dataset. refer network crnn future. training performed epochs using adam optimizer binary cross-entropy loss reference sound class activities predicted ones. dropout used regularizer every layer neural network make robust unseen data. early stopping used stop overﬁtting network training data. threshold used obtain binary decision sigmoid activations ﬁnal prediction layer. training stopped error rate test split improve epochs. neural network implementation done using pytorch library. tut-sed development dataset dataset recorded street context using binaural in-ear microphone sampling rate. recordings length minutes amounting total length minutes. dataset consists manual annotations sound event classes brakes squeaking children large vehicle people speaking people walking. dataset deﬁnes four-folds training testing splits benchmarking. details dataset given since dataset channels mbe-ambi gcc-ambi features dataset. single-channel version obtained taking mean binaural channels. synthetic dataset order assess performance presence channels audio generate synthetic datasets using method proposed three separate anechoic multichannel datasets temporally overlapping sources maximum three overlapping sources maximum overlapping sources synthesized. dataset three sets training test split generated recordings respectively. every recording length seconds sampled dataset consists stationary point sources. point sources sound events associated single spatial coordinate space example person speaking phone ringing. diffuse sources like ambient noise wind breeze etc. speciﬁc spatial coordinate therefore difﬁcult synthesize spatially hence study. audio recordings synthesized ﬁrst-order ambisonic format. commonly used format spatial audio especially virtual reality domain. proposed crnn compared existing state multichannel audio method proposed similar proposed crnn baseline method perform single-channel binaural multichannel audio. previously performance tested single-channel binaural audio. method recently concluded ieee audio acoustic signal processing research challenge dcase task real life sound event detection particular secured ﬁrst positions among submitted methods. ﬁrst position obtained mbe-mono audio feature close second position mbe-bin. proves method well suited single-channel binaural baselines. baseline method shown figure also based stacked convolutional recurrent neural network comparison proposed crnn method employ thus learns intrachannel information rnns learn inter-channel information. rest inputs outputs baseline crnn proposed crnn similar. paper consider crnn single-channel binaural audio features baselines report performance crnn multichannel ambisonic audio along crnn performance. order evaluate performance proposed crnn respect baseline crnn multichannel dataset methods trained individually using single-channel binaural ambisonic audio features synthetic dataset single-channel binaural audio features tut-sed development dataset. perform hyper-parameter search datasetfeature combinations individually assess performance using multichannel audio using f-scores test splits. metric scores reported mean separate runs cross-validation splits. order study individual contribution task performed experiment estimating number sound sources every time frame using feature. usage task motivated idea relative time difference arrival overlapping sound sources different consists four channels audio commonly referred channels where channels represents directive pressure-gradient recordings along axes cartesian coordinate system respectively. channel corresponds omnidirectional microphone recording. paper channel single-channel studies four channels four-channel studies. perform binauralization four-channel audio using real head-related transfer functions obtain binaural version used binaural studies. hrtfs measured authors dense grid directions anechoic conditions detailed overview hrtf measurement techniques simulation spatial sound scenes based them work reader referred order synthesize datasets isolated sound events dcase task dataset consists sound event classes examples each. sound event classes include speech cough door slam laughter phone knock. chose examples class randomly training four testing split. order synthesize recording sound example randomly associated spatial coordinate temporally overlapping examples spatial coordinate. further magnitude sound examples varied randomly give effect varying distance microphone. details synthesis procedure given proposed method evaluated using polyphonic metrics proposed particularly segment wise error rate f-score calculated onesecond length segments. f-score calculated one-second segment number true positives i.e. number sound event labels active predictions ground truth. number false positives i.e. number sound event labels active predictions inactive ground truth. number false negatives i.e. number sound event labels active ground truth inactive predictions. highlighted feature. proposed experiment identifying number active sources using feature better accuracy using feature. would mean based methods additionally beneﬁt using gcc. trained proposed crnn feature input number active sound sources output. similar training done using feature input. using single feature proposed crnn method example feature extractor branch removed feature extractor branch used. separate hyper-parameter search done individual features randomly best conﬁgurations gcc-ambi mbe-ambi features synthetic dataset around trainable weights. unlike task multi-label classiﬁcation task experiment estimating number sources multi-class classiﬁcation task hence experiment alone output sigmoid activation replaced softmax categorical cross entropy loss used. hyperparameter search carried proposed crnn baseline crnn combination dataset audio feature general hyperparameters remained given dataset independent feature used. sequence length frames batch size dropout gave best results feature synthetic dataset combinations. tut-sed dataset best results obtained using sequence length frames batch size dropout learning rate gave best results across datasets audio features. performance affected much exact number ﬁlters units. across datasets different number ﬁlters units seen give good evaluation metric scores. case synthetic dataset optimal number ﬁlters layer layer units layer similarly synthetic synthetic tut-sed dataset. correlation increasing number ﬁlters units increasing number overlapping sound events datasets shows bigger neural networks required recognizing highly overlapped sound events. evaluation results proposed crnn using single-channel binaural ambisonic audio features different polyphonic datasets presented table analyzing performance features ﬁrst polyphony f-scores comparable single multichannel increase polyphony scores binaural multichannel improves single-channel. particularly improvement signiﬁcant dataset highly overlapping sound events concretely using mbe-ambi instead mbe-mono dataset gives improvement fscore similar trend observed using baseline crnn feature results comparable proposed crnn. mbe-mono feature baseline crnn proposed crnn ideally exact scores across datasets since additional inter-channel information crnn learn from. deviations seen metric scores random initializations network even averaging scores separate runs cross-validation data. actual improvement using proposed crnn baseline crnn achieved training speed. shown figure crnn achieves better error rate lower number epochs mbe-bin mbe-ambi features. proposed crnn achieves exactly number weights baseline crnn different convolution connections feature extraction layer. another observation table mbe-mono feature across methods performance drops higher number overlapping sound events. using multichannel features especially mbe-ambi performance comparable three overlapping sound events datasets. general mbe-ambi seen perform better mbe-bin turn performs better mbe-mono. additionally performance seen signiﬁcantly improve multichannel audio sound scenes highly overlapping sound events. shows using additional audio channel information deﬁnitely helps reliable robust sed. table also reports performance using features together since extracted channel audio reports results binaural ambisonic versions audio. comparison respective features evaluation metric scores either comparable worse crnn baseline crnn methods. investigate this understand using feature provides additional information experiment estimating number sound sources frame carried out. average accuracy obtained estimating number sound sources frame using gcc-ambi mbe-ambi alone gave similar results obtained using binaural audio synthetic dataset tut-sed dataset although usage feature addition shown helpful binaural datasets present results table show provide additional information datasets studied paper. dominance features could explained strong head shadowing effects different source directions binaural recordings spatial coincidence ambisonic recordings encodes spatial information based inter-channel level differences. dominance feature hold audio formats rely phasetime-differences encode fig. learning curve proposed crnn baseline crnn methods ambisonic binaural features synthetic dataset. proposed crnn achieves better error rate lower number epochs mbe-ambi mbe-bin features. directional information insigniﬁcant level differences. audio captured linear arrays spaced omnidirectional microphones examples audio formats beneﬁt signiﬁcantly features instead level differences captured mbe. among audio features table using multichannel features especially mbe-ambi signiﬁcantly improves accuracy estimating overlapping sound events comparison single-channel mbe-mono feature. numbers using mbe-ambi instead mbe-mono proposed crnn method improves performance detection three overlapping sources four overlapping sources proves using multichannel audio helps recognize overlapping sound events better single-channel audio. evaluation metric scores real-life recordings tut-sed dataset presented table results consistent results obtained synthetic datasets. performances crnn crnn comparable multichannel feature mbe-bin achieves better single-channel mbe-mono feature. additionally error rate obtained using proposed crnn mbe-bin feature beats current score benchmarking dataset. paper proposed stacked convolutional recurrent neural network interintra-channel convolutions ﬁrst layer multichannel sound event detection task. interintra-channel convolutions implemented using convolutional neural network layer. shown proposed crnn method learns recognize overlapping sound events multichannel features faster state baseline multichannel method exactly number parameters performs better fewer number training epochs. multichannel task datasets used paper shown generalized cross-correlation phase transform feature additionally proposed assess performance using multichannel audio polyphonic sed. study carried four datasetsa) real-life recording tutsed development dataset three synthetic datasets temporally overlapping three temporally overlapping temporally overlapping sound events. recordings synthetic datasets three formats four-channel ﬁrst-order ambisonics binaural single-channel whereas real-life dataset binaural single-channel audio versions. performed individually datasets using proposed crnn. comparison using single-channel observed using multichannel audio overall f-score improved overall improved sound events recognized time frames four overlapping sound events. conclusion multichannel audio deﬁnitely improves performance using single-channel audio; collection real-life multichannel audio dataset worth effort. foggia petkov saggese strisciuglio vento audio surveillance roads system detecting anomalous sounds ieee transactions intelligent transportation systems vol. marques thomas martin mellinger ward moretti harris tyack estimating animal population density using passive acoustics biological reviews cambridge philosophical society vol. furnas callas using automated recorders occupancy models monitor common forest birds across large geographic region journal wildlife management vol. akır heittola huttunen virtanen polyphonic sound event detection using multi-label deep neural networks ieee international joint conference neural networks zhang mcloughlin song robust sound event recognition using convolutional neural networks ieee international conference acoustics speech signal processing adavanne parascandolo pertila heittola virtanen sound event detection multichannel audio using spatial harmonic features detection classiﬁcation acoustic scenes events hayashi watanabe toda hori roux takeda duration-controlled lstm polyphonic sound event detection ieee/acm transactions audio speech language processing vol. akır parascandolo heittola huttunen virtanen convolutional recurrent neural networks polyphonic sound event detection ieee/acm transacation audio speech language processing vol. adavanne pertil¨a virtanen sound event detection using spatial features convolutional recurrent neural network ieee international conference acoustics speech signal processing mesaros heittola diment elizalde shah vincent virtanen dcase challenge setup tasks datasets baseline system detection classiﬁcation acoustic scenes events ensemble convolutional neural networks weakly-supervised sound event detection using multiple scale input detection classiﬁcation acoustic scenes events ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol. paszke gross chintala chanan yang devito desmaison antiga lerer automatic differentiation pytorch neural information processing systems workshop autodiff adavanne politis virtanen direction arrival estimation multiple sound sources using convolutional recurrent neural network ieee international conference acoustics speech signal processing submitted review. available https//arxiv.org/abs/. bola˜nos pulkki hrir database measured actual source direction data audio engineering society convention head-related transfer function virtual auditory display", "year": "2018"}