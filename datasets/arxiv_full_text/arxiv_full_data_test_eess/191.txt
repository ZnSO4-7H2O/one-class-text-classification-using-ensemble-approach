{"title": "Online Nonlinear Estimation via Iterative L2-Space Projections:  Reproducing Kernel of Subspace", "tag": "eess", "abstract": " We propose a novel online learning paradigm for nonlinear-function estimation tasks based on the iterative projections in the L2 space with probability measure reflecting the stochastic property of input signals. The proposed learning algorithm exploits the reproducing kernel of the so-called dictionary subspace, based on the fact that any finite-dimensional space of functions has a reproducing kernel characterized by the Gram matrix. The L2-space geometry provides the best decorrelation property in principle. The proposed learning paradigm is significantly different from the conventional kernel-based learning paradigm in two senses: (i) the whole space is not a reproducing kernel Hilbert space and (ii) the minimum mean squared error estimator gives the best approximation of the desired nonlinear function in the dictionary subspace. It preserves efficiency in computing the inner product as well as in updating the Gram matrix when the dictionary grows. Monotone approximation, asymptotic optimality, and convergence of the proposed algorithm are analyzed based on the variable-metric version of adaptive projected subgradient method. Numerical examples show the efficacy of the proposed algorithm for real data over a variety of methods including the extended Kalman filter and many batch machine-learning methods such as the multilayer perceptron. ", "text": "projection-based kernel adaptive ﬁltering algorithms studied mainly casting nonlinear estimation minimization problem either euclidean space coefﬁcient vectors reproducing kernel hilbert space types formulation induce different geometries. latter type referred functional approach geometry dictionary subspace expressed euclidean space equivalently metric characterized kernel matrix functional approach tends exhibit better convergence behaviors former approach. supported theoretically speciﬁcally provided dictionary considered realizations input vectors autocorrelation matrix approximated squared kernel matrix essentially indicates eigenvalue spread functional approach reduced square root compared former approach principle. conventional kernel adaptive ﬁltering methods employ single kernel thereby working efﬁciently three conditions satisﬁed target nonlinear function sufﬁciently simple scale known prior adaptation design gaussian kernel appropriate scale scale time-invariant. multikernel adaptive ﬁltering efﬁcient solution case conditions violated case multi-component/partiallylinear functions remarkable feature multikernel adaptive ﬁltering ﬁnding well-ﬁtting kernel obtaining compact representation simultaneously achieved within convex analytic framework. existing functional approach multikernel adaptive ﬁltering called cartesian hyperplane projection along afﬁne subspace algorithm formulated cartesian product rkhss associated multiple kernels employed. here chypass multikernel extension hyperplane projection along afﬁne subspace algorithm efﬁcient functional approach derived formulating normalized least mean square algorithm functional subspace. decorrelation property chypass however suboptimal since counts correlations among different kernels. abstract— propose novel online learning paradigm nonlinear-function estimation tasks based iterative projections space probability measure reﬂecting stochastic property input signals. proposed learning algorithm exploits reproducing kernel so-called dictionary subspace based fact ﬁnite-dimensional space functions reproducing kernel characterized gram matrix. l-space geometry provides best decorrelation property principle. proposed learning paradigm signiﬁcantly different conventional kernelbased learning paradigm senses whole space reproducing kernel hilbert space minimum mean squared error estimator gives best approximation desired nonlinear function dictionary subspace. preserves efﬁciency computing inner product well updating gram matrix dictionary grows. monotone approximation asymptotic optimality convergence proposed algorithm analyzed based variable-metric version adaptive projected subgradient method. numerical examples show efﬁcacy proposed algorithm real data variety methods including extended kalman ﬁlter many batch machine-learning methods multilayer perceptron. metric dominant factor controlling convergence behaviors online learning algorithms witnessed extensive studies adaptive ﬁltering well recent advances stochastic optimization related idea called space dilation accelerating convergence subgradient method minimization nondifferentiable functions). metric projection used extensively adaptive/online learning algorithms main subject present study metric online learning algorithms nonlinear-function estimation tasks. adaptive projected subgradient method numerical examples show proposed algorithm enjoys better decorrelation property chypass multikernel nlms algorithm outperforms selective-update strategy extended kalman ﬁlter real data well batch learning methods compared literature projection-based methods tend show better tracking/convergence computational complexity compared bayesian stochastic gradient descent approaches. using well-known kernel trick rigorous framework projection-based linear adaptive ﬁltering extended kernel adaptive ﬁltering monotone approximation signiﬁcant properties projection-based methods ensuring stable tracking target function keeps changing. convergence also guaranteed target function time-independent moreover virtue well-established algebraic properties nonexpansive mappings projection-based methods high ﬂexibility algorithm design parallel-projection multi-domain adaptive learning sparsity-aware algorithms variants projection-based methods also lead convergence speed comparable bayesian approaches despite computational complexities. compared stochastic gradient descent algorithms norma projection-based methods offer tracking/convergence guarantees without elaborate step-size tuning efﬁciently update estimate even dictionary grow addition practical advantages stable tracking capabilities convergence guarantees variants immediately analyzed witnessed present work itself. comparisons projection-based methods bayesian approaches kernel recursive least squares tracker stochastic gradient descent algorithm summarized table best geometry perfect decorrelation property online nonlinear-function estimation ﬁnite-dimensional subspace general? immediate answer question space probability measure determined probability density function input vector henceforth simply call space. addition nice geometric property space sufﬁciently large accomodate subspace even expands time goes space however rkhs function value speciﬁc point well-deﬁned presence equivalence class. arises central question penetrating paper. paper propose efﬁcient online nonlinearfunction learning paradigm based iterative projections space. proposed learning paradigm minimum mean squared error estimator gives best approximation target nonlinear function dictionary subspace highlight hypass algorithm implicitly exploits reproducing kernel dictionary subspace updating estimates show constructing reproducing kernel ﬁnite-dimensional subspace terms gram matrix basis thus extend strategy hypass space principle long gram matrix computable least approximately. idea following make function values well-deﬁned dictionary subspace considering equivalence class deﬁne reproducing kernel dictionary subspace space. implementing proposed method efﬁciently present three practical examples computing gram matrix. basis contains multiple gaussian functions different centers scale parameters inner product analytically computed assuming input vector obeys normal distribution perhaps improper constant distribution analogy conjugate prior noninformative prior bayesian statistics. gram matrix approximated atoms dictionary certain condition. gram matrix recursively updated using matrix inversion lemma rank update. show approximate linear dependency condition ensures lower bound amount mmse reduction newly entering dictionaryelement keeping mind link coherence condition lemma proposition section iiic. computational complexity proposed algorithm order euclidean approach selective-update strategy employed monotone approximation asymptotic optimization convergence proposed algorithm proved full-updating case within framework variable-metric here input vector assumed random vector probability density function additive noise time nonlinear function assumed real hilbert space equipped inner product implies known space contains gaussian rkhs subset hence assumption weaker usually supposed literature kernel adaptive ﬁltering. notation denote null vector metric projection point onto given closed convex deﬁned particular linear variety said orthogonal projection. given m-dimensional real vectors deﬁne stands transposition. given pair integers denote integers i.e. m+··· denote identity matrix dictionary time assume value arbitrary point predeﬁned representative equivalence class functions seen section functions used dictionary proposed learning paradigm scale parameter existing kernel/multikernel adaptive ﬁltering approaches exploit properties reproducing kernels κqhq emphasize that given space different inner products give different reproducing kernels. important follow discussions presented section iii. assume e.g. assumption holds case gaussian kernels {κq}j∈j time instant size here orthogonal decomposition indicates hypass regarded projecting current estimate onto hyperplane rkhs reproducing kernel κmn. note that holds ﬁrst clarify metric online learning implement present compute/approximate autocorrelation matrix efﬁciently explain online dictionary-construction technique. ﬁnally discuss complexity issue together selectiveupdate strategy complexity reduction. achieved adopting metric identity matrix theory. words possesses best geometry online nonlinear estimation sense decorrelation possibly expanding dictionary subspace. core motivation present study. note well known better-conditioned correlation matrix leads faster convergence linear adaptive ﬁlter example). question formulate projection-based online learning algorithm working seen section ii-d normal vector hyperplane gives direction update readily available reproducing kernel known. widely known space reproducing kernel value given point well deﬁned presence equivalence classes fortunately however need reproducing kernel dictionary subspace already mentioned. fact regards element value speciﬁc point well deﬁned. value assumed predeﬁned. ··h) becomes ﬁnite-dimensional real hilbert space value function point well deﬁned. case systematic construct reproducing kernel space shown below. proposition independent gram matrix flh. deﬁne entry relation matrix cross-correlation vector expectation taken input deﬁned following fact holds deﬁnition inner product fact autocorrelation matrix gram matrix dictionary real hilbert space i.e. entry given proposed learning paradigm mmse estimator best point actual search space contrast existing kernel adaptive ﬁltering paradigm consider online learning functional space surface viewed euclidean space determined following function modiﬁed coefﬁcient vector sign function step size. shown update nonlinear estimator given dictionary remaining issues discussed compute efﬁciently construct dictionary shall also present selective-update strategy reduce computational complexity section iii-d. present three options estimate/approximate efﬁciently. ﬁrst option assumes multiple gaussian functions different scales options applied general case. analytical approach present examples analytical expressions inner product obtained using analogy conjugate prior noninformative prior proposition gaussian functions scale parameters respectively. case unknown input distribution suppose input distribution. case using analogy noninformative prior improper input assumed distribute uniformly inﬁnite interval. inner product given finite-sample approach sample average also possible approximate sample average. j∈jn jln} ﬁxed realizations input vectors then time matrix approximated ements associated input vectors j∈jn might given dictionary data suppose rkhs supposed reproducing kernel then gram matrix dictionary hence approximation natural extension g-metric studied recursive approach inverse autocorrelation matrix appearing approximated recursively using similar trick kernel recursive least squares algorithm assume dictionary change incremental way; i.e. {rn− dictionary unchanged estimate autocorrelation matrix updated updated recursively presented proposition proposed online learning algorithm metric takes particular beneﬁt indicated following proposition. proposition suppose i.e. proposition states amount mmse reduction funh condition least long thereby yielding efﬁcient mmse reduction. condition violated alternative option could better performance-complexity tradeoff select elements maximally coherent check condition respect selected elements. idea recursive approach comes certainly recursive least squares algorithm iteratively minimizes squared errors. fact viewed variable-metric projection algorithm nearlyunity step size dictionary constructed based novelty criterion follows function depending measurement added dictionary satisﬁes prespeciﬁed novelty criterion. particular case multiple gaussian functions possible option following. coarsest gaussian function added ﬁner gaussian added dictionary satisﬁes novelty criterion coarser gaussians not. note that deﬁnition factors funh funh involve expectation brings issue computation discussed section iii-b. analytical approach employed computation proposition applied. finite-sample/recursive approach employed sample averages most-recent measurements instance although matrix inversion requires cubic complexity general complexity analytical approach recursive approach adopted. however still computationally expensive dictionary size becomes large. therefore practice selective-update strategy i.e. select subset cardinality mknlms chypass. here analytical finitesample recursive table correspond respectively analytical finite-sample recursive approaches presented section iii-b. complexity required inverse matrix denoted vinv table figure shows evolutions computational complexities algorithms γupdate regularization parameter coefﬁcient vector corresponding selected basis functions submatrix corresponding selected dictionary ˜dn. straightforward obtain applying proposition analytical approach employed. otherwise submatrix updated time section convergence analysis proposed algorithm presented full-updating case; i.e. case presenting analysis show proposed algorithm derived apsm continuous convex functions nonempty closed convex subset. arbitrary apsm generates sequence posed online learning algorithm including selective-update strategy dictionary constructions summarized algorithm complexity discuss computational complexity terms number multiplications required iteration normalized gaussian functions used. suppose coherence condition employed. coherence condition requires complexity whereas condition requires complexity. suppose also employ selective-update strategy efﬁciency factor used table summarizes overall per-iteration complexity proposed algorithm nlms knlms krls-t hypass remark analytical approach metric ﬁxed time instant assumption hence assumption valid. finite-sample approach samples taking sample averages make recursive approach employed approximation becomes tight increases assumption thus assumption reasonable. convergence asymptotic optimality sequence converges point limn→∞ limn→∞ analytical approach employed assumptions finitesample/recursive approach employed assumptions proof. claim veriﬁed note analysis apsm directly applied dictionary subspace algorithm updates current estimate within dictionary subspace time instant. applied considering space instead rkhs. finite-sample/recursive approach argument variable-metric apsm applied considering ﬁxed dictionary subspace dictionary well constructed. speciﬁcally validated assumptions validated assumptions apply remark monotone approximation signiﬁcant sense proposed algorithm even track time-varying target function convergence also guaranteed deterministically time-independent apsm reproduces proposed algorithm. precisely metric characterized autocorrelation matrix dictionary subspace proposed algorithm exploits efﬁciently computable lieu means metric used fairly close involves time variations. therefore present analysis based variable-metric version apsm ﬁrst present assumptions assumption step-size condition exist boundedness dictionary size exists data consistency exists hilbert space ··h) boundedinstantaneous-error hyperslab deﬁned δmin δmax s.t. δmin σmin σmin maximal eigenvalues respectively. small metric-ﬂuctuations exist constant positive-deﬁnite matrix rrn×rn nonempty subset integer positive constant s.t. rrn×rn satisﬁes remark assumption reasonable almost impossible guarantee convergence case dictionary subspace keeps changing indeﬁnitely. input space compact coherence condition used construct dictionary instance dictionary size remains ﬁnite time index goes inﬁnity remark assumption requires respect ideal case noise zero clear since evaluation functional rkhs linear continuous hence bounded exists constant follows then load electrical power output depends highly ambient temperature strongly correlated target variable individually predict target variable employed sole input variable present experiment comparison purpose. different machine learning regression methods compared terms root mean squared error dataset problem settings used compare rmse performance proposed algorithm linear nlms knlms hypass chypass machine learning regression methods analyzed note proposed algorithm designed online learning analyzed batch methods. observed affects target variable variables model trees rules achieves lowest rmse among machine learning regression methods. following cross-validation employed i.e. datasets equally partitioned sets size sets trained validate repeated times shufﬂing datasets. comparison purposes shufﬂed data used. rmses test runs averaged obtain ﬁnal results. note estimator trained online fashion ﬁrst half dataset trained estimator applied half. choose best parameters algorithm ﬁrst coarse search rough regions good parameters exploit random target functions. since primary focus present study online learning possibly time-varying target functions analyzing convergence rate scope. interested readers referred detailed analysis apsm gives bound close estimate optimal point iteration. ﬁrst show decorrelation property proposed algorithm. show efﬁcacy proposed algorithm applications online predictions real datasets. kernel adaptive ﬁltering toolbox used experiment. throughout experiments dictionary data used compute sample average finite-sample approach. compare eigenvalue spreads modiﬁed autocorrelation matrices proposed algorithm existing multikernel adaptive ﬁltering algorithms namely mknlms chypass. input vectors drawn i.i.d. uniform distribution within gaussian functions scale parameters employed dictionary constructed sole coherence condition threshold meaningful comparison algorithms share dictionary constructed based coherence condition deﬁned cartesian product gaussian rkhss avoid numerical errors computing matrix inverses metric matrix modiﬁed modiﬁed autocorrelation matrix computed approximated every iteration figure plots evolutions eigenvalue spreads algorithm. proposed algorithm attains smaller eigenvalue spread better decorrelation property. analytical approach works relatively well despite noninformative distribution input vector. although recursive approach shows degradations initial phase dictionary size increases rapidly eigenvalue spread tends decrease successfully iteration number increases. finite-sample approach shows stable performance expense high computational complexity practice selective-update strategy reduce complexity clariﬁcation ˆrns mknlms chypass proposed algorithm illustrated figure here colormap array matlab used illustrations. particular observe off-diagonal elements proposed algorithm suppressed better algorithms supported quantitatively figure table summarizes parameter settings means standard deviations rmses runs including batch methods. observed finite-sample approach achieves lower rmse batch methods excluding top-two methods moreover observed multiple gaussian functions leads signiﬁcantly better results monokernel counterparts. normalized learning curves averaged runs smoothed plotted figure evolutions dictionary size plotted figure figure shows instance estimate algorithm test ﬁnal target values input data selected test ﬁnal run. fig. results regression task online prediction electrical power output. estimators trained iterations online fashion trained estimator used subsequent iterations. search combinations best parameters achieving best rmse averaged runs. nonlinear estimators gaussian functions employed ﬁxed scale parameters advantage using multiple gaussian functions elaborative parameter tuning needed. monokernel methods best scale parameter selected. functions employed ﬁxed scale parameters. monokernel methods best scale parameter selected. coherence threshold tuned ﬁnal dictionary sizes become among hypass chypass proposed algorithm. regularization parameter γupdate tuned carefully krls-t sensitivity. table summarizes parameter settings. here γupdate budget forgetting factor regularization parameter krls-t respectively. learning curves plotted figure evolutions dictionary size plotted figure observed hypass chypass proposed algorithm outperform despite less information. finite-sample approach performs worse analytical/recursive approach small dictionary size. online learning paradigm presented paper signiﬁcant extension conventional kernel adaptive ﬁltering framework rkhs space reproducing kernel induces best geometry sense decorrelation. proposed algorithm built upon fact reproducing kernel dictionary subspace obtained terms gram matrix. three approaches computing gram matrix presented. remarkable difference kernel adaptive ﬁltering whole space reproducing kernel. mmse estimator gives best approximation target nonlinear function dictionary subspace contrast case kernel adaptive ﬁltering. also condition ensures lower bound amount mmse reduction newly entering atom. selective-update strategy presented reduce computational complexity. analysis presented show monotone approximation asymptotic optimality convergence proposed algorithm full-updating case. numerical examples demonstrated efﬁcacy proposed algorithm using selective-update strategy position vehicle time speed light clock offset zero-mean gaussian noise variance given available measurements vehicle nonmaneuvering motion task predict next pseudo range measurement ﬁrst gps. experiment compare nmse performance proposed algorithm nlms knlms krls-t hypass chypass seven available measurements used estimate vehicle position next measurements based nonmaneuvering motion model presented noise models used tuned using conﬁdence interval. algorithms exploit less information measurement ﬁrst gps. next measurement estimated real datasets showing superior performance extended kalman ﬁlter comparable performance best batch machine-learning method tested. ﬁnally emphasize proposed paradigm extended straightforwardly functional spaces long gram matrix computed efﬁciently. ushio yukawa projection-based dual averaging stochastic sparse optimization proc. ieee icassp shor minimization methods non-differentiable functions. nagumo noda learning method system identiﬁcation ieee trans. automatic control vol. june hinamoto maekawa extended theory learning identiﬁcation electrical engineering japan vol. yamada slavakis yamada efﬁcient robust adaptive ﬁltering algorithm based parallel subgradient projection techniques ieee trans. signal processing vol. yukawa yamada efﬁcient adaptive stereo echo canceling schemes based simultaneous multiple state data ieice trans. fundamentals electronics communications computer sciences vol. pairwise optimal weight realization —acceleration technique set-theoretic adaptive parallel subgradient projection algorithm ieee trans. signal processing vol. dec. yukawa slavakis yamada multi-domain adaptive learning based feasibility splitting adaptive projected subgradient method ieice trans. fundamentals electronics communications computer sciences vol. theodoridis slavakis yamada adaptive learning world projections unifying framework linear nonlinear classiﬁcation regression tasks ieee signal processing magazine vol. jan. vaerenbergh l´azaro-gredilla santamar´ıa kernel recursive least-squares tracker time-varying regression ieee trans. neural networks learning systems vol. aug. takizawa yukawa adaptive nonlinear estimation based parallel projection along afﬁne subspaces reproducing kernel hilbert space ieee trans. signal processing vol. aug. duan zhao chen robust kernel adaptive ﬁlters based mean p-power error noisy chaotic time series prediction engineering applications artiﬁcial intelligence vol. scardapane comminiello scarpiniti uncini online sequential extreme learning machine kernels ieee trans. neural networks learning systems vol. m¨uller mika ratsch tsuda scholkopf introduction kernel-based learning algorithms ieee trans. neural networks vol. toda yukawa online model-selection learning nonlinear estimation based multikernel adaptive ﬁltering ieice trans. fundamentals electronics communications computer sciences vol. yamada ogura adaptive projected subgradient method asymptotic minimization sequence nonnegative convex functions numerical functional analysis optimization vol. bauschke burachik combettes elser luke wolkowicz fixed-point algorithms inverse problems science engineering. springer science business media vol. berlinet thomas-agnan reproducing kernel hilbert spaces probability statistics. springer science business media luenberger optimization vector space methods. york vaerenbergh santamar´ıa comparative study kernel adaptive ﬁltering algorithms ieee digital signal processing workshop ieee signal processing education software available https//github.com/steven/kafbox/. bergstra bengio random search hyper-parameter optimization mach. learn. res. vol. skog handel time synchronization errors loosely coupled gps-aided inertial navigation systems ieee trans. intelligent transportation systems vol. motoya ohnishi received b.s. degree electronics electrical engineering keio university tokyo japan currently working toward m.s. degrees electronics electrical engineering keio university tokyo japan electrical engineering royal institute technology stockholm sweden. research assistant department automatic control royal institute technology visiting researcher gritslab georgia institute technology atlanta currently research assistant riken center tokyo japan. research interests include mathematical signal processing machine learning robotics. masahiro yukawa received b.e. m.e. ph.d. degrees tokyo institute technology respectively. studied visiting/guest researcher university york u.k. half year technical university munich germany four months. worked riken japan special postdoctoral researcher three years niigata university japan associate professor another three years. studied machine learning group technical university berlin visiting professor. currently associate professor department electronics electrical engineering keio university japan. associate editor ieee transactions signal processing multidimensional systems signal processing ieice transactions fundamentals electronics communications computer sciences research interests include mathematical adaptive signal processing convex/sparse optimization machine learning. yukawa recipient research fellowship japan society promotion science april march received excellent paper award young researcher award ieice respectively yasujiro niwa outstanding paper award ericsson young scientist award telecom system technology award young scientists’ prize commendation science technology minister education culture sports science technology kddi foundation research award ffit academic award member ieice.", "year": "2017"}