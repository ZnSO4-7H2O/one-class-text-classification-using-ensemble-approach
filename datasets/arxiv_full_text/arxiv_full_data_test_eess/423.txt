{"title": "On Quantizer Design to Exploit Common Information in Layered Coding of  Vector Sources", "tag": "eess", "abstract": " This paper studies a layered coding framework with a relaxed hierarchical structure. Advances in wired/wireless communication and consumer electronic devices have created a requirement for serving the same content at different quality levels. The key challenge is to optimally encode all the required quality levels with efficient usage of storage and networking resources. The approach to store and transmit independent copies for every required quality level is highly wasteful in resources. Alternatively, conventional scalable coding has inherent loss due to its structure. This paper studies a layered coding framework with a relaxed hierarchical structure to transmit information common to different quality levels along with individual bit streams for each quality level. The flexibility of sharing only a properly selected subset of information from a lower quality level with the higher quality level, enables achieving operating points between conventional scalable coding and independent coding, to control the layered coding penalty. Jointly designing common and individual layers' coders overcomes the limitations of conventional scalable coding and non-scalable coding, by providing the flexibility of transmitting common and individual bit-streams for different quality levels. It extracts the common information between different quality levels with negligible performance penalty. Simulation results for practically important sources, confirm the superiority of the work. ", "text": "successively reﬁnable ﬁnite delays distortion measure employed combination rates layer. moreover ﬁxed receive rates non-scalable coding conventional scalable coding highest lowest total transmit rate respectively. thus non-scalable coding conventional scalable coding represent extreme points trade total transmit rate distortions decoders ﬁxed receive rates. previous work proposed novel layered coding paradigm multiple quality levels inspired information theoretic concept common information dependent random variables wherein subset information lower quality level shared higher quality level. ﬂexibility enables efﬁciently extracting common information quality levels achieve intermediate operating points trade total transmit rate distortions decoders effect controlling layered coding penalty. early results established information theoretic foundations framework later paper employed framework within standard audio coder demonstrate potential. paper tackle important problem designing quantizers layered coding framework. first design quantizers quality levels ﬁxed receive rates. need design three quantizers common layer whose output sent decoders quantizers reﬁning common layer information quality levels whose output sent individually decoders. ﬁrst propose technique jointly design quantizers across layers framework focusing setting quality levels. propose iterative approach designing three quantizers wherein iteration quantizer updated minimize overall cost function others ﬁxed iterations repeated convergence. given common layer quantizer individual layer reﬁning quantizers intervals common layer jointly designed minimize overall cost function. also develop optimal update rules common layer quantizer minimizes overall cost accounting current individual layer quantizers. propose complexity approach deriving quantizers. first employ optimal quantizer given rate common layer. given quantizer design optimal quantizers different required rates conditioned common layer interval. finally optimal common layer rate estimated numerically trying multiple allowed common rates selecting highest abstract—this paper studies layered coding framework relaxed hierarchical structure. advances wired/wireless communication consumer electronic devices created requirement serving content different quality levels. challenge optimally encode required quality levels efﬁcient usage storage networking resources. approach store transmit independent copies every required quality level highly wasteful resources. alternatively conventional scalable coding inherent loss structure. paper studies layered coding framework relaxed hierarchical structure transmit information common different quality levels along individual streams quality level. ﬂexibility sharing properly selected subset information lower quality level higher quality level enables achieving operating points conventional scalable coding independent coding control layered coding penalty. jointly designing common individual layers’ coders overcomes limitations conventional scalable coding non-scalable coding providing ﬂexibility transmitting common individual bit-streams different quality levels. extracts common information different quality levels negligible performance penalty. simulation results practically important sources conﬁrm superiority work. high-speed internet wireless communication mobile limited resource receivers created extremely heterogeneous network scenario data consumption devices highly diverse decoding display capabilities accessing content networks time varying bandwidth latency. primary challenge maintain optimal signal quality wide variety users ensuring efﬁcient resources storage transmission across network. simplest solution address challenge storing transmitting independent copies signal every type user provider serves. solution highly wasteful resources results extremely poor scalability. alternative solution conventional scalable coding generates layered bit-streams wherein base layer provides coarse quality reconstruction successive layers reﬁne quality incrementally. depending network channel user constraints suitable number layers transmitted decoded yielding prescribed quality level. however widely recognized inherent loss scalable coding structure signiﬁcantly worse distortion compared independent encoding given receive rates sources finally propose iterative technique joint design vector quantizers layers framework. simplicity ﬁrst explain approach setting quality levels. explain extended approach relaxed hierarchical structures. develop cost function explicitly controls tradeoff distortions receive rates total transmit rate. propose iterative approach jointly designing vector quantizers layers wherein estimate optimal quantizer partitions layers given reconstruction codebooks optimal reconstruction codebooks quality levels given quantizer partitions iteratively convergence. rest paper organized follows. section present preliminaries. sections describe proposed methods. experimental results presented section concluding remarks section mathematical results much impact foundation information shannon’s point-to-point communication theorems however communication model assumed seminal contributions inadequate realities modern networks. extensions theory multi-terminal settings proven difﬁcult despite several spectacular advances many questions remain partially answered many questions regarding conversion available insights practical approaches remain unanswered. fundamental theorem rate-distortion theory minimum rate communication required convey sequence random variables receiver reconstruct sequence average distortion given view point rate-distortion theory scalable coding addressed context successive reﬁnement information problem motivated scalable coding encoder generates layers information namely base layer rate enhancement layer rate base layer provides coarse reconstruction source enhancement layer used ‘reﬁne’ reconstruction beyond base layer base enhancement layer distortions respectively equitz cover established conditions source-distortion pair achieve rate-distortion optimality layers simultaneously. source-distortion pairs called successively reﬁnable literature. result states rate-distortion function indicating minimum achievable rate prescribed distortion given minimizing mutual information conditional distributions subject distortion constraint. abundance research work devoted ﬁnding rate-distortion bounds various settings network consists encoder transmits correlated sources receivers using three channels common channel linking encoder receivers private channels linking individual receivers. channels assumed noiseless channel speciﬁed communication cost base layer employs dzq. enhancement layer quantizers conditioned base layer quantization interval dead zone interval established base layer uniform quantizer otherwise proposed novel layered coding framework arises lab’s lossy generalization common information special case network. derived information theoretic characterization different quality levels source supremum conditional densities optimality following markov chains hold section illustrate novel layered coding paradigm signiﬁcance effective extraction common information quantizer design example involving simple uniform distribution. uniformly distributed random variable shown optimal entropy constrained scalar quantizer rate integer uniform quantizer levels. fig. shows optimal partitions uniform random variable rates resulting distortion respectively. clearly boundary points quantizer completely align partitions quantizer i.e. scalable coding receive rates successively reﬁnable hierarchical coding results enhancement layer distortion worse independent quantization rate log. implies information required achieve proper subset information required achieve hand independent coding wasteful obviously considerable overlap information required achieve distortions. instead address challenge common information based layered coding framework wherein part information required achieve sent decoder reconstructing higher quality objective minimize communication cost achieving decoder reconstructions allowed distortion. problem clearly involves rate trade offs seek optimal rate triple derived asymptotic minimum cost achievable network. different concept common information dependent random variables proposed g´acs k¨orner ahlswede k¨orner gave alternative characterization characterized network maximum shared rates receive rates respective minimal i.e. lossy generalization g´acs-k¨orner distortions deﬁned subject optimal entropy constrained quantizer laplacian source derived dead-zone plus uniform threshold quantization classiﬁcation rule nearly uniform reconstruction rule deadzone quantizer uniform step size intervals except dead-zone interval around zero wider intervals. current scalable coding standards scalable hevc video scalable audio base layer employs quantizing source. then enhancement layer scaled version base layer quantizes base layer reconstruction error. fig. partitions quantizing uniformly distributed random variable using common information based layered coding paradigm rate sent decoders rates sent corresponding decoders. quantizers ensuring rate-distortion optimality decoders reduction total transmit rate compared non-scalable coding. example clearly demonstrates appropriately designed quantizers proposed layered coding paradigm efﬁciently extract information common different quality levels. fig. depicts notion vector cases. motivation next illustrate joint design scalar vector quantizers propose technique complexity design laplacian sources ﬁnally propose iterative approach joint design vector quantizers multi-layers. primarily interested designing entropy constrained scalar/vector quantizer ﬁxed received rates decoder respectively. constraint translates trade total transmit rate distortions decoders extremes non-scalable coding incurring highest yielding lowest distortion optimal distortion given rate; conventional scalable coding lowest yielding high distortion enhancement layer scalable coding penalty. design layered coding quantizers optimize trade subject prescribed receive rate constraints deﬁne cost function paradigm subsumes special cases conventional scalable coding non-scalable coding moreover framework provides extra degree freedom rate-distortion optimality layers potentially achieved lower total transmit rate non-scalable coding appropriately designing quantizers layers. continuing example quantizing uniformly distributed source receive rates employing layered coding paradigm rate sent decoders rates sent corresponding decoders achieves overquantizers partitions optimal individual fig. partitions quantizing training shown large black dots. depicts individual partitions rates log. depicts partitions common information based layered coding paradigm rate sent decoders rates sent corresponding decoders. respectively. also note weights cost function obviously redundant decided keep harmlessly redundant notation consistency past literature. quantizers designed minimize cost function achieve best weighted distortions decoders given total transmit rate receive rates. design quantizers iteratively quantizer updated iteration minimize overall cost function others ﬁxed repeating iterations convergence. following subsections superscripts refer parameters individual layers common layer respectively. individual layer quantizer design given common layer quantizer intervals partition boundaries need design ecsq interval individual layer note that given common layer individual quantizers layers absolutely effect individual layers design optimal entropy constrained quantizers common layer quantizer interval corresponding rates speciﬁcally iteratively optimize quantizer interval partitions reconstruction points minimize entropy constrained distortion smart initializations since dead zone interval contains truncated laplacian distribution intervals contain truncated exponential distribution select initializations step optimal entropy constrained quantizers corresponding non-truncated distributions. note that achieve non-zero common rate negligible rate partition points align closely partition points rate conditions alignment partitions derived dead-zone coarser divided intervals intervals divided intervals where integers ratio dead-zone interval length intervals’ length. design technique numerically estimates common layer closely satisﬁes conditions rate note proposed design technique ensure joint optimality quantizers particularly since independently optimize common layer quantizer without considering effect layers. despite assumption obtain considerable performance improvements. design quantizers iteratively alternating steps optimal partitioning optimal codebook estimation convergence similar generalized lloyd algorithm design m-codebook ecvq common layer common layer region design ecvq individual layer number subregions layer common layer representative levels region subregion probabilities respectively finally common layer regions probabilities following overall iterative algorithm common layer quantizer design unlike individual layer quantizer design partition change common layer quantizer impact distortions rates hence standard ecsq update rules applicable. given quantizer partition boundaries representative levels individual layers optimal partition points common layer updated note ecsq design common individual layers number partitions i.e. known. circumvent this simply initialize algorithm large number partitions iteration based given algorithm reduces number partitions necessary. complexity quantizer design laplacian sources practically important case laplacian source distribution could complexity non-joint design below region regions. obtain quantizers distortions received rates before lower transmit rate means cost reduced. hence optimal common layer quantizer cannot irregular. straightforward extend concept common information quality levels. biggest challenge shared information exist levels. hence clearly common layer quality levels cannot exploit redundancies present. fact common information exist subset quality levels leading number bit-stream layers growing combinatorially number quality levels. recently demonstrated combinatorial message sharing used improve theoretically achievable region related problem multiple descriptions coding approach useful obtain asymptotic bounds intractable real world applications maintain large number bit-stream layers. hence propose employing linearly growing rate-splitting approach layer receives individual packet common packets received lower layers speciﬁcally decoders plan generate packets consisting individual packets rates common packets rates r···rl send decoders following source leading ﬂexibility structure quantizer regions. could potentially irregular quantizers common layer alone common individual layers combine together result overall regular quantizer. surprisingly despite ﬂexibility show following lemma regularity common layer quantizer true simple assumption. lemma ﬁnal overall quantization cells regular optimal common layer quantization cells. proof. assume common layer’s region regular. withloss generality assume consists regular subregions deﬁne common layer quantizer partition dividing region regions since assumed ﬁnal overall quantization regions regular subregions individual layers forced wholly contained within either region implies dividing common layer region regions alter ﬁnal overall quantization regions. since distortions decoder depend ﬁnal overall quantization regions change common layer region divided regions also note that receive rate decoder since ﬁnal overall quantization regions altered rate contributions subregions common layer region gets redistributed rate contributions subregions common layer region thus change. however dividing clearly increase ﬁrst second terms cost represents distortion rate penalties respectively. finally using approach explained section iv-c design best entropy constrained vector quantizers. speciﬁc example quality level coding step algorithm section iv-c assign sample training region common layer shared three levels subregion within private layer quality level subregion within common layer shared quality level subregion within private layer quality level subregion within private layer quality level minimize cost steps extended many layers appropriately. provide experimental results laplacian sources measure distortions decoder ﬁrst experiment ﬁxed receive rates fig. shows total transmit/storage rate versus excess distortion d+d−d∗−d∗ curve obtained employing quantizers designed proposed iterative technique various common layer rates ranging bits bits convex hull nonscalable scalable coding obtained using time sharing argument. note scalable coding employed current standards around worse efﬁcient scalable coding subsumed approach around distortion loss compared non-scalable coding. results clearly demonstrate that exploiting concept common information proposed approach achieve intermediate operating points considerably better performance compared convex hull non-scalable scalable coding. also seen fig. obtain interesting operating point distortions close non-scalable coding reduction total transmit rate compared non-scalable coding. thus conducted experiments different ﬁxed receive rate constraints tabulated corresponding results transmit rate savings negligible distortion loss table signiﬁcant transmit rate savings ranging clearly demonstrate capability proposed technique efﬁciently extract common information different quality levels. savings translate signiﬁcant operating cost reduction data centers storage transmitting caching intermediate nodes content distributors currently coding individual non-scalable copies different quality levels. experiments used laplacian source distortions decoder measured ﬁrst experiment used ﬁxed receive rates fig. plot versus curve obtained employing quantizers designed proposed technique various common layer rates ranging bits bits convex hull proposed paradigm obtained using time sharing argument. note scalable coding employed current standards around distortion loss compared efﬁcient scalable coding around distortion loss compared nonscalable coding. proposed technique operate points along convex hull considerably better performance compared scalable coding current standards. moreover note fig. equivalently obtain distortions close non-scalable coding reduction total transmit rate compared non-scalable coding. conducted another experiment different ﬁxed receive rate combinations obtained similar results transmit rate savings negligible distortion loss shown table finally last experiment used multivariate gaussian source multi layer scenario case encoder generates packets rates decoder receive packets rates respectively. total transmit rate proposed common information paradigm note comparison layered ecvq multi-layer case shows average compression efﬁciency multi-layer design roughly reduction total transmit rate compare reduction layer case. fundamental design technique common information based layered coding framework proposed wherein jointly designing common individual layers’ vector quantizers overcomes limitations conventional scalable coding non-scalable coding providing ﬂexibility transmitting common individual bit-streams different quality levels. proposed iterative scalar vector quantizer design technique optimizes quantizers jointly minimize overall cost iteration. extracts common information different quality levels negligible performance penalty also achieves better operating points tradeoff total transmit rate distortions decoders. simulation results practically important laplacian sources well multivariate normal sources conﬁrm usefulness proposed approach. note comparison joint design complexity design shows that joint design better ability extract common information however complexity approach faster extract common information. next experiment multivariate normal distribution non-scalable coding employ generalized lloyd algorithm design optimal ecvq. note gaussian sources successively reﬁnable asymptotically true ﬁnite delays hence beneﬁt proposed common information paradigm. shown table similar scalar case signiﬁcant reduction total transmit rate compared non-scalable coding negligible loss performance. results clearly demonstrate universal effectiveness proposed approach scalar vector quantizer design. viswanatha akyol rose combinatorial message sharing reﬁned multiple descriptions achievable region roc. ieee international symposium information theory aug. ahlswede korner common information related characteristics correlated information sources preprint. presented prague conference information theory information technology coding audio-visual objects part audio subpart general audio coding gy¨orgy linder optimal entropy-constrained scalar quantization uniform source ieee transactions information theory vol. november zhang framework efﬁcient progressive granularity scalable video coding ieee transactions circuits systems video technology vol. aggarwal regunathan rose efﬁcient bit-rate scalability weighted squared error optimization audio coding ieee transactions audio speech language processing vol. july gray wyner source coding simple network bell systems technical journal vol. nov. wyner common information dependent random variables ieee transactions information theory vol. nanjundaswamy viswanatha rose relaxing strict hierarchical constraints layered coding audio signals ieee international conference acoustics speech signal processing salehifar nanjundaswamy rose quantizer design exploiting common information layered coding ieee international conference acoustics speech signal processing mar. mehdi salehifar mehdi salehifar received b.sc degree electrical computer engineering university tehrantehraniran m.sc. ph.d. degrees electrical computer engineering university california santa barbara respectively. currently working electronics senior research engineer. research interests include signal processing general quantization theory information theory video/audio coding. worked electronics senior video researcher intern well qualcomm audio researcher. winner several titles national international mathematic competitions. tejaswi nanjundaswamy tejaswi nanjundaswamy received degree electronics communications engineering national institute technology karnataka india m.s. ph.d. degrees electrical computer engineering university california santa barbara respectively. currently post-doctoral researcher signal compression ucsb focuses audio/video compression processing related technologies. worked ittiam systems bangalore india senior engineer audio codecs effects development. also interned multimedia codecs division texas instruments india .dr. nanjundaswamy associate member audio engineering society student technical paper award convention. kenneth rose kenneth rose received ph.d. degree california institute technology pasadena joined department electrical computer engineering university california santa barbara currently professor. main research activities areas information theory signal processing include rate-distortion theory source source-channel coding audiovideo coding networking pattern recognition non-convex optimization. interested relations information theory estimation theory statistical physics potential impact fundamental practical problems diverse disciplines.prof. rose corecipient william bennett prize paper award ieee communications society well ieee signal processing society best paper awards.", "year": "2018"}