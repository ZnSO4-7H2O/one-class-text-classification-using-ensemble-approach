{"title": "On Using Backpropagation for Speech Texture Generation and Voice  Conversion", "tag": "eess", "abstract": " Inspired by recent work on neural network image generation which rely on backpropagation towards the network inputs, we present a proof-of-concept system for speech texture synthesis and voice conversion based on two mechanisms: approximate inversion of the representation learned by a speech recognition neural network, and on matching statistics of neuron activations between different source and target utterances. Similar to image texture synthesis and neural style transfer, the system works by optimizing a cost function with respect to the input waveform samples. To this end we use a differentiable mel-filterbank feature extraction pipeline and train a convolutional CTC speech recognition network. Our system is able to extract speaker characteristics from very limited amounts of target speaker data, as little as a few seconds, and can be used to generate realistic speech babble or reconstruct an utterance in a different voice. ", "text": "match high level network activations content utterance simultaneously matching level statistics computed lower level activations style utterance. julesz proposed visual texture discrimination function image’s level statistical properties. mcdermott applied idea sound showing perception sound textures relies matching certain level signal statistics. furthermore following earlier work image texture synthesis demonstrated simple sound textures rain synthesized using gradient-based optimization procedure iteratively update white noise signal match statistics observed texture signals. recently gatys proposed similar statistic matching algorithm synthesize visual textures. however instead manually designing relevant statistics function image pixels utilized deep convolutional neural network discriminatively trained image classiﬁcation task. speciﬁcally proposed match uncentered correlations neuron activations selected network layer. formally rw×h×d denote activations n-th convolutional layer width layer height number ﬁlters. gram matrix uncentered correlations rd×d deﬁned gatys demonstrated realistic visual textures synthesized matching gram matrices. words statistics necessary texture synthesis correlations values convolutional ﬁlters taken pixels given convolutional ﬁlter map. note gram features equation averaged image pixels therefore stationary respect pixel location. approximate network inversions statistic-matching texture synthesis generate images minimizing loss function backpropagation towards inputs. approaches combined sample images whose content similar seed image whose texture similar another approach style transfer attractive leverages pretrained neural network learned distribution natural images therefore require large dataset generation time single image inspired recent work neural network image generation rely backpropagation towards network inputs present proof-of-concept system speech texture synthesis voice conversion based mechanisms approximate inversion representation learned speech recognition neural network matching statistics neuron activations different source target utterances. similar image texture synthesis neural style transfer system works optimizing cost function respect input waveform samples. differentiable mel-ﬁlterbank feature extraction pipeline train convolutional speech recognition network. system able extract speaker characteristics limited amounts target speaker data little seconds used generate realistic speech babble reconstruct utterance different voice. index terms— texture synthesis voice conversion style transdeep neural networks family ﬂexible powerful machine learning models. trained discriminatively become technique choice many applications including image recognition speech recognition machine translation additionally neural networks used generate data applied speech synthesis image generation image inpainting superresolution representation learned discriminatively trained deep neural network approximately inverted turning classiﬁcation model generator. exact inversion impossible backpropagation algorithm used inputs activate network desired manner. technique applied computer vision domain order gain insights network operation adversarial examples make imperceptible modiﬁcations image inputs order change network’s predictions synthesize textures regenerate image according style another referred style transfer work investigate possibility converting discriminatively trained speech recognition network generator. particular investigate generating waveforms based solely activations selected network layers giving insights nature network’s internal representations speech texture synthesis generating waveforms result neuron activations shallow layers whose statistics similar real speech voice conversion speech analog image style transfer previous methods combined generate waveforms fig. spectrograms waveforms reconstructed layers speakers vctk dataset. reconstructions nearly exact reconstructions noisy barely intelligible. taking elementwise logarithm ﬁlterbank features. computing deltas delta-deltas using convolution time. feature extraction pipeline facilitates methods reconstructing waveform samples gradient-based optimization backpropagation directly waveform gradient-based optimization linear spectrogram followed grifﬁn-lim phase reconstruction. dual strategy works best ﬁrst perform spectrogram reconstruction invert spectrogram yield initial waveform optimized directly. l-bfgs optimizer optimization stages. implement waveform reconstruction based network activations following relu non-linearity speciﬁed layer. figure shows spectrograms waveform reconstructions speakers vctk dataset qualitatively established waveforms reconstructed shallow network layers intelligible speaker clearly identiﬁed. audible phase artifacts introduced reconstructions layer above ﬁnal pooling operation time. speech quality degrades many speaker characteristics preserved reconstructions fully connected layers. listening reconstructions layer remains possible recognize speaker’s gender. order reconstruct waveforms activations fully connected layers reconstruction cost must extended term penalizing differences total energy feature frame reference reconstruction. hypothesize network’s representation deeper layers learned degree invariance signal magnitude hampers reconstruction realistic signals. example network reliably predicts blank symbol silence white noise different amplitudes. addition energy matching penalty enables network correctly reconstruct silent segments. however even additional penalty reconstructions layers apply texture generation stylization techniques speech train fully convolutional speech recognition network following wall street journal dataset. network trained predict character sequences end-to-end fashion using criterion. parameters typical speech recognition network waveforms sampled segmented windows taken every window extract log-mel ﬁlterbank features augmented deltas delta-deltas. layer network architecture derived -dimensional convolution max-pooling -dimensional convolution max-pooling -dimensional convolution -dimensional convolution max-pooling blocks -dimensional convolution -dimensional fully connected layers ﬁlter pooling window sizes speciﬁed time frequency. layers batch normalization relu activations dropout regularization. convolutional layers dropout keep probability fully connected layers keep probability network trained using asynchronous workers adam optimizer using learning rate annealing also weight decay decoded using extended trigram language model kaldi recipe model reaches eval eval. network reach state-ofthe-art accuracy dataset reasonable performance easily amenable backpropagation towards inputs. even though network trained corpus vctk dataset subsequent experiments. goal generate waveforms result particular neuron activation pattern processed deep network. ideally waveform samples would optimized directly using backpropagation algorithm. possibility train networks operate waveforms however also possible implement typical speech feature pipeline differentiable way. follow second approach facilitated readily available tensorﬂow implementation signal processing routines waveform framing hamming window application. computation multiplies waveform frames fig. spectrograms textures synthesized gram matrices computed utterances vctk speakers well short utterance polish deeper layers used generated sound captures temporal structure. intuitively listening hard discern words whereas hear word boundaries c-c. also characteristic lower pitch synthesized male voices. also observe network training crucial gram features become speaker-selective texture synthesis work. random initialization network behaves differently training gram tensors computed shallow layers untrained network less sensitive speaker identity corresponding layers trained network deeper layers don’t exhibit dramatic decrease speaker sensitivity. contrast image texture synthesis style transfer reported work randomly initialized networks figure shows spectrograms generated speech textures based speech vctk dataset male native polish speaker. gram tensor computed ﬁrst layer activations captures fundamental frequency harmonics yields fairly uniform temporal structure. features computed deeper layers used longer term phonemic structure seen although overall speech intelligible. consequence increased temporal receptive ﬁeld ﬁlters deeper layers single activation function structure spanning tens frames enabling fig. embeddings speaker vectors computed original vctk recordings reconstructions network synthesized waveforms voice converted waveforms. synthesized voice converted utterances close original utterances reconstructions early layers. reconstructions deep layers converge single point indicating speaker identity lost. evaluate well reconstructions based different layers capture characteristics different speakers visualize embedding vectors computed using internal speaker identiﬁcation system uses resnet- architecture trained librivox using tripletloss nearest neighbor classiﬁcation using embeddings obtains nearly perfect accuracy original vctk signals. figure shows two-dimensional embedding vectors. reconstructions early layers signals speaker cluster together overlap. depth increases embeddings speakers begin converge single point indicating speakers become progressively difﬁcult recognize. conclude network’s internal representation becomes progressively speaker invariant increasing depth desirable property speaker-independent speech recognition. unlike image textures whose statistics assumed stationary across spatial dimensions dimensions speech spectrogram features i.e. time frequency different semantics treated differently. sound textures stationary time nonstationary across frequency. suggests features extracted layer activations involve correlations time alone. rt×f×d tensor activations n-th layer network consists ﬁlters computed frames frequencies. temporally stationary gram tensor rf×f×d×d written demonstrate gram tensors capture speaker identity using features simple nearest neighbor speaker identiﬁcation system. figure shows speaker identiﬁcation accuracy system ﬁrst utterances ﬁrst speakers vctk dataset. using lower network layers yields accuracy close whereas using similar gram tensors mel-spectrograms extended deltas delta-deltas yields success neural image style transfer prompted attempts apply audio. roberts trained audio clip embeddings using convolutional network applied directly waveforms attempted generate waveforms maximizing activations neurons selected layers. authors claim noisy results attribute quality learned ﬁlters. ulyanov used untrained single-layer network synthesize simple audio textures keyboard machine attempted audio style transfer different musical pieces. recent work wyse similar ours. examines application pretrained convolutional networks image recognition environmental sound classiﬁcation. example style transfer human speech crowing rooster demonstrates importance using network trained audio features line ﬁndings. best knowledge work ﬁrst demonstrate style transfer techniques applied speech recognition networks used voice conversion. speech babble sounds previously generated using unconditioned wavenet model trained synthesize speech waveforms. contrast demonstrate complex sound textures generated speech recognition network using limited amounts data target speaker. typical voice conversion systems rely advanced speech representations straight dedicated conversion function trained aligned parallel corpora different speakers. overview state-of-the-art area seen recent voice conversion challenge system produces samples inferior quality operates using different novel principle rather learning frame-to-frame conversion uses speech recognition network deﬁne speaker similarity cost optimized change perceived identity speaker. demonstrate proof-of-concept speech texture synthesis voice conversion system derives statistical description target voice activations deep convolutional neural network trained perform speech recognition. main beneﬁt proposed approach ability utilize limited amounts data target speaker. leveraging distribution natural speech captured pretrained network seconds speech sufﬁcient synthesize recognizable characteristics target voice. however proposed approach also quite slow requiring several thousand gradient descent steps. addition synthesized utterances relatively quality. proposed approach extended ways. first analogously fast image style transfer algorithms gram tensor loss used additional supervision speech synthesis neural network wavenet tacotron example might feasible style loss extend neural speech synthesis system wide speakers given seconds recorded speech one. second method depends pretrained speech recognition network. work used fairly basic network using feature extraction parameters tuned speech recognition. synthesis quality could probably improved using higher sampling rates increasing window overlap running network linear- rather mel-ﬁlterbank features. fig. spectrograms voice conversion mapping vctk utterance speakers performed matching neuron activations content utterance gram features computed speaker identity utterances methods described previous sections combined produce speech analog image style transfer voice conversion system. speciﬁcally reconstruct deep-layer activations content utterance shallow-layer gram features identity style utterances. listening converted samples found good tradeoff matching target speaker’s voice sound quality occurs optimizing loss spans layers layers matched style utterances using gram features layers c-fc matched content utterance. normalize contribution layer cost dividing squared difference gram activation matrices dimensionality. furthermore gram features style layers weight activations content layers weight activations layers fc-fc weight base reconstruction deepest layers provide signal middle responsible ﬁnal voice quality. speaker remains identiﬁable reconstructions layers described section including layers content loss leads natural sounding synthesis. speaker identity still changes gram feature weight sufﬁciently large. spectrograms utterance generated using procedure shown figure spectrograms converted utterances contain different pitch consistent opposite gender. however content loss applied directly neuron activations exact temporal structure content utterance retained. highlights limitation approach ﬁxed temporal alignment content utterance means unable model temporal variation characteristic different speakers changes speaking rate. authors thank yoram singer colin raffel matt hoffman joseph antognini navdeep jaitly helpful discussions inspirations skerry-ryan signal processing jansen sourish chaudhuri help speaker identiﬁcation system. graves fern´andez gomez schmidhuber connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks proc. icml. agiomyrgiannakis egberts henderson szczepaniak fast compact high quality lstm-rnn based statistical parametric speech synthesizers mobile devices arxiv. dong tang learning deep convolutional network image super-resolution european conference computer vision. springer simonyan vedaldi zisserman deep inside convolutional networks visualising image classiﬁcation models saliency maps arxiv. mcdermott oxenham simoncelli sound texture synthesis ﬁlter statistics ieee workshop applications signal processing audio acoustics mcdermott simoncelli sound texture perception statistics auditory periphery evidence sound synthesis neuron vol. wyse audio spectrogram representations processing convolutional neural networks arxiv. kawahara masuda-katsuse cheveigne restructuring speech representations using pitch-adaptive time– frequency smoothing instantaneous-frequency-based extraction possible role repetitive structure sounds speech communication vol. johnson alahi fei-fei perceptual losses real-time style transfer super-resolution european conference computer vision. springer dumoulin shlens kudlur learned representa-", "year": "2017"}