{"title": "Autoencoder based image compression: can the learning be quantization  independent?", "tag": "eess", "abstract": " This paper explores the problem of learning transforms for image compression via autoencoders. Usually, the rate-distortion performances of image compression are tuned by varying the quantization step size. In the case of autoen-coders, this in principle would require learning one transform per rate-distortion point at a given quantization step size. Here, we show that comparable performances can be obtained with a unique learned transform. The different rate-distortion points are then reached by varying the quantization step size at test time. This approach saves a lot of training time. ", "text": "paper explores problem learning transforms image compression autoencoders. usually ratedistortion performances image compression tuned varying quantization step size. case autoencoders principle would require learning transform rate-distortion point given quantization step size. here show comparable performances obtained unique learned transform. different rate-distortion points reached varying quantization step size test time. approach saves training time. image coding standards linear invertible transforms convert image coefﬁcients statistical dependencies suited scalar quantization. notably discrete cosine transform commonly used reasons image-independent implying need transmitted approaches optimal orthogonal transform terms rate-distortion assuming natural images modeled zero-mean gaussian-markov processes high correlation deep autoencoders shown promising tools ﬁnding alternative transforms autoencoders learn encoder-decoder non-linear transform natural images. best image compression algorithms based autoencoders transform learned ratedistortion point given quantization step size. then quantization step size remains unchanged test time training test conditions identical. contrast image coding standards implement adaptive quantizations quantization imposed training? answer this propose approach transform quantization learned jointly. then investigate whether test time compression falls apart coefﬁcients obtained learned transform quantized using quantization step sizes differ training stage. code reproduce numerical results train autoencoders available online. matrices tensors denoted bold letters. frobenius norm elementwise multiplication autoencoder neural network encoder parametrized computes representation data decoder parametrized gives reconstruction figure autoencoders used denoising dimensionality reduction. used compression representation also quantized leading quantized representation autoencoder fully-connected layers number parameters depends image size. implies autoencoder trained image size. avoid this architecture without fully-connected layer chosen. exclusively comprises convolutional layers non-linear operators. case rh×w×m feature maps size figure basic autoencoder training minimizes image reconstruction error order create rate-distortion optimization authors minimization entropy quantized representation. moreover allocation performed learning normalization feature encoder followed normalizations encoder side parametrized denoted similarly normalizations decoder side parametrized followed decoder denoted finally leads matrix rh×w×m contains realizations {εij}j=...n continuous random variable probability density function function minimized differentiable respect thus learned gradient-based methods. however {δi}i=...m cannot learned function minimized differentiable respect {δi}i=...m. resolved using change variable random variable following continuous uniform distribution support minimization {δi}i=...m feasible matrix rh×w×m contains realizations {τij}j=...n coefﬁcients matrix rh×w×m equal detail left unknown. similar manner replaced function parametrized learned ˜pi. three groups parameters {δi}i=...m {ψ}i=...m. three groups learned alternating three different stochastic gradient descents. training heuristics detailed code. section developped approach learning explictly transform quantization step size feature evaluating approach section section studies would happen test time coefﬁcients quantized using quantization step sizes differ training stage. ﬁrst requires understanding internal structure training. section studies different feature maps training. deep convolutional autoencoder must ﬁrst built trained. composition convolutional layer generalized divisive normalization convolutional layer convolutional layer. probability mass function quantized feaˆ ture coefﬁcients {ˆyij}j=...n. expectation approximated averaging training images. unfortunately makes minimization unusable. indeed derivative quantization respect input point. consequently cannot learned gradient-based methods around issue ﬁxes quantization step size approximates uniform scalar quantization addition uniform noise support note that even though quantization step size ﬁxed allocation varies different feature maps normalizations. next section consider instead remove normalizations learn explicitly quantization step size feature address problem optimizing quantization step size feature quantization function minimized implicit function quantization step sizes {δi}i=...m. target make explicit function {δi}i=...m. {...−δi ...} probability density function feature coefﬁcients {yij}j=...n denotes probability density function continuous uniform distribution support normalizations removed using becomes reverse composition replacing inverse generalized divisive normalization convolutional layer transpose convolutional layer important stress channel convolutional strides paddings chosen times smaller respectively height width therefore number pixels twice number coefﬁcients training contains luminance images size extracted imagenet minimization note that placed immediately igdn placed immediately learned autoencoder architecture training would correspond training test luminance images size created kodak suite. here refers test luminance image. figure shows normed histogram feature feature averaged test set. every feature except normed histogram similar displayed. precise let’s write probability density function laplace distribution mean scale exists well normed histogram feature note scales belong figure transformed coefﬁcients zero-mean laplace distribution proves uniform reconstruction quantizer constant decision offsets approaches optimal scalar quantizer terms squared-error distortion quantization step size. case laplace probability density functions zero-mean uniform scalar quantizers used instead urq. point problematic extra luminance images used compute approximation mean feature then test time feature centered being quantized. note {µi}i=...m depend test luminance images thus incurring transmission cost. regarding point must noted decoder mapping exactly decoder mapping uniform scalar quantization quantization step size. since case comes close requirements proof test time rate-distortion trade-off collapse quantization step sizes deviate learned values. veriﬁed section shortcoming previous ﬁtting reveal information matrix encodes. discover visualizations needed. common exploring deep convolutional neural network trained image recognition look image input resulting maximization pixels given neural activation precisely maximize image pixels given neural activation output class probability. shows image features characterize class according cnn. case maximization image pixels given coefﬁcient yield interpretable images. indeed coefﬁcients bounded. explain maximization often returns saturated images. case unique transform learned allocation done learning quantization step size feature map. precisely single autoencoder trained {δi}i=...m learned section test time rate varies quantization step sizes equal learned quantization step sizes multiplied case unique transform learned aldetails location learned normalizations. single autoencoder trained training quantization step size test time rate varies quantization step size spans case autoencoder architecture described beginning section case also placed igdn placed autoencoders trained luminance images size extracted imagenet. then test time luminance images kodak suite inserted autoencoders. rate estimated empirical entropy quantized coefﬁcients assuming quantized coefﬁcients i.i.d. note that case also implemented binarizer binary arithmetic coder compress quantized coefﬁcients losslessly code. difference estimated rate exact rate lossless coding always smaller bbp. figure shows rate-distortion curves averaged luminance images. jpeg curve obtained using imagemagick. curve computed version hm-.. hardly difference case. means explicit learning transform quantization step sizes equivalent learning transform normalizations quantization step size imposed. note that case learning {δi}i=...m involves parameters whereas case involves parameters. case perform well case. minimization training provide learned transforms used various quantization step sizes test time. convenient train autoencoder compression rate single training takes days nvidia finally learned transforms yield better rate-distortion performances jpeg. quality image reconstruction experiment figure another experiment luminance images created bsds seen online. using unique transform learned autoencoders various quantization step sizes test time possible compress well learning transform ratedistortion point given quantization step size. moreover learned transformed outperform image compression algorithms based transforms. alternatively information feature encodes seen follows. coefﬁcients feature feature maps contains signiﬁcant information. then single coefﬁcient feature displayed. selected near tails laplace distribution feature figure shows crop top-left single coefﬁcient located top-left corner feature feature encodes spatially localized image feature whereas feature encodes spatially extended image feature. moreover image feature turned symmetrical feature respect mean pixel intensity moving right tail laplace distribution feature left tail. linear behaviour observed feature interesting that given ﬁtting section similar coefﬁcients blocks prediction error samples terms distribution. however looking information feature encodes nothing coefﬁcients. evaluate terms rate-distortion performances whether learning quantization matters whether test time efﬁcient quantize coefﬁcients obtained learned transform using quantization step sizes differ training stage. done comparing three cases. case follows approach transform learned rate-distortion point allocation learned normalizations. details autoencoder trained training test time quantization step size ﬁxed gary sullivan jens-rainer woo-jin thomas wiegand overview high efﬁciency video coding standard ieee transactions circuits systems video technology vol. december david martin charless fowlkes doron jitendra malik database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics proc. int’l conf. computer vision george toderici sean o’malley sung hwang damien vincent david minnen shumeet baluja michele covell rahul sukthankar variable rate image compression recurrent neural networks iclr", "year": "2018"}