{"title": "Fundamental Limits on Data Acquisition: Trade-offs between Sample  Complexity and Query Difficulty", "tag": "eess", "abstract": " We consider query-based data acquisition and the corresponding information recovery problem, where the goal is to recover $k$ binary variables (information bits) from parity measurements of those variables. The queries and the corresponding parity measurements are designed using the encoding rule of Fountain codes. By using Fountain codes, we can design potentially limitless number of queries, and corresponding parity measurements, and guarantee that the original $k$ information bits can be recovered with high probability from any sufficiently large set of measurements of size $n$. In the query design, the average number of information bits that is associated with one parity measurement is called query difficulty ($\\bar{d}$) and the minimum number of measurements required to recover the $k$ information bits for a fixed $\\bar{d}$ is called sample complexity ($n$). We analyze the fundamental trade-offs between the query difficulty and the sample complexity, and show that the sample complexity of $n=c\\max\\{k,(k\\log k)/\\bar{d}\\}$ for some constant $c>0$ is necessary and sufficient to recover $k$ information bits with high probability as $k\\to\\infty$. ", "text": "consider query-based data acquisition corresponding information recovery problem goal recover binary variables parity measurements variables. queries corresponding parity measurements designed using encoding rule fountain codes. using fountain codes design potentially limitless number queries corresponding parity measurements guarantee original information bits recovered high probability sufﬁciently large measurements size query design average number information bits associated parity measurement called query difﬁculty minimum number measurements required recover information bits ﬁxed called sample complexity analyze fundamental trade-offs query difﬁculty sample complexity show sample complexity max{k constant necessary sufﬁcient recover information bits high probability query-based data acquisition arises diverse applications including crowdsourcing active learning experimental design community recovery clustering graphs applications query-based data acquisition modeled questions problem oracle player oracle knows values information bits player aims recover player designs queries oracle receives answers oracle. paper consider query-based data acquisition goal recovering values variables assume variables answers binary consider parity check type measurement corresponds exclusive subset information bits. querying parity symbols information bits generalizes questions model generalization focus paper wide range applications particular crowdsourcing systems consider crowdsourcing system consisting number workers particular task expected work assume task classify collection images exclusive groups e.g. whether image suitable children. worker system given query subset images asked provide binary answer regarding images. assume worker skip query worker unsure answer. probability worker skips query unknown stage query design different workers system depending abilities efforts. therefore query designer’s point view natural assume random subset designed queries answered workers crowdsourcing system. query designer’s objective design queries received number answers exceeds threshold regardless subset answers collected original binary bits recovered high probability. paper show fountain codes naturally suited crowdsourcing query design problem. fountain codes type forward error correcting codes suitable binary erasure channels unknown erasure probabilities. type codes subject much research reliable internet packet transmissions packets transmitted source randomly lost arrive destination. chung∗ school electrical engineering kaist south korea. department mathematical sciences kaist south korea. alfred hero department eecs university michigan. work partially supported national research foundation korea grant number reaa united states army research ofﬁce grant wnf---. input symbols fountain codes produce potentially limitless number parity measurements also called output symbols. using well-designed fountain codes guarantee that given output symbols size small overhead input symbols recovered high probability. examples fountain codes lt-codes raptor codes using fountain codes design potentially limitless number queries desired properties suitable crowdsourcing example. however fountain code framework must extended order account worker’s limited capacity answer difﬁcult queries. query difﬁculty deﬁned average number input symbols required compute single parity measurement. query difﬁculty related encoding complexity one-stage encoding different encoding complexity encoding done multiple stages. query difﬁculty represents number input symbols average worker must know calculate parity measurement. depending query difﬁculty number answers required recover input symbols vary greatly. call minimum number measurements required recover input symbols sample complexity. sample complexity function query difﬁculty well number input symbols recovered. consider extreme cases. first consider case query difﬁculty equal speciﬁcally assume query asks value variable time. since known queries would answered workers stage query design queries designed uniformly randomly picking variable time. querying scenarios randomly selected measurements order recover information bits error probability less constant required number queries scales hand query designed generate parity measurement randomly selected bits time required number measurements constant therefore extreme cases observe query difﬁculty equal sample complexity scales whereas query difﬁculty order sample complexity scales question sample complexity scales query difﬁculty increases paper analyze fundamental trade-offs sample complexity query difﬁculty recovering information bits. papers analyzed trade-offs assumed parity measurements involve ﬁxed number input symbols. case pairwise measurements considered general integer considered. note paper generalize work ﬁxing number instead allowing number follows distribution denotes probability value chosen analyze sample complexity function query difﬁculty number input symbols. assuming follows prescribed distribution generate potentially limitless parity measurements using encoding rule employed fountain codes; guaranteeing ﬁxed number measurements possible recover input symbols high probability. framework thus suitable situations parity measurements erased arbitrary probabilities thus required ability generate potentially limitless number queries corresponding parity measurements. previous frameworks main contribution paper specify fundamental trade-offs sample complexity query difﬁculty generalized measurement model. show sample complexity necessary sufﬁcient recover input symbols high probability scales constant note sample complexity inversely proportional query difﬁculty particular query difﬁculty sample complexity scales whereas sample complexity scales rest paper organized follows. section explain encoding rule fountain codes state main problem paper. section provide main results showing fundamental trade-offs sample complexity query difﬁculty. section prove main theorem. technical details proof presented appendices. section simulation results provided support theoretical results. section provide conclusions discuss possible future research directions. notation binary variables i.e. denote k-dimensional unit vector j-th element equal vector denotes number vector vectors inner product denoted integers notation indicate mod. vectors write means notations describe asymptotics real sequences {ak} {bk} implies positive real number implies positive real numbers logarithmic function base consider k-dimensional binary random vector uniformly randomly distributed call input symbols. learn values observing total parity measurements different subsets bits. consider k-dimensional binary vectors parity measurement associated vector deﬁned call parity measurements output symbols. determines subset picked calculating i-th parity measurement. process designing {vi} called query design encoding. fountain codes also known erasure rateless codes encoding. distribution denotes encoding fountain codes vector generated independently randomly ﬁrst sampling weight distribution selecting k-dimension vector weight uniformly random vectors weight consider arbitrary output symbols generated encoding rule. relationship input symbols output symbols depicted bipartite graph input nodes side output nodes side shown denote average degree output nodes number indicates average number input symbols involved parity measurement related difﬁculty calculating parity measurement. call number query difﬁculty. proper choice distribution fountain codes guarantee minimized larger threshold. minimum number required guarantee ﬁxed called sample complexity. fundamental limits guarantee reliable information recovery input symbols ﬁxed query difﬁculty iii. main results fundamental trade-offs sample complexity query difficulty section state main results sample complexity necessary sufﬁcient make scales terms max{k constant independent parity measurements generated encoding rule fountain codes explained section ﬁrst state well-known lower bound sample complexity fountain codes presented proposition reliably recover input symbols measurements generated fountain codes necessary represents linear equation unknown input symbols since unknowns necessary least linear equations solve linear system reliably. second condition property random graphs. bipartite graph input nodes output nodes input node isolated connected output nodes. analyze probability input node isolated edges designed encoding rule fountain codes. error probability bounded probability input node isolated since input node isolated decoding error happens. consider output node degree probability input node connected output node degree equals d/k. since output node degree probability probability input node connected output node equals main contribution paper showing bound indeed achievable properly designed fountain codes provide particular output degree distribution control query difﬁculty show possible reliably recover information bits sample complexity obeying theorem states query difﬁculty sample complexity reliably recover input symbols inversely proportional query difﬁculty query difﬁculty increase i.e. necessary sufﬁcient reliably recover information bits. regime ratio converges hand increase query difﬁculty enough samples results positive limit increasing query difﬁculty longer helps reducing sample complexity. using soliton distribution encoding rule fountain codes design potentially limitless number queries corresponding parity measurements. theorem shows measurements size larger reliably recover information bits moreover sample size optimal constants shown proposition thus results provide optimal query design strategy reliable information recovery arbitrary parity measurements optimal terms sample complexity ﬁxed query difﬁculty. showing sample complexity sufﬁcient make upper bound converge equal cu·max{k constant deﬁned optimal decoding rule minimizes probability error maximum likelihood decoding uniformly distributed input symbols. assume collect parity measurements equals consider matrix whose i-th i.e. i-th standard unit vector. last equality follows symmetry sampling matrix since output samples generated independently identically distributed vi’s weight probability fig. monte carlo simulation probability error sample complexity normalized observe phase transition equal three query difﬁculties considered. section provide empirical performance analysis probability error recovery information bits function sample complexity query difﬁculty. fig. provide monte carlo simulation results probability error deﬁned number information bits terms normalized sample complexity normalized recover ﬁxed plot query difﬁculty. simulations three different query difﬁculties parity measurements designed ﬁrst sampling soliton distribution generating measurements encoding rule fountain codes. fig. simulation conditions except horizontal axis un-normalized sample complexity. query difﬁculty increases sample complexity make close decreases. illustrates trade-offs query difﬁculty sample complexity. around normalized sample complexity equal theorem stated sample complexity max{k constant guarantee simulation results show sufﬁcient produce dramatic decrease since phase transition occurs vicinity normalized sample complexity equal ﬁgure demonstrates trade-offs query difﬁculty sample complexity. speciﬁcally required number parity measurements reliably recover information bits inversely proportional query difﬁculty note soliton distribution query difﬁculty thus max{k ¯d). show simulation un-normalized sample complexity indexing horizontal axis. plot observe query difﬁculty increases required number samples make close decreases. paper analyzed fundamental trade-offs query difﬁculty sample complexity query-based data acquisition system associated crowdsourcing task workers non-responsive certain queries considered information recovery binary variables parity measurements subsets variables. used query design based encoding rules fountain codes design potentially limitless numbers queries. showed proposed query design policy guarantees original information bits recovered high probability measurements size larger threshold. obtained necessary sufﬁcient conditions sample complexity max{k ¯d}. several interesting future research directions related work. directions includes analyzing trade-offs query difﬁculty sample complexity partial information recovery problems. paper considered exact information recovery meaning aimed recover information bits high probability. depending scenarios could enough recover information bits then question much relaxed recovery condition would help reducing sample complexity given query difﬁculty especially interesting question might whether possible recover information bits measurements even query difﬁculty increase exact recovery problem impossible reliably recover input symbols sample complexity query difﬁculty necessary least sample complexity exact recovery makes ratio goes therefore would interesting whether sample complexity sufﬁcient partial recovery problem even query difﬁculty another interesting direction apply proposed query design real crowdsourcing systems analyze experimental trade-offs query difﬁculty sample complexity. especially collected measurements contain inaccurate answers probability measurements include inaccurate answers changes depending query difﬁculty corresponding sample complexity might different function therefore would interesting query difﬁculty minimizes sample complexity crowdsourcing systems random erasures inaccurate answers direction research would help guiding design sample-efﬁcient crowdsourcing systems. mackay information-based objective functions active data selection neural computation vol. settles active learning literature survey university wisconsin madison vol. lindley measure information provided experiment annals mathematical statistics fedorov theory optimal experiments. elsevier abbe sandon community detection general stochastic block models fundamental limits efﬁcient algorithms hajek information limits recovering hidden community ieee transactions information theory chung sadler zheng hero unequal error protection querying policies noisy questions problem shokrollahi raptor codes ieee transactions information theory vol. chen goldsmith information recovery pairwise measurements ieee transactions information theory", "year": "2017"}