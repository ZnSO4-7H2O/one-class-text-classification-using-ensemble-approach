{"title": "Speaker Clustering With Neural Networks And Audio Processing", "tag": "eess", "abstract": " Speaker clustering is the task of differentiating speakers in a recording. In a way, the aim is to answer \"who spoke when\" in audio recordings. A common method used in industry is feature extraction directly from the recording thanks to MFCC features, and by using well-known techniques such as Gaussian Mixture Models (GMM) and Hidden Markov Models (HMM). In this paper, we studied neural networks (especially CNN) followed by clustering and audio processing in the quest to reach similar accuracy to state-of-the-art methods. ", "text": "speaker clustering task diﬀerentiating speakers recording. answer \"who spoke when\" audio recordings. common method used industry feature extraction directly recording thanks mfcc features using well-known techniques gaussian mixture models hidden markov models paper studied neural networks followed clustering audio processing quest reach similar accuracy state-of-the-art methods. convolutional neural networks successful comes classiﬁcation tasks high-dimensional spaces especially image classiﬁcation. every ways increase eﬃciency found. useful show hidden features data fed. result also used achieve advanced tasks feature representation created model cluster unknown speakers plays role feature extractor. spectrogram instead mfcc usually used speech recognition tasks. avent neural networks common methods involved gaussian mixture models derivative methods hidden markov models produced best results. paper create extract useful features voice recognition. train classic timit corpus dataset followed fully-connected layers order classify. objective make non-linear data become linearly separable. feature extractor retrieve output speciﬁc layers used feature vectors. feature vectors ﬁnally clustering algorithms cluster vector create groups corresponding speaker. usually input data models processed thanks mfcc features ﬁeld speech recognition recent papers demonstrates achieve comparables results obtain linearly separable data thanks them whether using signal data input mel-spectrogram figure left spectrogram sample timit corpus dataset right mel-spectrogram. mel-spectrogram shows interesting voice features instead spectrogram. model instead considering vector mfcc coeﬃcients input directly mel-spectrogram classical would classify d-matrix image. here channel corresponding decibels inputs arrays certain dimension. choose study packs frames length second step window expect input arrays. network trained timit dataset. shown figure samples dataset signiﬁcant enough network without audio processing. overcome situation compute log-spectrogram allows eﬃciently extract voice characteristics speaker. architecture training samples matrix dimensions corresponding snippets seconds. also deﬁne label denotes speaker identiﬁer. ﬁrst need train classiﬁcation problem would like able classiﬁer since almost impossible obtain true classiﬁer construct thanks approximation trying keep idea mentioned earlier goal train feature extractor. split timit dataset training speakers validation speakers. first stage convolution ﬁlter order obtain features directly logspectrograms. stage involves convolution layer followed max-pooling. ﬁrst stage ﬁlters convolution layer kernel size max-pooling layer pool-size pixels. second stage double ﬁlters using ﬁlters parameters kernel pool-size same. ﬁlter matrix dimension convolutional layer. choose linear activation function. denote i-th j-th column output ﬁrst convolutional layer right ﬁrst dense layer composed ﬁlters relu activation function chose dropout layer useful prevent overﬁtting rate finally last dense layer composed ﬁlters also relu activation function output layer feature extractor. since would like train classiﬁcation task model softmax layer ﬁlters corresponds number speakers. model designed figure training phase considered stochastic gradient descent optimizer categorical cross-entropy loss function. major idea suppose thanks feature extractor second long snippets data points unknown input data linearly separable. result obtain ﬁlter outputs last fully-connected layer process vector feature space figure dimensionality reduction data points. left graph reduced space right graph t-sne reduced space cosine metric. fact groups easily visualizable right image shows non-linearity aspect data. color corresponds diﬀerent speakers. context useful t-sne order easily identify diﬀerent clusters. figure compared dimensionality reduction t-sne. although detect clusters t-sne’s ability detect clusters high-dimensional space gives satisfying results. t-sne representation comforts idea architecture produces linearly separable data. feature space hierarchical clustering cosine metric produces dendrogram. thanks this detect many speakers splitting good thanks clustering previous state able identify speaker talking every second. needless second accurate enough. since want know moment speaker stops talking another speaker starts talking interesting take look voice characteristics pitch class. perform segmentation start produce seconds frame length centered around second speakers stops talking speaker begins. compute chroma feature frames gives pitch classes. applied clustering algorithm feature space able train classiﬁer order identify speaker seconds recording them. support vector clustering linear kernel using one-vs-one approach. shown figure good separations clusters identiﬁcation task easily performed trained svc. although obtain high accuracy training test sets real-life applications point fact model encounters diﬃculties comes analyze voices presence noise resonance. moreover audio pre-processing model unable detect short interventions main improvements", "year": "2018"}