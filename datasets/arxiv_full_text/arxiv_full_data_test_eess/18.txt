{"title": "Kernel-based Reconstruction of Space-time Functions on Dynamic Graphs", "tag": "eess", "abstract": " Graph-based methods pervade the inference toolkits of numerous disciplines including sociology, biology, neuroscience, physics, chemistry, and engineering. A challenging problem encountered in this context pertains to determining the attributes of a set of vertices given those of another subset at possibly different time instants. Leveraging spatiotemporal dynamics can drastically reduce the number of observed vertices, and hence the cost of sampling. Alleviating the limited flexibility of existing approaches, the present paper broadens the existing kernel-based graph function reconstruction framework to accommodate time-evolving functions over possibly time-evolving topologies. This approach inherits the versatility and generality of kernel-based methods, for which no knowledge on distributions or second-order statistics is required. Systematic guidelines are provided to construct two families of space-time kernels with complementary strengths. The first facilitates judicious control of regularization on a space-time frequency plane, whereas the second can afford time-varying topologies. Batch and online estimators are also put forth, and a novel kernel Kalman filter is developed to obtain these estimates at affordable computational cost. Numerical tests with real data sets corroborate the merits of the proposed methods relative to competing alternatives. ", "text": "impact sampling costs reducing number vertices need observed attain target performance. reduction paramount interest applications invasive electrocorticography observing vertex requires implantation intracranial electrode extensive body literature dealt reconstructing time-invariant graph functions. machine learning works typically rely smoothness reconstruct either binary-valued real-valued functions whereas community signal processing graphs focuses parametric estimators real-valued functions adhering bandlimited model functions conﬁned span eigenvectors graph laplacian adjacency matrices schemes subsumed encompassing framework time-invariant kernel-based learning schemes tailored time-evolving functions graphs include predict function values time given observations time however schemes assume function interest adheres speciﬁc vector autoregression vertices observed previous time instances. moreover requires gaussianity along rather form stationarity. works target time-invariant functions afford tracking sufﬁciently slow variations. case dictionary learning approach distributed algorithms unfortunately ﬂexibility algorithms capture spatial information also limited since focuses laplacian regularization whereas require signal bandlimited. different approaches investigate special instances reconstruction problem domain-speciﬁc requirements assumptions worth mentioning approach deals time-evolving topologies. existing kernel-based learning framework naturally extended subsume time-evolving functions possibly dynamic graphs notion graph extension time dimension receives treatment spatial dimension. versatility kernel-based methods leverage spatial information thereby inherited broadened account temporal dynamics well. incidentally vantage point also accommodates time-varying sampling sets topologies. second families space-time kernels introduced generalizing laplacian kernels ﬁrst family enables kernel design bidimensional frequency domain whereas second caters time-varying topologies. third contribution comprises function estimators abstract—graph-based methods pervade inference toolknumerous disciplines including sociology biology neuroscience physics chemistry engineering. challenging problem encountered context pertains determining attributes vertices given another subset possibly different time instants. leveraging spatiotemporal dynamics drastically reduce number observed vertices hence cost sampling. alleviating limited ﬂexibility existing approaches present paper broadens existing kernel-based graph function reconstruction framework accommodate time-evolving functions possibly time-evolving topologies. approach inherits versatility generality kernel-based methods knowledge distributions second-order statistics required. systematic guidelines provided construct families space-time kernels complementary strengths. ﬁrst facilitates judicious control regularization space-time frequency plane whereas second afford time-varying topologies. batch online estimators also forth novel kernel kalman ﬁlter developed obtain estimates affordable computational cost. numerical tests real data sets corroborate merits proposed methods relative competing alternatives. number applications involving social biological brain sensor transportation communication networks call efﬁcient methods infer attributes vertices given attributes vertices example social network vertices edges respectively representing persons friendships interested determining individual’s consumption trends based friends. task emerges sampling cost constraints impossibility poll country’s entire population political orientation limit number vertices known attributes. existing approaches typically formulate problem reconstruction function signal graph rely smoothness respect graph sense neighboring vertices similar function values. principle suggests instance estimating person’s looking friends’ age. challenging problem involves reconstructing timeevolving functions graphs ones describing time-dependent activity regions brain network given values subset vertices time instants. efﬁciently exploiting spatiotemporal dynamics markedly complementary strengths based popular kernel ridge regression criterion; e.g. whereas ﬁrst handle sophisticated forms spatiotemporal regularization second afford efﬁcient implementation online operation meaning estimates reﬁned observations become available. proposed kernel kalman ﬁlter ﬁnds exact online estimates implicitly operating inﬁnite-dimensional space. major novelty paper purely deterministic methodology obviates need assumptions data distributions stationarity knowledge second-order statistics. proposed schemes therefore special interest absence sufﬁcient historical data latter incorporated available covariance kernels although complicated dynamics accommodated simply rely assumption target function smooth graph time reasonable whenever graph properly constructed sampling interval attuned temporal dynamics function. novel online estimator constitutes ﬁrst fully deterministic rigorous application kalman ﬁlter kernelbased learning. although already proposed kernelbased work heavily relies heuristics approximations explicitly operate feature space. moreover algorithm involves solving challenging preimage problem time step increases inaccuracy computational cost. another developed within framework kernel-based learning formulation probabilistic requires historical data estimate data distribution. rest paper structured follows. sec. formulates problem sec. reviews kernel-based learning time-invariant functions. sec. generalizes framework reconstruct time-evolving functions develops estimators together kkf. space-time kernels designed sec. numerical tests sec. conﬁrm beneﬁts proposed algorithms. finally sec. summarizes closing remarks whereas appendix provides proofs main results. notation scalars denoted lowercase letters vectors bold lowercase matrices bold uppercase. entry matrix superscripts respectively denote transpose pseudo-inverse. vec{a} {bt}t unvec{a} matrices {at}t btridiag{a rep= resents symmetric block tridiagonal matrix latter deﬁned rm×m rn×n n-th column identity matrix represented inn. matrix vector ||x|| xa−x ||x|| ||x||i. represents cone positive deﬁnite matrices. finally stands kronecker delta expectation. vertex adjacency matrix whose )-th entry assigns weight pair vertices time timeinvariant graph special case usual e.g. paper assumes non-negative weights undirected self-links edge deﬁned vertices said adjacent connected neighbors time time-evolving function signal graph time indices. value vertex time shorthand version thought value attribute time social network denote annual income person year function values time collected time vertices indices time-dependent arbitrary observed. resulting samples expressed models observation error. social networks encompasses scenarios subset persons surveyed attribute interest; e.g. annual income. letting observations conveniently expressed sampling matrix contains ones positions zeros elsewhere. broad goal paper reconstruct observations {y}t formulations considered batch formulation aims ﬁnding observations {y}t online formulation given together time goal possibly based previous estimate bounded complexity time slot even solve problems explicit parametric model temporal spatial evolution adopted. instance solely rely assumption evolves smoothly eigensignals so-called linear shift-invariant ﬁlters graph counterparts linear time-invariant ﬁlters signal processing time resembles sense synthesis equation fourier transform therefore interpret follows ρlr. hence sorting eigenvectors increasing order associated eigenvalue tantamount sorting decreasing order smoothness. similarly complex exponentials traditional fourier basis indexed frequency thought proxy time-domain smoothness. comparing scenarios suggests interpreting index graph frequency back seen penalizes high-frequency components heavily low-frequency ones thus promoting estimates low-pass graph fourier transform. ﬁner control energy distributed across frequency attained upon applying transformation iii. background kernel-based reconstruction section reviews existing framework kernelbased reconstruction time-invariant functions special case batch problem sec. reﬂecting scenario notation devoid time indices. result problem becomes ﬁnding given rn×n }s×n tempted seek least-squares estimate minf sf|| noting unknowns cannot generally identiﬁed samples dismisses approach. underdeterminacy prompts estimates form minf sf|| regularizer promotes certain structure customary encourages smooth estimates penalizing functions exhibit pronounced variations among neighboring vertices instance means so-called laplacian regularizer heavily penalizes differences function values vertices connected strong links expression formalizes notion smoothness introduced sec. according function smooth takes similar values neighboring vertices. since small smooth large otherwise acts proxy quantifying smoothness sense given former said smoother functions vice versa. general latter rewritten algebra e.g. readily follows turn implies positive semideﬁnite. therefore admits eigenvalue decomposition diag {λv} eigenvectors eigenvalues weighted superposition magnitude projections onto eigenvectors weights given corresponding eigenvalues. described next provides insightful interpretation transformed domain. speciﬁcally number works advocate term graph fourier transform frequency representation refer e.g. main argument resides play role analogous complex exponentials signal processing time signals sense complex exponentials eigensignals continuous counterpart laplacian operator arbitrary positive semideﬁnite matrix necessarily laplacian kernel. user-selected parameter balances importance regularizer relative ﬁtting term s−||y sf|| estimators well-documented merits solid grounds statistical learning theory; e.g. different regularizers ﬁtting functions lead even general algorithms; e.g. unfortunately estimator account possible relation e.g. instance varies slowly time estimate well beneﬁt leveraging observations time instants exploiting temporal dynamics potentially reduces number sampled vertices required attain target estimation performance turn markedly reduce sampling costs. incorporating temporal dynamics kernel-based reconstruction handle single snapshot necessitates appropriate reformulation timeevolving function reconstruction problem reconstructing time-invariant function. appealing possibility replace extended version vertex replicated times yield extended vertex ))-th entry extended adjacency matrix equals weight edge vn]). time-varying function thus replaced extended time-invariant counterpart connectivity time slot {vn}n connected according deﬁnition denote vertex time-varying graph. graph vertex adjacency matrix extended graph t-th diagonal block equals general exist multiple graph extensions given time-varying graph. diagonal blocks dictated whereas remaining entries freely selected. reconstruction problem interested selecting off-diagonal entries capture space-time dynamics example consider extended graph btridiag{av rn×n {vn}n instance connect vertex neighbors previous time instant setting connect vertex replicas adjacent time instants setting diagonal. fig. pictorially illustrates latter choice. notice extended graph treats time dimension spatial dimension. thus ﬂexibility graphs convey relational information carries time domain. major beneﬁt approach lays grounds design doubly-selective kernels sec. v-a. extended fig. original graph extended graph diagonal edges connecting vertices time instant represented solid lines whereas edges connecting vertices different time instants represented dot-dashed lines. following result formalizes latter claim. resulting algorithm termed summarized algorithm probabilistic terminology step yields prediction step provides covariance matrix prediction error step yields kalman gain step returns posterior estimate upon correcting prediction innovations scaled kalman gain step ﬁnds error posterior estimate. algorithms online estimate given since proposed derived within fully deterministic framework notions mean covariance statistical independence mean-square error required used describe connection classical furthermore proposed explicitly involve state-space model major novelty indeed surprising result present paper. proposed generalizes probabilistic since latter recovered upon setting covariance matrix previously mentioned probabilistic setup. therefore natural assumptions required probabilistic stronger involved kkf. speciﬁcally probabilistic must adhere linear state-space model known transition matrix regarding present therefore provides estimates past present future values solution online problem formulated sec. includes sequence present estimates obtained solving closed form applying however approach yield desirable online algorithm since complexity time slot cubic therefore increasing reason approach satisfactory since online problem formulation sec. requires complexity time slot desired algorithm bounded. algorithm satisfy bounded-complexity requirement provides exact estimate developed next case kernel matrix positive deﬁnite matrix satisfying process developing desired online algorithm involves steps. ﬁrst step expresses weighted least-squares problem amenable solver. second step applied solve problem. ﬁrst step accomplished following result. although probabilistic assumption required throughout derivation proposed online algorithm exploring link conventional probabilistic setup state estimation provides intuition behind solved kalman ﬁltering. suppose obeys random model initialized zeromean noise covariance observations follow model zero-mean noise covariance referred state-transition matrix. scenario easily obtaining maximum posteriori minimum mean square error estimators given observations time gaussian distributed reduces minimizing state noise uncorrelated time known covariance matrix observation noise must uncorrelated time known covariance matrix. correspondingly performance guarantees probabilistic also stronger resulting estimate optimal linear estimators. furthermore jointly gaussian probabilistic estimate optimal mean-square error sense among estimators. contrast requirements proposed much weaker since requires evolve smoothly respect given extended graph guarantees also weaker; e.g. however since generalizes probabilistic reconstruction performance former judiciously selected cannot worse reconstruction performance latter given criterion. caveat however selection necessarily easy. remark algorithm requires operations time slot whereas complexity evaluating t-th time slot increases becomes eventually prohibitive. large algorithm computationally efﬁcient single plain evaluation whereas overall complexity former latter e.g. constant remark algorithm provides estimates form obtain estimates execute algorithm time conversely obtain estimates extend algorithm capitalizing notion kalman smoothing remark similar probabilistic requires inverse covariance matrix block tridiagonal proposed requires inverse kernel matrix form fortunately straightforward extend algorithms accommodate inverse covariance kernel matrices number non-zero diagonals price increasing time interval consecutive estimates. illustrate approach suppose block tridiagonal blocks size blocks size block tridiagonal case proposed estimate replacing bdiag ]s]} note sampling interval associated index twice associated setting diag{ denotes laplacian matrix extended graph. unfortunately design prevents separate control spatial temporal variability estimates thus limiting user’s ability ﬂexibly account spatial temporal information. instance sampling intervals small relative time dynamics meaning vary signiﬁcantly samples favors estimates sacriﬁce spatial smoothness increase temporal smoothness. section proposes families space-time kernels temporal spatial smoothness separately tuned. sec. describes designs time-invariant topologies whereas sec. deals time-varying case. frequency interpretation proved decisive interpret design laplacian kernels reconstructing time-invariant functions. introducing time dimension sec. prompts analogous methodology kernels speciﬁed bidimensional plane spatiotemporal frequency; graph ﬁlter design domain. section accomplishes task generalizing laplacian kernels sec. iii. much regularizers associated proposed kernels weight spatial temporal frequency component separately prescribed. throughout section time-invariant topology assumed i.e. clearly rewritten frequency transform separately weight r-th entry matrices corresponds ˇn-th spatial frequency ˇt-th temporal frequency. kernels satisfying second equality termed doubly selective. kernels preserve ﬂexibility counterparts time-invariant functions. instance promotes doubly low-pass estimates left entries small whereas rest large. determine form doubly-selective kernel recall linear bidimensional transform expressed matrix matrix stand orthogonal transformations along space time respectively. hand vectorizing rightmost term yields kernels form referred kronecker spacetime kernels. transformation selected several ways. instance immediate construction beginning sec. recovered one-dimensional spectral weight ones table another possibility focus separable maps form denote one-dimensional spectral maps. resulting kronecker kernel expressed proposed kronecker kernels arise intuitive graph extension afford ﬂexible adjustment frequency response. unfortunately kronecker kernel suitable online algorithm sec. since latter requires inverse kernel matrix block tridiagonal. rest section describes subfamily kronecker kernels suitable algorithm. block tridiagonal i.e. tridiagonal necessary must zero )-th entry obtained instance sets )-th entry unless extended graph construction vertex connected replicas adjacent time slots. tridiagonal ensures invertible. thus price paid online implementation sec. limited ﬂexibility specifying temporal frequency response. note intrinsic limitation proposed algorithm inherent classical well; recall latter assumes vector autoregressive processes order case temporal frequency response kernel obtained analytically approximating resulting laplacian sufﬁciently large circulant matrix. implies eigenvectors approximately conventional fourier basis therefore notion temporal frequency embodied preserves conventional meaning; upon expression provides general form doublyselective kernel speciﬁc construction capturing spatiotemporal dynamics still required. next procedure serves purpose paralleling approach sec. iii. involves following steps. since laplacian kernel matrix shares eigenvectors laplacian matrix construct extended graph laplacian matrix diagonalizable matrix form orthogonal rt×t rn×n must design spectral weight obtain eigenvalues regarding explicit construction extended graph whose laplacian matrix diagonalizable matrix form orthogonal rt×t rn×n provided next. consider extended adjacency matrix given adjacency matrix adjacency matrix selected capture temporal dynamics. speciﬁcally deﬁnition extended adjacency matrix sec. dictates weight edge given entry whereas weight edge given entry simple choice described later. note differs kronecker graphs although interpreted cartesian graph cartesian graphs considered graph signal processing literature graph ﬁltering fourier transforms time-varying functions signal reconstruction. readily seen diag{ ¯a}− diag{at diag{av laplacian matrices associated respectively. diag expression reveals graph extension proposed indeed satisﬁes objective requires eigenvector matrix form thus always possible construct graph extension satisfying goal must construct spectral yields upon entrywise application separately control frequency response along spatial section compares performance proposed schemes state-of-the-art alternatives illustrates trade-offs inherent time-varying function reconstruction real-data experiments. unless otherwise stated compared estimators include distributed least squares reconstruction step size µdlsr parameter βdlsr; least mean-square algorithm step size µlms; bandlimited instantaneous estimator results applying separately instantaneous estimator reconstruction diffusion kernel parameter proposed kernel given diffusion kernel parameter dlsr bl-ie also bandwidth parameter ﬁrst data comprises hourly temperature measurements stations across continental u.s. temperature reconstruction extensively employed literature analyze performance remark paper rows thought graph functions graph adjacency matrix whereas columns thought graph functions graph adjacency matrix principle column need correspond different time instant e.g. different movie recommender system application. estimators therefore used matrix completion upon properly creating extended graph graph kernel matrix. towards space-time kernels deﬁned readily generalize spacespace kernels promote smoothness graphs. time-invariant topologies sec. proposed kernels designed interpreted two-dimensional frequency plane. section deals changing topologies bidimensional frequency notion deﬁned. recognize claim suppose remains constant recall ˇn-th eigenvector equivalently ˇn-th column case bidimensional transform exists expressed whose entry corresponds ˇn-th spatial frequency ˇt-th temporal frequency. fundamentally precise meaning latter statement ˇnˇt ˇt-th temporal frequency component ˇn-th spatial frequency component i.e. ˇt-th temporal frequency component time series generally differ thus precluding natural deﬁnition aforementioned sequence therefore bidimensional frequency transform. nonetheless shown next notion spatial frequency slot still utilized design space-time kernels time-varying topologies. inference tools graphs time-invariant graph constructed following approach nearest neighbors relies geographical distances. function represents temperature station t-th sampling instant. ﬁrst experiment latter corresponds t-th hour whereas rest corresponds temperature t-th day. fig. depicts true temperature measured unobserved randomly picked station ﬁrst hours along estimates typical realization time-invariant sampling drawn random within sampling sets elements. different instantaneous alternatives whose error decrease time observed successfully leverage time dynamics track temperature unobserved station. hand dlsr unable track rapid variations since design assumes slowly changing functions. matrix comprising rows whose indices fig. shows nmse averaged possible elements. observed instantaneous estimators outperform dlsr cope slow variations furthermore error half error nearest alternative demonstrating importance exploiting time dynamics. fig. shows impact number observed vertices nmse}t days averaged sets elements. observe consistently outperforms alternatives. still advantage krr-ie pronounced small since case exploiting time dynamics critical. large value leads estimator relies heavily time dynamics vice versa. fig. shows nmse}t days averaged sets elements. kernel adopted regularized laplacian diffusion kernels table observed exists optimum value leads best reconstruction performance. corresponds optimal trade-off point space-time kernel created using time-invariant diffusion kernel generated time-invariant fig. showcases superior reconstruction performance among competing approaches even small number samples. result suggests ecog diagnosis technique could efﬁciently conducted even smaller number intracranial electrodes great impact patient’s experience. investigated kernel-based reconstruction space-time functions graphs. adopted approach relied construction extended graph regards time dimension spatial dimension. several kernel designs introduced together batch online function estimators. future research deal multi-kernel distributed versions proposed algorithms. second data provided bureau economic analysis u.s. department commerce contains annual investments pair sectors among economic sectors interval entry trillions dollars sectors year dlsr since cannot handle time-varying topologies. value corresponds total production n-th sector year sampling interval years disjoint subsets years used generating signal constructing graphs. next experiment demonstrates ability handle time-varying topologies. fig. plots nmse}t averaged sets elements. utilizes kernel diffusion kernel constructed again fig. showcases superior performance proposed whose error signiﬁcantly less error competing alternatives. third data obtained epilepsy study diagnosis epilepsy heavily based analysis ecog data; sec. next experiments utilize ecog time series obtained electrodes implanted patient’s brain onset seizure. symmetric time-invariant adjacency matrix obtained using method ecog data onset seizure. function comprises electrical signal n-th electrode t-th sampling instant onset seizure period samples. values normalized subtracting temporal mean time series onset seizure. goal experiment illustrate reconstruction performance proposed capturing complex spatio-temporal dynamics brain signals. averaged sets size proposed t-th iteration algorithm returns given simplify notation collect function values time estimates given observations time rest expressing solutions least-squares problems. deﬁne matrix smola kondor kernels regularization graphs learning theory kernel machines springer shuman narang frossard ortega vanthe emerging ﬁeld signal processing graphs dergheynst extending high-dimensional data analysis networks irregular domains ieee sig. process. mag. vol. sandryhaila moura discrete signal processing graphs ieee trans. sig. process. vol. apr. kashima oyama yamanishi tsuda pairwise kernels efﬁcient alternative generalization analysis paciﬁcasia conf. knowledge discovery data mining. springer sandryhaila moura data analysis signal processing graphs representation processing massive data sets irregular structure ieee sig. process. mag. vol. shen baingana giannakis nonlinear structural vector autoregressive models inferring effective brain network connectivity arxiv preprint arxiv. lorenzo barbarossa banelli sardellitti adaptive least mean squares estimation graph signals ieee trans. sig. info. process. netw. vol. early access sch¨olkopf smola learning kernels support vector machines regularization optimization beyond press ralaivola d’alch´e-buc time series ﬁltering smoothing learning using kernel kalman ﬁlter proc. ieee int. joint conf. neural netw. ieee vol. isuﬁ loukas simonetto leus separable autoregressive moving average graph-temporal ﬁlters proc. european sig. process. conf. budapest hungary aug.", "year": "2016"}