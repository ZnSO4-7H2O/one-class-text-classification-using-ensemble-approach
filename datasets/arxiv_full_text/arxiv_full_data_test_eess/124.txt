{"title": "Knowledge Transfer from Weakly Labeled Audio using Convolutional Neural  Network for Sound Events and Scenes", "tag": "eess", "abstract": " In this work we propose approaches to effectively transfer knowledge from weakly labeled web audio data. We first describe a convolutional neural network (CNN) based framework for sound event detection and classification using weakly labeled audio data. Our model trains efficiently from audios of variable lengths; hence, it is well suited for transfer learning. We then propose methods to learn representations using this model which can be effectively used for solving the target task. We study both transductive and inductive transfer learning tasks, showing the effectiveness of our methods for both domain and task adaptation. We show that the learned representations using the proposed CNN model generalizes well enough to reach human level accuracy on ESC-50 sound events dataset and set state of art results on this dataset. We further use them for acoustic scene classification task and once again show that our proposed approaches suit well for this task as well. We also show that our methods are helpful in capturing semantic meanings and relations as well. Moreover, in this process we also set state-of-art results on Audioset dataset, relying on balanced training set. ", "text": "work propose approaches effectively transfer knowledge weakly labeled audio data. ﬁrst describe convolutional neural network based framework sound event detection classiﬁcation using weakly labeled audio data. model trains efﬁciently audios variable lengths; hence well suited transfer learning. propose methods learn representations using model effectively used solving target task. study transductive inductive transfer learning tasks showing effectiveness methods domain task adaptation. show learned representations using proposed model generalizes well enough reach human level accuracy esc- sound events dataset sets state results dataset. acoustic scene classiﬁcation task show proposed approaches suit well task well. also show methods helpful capturing semantic meanings relations well. moreover process also state-of-art results audioset dataset using balanced training set. sound plays crucial role interaction surroundings. hence critical success artiﬁcial intelligence machines computers able comprehend sounds humans considerable interests sound event detection classiﬁcation research recent years. motivation also comes immediate applications surveillance content based indexing retrieval multimedia name few. sound event detection classiﬁcation constrained availability large scale datasets. labeling sound events audio recording extremely difﬁcult task. besides also situations marking beginnings ends sound event audio recording inherently ambiguous open interpretation annotator address concerns introduced weak labeling approaches sound event detection. recently large scale weakly labeled dataset sound events audioset released. weak label learning sound events also included year’s dcase challenge. although weak labeling addresses data availability constraints certain extent creating large datasets along lines audioset still easy. even weak labeling done manually resource intensive time consuming process. moreover might difﬁcult collect large amounts labeled data form certain cases. examples sound events inherently rare. deep learning methods based cnns directly useful cases. however pointed ellis future perspectives attempt address problem transferring knowledge model trained large dataset. motivation also comes computer vision deep models successfully used transfer knowledge domain another well task another approach generally referred transfer learning remains less unexplored context sound events scenes. audio related works transfer learning another earlier work models ﬁrst trained sound events tested another understand idea objectness sounds. transfer learning computer vision successful because availability large datasets imagenet provides reasonable collection labeled examples large number visual objects. allows train deep models learn enough information source data useful solving tasks. sounds primary problem lack large scale dataset; large scale imply both vocabulary sound events well overall dataset size. vocabulary sound events important learning algorithm needs wide variety sound events learn models might useful solving tasks. lack large dataset soundnet proposed transfer knowledge visual models sound event recognition. models trained visual objects scenes teach feature extractor network audio. however remains seen direct approach audio audio knowledge transfer done. paper propose methods effectively transfer knowledge based sound event model trained large dataset ﬁrst train deep model audioset dataset provides weakly labeled audio examples youtube sound events. proposed weak label learning works efﬁciently smoothly audio recordings variable length. makes well suited application transfer learning. moreover also outperforms previous methods computationally much efﬁcient. model transductive well inductive transfer learning scenarios transductive learning scenario might also referred domain adaptation models sound event classiﬁcation esc- dataset case target task still sound event classiﬁcation audio recordings different domain. inductive transfer learning might also referred task adaptation perform acoustic scene classiﬁcation task dcase dataset task adaptation target task different source task. cases model serves framework learn representations audios used train classiﬁer. propose different methods adapt network target tasks obtain discriminative representations. moreover show representations capture higher level semantic information well. method also helps automatically understand relationship acoustic scenes sound events. best knowledge ﬁrst work extensively explores proposes methods transfer knowledge based model trained large-scale sound event dataset audioset. fig. left right deep weakly labeled audio. consists convolutional pooling layers. convolutional layers. global pooling layer transforms segment level output recording level output. bottom left adapting target task. different methods proposed. parameters transferred. onwards adapted target task. newly added layers shown green outline. layers updated task adaptive training shown dashed outline. bottom right obtaining representations audios. network several approaches proposed sound event classiﬁcation cite few. however works formulated around strongly labeled data. done weakly labeled data almost always limited terms scale offering little insight well might generalize large scale scenarios useful transfer learning. dcase weakly labeled challenge works based also considers events audioset. analyzes popular architectures resnet large scale sound event classiﬁcation videos. however training procedure makes simplistic strong label assumption weakly labeled audios. sound event assumed present whole audio recording. training cnns audio recording chunked small segments network target labels segments label whole recording. training procedure referred strong label assumption training work give alternate approach premised ideas proposed treats weak labels weak training cnns. going details would like mention logmel spectrograms used training cnns work. audio recordings sampled sampling frequency. bands used. window size overlap used obtainining features. network architecture proposed deep framework weakly labeled audio shown left right panels figure block consists convolutional layers followed pooling. consists colvolutional layer followed pooling layer. relu activation used cases. convolutional layers blocks ﬁlters used. stride padding ﬁxed number ﬁlters used convolutional layer blocks pooling done window stride also convolutional layer relu activation. ﬁlters size used stride padding used secondary output layer covolutional layer ﬁlters size sigmoid output. layer produces segment level output number classes number segments. segment level outputs aggregated using global pooling layer produce dimensional output whole recording. network scans whole input produces outputs corresponding segments logmel-frames moving frames. example input logmel spectrogram consisting logmel-frames produce segments since weakly labeled audio labels full recording outputs segment level pooled obtain full recording level output. loss computed respect recording level output. hence network treats weak labels weak. overall style weak label learning sounds. segment size segment size controlled network design. frames respectively logmel spectorgrams. note that needed segment level outputs give temporal localization events unlike slat fully connected dense layers used architectures network fully convolutional allows process audio recordings variable length. makes well suited transfer learning. multi-label training loss data including audioset expected multiple sound events might simultaneously present recording. hence employ multi-label training loss. sigmoid output gives class speciﬁc posteriors given input. binary cross entropy loss respect class given log. target network output class respectively. training loss mean losses classes network trained source task audios used obtain representations audios target task. obtaining representations shown bottom right panel segment level outputs serves base representations audios. segments level representations mapped full recording level representations. apply either mapping. finally obtain dimensional representations full recordings. training blocks embeds knowledge source audio data mapped source labels ﬁlters makes well suited transfer learning used train classiﬁers target task. moreover outputs gives distribution source labels useful target task trained large collection sound events. propose broad methods representation learning audios target tasks using direct off-the-shelf representations method treated ready mode obtaining representations. logmel spectrograms audio recordings target task outputs aggregated segments obtain dimensional representations respectively. transfer adapt learning representations second method ﬁrst adapt network target task extract features expect discriminative better suited target classiﬁcation task. propose methods achieve goal. methods shown figure three methods parameters transferred updated target adaptation training. number classes target dataset. performs direct adaptation target task. here replaced covolutional layer ﬁlters. parameters updated using training target task. call network convolutional layer ﬁlters added network adapted target task. shown dashed boundaries adaptive training updated. idea capture target speciﬁc information ﬁrst transitioning source label space going target label space. fully connected layer size added global pooling layer again updated network adaptation training. motivation behind network except tries learn mapping full recording level instead segment level. note activation function changed relu sigmoid target task multi-label problem activation ﬁnal layer kept sigmoid loss function similar deﬁned section used. however target task audios single label softmax output categorical cross entropy loss. things worth noting. first target task audio recordings different length proposed methods handle cases efﬁciently. moreover target task dataset either strongly weakly labeled proposed methods used learn representations cases. lastly emphasize again focus exploiting learn representations audios target task. classiﬁer trained representations even target task dataset small. table left comparison mauc events audioset. right comparison average relative training inference times. bottom table comparison sound events lowest highest using baseline slat weakly labeled data currently available. compare performance slat used training networks. validation used tuning parameters selecting best model. show experimental results transfer learning using task adaptive training training target task used. learning rate process ﬁxed updates done epochs network used obtain representations. linear svms trained representations obtained different methods. slack parameter svms tuned cross validation training set. page detailed results analysis. audioset results audioset dataset consists weak labels sound events youtube videos. total dataset consists million audio recordings. balanced training training balanced training provides total around training audio recordings least examples class. however multi-label nature data actual number examples several classes much higher. small subset unbalanced audioset used validation experiments. results reported full eval audioset around test audio recordings least examples class. similar area curves average precision class used evaluation metrics. table shows mean mean classes audioset. absolute improvement mauc obtained using right table shows relative computational times normalized comparison. faster average inference. hence suitable real applications. training average faster full pass training data. performance classes available here. bottom table tables shows comparison sound events slat achieved least highest aps. performance classes average doubles easier sound classes relative improvement obtained using http//www.cs.cmu.edu/%ealnu/tlweak.htm domain adaptation sound event classiﬁcation section used learning representations esc- dataset. esc- dataset consists total sound events broad categories animals natural soundscapes water sounds human speech sounds domestic sounds exterior sounds dataset consists total recordings. comes pre-divided folds. four folds used training validation remaining fold used testing. done ways average accuracy across runs accuracy reported. human accuracy dataset left table tab. compares mean accuracy classes state-of-art esc- dataset. outperform best method setting state-of-art results esc-. right table tab. shows performance different representations proposed work. note that even off-the-shelf representations using able achieve better human performance dataset. shows excellent capturing sound event knowledge. task adaptive training gives improvement. mapping converting segment level representations full recording representations performs better. sigmoid output give good performance using linear svms. however task adaptive training activations changed relu obtain good performance representations. classwise confusion matrix found here. task adaptation acoustic scene classiﬁcation scenes park home possess complex acoustic characteristics. often composed several sound events meshed together complex manner. study utility transferring learning acoustic scenes dcase dataset. provides seconds examples acoustic scenes listed upper table tab. total duration data around hours. dataset comes pre-divided folds used training remaining test. done ways average accuracies across runs reported. upper table table compares accuracies different acoustic scenes baseline proposed method. absolute improvement scenes observed. certain scenes hard classify park train absolute improvement respectively obtained. note task representations task adapted networks perform much better compared obtained directly fig. left t-sne visualizations esc- color coded higher semantic categories right t-sne visualizations dcase first alphabet some orest. bottom table sound events frequently among maximally active events given scene. network mapping performs better compared avg. semantic understanding draw semantic inferences proposed methods. left panel shows dimensional t-sne embeddings representations obtained esc-. embeddings color coded broad categories esc- dataset semantically higher level groups sound events. note plot representations capable capturing higher level semantic information. vacuum cleaner domestic closely resembles chainsaw engine handsaw exterior category representations also lies closer exterior sounds similarly visualization acoustic scenes shown right panel fig. acoustic scenes often understood sound events. neuron essentially representing sound event class activations neurons used understand sceneevent relations. input given scene list maximally activated neurons note events occurred frequently lists. highly active events scenes shown table observe several sound events expected occur corresponding scene. hence scene-events relations semantically meaningful. shows network managed successfully transfer knowledge learn relationships. analysis semantics here. ﬁrst proposed based model weakly labeled learning sound events. model sets state-of-art results audioset also computationally efﬁcient well suited transfer learning. proposed methods learn representations audios target task using model. state results esc- dataset achieving accuracy surpasses human accuracy dataset. besides achieving excellent performance methods transfer knowledge also helpful higher level semantic understanding. example automatically discovering relationships scenes sound events also important contribution work. pradeep atrey namunu maddage mohan kankanhalli audio based event detection multimedia surveillance ieee international conference acoustics speech signal processing proceedings. ieee vol. tong yang jiang shoou-i zhenzhong zhigang waito ehsan younessian alexander hauptmann e-lamp integration innovative ideas multimedia event detection machine vision applications vol. shoou-i jiang zexi xiaojun chang xingzhong chuang zhenzhong zhongwen xuanchong yang informedia trecvid nist trecvid video retrieval evaluation workshop vol. jort gemmeke daniel ellis dylan freedman jansen wade lawrence channing moore manoj plakal marvin ritter audio ontology human-labeled dataset audio events ieee icassp oquab bottou laptev sivic learning transferring mid-level image representations using convolutional neural networks proceedings ieee conference computer vision pattern recognition yosinski clune bengio lipson transferable features deep neural networks? advances neural information processing systems sinno jialin qiang yang survey transfer learning ieee transactions knowledge data engineering vol. sparse autoencoder-based feature transfer learning speech emotion recognition affective computing intelligent interaction humaine association conference ieee shawn hershey sourish chaudhuri daniel ellis jort gemmeke jansen channing moore manoj plakal devin platt saurous bryan seybold architectures large-scale audio classiﬁcation acoustics speech signal processing ieee international conference ieee annamaria mesaros toni heittola tuomas virtanen database acoustic scene classiﬁcation sound event deth european signal processing conference tection vol. karol piczak environmental sound classiﬁcation convolutional neural networks machine learning signal processing ieee international workshop ieee justin salamon juan pablo bello deep convolutional neural networks data augmentation environmental sound classiﬁcation ieee signal processing letters vol. kong qiang huang wang plumbley attention localization based deep convolutional recurrent model forweakly supervised audio tagging proceedings interspeech weaklysupervised audio event detection using event-speciﬁc gaussian ﬁlters fully convolutional networks acoustics speech signal processing ieee international conference ieee buckley voorhees retrieval evaluation incomplete information proceedings annual international sigir conference research development information retrieval. tokozume harada learning environmental sounds end-to-end convolutional neural network acoustics speech signal processing ieee international conference ieee", "year": "2017"}