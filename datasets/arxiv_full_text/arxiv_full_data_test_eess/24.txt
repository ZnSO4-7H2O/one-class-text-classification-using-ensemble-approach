{"title": "Convolutional Sparse Representations with Gradient Penalties", "tag": "eess", "abstract": " While convolutional sparse representations enjoy a number of useful properties, they have received limited attention for image reconstruction problems. The present paper compares the performance of block-based and convolutional sparse representations in the removal of Gaussian white noise. While the usual formulation of the convolutional sparse coding problem is slightly inferior to the block-based representations in this problem, the performance of the convolutional form can be boosted beyond that of the block-based form by the inclusion of suitable penalties on the gradients of the coefficient maps. ", "text": "emphasised extensions relevance beyond speciﬁc denoising test problem considered here improved performance reported problem also expected impact general image reconstruction problems e.g. convolutional sparse coding employed prior within plug-and-play priors framework also evidence inclusion gradient penalties enhances performance convolutional sparse representations certain image decomposition/restoration problems allow distinct weighting term ﬁlter present efﬁcient approach solving problem alternating direction method multipliers framework. outline method presented basis extensions proposed following sections. convolutional sparse representations enjoy number useful properties received limited attention image reconstruction problems. present paper compares performance block-based convolutional sparse representations removal gaussian white noise. usual formulation convolutional sparse coding problem slightly inferior block-based representations problem performance convolutional form boosted beyond block-based form inclusion suitable penalties gradients coefﬁcient maps. index terms— convolutional sparse representations convosparse representations well-established tool inverse problems wide variety areas including signal image processing computer vision machine learning standard form linear representation dictionary representation signal represented. linear transform fast transform operator discrete wavelet transform representations computed large images learned training data represented explicit matrix feasible standard approach independently compute representations overlapping image patches. convolutional sparse representations recent alternative replace general linear elements dictionary linear ﬁlters representation consists coefﬁcient maps growing interest imaging image processing applications convolutional form surprisingly denoising gaussian white noise arguably simplest imaging inverse problems received almost attention beyond brief example providing insufﬁcient detail reproducibilty present paper argues that despite numerous advantages many contexts convolutional form competitive gaussian white noise denoising problem deﬁciencies mitigated moving beyond simple regularization speciﬁc form investigated consisting additional penalties gradients coefﬁcient map. since diagonal grouped together term; independent linear systems described sec. composed rank-one diagonal terms sherman-morrison solution directly applied without substantial increase computational cost. instead independently applying scalar coefﬁcient treat coefﬁcient maps multi-channel image apply vector originally designed restoration colour images. corresponding extension cbpdn problem written solving large linear system observation decomposed independent linear systems system matrix consisting rank-one diagonal terms solved efﬁciently exploiting sherman-morrison formula extension include penalty gradients coefﬁcient maps proposed primary purpose extension regularization impulse ﬁlter intended represent low-frequency components image small non-zero regularization dictionary ﬁlters found provide small improvement impulse noise denoising performance considering edge-smoothing effect gradient regularization reasonable alternative consider total variation regularization. consider three different variants performance standard block-based sparse coding different convolutional sparse coding methods described sections compared gaussian white noise restoration problem. standard sparse coding computed basis pursuit denoising problem standard dictionary matrix) resulting denoised blocks aggregated averaging obtain denoised image. different dictionaries standard convolutional learned training images pixels each. convolutional dictionary consisted ﬁlters size learned convolutional dictionary learning algorithm described standard dictionary consisted vectors coefﬁcients learned non-convolutional variant algorithm used learning convolutional dictionary applied image blocks training images. standard dictionary used bpdn experiments convolutional dictionary used cbpdn experiments. greyscale reference images depicted fig. constructed cropping regions pixels wellknown standard test images. regions chosen contain diversity content avoiding large smooth areas size chosen relatively small would computationally feasible optimise method parameters grid search. reference images scaled pixel values interval corresponding test images constructed adding gaussian white noise standard deviation following standard practice cbpdn decomposition applied highpass ﬁltered images obtained subtracting lowpass component computed tikhonov regularization regularization parameter regularization motivated exploration additional alternative forms regularization standard regularization applied coefﬁcient maps alternative introducing regularization however would consider regularization components dmxm reconstructed image written although left hand side algebraic form rank-one rather diagonal therefore grouped together term solution case left hand side rank-three plus diagonal cannot solved using simple sherman-morrison approach still efﬁcient solution iterated application sherman-morrison formula used solve cbpdn problem multi-channel image dictionary involves table comparison denoising performance different denoising methods test images parameters individually optimised image. bold values indicate best performing cbpdn method. italic value bpdn indicates bpdn gave best overall performance image. ﬁrst experiments results displayed table denoising performance different methods individually optimised image search logarithmically spaced grid parameters. main points worth noting bpdn consistently better cbpdn small margin. cbpdn gradient regularization gives similar performance cbpdn slightly better test images slightly worse others. cbpdn gives best overall performance three test images performance within tenths best cases. consistently better cbpdn better bpdn test cases. comparison cbpdn cbpdn former sometimes better moderate margin worse small amount. cbpdn always worse tv-augmented cbpdn methods sometimes better cbpdn. computation times iteration different methods approximately bpdn cbpdn cbpdn cbpdn cbpdn cbpdn i.e. improved performance methods obtained signiﬁcant computational cost. differences psnr values methods optimised parameters optimised displayed table note that cbpdn positive difference cases small negative difference cases i.e. test images convolutional representation regularization term competitive baseline cbpdn. methods performance substantially degraded without term. table comparison denoising performance different denoising methods test images parameters obtained optimising separate image set. bold values indicate best performing cbpdn method. italic value bpdn indicates bpdn gave best overall performance image. ﬁnal experiments considers realistic scenario ground truth available parameter selection test images making necessary choose parameters optimising distinct parameter selection image set. parameters selected test images ﬁnding values giving best average performance separate image search logarithmically spaced grid. results experiment presented table overall relative performances different methods differ qualitatively experiments reported table strictly apples-to-apples comparison bpdn cbpdn denoising methods difﬁcult construct careful attempt reported indicates bpdn slightly superior baseline cbpdn augmentation baseline cbpdn functional appropriate term substantially boosts performance surpassing bpdn test cases considered here. respect speciﬁc form additional term scalar applied independently coefﬁcient somewhat superior joint vector term coefﬁcient maps methods substantially superior applied reconstruction domain rather coefﬁcient maps indicating gain term coefﬁcient maps viewed simply resulting denoising synthesis sparse representation image models. particularly interesting convolutional sparse coding problem penalty competitive performance usual cbpdn form penalty. abstract level results suggest penalties exploit spatial structure coefﬁcient maps necessary achieve true potential convolutional model. bristow eriksson lucey fast convolutional sparse coding proc. ieee conf. comp. vis. pat. recog. jun. doi./cvpr.. wohlberg efﬁcient convolutional sparse coding proc. ieee int. conf. acoust. speech signal process. doi./icassp.. wohlberg convolutional sparse representation color images proc. ieee southwest symposium image analysis interpretation santa mar. doi./ssiai.. wohlberg sporco python package standard convolutional sparse representations proceedings python science conference austin jul. doi./shinma-fce- quan w.-k. jeong compressed sensing reconstruction dynamic contrast enhanced using gpuaccelerated convolutional sparse coding proc. ieee int. symp. biomed. imaging apr. doi./ isbi.. sreehari venkatakrishnan wohlberg buzzard drummy bouman plug-and-play priors bright ﬁeld electron tomography sparse interpolation ieee trans. computational imaging vol. dec. doi./tci.. boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers foundations trends machine learning vol. doi./", "year": "2017"}