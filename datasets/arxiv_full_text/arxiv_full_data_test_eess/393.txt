{"title": "Blind Joint MIMO Channel Estimation and Decoding", "tag": "eess", "abstract": " We propose a method for MIMO decoding when channel state information (CSI) is unknown to both the transmitter and receiver. The proposed method requires some structure in the transmitted signal for the decoding to be effective, in particular that the underlying sources are drawn from a hypercubic space. Our proposed technique fits a minimum volume parallelepiped to the received samples. This problem can be expressed as a non-convex optimization problem that can be solved with high probability by gradient descent. Our blind decoding algorithm can be used when communicating over unknown MIMO wireless channels using either BPSK or MPAM modulation. We apply our technique to jointly estimate MIMO channel gain matrices and decode the underlying transmissions with only knowledge of the transmitted constellation and without the use of pilot symbols. Our results provide theoretical guarantees that the proposed algorithm is correct when applied to small MIMO systems. Empirical results show small sample size requirements, making this algorithm suitable for block-fading channels with coherence times typically seen in practice. Our approach has a loss of less than 3dB compared to zero-forcing with perfect CSI, imposing a similar performance penalty as space-time coding techniques without the loss of rate incurred by those techniques. ", "text": "proving channel estimation techniques through example sparse dictionary learning active area research. practice channel state information always needed decode schemes communicate without impose losses rate increased symbol error rates additionally blind decoding schemes mimo systems exist often inefﬁcient terms complexity sample size requirements. discussed detail section obtaining accurate channel estimates likely become challenging future generation wireless systems likely increased spatial diversity decreased coherence times hence improvements channel estimation schemes communicate without potential reduce overhead well improve performance current future wireless systems. work also motived research physical-layer security. several works proposed keyless authentication schemes attempt identify users based properties physical channel communicate survey). since mimo channels often well conditioned hence invertible schemes require remains hidden adversary. work shows mimo systems inherently leak underlying source structured. security perspective work implies eavesdropper need knowledge pilot symbols knowledge data transmitted order efﬁciently intercept decode mimo communications. scheme attempts provide security hiding obscuring pilot symbols insecure. blind decoding technique introduced work motivated classical problem convex optimization ﬁtting minimum volume ellipsoid samples given method proposed work samples within parallelepiped i.e. n-dimensional polytope parallel congruent opposite faces thereby recovering inverse channel gain matrix. work focus mimo systems small number transmit antennas speciﬁcally choice parameters captures nearly mimo systems wireless systems deployed today example outline major contributions work follows introduce novel optimization problem whose solutions capture blind decoding problem. formulation exploits structure underlying constellation solving optimization problem estimates channel gain matrix detects underlying data symbols using fewer samabstract—we propose method mimo decoding channel state information unknown transmitter receiver. proposed method requires structure transmitted signal decoding effective particular underlying sources drawn hypercubic space. proposed technique minimum volume parallelepiped received samples. problem expressed non-convex optimization problem solved high probability gradient descent. blind decoding algorithm used communicating unknown mimo wireless channels using either bpsk mpam modulation. apply technique jointly estimate mimo channel gain matrices decode underlying transmissions knowledge transmitted constellation without pilot symbols. results provide theoretical guarantees proposed algorithm correct applied small mimo systems. empirical results show small sample size requirements making algorithm suitable block-fading channels coherence times typically seen practice. approach loss less compared zero-forcing perfect imposing similar performance penalty space-time coding techniques without loss rate incurred techniques. work propose method blindly estimate mimo channels decode underlying transmissions. given knowledge statistics channel gain matrix constellation channel noise exploit geometry constellation order jointly estimate channel gain matrix decode underlying data. precisely exploit fact underlying constellation often hypercubic i.e. forms regular n-dimensional polytope mutually perpendicular sides case bpsk mpam modulation. technique presented work also applied decoding estimation simo channel gains unknown receiver coordination among transmitters. work presented part ieee globecom singapore. dean supported fannie john hertz foundation. work supported part center science information grant ccf- despite fact problem non-convex give theoretical empirical results showing gradient descent effective solving blind mimo decoding problem. precisely general values derive sufﬁcient conditions ensure global optima correspond solutions blind decoding problem case bpsk transmitted limit inﬁnite snr. relate blind decoding problem hadamard maximal determinant problem. present necessary conditions spurious optima within domain optimization problem implying approach always return solution blind decoding problem. further provide evidence formulating equivalent necessary conditions larger values likely intractable. give theoretical results relate number observed samples probability method returns correct solution blind decoding problem. theoretical results nearly exactly match empirical results. notice captures majority mimo systems today. although seems difﬁcult provide theoretical evidence gradient descent performs well large values present empirical evidence suggesting gradient descent efﬁciently solves non-convex optimization problem blind decoding problem values high values large further empirical evidence shows approach robust presence awgn decoding performance comparable known methods communicate mimo channel without imperfect csi. particular blind method outperforms existing non-blind methods somewhat inaccurate. remainder paper organized follows. section provide survey techniques related work. section describes system model. section outlines optimization problem solves blind decoding problem well algorithms solve optimization problem; theoretical performance algorithms shown section section presents empirical results support theory contained section concluding remarks provided section vii. proofs contained section found appendices. problem joint blind channel estimation decoding new. example authors apply mmse techniques blind decoding mimo problems small alphabets simultaneously recovering underlying channel gain matrix. approach requires number samples received signals used algorithm grow linearly constellation size exponential number transmit antennas. approach requires underlying constellation discrete; however constellations also hypercubic approach requires fewer received samples approach based simulation results. addition blind decoding algorithms previously applied hypercubic sources. authors present statistical learning algorithm applies modiﬁed version gram-schmidt algorithm estimate covariance matrix received signals learn channel gain matrix. different setting authors learn parallelepiped covariance matrix ﬁrst orthogonalizing recovering rotation higher order statistics. method rely covariance matrix estimation empirical results show requires fewer samples techniques especially channel gain matrix high condition number. blind source separation separation unknown signals mixed unknown process little information mixing process source signals. several previous works considered using blind source separation techniques detection signals transmitted unknown mimo channels. blind source separation typically accomplished techniques principle component analysis independent component analysis non-negative matrix factorization survey techniques. techniques exploit structure mixing process example requires mixing process toeplitz matrix. technique differs traditional blind source separation obtain estimate source signals learning inverse mixing process rather directly estimating source signals. output algorithm produces estimate mixing process i.e. channel gain matrix source signal i.e. transmitted symbols. similarly blind channel estimation techniques studied although commonly siso channels. surveys topic. approach presented paper viewed outside context communicating unknown mimo channel general technique performs blind source separation sources mixed unknown linear process. many techniques exist communications unknown mimo channels rely channel estimation. example space-time block coding introduced alamouti formalized tarokh techniques rely coding transmissions using sets highly orthogonal codes receiver recover transmission without csi. case transmitters rate space-time block codes exist impose penalty terms receiver. larger numbers transmitters rate codes exist. techniques require coding transmitter thus impose rate penalty. numerical simulation shows decoding performance technique comparable rate stbc methods. work focuses real-valued mimo channel block fading awgn. section iv-c discuss work extended complex-valued channels channels receivers transmitters. inputoutput relation channel characterized drawn standard m-pam bpsk constellation; respectively. channel matrix rn×n drawn random distribution. simulations section take drawn i.i.d. entries however approach requires full rank thus considered drawn arbitrary distribution entirely deterministic. noise i.i.d. entries drawn assume blockfading meaning value remains constant coherence time redrawn. receiver sees samples assume receiver knows constellation knowledge points drawn knowledge matrix given messages denote k-dimensional matrix formed taking symbol column corresponding matrix received symbols columns. notice cannot hope recover exactly. indeed since constellation invariant sign ﬂips permutations always write att−x product permutation matrix diagonal matrix entries distinguish solutions matrix termed admissible transform matrix thus work recover inevitable sign permutation ambiguities pose huge problem practice ignore comparing results mimo decoding algorithms known csi. justify approach follows. first non-blind estimation case assuming permutation ambiguity could resolved single pilot symbol. additionally consider simo multiple access channel ignore issue permutations received signals example assuming identiﬁcation occurs higher protocol layer. finally note sign ambiguity easily resolved differential modulation. practice also possible resolve ambiguities examining structure transmission scheme present either protocol/framing data structure underlying data. could prove difﬁcult however data encrypted compressed underlying transmission protocol designed thwart analysis. notation rounds elements nearest element denotes condition number matrix ratio largest smallest fig. program given aims linear transformation transforms observed mimo samples resulting within unit ball ﬁnding maximally-valued determinant effectively ﬁnding parallelepiped minimal volume observed samples. singular value denotes column vector formed column denotes vector formed gaussian binomial coefﬁcient prime power counts number dimensional subspaces vector space dimension ﬁnite ﬁeld elements. vector spaces notation denotes subspace rn×n corresponds length column vector obtained stacking columns usual ordering. given matrix cols denotes vectors comprise columns drawn hypercube values contained n-dimensional parallelepiped. received symbols slightly distorted parallelepiped. reasonable levels distortion minimal. thus formulate problem blindly estimating channel gain matrix ﬁtting parallelepiped observed symbols express problem optimization problem. given samples consider program domain invertible matrices meaning objective function necessarily convex. however show condition satisﬁed solutions form correspond global optima problem; moreover show often optima gradient descent them. section present three separate algorithms. ﬁrst present algorithm simple algorithm using gradient descent usual manner solve demonstrate section that practice algorithm sufﬁcient recover solution blind decoding problem. section iv-a present algorithm slightly modiﬁed version algorithm theoretical guarantee correctness conditions given section finally section iv-b include description interior-point method informally seeking maximize determinant subject ∞-norm constraints ﬁnding minimum volume parallelepiped observed samples. since inverse maximizing effectively ﬁnding minimal maps ∞-ball observed samples. depicted figure quantity present adds margin constraint account presence awgn. practice values close appear optimal captures additive gaussian channel noise. careful analysis warranted understand performance algorithm affected value optimal methods margining constraints optimization problem method ellipsoid peeling given methods presented lead improvements performance. however simple margin presented works well practice. order meaningful problem require full rank. full rank maximum exist formally shown proposition below proven appendix proposition matrix full rank unbounded above. conversely full rank bounded feasible. model full rank high probability thus assume always full rank turn attention solving maximizing determinant positive-semideﬁnite matrix classic problem convex optimization. unfortunately matrix necessarily even symmetric problem convex. order solve problem apply matlab fmincon solver uses gradient descent solve non-linear conic optimization problems. gradient problem given following value begin gradient descent check well conditioned. noted above must full rank problem make sense; however full rank poorly conditioned similar issues arise invert channel. thus return fail condition number larger κmax. gradient descent algorithm requires starting point input denoted draw matrix uniformly random orthogonal matrices using method described check random fact satisﬁes constraints; generate random matrix scale matrix constant term suitable starting condition. guaranteed suitable κmax iterations. algorithm summarized algorithm below. sufﬁciently large. however problem geometry studied section non-convex fact small non-zero probability gradient descent terminate global optimum. moreover section show algorithm fail global optima speciﬁc dimensions probability failure low. subsection present modiﬁed gradient descent algorithm algorithm shown below motivated theory section show strict solutions vertices problem boundary. algorithm always terminates vertex feasible region conjectured that general slightly larger non-singular vertices global optima solutions channel estimation problem implying algorithm always correct. describing algorithm make following observations. feasible region bounded halfspaces forming dimensional polytope. denote polytope notice changed without effecting whether constraints rows satisﬁed. further vertex vertex n-dimensional polytope deﬁnes problem boundary. algorithm begins choosing starting position manner algorithm performing gradient descent. section shown optima contained problem boundary also possible critical points problem boundary exist low-dimensional afﬁne subspaces along objective function constant valued. gradient descent reaches subspace algorithm continues choosing direction subspace random moving direction edge feasible region reached. point algorithm either reached vertex case terminates gradient descent continued point. process repeated vertex reached. results paper readily extend complex channels. accomplish mapping n-dimensional complex channel n-dimensional real channel usual manner. note mapping imposes additional constraints optimization problem. however section derive necessary sufﬁcient conditions algorithm return correct solution blind decoding problem. results directly imply simply ignore constraints solve n×n-dimensional real problem using algorithm since approach return correct channel gain matrix factor n-dimensional amount side information needed recover identical n-dimensional real case. whether structure present complex channels utilized create efﬁcient algorithm reduce required amount side information open question. receivers transmitters receiver still apply algorithms simply discarding received signals clearly suboptimal. optimization case topic future research. transmitters receivers nullspace channel gain matrix always non-trivial thus unbounded above. case assume transmit signals uncoded channel gain matrix full rank case work detecting signals transmitted channel meaningful problem. proving correctness algorithm solves nonconvex problem often difﬁcult task. section groundwork analysis studying noiseless case. provide guarantees correctness algorithm motivation studying algorithm algorithm become apparent following subsections difﬁcultly proving correctness algorithms general larger values results section strongly supported empirical results shown section results section suppose also focus bpsk case +}n. deriving matching theoretical results larger presence noise remains open problem; however empirical results given section show algorithms still work extremely well cases. mentioned section problem non-convex optimization problem thus several optima. analysis gradient descent applied problem proceed follows. first show section optimization problem reduces hadamard maximal determinant problem asks maximum value n-dimensional matrix whose entries contained unit disk. result establish guiding intuition remainder section. additionally show completely understanding problem geometry would solve hadamard maximal determinant problem; since latter considered algorithms perform gradient descent objective function subject convex constraints. na¨ıve implementation gradient descent stay within constraints. many algorithms perform constrained optimization overview completeness propose using interior-point method standard logarithm barrier function perform gradient descent step algorithms method attractive simple implement reasonable computational complexity numerical stability. results section obtained using method. formulate unconstrained optimization function using following barrier function section prove optima problem boundary feasible region. notice interior-point method efﬁcient algorithm given fact. base analysis contained paper gradient descent makes analysis tractable easily understood. defer investigating efﬁcient algorithms problem topic future research. section present theorems describe algorithms correctly solve blind decoding problem. proofs theorems contained within appendices work. remainder section organized follows. section show algorithm always terminate vertex feasible region strict optima vertices. section state necessary conditions global optima contains solutions blind decoding problem. finally conclude stating theoretical guarantees; namely necessary sufﬁcient conditions algorithm correctly solve blind decoding problem cases additionally conjecture performance algorithms larger note practice values captures nearly mimo systems currently today. reduction hadamard maximal determinant problem proceed showing equivalence hadamard maximal determinant problem case problem related ﬁnding dimensions hadamard matrices exist. hadamard matrix +}-valued matrix mutually orthogonal rows columns. hadamard matrices known exist conjectured exist lemma exists efﬁcient algorithm solves exists efﬁcient solution hadamard maximal determinant problem. proof. show given efﬁcient algorithm solve obtain solutions hadamard maximal determinant problem. given arbitrary full-rank samples setting arrive following optimization problem equivalent n×n. observation many consequences. many questions regarding hadamard maximal determinant problem remained open since problem originally posed hadamard even moderately sized values maximum value obtainable remains unknown unveriﬁed. however reduction holds empirically program given appears become easier grows relative roughly constraints removing vertices feasible region leaves vertices correspond solutions. show next subsection algorithm guaranteed terminate vertex removing vertices increases likelihood terminate vertex corresponds solution blind decoding problem. following facts consequences computational equivalence hadamard maximal determinant problem blind decoding problem value objective function vertices problem boundary strict optima correspond possible determinants of{− +}-valued matrices. understanding general open problem considered difﬁcult establishing upper bound maximum value determinant. finally state following lemma follows directly proof lemma lemma always global optimum vertex feasible region. hadamard matrix exists global optima strict vertices. subsection show algorithm guaranteed terminate vertex feasible reason. result important solutions blind decoding problem vertices shown following claim. claim solutions blind decoding problem vertices feasible region deﬁned proof. simple consequence fact +}n×k. takes +}n×k satisfy exactly constraints equality. since constrained region given polytope faces linearly independent corresponds vertex feasible region. notice convex claim immediately obvious obvious either gradient descent algorithm terminate vertex. show small non-zero chance gradient descent terminate vertex. motivates study algorithm algorithm concretely ﬁrst result regarding behavior algorithm stated follows theorem algorithm terminates vertex feasible region probability ﬁrst step proof theorem showing optima problem boundary. formally proven lemma lemma simple consequence facts objective function consists composition monotonically increasing function multilinear function problem boundary convex. facts imply that given point within feasible region boundary always move away origin increases objective function. already established lemma hadamard matrix exists optima strict. conversely dimensions hadamard matrix exist non-strict optima. lemma know non-strict optima must boundary feasible region. lemma corollary characterize non-strict optima show non-strict optima exists must restricted linear interval contained face polytope deﬁnes feasible region. show strict optima regardless existence hadamard matrix must vertices. fact together lemma guarantee algorithm reaches vertex. lemma show gradient descent proceed towards vertex. notice constraints independently vertex feasible region exactly constraints active row. fact show lemma gradient descent terminates least active constraints row. occurs edge feasible region; indeed exactly line move staying boundary feasible region affecting active constraints. show objective function must constant along line. thus active constraints simply choose direction random move direction reach vertex. thus guaranteed algorithm terminate vertex feasible region. established algorithm always terminates vertex feasible region. however point either global local optima correspond solution blind decoding problem. light study vertices feasible region correspond solutions blind decoding problem understanding when ever local optima exist. ﬁrst derive sufﬁcient condition solutions blind decoding problem correspond global optima precisely study following condition deﬁnition matrix corresponding cols maximal subset maximal subset property contains subset columns that viewed matrix -valued determinant matrices deﬁnition state sufﬁcient condition solutions problem global optima. lemma maximal subset property then atms matrices form global optima lemma proved appendix small compute probability samples maximal subset property; result given appendix section show empirical success probability algorithm samples exactly matches probability distribution derived appendix natural whether converse lemma true. fact explicit counterexample exists found computer simulation shows maximal subset property necessary condition. important probability ﬁnding maximal subset uniform sampling decreases rapidly grow. indeed agrees empirical observations speciﬁcally table found section show increase required maintain constant success probability appears less quadratic established maximum subset property sufﬁcient condition continue analysis geometry nonconvex optimization problem considering speciﬁc values easily checked full-rank matrices +}n×n maximal subset property. words possible values determinant contains possible absolute values verify atms orthogonal matrices elements elements set. further since hadamard matrix exists optima strict vertices feasible region. facts imply following theorem. theorem algorithms correct maximal subset property. maximal subset property alone sufﬁcient ensure algorithms succeed. indeed exists matrix implies existence spurious optima whenever however spurious optima exist contains least additional distinct column beyond three required maximal subset property. distinct column mean column distinct notice also implies columns pairwise linearly independent. further note algorithm longer guaranteed correct contains hadamard matrix. formally state theorem proven appendix regarding performance algorithm turn attention quantifying probability conditions required theorems hold. denote probability collection vectors rank probability solver succeeding given samples chosen uniformly random given explicit formula derived appendix notice equation expresses probability global optima contains solutions blind decoding problem. since require existence fourth distinct vector probability samples chosen uniformly global optima solutions note that collection samples contains additional distinct column algorithm still non-zero probability ﬁnding solution blind decoding problem global optima still contains solutions. thus probability success algorithm conditioned uniform selection samples bounded section compare distributions empirical results. dimension problem geometry gets slightly complicated. possible values determinants increases {±±} means non-singular vertices global optima however show optima indeed global optima. unfortunately global optima solutions blind decoding problem. nonetheless able show algorithms succeed probability proper input conditions. stating theorem proved appendix must also introduce equivalence classes hadamard matrices. hadamard matrices equivalent exists equivalence relation thus decomposes hadamard matrices equivalence classes. denotes notice vectors appear column vectors either vector belongs equivalence class appears column vector equivalence class. state result case proven appendix notice hadamard matrix exists lemma optima strict hence gradient descent always terminate vertex even without modiﬁcation given algorithm theorem algorithm correct probability cols contains least four linearly independent vectors ﬁfth vector theorem implies always require least samples order solve blind decoding problem. further assuming source symbols chosen uniformly random result allows quantify success probability blind decoding algorithm. done appendix show success probability given subsection discuss performance algorithm larger values figure algorithm attempt maximal determinant matrices described lemma algorithm terminated global maximum time supporting claim local maxima cases explicitly proven dimension also suggests similar theoretical guarantee exist proving result manner used case would computationally expensive. dimensions analysis seems extremely difﬁcult. indeed even reasonably small values possible determinants +}-valued matrices well understood large values maximal value determinant known special cases example fig. probability algorithm ﬁnds maximal-determinant matrix used described lemma averaged samples. dimension optima global. this guaranteed terminate maximal matrix. however grows relative probability increases again. figure fig. success probability algorithm versus number samples simulations trials. predicted results probability requisite subset columns ensure correctness algorithm discussed section appendix probability success algorithm empirically grows toward sufﬁciently large. intuitively happens adding additional constraints removes vertices feasible region. effect removing local optima well global optima correspond solutions problem. based theory established section empirical results section make following conjecture behavior algorithms general values conjecture slightly larger selected uniformly matrices maximal subset property high probability optima atms established theoretical results regarding correctness algorithm turn attention empirical results. simulation results contained section entirely based algorithm demonstrate algorithm unnecessary practice least dimensions. empirical performance algorithm noticeably improve performance algorithm order assess performance algorithm constructed sets experiments. ﬁrst algorithm various values without channel noise order empirically test conditions solver return correct solution. second algorithm using realistic channel conditions compared results zero-forcing maximum-likelihood decoders perfect imperfect csi. fig. empirical success probability algorithm awgn. simulations trials. beginning local optima exist contains maximum subset property. however increases local optima become infeasible increasing success probability algorithm table shows number samples required various values recover correct form success rate using algorithm table bottom represents number samples needed ensure success rate using either ilsp ilse techniques presented table summarizes number samples required various values algorithm probability returning optimal solution values presented success probability almost entirely conditioned upon input value rather randomness algorithm running algorithm multiple times improve success rates. figure shows expected success rate based theory section appendix results plot case corresponds binary phase shift keying absence noise. theoretical success probability probability maximal subset property. success guaranteed maximal subset property well least additional distinct vector. expected success rate algorithm known maximal subset property alone. probability maximal subset property plotted lower bound performance case; upper bound given figure expresses probability maximal subset property well additional vector. shown section know algorithm succeed probability maximal subset property alone; reﬂected theoretical prediction case. note empirical observations match expected theoretical performance. figure shows empirical success probability algorithm plot demonstrates important features regarding performance algorithm grows. first know local optima exist. figure section gives probability maximal subset property algorithm global optima. however figure large enough values success probability algorithm exceeds probability. additional samples cause local optima become infeasible increasing probability algorithm global optima. additionally results show required values appear scale favorably grows. note captures figures shows symbol error rate performance blind decoder compared standard mimo decoding algorithms. figure gives example high high modulation order parameters figure shows case values typically found modern cellular systems. despite less side information performance blind decoder slightly worse decoders perfect csi; appears less loss associated blind decoder. simulation used fading block length samples total fading blocks snr. high dimensions large numbers constraints leads numerical instability requiring step size extremely small making solver slow converge. improving runtime algorithm example intelligent selection subset received samples topic future research. motivated real-world usage compared blind decoding decoders imperfect csi. assume channel estimated known pilot symbols corrupted gaussian noise error i.i.d. gaussian. realistic assumption wireless systems model used simulations. figures also plot performance algorithm variance error channel gain matrix awgn channel estimation error. cases algorithm signiﬁcantly outperforms decoders. provided algorithm jointly estimate mimo channels decode underlying transmissions block fading channels. algorithm performs gradient descent non-convex optimization problem. empirically algorithm performance loss order several decibels versus schemes perfect performance becomes superior knowledge imperfect. algorithm practical works well block-fading channels realistic coherence times. addition important application decoding mimo channels missing imperfect algorithm potentially useful point view eavesdropper know pilot symbols trying recover present in-depth analysis geometry nonconvex optimization problem. speciﬁcally prove small values optima global give necessary sufﬁcient conditions optima correspond solutions blind decoding. general values relate problem hadamard maximal determinant problem give evidence providing matching theoretical guarantees larger values likely difﬁcult. however empirical results suggest algorithm remains feasible values commonly found modern mimo systems. fig. decoding performance compared zero-forcing maximum-likelihood decoder implemented parallel channel decomposition. ﬁgure estimation error present bottom ﬁgure compares blind decoding decoding imperfect ﬁgures show error even percent channel noise variance blind decoding outperforms algorithms. blind decoding algorithm appears loss decoding perfect csi. provide theoretical results analytically explain performance presence awgn also leave open possible extensions rectangular complexvalued channels well efﬁcient algorithms gradient descent solve blind decoding problem. grows without bound grows. conversely full rank left nullspace thus non-zero always affect maximum bounded implying bounded above. similarly always satisﬁes example consider claim suppose maximal subset property. matrices proof. guaranteed maximal subset property. matrix whose columns maximal. would imply cannot contained n×n. also fact feasible region formed n-dimensional polytope categorize optima optimization problem stated following lemma. denote polytope describes feasible region given denote face polytope. since matrix acts constraints independent manner active linearly independent constraints active row. rows active vertex further active rows lies face dimension lemma suppose interior face exists interval deﬁned satisﬁes further points given λjeiv face lower dimension proof. polytope bounded rows vertex exists active. nonactive must thus exists cols true since full rank. require following claim remains show appropriate values points given λjeiv lower dimensional face since exists must exactly values λiejv thus bounding values additional constraint active implying lower dimensional face λjeiv established lemma show following corollary allows characterize optima corollary non-strict optima restricted interval given further points interval lower dimensional face. interior face strict optimum. indeed along move must either direction either increase value objective function keep objective function constant. result needed complete theorem provides insight geometry problem. order understand gradient descent behave boundary feasible region must consider directional derivatives directions problem boundary. rn×n direction that feasible also feasible. gradient descent terminate show hold either active linearly independent constraints. lemma well corollary motivate algorithm implies corner cases algorithm fail corner cases easily handled forcing algorithm terminate nearest vertex. lemma fewer active constraints exists non-zero matrix satisﬁes proof. consider denoted corresponding elements corresponding gradient matrix given feasible values orthogonal fewer linearly independent constraints active always subspace least dimension select precisely suppose constraints given active. subspace spanned vectors must always null space least dimension two; contained nullspace feasible. long nullspace dimension least exists satisﬁes words lemma gradient descent terminates vertex must least exactly active linearly independent constraints. case move along interval guaranteed corollary reach lower dimensional face. lower dimensional face either vertex case reached strict optima algorithm terminate exist positive gradient resume gradient descent. completes theorem know maximal subset property algorithm always terminate global maximum global optima contains solutions form however maximal subset property alone ensure global optima solutions blind decoding problem. spurious optima must form algorithm correct probability spurious optima. following lemma shows choice restricts orthogonal. lemma suppose given proof. know that theorem algorithm must terminate vertex. thus must +}n×k implies that consider linear operators determinant whose action preserves norms vector consider action surface sphere contains vectors sphere radius comprise columns action linear operator determinant mapped ellipsoid vol. further know four distinct columns always case. consider case maximal subset property four columns distinct. matrix formed subset three columns maximal subset property. must exists ±vj. also must −qvj thus whenever contain columns distinct columns also optima completes theorem appendix prove theorem gives necessary sufﬁcient conditions algorithms return correct solutions blind decoding problem notice hadamard matrix exists know lemma optima strict vertices feasible region. thus algorithm needed case. considering speciﬁc case prove following general statement values hadamard matrix exists. result used proof theorem hadamard matrix exists characterize solutions noiseless case follows. claim ellipsoid centered origin volume contains points given cols sphere. proof. consider arbitrary ellipsoid centered origin. described following equation require ellipsoid contain points substituting points seen must thus must diagonal. eigenvalues inverse squares length semi-axes ellipsoid. implies volume ellipsoid given abc. solution gives implying required ellipsoid fact sphere. established implies show feasible elements fact atms. proposition suppose proof. notice vectors cols form face unit cube. since cols must also form face unit cube. orthogonal must preserve norms planes. fact restricts symmetries cube. symmetries cube correspond atms. finally notice diag implies n×k. similarly permutation matrices feasible feasible. possible ±-valued matrices contain four distinct columns expressed xpd. implies possible four distinct columns hence choice four distinct columns sufﬁcient ensure spurious optima. turn attention showing converse requiring four distinct columns fact necessary algorithm correct probability first choice maximal subset property spurious optima exist. small collection three vectors atms maximal subset property check always true. therefore always lemma know must determinant one. matrix guaranteed maximal subset property. must hadamard. matrix also maximum value determinant matrices thus must also hadamard. since points -norm claim must orthogonal. completes proof lemma point might tempted conjecture that fact maximal subset property sufﬁcient; orthogonal matrix lemma replaced however case show proof theorem given below. theorem algorithm correct probability cols contains least four linearly independent vectors ﬁfth vector prove theorem parts. first lemma show maximal subset property algorithm return correct solution blind decoding problem probability addition extra vector separate equivalence class global optima correspond solutions blind decoding problem. lemma fact prove slightly general statement give probability global optima correspond solutions blind decoding problem given input algorithm chosen uniformly random. second lemma show values optima indeed global despite fact suboptimal vertices. lemma collection samples chosen uniformly random probability global optima correspond solutions blind decoding problem given proof. verify matrix maximal subset property must matrix full rank matrices obtained choosing subset three rows must also full rank. probability random vectors dimension hadamard matrix given observation maximal subset algorithm succeeds time notice given above similarly notice vectors appear either product times vector similarly times consider collection vectors contains independent elements vectors collection belong equivalence class matrices containing factor optima otherwise matrices outside feasible region global optima correspond solutions. reason adding constraints removes vertices feasible region thus increasing success probability algorithm given argument probability −n−k global optima solutions conditioned fact matrix maximal subset property. since equation gives probability maximum subset holding express probability that given random samples global optima solutions order arrive desired result still need show vertices correspond matrices determinant local optima proven completes theorem lemma optima global. proof. optima vertices problem boundary optima correspond non-singular {±}-valued matrices lemma need show matrices determinant local optima. begin facts veriﬁed computer simulation. create graph node matrix edges pair nodes hamming distance one. remove graph nodes correspond determinant zero leaving nodes determinant since determinant linear function columns matrix value changes linearly along edge graph. studying geometry graph give insight paths algorithm travel arriving optima. ﬁrst observation graph partitioned components. veriﬁed example either inspecting least eigenvalues laplacian adjacency matrix graph performing breadth-ﬁrst search. component graph corresponds {±}-valued matrix positive determinants matrices negative determinants. means traverse either component graphs without objective function changing sign. implies that edge graph objective function either fig. matrix determinant hamming distance away matrix determinant hamming distance away also determinant objective function constant lines monotonically increasing line veriﬁed maximum hamming distance matrix determinant determinant hamming distance clearly local optima. thus turn attention remaining determinate matrices hamming distance determinant matrices connected matrices distance away matrix. depicted figure matrix distance optimal matrix adjacent suboptimal matrices corollary know objective function constant afﬁne subspace. constant along lines line implies objective function cannot constant line further know determinants sign cannot critical points line. thus objective function along line monotonically increasing implying determinant matrix local optimum. section consider distribution rank collection vectors drawn uniformly rank collection vectors less equal rank allow obtain exact expression probability binary vectors full rank begin stating simple lower bound shows probability collection vectors full rank decays exponentially fast number vectors grow. section refer size collection vectors. compute exact distribution dimension subspace spanned random subset vector space ﬁnite ﬁeld. distribution compute exact expression computation makes m¨obius inversion formula standard tool combinatorics number theory provides natural count elements partially ordered sets using overcountingtreatment m¨obius undercounting procedure. full functions applications interesting counting number collections vectors span given subspace. number k-tuples span subspace number k-tuples span either subspace clearly prasad murthy joint approximately sparse channel estimation data detection ofdm systems using sparse bayesian learning ieee trans. signal process. vol. talwar viberg paulraj blind separation synchronous co-channel digital signals using antenna array. algorithms ieee trans. signal process. vol. nguyen regev learning parallelepiped cryptanalysis ntru signatures annual international conference theory applications cryptographic techniques springer hyv¨arinen independent component analysis algorithms applications neural networks vol. ling strohmer blind deconvolution meets blind demixing authors would like thank yonathan morin insightful conversation mimo decoding channel estimation comments preliminary version work mainak chowdhury discussion non-coherent mimo channels optimization milind discussion optimization blind-source separation statistical learning techniques ronny hadani comments algorithm complex-valued channels jonathan perlstein providing counterexample case comments preliminary version work.", "year": "2018"}