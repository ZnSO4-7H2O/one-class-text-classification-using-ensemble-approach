{"title": "Coded Caching in a Multi-Server System with Random Topology", "tag": "eess", "abstract": " Cache-aided content delivery is studied in a multi-server system with $P$ servers and $K$ users, each equipped with a local cache memory. In the delivery phase, each user connects randomly to any $\\rho$ out of $P$ servers. Thanks to the availability of multiple servers, which model small base stations with limited storage capacity, user demands can be satisfied with reduced storage capacity at each server and reduced delivery rate per server; however, this also leads to reduced multicasting opportunities compared to a single server serving all the users simultaneously. A joint storage and proactive caching scheme is proposed, which exploits coded storage across the servers, uncoded cache placement at the users, and coded delivery. The delivery \\textit{latency} is studied for both \\textit{successive} and \\textit{simultaneous} transmission from the servers. It is shown that, with successive transmission the achievable average delivery latency is comparable to that achieved by a single server, while the gap between the two depends on $\\rho$, the available redundancy across servers, and can be reduced by increasing the storage capacity at the SBSs. ", "text": "users directly reducing latency backhaul load energy consumption coding distributed storage systems extensively studied literature femtocaching scenario ideal rateless maximum distance separable codes allow users recover contents collecting parity bits subset sbss connect work combine distributed storage sbss similar femtocaching framework cache storage users consider coded delivery error-free shared broadcast links consider library ﬁles stored across sbss equipped limited-capacity storage space differently existing literature consider random connectivity model delivery phase user connects random subset abstract—cache-aided content delivery studied multiserver system servers users equipped local cache memory. delivery phase user connects randomly servers. thanks availability multiple servers model small base stations limited storage capacity user demands satisﬁed reduced storage capacity server reduced delivery rate server; however also leads reduced multicasting opportunities compared single server serving users simultaneously. joint storage proactive caching scheme proposed exploits coded storage across servers uncoded cache placement users coded delivery. delivery latency studied successive simultaneous transmission servers. shown that successive transmission achievable average delivery latency comparable achieved single server depends available redundancy across servers reduced increasing storage capacity sbss. unprecedented growth transmitted data volumes across networks necessitates design efﬁcient delivery methods exploit available memory space processing power individual network nodes increase throughput efﬁciency data availability. coded caching distributed storage received signiﬁcant attention recent years promising techniques achieve goals. proactive caching part data pushed nodes’ local cache memories off-peak hours called placement phase reduce burden network particularly wireless downlink peak-trafﬁc periods users place requests called delivery phase. intelligent design cache contents creates multicasting opportunities across users multiple demands satisﬁed simultaneously coded delivery. coded caching able utilize cumulative cache capacity network satisfy users much lower rates equivalently lower delivery latency different type coded caching shown improve delivery performance so-called femtocaching scenario femtocaching ﬁles replicated coded multiple cache-equipped small base stations user reconstruct request subset available sbss. sbss edge caches provide contents work supported part european union‘s research innovation programme marie sklodowska-curie action scavenge european research council starting grant beacon sbss physical variations channel resource constraints. importantly connections form network topology known advance placement phase; therefore cache placement cannot designed particular network topology. storing ﬁles across multiple sbss allowing users connect randomly subset results loss multicasting opportunities servers indicating trade-off coded caching gain ﬂexibility provided distributed storage across servers which best knowledge studied before. hand presence multiple servers improve latency user requests satisﬁed parallel. accordingly scenarios discussed depending delivery protocol. servers transmit successively i.e. time-division transmission total latency latencies link delivering requests. servers operate parallel i.e. simultaneous transmission latency given link maximum latency. propose practical coded cache placement delivery scheme exploits coding across servers simultaneously coded caching delivery users. successive transmission scenario show cost ﬂexibility distributed storage scaling latency constant. also characterize average worst-case latency proposed scheme assuming users connect uniformly random subset servers; show relatively close best-case performance single-server centralized delivery time derived achieved users connect servers. observe that server storage capacities increase average delivery time-user cache memory trade-off improves approaching single-server delivery time. also identify delivery latency proposed scheme servers transmit simultaneously characterize achievable average worst case delivery time function server storage capacity different values. related work authors study coded caching schemes presented parity servers available. authors consider special scenarios parity servers. propose scheme stripes ﬁles blocks codes across servers systematic code also propose scheme scenario ﬁles stored whole units servers without striping. work specify servers parity servers instead propose scheme generalizes type code number storage servers. study impact topology maximum delivery rates trade-off server storage space average rates. authors consider multiple servers serving users intermediate network relay nodes server access ﬁles library. authors study delivery delay considering simultaneous transmission servers. note that model considers limited storage servers random topology delivery network unknown placement phase. another line related works study caching combination networks consider single server serving cache-equipped users multiple relay nodes. server connected relays unicast links turn serve distinct subset ﬁxed number users unicast links. combination network cache-enabled relay nodes considered however symmetry standard combination network would unrealistic many practical scenarios assumption ﬁxed known network topology placement phase make caching scheme analysis fundamentally different paper. notations. integers denote denoted sets denoted calligraphic font denotes cardinality deﬁne consider system model illustrated fig. servers denoted serving users denoted library ﬁles length bits uniformly distributed user access local cache memory capacity bits server storage memory capacity bits. caching scheme consists phases placement phase delivery phase. consider centralized placement scenario carried centrally knowledge servers users participating delivery phase. however neither user demands network topology known advance placement phase. delivery phase assume user randomly connects servers requests single library. denote servers connects |zj| denotes index requests. example fig. demand vector denoted topology network i.e. users connected servers demands users revealed servers beginning delivery phase. complete library must stored servers coded manner provide redundancy since user connects random subset servers. since user able reconstruct requested cache memory servers connected total cache capacity user servers must sufﬁcient store whole library; must denote users served deﬁne random variable |kp| denotes number users served shall denote particular realization given topology example fig. server transmits message size bits users connected i.e. users corresponding shared link. message function demand vector network topology storage memory contents server cache contents users user receives messages reconstructs requested using messages local cache contents. goal minimize delivery time time user requests satisﬁed. delivery time depends operation sbss. transmits orthogonal frequency band requests delivered parallel normalized delivery time given maxp number bits transmitted server delivery phase. instead servers transmit successively time-division manner suitable user devices simple capable multihoming multiple frequencies goal average worst-case delivery time worst case refers fact users correctly decode requested ﬁles independent combination ﬁles requested them averaging possible network topologies. assuming difﬁcult users requesting different corresponds worst-case scenario. ﬁrst note system model brings together aspects distributed storage proactive caching/coded delivery. this consider system without user caches i.e. equivalent distributed storage system unreliable servers. known codes provide much higher reliability efﬁciency compared replication scenario hand servers reliable i.e. system equivalent coded delivery provides signiﬁcant reductions delivery rate. accordingly proposed scheme brings together beneﬁts coded storage coded delivery. illustrate main ingredients proposed scheme section. extension server assume capacities presented section iv-a. ﬁrst describe ﬁles stored across servers order guarantee user request satisﬁed servers user connect assume initially integer i.e. solution non-integer values obtained memory-sharing divided equal-size nonja overlapping sub-segments denoted sub-segments segment coded together using linear code generator matrix giving output coded versions segment denoted linear combination subsegments segment corresponding subset stored server since sub-segment length length; hence server storage capacity constraint equality. remark assume user knows generator matrix code able reconstruct coded uncoded segment stored symbol cache memory. ﬁrst make following observation placement scheme worst-case demand scenario consider users. users share caches segment requested remaining consider server users connected then subset includes i.e. segment wdkhi\\{k} needed user available cache hi\\{k}; available caches remaining users coded version wdkhi\\{k} stored dkhi\\{k} since users know generator matrix user wdkhi\\{k} cache reconstruct dkhi\\{k} well. then includes least user transmits topologies assuming uniformly random user-server association; user connects servers uniform distribution. seen above delivery time depends topology given topology worst-case delivery time refers worst-case demand combination user requests different ﬁle. denote possible topologies. theorem presents average normalized worst-case delivery time proposed scheme. theorem worst-case average normalized delivery time proposed scheme topologies uniformly random user-server association given users served particular server. proof. topology represented particular tuple topology distinct tuples necessarily distinct. demonstrated fig. distinct topologies tuple associated them. expectation worstcase delivery rate possible topologies written server storage capacity ﬁxed minimum server storage capacity would allow reconstruction demand combination given n−mu fraction library user caches placement phase transmit remaining fraction library servers. worsttransmitted message user decode coded version dkhi\\{k} requested segment wdkhi\\{k}. transmissions servers receives coded versions missing segment servers connected since segment coded code user able decode missing segment. note transmitted message server transmischaracterize best worst network topologies lead minimum maximum delivery times respectively present following lemma without proof. lemma satisfying lemma deduced second summation term takes minimum maxp minp+ i.e. values close possible. corresponds class topologies highest delivery times topology requires minimum delivery time either server equivalently users connected servers best worst network topologies different successive transmission scenario. topology minimum value maximum i.e. values close possible best delivery time contrary successive transmission scenario would worst topology. corresponding delivery time obtained substituting topology maximum possible i.e. topology least server connected users worst topology since highest delivery time. minimize delivery time scenario parallel simultaneous delivery servers greedy server allocation algorithm used applies algorithm used section greedy manner balance number transmissions servers iteration. fig. plot achievable trade-off user cache capacity normalized delivery time best worst topologies average normalized delivery time topologies successive transmission. average normalized delivery time parallel transmission scenario also plotted. trade-off curves plotted different server storage capacities. observe worst best topologies signiﬁcant. deduce that successive transmission worst topology delivery time; hence average delivery time proposed scheme within multiplicative factor best topology delivery time. fig. average delivery time-server storage capacity trade-off plotted server storage capacities plot obtained performing multiple simulations random realizations topology averaging achievable delivery time them. observe fig. delivery time decreases signiﬁcantly particularly values redundancy server storage increases. observe fig. average delivery time decreases rapidly initial increase server storage capacity decrease become signiﬁcantly fast high values. because thanks mds-coded caching servers number available multicasting opportunities increases redundancy across servers. assume server memories; integer non-integer values solution obtained memory-sharing. case code used server storage placement allows user reconstruct requested connecting servers. user cache placement done previous section. delivery phase user randomly connects servers. degree freedom thanks additional storage space available server. user particular segment subset servers connected receiving copy servers. choice servers deliver coded subsegments user done multicasting opportunities across network maximized. construct incidence matrix dimensions server connected user otherwise. consider −element subset segments wdkhi\\{k}∀k consider columns corresponding users matrix formed them. deﬁne minimum cover smallest submatrix least non-zero values column. servers corresponding rows submatrix transmit coded message satisfy completely requests missing segments corresponding therefore total number transmissions required deliver segments wdkhi\\{k} equal minimum cover example consider incidence matrix shown fig. corresponds system servers users user connects servers. assume server storage capacity setting coded subsegments requested ﬁles delivered users multicasting sufﬁcient user receive coded segments servers. then user consider submatrix corresponding columns rows smallest submatrix satisfying condition column least hence minimum cover equal number rows submatrix similarly minimum cover thus segments wdk{}\\{k} transmits dk{}\\{k} transfig. average normalized delivery time user cache capacity server best worst storage capacities topologies illustrated fig. average delivery time parallel transmissions also plotted. opposed delivery time successive transmissions delivery time saturate keeps decreasing ﬁles stored servers. also observe reduction delivery time saturates increases. presented multi-server coded caching delivery network cache-equipped users connect randomly subset available servers limited storage capacity. allows server limited amount storage capacity requires coded storage across servers account random topology. proposed novel scheme jointly applies mds-coded caching servers uncoded caching coded delivery users. achievable delivery times scheme successive simultaneous transmissions sbss presented averages topologies studied. analysis shows that price robustness reliability using distributed storage much even servers operate time-division manner. golrezaei molisch dimakis caire femtocaching device-to-device collaboration architecture wireless video distribution ieee commun. mag. vol. apr.", "year": "2017"}