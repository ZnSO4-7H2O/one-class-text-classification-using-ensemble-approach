{"title": "Symbol Error Rate Performance of Box-relaxation Decoders in Massive MIMO", "tag": "eess", "abstract": " The maximum-likelihood (ML) decoder for symbol detection in large multiple-input multiple-output wireless communication systems is typically computationally prohibitive. In this paper, we study a popular and practical alternative, namely the Box-relaxation optimization (BRO) decoder, which is a natural convex relaxation of the ML. For iid real Gaussian channels with additive Gaussian noise, we obtain exact asymptotic expressions for the symbol error rate (SER) of the BRO. The formulas are particularly simple, they yield useful insights, and they allow accurate comparisons to the matched-filter bound (MFB) and to the zero-forcing decoder. For BPSK signals the SER performance of the BRO is within 3dB of the MFB for square systems, and it approaches the MFB as the number of receive antennas grows large compared to the number of transmit antennas. Our analysis further characterizes the empirical density function of the solution of the BRO, and shows that error events for any fixed number of symbols are asymptotically independent. The fundamental tool behind the analysis is the convex Gaussian min-max theorem. ", "text": "consider problem recovering unknown vector transmitted symbols belonging ﬁnite constellation noisy multiple-input multiple-output relation rm×n mimo channel matrix noise vector. assume real gaussian channel additive gaussian noise. particular entries entries normalization signal-to-noise ratio varies inversely proportional noise variance interested large-system limit number transmit antennas number receive antennas large. simplicity exposition assume part paper ndimensional bpsk vector i.e. {±}n. extensions m-ary constellations also provided. maximum-likelihood decoder. decoder bpsk signal recovery maximizes block error probability given minx∈{±}n solving exact solution often computationally intractable especially large therefore variety heuristics proposed box-relaxation optimization decoder. heuristic consider paper box-relaxation optimization decoder consists steps. ﬁrst involves solving convex relaxation algorithm {±}n relaxed output optimization hard-thresholded second step produce ﬁnal binary estimate. formally algorithm outputs estimate given sign function returns sign input acts element-wise input vectors. decoder naturally extends case recovering signals higher-order constellations; section iii. symbol error rate. evaluate performance decoder symbol error rate deﬁned abstract—the maximum-likelihood decoder symbol detection large multiple-input multiple-output wireless communication systems typically computationally prohibitive. paper study popular practical alternative namely box-relaxation optimization decoder natural convex relaxation real gaussian channels additive gaussian noise obtain exact asymptotic expressions symbol error rate bro. formulas particularly simple yield useful insights allow accurate comparisons matchedﬁlter bound zero-forcing decoder. bpsk signals performance within square systems approaches number receive antennas grows large compared number transmit antennas. analysis characterizes empirical density function solution shows error events ﬁxed number symbols asymptotically independent. fundamental tool behind analysis convex gaussian min-max theorem. problem recovering unknown vector symbols belong ﬁnite constellation noise corrupted linearly related measurements arises numerous applications particular multiple-input multiple output wireless communication systems result large host exact heuristic optimization algorithms proposed years. exact algorithms sphere decoding variants become computationally prohibitive problem dimension grows scenario typical modern massive mimo systems e.g. heuristic algorithms zero-forcing mmse decision-feedback etc. inferior performances often difﬁcult precisely characterize. popular heuristic called box-relaxation optimization decoder natural convex relaxation maximum-likelihood decoder allows recover signal efﬁcient convex optimization followed hard thresholding e.g. despite popularity little known analytically decoding performance method. paper close deriving exact asymptotic error-rate characterizations assumption real gaussian wireless channel additive gaussian noise. theorem characterizes leads accurate comparison decoder. extend results m-pam constellations section iii. section includes main technical result paper namely theorem well detailed proof. paper concludes section discussion future research directions. finally technical proofs deferred appendix. precisely analyze error-rate performance decoder bpsk signals. main theorem section ii-a evaluates symbol error rate simple closed-form bounds computed section ii-b. sections ii-c ii-d bounds compare decoder matched-ﬁlter bound zeroforcing decoder respectively. standard notation plimn→∞ denote sequence random variables converges probability towards constant limits taken regime keep notation short simply write finally denote q-function e−h/. sociated standard normal density theorem denote symbol-error-rate box-relaxation optimization decoder ﬁxed unknown bpsk signal {±}n. constant constant then limit holds theorem explicitly characterizes high-probability limit randomness channel matrix noise vector function deterministic strictly convex parametrized value proportionality factor proof theorem uses convex gaussian min-max theorem thus found major precisely quantifying squared-error performance regularized m-estimators high-dimensions lasso paper extend applicability cgmt characterization performance arrive theorem ii.. that along paper derive ﬁrst rigorous precise characterization decoder largesystem limit numbers receive transmit antennas grow proportionally large ﬁxed rate m/n. complement precise error formulas closed-form tight upper lower bounds simple functions bounds allow useful insights decoding performance allow quantitative comparison matched-ﬁlter bound zero-forcing decoder. concrete example bpsk signals highsnr) q-function tail massive mimo systems. however replica method involves conjectured assumptions remain mostly unveriﬁed rigorous means; please references therein. contrast analysis rigorous techniques used fundamentally different. based recent advances comparison inequalities gaussian processes; particular convex gaussian min-max theorem present paper signiﬁcantly extended version conference paper related recent line work authors proposed investigated performance class iterative decoding methods signal detection large mimo systems rely approximate message passing decoding methods papers discuss different decoder analysis tools used also different ones presented here. interestingly paper appeared authors used results show proposed algorithm achieves error-rate performance decoder. probability error recall symbolerror probability given also bounded thus using theorem show appendix converges value corollary setting theorem denote symbol-error probability minimizer then solving order evaluate large-system limit needs compute unique positive minimizer function strictly convex hence done numerically efﬁcient way. convexity also described unique solution ﬁrst order optimality conditions minimization program analyzing properties derive section ii-b simple closed-form bounds quantity interest namely numerical illustration figure illustrates accuracy prediction theorem ii.. note although theorem requires prediction already accurate scale hundreds. derive simple closed-form upper lower bounds limiting value ser. show bounds tight. proof deferred appendix theorem unique minimizer then values values holds furthermore upper bound becomes tight directly establishes upper lower bounds performance bro. bounds given closedform simple functions snr. stated theorem upper bound becomes tight high-snr regime. hence limit formal statement result given theorem appendix fact intuitively understood follows highsnr expect going zero small). case last summands negligible; then solution fig. symbol-error probability function different values ratio receive transmit antennas. theoretical prediction follows theorem ii.. simulations used data sample averages independent realizations channel matrix noise vector value snr. establish large-system error performance wide class performance metrics; class includes squared-error special cases. explicitly characterize limiting empirical distribution theorem holds long ratio proportionality greater begin with note allows number receive antennas less number transmit antennas half them. system linear equations underdetermined; hence recovering true solution generally ill-posed even absence noise. however problem interest a-priori known true solution takes values {±}n. decoder uses information enforcing ∞-norm constraint course idea using convex optimization constraints promote particular structure unknown signal solve underdetermined system equations core ideas emerged compressed sensing literature fact well-understood noiseless case program successfully recovers true {±}n high probability randomness necessary condition naturally arises proof observe similar minimization unconstrained. therefore contrast decoder require i.e. number receive antennas larger number transmit antennas. case large full column-rank probability unique closed-form solution particular well-known result literature standard tools random matrix theory derive symbol-error probability decoder convenience reader brieﬂy summarize main idea here. without loss generality consider last decomposition rm×n matrix orthogonal columns rn×n upper triangular. deﬁne note diagonal element rotational holds next following well-known facts e.g. independent matrices. hence independent rnn; random variable degrees freedom. thus corresponding formula bpsk single-input singleoutput gaussian channel symbol-error probability zero-forcing decoder fig. symbol-error probability comparison high-snr approximation matched ﬁlter bound theorem successfully predicts curves sandwiched corresponding green blue ones finally section ii-c show lower bound snr) operational meaning equal denote corresponds probability error detecting from xnan assumed known denotes column estimate equal sign projection vector direction without loss generality assume then output matched ﬁlter becomes sign fig. symbol error probability relaxation optimization function bpsk -pam -pam signals. theoretical prediction follows theorem iii.. simulations used data averages independent realizations channel matrix noise vector value snr. theorem iii. denote symbol error rate detection scheme typical transmitted signal symbol takes values equal probability constant noise variance then limit constant using tools random matrix theory. alternatively obtain result using cgmt proof technique similar theorem ii.. random-matrix-theory tools analysis decoder large possible minimizer ˆxzf expressed closed-form function contrary case decoder cgmt critical establishing theorem ii.. positive integer. antenna transmits single i.e. {±}n setting section always assume additive gaussian noise variance decoder given minx∈cn often computationally intractable large number receive/transmit antennas. consider natural extension box-relaxation decoder bpsk speciﬁcally m-pam symbol transmission outputs estimate follows optimization convex simply selects symbol value closest solution among total choices therefore proposed decoder computationally efﬁcient. next section evaluate error-rate performance. theorem iii. precisely characterizes largesystem limit m-pam transmission. assume typical sequence symbols sent channel i.e. transmitted symbol takes values equal probability result extends distributions constellation fig. empirical distribution error vector solution bro. empirical histograms shown averages realizations channel matrix noise vector number transmit antennas. compared asymptotic limiting distribution predicted theorem limiting density supported interval point masses different values shown. theorem main technical result paper. section iv-b show used prove theorem ii.. next section iv-c rely theorem prove error events ﬁxed number bits asymptotically independent. rest section devoted proof theorem iv.. sufﬁces prove ˆwi≤−} hand easily checked eγ∼n note indicator function {w≤−} lipschitz cannot directly apply theorem iv.. however since discontinuity point indicator function -measure zero also continuous random variable appropriately approximate indicator lipschitz functions conclude desired based theorem iv.. somewhat standard argument note deﬁnes random probability measure; hand deterministic measure. terminology standard theory random matrices sequence random measures converges weakly deterministic measure every remarks followed statement theorem section readily extended general m-pam constellations. guarantees theorem iii. hold long ratio transmit receive antennas larger thus successful transmission possible fewer number receive transmit antennas. minimum allowed ratio increases higher-order constellations. similar theorem show following simple upper bound probability error values section includes proof theorem ii.. fact towards proving theorem obtain general result stated theorem below. simplicity make following notation onwards. event holds probability approaching limn→∞ also denote convergence following shorthands probability; denote random variables distribution; denote n-dimensional euclidean norm. following theorem characterizes limit empirical distribution optimal solution yields theorem corollary. theorem recall deﬁnition theorem assume without loss generality consider empirical density function gaussian matrices number applications high-dimensional convex geometry idea combining convexity attributed stojnic thrampoulidis built signiﬁcantly extended idea arriving cgmt appears simplifying begin simplifying entries gaussian then vector wg−√ problem appears first since entries norm optimizing direction straightforward gives here obtain corollary theorem error events ﬁxed number bits asymptotically independent. defer proof corollary appendix corollary notation deﬁnition theorem bounded lipschitz functions ﬁxed then holds fundamental tool behind analysis convex gaussian min-max theorem cgmt associates primary optimization problem simpliﬁed auxiliary optimization problem tightly infer properties original optimal cost optimal solution etc.. particular problems deﬁned respectively follows rm×n rnsu denote optimal minimizers respectively. swsu convex compact sets continuous convex-concave entries standard normal. theorem arbitrary open subset sw/s. denote optimal cost optimization minimization constrained suppose exist constants limit holds w.p.a. φ+η. then limn→∞ hard argue conditions regarding optimal cost imply following solution w.p.a.. non-trivial powerful part theorem conclusion true optimal solution well. cgmt builds upon classical result gordon gordon’s original result classically used establish non-asymptotic probabilistic lower bounds minimum singular value next point-wise convergence implies uniform convergence thanks convexity. follows also known literature estimation theory convexity lemma point wise convergence convex functions implies uniform convergence compact subsets hence random optimization converges following deterministic optimization expanding square second summand applying integration parts objective function exactly deﬁned summands objective function non-negative thus consequently remark objective function strictly convex optimization variable known convex. alternatively check strict convexity shown second derivative positive.) hence unique minimizer call these takes standard argument conclude minimizer converges probability optimal solution tools necessary study properties optimal solution lemma establishes lipschitz functions lemma l-lipschitz random variable statement theorem iv.. holds gaussians. derived formulas previously unknown. also cgmt previously used analyze squarederror performance; here illustrate ﬁrst time analyze error-rate performance convex optimizationbased massive mimo decoders. future work seek extend analysis complex gaussian channels symbols originating complexvalued constellations. core task requires extending cgmt complex-valued gaussian matrices extension currently unavailable; thus poses challenging practically important research direction. appears accessible establishing universality results channels beyond gaussians. believe possible combining ideas paper extended cgmt techniques universality property proven squared-error bpsk signal recovery using proved corollary error events ﬁxed number bits solution iid. fact potentially signiﬁcant consequences explored. example implies that block data error bits are. means output used various local methods reduce ser. planning explore implications future work. c.-k. j.-c. chen k.-k. wong ting message passing algorithm distributed downlink regularized zeroforcing beamforming cooperative base stations wireless communications ieee transactions vol. narasimhan chockalingam channel hardeningexploiting message passing receiver large-scale mimo systems selected topics signal processing ieee journal vol. foschini layered space-time architecture wireless communication fading environment using multielement antennas bell labs technical journal vol. satisfying conditions cgmt following result uses lemma strong-convexity show optimal cost strictly increases optimization constrained outside deﬁned proof deferred appendix lemma l-lipschitz random variable statement theorem deﬁned finally denote objective function exists constant following statement holds w.p.a. completing proof last section showed conditions cgmt theorem satisﬁed. hence application yields minimizer satisﬁes w.p.a.. proves part theorem iv.. remains prove part recall note footnote sufﬁces prove continuous functions compact support. course statement true lipschitz continuous functions part theorem. continuous compactly supported functions also bounded. implication lipschitz bounded functions continuous bounded functions standard part known literature portmanteau theorem; example paper used recently developed cgmt framework precisely compute large-system error-rate performance popular box-relaxation optimization method recovering signals m-ary constellations channel matrix additive noise real w.-k. davidson wong z.-q. p.-c. ching quasi-maximum-likelihood multiuser detection using semi-deﬁnite relaxation application synchronous cdma signal processing ieee transactions vol. tanaka statistical-mechanics approach large-system analysis cdma multiuser detectors ieee transactions information theory vol. verd´u randomly spread cdma asymptotics statistical physics ieee transactions information theory vol. atitallah thrampoulidis kammoun al-naffouri hassibi m.-s. alouini analysis regularized least squares bpsk recovery ieee international conference acoustics speech signal processing ieee atitallah thrampoulidis kammoun alnaffouri m.-s. alouini hassibi box-lasso application gssk modulation massive mimo systems information theory ieee international symposium topics random matrix theory. american mathematical society vol. graduate studies mathematics. gordon some inequalities gaussian processes applications israel journal mathematics vol. proof theorem here prove ﬁrst part theorem namely lower upper bounds tightness upper bound high-snr shown later section decreasing nature function sufﬁces prove shown lemma below. proof lemma builds understanding behavior function function composed additive terms. ﬁrst linear second simply view remaining terms single function namely gather properties observe following. last term upper bounded using large enough snr. theorem values exist large enough nominator ﬁrst probability term upper bounded deterministic statement minimizer lemma proof. statement follows easily direct calculations. readily observed strictly greater proves statement last statement argue follows strictly increasing strict convexity thus sufﬁces compute limits satisﬁes proof. recall theorem function strictly convex. hence unique positive solution ﬁrst-order optimality condition convenient rest proof deﬁne function follows observe nonnegative long therefore repeat technical arguments section iv-e conclude random optimization converges following deterministic optimization identiﬁed function statement theorem. lemma second derivative strictly positive hence unique minimizer denote arguments section iv-e show first consider case then thresholding rule implies error ˜wi| equivalently view noting follows error occurs |hi| next consider case error event corresponds view translates putting together conditioning high-probability events proof theorem requires repeating mutatis mutandis line arguments detailed section proof theorem ii.. omit details brevity show necessary calculations yield function idea section thanks cgmt sufﬁces analyze corresponding auxiliary optimization instead original optimization repeating steps section iv-e corresponding becomes course similar equation next follow steps section iv-e study convergence ﬁrst summands fact third summand recall takes values equal probability denote", "year": "2017"}