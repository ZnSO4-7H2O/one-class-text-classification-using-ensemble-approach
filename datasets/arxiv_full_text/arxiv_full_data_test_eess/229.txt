{"title": "DeepJ: Style-Specific Music Generation", "tag": "eess", "abstract": " Recent advances in deep neural networks have enabled algorithms to compose music that is comparable to music composed by humans. However, few algorithms allow the user to generate music with tunable parameters. The ability to tune properties of generated music will yield more practical benefits for aiding artists, filmmakers, and composers in their creative tasks. In this paper, we introduce DeepJ - an end-to-end generative model that is capable of composing music conditioned on a specific mixture of composer styles. Our innovations include methods to learn musical style and music dynamics. We use our model to demonstrate a simple technique for controlling the style of generated music as a proof of concept. Evaluation of our model using human raters shows that we have improved over the Biaxial LSTM approach. ", "text": "abstract—recent advances deep neural networks enabled algorithms compose music comparable music composed humans. however algorithms allow user generate music tunable parameters. ability tune properties generated music yield practical beneﬁts aiding artists ﬁlmmakers composers creative tasks. paper introduce deepj end-to-end generative model capable composing music conditioned speciﬁc mixture composer styles. innovations include methods learn musical style music dynamics. model demonstrate simple technique controlling style generated music proof concept. evaluation model using human raters shows improved biaxial lstm approach. believe model capable generating music various styles yield practical beneﬁts ﬁlmmakers music composers need customize generated music creative tasks. example ﬁlmmaker wish match video music particular style convey desired emotion. method incorporating style serves proof concept idea. technique could extended tunable parameters generated music mood emotion. music composition challenging craft artists express ever since dawn civilization. designing algorithms produce humanlevel music intelligence difﬁcult rewarding challenge. recently advances neural networks helped transition writing music composition rules developing probabilistic models learn empirically-driven rules vast collection existing music. neural network music generation algorithms limited particular styles music jazz bach chorales music music generation methods created interesting compositions specialization particular style music undesirable practical applications. genre-agnostic methods composing music biaxial lstm able train using variety musical styles unable style-consistent outputs pivoting different styles within composition. paper introduce deepj deep learning model capable composing polyphonic music conditioned speciﬁc mixture multiple composer styles. main contributions work incorporation enforcing musical style model’s output learning music dynamics. context paper refer style either music genre particular composer’s style. data limitations paper focuses composers different classical eras believe monophonic music generation focuses task generating melody single tune without harmony. early work concert attempted generate melody estimating probability playing next note function previous notes. mozer pointed advantage recurrent networks restricted markovian predictions memory capacity simple backpropagation time practically limited. recent work improved using long-short term memory units make predictions based distal events. summary methods attempt learn melody composition modeling note play next probabilistically conditioned previously generated notes polyphonic music generation complex monophonic music generation. time step model needs predict probability combination notes played next time step. early work polyphonic composition involving neural networks attempted model sequences using combination rnns restricted boltzmann machines work demonstrated excellent results using generate distribution notes time step conditioned previous time step. recent architectures biaxial lstm describe type deep learning able model joint probability distribution notes transposition invariance data representation biaxial lstm architecture uses piano roll representation notes dense representation midi music. piano roll represents notes played time step binary vector represents note corresponding index played represents note played. piece music binary matrix number playable notes number time steps. following example representing notes held time steps followed time steps silence representation captures notes. representing note play insufﬁcient capturing midi actions difference holding note versus replaying note. note replay deﬁned event note re-attacked immediately note ends time steps successive plays. deﬁned replay matrix treplay similar tplay. note play replay jointly deﬁnes note representation. architecture biaxial lstm architecture models polyphonic music modeling note within time step probability conditioned previous time steps notes within current time step already generated. order generation ﬁrst time step last time step lowest note highest note length desired generation number possible notes. denote binary random variable note played otherwise. time step model learns following conditional probability n-th note expression above model must condition probability playing note along axes time note. model performs biaxial probability conditioning lstms take inputs along different axes respectively. hence biaxial lstm consists primary modules time-axis module note-axis module. input model notes previous time step. note octaves layer transforms note tensor note surrounding octave notes augmenting note spatial context. every note feature lstm shared weights across notes timeaxis. time-axis module takes note octaves recurrent states previous time step outputs higher level note features note. time-axis section inspired convolution neural network. consists layer stacked lstm units recurrent time connected note octave input fashion similar convolution kernel. weights lstm unit shared across note forcing time-axis section learn note invariant features. allows module generalize different transpositions. time axis section outputs features note computed parallel. taking note feature outputs time axis input note-axis lstm sweeps lowest note feature highest note feature make predictions note conditioned predicted lower notes. note-axis consists another layer stacked lstm recurrent note. note’s features ﬁrst concatenated lower chosen note. current note considered lowest note zeros concatenated. words determine chosen note i-th note’s features concatenated lower chosen note feeding note-axis module sample) denotes concatenation operator sample function samples sigmoid probability output using coin ﬂip. method providing previous chosen notes enables note-axis lstm learn probability estimations based lower chosen notes. used gives position time step relative measure binary format found contextual input necessary aiding model produce outputs consistent tempo. model different representation beat achieves effect describe following section. biaxial architecture created musically plausible results measure-level structure. build upon architecture introduce simple intuitive method learning enforcing musical style. also introduce learned music dynamics note’s representation. next subsections introduce data representation preprocessing step used train model present model architecture. deepj uses note representation biaxial architecture except augment representation music dynamics. contextual inputs described biaxial lstm’s paper improved method representing beat. dynamics music deﬁned relative volume note. midi ﬁles contain information loud note note event. keep track dynamics every note dynamics matrix that time step stores values note’s dynamics scaled denotes loudest possible volume. following example holding notes previous example time steps volume. preliminary work also tried alternate representation dynamics categorical value bins suggested wavenet instead predicting scalar value model would learn multinomial distribution note dynamics. would randomly sample dynamics generation multinomial distribution. contrary wavenet’s results experiments concluded scalar representation yielded results harmonious. collected dataset different music composers constitute particular artistic style. style music encoded one-hot representation artists. generation various styles changing vector representation style include desired styles normalize vector one. example representation composer composer mixture follows also encode several contextual inputs guide model create music better long term structure. feed model beat position current one-hot vector dimension equal number quantizations bar. note representation make assumptions time signature. using represent one-hot vector position every music cycles beat vectors chose capture fast notes without computationally expensive. deepj attempts generate music notes dynamics. time step note model produces three outputs play probability replay probability dynamics. train three outputs simultaneously. play replay treated logistic regression problems trained using binary cross entropy deﬁned biaxial lstm dynamics trained using mean squared error. thus introduce following loss functions experiments naive deﬁnition lreplay ldynamics yields poor results dynamics tend near zero replay probability. discovered better method mask loss ldynamics whenever note played thereby setting losses zero. note played replay dynamics never used generation thus unnecessary impose learning constraints values used. base model architecture biaxial lstm’s design. primary difference architecture biaxial lstm style conditioning every layer. musical styles necessarily orthogonal other. example classical baroque piece likely share many characteristics. hence believe representing style using learned distributed representation appropriate one-hot representation provided input. linear hidden layer represented linearly project one-hot style input style embedding initial experiments directly style embedding contextual input network. however discovered generation network tends ignore style embedding fails produce music faithful style. inspired conditioning technique wavenet introduce global conditioning biaxial architecture. lstm layer connect style embedding another fully-connected hidden layer tanh activation produce latent non-linear representation style indexes lstm layer. single latent representation inﬂuences notes lstm layers summation. order make layers’ dimensionality compatible summation broadcast latent style vector across notes. words i-th note input features lstm output note deﬁned also compared concatenation summation layer joining methods discovered summation provided stronger global effect network. addition also modiﬁed original biaxial architecture introducing minor improvement. used -dimensional convolution layer extract note features note’s octave neighborhood. found slightly improved training loss compared biaxial’s method directly feeding note’s neighboring octave lstm layers. experiments trained deepj composers three major classical periods using dataset original biaxial architecture trained training consists midi music composers ranging bach tchaikovsky. midi ﬁles come standard pitch range truncate range order reduce note input dimensionality. also quantized midi inputs resolution time steps bar. training performed using stochastic gradient descent nesterov adam optimizer speciﬁc hyperparameters used units lstms time-axis units lstms note axis. used dimensions represent style embedding space ﬁlters note octave convolution layer. used truncated backpropagation bars back time. dropout used regularizer deepj training. model uses dropout non-input layers dropout input layers. notice inclusion dropout training helps model recover mistakes makes generation becomes less dependent speciﬁc inputs. training generated samples genres music. performed generation sampling model’s probability distribution using coin determine whether play note not. deciding play note sample replay probability determine note re-attacked. dynamics level directly used model given note played. priming method generate music sometimes resulted model generating long initial periods silence. overcome problem implemented adaptive temperature adjustment method increase temperature output sigmoid functions proportional many time steps silence model produces number consecutive prior time steps silent outputs model produced. whenever model produces non-silent output time step reset prevented model outputting long periods silence. composers. participants given control samples given real samples. participants classiﬁed styles deepj outputs correctly time control samples classiﬁed correctly time statistical hypothesis test signiﬁcance level reveals statistical difference accuracy classiﬁcation deepj human composers. interesting observation participants found challenging classify control samples. suspect fact classical music sometimes share traits baroque romantic compositions making difﬁcult distinguish two. contrast deepj produces music forced composers particular genre together bringing average characteristics genre make discriminable. overall data demonstrates deepj produces music style approximately distinguishable composed humans. similar evaluation used sequence tutor goal experiment evaluate deepj’s music generation compared biaxial architecture’s generation. conducted user survey using amazon mechanical turk users asked listen pairs random samples outputs produced deepj biaxial choose preferred users preferred deepj’s compositions biaxial’s user preferences selected deepj shows deepj produces better sounding music biaxial. demonstrates methods learning music dynamics style effective improving quality generated music. volume adds level emotion music style makes output stylistically consistent preventing changing style middle piece. evaluate style diversity deepj surveyed individuals musical backgrounds asked classify music generated deepj baroque classical romantic. goal experiment evaluate extent deepj create stylistically distinct music testing humans identify genres. study participants given music samples generated deepj real fig. style embedding t-sne visualization perplexity learning rate iterations. blue yellow dots baroque classical romantic composers respectively. analyze capacity model learn style visualizing style embedding space using t-sne notice composers similar classical periods tend cluster together. t-sne visualization baroque composers cluster left classical composer cluster left romantic composers dominate right. clustering behavior indicates deepj learned similarity differences composers. interesting result note beethoven labeled classical composer falls cluster centers classical romantic composers. consistent observation known composer represents transition classical romantic eras. generated samples produced deepj also demonstrate model learned artistic style. discovered samples produced baroque conditioning exhibited counterpoint polyphonic characteristics similar bach’s compositions. similarly also noticed output conditioned classical music demonstrates less complexity homophony compared baroque outputs. output conditioned romantic period music generally freedom rhythm mozer neural network music composition prediction exploring beneﬁts psychoacoustic constraints multi-scale processing connect. sci. vol. available http//dx.doi.org/./ boulanger-lewandowski bengio vincent modeling temporal dependencies high-dimensional sequences application polyphonic music generation transcription proceedings international conference machine learning icml edinburgh scotland june july available http//icml.cc//papers/.pdf sutskever salakhutdinov simple prevent neural networks overﬁtting journal machine learning research vol. available http //dl.acm.org/citation.cfm?id= jaques turner sequence tutor conservative ﬁne-tuning sequence generation models klcontrol proceedings international conference machine learning vol. abs/. available http//arxiv.org/abs/. fig. example baroque output deepj exhibiting fugue characteristics. second introduces melody line left hand imitating existing right hand melody. interesting result found addition dynamics made output sound similar human playing also improved qualitative output model. believe dynamics provide model additional contextual information make prediction emotional qualities composer attempted convey dynamics. another hypothesis dynamics served multitask teaching signal trains model harder task leads better performance furthermore also notice model learns dynamics particular styles. similar training data baroque music generated deepj tends constant dynamics compared varying dynamics classical romantic outputs. introduced model goal learning musical style using deep neural networks successfully demonstrated method using distributed representation style inﬂuence model generate music given mixture artist styles. improved biaxial model adding volume style turn improved overall quality generated music. model also solved style consistency problems present biaxial architecture. however lack long term structure central theme generated music still problem solved. would interesting combine reinforcement learning methods models sequence tutor exploring adversarial methods train models better long term structure. addition developing sparse representation music preferred representation used biaxial expensive train. parallel networks correia ciesielski liapis computational intelligence music sound design. evomusart available https//link.springer.com/chapter/./ ----", "year": "2018"}