{"title": "WSNet: Compact and Efficient Networks Through Weight Sampling", "tag": "eess", "abstract": " We present a new approach and a novel architecture, termed WSNet, for learning compact and efficient deep neural networks. Existing approaches conventionally learn full model parameters independently and then compress them via ad hoc processing such as model pruning or filter factorization. Alternatively, WSNet proposes learning model parameters by sampling from a compact set of learnable parameters, which naturally enforces {parameter sharing} throughout the learning process. We demonstrate that such a novel weight sampling approach (and induced WSNet) promotes both weights and computation sharing favorably. By employing this method, we can more efficiently learn much smaller networks with competitive performance compared to baseline networks with equal numbers of convolution filters. Specifically, we consider learning compact and efficient 1D convolutional neural networks for audio classification. Extensive experiments on multiple audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are up to 180 times smaller and theoretically up to 16 times faster than the well-established baselines, without noticeable performance drop. ", "text": "present approach novel architecture termed wsnet learning compact efﬁcient deep neural networks. existing approaches conventionally learn full model parameters independently compress processing model pruning ﬁlter factorization. alternatively wsnet proposes learning model parameters sampling compact learnable parameters naturally enforces parameter sharing throughout learning process. demonstrate novel weight sampling approach promotes weights computation sharing favorably. employing method efﬁciently learn much smaller networks competitive performance compared baseline networks equal numbers convolution ﬁlters. speciﬁcally consider learning compact efﬁcient convolutional neural networks audio classiﬁcation. extensive experiments multiple audio classiﬁcation datasets verify effectiveness wsnet. combined weight quantization resulted models smaller theoretically faster well-established baselines without noticeable performance drop. introduction despite remarkable successes various applications deep neural networks usually suffer following problems stem inherent huge parameter space. first state-of-the-art deep architectures prone over-ﬁtting even trained large datasets secondly dnns usually consume large amount storage memory energy makes difﬁcult national university singapore singapore snap inc. research angeles bytedance inc. menlo park microsoft research redmond institute beijing china. correspondence xiaojie <xjjingmail.com>. devices limited memory power different existing works model compression acceleration ignore strong dependencies among weights learn ﬁlters independently based existing network architectures paper proposes explicitly enforce parameter sharing among ﬁlters effectively learn compact efﬁcient deep networks. paper propose weight sampling deep neural network signiﬁcantly reduce model size computation cost achieving smaller size speedup negligible performance drop even achieving better performance baseline speciﬁcally wsnet parameterized layerwise condensed ﬁlters ﬁlter participating actual convolutions directly sampled spatial channel dimensions. since condensed ﬁlters significantly fewer parameters independently trained ﬁlters conventional cnns learning sampling makes wsnet compact model compared conventional cnns. addition reduce ubiquitous computational redundancy convolving overlapped ﬁlters input patches propose integral image based method dramatically reduce computation cost wsnet training inference. integral image method also advantageous enables weight sampling different ﬁlter size minimizes computational overhead enhance learning capability wsnet. order demonstrate efﬁcacy wsnet conduct extensive experiments challenging audio classiﬁcation tasks. test dataset including esc- urbansoundk dcase musicdetk wsnet signiﬁcantly reduces model size baseline comparable even higher classiﬁcation accuracy. compressing wsnet subject negligible accuracy drop. time wsnet signiﬁcantly reduces computation cost results strongly establish capability wsnet learn compact efﬁcient networks. last least provide intuitive method extend wsnet cnns recent works network compression adopt weight pruning ﬁlter decomposition hashed networks weight quantization however although works reduce model size also suffer large performance drop. bucilu caruana based student-teacher approches difﬁcult apply tasks since require training teacher network advance. denil predicts parameters based number weight values. proposes iterative hard thresholding method achieve relatively small compression ratios. gong uses binning method applied fully connected layers. hinton compresses deep models transferring knowledge pre-trained larger networks smaller networks. terms deep model acceleration factorization quantization methods listed reduce computation latency inference. lcnn also used speed computation pratice. comparatively wsnet superior learns networks smaller model size faster computation versus baselines. wsnet presents class novel models appealing properties small model size small computation cost. recently proposed efﬁcient model architectures include class inception models class residual models factorized networks fully factorized convolutions. mobilenet flattened networks based factorization convolutions. shufﬂenet uses group convolution channel shufﬂe reduce computational cost. compared works wsnet presents model design strategy ﬂexible generalizable parameters deep networks obtained conveniently compact representation proposed weight sampling method. audio classiﬁcation aims classify surrounding environment audio stream generated given audio input compared based methods pre-computed features e.g. mfcc spectrogram recently proposed soundnet yields signiﬁcant state-of-the-art results directly taking dimensional wave signals input. paper demonstrate proposed wsnet achieves comparable even better performance soundnet signiﬁcantly smaller size faster speed. spatial length input channel input number ﬁlters respectively. assume output spatial size input holds true using zero padded convolution. convolution kernel used actual convolution wsnet shape kernel size. {··· denotes ﬁlter {··· denotes input patch spatially spans convolution assuming stride zero padding computed wsnet instead learning weight independently obtained sampling learned condensed ﬁlter shape goal training wsnet thus cast learn compact dnns satisfy condition l∗m∗ wsnet uses condensed ﬁlter convolutional layer. quantize advantage wsnet achieving compact networks deﬁne compactness learned layer wsnet w.r.t. conventional layer independently learned weights order seek compact networks without limitation propose channel sharing strategy wsnet learn weight sampling along channel dimension. illustrated figure actual ﬁlter used convolution generated repeating sampling times. relation channels ﬁlters channel sampling therefore compactness wsnet along channel dimension achieves introduced later experiments observe repeated weight sampling along channel dimension signiﬁcantly reduces model size wsnet without signiﬁcant performance drop. notable advantage channel sharing maximum compactness large paves learning much aggressively smaller models attribute effectiveness channel sharing reducing redundancy along channel dimension especially layers. general architecture design number ﬁlter channels grows linearly layer depth. however spatial size kernels becomes smaller remains unchanged. implies redundancy higher layers mainly come channel dimension. analysis weight sampling along spatial/channel dimensions conveniently generalized convolution layers fully connected layers. fully connected layer treat weights ﬂattened vector channel along spatial sampling performed reduce size learnable parameters. details please refer supplementary material. wsnet trained scratch similar conventional deep convolutional networks using standard error back-propagation. since every weight klmn convolutional kernel sampled condensed ﬁlter along spatial channel dimension difference gradient summation gradients weights tied therefore simply recording position mapping tied weights gradient calculated conventional cross-entropy loss function. open-sourced machine learning libraries represent computation graphs tensorflow equation calculated automatically. figure illustration wsnet learns small condensed ﬁlters weight sampling along dimensions spatial dimension channel dimension ﬁgure depicts procedure generating continuous ﬁlters convolve input. spatial sampling ﬁlters extracted condensed ﬁlter stride channel sampling channel ﬁlter sampled repeatedly times achieve equal input channel. please refer section detailed explanations. ﬁgures paper best viewed zoomed-in pdf. conventional cnns ﬁlters layer learned independently presents disadvantages. firstly resulted dnns large number parameters impedes deployment computation resource constrained platforms. second over-parameterization makes network prone overﬁtting getting stuck local minima. solve problems novel weight sampling method proposed efﬁciently reuse weights among ﬁlters. speciﬁcally convolutional layer wsnet convolutional ﬁlters sampled condensed ﬁlter illustrated figure scanning weight sharing ﬁlter window size stride could sample ﬁlters ﬁlter size formally equation ﬁlter size condensed ﬁlter sampled ﬁlters although experimentally veriﬁed weight sampling strategy could learn compact deep models negligible loss classiﬁcation accuracy maximal compactness limited ﬁlter size mentioned section inner product column convolution result ﬁlter sampled input patch summation values segment since repeated calculations ﬁlter input patch overlapped e.g. green segment indicated arrow performing convolution construct integral image using according based convolutional results sampled ﬁlter input patch retrieved directly time complexity according e.g. results notation deﬁnitions please refer sec. comparisons computation costs wsnet baselines using conventional architectures introduced section performance wsnet might adversely affected size condensed ﬁlter decreased aggressively enhance learning capability wsnet could sample ﬁlters condensed ﬁlter. speciﬁcally smaller sampling stride performing spatial sampling. order keep shape weights unchanged following layer append convolution layer shape reduce channels densely sampled ﬁlters. experimentally veriﬁed denser weight sampling effectively improve performance wsnet section however since also brings extra parameters computational cost wsnet denser weight sampling used lower layers wsnet whose ﬁlter number small. besides also conduct channel sampling added convolution layers reduce sizes. figure variant integral image method used practice efﬁcient illustrated figure instead repeatedly sampling along channel dimension convolve input wrap channels summing matrixes evenly divided along channels i.e. since channle channel overall computation cost reduced demonstrated conventional incurs severe computational redundancies. concretely seen item ouput feature equal summation inner products vector column vector therefore overlapped ﬁlters sampled condensed ﬁlter convolves overlapped input windows partially repeated calculations exist eliminate redundancy convolution speed-up wsnet propose novel integral image method enable efﬁcient computation sharing computations. paper focus wsnet convnets. comprehensive experiments clearly demonstrate advantages learning compact computation-efﬁcient networks. note wsnet general also applied build convnets. convnets ﬁlter three dimensions including spatial dimensions channel dimension. straightforward extension wsnet convnets follows spatial sampling ﬁlter sampled patch condensed ﬁlter. channel sampling remains convnets i.e. repeat sampling channel dimension condensed ﬁlter. following notations wsnet convnets denote ﬁlters sampling strides along spatial dimensions compactness wsnet along channel dimension. compactnesses deniﬁnition) wsnet along spatial channel dimension respectively. however straightforward extension wsnet convnets optimum believe sophisticated effective methods applying wsnet convnets would like explore future work. nevertheless conduct preliminary experiments convents using intuitive extension verify effectiveness wsnet image classiﬁcation tasks datasets baseline networks collect large-scale music detection dataset publicly available platforms conducting experiments. fair comparison previous literatures also test wsnet three standard publicly available datasets esc- urbansoundk dcase. space limit please refer details used datasets supplementary material. rt×n thus takes times calculating omit case padding clear description. zero padding applied freely convolutional results padded areas even without using since based computation cost proposed integral image method practice adopt variant method boost computation efﬁciency wsnet illustrated repeat times along channel dimension make equal channel input however could ﬁrst wrap channels accumulating values interval along ﬁrst item computational cost warping channels obtain since dominating term smaller overall computation cost thus largely reduced. combining theoretical acceleration compared baseline finally note integral image method applied wsnet naturally takes advantage property weight sampling redundant computations exist overlapped ﬁlters input patches. different deep model speedup methods require solve time-consuming optimization problems incur performance drop integral image method seamlessly embeded wsnet without negatively affecting ﬁnal performance. table baseline- conﬁgurations baseline network used musicdetk. convolutional layer followed nonlinearity layer batch normalization layer pooling layer omitted table brevity. strides pooling layers padding strategies adopted convolutional layers fully connected layers size preserving. table baseline- conﬁguration baseline network used esc- urbansoundk dcase. baseline adapted soundnet applying pooling layers last convolutional layer. brevity nonlinearity layer batch normalization layer pooling layer following convolutional layer omitted. kernel sizes pooling layers following conv-conv conv-conv respectively. stride every pooling layers table ablative study effects different settings wsnet model size computation cost classiﬁcation accuracy esc-. clear description name wsnets different settings combination symbols s/c/d/q. denotes weight sampling along spatial dimension; denotes weight sampling along channel dimension. denotes denser ﬁlter sampling. denotes weight quantization. numbers subscripts s/c/d/q denotes maximum compactness spatial/channel dimension layers ratio number ﬁlters wsnet versus baseline ratio wsnet’s size weight quantization respectively. model size computational cost provided baseline. model size mult-adds wsnet provide ratio baseline’s model size versus wsnet’s model size ratio baseline’s mult-adds versus wsnet’s mult-adds. evaluation criteria demonstrate wsnet capable learning compact efﬁcient models conventional cnns three evaluation criteria used experiments model size number multiply adds calculation classiﬁcation accuracy. results wsnet models also give different runs. implementation details wsnet implemented trained scratch tensorﬂow following adam optimizer ﬁxed learning rate momentum term batch size used throughout experiments. initialized weights zero mean gaussian noise standard deviation network used musicdetk dropout ratio dropout layers fully connected layer overall training takes iterations. ablation analysis investigate effects component wsnet model size computational cost classiﬁcation accuracy. comparative study results different settings wsnet listed table clear description name wsnets different settings combination symbols s/c/d/q. please refer caption table detailed meanings. spatial sampling. test performance wsnet using different sampling stride spatial sampling. listed table slightly outperforms classiﬁcation accuracy baseline possibly reducing overﬁtting models. sampling stride i.e. compactness spatial dimension classiﬁcation accuracy drops note maximum compactness along spatial dimension equal ﬁlter size thus layer conv{-} ﬁlter sizes compactnesses limited results clearly demonstrate spatial sampling enables wsnet learn signiﬁcantly smaller model comparable accuracies w.r.t. baseline. channel sampling. three different compactness along channel dimension i.e. tested comparing baslines. observed table linearly reduced model size without incurring noticeable drop accuracy. fact even improve accuracy upon baselines demonstrating effectiveness channel sampling wsnet. learning compact models demonstrates better performance compared compactness spatial dimension suggests focus channel sampling compactness along spatial dimension high. simultaneously perform weight sampling spatial channel dimensions. demonstrated results wsnet learn highly compact models without signiﬁcant performance drop denser weight sampling. denser weight sampling used enhance learning capability wsnet aggressive compactness make performance loss caused sharing much parameters among ﬁlters. shown table sampling ﬁlters conv{-} signiﬁcantly outperforms results demonstrate effectiveness denser weight sampling boost performance. integral image efﬁcient computation. evidenced last column table proposed integral image method consistently reduces computation cost wsnet. smaller baseline computation cost signiﬁcantly reduced times. extra computation cost brought convolution denser ﬁlter sampling achieves lower acceleration group convolution used alleviate computation cost added convolution layers. explore direction future work. weight quantization. observed table using bins represent weight byte scdq scdq much smaller model size compared baselines incurring negligible accuracy loss. result demonstrates weight quantization complementary wsnet used jointly effectively reduce model size wsnet. please ref. supplementary material details weight quantization methods. wsnet versus narrowed baselines. verify wsnet’s capacity learning compact models compare wsnet baselines compressed intuitive i.e. reducing number ﬁlters layer. ﬁlters layer reduced overall parameters baselines reduced figure plot baseline accuracy varies respect different compression ratios accuracies wsnet model size compressed baselines. shown figure wsnet outperforms baselines large margin across compression ratios. particularly compression ratios large baselines suffer severe performance drop. contrast wstable comparison state-of-the-arts using cnns esc-. results wsnet obtained -folder validation. please refer table meaning symbols s/c/d/q. soundnet∗ extra training data methods provided training data. table test error rates wsnet hashnet cifar mnist. baselines used mnist/cifar simple -layer fully connected network -layer convolutional network respectively. model size provided baseline. model size wsnet/hashnet provide ratio baseline’s model size versus model size wsnet/hashnet. wsnet layer-wise compactness speciﬁcally convolutional layer wsnet compactness along spatial/channel dimension √n/√n respectively. value. wsnet goes overcome limitations existing quantization methods capturing common correlations among learned ﬁlters. example ﬁlters ﬁrst layer soundnet learn similar constituent patterns e.g. descending/ascending slope lines. proposed weight sampling method enables wsnet learn shared patterns explicitly sampling ﬁlters condensed ﬁlter overlapping. time non-overlapped parts sampled ﬁlters able learn different features endows wsnet strong learning capabilities. main reason wsnet learn much smaller networks without noticeable performance drop compared baselines. moreover sampled ﬁlters overlapped could integral image based method speed wsnets wsnet able learn smaller faster networks effectively. achieves comparable accuracies full-size baselines clearly demonstrates effectiveness weight sampling methods proposed wsnet. supplementary material also present comparison wsnet narrowed baselines musicdetk. comparison wsnet state-of-the-arts esc- listed table compared soundnet trained provided data wsnets signiﬁcantly outperform classiﬁcation accuracy smaller models. pre-training using large number unlabeled videos soundnet∗ achieves better accuracy wsnet. however since unsupervised pre-training method orthogonal wsnet believe wsnet achieve better performance training similar soundnet large amount unlabeled video data. space limit experimental results datasets well ablative study musicdetk please refer supplementary material. argue reasons success wsnet epitome methods successfully deployed sparse coding literatures coding dictionaries formed overlapping patches epitome free parameters. indicates effective representations complex signals generated low-dimensional space thus motivates learn compact ﬁlters deep neural networks i.e. ﬁlters participate actual convolution generated condensed ﬁlters. weight quantization techniques successfully applied compressing deep models multiple weights encoded since wsnet hashnet explore weights tying compare mnist cifar. fair comparison baselines used hyperparameters training follow dataset hold training samples form validation set. comparison results wsnet hashnet mnist/cifar listed table observe learning networks sizes wsnet achieves signiﬁcantly lower error rates hashnet datasets. results clearly demonstrate advantages wsnet learning compact models. furthermore also conduct experiment cifar state-of-the-art resnet baseline. resnet achieves top- accuracy params wsnet experimental settings follow wsnet able achieve smaller model size slight performance drop promising results demonstrate effectiveness wsnet. conclusion paper present class weight sampling networks highly compact efﬁcient. novel weight sampling method proposed sample ﬁlters condensed ﬁlters much smaller independently trained ﬁlters conventional networks. weight sampling conducted dimensions condensed ﬁlters i.e. spatial sampling channel sampling. taking advantage overlapping property ﬁlters wsnet propose integral image method efﬁcient computation. extensive experiments four audio classiﬁcation datasets including musicdetk urbansoundk dcase clearly demonstrate wsnet learn compact efﬁcient networks competitive performance. references abadi mart´ın agarwal ashish barham paul brevdo eugene chen zhifeng citro craig corrado greg davis andy dean jeffrey devin matthieu tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. barchiesi daniele giannoulis dimitrios stowell plumbley mark acoustic scene classiﬁcation classifying environments sounds produce. ieee signal processing magazine hanjalic alan zhang hong-jiang lian-hong. ﬂexible framework audio effects detection auditory context inference. ieee transactions audio speech language processing davis steven mermelstein paul. comparison parametric representation monosyllabic word recognition continuously spoken sentences. ieee trans. assp aug. denton emily zaremba wojciech bruna joan lecun yann fergus rob. exploiting linear structure within convolutional networks efﬁcient evaluation. nips srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. jmlr stowell giannoulis dimitrios benetos emmanouil lagrange mathieu plumbley mark detection classiﬁcation acoustic scenes events. ieee transactions multimedia szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. cvpr howard andrew menglong chen kalenichenko dmitry wang weijun weyand tobias andreetto marco adam hartwig. mobilenets efﬁcient convolutional neural networks mobile vision applications. arxiv preprint arxiv. yong-deok park eunhyeok sungjoo choi taelim yang shin dongjun. compression deep convolutional neural networks fast power mobile applications. arxiv preprint arxiv. lebedev vadim ganin yaroslav rakhuba maksim oseledets ivan lempitsky victor. speeding-up convolutional neural networks using ﬁne-tuned cp-decomposition. arxiv preprint arxiv.", "year": "2017"}