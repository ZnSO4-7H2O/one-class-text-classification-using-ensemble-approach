{"title": "SLAC: A Sparsely Labeled Dataset for Action Classification and  Localization", "tag": ["cs.CV", "cs.AI"], "abstract": "This paper describes a procedure for the creation of large-scale video datasets for action classification and localization from unconstrained, realistic web data. The scalability of the proposed procedure is demonstrated by building a novel video benchmark, named SLAC (Sparsely Labeled ACtions), consisting of over 520K untrimmed videos and 1.75M clip annotations spanning 200 action categories. Using our proposed framework, annotating a clip takes merely 8.8 seconds on average. This represents a saving in labeling time of over 95% compared to the traditional procedure of manual trimming and localization of actions. Our approach dramatically reduces the amount of human labeling by automatically identifying hard clips, i.e., clips that contain coherent actions but lead to prediction disagreement between action classifiers. A human annotator can disambiguate whether such a clip truly contains the hypothesized action in a handful of seconds, thus generating labels for highly informative samples at little cost. We show that our large-scale dataset can be used to effectively pre-train action recognition models, significantly improving final metrics on smaller-scale benchmarks after fine-tuning. On Kinetics, UCF-101 and HMDB-51, models pre-trained on SLAC outperform baselines trained from scratch, by 2.0%, 20.1% and 35.4% in top-1 accuracy, respectively when RGB input is used. Furthermore, we introduce a simple procedure that leverages the sparse labels in SLAC to pre-train action localization models. On THUMOS14 and ActivityNet-v1.3, our localization model improves the mAP of baseline model by 8.6% and 2.5%, respectively.", "text": "enabled successful application progressively sophisticated learning models. image categorization moved caltech introduced contained examples imagenet dataset introduced includes examples. object detection seen similar trend scaling-up training sizes despite additional human annotation cost involved collecting bounding-box information. pascal ﬁrst released contained examples recently introduced coco dataset consists images object instance annotations. video domain scale datasets action classiﬁcation action localization widening. action classiﬁcation datasets created years consisted thousands examples hollywood recent benchmarks scaleddataset sizes orders magnitude contains videos kinetics videos). seen similar growth datasets action localization. thumos created contained trimmed videos untrimmed videos localization annotations. today largest benchmarks action localization moderately larger. example activitynet includes videos annotations includes clips charade contains temporally localized intervals. table give detailed comparisons different video datasets. action localization datasets still much smaller object detection? action localization datasets still order magnitude smaller action categorization? paper forward hypothesis primarily reasons. first gathering temporal annotations video time-consuming. according experiments professional annotators manually annotating start action segments video takes times length video. order give accurate temporal annotations annotators need watch entire paper describes procedure creation large-scale video datasets action classiﬁcation localization unconstrained realistic data. scalability proposed procedure demonstrated building novel video benchmark named slac consisting untrimmed videos clip annotations spanning action categories. using proposed framework annotating clip takes merely seconds average. represents saving labeling time compared traditional procedure manual trimming localization actions. approach dramatically reduces amount human labeling automatically identifying hard clips i.e. clips contain coherent actions lead prediction disagreement action classiﬁers. human annotator disambiguate whether clip truly contains hypothesized action handful seconds thus generating labels highly informative samples little cost. show large-scale dataset used effectively pretrain action recognition models signiﬁcantly improving ﬁnal metrics smaller-scale benchmarks ﬁne-tuning. kinetics ucf- hmdb- models pre-trained slac outperform baselines trained scratch top- accuracy respectively input used. furthermore introduce simple procedure leverages sparse labels slac pre-train action localization models. thumos activitynet-v. localization model improves baseline model respectively. ﬁelds image categorization object detection witnessed parallel outstanding progress recent years arguably advances result steady improvements growths datasets table comparisons slac video datasets. note annotations sportsm produced automatically analyzing text metadata surrounding videos thus inaccurate. sequence also replay several parts video back forth exact boundaries. second marking temporal action boundaries often ambiguous. object boundaries deﬁned physical extents action boundaries frequently blurry result smooth continuity human movements poor deﬁnition constitutes action. work propose recast temporal annotation task form efﬁcient less ambiguous. idea sample small number short clips video annotators review. active learning algorithm used select easy clip several hard clips label. annotator asked simply conﬁrm whether clips truly contain hypothesized action. experiments suggest providing binary yes/no answer easy fast annotators. induces saving labeling time compared traditional procedure exhaustive review entire video manual marking action boundaries. limited amount human intervention allows build large-scale datasets high-quality consistent annotations. procedure gives rise sparse annotated clips demonstrate models supervised annotations superior generalization performance action classiﬁcation localization tasks. action classiﬁcation large-scale dataset leveraged pre-train video models. show ﬁne-tuning models well-established action classiﬁcation benchmarks yields signiﬁcant gains learning scratch. kinetics hmdb able improve resnet-based convnet baseline respectively. also show pre-training slac beneﬁcial pre-training sportsm kinetics. sports-m annotations generated prediction algorithm inevitably injects prominent noise dataset. also average length sportsm videos minutes predicted action happen short period time whole video. poses substantial difﬁculties learning good video representations action classiﬁcation discussed section compared kinetics slac contains nearly times clip annotations explain superior generalization performance deep models trained benchmark. finally demonstrate sparse clip annotations slac also used pre-train action localization models give dense predictions frame. thumos challenge activitynet-v. datasets able improve baseline models map. rest paper organized follows. review past work video dataset collection section slac dataset collection procedure elaborated section evaluate merits slac dataset action classiﬁcation section action localization section conclude work section motivation behind work creation annotated video collection serve instrumental training learning general spatiotemporal features i.e. features successfully transferred action recognition detection domains ﬁne-tuned effectively smaller benchmarks. last decade several video datasets created similar reasons. example action recognition hmdb datasets introduced provide benchmarks higher variety actions realistic capture conditions compared precedent datasets benchmarks indeed served community well enabling handdesign effective robust features action recognition space-time interest points dense trajectories however datasets large enough support modern end-to-end training deep models. large-scale kinetics dataset recently introduced shown enable pre-training deep models ﬁne-tuned achieve state-of-theart performance smaller action recognition benchmarks hmdb ucf. although kinetics represents remarkable resource training deep action classiﬁers learning general video features cannot used learning action detection models consists trimmed videos. objective introduce benchmark enable learning action recognition models also pre-training deep models action localization untrimmed videos. action detection unconstrained videos crucial automatically understand internet videos typically several minutes long include multiple actions several people interacting. recently several datasets untrimmed video analysis presented. thumos challenge includes trimmed training videos actions. subsequently extended multithumos contains action classes temporal frame-level annotations. shortcoming thumos multithumos include examples sport activities. datasets granularity classes focused narrow domains include mpii cooking mpii cooking depict human subjects preparing dishes. models trained domain-focused videos generalize optimally recognition every-day activities. conversely charades dataset purposefully designed include general daily activities. since uneventful activities rarely shown videos shared internet authors asked people everyday routine activities homes front camera previously done activities daily living dataset activitynetv. includes hours untrimmed videos temporal annotations actions spanning many common activities. finally recently introduced dataset provides person-centric annotations action annotations spatiotemporal corresponding person performing atomic action shaking hands kicking object. common trait aforementioned datasets action detection localization training videos obtained exhaustive manual search individual actions video sequence. produce accurate temporal localizations annotator must typically stop slow down play back forth video multiple times. renders process annotation time consuming. hampered scaling size benchmarks action detection. approach bypasses manual annotation temporal extents automatically identifying hard clips hypothesized informative easy clips. furthermore since labeling clips posed form simple yes/no questions annotators provide labels efﬁciently consistently without need stop replay video. approach admittedly downsides. ﬁrst annotation pipeline yields sparse labeling clips clips video remain unannotated thus usable supervised training. second disadvantage that since manual labels sought clips deemed relevant method labeled dataset inherently biased rather uniform sampling data source. demonstrate empirically beneﬁts approach outweigh shortcomings. speciﬁcally propose straightforward procedure pre-training action models sparse localization labels show improves model performances considerably ﬁne-tuning fully-labeled data compared learning scratch pre-training datasets. furthermore show dataset bias small practice transfer learning experiments four different benchmarks action classiﬁcation localization approach yields consistent metric gains. slac dataset includes action classes taken activitynet-v. dataset. total videos retrieved youtube. video strictly shorter minutes average length minutes. -second clips sampled novel active sampling approach clips annotated positive negative samples respectively team professional annotators working hours day. overall annotation takes effectively working days complete including hour training annotators. split slac training validation testing videos respectively. dataset webpage hosted http//slac.csail.mit.edu. overview data collection pipeline shown figure next elaborate detail collection procedure. collecting large-scale action dataset challenging several reasons. first compared negative examples human action number positive examples arguably much smaller. uniformly sample video/clips annotate large number negative examples less useful positive examples training video models. second negative examples equally useful. hypothesize hard negative address ﬁrst challenge sample positive examples annotation bias sampling distribution towards videos likely contain actions. achieved leveraging video search engine retrieve videos actions interest person detectors exclude videos contain humans general three types negative samples human action datasets increasing levels mining difﬁculty person. since interested human actions negative videos without human presence excluded annotation much possible. context. another common type negative samples represented videos contain people lack context ongoing action. instance objective collect videos action archery exclude videos arrows bows present videos. action. hardest negative examples videos include people context action interest performed. identify hard negative examples sending annotators validation. concrete examples prototypical negative samples shown figure archery action localize. examples person animations video. examples context clips people present clips related archery. example action represented person table de-duplication slac within itself test/validation sets prior video datasets. starting initial videos show number duplicate videos removed step. based taxonomy activitynet-v. human actions deﬁned retrieve candidate videos youtube using class label query videos matching titles descriptions. internal de-duplication. de-duplicate videos within slac since youtube include several copies video possibly differing post-processing steps saturation/contrast enhancement duration. speciﬁcally resize video short edge size aspect ratio preserved evenly sample types negatives hypothesize context clips easier recognize less useful action clips. unfortunately types videos returned youtube video description matching class label. want bias distribution clip sampling human annotation towards action hard negative samples away context easy negative samples. achieve this trained image classiﬁers used sample clips. youtube frame-based model. ﬁrst model trained frames extracted videos retrieved youtube action class. videos frames person detected used positive example training. sums total frames also randomly choose frames person score examples background class. google image-based model. second model trained images retrieved google images using class labels queries. strategy yields total images thresholding person detection. also class labels imagenet keywords retrieve examples google images assign images background class. measure consensus classiﬁers bias distribution sampling clips annotations towards action samples. active learning consensus method known query-by-committee first video randomly sample frame shot video. denote frames representing candidate clips annotation video. next want deﬁne sampling distribution order sample frames informative annotation. deﬁne unnormalized sampling probability follows -dimensional predicted probabilities youtube frame classiﬁers google image classiﬁer respectively. denotes action label used retrieve video. therefore represents consensus ﬁrst term choose ensuring manual inspection thresholds removes duplicate videos large subset video pairs high similarity scores. de-duplication step remove videos. external de-duplication. perform additional deduplication ensure slac overlap validation testing sets video datasets namely kinetics activitynet hmdb. taxonomy slac datasets different action class slac ﬁrst manually compile list similar action classes {bi}i target dataset. example selected actions swimming backstroke swimming breast stroke swimming butterﬂy stroke kinetics classes contain duplicates slac videos action swimming. compute similarity between slac video class target video actions {bi}i. remove video slac similarity score video target dataset threshold. sequentially de-duplicate slac respect kinetics activitynet hmdb remove videos slac. step person detector remove ﬁrst kind negative samples videos contain people. reduce computation shot detection performed beforehand. compute dissimilarity score color histograms consecutive frames threshold score gradient shot boundary. shot frames sampled person score computed. faster r-cnn trained coco person class maximum score person present frames. threshold person score used shots people shots initial proposals human actions used processing. verify accuracy procedure test person detector validation activitynet-v. recall shots video scores threshold remove video. removes videos. term divergence represents disagreement class probabilities image classiﬁers. third term computes average similarity frames frame similarity assessed cosine similarity conv feature activation extracted imagenet-trained resnet- model. third term biases sampling towards representative frames video. compute {r}x frame candidates perform normalization obtain proper sampling distribution. sample frames according distribution retain samples ﬁrst term greater chosen addition also sample extra frame corresponding frame highest value ﬁrst term sample viewed easy positive example. select samples hypothesize small number easy positive examples useful model training well. sampling video frames take second either side frame convert -second clip. also ensure clips cross shot boundaries. result total clips sampled videos. video annotation task aimed labeling clips -second duration clear consistent manner. annotation guideline prepare detailed annotation guideline providing clear action deﬁnitions. text descriptions positive/negative clip examples clariﬁcations provided annotators reduce ambiguity action annotation. illustration guideline shown figure annotation tool. annotation tool visualized figure clips organized grid layout automatically played. annotators quickly mark positive/negative samples clicking clips. border color visualizes current annotations positive clips figure annotation tool. user quickly border color clips mark positive/negative example simple clicking. action annotated kayaking. space constrain show clips here. annotation quality control. ensure high annotation quality ﬁrst ensure clips action label labeled annotator. removes interannotator variance. second personally labeled clips kicking annotation. refer clips golden mixed clips annotated. clips golden compute annotation accuracy annotator comparing labels provided given annotators. monitor accuracy iteratively give customized clariﬁcations annotator accuracy. ensure annotators accuracy. ﬁnal average accuracy annotation cost. hire professional annotators working hours day. annotation task ﬁnished working days. last days several annotators already completed annotations passed accuracy check others receive clariﬁcations re-annotate actions golden accuracy. procedure gathered average annotator hour. average annotation cost seconds clip seconds video. compare cost traditional annotation action localization take videos slac group annotators manually mark start actions video. statistics shown table total saving numbers suggest collecting localizations manually unachievable practice scale dataset long time would take consequent monetary cost. section present results action recognition. first train convnet slac training evaluate slac validation test set. second evaluate efﬁcacy slac pre-training models compare slac respect large scale datasets meanwhile experiment fusing optical features compare state-of-the-art literature. evaluations slac network architecture. resnet emerged successful models image classiﬁcation. experiment version namely resd extends convolutional operators resd model adopted paper parametric layers uses input adjacent frames ﬁxed resolution denote resd-. implementation details. implement approach caffe training resd- model positive negative clips slac. positive classes assign single background label negative clips. therefore resd- trained -way softmax loss. slac highly imbalanced positive negative clips training clips negative average positive class accounts total clips. prevent model training biased negative clips weighted sampling assemble training mini-batches. empirically weight negative clips positive clips. weights serve unnormalized multi-nomial distribution sample clips form mini-batches. mini-batch size speed model training large-scale datasets hosts nvidia gpus host distributed training parameter synchronization. therefore total mini-batch size yields throughput video clips second. train slac epochs ﬁrst epochs learning rate warmup reduce learning rate factor every epochs. base learning rate effective learning rate warmup training evaluate slac validation test set. evaluation metrics. clip validation test sample single sequence frames beginning clip. since validation test highly imbalanced positive negative clips training report class-agnostic class-aware clip accuracy. class-agnostic clip count number clips predicted label correct divide total number clips validation/test set. class-aware clip ﬁrst group clips based groundtruth class label compute top- accuracy class ﬁnally compute average class-wise top-accuracies. results. shown table input achieve class-agnostic class-aware clip accuracy validation set. test obtain class-agnostic class-aware clip accuracy. also experiment using optical input video model. adopt efﬁcient farneback’s approach quickly compute optical ﬂow. optical number frames resolution rgb. results using optical alone also reported table worse input alone class-agnostic classaware clip accuracy. also experiment combining optical late fusion ﬁnal prediction score weighted separate prediction scores optical ﬂow. empirically weights optical weight bigger input alone gives higher accuracy optical input. late fusion optical improves results input alone class-agnostic class-aware clip validation class-agnostic class-aware clip test set. given trained resd- model ﬁnetune target datasets. comparing performance target datasets using ﬁnetuned models models trained scratch evaluate efﬁcacy slac pre-training large models. evaluation metrics. test time measure performance according three metrics. clip sample single -frame clip beginning video report top- accuracy. video evenly sample clips frames video average clip predictions report top- accuracy. dense prediction multi-crops. densely sample clips frames stride frames. clip also take different crops well mirrored versions. average predictions crops clips report top- accuracy. action recognition datasets. datasets transfer learning action recognition experiments. ucf- hmdb- used target datasets. kinetics used pre-training dataset target dataset. sports-m used pre-training dataset. ﬁnetune models ucf- hmdb- kinetics epochs. many long videos sportsm shorter clips better utilize data training samples. kinetics report accuracy validation annotations testing publicly available. table comparisons slac datasets pre-training action recognition models. test performance evaluated ﬁnetuning ucf- hmdb- kinetics. numbers obtained training model input. hmdb split- training testing. results. shown table input pre-training slac improves clip video hmdb- kinetics respectively. optical input used pre-training slac improves clip video ucf- hmdb- kinetics respectively. comparisons pre-training datasets. efﬁcacy slac pre-training dataset compare large scale datasets transfer learning. results summarized table pre-training kinetics better sportsm ﬁnetuning models hmdb annotations sportsm noisy despite order magnitude numerous kinetics. hand pre-training sportsm still improves accuracy kinetics albeit slightly. slac reports best accuracy pre-training dataset three target datasets large-scale high-quality annotations. gain video comparing kinetics sports-m. table slac achieves state-of-the-art performance transfer learning. feature fusion. two-stream model shown successful action recognition combing appearance information motion information optical ﬂow. table presents results transfer learning slac using optical input alone. overall observe similar performance gain optical pre-training slac rgb. comparing accuracy optical pre-training always better optical three target datasets. expected combining optical using streams always improves ﬁnal accuracy. improvements video metric hmdb- kinetics rgb-only models respectively comparisons methods. finally compare slac-pretrained resd- model state-ofthe-art table ucf- hmdb- simply using off-the-shelf resnet layer leveraging power large-scale well annotated dataset outperform three competitive methods least kinetics validation model performs slightly worse two-stream model several factors contributing performance gap. first sophisticated model parameters inﬂated imagenet pre-trainining. second uses tv-l optical expensive accurate farneback optical adopted model. third train video frames size uses frames size leave empirical explorations differences performance future work. section present results action localization popular benchmarks thumos activitynet-v.. framework. task action localization adopt framework first candidate video segments proposed untrimmed videos; second dense frame-wise labels predicted proposal segments localize actions accurately. note frames input experiments action localization. network architecture. dense frame-wise action prediction operators used localization model. given input frames size full localization model uses resd- trunk produces feature maps size successive conv-deconv operators produce feature maps size -way softmax used generate predictions actions background class. conv-deconv operator originally proposed implements convolution space deconvolution time single step involves parameters learn separate conv operator deconv operator. number parameters resd- trunk conv-deconv operators extra parameters. details localization architecture found supplementary materials. training. pre-train slac randomly take frames -second slac clips training examples give frames uniform label based clip annotation. similar trained recognition models single background label assigned negative examples sampling weights positive negative samples respectively. pre-trained model ﬁnetuned thumos activitynet-v.. action annotation expand seconds beginning segment. randomly choose frames training examples. frames within original segment assigned positive action label frames outside original segments assinged background label. localization models ﬁnetuned using distributed training hosts host epochs including warm-up epochs learning rate increases linearly afterwards learning rate discounted every epochs. inference. although localization model produce frame-level dense predictions non-trivial design robust algorithm group frames form contiguous labeling action segments based prediction scores. hand existing segment proposal methods generate candidate segments high recall. therefore thumos adopt proposals apply dense prediction model localization; activitynet adopt output proposals follow method reﬁne proposal boundaries. given segment proposal ﬁrst extend percentage side assign label highest mean prediction entire extended segment. proposals assigned background label abandoned. otherwise perform gaussian density estimation reﬁne proposal boundaries computing mean standard deviation action scores. scan prediction score predicted action label side extended proposal keep shrinking boundary action score frame higher tuned small held-out videos taken training comparison. compare performance action localization models without slac pre-training thumos test activitynet-v. validation table table respectively. general pre-training signiﬁcantly improves performance localization models terms mean average precision speciﬁcally framework good improving hits high thresholds explained accurate action boundary prediction frame-wise dense labeling. compared localization model based resd conv-deconv operators trained scratch model architecture pre-trained slac achieves absolute gain thumos activitynet-v.. also compare original conv-deconv work pre-trained sports-m model outperforms thumos activitynet-v.. state-of-the-art methods different frameworks shown last rows table performance beats competitive r-cd model exploring best framework action localization beyond focus paper leave future work. present novel highly efﬁcient data collection procedure construct large-scale human action video datasets. proposed pipeline yields reduction annotation time. resulting dataset slac includes clip annotations spanning untrimmed videos. demonstrate slac consistently outperforms sports-m kinetics pretraining dataset action recognition models. also show superior transfer learning performance action localization thumos activitynet-v. benchmarks. dataset pretrained models code released upon publication paper.", "year": 2017}