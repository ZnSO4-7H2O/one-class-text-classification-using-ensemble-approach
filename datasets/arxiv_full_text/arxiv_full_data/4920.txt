{"title": "Beating the World's Best at Super Smash Bros. with Deep Reinforcement  Learning", "tag": ["cs.LG", "cs.AI", "I.2.6"], "abstract": "There has been a recent explosion in the capabilities of game-playing artificial intelligence. Many classes of RL tasks, from Atari games to motor control to board games, are now solvable by fairly generic algorithms, based on deep learning, that learn to play from experience with minimal knowledge of the specific domain of interest. In this work, we will investigate the performance of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting game. The SSBM environment has complex dynamics and partial observability, making it challenging for human and machine alike. The multi-player aspect poses an additional challenge, as the vast majority of recent advances in RL have focused on single-agent environments. Nonetheless, we will show that it is possible to train agents that are competitive against and even surpass human professionals, a new result for the multi-player video game setting.", "text": "recent explosion capabilities game-playing artiﬁcial intelligence. many classes tasks atari games motor control board games solvable fairly generic algorithms based deep learning learn play experience minimal knowledge speciﬁc domain interest. work investigate performance methods super smash bros. melee popular console ﬁghting game. ssbm environment complex dynamics partial observability making challenging human machine alike. multi-player aspect poses additional challenge vast majority recent advances focused single-agent environments. nonetheless show possible train agents competitive even surpass human professionals result multi-player video game setting. ssbm environment focus super smash bros. melee fast-paced multi-player ﬁghting game released nintendo gamecube. ssbm steadily grown popularity -year history today sports active tournament professional scene. metagame constantly evolving mechanics discovered reﬁned players push ever greater levels skill. standpoint ssbm environment poses several challenges large partially observable state complex transition dynamics delayed rewards. also great deal diversity environment unique characters multitude different stages. partial observability comes limits human reaction time along several frames built-in input delay forces players anticipate opponent’s actions ahead time. furthermore multi-player game adds entirely dimension complexity success longer single absolute measure given environment instead must deﬁned relative variable unpredictable adversary. past years seen renaissance sorts neural network models machine learning. driven part hardware advances gpus accelerate training ﬁrst breakthroughs came convolutional architectures able achieve record performance image classiﬁcation today technique known deep learning many layers build increasingly abstract representations inputs. paper focus vision game-playing. back early neural networks used reach expert-level play backgammon recently breakthroughs learning play various video games even ancient board game long thwarted attempts researchers build human-level programs fell combination neural networks monte-carlo tree search state action reward many previous applications deep video games used pixels observations. partly pragmatic reasons instead features read game’s memory frame consisting player’s position velocity action state along several values. allows focus purely challenge playing ssbm rather perception. case game features readily inferred pixels; deep networks known perform quite well vision tasks good reason believe pixel-based models would perform similarly. pixel-based networks would also better able deal projectiles currently know read game memory. game runs natively frames second lower skipping every frame. actions sent skipped frames equivalent controller changing state. better match human play would lower skipping frames would make impossible perform certain important actions humans perform regularly gamecube controller analog sticks buttons triggers directional relevant ssbm. make things easier eliminate inputs leaving discrete positions main analog stick buttons total discrete actions. sufﬁces majority relevant actions ssbm although proﬁcient humans routinely make controller inputs outside limited goal ssbm knock opponent sending bounds give scores events. opponents sent ﬂying depends damage displayed screen. damage dealt score small weighting factor. although ultimate objective reward signal important humans felt appropriate include without learning sparse signal alone would difﬁcult. players re-spawn middle stage koed. tournaments games four kos. simplify navigating ssbm menus instead game mode inﬁnite time arbitrarily mark episodes every seconds. methods used main classes model-free algorithms qlearning policy gradients. standard follow brief review techniques. henceforth denote states denote actions denote rewards three optionally indexed time step. capital letters denote random variables. assume future actions taken according policy practice estimate single sampled trajectory also truncate order reduce variance estimate objective function becomes approximated neural network stochastic gradient descent learn parameters. note second objective considered constant regards gradients; wish adjust become better predictor future rewards adjust future rewards match past prediction. learn policy construct policy always takes best action learned repeat. known policy iteration guaranteed quickly converge optimal policy small environments. course interesting environments like ssbm large state spaces prohibitive exhaustively explore entire space. cases common generate experiences using \u0001-greedy strategy random action taken probability explore promising actions also take actions boltzmann distribution predicted q-values. state take action probability proportional exp) temperature parameter must chosen match scale q-values. literature approach might referred n-step sarsa. deepmind’s original work using deep qnetworks atari games employed slightly different algorithm based bellman equation principle would allow directly learn optimal policy independent policy used generate experiences. however found much less stable sarsa q-values rapidly diverging reality likely iteration maximum operator training. exist techniques doubledqn alleviate effect warrant exploration. policy gradient methods policy gradient methods work slightly differently qlearning. main feature explicit representation policy maps states actions directly updated based experience. reinforce learning rule prototypical example sampled future reward baseline reward learning rate. intuitively increases probability taking actions performed better baseline vice-versa. shown that expectation maximizes expected discounted rewards averaged states. actor-critic algorithm extension reinforce replaces baseline parameterized function state known critic. critic attempts predict expected future reward state assuming policy followed similar function ideally removes state-dependent variance reward signal leaving action-dependent component advantage inform policy updates. experience value networks perform quite well explaining variance rewards. issue actor-critics face premature convergence suboptimal deterministic policy. because policy deterministic different actions longer explored never receive evidence actions might better policy never changes. simple workaround noise policy like q-learning. however q-values can’t explicitly explore similarly-valued actions similar probabilities. instead entropy term learning rule nudges policy towards randomness tune scale entropy term actor neither plunges deterministism remains stuck uniform randomness since entropy simply expected log-probability resulting actor-critic policy gradient form entropy scale constant negative distortion reward signal. therefore like reinforce baseline affect overall validity policy gradient maximizer total discounted reward. overall approach closely resembles deepmind’s asynchronous advantage actor-critic although perform asynchronous gradient updates similar network actor network outputs vector containing probabilities action. training despite years ssbm trivial emulate empirically found that modern reach framerates real time typically found servers manage quite slow compared performance-engineered atari learning environment atari games hundred times faster real time. means generating experiences major bottleneck. remedy many parallel agents periodically send experiences trainer maintains circular queue recent experiences. help trainer continually performs stochastic gradient descent experiences periodically saving snapshots neural network weights agents load. asynchronous setup technically breaks assumption reinforce learning rule data generated current policy network practice appear problem likely gradient steps sufﬁciently small change policy signiﬁcantly time experience sits queue. upside time wasted waiting part either agents trainer. hyper-parameters policies used epsilon value discount factor rewards seconds future worth half much rewards present. tried different values discounted reward summation settled neural networks used architectures fully-connected hidden layers size thorough attempts different architectures yield improvements some policy network actually worse. hand number sizes critic layers much effect. weight variables initialized random columns norm biases zero-mean normals standard deviation nonlinearity smoothed version traditional leaky relu call leaky softplus learning rate second-order methods whenever gradient descent employed must worry choosing right learning rate. must large local linearity assumption breaks loss fails decrease small learning unnecessarily slow. ideally learning rate would large possible still ensuring convergence. often hand-tuning sufﬁces; case learning rate gave reasonable results. principled approach higher-order derivatives adjust learning rate even gradient direction. error surface relatively take larger step; curved take small step. incidentally solves another issue ﬁrst-order methods scaling loss function translates equivalent scaling gradients mean dramatic change learning dynamics even though optimization problem effectively unchanged. began testing algorithms in-game appropriate parameter tuning learners actor-critics proved capable defeating highest difﬁculty setting reached similar average reward levels within day. algorithm found little variance experiments different initializations. however algorithms found qualitatively different policies other. actor-critics pursued standard strategy attacking counter-attacking similar humans play. q-learners hand would consistently unintuitive strategy tricking in-game killing itself. multi-step tactic fairly impressive; involves moving edge stage allowing enemy attempt -attack string ﬁrst hits second misses causes enemy suicide openai baseline openai released universe provide uniform interface collection various environments also provide starter agent implementing algorithm baseline solving gym/universe tasks main work interface lacks support multiagent environments implemented ssbm environment easy access. allowed openai’s starter agent task above falcon level falcon battleﬁeld. however running several days -core machine average reward never surpassed level actor-critic able reach hours. suggests ssbm even using underlying game however optimizing loss function optimizing policy policy iteration. means care change policy well change loss approach known trust region policy optimization constrains gradient step change policy bounded. change measured divergence policy policy averaged states batch chosen bound change policy. method lagrange multipliers shows optimal direction given solution unfortunately practice invert even store memory quadratic number parameters already quite large neural networks. thus resort conjugate gradient method requires ability take matrix-vector products follows note taking gradients scalars done efﬁciently automatic differentiation. step conjugate gradient descent improves progress direction policy gradient within constrained policy region cost extra computation time. practice found policy bound conjugate gradient iterations worked best. results unless otherwise stated agents human played captain falcon stage battleﬁeld chose captain falcon popular characters doesn’t projectile attacks using character stage greatly simpliﬁes environment makes possible directly compare learning curves scores. self-play agents trained in-game successful would pose challenge even low-level competitive human players. quality opponent in-game pursues speciﬁc strategy reﬂect experienced players actually play. without ever played human-level opponent surprising trained agents human-level. switching player structs state representation network play either player allowing train versions similar fashion alphago week self-training actor-critic network exhibited strong play similar expert human. author midlevel player hard-pressed defeat conventional tactics. another week training brought copy network major tournaments performed favorably professional players willing face even well-trained network exhibited strange weaknesses however. particularly clever player found simple strategy crouching edge stage caused network behave oddly refusing attack eventually koing falling side stage. hypothesis explain weakness lack diversity training since network played copies itself never encountered degenerate strategy. another limitation network trained play speciﬁc character particular stage predictably performs much worse variables changed. attempts train networks play multiple characters simultaneously train experiences generated multiple characters’ points view much success. anecdotally observed networks would appropriately change strategy based character choosing moves agent diversity simple solution playing multiple characters different network character. popular competitive characters networks train several days. results fairly good networks becoming challenging author play against. addition exhibit strange behavior earlier falcon-bot. suspect added uncertainty environment training different opponents. networks became opponents train future networks providing concrete benchmark measuring performance. empirically none future attempts able degenerate counterstrategies benchmark networks tentatively declare weakness resolved. character transfer training network play character found efﬁcient initialize already-trained network scratch. measure amount time taken reach average reward benchmark agents. measure transfer provides signiﬁcant speedup training. especially true similar pairs characters falco. whole results unsurprising many basic tactics universal characters. data also reveal overall ease playing character peach falco trained fairly quickly captain falcon signiﬁcantly slower rest. extent matches consensus ssbm community ranks characters falco marth sheik peach falcon. main difference peach performs better would naively expected community rankings. likely quick powerful attacks easier agents learn compared movement speed offered characters like marth falcon. discussion actor-critic q-learning found q-learners perform well learning self-play general playing networks training. could argued learning q-function intrinsically harder learning policy. technically true sense directly policy performs least well easy however found q-learners perform reasonably well ﬁxed opponents in-game benchmark networks. leads believe issue non-stationary nature playing agents also training. scenario function keep policy iteration also changes opponent. exploration exploitation main method quantitatively measuring tendency agent explore different actions average entropy policy. q-networks directly controlled temperature parameter. actor-critics entropy scale factor nudges direction policy gradient towards randomness training. looking mean entropy many states misleading however. typically minimum entropy quickly dips average remains many cases found seemingly high-entropy agents actually play repetitively. suggests frames action taken largely irrelevant agent’s performance. indeed attack initiated ssbm generally cannot aborted duration last order seconds. principled approach exploration would attempt quantify agent’s uncertainty prefer explore actions agent unsure. even measuring much state/action explored quite difﬁcult known bandit algorithms applied main criticism agents play unrealistic reaction speed frames compared humans. fair captain falcon popular characters perhaps worst equipped take advantage reaction speed attacks take many frames become active many characters attacks become active half time even immediately additional reason using falcon initially. issue reaction time highlights difference between neural net-based agents humans. neural effectively cloned state asked action destroyed frame. cloning destruction don’t really take place perspective puts network stark contrast people memory respond continually sensory experiences internal thoughts. closest neural network recurrence network outputs action also memory state current game state also previous memory. unfortunately network known difﬁcult train able train competent recurrent agent. avenue certainly warrants investigation recurrent networks would able deal amount delay could even principle handle projectiles learning remember ﬁred simulating trajectory memory. instead deal action delay frames network takes previous frames input along actions taken frames. sufﬁcient train fairly strong agents delay performance dropped sharply around frames. suspect cause drop performance simply handicap given delay separation actions rewards making harder tell actions really responsible already sparse rewards. goal work threefold. introduce environment reinforcement learning community super smash bros melee competitive multi-player game offers variety stages characters. analyze difﬁculties posed adapting traditional reinforcement learning algorithms typically designed around stationary markov decision process multi-player games adversary learn. finally demonstrate agent based deep learning sets state environment surpassing abilities highly-ranked human players. references marc bellemare sriram srinivasan georg ostrovski schaul david saxton remi munos. unifying count-based exploration intrinsic motivation alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep nips deep learning workreinforcement learning. shop. mnih puigdomenech badia mirza graves lillicrap harley silasynchronous methods kavukcuoglu. arxiv preprint deep reinforcement learning. arxiv. john schulman sergey levine philipp moritz michael jordan pieter abbeel. trust proceedings region policy optimization. international conference machine learning david silver huang christopher maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot sander dieleman dominik grewe ilya sutskever timothy lillicrap madeleine leach koray kavukcuoglu thore graepel demis hassabis. mastering game deep neural networks tree search. nature", "year": 2017}