{"title": "DropNeuron: Simplifying the Structure of Deep Neural Networks", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Deep learning using multi-layer neural networks (NNs) architecture manifests superb power in modern machine learning systems. The trained Deep Neural Networks (DNNs) are typically large. The question we would like to address is whether it is possible to simplify the NN during training process to achieve a reasonable performance within an acceptable computational time. We presented a novel approach of optimising a deep neural network through regularisation of net- work architecture. We proposed regularisers which support a simple mechanism of dropping neurons during a network training process. The method supports the construction of a simpler deep neural networks with compatible performance with its simplified version. As a proof of concept, we evaluate the proposed method with examples including sparse linear regression, deep autoencoder and convolutional neural network. The valuations demonstrate excellent performance.  The code for this work can be found in http://www.github.com/panweihit/DropNeuron", "text": "deep learning using multi-layer neural networks architecture manifests superb power modern machine learning systems. trained deep neural networks typically large. question would like address whether possible simplify training process achieve reasonable performance within acceptable computational time. presented novel approach optimising deep neural network regularisation network architecture. proposed regularisers support simple mechanism dropping neurons network training process. method supports construction simpler deep neural networks compatible performance simpliﬁed version. proof concept evaluate proposed method examples including sparse linear regression deep autoencoder convolutional neural network. valuations demonstrate excellent performance. code work found http//www.github.com/panweihit/ dropneuron commonly accepted deep learning system underlying neural network complex. argue perception true. many neurons associated connections incoming outgoing ones dropped permanently results much smaller size. similar sparse distributed representations brain. human neocortex roughly billion neurons given time small percent active performing particular cognitive function non-sequence time dependent data active neurons ﬁxed change time dropping neurons also idea dropout successful regularisation technique prevent overﬁtting nns. work neurons dropped temporarily training. prediction model still full size fully connected. hereafter training simple network achieve comparable performance fully connected number neurons connections possible. dropping connections difﬁcult introducing weight decay regularisers. however dropping neurons challenging. hand weight decay regularisation can’t penalise connections associated neuron simultaneously. hand attempted suppress neurons rectiﬁer activation function regularisation techniques like sparsity sparse autoencoder variants constraints like max-norm however neuron ﬁring training still can’t dropped testing prediction since connections’ weights zeros. alternative network pruning dropping connections threshold widely studied compress pre-trained fully connected models reduce network complexity over-ﬁtting early work recently unfortunately pruning strategy effectively drop neurons. example consist large number neurons connections. though model size/storage space challenging brings another challenge chip design storage computation e.g. fpga etc. example sparse matrix computation still paper propose strategy drop neurons. neuron dropped regularising incoming connections’ weights and/or outgoing connections’ weights neurons zeros. furthermore show achieve simplest network intractable convex relaxation cost function alleviate difﬁculty. relaxation yield simple network minimal. realised introducing regularisers penalise incoming outgoing connections respectively. regularisers form inspired group lasso test strategy three tasks ﬁrst sparse linear regression widely used benchmark compressive sensing second unsupervised learning using autoencoder mnist data; third convolutional lenet- structure classiﬁcation mnist data. evaluation demonstrates possibility dropping neurons still achieving good performance. following notation throughout paper. bold lower case letters denote vectors bold upper case letters denote matrices standard weight letters denote scalar quantities. subscripts denote variables well p-th number features input. subscripts denote either entire rows q-th column standard capital letter entire columns denotes element subscript denote element index speciﬁc variable column variable also indicator neurons layer example consist neurons input layer indexed start case three layer single hidden layer. generalisation multiple layers straightforward. denote weight matrices connecting ﬁrst layer hidden layer connecting hidden layer output layer respectively. linearly transform layers’ inputs applying element-wise non-linearity denote biases shift input non-linearity. assume model output dimensional vectors input dimensional vectors hidden units. thus matrix matrix dimensional vector. standard model would output following given input observed outputs outputs model pass output modely element-wise softmax function obtain normalised scorespnd expynd)/ expynd)). taking function results softmax loss mixture regulariser regulariser known elastic net. goal introducing regulariser regulariser penalise connections’ weights neurons prevent overﬁtting. however application regularisers alone deep neural network successful linear regression logistic regression. hand hardware computation especially using dropping connections save computation time memory unless special coding processing used introduction dropout achieve great success avoid over-ﬁtting practice regularisers. regularisation techniques suitable preventing overﬁtting helpful simplifying structure. believe automatically simplify structure training deﬁne proper regulariser exploring sparsity structure deep learning system. denote i-th encodes incoming connections’ weights layer i-th neuron layer i.e. denote j-th column encodes outgoing connections’ weights j-th neuron layer i.e. denotes i-th feature/neuron input layer. used regularise outgoing connections’ weights neurons across different layers whole network. idea introducing regularisers embed dropping mechanism deep training process. dropping mechanism guided regularisers. dropping principle first perform network pruning small weights. training magnitude less estimated weights tend small e.g. straightforward idea prune/remove weights threshold reduce network complexity over-ﬁtting. actually idea network pruning proved valid recently pruned state-of-the-art fully connected models pre-trained loss accuracy. select proper threshold drop connections. dropping principle similar previous work fairly simple loss accuracy pruning. shown experiments pruning reduced number parameters unfortunately pruning effectively drop neurons. dropping principle force regulariser small. taking neuron layer example incoming connections’ weights forced zeros. means i.e. received information neurons previous layer. mathematically valid however sufﬁcient necessary condition deﬁnitely unique substituted others e.g. dropping root sign becomes exactly norm simply summed neuron layer expected dropped norm) replaced norm independent grouping effect incoming weights neuron lost. idea inspired group lasso extent known extension lasso well studied statistics. though purpose group lasso different regularisation norm same. neurons layer conceptual idea removing incoming weights neuron therefore removal illustrated comparing fig. andc. dropping principle force regulariser small. taking neuron layer i.e. means blocked send information neurons next layer. situations blocking exist outputs several neurons layer e.g. exactly same. simple examples include regression problem p-th q-th feature exactly same; image classiﬁcation problem pixel pixel images exactly same. therefore expected outgoing weights neuron around. weight neuron conceptual idea removing outgoing weights neuron neurons layer therefore removal illustrated comparing fig. consequence introducing cost functions promote group removal neuron’s connections training process. emphasised regularisation parameters/hyperparameter tuned carefully. denotes k-th neuron figure graphical illustration dropneuron strategy section layer denotes weight connection neuron layer neuron layer bottom ﬁgures showed neuron removed either incoming connections’ weights outgoing connections’ weights zeros simultaneously. regularisers regulariser regulariser convex functions convexity promises differentiation cost function training process using backpropagation conjunction optimization method gradient descent. would like replace regulariser regulariser following respectively minimisation cost function sparsest solution generally intractable exhaustive search. therefore tightest convex relaxation solution convex relaxations suboptimal norm solution works well practice hugely facilitate optimisation. therefore relaxation yields nearly sparsest solution words nearly minimal later section surprise relaxed solution ﬁrst task sparse linear regression almost exact compared true solution. suspect exist performance guarantee like restricted isometry property compressive sensing implementation based tensorflow framework using acceleration. code available line. training prune small-weight connections connections absolute weights threshold removed network without reducing test accuracy. throughout examples following abbreviation indicate regularisation methods. regularisation pruning dropout dndropneuron fully connected layer started simple sparse linear regression problem classic problem compressive sensing sparse signal recovery. inputs outputs synthetically generated follows. first random feature matrix rm×n often overcomplete created whose columns drawn uniformly surface unit sphere next sparse coefﬁcient vectors randomly generated nonzero entries. nonzero magnitudes drawn i.i.d. experiment-dependent distribution. signals computed contaminated adding noise certain distribution. i.e. compressive sensing sparse signal recovery setting several algorithms presented attempts estimate training formulated neural network extreme case hidden layer neuron thin layer. minimisation cost function mean square error loss regulariser weight typically yield exact solution satisfy conditions like restricted isometry property rather using single hidden layer single neuron training speciﬁed multi-layer structure neurons layer. simple activation function assumed linear. therefore training main concern deep neural network framework prediction error test interesting. experiment number example training test same. used standard normalised mean square error metric i.e. nmse evaluate prediction accuracies models. seems deep neural architecture multiple layers many neurons overly used simple example. naturally expected prediction error small possible especially adding regularisation technique dropout. however results seems counter-intuitive method yield impressive performance. first number features nonzero elements hidden layer speciﬁed neurons layer. therefore output layer .after layer applied dropout keeping probability number example much greater number unknown weight setup experiment follows optimizer adamoptimizer; number epochs learning rate batch size dropout keep probability trials generate different feature matrix outcases independent put. illustration show training result trial prediction nmse using dropout spare vector entries nonzeros. estimated weights using dropout shown appendix sparse implying fully connected architecture. test nmse around using data training result using dropneuron found appendix sparse. zeros weights found nonzero entry fact nonzero entries appear second column means second neuron hidden layer necessary kept dropping neurons. similarly second neuron output layer necessary exist. meanwhile notice close nonzero entry investigate structure again considering effect linear activation function estimated network architecture dropping unnecessary neurons almost reveal true additive structure third tenth feature. conceptual illustration strategy dropping neurons regression problem found fig. considered image dataset mnist number training examples test examples respectively image sizes digit images classes. used autoencoder units logistic mean square error loss. let’s train autoencoder epochs. epochs visualise reconstructed inputs encoded representation without using different combination regularisations showed fig. fig. illustrated sparsity pattern estimated weight matrix setting nonzeros weights instead true value. shown fig. bottom rows zeros. means corresponding features/pixels image effect subsequent layer. training testing process patch fig. typically vectorised left right blue pixels inputs targeting bottom area weight matrix. apparently blue pixels picture background without useful information. furthermore conceptual illustration dropneuron strategy found fig. summary training testing statistics found table ambitiously expect dropneuron yield nmse total sparsity level/high compression rate neurons simultaneously. turns total sparsity level/compression rate/nmse slightly higher/lower/higher using dropneuron dropout together regularisation pruning. however number neurons dropped using dropneuron much higher. mean square error metric loss function minimised. training process trying recover input image. can’t guarantee lower nmse involves ﬁtting noise. importantly need consider unsupervised nature task feature representation. dropping neurons could well known fully connected layer parameter intensive typically raised problem store many parameters store single machine across multiple ones. consequence communication among machines inhibitor computation efﬁciency. example consider lenet- convolutional layers fully connected layers classiﬁcation mnist dataset. noted competing state-of-art accuracy various network structures exhaustive tuning hyperparameters batch size initial weights learning rate etc. would like demonstrate models trained dropneuron regularisation achieve comparable accuracy ones trained regularisations dropout regularisation ﬁxing conditions. fig. fig. illustrated actual training weights sparsity patterns fully connected layers combinations various regularisations. summary training testing statistics found table noted approach unable drop neurons ﬁlters convolutional layers. even compression rate presented novel approach optimising deep neural network regularisation network architecture. proposed regularisers support simple mechanism dropping neurons network training process. method supports construction simpler deep neural networks compatible performance simpliﬁed version. evaluate proposed method examples including sparse linear regression deep autoencoding convolutional net. valuations demonstrate excellent performance. research early stage. first noticed speciﬁc deep structures convolutional recurrent restricted boltzmann machine regularisers need adjusted respectively. second also notice dropout training deep approximate bayesian inference deep gaussian processes offer mathematically grounded framework reason model uncertainty regulariser regulariser potentially explained bayesian perspective introducing speciﬁc kernel functions references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning yoshua bengio aaron courville pierre vincent. representation learning review perspectives. pattern analysis machine intelligence ieee transactions song jeff pool john tran william dally. learning weights connections efﬁcient neural network. advances neural information processing systems pages song huizi william dally. deep compression compressing deep neural networks pruning trained quantization huffman coding. international conference learning representations geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov. improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research figure group dashed lines denote regularisation in-coming connections upper neuron input layer. group blue dashed lines denote regularisation out-going connections middle neuron output layer. group yellow dashed lines together lower blue dashed line denote regularisation in-coming connections lower neuron output layer. black dashed lines denote regularisation connections make alive neurons sparsely connected.", "year": 2016}