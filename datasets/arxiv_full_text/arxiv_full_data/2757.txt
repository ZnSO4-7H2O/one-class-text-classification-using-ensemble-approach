{"title": "Minimal Exploration in Structured Stochastic Bandits", "tag": ["stat.ML", "cs.AI", "cs.LG", "math.OC"], "abstract": "This paper introduces and addresses a wide class of stochastic bandit problems where the function mapping the arm to the corresponding reward exhibits some known structural properties. Most existing structures (e.g. linear, Lipschitz, unimodal, combinatorial, dueling, ...) are covered by our framework. We derive an asymptotic instance-specific regret lower bound for these problems, and develop OSSB, an algorithm whose regret matches this fundamental limit. OSSB is not based on the classical principle of \"optimism in the face of uncertainty\" or on Thompson sampling, and rather aims at matching the minimal exploration rates of sub-optimal arms as characterized in the derivation of the regret lower bound. We illustrate the efficiency of OSSB using numerical experiments in the case of the linear bandit problem and show that OSSB outperforms existing algorithms, including Thompson sampling.", "text": "paper introduces addresses wide class stochastic bandit problems function mapping corresponding reward exhibits known structural properties. existing structures covered framework. derive asymptotic instance-speciﬁc regret lower bound problems develop ossb algorithm whose regret matches fundamental limit. ossb based classical principle optimism face uncertainty thompson sampling rather aims matching minimal exploration rates sub-optimal arms characterized derivation regret lower bound. illustrate efﬁciency ossb using numerical experiments case linear bandit problem show ossb outperforms existing algorithms including thompson sampling. numerous extensions classical stochastic problem recently investigated. extensions motivated applications arising various ﬁelds including e.g. on-line services often concern structural properties mapping arms average rewards. mapping instance linear convex unimodal lipschitz exhibit combinatorial structure seminal paper robbins develop comprehensive theory problems unrelated arms i.e. without structure. derive asymptotic instance-speciﬁc regret lower bounds propose algorithms achieving minimal regret. algorithms considerably simpliﬁed today elementary indexbased asymptotically optimal algorithms developing similar comprehensive theory problems structure considerably challenging. structure rewards observed given actually provide side-information average rewards arms. side-information exploited accelerate much possible process learning average rewards. recently instance-speciﬁc regret lower bounds asymptotically optimal algorithms could derived problems ﬁnite arms speciﬁc structures namely linear lipschitz unimodal paper investigate large class structured problems. class extends classical stochastic problem directions allows arbitrary structure; allows different kinds feedback. precisely generic problem follows. round decision maker selects ﬁnite unknown parameter chosen round decision maker observes real-valued random variable expectation distribution observations )x∈x independent across arms rounds. chosen also receives unobserved deterministic reward )x∈x parameter lies compact encodes structural properties problem. class distributions mapping encode structure problem known decision maker whereas initially unknown. denote selected round algorithm selection based previously selected arms corresponding observations. hence possible selection rules consists algorithms σ-algebra generated performance algorithm deﬁned regret round problem generic kind structure considered. particular problem includes classical linear unimodal dueling lipschitz bandit problems particular examples section details. contributions paper follows develop ossb simple asymptotically optimal algorithm i.e. regret matches lower bound. ossb optimally exploits structure problem minimize regret. brieﬂy exemplify numerical performance ossb case linear bandits. ossb outperforms existing algorithms glm-ucb recently proposed asymptotically optimal algorithm noticed structured bandits algorithm based principle optimism thompson sampling achieve asymptotically minimal regret. design ossb follow principles rather inspired derivation regret lower bound. obtain bound characterize minimal rates sub-optimal arms explored. ossb aims sampling sub-optimal arms match rates. latter depends unknown parameter ossb needs accurately estimate ossb hence alternates three phases exploitation exploration estimation main technical contribution paper ﬁnite-time regret analysis ossb generic structure. spite simplicity algorithm analysis involved. surprisingly uses concentration-of-measure arguments also requires establish minimal exploration rates essentially smooth respect parameter complication arises estimation phase ossb minimal exploration rates converge estimate gets accurate. remainder paper organized follows. next section survey recent results structured stochastic bandits. section illustrate versatility problem casting existing structured bandit problems framework. section devoted derivation regret lower bound. sections present ossb provide upper bound regret. finally section explores numerical performance ossb case linear structures. usually problems reward random variable given feedback decision maker. model reward deterministic observed observation chosen round illustrate section usual formulations speciﬁc instances model. structured bandits generated many recent contributions since natural applications design computer systems instance recommender systems information retrieval routing networks network optimization inﬂuence maximization social networks large number existing structures investigated including linear combinatorial lipschitz unimodal results paper cover models considered body work ﬁrst applied problems structure allowed parameters. here focus generic stochastic bandits ﬁnite potentially large number arms. continuous well adversarial versions problem investigated survey performance thompson sampling generic bandit problems appeared literature however recent results prove thompson sampling optimal structured bandits. generic structured bandits treated authors show regret algorithm must scale optimal value semi-inﬁnite linear program propose asymptotically optimal algorithms. however proposed algorithms involved poor numerical performance furthermore performance guarantees asymptotic ﬁnite time analysis available. knowledge algorithm ﬁrst covers completely generic problems asymptotically optimal amenable ﬁnite-time regret analysis. algorithm spirit dmed algorithm presented well algorithm generic enough optimal structured bandit setting. similar dmed algorithm relies repeatedly solving optimization problem exploring according solution thus moving away family algorithms. class problems described introduction covers known bandit problems illustrated following examples. classical bandits. classical problem bernoulli rewards obtained making following choices |x|; bernoulli distribution mean linear bandits. ﬁnite linear bandit problems framework choose ﬁnite subset pick unknown vector deﬁne possible parameters x)x∈x rd}; gaussian distribution unit variance centered observe framework also includes generalized linear bandit problems considered need deﬁne function dueling bandits. model dueling bandits using framework arms d}}; denotes probability better conventions parameters exists condorcet winner; bernoulli distribution mean ﬁnally deﬁne rewards lipschitz bandits. ﬁnite lipschitz bandits arms ﬁnite subset metric space endowed distance scalar mapping lipschitz continuous respect parameters classical bandits structure encoded distance example local structure arms close similar rewards. unimodal bandits. unimodal bandits obtained follows. ...|x|} scalar added assumption unimodal. namely exists mapping stricly incrasing strictly decreasing ...|x|}. combinatorial bandits. combinatorial bandit problems bandit feedback particular instances linear bandits arms subset model combinatorial problems semi-bandit feedback need slight extension framework described introduction. precisely arms still subset observation d-dimensional r.v. independent components mean distribution unknown vector semi-bandit feedback decision maker gets detailed information various components selected arm. derive regret lower bounds strategy consists restricting attention so-called uniformly good algorithms uniformly good simple change-of-measure argument enough prove problems without structure uniformly good algorithm number times sub-optimal played greater time horizon grows large denotes optimal kullback-leibler divergence distributions ν)). refer direct elegant proof. structured problems follow strategy derive constraints number times sub-optimal played uniformly good algorithm. show number greater asymptotically solutions semi-inﬁnite linear program whose constraints directly depend structure problem. stating lower bound introduce following notations. optimal deﬁne denote kullback-leibler divergence distributions )x∈x denote solutions semi-inﬁnite linear program program indicates number times played. regret lower bound understood follows. confusing parameters cannot differentiated sampling optimal hence distinguishing requires sample suboptimal arms further since uniformly good algorithm must identify best high probability ensure regret algorithm must distinguish parameters. constraint states uniformly good required ensure enough statistical information perform test. summary sub-optimal clnt represents asymptotically minimal number times sampled. noted lower bound instance-speciﬁc attainable propose algorithm attains proof theorem presented appendix leverages techniques used context controlled markov chains next show usual structures considered section semi-inﬁnite linear program reduces simpler optimization problems sometimes even solved explicitly. simplifying important since proposed asymptotically optimal algorithm requires solve program. following examples please refer section deﬁnitions notations. mentioned already solutions classical θ)). linear bandits. class problems recently proved equivalent following optimization problem solution explicit problem reduces variables constraints. dueling bandits. solution follows assume simplify exists unique minimizing denote index. section propose ossb algorithm asymptotically optimal i.e. regret matches lower bound theorem ossb pseudo-code presented algorithm takes input parameters control amount exploration performed algorithm. design ossb guided necessity explore suboptimal arms much prescribed solution optimization problem i.e. sub-optimal explored times. known sampling times selecting largest estimated reward yield minimal regret. since unknown estimate deﬁne empirical averages number times selected round idea ossb )x∈x estimator explore arms match estimated solution optimization problem c)ln work ensure certainty equivalence i.e. sufﬁciently fast rate. ossb algorithm three components. precisely ossb alternate three phases exploitation estimation exploration. round ﬁrst attempts identify optimal arm. calculate largest empirical reward. c)ln enter exploitation phase enough information infer w.h.p. select otherwise need gather information identify optimal arm. goals make sure components accurately estimated make sure c)ln maintain counter number times entered expoitation phase. choose possible arms namely least played farthest satisfying c)ln consider number times selected. much smaller possibility selected enough times accurately estimated enter estimation phase select ensure certainty equivalence holds. otherwise enter exploration phase select explore dictated solution since close theorem states ossb asymptotically optimal. complete proof presented appendix sketch proof provided next section. prove theorem bernoulli subgaussian observations analysis easily extended rewards -parameter exponential family distributions. state asymptotic result here actually perform ﬁnite time analysis ossb ﬁnite time regret upper bound ossb displayed next section. assumption =ber) assumption conclude section remark computational complexity ossb algorithm. ossb requires solve optimization problem round. complexity solving problem strongly depends problem structure. general structures complexity problem difﬁcult assess. however problems exempliﬁed section problem usually easy solve. note algorithm proposed linear bandits requires solve once hence simpler implement; performance however much worse practice ossb illustrated section proof theorem presented appendix detail articulated four steps. ﬁrst notice probability selecting suboptimal exploitation phase using concentration inequality kl-divergences show probability small regret caused exploitation phase upper bounded ﬁnite depends solely |x|. second step involved show lemma stating solutions continuous. main difﬁculty ﬁnite optimization problem linear program. proof strategy similar used prove berge’s maximal theorem additional difﬁculty feasible compact berge’s theorem cannot applied directly. using assumptions value solution continuous. lemma optimal value continuous. admits unique solution )x∈x continuous lemma fact interesting right since optimization problems occur bandit problems. third step upper bound number times solution well estimated previous step implies θ||∞ well-chosen using deviation result show expected regret caused events ﬁnite upper bounded finally counting argument ensures regret incurred i.e. solution well estimated upper bounded assess efﬁciency ossb compare performance reasonable time horizons state algorithms linear bandit problems. considered linear bandit gaussian rewards unit variance arms unit length parameters generated uniformly random. implementation ossb since typically chosen literature performance algorithm appear sensitive choice baselines select extension thompson sampling presented algorithm presented figure presents regret various algorithms averaged parameters. ossb clearly exhibits best performance terms average regret. figure regret various algorithms linear bandit setting arms regret averaged randomly generated parameters trials. colored regions represent conﬁdence intervals. paper develop uniﬁed solution wide class stochastic structured bandit problems. ﬁrst time derive problems asymptotic regret lower bound devise ossb simple asymptotically optimal algorithm. implementation ossb requires solve optimization problem deﬁning minimal exploration rates sub-optimal arms. general case problem semi-inﬁnite linear program hard solve reasonable time. studying complexity semi-inﬁnite depending structural properties reward function interesting research direction. indeed asymptotically optimal algorithm needs learn minimal exploration rates sub-optimal arms hence needs solve semi-inﬁnite characterizing complexity latter would thus yield important insights trade-off complexity sequential selection algorithms regret. agrawal. continuum-armed bandit problem. siam control optim. agrawal goyal. thompson sampling contextual bandits linear payoffs. icml awerbuch kleinberg. online linear optimization adaptive routing. comput. syst. sci. combes talebi proutiere lelarge. combinatorial bandits revisited. nips dani hayes kakade. stochastic linear optimization bandit feedback. colt durand gagné. thompson sampling combinatorial bandits application online krishnamachari jain. combinatorial network optimization unknown variables multi-armed bandits linear rewards individual observations. ieee/acm trans. networking glashoff s.-a. gustafson. linear optimization approximation. springer verlag berlin gopalan mannor mansour. thompson sampling complex online problems. icml rusmevichientong tsitsiklis. linearly parameterized bandits. math. oper. res. ashkan eydgahi kveton. efﬁcient learning large-scale combinatorial semi-bandits. ﬁrst recall original framework presented whose results apply establish asymptotic lower bound theorem consider controlled markov chain measurable state space control given control transition probabilities parametrized compact metric space. denote probability move state state given control parameter parameter known. decision maker provided ﬁnite stationary control laws control mapping control applied state applied control assumed decision maker always selects control markov chain ergodic stationary distribution expected reward obtained applying control state denoted expected reward achieved control decision maker knows mapping selects chosen control time solely depends observed states past chosen control laws noted framework corresponding results straightforwardly extended case mapping arbitrary long mapping known decision maker. course remains unknown. apply framework structured bandit problem consider compact metric space encoding structural properties average reward function markov chain values control laws available arms. laws constant sense control applied control depend state markov chain corresponds selecting state markov chain time given observation transition probabilities chosen control chosen distributed independently therefore kullback-leibler information number framework simply kullback-leibler divergence distributions consider deﬁne consisting confusing parameters optimal parameter statistically indistinguishable playing lemma ﬁrst proven analyze lipshitz bandits useful generic bandit problems well allows bound expected number times optimal correctly identiﬁed arms sampled enough meet constraints optimization problem lemma proven context unimodal bandits. however versatile allows upper bound expected cardinality rounds selected accurately estimated. shown lemma sets rounds cause ﬁnite regret. lemma states optimization problem theorem continuous respect fact cornerstone analysis. since bandit problems feature optimization problems consider here lemma seems interesting right. main difﬁculty prove lemma comes fact ﬁnite optimization problem linear program. proof strategy similar used prove berge’s maximal theorem added difﬁculty feasible compact berge’s theorem cannot applied directly. lemma optimal value continuous. admits unique solution )x∈x continuous proof. deﬁne ease notation vector notation denote vectors r|x| whose respective components r|x| deﬁne ||v||∞ maxx∈x |v|. deﬁne minimum deﬁne minimum since consider deﬁne otherwise. therefore consider ﬁxed consider sequence converging prove →k→∞ sufﬁcient prove supk→∞ ﬁrst prove supk→∞ assumption exists deﬁnition consider optimal solution deﬁne sequence proven supk→∞ prove exists sequence prove bounded contradiction. unbounded admits subsequence ||ckm|| →m→∞ readily implies →m→∞ since →m→∞ components strictly positive. therefore supk→∞ contradiction since proven supk→∞ hence bounded. consider accumulation points subsequence converging since continuity implies →m→∞ since holds accumulation points proven concludes proof ﬁrst statement. second statement follows directly. certainty equivalence. upper bound number times suboptimal chosen estimated sufﬁcient accuracy. deﬁne event exploitation phase m||∞ deﬁne event occurs ∪x∈xb. prove occurs minx assume false exists rounds ts/} minx rounds minx incremented least minx since estimation exploration phase. left regret caused rounds estimated accurately exploitation phase. deﬁne event occur. deﬁne regret caused events assume occurs ﬁrst upper bound minx∈x since exploitation phase exists m)ln ||c)||∞ln hence ||c)||∞ln c)ln ||c)||∞ln cases ||c)||∞ln since incremented whenever occurs deduce |x|||c)||∞ln bound number times selected. ε|x|||c)||∞ln c)ln deduce", "year": 2017}