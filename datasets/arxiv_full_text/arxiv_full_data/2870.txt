{"title": "Variance Reduction for Policy Gradient with Action-Dependent Factorized  Baselines", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of the suboptimality of the optimal state-dependent baseline. The result is a computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by a synthetic 2000-dimensional target matching task. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. Finally, we show that the general idea of including additional information in baselines for improved variance reduction can be extended to partially observed and multi-agent tasks.", "text": "cathy aravind rajeswaran duan vikash kumar alexandre bayen sham kakade igor mordatch pieter abbeel cathywueecs.berkeley.edu aravrajcs.washington.edu rockyduaneecs.berkeley.edu vikashcs.washington.edu bayenberkeley.edu shamcs.washington.edu igor.mordatchgmail.com pabbeelcs.berkeley.edu department eecs berkeley department university washington openai institute transportation studies berkeley policy gradient methods enjoyed great success deep reinforcement learning suffer high variance gradient estimates. high variance problem particularly exasperated problems long horizons high-dimensional action spaces. mitigate issue derive bias-free action-dependent baseline variance reduction fully exploits structural form stochastic policy make additional assumptions mdp. demonstrate quantify beneﬁt action-dependent baseline theoretical analysis well numerical results including analysis suboptimality optimal state-dependent baseline. result computationally efﬁcient policy gradient algorithm scales high-dimensional control problems demonstrated synthetic -dimensional target matching task. experimental results indicate action-dependent baselines allow faster learning standard reinforcement learning benchmarks highdimensional hand manipulation synthetic tasks. finally show general idea including additional information baselines improved variance reduction extended partially observed multi-agent tasks. deep reinforcement learning achieved impressive results recent years domains video games visual inputs board games simulated control tasks robotics important class methods behind many success stories policy gradient methods directly optimize parameters stochastic policy local gradient information obtained interacting environment using current policy. policy gradient methods operate increasing probability actions proportional future rewards inﬂuenced actions. average actions perform better acquire higher probability policy’s expected performance improves. critical challenge policy gradient methods high variance gradient estimator. high variance caused part difﬁculty credit assignment actions affected future rewards. issues exacerbated long horizon problems assigning credproperly becomes even challenging. reduce variance baseline often employed allows increase decrease probability actions based whether perform better worse average performance starting state. particularly useful long horizon problems since baseline helps temporal credit assignment removing inﬂuence future actions total reward. better baseline predicts average performance accurately lead lower variance gradient estimator. insight paper individual actions produced policy decomposed multiple factors incorporate additional information baseline reduce variance. particular factors conditionally independent given current state compute separate baseline factor whose value depend quantities interest except factor. serves help credit assignment removing inﬂuence factors rewards thereby reducing variance. words information factors provide better evaluation well speciﬁc factor performs. factorized policies common examples listed below. continuous control robotics tasks multivariate gaussian policies diagonal covariance matrix often used. cases action coordinate considered factor. similarly factorized categorical policies used game domains like board games atari. multi-agent distributed systems agent deploys policy thus actions agent considered factor union actions particularly useful recent emerging paradigm centralized learning decentralized execution contrast previous example factorized policies common design choice problems dictated problem setting. demonstrate action-dependent baselines consistently improve performance compared baselines state information. relative performance gain task-speciﬁc certain tasks observe signiﬁcant speed-up learning process. evaluate proposed method standard benchmark continuous control tasks well high-dimensional door opening task ﬁve-ﬁngered hand synthetic high-dimensional target matching task blind insertion pomdp task multi-agent communication task. believe method facilitate applications reinforcement learning methods domains extremely highdimensional actions including multi-agent systems. videos additional results paper available https//sites.google.com/view/ad-baselines. three main classes methods reinforcement learning include value-based methods policy-based methods actor-critic methods valuebased actor-critic methods usually compute gradient objective critics often biased unless strict compatibility conditions conditions rarely satisﬁed practice stochastic gradient methods powerful function approximators. comparison policy gradient methods able compute unbiased gradient suffer high variance. policy gradient methods therefore usually less sample efﬁcient stable critic-based methods large body work investigated variance reduction techniques policy gradient methods. effective method reduce variance without introducing bias using baseline widely studied however fully exploiting factorizability policy probability distribution reduce variance studied. recently methods like q-prop make action-dependent control variate technique commonly used monte carlo methods recently adopted since q-prop utilizes off-policy data potential sample efﬁcient pure on-policy methods. however q-prop signiﬁcantly computationally expensive since needs perform large number gradient updates critic using off-policy data thus suitable fast simulators. contrast formulation action-dependent baselines little computational overhead improves sample efﬁciency compared on-policy methods state-only baseline. idea using additional information baseline critic also studied contexts. methods guided policy search variants train policies high-dimensional observations like images dimensional encoding problem like joint positions training process. recent efforts multi-agent systems also additional information centralized training phase speed-up learning. however using structure policy parameterization enhance learning speed work explored. notation paper assumes discrete-time markov decision process deﬁned n-dimensional state space m-dimensional action space transition probability function bounded reward function initial state distribution discount factor. presented models based optimization stochastic policy parameterized γtr] denotes whole trajectory goal optimal policy maxθ describe samples cumulative discounted return describe function approximation q-function describing abstract action-value function. partially observable markov decision process components required namely observations observation probability distribution. fully observable case though analysis article written policies states analysis done policies observations. important technique used derivation policy gradient known score function estimator also comes justiﬁcation baselines. suppose want estimate ∇θex] family distributions common support. suppose continuous case shown reduce variance gradient estimator without introducing bias subtracting quantity dependent appendix derivation optimal state-dependent baseline. practice rich internal structure policy parameterization. example continuous control tasks common parameterization make multivariate gaussian diagonal variance case dimension action conditionally independent dimensions given current state another example policy outputs tuple discrete actions factorized categorical distributions. following subsections show structure exploited reduce variance gradient estimator without introducing bias changing form baseline. then derive optimal action-dependent baseline class problems analyze suboptimality non-optimal baselines terms variance reduction. propose several practical baselines implementation purposes. conclude section overall policy gradient algorithm action-dependent baselines factorized policies. provide exposition situations conditional independence assumption hold stochastic policies general covariance structures appendix compatibility variance reduction techniques appendix following analyze action-dependent baselines policies conditionally independent factors. example multivariate gaussian policies diagonal covariance structure commonly used continuous control tasks. assuming m-dimensional action space appendix show methodology also applies general policy structures conditional independence assumption hold. result bias-free albeit different baselines. optimal action-dependent baseline section derive optimal action-dependent baseline show better state-only baseline. seek optimal baseline minimize variance policy gradient estimate. first write variance policy gradient action-dependent baseline. deﬁne observation space cases different function approximators used control different actions synergies without weight sharing. appendix full derivation. since optimal action-dependent baseline different different action coordinates outside family state-dependent baselines barring pathological cases. appendices full derivation. conclude optimal action-dependent baseline degenerate optimal state-dependent baseline. equation states variance difference weighted deviation per-component score-weighted marginalized component weight overall yj). suggests difference particularly large function highly sensitive actions especially along directions inﬂuence gradient most. empirical results section additionally demonstrate beneﬁt action-dependent state-only baselines. marginalized baseline even though optimal state-only baseline known rarely used practice rather computational conceptual beneﬁt choice eat] often used. similarly propose action-dependent analogue. particular probability policy factor loosely correlated action-value function proposed baseline close optimal baseline. added beneﬁt requiring learning function approximator estimating implicitly using obtain baselines action coordinate. function approximating samples mean marginalized baseline though reduced computational burden learning functions function monte carlo samples still computationally expensive. particular using deep neural networks approximate q-function forward propagation network even computationally expensive stepping fast simulator settings propose following computationally practical baseline algorithm policy gradient factorized policies using action-dependent baselines require number iterations batch size initial policy parameters initialize action-value function estimate policy computing baseline done either proposed technique section similar algorithm written general policies makes assumptions conditional independence across action dimensions. continuous control benchmarks firstly present results proposed action-dependent baselines popular benchmark tasks. tasks widely studied deep reinforcement learning community studied tasks include hopper half-cheetah locomotion tasks simulated mujoco addition tasks also consider door opening task high-dimensional multi-ﬁngered hand introduced rajeswaran study effectiveness proposed approach high-dimensional tasks. figure presents learning curves tasks. compare action-dependent baseline baseline uses information states common approach literature. observe action-dependent baselines perform consistently better. popular baseline parameterization choice linear function small number non-linear features state especially policy gradient methods. work enable fair comparison random fourier feature representation baseline features constructed matrix element independently drawn standard normal distribution random phase shift bandwidth parameter. features approximate rkhs features kernel. using features baseline parameterized appropriate inputs baseline trainable parameters. trained parameterization. representation chosen reasons wish number trainable parameters baseline architectures parameters action-dependent case since ﬁnal representation linear possible accurately estimate optimal parameters newton step thereby alleviating results confounding optimization issues. policy optimization variant natural policy gradient method described rajeswaran appendix experimental details. figure comparison value function baseline action-conditioned baseline various continuous control tasks. action-dependent baseline performs consistently better across tasks. choice action-dependent baseline form next study inﬂuence computing baseline using empirical averages sampled q-function versus using mean-action action-coordinate computing baseline experiments shown figure variants perform comparably latter performing slightly better towards learning process. suggests though sampling q-function might provide better estimate conditional expectation theory function approximation ﬁnite samples injects errors degrade quality estimates. particular sub-sampling q-function likely produce better results learned q-function accurate large fraction action space getting high quality approximations might hard practice. high-dimensional action spaces intuitively beneﬁt action-dependent baseline greater higher dimensional problems. show effect simple synthetic example called m-dimtargetmatching. example one-step comprising single state m-dimensional action space ﬁxed vector reward given negative squared loss action vector optimal action thus match figure variants action-dependent baseline sampling q-function estimate conditional expectation; using mean action form linear approximation conditional expectation. variants perform comparably latter computationally efﬁcient. given vector selecting results demonstrative example shown table shows action-dependent baseline successfully improves convergence higher dimensional problems lower dimensional problems. lack state information linear baseline reduces whitening returns. action-dependent baseline hand allows learning algorithm assess advantage individual action dimension utilizing information action dimensions. additionally experiment demonstrates algorithm scales well computationally high-dimensional problems. table shown results synthetic high-dimensional target matching task dimensional action spaces. high dimensions linear feature action-dependent baseline provides notable consistent variance reduction compared linear feature baseline resulting around faster convergence. corresponding learning curves appendix partially observable multi-agent tasks finally also consider extension core idea using global information studying pomdp task multi-agent task. blind peg-insertion task widely studied robot learning literature task requires robot insert hole robot blind location hole. thus expect searching behavior emerge robot learns hole present table performs appropriate sweeping motions till able hole. case consider baseline given access location hole. observe baseline additional information enables faster learning. multi-agent setting analyze two-agent particle environment task goal agent reach goal goal known agent continuous communication channel. similar training procedures employed recent related works lowe levine figure shows including inclusion information agents action-dependent baseline improves training performance indicating variance reduction multi-agent reinforcement learning. action-dependent baseline enables using additional signals beyond state achieve bias-free variance reduction. work consider conditionally independent policies general policies derive optimal action-dependent baseline. provide analysis variance success percentage blind insertion task. policy still acts observations know hole location. however baseline access goal information addition observations action helps speed learning. comparison blue baseline access observations actions. training curve multi-agent communication task agents. policies simultaneously trained agent. policy acts observations respective agent only. however shared baseline access agent’s state action addition state action results considerably faster training. comparison blue independent learners baseline access single agent’s state action. reduction improvement non-optimal baselines including traditional optimal baseline depends state. additionally propose several practical action-dependent baselines perform well variety continuous control tasks synthetic high-dimensional action problems. additional signals beyond local state generalizes problem settings instance pomdp multi-agent tasks. future work propose investigate related methods settings large-scale problems. duan chen rein houthooft john schulman pieter abbeel. benchmarking deep reinforcement learning continuous control. proceedings international conference machine learning evan greensmith peter bartlett jonathan baxter. variance reduction techniques gradient estimates reinforcement learning. journal machine learning research shixiang timothy lillicrap zoubin ghahramani richard turner sergey levine. qprop sample-efﬁcient policy gradient off-policy critic. international conference learning representations timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous control deep reinforcement learning. international conference learning representations ryan lowe aviv tamar jean harb pieter abbeel igor mordatch. multi-agent actorcritic mixed cooperative-competitive environments. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning aravind rajeswaran vikash kumar abhishek gupta john schulman emanuel todorov sergey levine. learning complex dexterous manipulation deep reinforcement learning demonstrations. corr abs/. john schulman sergey levine pieter abbeel michael jordan philipp moritz. trust region policy optimization. proceedings international conference machine learning john schulman philipp moritz sergey levine michael jordan pieter abbeel. highdimensional continuous control using generalized advantage estimation. international conference learning representations david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature richard sutton david mcallester satinder singh yishay mansour. policy gradient methods reinforcement learning function approximation. advances neural information processing systems emanuel todorov zoubin ghahramani. analysis synergies underlying complex hand manipulation. engineering medicine biology society iembs’. annual international conference ieee volume ieee emanuel todorov weiwei xiuchuan pan. task parameters motor synergies hierarchical framework approximately optimal control redundant manipulators. journal field robotics weaver nigel tao. optimal reward baseline gradient-based reinforcement learning. proceedings seventeenth conference uncertainty artiﬁcial intelligence morgan kaufmann publishers inc. provide derivation optimal state-dependent baseline minimizes variance policy gradient estimate based precisely minimize trace covariance policy gradient; variance components vectors. recall policy gradient expression state-dependent baseline derive optimal action-dependent baseline minimizes variance policy gradient estimate. first write variance policy gradient action-dependent t|st) component baseline. recall following notations deﬁne policy gradient translates meaning different subsets parameters strongly inﬂuence different action dimensions factors. true case distributed systems construction also true altogether forms action conditioned different factors form certain directed acyclic graphical model without loss generality assume following factorization holds denotes indices parents factor. denote indices descendants graphical model case baseline bi\\d words baseline depend factors factor inﬂuence. overall gradient estimator given applicability techniques general action spaces crucial importance many application domains conditional independence assumption hold language tasks compositional domains. even continuous control tasks hand manipulation many tasks common practice conditionally independent factorized policies reasonable expect training improvement policies without full conditionally independence structure. computing action-dependent baselines general actions marginalization presented section apply general action setting. instead individual baselines trained according factorization ﬁtted data collected previous iteration. general case means ﬁtting functions wish instead learn overall function marginalize block factors. another interesting example explore sparse covariance structures. algorithm policy gradient general factorization policies using action-dependent baselines require number iterations batch size initial policy parameters temporal difference learning methods allow smoothly interpolate high-bias low-variance estimates low-bias high-variance estimates policy gradient. methods based idea able predict future returns thereby bootstrapping learning procedure. particular using value function baseline unbiased estimator uses exponential averaging temporal difference terms trajectory signiﬁcantly reduce variance advantage cost small bias similarly unbiased estimator thus temporal difference error action dependent baselines unbiased estimator advantage function well. allows procedure reduce variance cost bias. following study shows action-dependent baselines consistent procedures temporal differences estimates advantage function. results summarized figure suggests slightly biasing gradient reduce variance produces best results high-bias estimates perform poorly. prior work baselines utilize global information employ high-bias variant. results suggest potential improve upon results carefully studying bias-variance trade-off. figure study inﬂuence allows trade bias variance desired. high bias gradient corresponding smaller values make progress while. high variance gradient trouble learning initially. allowing small bias reduce variance corresponding intermediate produces best overall result consistent ﬁndings schulman figure shows resulting training curves synthetic high-dimensional target matching task described section higher dimensional action spaces action-dependent baseline consistently converges optimal solution faster statebaseline. parameters unless otherwise stated following parameters used experiments work λgae kldesired policies policies used -layer fully connected networks hidden sizes=. initialization policy initialized xavier initialization except ﬁnal layer weights scaled note since baseline linear estimated newton step initialization inconsequential. per-experiment conﬁguration following parameters table state-only action-dependent versions experiments. m-dimtargetmatching experiments linear feature baseline. table details dimensionality action space task. figure shown learning curve synthetic high-dimensional target matching task dimensional action spaces. high dimensions linear feature actiondependent baseline provides notable consistent variance reduction compared linear feature state baseline. dimensions method converges faster optimal solution.", "year": 2018}