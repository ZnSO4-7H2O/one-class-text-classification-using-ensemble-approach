{"title": "Robust Subspace Clustering via Tighter Rank Approximation", "tag": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "abstract": "Matrix rank minimization problem is in general NP-hard. The nuclear norm is used to substitute the rank function in many recent studies. Nevertheless, the nuclear norm approximation adds all singular values together and the approximation error may depend heavily on the magnitudes of singular values. This might restrict its capability in dealing with many practical problems. In this paper, an arctangent function is used as a tighter approximation to the rank function. We use it on the challenging subspace clustering problem. For this nonconvex minimization problem, we develop an effective optimization procedure based on a type of augmented Lagrange multipliers (ALM) method. Extensive experiments on face clustering and motion segmentation show that the proposed method is effective for rank approximation.", "text": "rank function counts number nonvanishing singular values nuclear norm sums amplitudes. result nuclear norm dominated large singular values. variations standard nuclear norm shown promising recent research number nonconvex surrogate functions come better approximate rank function logarithm determinant schatten-p norm truncated nuclear norm others general solve following low-rank minimization problem many real-world applications high-dimensional data reside union multiple low-dimensional subspaces rather single low-dimensional subspace subspace clustering deals exactly structure clustering data points according underlying subspaces. numerous applications computer vision image processing therefore subspace clustering drawn signiﬁcant attention recent years practice underlying subspace structure often corrupted noise outliers thus data deviate original subspaces. necessary develop robust estimation techniques. number approaches subspace clustering proposed past decades. according survey roughly divided four categories algebraic methods; iterative methods; statistical methods; spectral clustering-based methods. among them spectral clustering-based methods obtained state-of-the-art results including sparse subspace clustering rank representation matrix rank minimization problem general np-hard. nuclear norm used substitute rank function many recent studies. nevertheless nuclear norm approximation adds singular values together approximation error depend heavily magnitudes singular values. might restrict capability dealing many practical problems. paper arctangent function used tighter approximation rank function. challenging subspace clustering problem. nonconvex minimization problem develop eﬀective optimization procedure based type augmented lagrange multipliers method. extensive experiments face clustering motion segmentation show proposed method eﬀective rank approximation. matrix rank minimization arises control machine learning signal processing areas diﬃcult solve discontinuity nonconvexity rank function. existing algorithms largely based nuclear norm heuristic i.e. replace rank nuclear norm nuclear norm matrix rank approximation matrix subspace clustering setting. three advantages approximation function. first approximates rank much better nuclear norm does rank approximation value approaches rank situation. clearly arctangent reﬂects real rank pretty well broad range singular values. second diﬀerentiable concave monotonically in∂xi variant absolutely symmetric i.e. invariant arbitrary permutation sign changes components based properties following theorems proved appendix perform subspace clustering steps ﬁrst learning aﬃnity matrix encodes subspace membership information applying spectral clustering algorithms learned aﬃnity matrix obtain ﬁnal clustering results. main diﬀerence obtain good aﬃnity matrix. assumes data point represented sparse linear combination points. popular l-norm heuristic used capture sparsity. enjoys great performance face clustering motion segmentation data. good theoretical understanding ssc. instance shows disjoint subspaces exactly recovered certain conditions; geometric analysis signiﬁcantly broadens scope intersecting subspaces. however data points assumed lying exactly subspace. assumption violated presence corrupted data. extends adding adversarial random noise. however ssc’s solution might sparse thus aﬃnity graph single subspace fully connected body address issue another regularization term introduced promote connectivity graph also represents data point linear combination points. lowest rank representation data points jointly nuclear norm used common surrogate rank function. presence noise outliers solves following problem rank well sparsity requirement help counteract corruptions. variant works even presence arbitrarily large outliers however never shown succeed strong independent subspace condition view issues nuclear norm mentioned beginning propose arctangent function instead work. demonstrate enhanced performance proposed algorithm benchmark data sets. convert problem ﬁrst term concave second term convex apply diﬀerence convex optimization method. linear approximation used iteration programing. iteration points. increasing power enhances separation ability presence noise. however excessively large would break aﬃnities points group. order compare experiments postprocessing procedure lrr. obtaining directly utilize spectral clustering algorithm ncuts cluster samples. algorithm summarizes complete subspace clustering steps proposed method. since objective function nonconvex would easy prove convergence theory. paper mathematically prove optimization algorithm least convergent subsequence converges accumulation point moreover accumulation point algorithm stationary point. although ﬁnal solution might local optimum results superior global optimal solution convex approaches. previous work also reports similar observations section presents experiments proposed algorithm extended yale hopkins databases standard tests robust subspace clustering algorithms. shown challenge hopkins dataset small principal angles subspaces. eyaleb challenge lies small principal angles another factor data points diﬀerent subspaces close. results compared several state-of-the-art subspace clustering algorithms including lrsc spectral curvature clustering local subspace aﬃnity terms misclassiﬁcation rate. fair comparison follow experimental setup obtain results. methods tune parameters obtain general value depends prior best results. knowledge noise level data. noise heavy small adopted. aﬀect convergence speed. larger values fewer iterations required algorithm converge meanwhile lose precision ﬁnal objective function value. literature value often chosen iteration stops relative normed diﬀerence successive iterations maximum iterations. face clustering refers partitioning face images multiple individuals multiple subspaces according identity individual. face images heavily contaminated sparse gross errors varying lighting left-hand side equation bounded term right-hand side nonnegative term bounded. therefore bounded. bounded according last term right-hand side thus multiplying constant matrix bounded. condition invertible multiplying constant matrix bounded. finally bounded second last term bounded. therefore {et} bounded. proof. based conditions penalty parameter sequence {µt} algorithm generates bounded lemma sequence point exists e.g. without loss generality assume converges shown below tions model errors experiment. eyaleb database contains cropped face images individuals taken diﬀerent illumination conditions. subjects divided four groups follows subjects table provides best performance method. shown table proposed method lowest mean clustering error rates settings. particular challenging case subjects mean clustering error rate improvement signiﬁcant compared rank representation based subspace clustering i.e. lrsc. example improvement observed cases subjects respectively. demonstrates importance accurate rank approximation. figure shows obtained aﬃnity graph matrix subjects scenarios. distinct blockdiagonal structure means cluster becomes highly compact diﬀerent subjects well separated. figure plots progress objective function values observed iterations value objective function decreases monotonically. empirically veriﬁes convergence optimization method. figure shows culstering error rate diﬀerent sequences. clustering error rate varies demonstrates performs well pretty wide range values another advantage work propose arctangent concave rank approximation function. nice properties compared standard nuclear norm. apply function rank representation-based subspace clustering problem develop iterative algorithm optimizing associated objective function. extensive experimental results demonstrate that compared many stateof-the-art algorithms proposed algorithm gives lowest clustering error rates many benchmark datasets. fully demonstrates signiﬁcance accurate rank approximation. interesting future work includes applications arctangent rank approximation; example matrix completion. since ensure validity independent subspace segmentation worthwhile investigate somewhat dependent possibly disjoint subspace clustering. compare average computational time function number subjects figure experiments conducted timed machine intel xeon .ghz cores memory running ubuntu matlab observe computational time higher little slower cases. motion segmentation involves segmenting video sequence multiple moving objects multiple spatiotemporal regions corresponding diﬀerent motions. motion sequences divided three main categories checkerboard traﬃc articulated non-rigid motion sequences. hopkins dataset includes video sequences motions corresponding low-dimensional subspaces ambient space. sequence represents data motion segmentation problems total. several example frames shown figure trajectories extracted automatically tracker above holds frobenius norm unitarily invariant; holds unitarily invariant; true neumann’s trace inequality; holds deﬁnition therefore lower bound note equality attained minimizing therefore eventually diagv minimizer problem", "year": 2015}