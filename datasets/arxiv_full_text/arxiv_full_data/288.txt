{"title": "Towards Transparent AI Systems: Interpreting Visual Question Answering  Models", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques -- guided backpropagation and occlusion -- to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. We found that even without explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.", "text": "figure goal work interpret visual question answering models. interested answering question– model predict does? approach evidence test input model focuses answering question. example whole important word question model predicting answer unfortunately today’s machine perception intelligent systems fail fail spectacularly disgraceful manner without warning explanation leaving user staring incoherent output wondering system did. work focus visual question answering given image free-form natural language question image machine produce natural language answer output speciﬁcally interpret recent state-of-art model trained recently released dataset. deep neural networks shown striking progress obtained state-of-the-art results many research ﬁelds recent years. however often unsatisfying know predict paper address problem interpreting visual question answering models. speciﬁcally interested ﬁnding part input model focuses answering question. tackle problem visualization techniques guided backpropagation occlusion important words question important regions image. present qualitative quantitative analyses importance maps. found even without explicit attention mechanisms models sometimes implicitly attending relevant regions image often appropriate words question. witnessing excitement research community frenzy media regarding advances fueled combination massive datasets advances deep neural networks community made remarkable progress past years variety ‘low-level’ tasks image classiﬁcation machine translation speech recognition neural networks also demonstrating potential ‘high-level’ tasks learning model uses convolutional neural network based embedding image long-short term memory based embedding question combines embeddings uses multilayer perceptron classiﬁer predict probability distribution answers. sustained interactions system make clear system non-trivial level intelligence however deeply unsatisfying know system predicts root cause lack transparency interpretability. rare exceptions emphasis machine learning computer vision communities today building systems good predictive performance transparency. result users intelligent systems perceive inscrutable black boxes cannot understood trusted. interested question transparency system does? speciﬁcally evidence test input supports particular prediction? context question expressed subproblems work visualization methods tackle problems. ﬁrst method uses guided backpropagation analyze important words question important regions image. second method occlude portions input observe change prediction probabilities model compute importance question words image regions. sec. present qualitative quantitative analyses image/question ‘importance maps’ question importance maps analyzed using part-of-speech tags; image importance maps compared ‘human attention maps’ maps showing humans look answering question image found even without many gradient based methods proposed recent years ﬁeld computer vision visualize deep convolutional neural networks. focused task image classiﬁcation iconic images main object occupies image. work differs ways also compute gradients w.r.t. input question guided backpropagation task model look different regions image different questions. literature review ﬁrst study problem vqa. occlusion experiment inspired mask small regions image gray patch observe output image classiﬁcation model. evaluate model looks regions image humans answering question image. recent works begun study task providing interpretable posthoc explanations classiﬁer predictions. methods typically involve ﬁtting training secondary interpretable mechanism base ‘black-box’ classiﬁer predictions. contrast work directly computes importance maps model interest without another layer training high level view model learned function takes input image question image parameterized parameters produces answer order gauge importance components consider best linear approximation around test point figure results discrete derivatives experiment. shows heat maps questions showing importance words questions. encouragingly vegetable important word ﬁrst question predicted answer broccoli. shows importance different regions images. image region containing parrot affects model’s prediction most. visualization provide justiﬁcations predictions resulting increased transparency inner working model. best viewed color. intuitively quantities need compute i.e. partial derivatives function w.r.t. inputs expressions superﬁcially look similar gradients computed backpropagation-based training neural networks. however differences compute partial derivatives probability predicted output ground-truth output; compute partial derivatives w.r.t. inputs parameters. guided backpropagation guided backpropagation gradient-based visualization technique used visualize activations neurons different layers cnns. shown perform better counterparts deconvolution especially visualizing higher order layers. intuitively speaking modiﬁed version speciﬁcally guided identical classical except backward pass computed rectiﬁed linear units denote input layer denote output. recall relu deﬁned relu max. ∂f/∂hl+ denote partial derivative w.r.t. output relu difference between backprops guided compute ‘gradients’ probability predicted answer w.r.t. inputs note language pathway models typically contain relus thus true gradients language side. interpret words/pixels highest gradients received important model since small changes lead largest changes model’s conﬁdence predicted answer. discrete derivatives method systematically occlude subsets input forward propagate masked input model compute change probability answer predicted unmasked original input. since inputs model focus input time keeping input ﬁxed speciﬁcally compute importance question word mask word dropping question feed masked question original image inputs model. importance score question word computed change probability original predicted answer. follow procedure images compute importance image regions. divide image grid size occlude cell time gray patch feed perturbed image entire question model compute decrease probability original predicted answer. generated importance maps shown fig. analyzing image importance recently collected human attention annotations pairs dataset given blurry image question humans asked deblur regions image helpful answering question. human attention maps. human attention dataset contains annotations pairs validation set. following evaluation protocol take absolute value importance maps compute mean rank-correlation human attention maps. speciﬁcally ﬁrst scale image importance human attention maps normalize spatially rank pixels according spatial attention compute correlation ranked lists. results shown table importance maps weakly positively correlated human attention maps although interhuman correlation. thus techniques revealed interesting ﬁnding even without attention mechanisms models implicitly attending relevant regions image. ‘sensible’ model’s prediction. plot probability word important question given certain tag. reliable statistics picked frequent tags validation dataset grouped similar tags category e.g. grouped wh-words. histogram seen fig. indeed wh-words important followed adjectives nouns. adjectives nouns rank high many questions tend characteristics objects objects themselves. ﬁnding suggests language model part model strong able learn focus appropriate words without explicit attention procedure. note many occlusions model’s predicted answer different original predicted answer. fact found number times predicted answer changes correlates model’s accuracy. able predict success/failure accurately times. suggests features characterize importance maps provide useful signals predicting model’s oncoming failures. paper experimented visualization methods guided backpropagation occlusion interpret deep learning models task visual question answering. although focus model work methods generalizable end-to-end models. occlusion method even applied model considering black box. believe methods results helpful interpreting current models designing next generation models. acknowledgements. work supported part following national science foundation career awards army research ofﬁce awards ictas junior faculty awards army research grant wnf--- ofﬁce naval research grant n--- paul allen family foundation allen distinguished investigator award google faculty research award education research grant nvidia donation views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied u.s. government sponsor.", "year": 2016}