{"title": "Deep-FSMN for Large Vocabulary Continuous Speech Recognition", "tag": ["cs.NE", "cs.CL"], "abstract": "In this paper, we present an improved feedforward sequential memory networks (FSMN) architecture, namely Deep-FSMN (DFSMN), by introducing skip connections between memory blocks in adjacent layers. These skip connections enable the information flow across different layers and thus alleviate the gradient vanishing problem when building very deep structure. As a result, DFSMN significantly benefits from these skip connections and deep structure. We have compared the performance of DFSMN to BLSTM both with and without lower frame rate (LFR) on several large speech recognition tasks, including English and Mandarin. Experimental results shown that DFSMN can consistently outperform BLSTM with dramatic gain, especially trained with LFR using CD-Phone as modeling units. In the 2000 hours Fisher (FSH) task, the proposed DFSMN can achieve a word error rate of 9.4% by purely using the cross-entropy criterion and decoding with a 3-gram language model, which achieves a 1.5% absolute improvement compared to the BLSTM. In a 20000 hours Mandarin recognition task, the LFR trained DFSMN can achieve more than 20% relative improvement compared to the LFR trained BLSTM. Moreover, we can easily design the lookahead filter order of the memory blocks in DFSMN to control the latency for real-time applications.", "text": "paper present improved feedforward sequential memory networks architecture namely deep-fsmn introducing skip connections memory blocks adjacent layers. skip connections enable information across different layers thus alleviate gradient vanishing problem building deep structure. result dfsmn signiﬁcantly beneﬁts skip connections deep structure. compared performance dfsmn blstm without lower frame rate several large speech recognition tasks including english mandarin. experimental results shown dfsmn consistently outperform blstm dramatic gain especially trained using cd-phone modeling units. hours fisher task proposed dfsmn achieve word error rate purely using crossentropy criterion decoding -gram language model achieves absolute improvement compared blstm. hours mandarin recognition task trained dfsmn achieve relative improvement compared trained blstm. moreover easily design lookahead ﬁlter order memory blocks dfsmn control latency real-time applications. recently deep neural networks become dominant acoustic models large vocabulary continuous speech recognition systems. depending networks connected exist various types deep neural networks feedforward fully-connected neural networks convolutional neural networks recurrent neural networks opposed fnns learn ﬁxed-size input ﬁxed-size output rnns learn model sequential data extended period time store memory network weights carry rather complicated transformations sequential data. thereby researchers paid attention rnns especially long short-term memory networks widely observed lstm variations signiﬁcantly outperform fnns various acoustic modeling tasks. rnns theoretically powerful learning rnns usually relies so-called back-propagation time internal recurrent cycles. bptt signiﬁcantly increases computational complexity learning even worse cause many problems learning gradient vanishing exploding alternative feedforward architecture proposed model long-term dependency. straightforward attempt so-called unfolded unfolded time ﬁxed number time steps. unfolded needs comparable training time standard fnns achieving better performance fnns. time delay neural network another popular feedforward architecture efﬁciently model long temporal contexts. recently proposed simple non-recurrent structure namely feedforward sequential memory networks effectively model long term dependency sequential data without using recurrent feedback. experimental results acoustic modeling language modeling tasks shown fsmn signiﬁcantly outperform recurrent neural networks models learned much reliably faster. furthermore variant fsmn architecture namely compact fsmn proposed simplify fsmn architecture speed learning. work based previous fsmn works recent works neural networks deep architecture presented improved fsmn structure namely deep-fsmn introducing skip connections memory blocks adjacent layers. skip connections enable information across different layers thus alleviate gradient vanishing problem building deep structure. moreover considering demand real-world applications propose combine dfsmn lower frame rate technology speed decoding optimize dfsmn topology meet latency requirement. previous fsmn works evaluated popular hours switchboard task. work evaluate performance dfsmn several much larger speech recognition tasks including english mandarin. firstly hours english fisher task proposed dfsmn much smaller model size achieve absolute word error rate reduction compared popular blstm. furthermore hours mandarin task trained dfsmn achieved relative improvement compared latency controlled blstm importantly easily design lookahead ﬁlter order memory blocks dfsmn match latency demand real-time applications. experiments trained dfsmn frames delay still outperform trained lcblstm frames delay. fsmn proposed inspired ﬁlter design knowledge digital signal processing inﬁnite impulse response ﬁlter well approximated using high-order ﬁnite impulse response ﬁlter. recurrent layer rnns shown figure fsmn cfsmn remain pure feedforward structure efﬁciently learned using standard back-propagation mini-batch based stochastic gradient descent previous cfsmn introduced section standard hidden layer decomposed layers using low-rank weight matrix factorization. thereby cfsmn cfsmn-layers layers total number layers want train deeper cfsmn directly adding cfsmn-layers suffer gradient vanishing problem. inspired recent works training deep neural architectures skip connection residual highway networks propose improved fsmn architecture namely deep-fsmn work architecture dfsmn shown figure skip connections memory blocks standard cfsmn output lower layer memory block directed higher layer memory block. backpropagation gradients higher layer also assigned directly lower layer help overcome gradient vanishing problem. formulation memory block dfsmn takes following form here denotes linear output linear projection layer. denotes output memory block. denotes look-back order lookahead order memory block respectively. denotes skip connection within memory block linear nonlinear transformation. example dimensions memory blocks same identity mapping following speech signal information adjacent frames strong redundancy overlap. similar dilated convolutional layer wavenet stride factors memory block order remove redundancy. conceptually viewed ﬁrst-order ﬁlter precisely approximated high-order ﬁlter. therefore fsmn extends standard feedforward fully connected neural networks augmenting memory blocks adopt tapped-delay line structure ﬁlters hidden layers. instance figure shows fsmn memory block added hidden layer. learnable fir-like memory blocks fsmns used encode long context information ﬁxed-size representation helps model capture long-term dependency. moreover several memory blocks multiple hidden layers deep neural network capture context information various abstraction levels. depending encoding method used proposed versions fsmns namely scalar fsmns vectorized fsmns vfsmn formulation memory block takes following form denotes element-wise multiplication equally-sized vectors. called look-back order denoting number historical items looking back past called lookahead order representing size lookahead window future. output memory block regarded ﬁxed-size representation long surrounding context time instance shown figure next hidden layer result calculate activation units next hidden layer follows considering additional parameters introduced memory blocks variant fsmn architecture namely compact fsmn proposed simplify fsmn architecture speed learning. shown figure cfsmn single cfsmn-layer layer. compared standard fsmn cfsmn viewed inserting smaller linear projection layer nonlinear hidden layers adding memory blocks linear projection layers instead hidden layers. encoding formulation memory block cfsmn takes following form models trained distributed manner using bmuf optimization gpus frame-level cross entropy criterion. initial learning rate momentum kept cfsmn mini-batch blstm model trained using standard full-sequence bptt mini-batch sequences. performances baseline models shown table trained dfsmn various architectures denoted ]-nd number cfsmnlayer relu layer respectively. experiments kept ﬁxed. ﬁrst experiment investigated inﬂuence number cfsmn-layers size stride ﬁnal speech recognition performance. trained cfsmn eight twelve cfsmn-layers. detailed architectures experimental results listed table here dfsmn denotes dfsmn results indicate advantage using stride memory block. achieve consistent performance improvement using deeper architecture. table summarized experimental results various systems hours task. implementation blstm achieve relative improvement compared baseline system. comparison well-trained blstm hidden layers achieves decoding gram language model. proposed dfsmn achieve purely using cross-entropy criterion without feature space speaker space adaptation technologies competitive performance task. compared baseline blstm system proposed dfsmn achieve absolute reduction smaller model size. mandarin recognition task evaluated performance proposed dfsmn tasks namely -hourtask -hour-task consist hours hours training data respectively. -hour-task subset full -hour-task. training data collected many domains sport tourism game literature test contains hours data used evaluate performance models. evaluation performed term character error rate sample rate data khz. acoustic feature used experiments -dimensional log-mel ﬁlterbank energies computed window shift. speech recognition system applied real-time applications essential control latency. dfsmn easily design lookahead ﬁlters order stride meet demand latency. experiments evaluate performance dfsmn different latency. english recognition task standard fisher task training consists hours data switchboard fisher evaluation performed term word error rate switchboard part standard nist evaluation denoted hube. input speech sampled analyzed using hamming window ﬁxed frame shift. computed -dimensional ﬁlter-bank features includes energy coefﬁcients distributed scale along ﬁrst second temporal derivatives. experiments task tri-gram language model trained million words training transcripts million words fisher english part transcripts. hybrid dnn-hmm baseline system follow training procedure described train conventional context dependent dnn-hmm using tied-state alignment obtained trained gmm-hmm baseline system. contains hidden layers rectiﬁed linear units layer. inputs stacked feature context window size hybrid blstm-hmm baseline system trained deep blstm following conﬁgurations blstm consists three blstm layers blstm layer followed low-rank linear recurrent projection layer units. cfsmn based system trained cfsmn architecture ∗-×]-×--. inputs -dimensional features context window cfsmn consists cfsmn-layers followed relu hidden layers linear projection layer. conventional hybrid models cd-state existing cross-entropy trained hybrid cd-dnn-hmm system used realign generate frame-level targets. consists cd-states. baseline hybrid cd-lcblstmhmm system follow well-tuned conﬁguration train lcblstm respectively. baseline lcblstm consists blstm layers relu layers softmax output layer. cfsmn based model trained cfsmn architecture being inputs -dimensional features context window lcblstm cfsmn respectively. trained hybrid models cd-phone ﬁrstly cd-states cd-phones subsample averaging one-hot target labels producing soft targets. baseline trained lcblstm system similar model architecture baseline system respectively. trained cfsmn models trained cfsmns eight cfsmn-layers denoted lfr-cfsmn lfr-cfsmn lfr-cfsmn respectively. inputs -dimensional features context window lcblstm cfsmn respectively. trained dfsmn model model topology denoted ]-nd experiments evaluate performance lfr-dfsmn denoted lfrdfsmn lfr-dfsmn respectively. models trained distributed manner using bmuf optimization gpus frame-level cross entropy criterion. listed experimental results table comparison. compared baseline cd-state trained lcblstm cfsmn achieve relative improvement. lfr-cfsmn cd-phone achieve absolute reduction compared cfsmn cd-state shows beneﬁt modeling units. cfsmn dfsmn beneﬁt deep architecture dfsmn outperform cfsmn similar model topology. however deep cfsmn suffer performance degradation dfsmn achieve consistent improvement. moreover proposed lfr-dfsmn signiﬁcantly outperform lfr-lcblstm relative reduction. train much deeper lcblstm achieve better performance highwaylstm however improvement limited without increasing total parameters relatively improvement. table compared training time decoding real-time factor trained dfsmn lcblstm. results shown dfsmn achieve times speedup training decoding real-time factor task compare performance lfr-lcblstm lfr-dfsmn large corpus consists hours training data. lfr-lcblstm conﬁgurations -hour-task. lfr-dfsmn trained dfsmn model topology ]-×-- denoted lfr-dfsmn. number fsmn layers layers lookback ﬁlter order investigate inﬂuence lookahead ﬁlter order performance. models trained distributed manner using bmuf optimization gpus frame-level cross entropy criterion. baseline lfr-lcblstm number delay frame time instance lfr-dfsmn control number delay frame setting lookahead ﬁlter order. experimental results table shown reduce number delay frame loss performance. result latency suitable real-time applications. finally proposed dfsmn frames latency achieve relative improvement compared lcblstm frames latency. presented improved fsmn structure namely deep-fsmn applied many large speech recognition tasks. dfsmn signiﬁcantly beneﬁt skip connections deeper architecture. experimental results shown dfsmn consistently outperform blstm dramatic gain especially combined lower frame rate. hours fisher english task proposed dfsmn achieve purely using cross-entropy criterion without adaptation technology. hours mandarin recognition task trained dfsmn achieve relative improvement compared trained lcblstm smaller model size lower latency. experiments results suggest dfsmn strong alternative blstm acoustic modeling. abdel rahman mohamed george dahl geoffrey hinton acoustic modeling using deep belief networks audio speech language processing ieee transactions vol. george dahl dong deng alex acero contextdependent pre-trained deep neural networks large vocabulary speech recognition audio speech language processing ieee transactions vol. ossama abdel-hamid abdel rahman mohamed jiang gerald penn applying convolutional neural networks concepts hybrid nn-hmm model speech recognition acoustics speech signal processing ieee international conference ossama abdel hamid abdel rahman mohamed jiang deng gerald penn dong convolutional neural networks speech recognition audio speech language processing ieee/acm transactions vol. graves mohamed hinton speech recognition deep recurrent neural networks proceedings ieee international conference acoustics speech signal processing kalchbrenner danihelka alex graves grid long short-term memory arxiv preprint arxiv. tara sainath modeling time-frequency patterns lstm convolutional architectures lvcsr tasks. interspeech bengio simard frasconi learning long-term dependencies gradient descent difﬁcult ieee transactions neural networks vol. saon soltau emami picheny unfolded recurrent neural networks speech recognition proceedings interspeech vijayaditya peddinti daniel povey sanjeev khudanpur time delay neural network architecture efﬁcient modeling long temporal contexts proceedings interspeech vijayaditya peddinti guoguo chen daniel povey sanjeev khudanpur reverberation robust acoustic modeling using i-vectors time delay neural networks proceedings interspeech alexander waibel toshiyuki hanazawa geoffrey hinton kiyohiro shikano kevin lang phoneme recognition using time-delay neural networks acoustics speech signal processing ieee transactions vol. shiliang zhang jiang shifu xiong li-rong compact feedforward sequential memory networks large vocabulary continuous speech recognition. interspeech shiliang zhang cong jiang lirong nonrecurrent neural structure long-term dependence ieee/acm transactions audio speech language processing vol. zhang guoguo chen dong kaisheng yaco sanjeev khudanpur james glass highway long short-term acoustics memory rnns distant speech recognition speech signal processing ieee international conference ieee shaofei zhijie improving latency-controlled blstm acoustic models online speech recognition acoustics speech signal processing ieee international conference ieee aaron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu wavenet generative model audio arxiv preprint arxiv. wayne xiong jasha droppo xuedong huang frank seide mike seltzer andreas stolcke dong geoffrey zweig achieving human parity conversational speech recognition arxiv preprint arxiv. shiliang zhang jiang li-rong rectiﬁed linear neural networks tied-scalar regularization lvcsr sixteenth annual conference international speech communication association chen qiang scalable training deep learning machines incremental block training intra-block parallel optimization blockwise model-update ﬁltering acoustics speech signal processing ieee", "year": 2018}