{"title": "Classification via Tensor Decompositions of Echo State Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "This work introduces a tensor-based method to perform supervised classification on spatiotemporal data processed in an echo state network. Typically when performing supervised classification tasks on data processed in an echo state network, the entire collection of hidden layer node states from the training dataset is shaped into a matrix, allowing one to use standard linear algebra techniques to train the output layer. However, the collection of hidden layer states is multidimensional in nature, and representing it as a matrix may lead to undesirable numerical conditions or loss of spatial and temporal correlations in the data.  This work proposes a tensor-based supervised classification method on echo state network data that preserves and exploits the multidimensional nature of the hidden layer states. The method, which is based on orthogonal Tucker decompositions of tensors, is compared with the standard linear output weight approach in several numerical experiments on both synthetic and natural data. The results show that the tensor-based approach tends to outperform the standard approach in terms of classification accuracy.", "text": "abstract—this work introduces tensor-based method perform supervised classiﬁcation spatiotemporal data processed echo state network. typically performing supervised classiﬁcation tasks data processed echo state network entire collection hidden layer node states training dataset shaped matrix allowing standard linear algebra techniques train output layer. however collection hidden layer states multidimensional nature representing matrix lead undesirable numerical conditions loss spatial temporal correlations data. work proposes tensor-based supervised classiﬁcation method echo state network data preserves exploits multidimensional nature hidden layer states. method based orthogonal tucker decompositions tensors compared standard linear output weight approach several numerical experiments synthetic natural data. results show tensor-based approach tends outperform standard approach terms classiﬁcation accuracy. echo state networks ﬁrst introduced name liquid state machines shown effective performing classiﬁcation temporal prediction tasks spatiotemporal data including diverse tasks speech recognition chaotic timeseries prediction forming objective function reinfocement learning methods esns special type recurrent neural network typically hidden layer trained using computational expensive back propagation method esns however weights hidden layer randomly assigned sparse random connections among nodes. hidden layer often called reservoir collection node values called reservoir states. advantages esns offer traditional rnns include much faster training time conﬁguration require retraining applications different datasets. esns output layer trained particular task training generally produces linear output weights using regularized least squares approach recently output layer replaced classiﬁcation scheme based principal components reservoir states approach showed promise improving classiﬁcation accuracy robust noisy perturbations input data traditional linear trained output weights approach. however collection reservoir states generated multidimensional nature. trained linear output weights principal components approaches require superﬁcially ﬂatten reservoir data matrices potentially eliminating weakening spatial temporal correlations present data process. additionally ﬂattening reservoir data result overdetermined system especially trained linear output weights approach yield overtrained overly sensitive results. example work proposes tensor-based supervised classiﬁcation method reservoir states esns. tensors multidimensional arrays natural structures representing collections reservoir data. rather using reservoir data tensors approximated using tucker decomposition mode factor matrix smaller rank along associated core tensor extracts higher order features data. decomposition enables reduce complexity training tensor data keeping signiﬁcant contributions preserving multidimensional correlations among features. following notation appears work. tensors written script capital letters e.g. matrices appear capital latin letters e.g. vectors scalars lower case latin greek letters e.g. elements array given ‘matlab’ notation. thus element matrix element column denoted vector determined extracting column denoted similar notation holds lower higher-order arrays. usual matrix-matrix multiplication represented writing matrices adjacent another. contrast modal tensor-matrix multiplication deﬁned section superscripts denote indices powerwise multiplication except base hermitian transpose matrix given finally denote vector length clear context k-th position zeros elsewhere. remainder paper organized follows. section discusses background information esns relevant tensor decompositions. section describes proposed classiﬁcation method using tensor decompositions tensor higher-order analogue vector; vector ﬁrst-order tensor matrix second-order tensor tensors natural structures represent investigate multidimensional data. example video considered third-order tensor ﬁrst modes describing pixel coordinates single frame third mode representing time variations. tensors used represent explore relationships among data diverse research areas applications including video processing multiarray signal processing independent component analysis others. challenge employing tensor methods volume data suffers so-called ‘curse dimensionality’ amount data increases exponentially additional mode naive tensor methods quickly become intractable. help alleviate challenge various approaches proposed including several types low-rank sparse representations work employ orthogonal tucker- decomposition. discussing decomposition deﬁnitions need introduced. ri×i×···×in th-order tensor mode dimension rjn×in matrix. mode product nth-order tensor ri×i×···×in−×jn×in+×···×in whose entries given core tensor rj×j×i original tensor order greater written using factor matrices. illustration decomposition shown figure type decomposition used perform feature extraction classiﬁcation collection -dimensional signals third mode tensors correspond individual samples training dataset. note decomposition basis matrices universal across entire tensor classiﬁcation tasks method generate different basis matrices distinct classes data set. decomposition classiﬁcation methods using discussed section echo state network data. spatiotemporal data processed echo state network follows. rl×t input spatial dimension temporal length values hidden layer nodes also referred reservoir states input denoted determined recursion traditionally collection linear output weights trained unfolded collection training reservoir tensors. |tr| unfolding along third mode. written contatenation input belongs class time wout describes data well wouta observation drives classiﬁcation scheme. input test reservoir states predicted belong class time maximal element vector woutx entry. similarly predicted belong class overall vector training output weights fast however approach weakensses. system typically overdetermined. although regularization tries overcome this matrix several orders magnitude columns rows practice poorly represent data even regularization. controlled model selecting subset times sample reservoir. common method single point however accuracy results suffer greatly even nonlinear activation function ﬁxed input weights wres ﬁxed randomly assigned reservoir weights bias value leaking rate. work output feedback recursion sometimes used literature since incompatible using proposed tensor approach. collection training inputs class. suppose inputs processed weights parameters. denote reservoir states input reservoir states concatenated along third mode tensor ×|trk| though reservoir states hold ‘memory’ previous states using subset data generally results information loss. moreover unfolding procedure loses temporal correlations reservoir node behavior. finally using single linear output weight simply insufﬁcient separate classes dataset well shown alleviate deﬁciencies encountered using method paper proposes using classiﬁcation method based decomposition tensor training reservoirs. approximations orthogonal tucker decompositions tensors factors orthogonal columns rj×j×|tr| rj×j×trk core tensors. core tensors intrepreted entry describing strength feature reservoir states input captured interaction bases approximate tucker- decomposition form higher-order orthogonal iteration algorithm ﬁrst introduced explored completeness include pseudocode algorithm similar procedure used obtain decompositions work randomly initialize reasons. first ﬁnding dominant subspaces unfolded matrices computationally intensive task large. practice step intractable even steps algorithm computable. second noted experiments randomly initializing factors results additional iterations. algorithm uses stopping criterion based convergence singular values found steps avoid problems difference signs using criterion based factor matrices. perform classiﬁcation using decompositions obtained collections reservoir states training set. suppose input signal reservoir states although matrix rather three dimensional tensor still expressed terms factor matrices tucker decompositions. indeed frontal slice form collection reservoir states training place reasonable assume inputs class similar reservoir responses therefore also produce similar frontal slices core tensors. therefore predict input belongs class slices describe well. class section results numerical experiments given comparing classiﬁcation accuracy using esns linear output weight approach proposed tensorbased classiﬁcation methods three datasets used. ﬁrst dataset uses inputs randomly switch sine wave square wave segments. second dataset subset usps collection handwritten digits. ﬁnal dataset collection cepstrum coefﬁcients audio recordings speakers saying japanese vowel ‘ae’. experiments performed matlab ram. several parameter combinations considered dataset experiments repeated number times combination. randomizations weights wres generating training testing datasets reselected experiment however held constant classiﬁcation methods within single experiment. test element segments length inclassiﬁed according pattern test reservoir states collection states corresponding input segment segment classiﬁed ‘sine’ wave ‘square’ wave segment inequality sign ﬂipped. trained linear output weights-based classiﬁcation generate single matrix wout equation perform classiﬁcation test pointwise block-wise input segment equation mean standard deviation percent classiﬁcation accuracy results using methods simulations several parameter choices displayed table tensor-based classiﬁcation method columns labeled ‘tensor’ achieved accuracy every simulation training testing datasets. pointwise blockwise output weight classiﬁcation methods columns labeled ‘weights ‘weights respectively achieved good classiﬁcation accuracy parameter choices poor results others. block-wise method sensitive number nodes bias choice particular achieved near-perfect test classiﬁcation accuracy chosen well. hand pointwise output weights method achieved near perfect results chosen well sufﬁciently large. although instance-based classiﬁer classiﬁcation accuracy test guaranteed. individual sine square segments indeed identical whether training testing set. however resulting reservoir states segments distinct memory reservoir. reservoir resting state accepting segments sequence initial state continue propagating reservoir time. ouput weight wout includes contamination trained however tensor decomposition method capture contribution initial state small number factors focusing primarily reservoir behavior stemming input itself. overall tensorbased approach outperformed output weights method achieved higher classiﬁcation accuracy test parameter choices. sample training input shown figure dataset type studied previous work including matrix principal component output layer classiﬁcation methods study using photonic reservoirs. formed generating disinput patterns containing segments tinct randomly placed sine square waves. test contains distinct patgenerated similarly simulations terns. parameters used {tanh sin} experiments repeated times randomizations wres training testing sets. tensor-based classiﬁcation used method ranks ⌊n/⌋ choice justiﬁed dataset rather simple; type square wave type sine wave used generating samples. table results ‘sine squre wave’ example. entries represent mean standard deviation classiﬁcation accuracy trials. training testing classiﬁcation accuracy shown standard linearout output weight approaches esns along proposed tensor-based classiﬁcation method. several parameters included. collection experiments classiﬁcation performed grayscale images handwritten digits. dataset partitioned classes representing digits samples images shown figure images treated spatiotemporal signals coordinates corresponding spatial dimension coordinates corresponding temporal dimension. training dataset images class randomly selected paired indicator matrix input belongs class. test formed similarly distinct images class test training overlap samples. trained linear output weights-based classiﬁcation single output weight matrix wout generated equation simulation. classiﬁcation performed entire collection reservoir states test input equation results simulations presented figure results using hooi algorithm displayed blue results using linear output weights shown red. maximal mean accuracy pairs shown tensor-based approach. note tensor-based approach consistently yields higher classiﬁcation accuracy trained linear output weight approach. although results competitive state-of-the-art particular dataset show tensor-based classiﬁcation method yields higher results standard techniques. collection experiments speaker identiﬁcation performed using samples audio recordings. dataset contains samples nine male speakers saying japanese vowel ‘ae’. data split training samples utterances speaker test samples utterances speaker. sample array fig. average percent classiﬁcation accuracy simulations usps dataset using tensor-based hooi algorithm linear output weights method vertical lines represent standard deviation mean. cepstrum coefﬁcients temporal length sample using cepstrum coefﬁcients bias terms. dataset ﬁrst appearing obtained popular machine learning literature test accuracy reported using ensemble classiﬁer esns four leaky-integrator nodes. examples below single nodes nonlinear activation function bias classiﬁers found using algorithm collection reservoir states corresponding training inputs. test inputs modiﬁed adding gaussian noise classiﬁcation performed resulting reservoir states using trained linear output weights tensor method. pair simulations performed randomizations wres simulation. test classiﬁcation accuracy results displayed figure ﬁgure blue lines correspond tensor-based method lines correspond linear output weight method. individual lines within method correspond different levels noise added test inputs. x-axis number reservoir nodes points lines give mean accuracy simulations vertical lines represent standard deviation mean. ﬁgure classiﬁcation accuracy initially decreases increases eventually improve large enough consistent results published methods degrade level added noise increases however tensor-based method consistently yielded better accuracy results. tensor-based approach higher mean classiﬁcation accuracy parameter choices standard deviation smaller indicating results less sensitive randomizations wout win. work introduced tensor-based method perform supervised classiﬁcation spatiotemporal data processed esn. numerical experiments demonstrate proposed method outperform traditional trained linear output weights method terms classiﬁcation accuracy. future directions include investigating types tensor decompositions including rank polyadic sparse decompositions well using types instancebased classiﬁers resulting decompositions. bush anderson. modeling reward functions incomplete state representations echo state networks. ieee international joint conference neural networks cichocki mandic phan caiafa zhou zhao lathauwer. tensor decomopsitions signal processing applications two-way multiway component analysis. ieee signal processing magazine goudarzi banda lakin teuscher stefanovic. comparitive study reservoir computing temporal signal processing. technical report university mexico fig. percent classiﬁcation test accuracy japanese vowel dataset. results plotted number nodes reservoir classiﬁcation methods several amounts noise added test sets. points along lines represent mean accuracy simulations vertical lines represent standard deviation mean. jaeger. ‘echo state’ approach analysing training recurrent neural networks erratum note. technical report report number fraunhofer institute autonomous intelligent systems jaeger. long short-term memory echo state networks details simulation study. technical report jacobs university bremen jaeger lukosevicius popovici siewert. optimization applications echo state networks leaky-integrator neurons. neural networks prater shen tang. overcomplete tensor decomposition convex optimization. computational advances multisensor adaptive processing ieee international workshop page skowronski natschl ager markram. minimum mean squared error time series classiﬁcation using echo state network prediction model. proceedings icsas strickert. self-organizing neural networks sequence processing. thesis univ. osnabr¨uck department computer science szita gyenes lorincz. reinforcement learning echo verstraeten schrauwen stroobandt capenhout. isolated word recognition liquid state machine case study. information processing letters special issue applications spiking neural networks", "year": 2017}