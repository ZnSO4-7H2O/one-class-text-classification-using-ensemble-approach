{"title": "ADASECANT: Robust Adaptive Secant Method for Stochastic Gradient", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Stochastic gradient algorithms have been the main focus of large-scale learning problems and they led to important successes in machine learning. The convergence of SGD depends on the careful choice of learning rate and the amount of the noise in stochastic estimates of the gradients. In this paper, we propose a new adaptive learning rate algorithm, which utilizes curvature information for automatically tuning the learning rates. The information about the element-wise curvature of the loss function is estimated from the local statistics of the stochastic first order gradients. We further propose a new variance reduction technique to speed up the convergence. In our preliminary experiments with deep neural networks, we obtained better performance compared to the popular stochastic gradient algorithms.", "text": "stochastic gradient algorithms main focus large-scale learning problems important successes machine learning. convergence depends careful choice learning rate amount noise stochastic estimates gradients. paper propose adaptive learning rate algorithm utilizes curvature information automatically tuning learning rates. information element-wise curvature loss function estimated local statistics stochastic ﬁrst order gradients. propose variance reduction technique speed convergence. preliminary experiments deep neural networks obtained better performance compared popular stochastic gradient algorithms. paper develop stochastic gradient algorithm reduces burden extensive hyper-parameter search optimization algorithm. proposed algorithm exploits variance estimator curvature cost function uses obtain automatically tuned adaptive learning rate parameter. deep learning numerical optimization several papers suggest using diagonal approximation hessian order estimate optimal learning rates stochastic gradient descent high dimensional parameter spaces fundamental advantage using approximation inverting trivial cheap operation. however generally neural networks inverse diagonal hessian usually approximation diagonal inverse hessian. examples options obtaining diagonal approximation hessian gauss-newton matrix ﬁnite differences estimations however sensitive noise stochastic gradient. suggested reliable estimate local curvature stochastic setting keeping track variance average gradients. paper followed different approach instead using diagonal estimate hessian proposed estimate curvature along direction gradient apply variance reduction technique compute reliably. using root mean square statistics variance gradients reduced adaptively simple transformation. keep track estimation curvature using technique similar proposed uses variability expected loss. standard adaptive learning rate algorithms scale gradients regular newton-like second order methods perform complicate transformations e.g. rotating gradient vector. newton quasi-newton methods also invariant afﬁne transformations parameter space. proposed adasecant algorithm basically stochastic rank- quasi-newton method. comparison adaptive learning algorithms instead scaling gradient parameter adasecant also perform afﬁne transformation them. directional newton method proposed solving equations multiple variables advantage directional newton method proposed compared newton’s method that require matrix inversion still maintains quadratic rate convergence. parameter vector update objective function unit vector direction optimization algorithm follow. denoting hessian matrix ∇θif element gradient vector update reformulation equation diagonal element step-size diag neural network layers. block normalization gradient adds additional noise practice observe negative impact conjecture angle stochastic gradient block-normalized gradient still less degrees. update step deﬁned per-parameter learning rate variance reduction techniques stochastic gradient estimators well-studied machine learning literature. proposed ways dealing problem. paper proposed variance reduction technique stochastic gradient descent relies basic statistics related gradient. refer element gradient vector respect parameters expectation taken minibatches different trajectories parameters. adapting possible control inﬂuence high variance unbiased variance biased ˜gi. denoting stochastic gradient obtained next minibatch well balances inﬂuences keeps close possible true gradient result order estimate dimension keep track estimation i−e))] training. necessary sufﬁcient condition here variance reduction keep positive achieve positive estimate used root mean square statistics expectations. used compute secant update taken minibatches past values parameters. computing expectation numerically unstable stochastic setting. decided stable second ∇θif ∇θif. compute moving averages also adopted used algorithm dynamically decide time constant based step size taken. result algorithm used give bigger weights updates large step-size smaller weights updates smaller step-size. assuming moving average update rule written algorithm similar instead incrementing outlier detected time-constant reset note assigns approximately amount weight current average previous observations. mechanism made learning stable without outlier gradients saturate large value. correction parameters allows ﬁne-grained variance reduction parameter independently. noise stochastic gradient methods advantages terms generalization optimization. introduces exploration exploitation trade-off controlled upper bounding values value thresholded block-wise normalized gradients weight matrix bias vectors compute described section makes adasecant scale-invariant thus robust scale inputs number layers network. observed empirically easier train deep neural networks block normalized gradient descent. learning rate decrease noise estimation adaptive step-sizes adasecant convergence would guaranteed. ensure developed variant adagrad thresholding scaling factor lower bounded assuming accumulated norm past gradients parameter update thresholded ensuring algorithm converge initial stages training accumulated norm per-parameter gradients less accumulated perparameter norm gradient less adagrad augment learning-rate determined adasecant update i.e. per-parameter learning rate determined adasecant. behavior tends create unstabilities training adasecant. modiﬁcation adagrad algorithm ensure that reduce learning rate determined adasecant algorithm update i.e. learning rate bounded. beginning training parameters neural network -valued gradients e.g. existence dropout relu units. however phenomena cause per-parameter learning rate scaled adagrad unbounded. performed experiments mnist maxout networks comparing adasecant popular stochastic gradient learning algorithms adagrad rmsprop adadelta sgd+momentum results summarized figure appendinx showed adasecant converges fast faster techniques including hand-tuned global learning rate momentum rmsprop adagrad. described stochastic gradient algorithm adaptive learning rates fairly insensitive tuning hyper-parameters doesn’t require tuning learning rates. furthermore variance reduction technique proposed improves convergence stochastic gradients high variance. according preliminary experiments presented able obtain better training performance compared popular well-tuned stochastic gradient algorithms. future work comprehensive analysis help better understand algorithm analytically empirically. acknowledgments thank developers theano pylearn computational resources provided compute canada calcul qu´ebec. work partially supported nserc cifar canada research chairs project grant -sgr-. would like thank schaul valuable discussions. also thank kyunghyun orhan firat proof-reading giving feedbacks paper.", "year": 2014}