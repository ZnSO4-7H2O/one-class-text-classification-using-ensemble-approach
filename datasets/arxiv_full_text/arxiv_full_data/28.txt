{"title": "ALICE: Towards Understanding Adversarial Learning for Joint Distribution  Matching", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "cs.NE"], "abstract": "We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.", "text": "investigate non-identiﬁability issues associated bidirectional adversarial training joint distribution matching. within framework conditional entropy propose adversarial non-adversarial approaches learn desirable matched joint distributions unsupervised supervised tasks. unify broad family adversarial models joint distribution matching problems. approach stabilizes learning unsupervised bidirectional adversarial learning methods. further introduce extension semi-supervised learning tasks. theoretical results validated synthetic data real-world applications. deep directed generative models powerful framework modeling complex data distributions. generative adversarial networks implicitly learn data generating distribution; speciﬁcally learn sample order this trains generator mimic real samples learning mapping latent space data space. concurrently discriminator trained distinguish generated real samples. idea behind discriminator ﬁnds difﬁcult distinguish real artiﬁcial samples generator likely good approximation true data distribution. standard form yields one-way mapping i.e. lacks inverse mapping mechanism preventing able inference. ability compute posterior distribution latent variable conditioned given observation important data interpretation downstream applications efforts made simultaneously learn efﬁcient bidirectional model produce high-quality samples latent data spaces among them recently proposed adversarially learned inference casts learning bidirectional model gan-like adversarial framework. speciﬁcally discriminator trained distinguish joint distributions real data sample inferred latent code real latent code generated data sample. inspiring elegant approach tends produce reconstructions necessarily faithful reproductions inputs seeks match joint distributions dependency structure random variables within joint speciﬁed constrained. practice results solutions satisfy ali’s objective able produce real-looking samples difﬁculties reconstructing observed data also difﬁculty discovering correct pairing relationship domain transformation tasks paper ﬁrst describe non-identiﬁability issue ali. solve problem propose regularize using framework conditional entropy hence call proposed approach alice. adversarial learning schemes proposed estimate conditional entropy unsupervised supervised learning paradigms. provide uniﬁed view family recently proposed models perspective joint distribution matching including cyclegan conditional extensive experiments synthetic real data demonstrate alice signiﬁcantly stable train consistently yields viable solutions without sensitive perturbations model architecture i.e. hyperparameters. also show alice results faithful image reconstructions. further framework leverage paired data semi-supervised tasks. empirically demonstrated discovery relationships cross domain tasks based image data. background consider general marginal distributions domain inferred based using conditional distributions further combined structure domains characterized joint distributions generate samples random variables adversarial methods provide sampling mechanism requires gradient backpropagation without need specify conditional densities. speciﬁcally instead sampling directly desired conditional distribution random variable generated deterministic transformation inputs variable source domain independent noise e.g. gaussian distribution. without loss generality universal distribution approximator speciﬁcation i.e. sampling procedure conditionals carried following generating processes generators speciﬁed neural networks parameters respectively. practice inputs simple concatenations respectively. note implies parameterized respectively hence subscripts. denotes true distribution data speciﬁed simple parametric distribution e.g. isotropic gaussian. order matching trains ω-parameterized adversarial discriminator network distinguish samples formally minimax objective given following expression lgan ex∼q)] e˜x∼pθ z∼p))] sigmoid function. following lemma characterizes solutions terms marginals lemma optimal decoder discriminator parameterized saddle point objective alternatively matches joint distributions using adversarial discriminator network similar parameterized minimax objective written lemma optimum generators discriminator parameters lemma solution achieved guaranteed marginals conditional distributions pair match. note implies match; however imposes restrictions conditionals. identiﬁability issues described below. adversarial learning information measures relationship random variables speciﬁed constrained ali. result possible matched distribution undesirable given application. illustrate issue figure shows solutions objective simple problem. data latent random variables take possible values respectively. case marginals known i.e. matched joint distribution represented contingency table. figure represents possible solutions objective figures represent opposite extreme solutions respectively. note although generate realistic values sample poor reconstruction ability since sequence easily result exceptions model achieve perfect reconstruction correspond illustrated figures respectively. simple example ﬂexibility joint distribution quite likely obtain undesirable solution objective. instance poor reconstruction ability single instance potentially possible value e.g. figure generate either equal probability. many applications require meaningful mappings. consider scenarios figure illustration possible solutions objective. ﬁrst shows mappings domains second shows matched joint distribution contingency tables parameterized inferred corresponding reconstruct high probability. figure corresponds either figures mapping e.g. image tagging images tags. case paired samples desired joint distribution usually available thus leverage supervised information resolve ambiguity figure simple example figure order alleviate identiﬁability issues associated solutions objective impose constraints conditionals furthermore fully mitigate identiﬁability issues require supervision i.e. paired samples domains deal problem undesirable matched joint distributions propose information-theoretic measure regularize ali. done controlling uncertainty pairs random variables i.e. using conditional entropies. uncertainty given linked fact deterministic mapping intuitively controlling uncertainty restrict solutions objective joint distributions whose mappings result better reconstruction ability. therefore propose denoted regularization term framework termed conditional entropy deﬁned following minimax objective dependent underlying distributions random variables parametrized made clearer below. ideally could select desirable solutions evaluating saddle points objective identiﬁed. however practice intractable access saddle points beforehand. below propose approximate training unsupervised supervised tasks. since symmetric terms according derive theoretical results. similar arguments hold discussed supplementary material unsupervised learning absence explicit probability distributions needed computing bound using criterion cycle-consistency denote reconstruction generating procedure desire high likelihood begins cycle hence similar original lemma shows cycle-consistency upper bound conditional entropy lemma joint distributions lcycle. note reaches optimum reach saddle point lcycle accordingly thus effectively approaches unlike upper bound lcycle easily approximated monte carlo simulation. importantly readily added ali’s objective without additional changes original training procedure. cycle-consistency property previously leveraged cyclegan discogan dualgan however cycle-consistency lcycle implemented losses real-valued data images. consequence -based pixel-wise loss generated samples tend blurry recognizing limitation suggest enforce cycle-consistency using fully adversarial training alternative lcycle speciﬁcally reconstruct specify η-parameterized discriminator distinguish reconstruction finally fully adversarial training algorithm unsupervised learning using alice framework cycle thus ﬁxed maximize result replacing lcycle paired samples critical. encourages generators mimic reconstruction relationship implied ﬁrst joint; contrary model reduce basic discussed section generate realistic sample objective enjoys many theoretical properties gan. particularly proposition guarantees existence optimal generator discriminator. corollary cycle-consistency satisﬁed achieved) deterministic mapping enforces qφpθ)] indicates conditionals matched. contrary matched conditionals enforce indicates corresponding mapping becomes deterministic. semi-supervised learning objective optimized unsupervised identiﬁability issues associated largely reduced cycle-consistency-enforcing bound lemma means samples training data probabilistically paired high certainty conditionals though perhaps desired conﬁguration. realworld applications obtaining correctly paired data samples entire dataset expensive even impossible. however situations obtaining paired data small subset observations feasible. case leverage small empirically paired samples provide guidance selecting correct conﬁguration. suggests alice suitable semi-supervised classiﬁcation. paired sample drawn empirical distribution desirable joint distribution well speciﬁed. thus directly approximate note reaches optimum reach saddle point lmap accordingly thus approaches employ standard losses supervised learning objectives approximate lmap cross-entropy loss alternatively also improve generation ability propose adversarial learning scheme directly match paired empirical conditional using conditional alternative lmap χ-parameterized discriminator used distinguish true pair artiﬁcially generated using fully adversarial training algorithm supervised learning using alice result replacing lmap proposition optimum generators discriminator form saddle points objective proof provided proposition enforces generator correctly paired sample space. together theoretical result lemma corollary optimum achieved corollary indicates ali’s drawbacks associated identiﬁability issues alleviated fully supervised learning scenario. conditional gans used boost perfomance direction mapping. tying weights discriminators conditional gans alice recovers triangle practice samples paired often contain enough information readily approximate sufﬁcient statistics entire dataset. case following objective semi-supervised learning ﬁrst terms operate entire last term applies paired subset. note train fully adversarially replacing lcycle lmap cycle respectively. three terms treated equal weighting experiments speciﬁcially mentioned course introduce additional hyperparameters adjust relative emphasis term. related work uniﬁed perspective joint distribution matching connecting cyclegan. provide information theoretical interpretation cycleconsistency show equivalent controlling conditional entropies matching conditional distributions. cycle-consistency satisﬁed corollary shows conditionals matched cyclegan. also train additional discriminators guarantee matching marginals using original objective reveals equivalence cyclegan latter also guarantee matching joint distributions practice cyclegan easier train decomposes joint distribution matching objective four subproblems. approach leverages similar idea improves adversarially learned cycle-consistency high quality samples interest. stochastic mapping deterministic mapping. propose enforce cycle-consistency case stochastic mappings speciﬁed cycle-consistency achieved corollary shows bounded conditional entropy vanishes thus corresponding mapping reduces deterministic. literture deterministic mapping empirically tested ali’s framework without explicitly specifying cycle-consistency. bigan uses deterministic mappings. theory deterministic mappings guarantee cycle-consistency ali’s framework. however achieve this model delta distribution another distribution sense divergence asymmetry cost function extremely cost generating fake-looking samples explains underﬁtting reasoning behind subpar reconstruction ability ali. therefore alice explicitly cycle-consistency regularization accelerate stabilize training. conditional gans joint distribution matching. conditional variants widely used supervised tasks. scheme learn conditional entropy borrows formulation conditional authors’ knowledge ﬁrst attempt study conditional formulation joint distribution matching problem. moreover potential leverage well-deﬁned distribution implied paired data resolve ambiguity issues unsupervised variants experimental results code reproduce experiments https//github.com/chunyuanli/alice effectiveness stability cycle-consistency highlight role regularization unsupervised learning perform experiment dataset. gaussian mixture model mixture components chosen standard gaussian following covariance matrices centroids chosen distribution exhibits severely separated modes makes relatively hard task despite nature. following study stability exhaustive grid search architectural choices hyper-parameters experiments method. report mean squared error inception score quantitatively evaluate performance generative models. proxy reconstruction quality reﬂects plausibility variety sample generation. lower higher indicate better results. details grid search calculation icp. train samples test samples. ground-truth test samples shown figure respectively. compare alice denoising auto-encoders report distribution values experiments figure respectively. reference samples drawn oracle yield icp=.±.. alice yields larger experiments ali’s wildly varies across different runs. results demonstrate alice consistent quantitatively reliable ali. yields lowest expected also results weakest generation ability. comparatively alice demonstrates acceptable reconstruction ability compared though signiﬁcantly improvement ali. figure shows qualitative results test set. since ali’s results vary largely trial trial present highest icp. ﬁgure color samples different mixture components highlight correspondance ground truth figure reconstructions figure importantly though reconstruction recover shape manifold individual reconstructed sample substantially away original mixture component hence poor mse. occurs adversarial training requires generated samples look realistic i.e. located figure qualitative results data. two-column blocks represent results method left right ﬁrst left sampling right reconstruction colors indicate mixture component membership. second shows reconstructions linearly interpolated samples near true samples mapping observed latent spaces speciﬁed. also consider various combinations stochastic/deterministic mappings conclude models deterministic mappings tend lower reconstruction ability higher generation ability. terms estimated latent space figure alice results better latent representation sense mapping consistency distribution consistency results reconstruction sampling shown figure also investigate latent space interpolation pair test examples. linearly interpolate intermediate points back original space show index samples better visualization. figure shows alice’s interpolation smooth consistent ground-truth distributions. interpolation using results realistic samples transition order-wise consistent. daes provides smooth transitions samples original space look unrealistic located probability density regions true model. investigate impact different amount regularization three datasets including dataset mnist cifar- section results show regularizer improve image generation reconstruction large range weighting hyperparameter values. reconstruction cross-domain transformation real datasets image-to-image translation tasks considered. car-to-car domain includes images different angles seek demonstrate power adversarially learned reconstruction weak supervision. edge-to-shoe domain consists shoe photos domain consists edge images report extensive quantitative comparisons. cycle-consistency applied domains. goal discover cross-domain relationship maintaining reconstruction ability domain. adversarially learned reconstruction demonstrate effectiveness fully adversarial scheme real datasets place losses discogan practice feature matching used help adversarial objective reach optimum. also compared baseline scheme adversarially discriminates reconstruction results shown figure bottom shows ground-truth images discogan loss marginal schemes respectively) bigan note bigan best variant grid search compasion. proposed joint scheme retain crispness characteristic adversariallytrained models tends blurry. marginal provides realistic images faithful reproductions inputs. explains observations terms performance gain. bigan learns shapes cars misses textures. sign underﬁtting thus indicating bigan easy train. weak supervision discogan bigan unsupervised methods exhibit different cross-domain pairing conﬁgurations different training epochs indicative nonidentiﬁability issues. leverage weak supervision help convergence guide pairing. results shown figure methods times width colored lines reﬂect standard deviation. start true pairs supervision yields signiﬁcantly higher accuracy discogan/bigan. provided supervison angles yields comparable angle prediction accuracy full angle supervison testing. shows alice’s ability terms zero-shot learning i.e. predicting unseen pairs. show enforcing different weak supervision strategies affects ﬁnal pairing conﬁgurations i.e. leverage supervision obtain desirable joint distribution. quantitative comparison quantitatively assess generated images structural similarity established image quality metric correlates well human visual perception. ssim values higher better. ssim alice prediction reconstruction shown figure edge-to-shoe task. baseline discogan -based supervision -sup). bigan/ali highlighted circle outperformed alice aspects unpaired setting cycle-consistency regularization shows signiﬁcant performance gains particularly reconstruction. supervision leveraged ssim signiﬁcantly increased prediction. adversarial-based supervision a-sup) shows higher prediction -sup. alice achieves similar performance full supervision setup indicating advantage semi-supervised learning. several generated edge images shown figure a-sup tends provide details -sup. methods generate correct paired edges quality higher bigan discogan. also report metrics results edge domain only consistent results presented here. one-side cycle-consistency uncertainty domain desirable consider one-side cycle-consistency. demonstrated celeba face dataset face associated -dimensional attribute vector. results figure ﬁrst task consider images generated -dimensional gaussian latent space apply lcycle compare alice reconstruction alice shows faithful reproduction input subjects. second task consider attribute space images generated. mapping attribute classiﬁcation. apply lcycle domains. paired samples considered attribute domain predicted attributes still reach accuracy comparable fully supervised case. test diversity ﬁrst predict attributes true face image generated multiple images conditioned predicted attributes. four examples shown conclusion studied problem non-identiﬁability bidirectional adversarial networks. uniﬁed perspective understanding various models joint matching provided tackle problem. insight enables propose alice reduce ambiguity control conditionals unsupervised semi-supervised learning. future work proposed view provide opportunities leverage advantages model advance joint-distribution modeling. zhang. triple generative adversarial nets. nips zhao mathieu lecun. energy-based generative adversarial network. iclr salimans goodfellow zaremba cheung radford chen. improved techniques since paper constrain correlation random variables using information theoretical measures ﬁrst review related concepts. probability measure random variables following additive subtractive relationships various information measures including mutual information variation information conditional entropy following shows negative probability reconstruction related variation information mutual information. support denote encoder probability measure decoder probability measure. note reconstruction loss proof cycle-consistency conditional using adversarial traning shown below. follows proof original paper ﬁrst show implication optimal discriminator show corresponding optimal generator. lemma optimial generator discriminator parameters further proof observed paired data marginal empirical distribution paired data. also paired dataset. start observation -component gaussian mixture model means standard derivation isotropic gaussian mean standard derivation consider various network architectures compare stability methods. hyperparameters includes number layers number neurons discriminator generators update frenquency discriminator generator. grid search speciﬁcation summarized table hence total number experiments generalized version inception score calculated exkl||p) denotes generated sample label predicted classiﬁer trained off-line using entire training set. also worth noting although inherit name inception score evaluation related inception model trained imagenet dataset. classiﬁer regular -layer neural nets trained dataset interest yields classiﬁcation accuracy dataset. figure qualitative results data. every columns indicate results method left space reconstruction right space sampling respectively. reconstruction sampling show additional results econstruction sampling figure alice shows good sampling ability reﬂects guassian characteristics components ali’s samples tends concentrated reﬂected shrinked guassian components. learns indentity mapping thus show weak generation ability. summary four variants alice alice general ce-based framework regularize objectives bidiretional adversarial training order obtain desirable solutions. clearly show versatility alice summarize four variants test effectivenss datasets. unsupervised learning forms cycle-consistency/reconstruction considered bound explicit cycle-consistency explicitly speciﬁed k-norm reconstruction; implicit cycle-consistency implicitly learned reconstruction adversarial training disucssion explicit methods losses similarity/quality reconstruction original sample measured terms metric. easy implement optimize. however lead visually quality reconstruction high dimensions. implicit methods adversarial training essentially requires reconstruction close original sample terms metric adversarial feature learning). theoretically guarantees perfect reconstruction however hard achieve practice espcially high dimension spaces. results effectivenss algorithms demonstrated data dimension figure unsupervised variants tested dataset described above results figure supervised variants create dataset z-domain -component x-domain -component gmm. since domain symmtric ambiguity exists cycle-gan variants attempt discover relationship domains pure unsupervised setting. indeed observed random switching discoverd corresponded components different runs cycle-gan. adding tiny fraction pairwise information easily learn correct correspondences entire datasets. figure pairs pre-speciﬁed points x-domain paired points z-domain opposite signs. explicit implicit alice correct pairing conﬁgurations unlabeled samples. inspires manually labeling relations samples domains alice automatically control full datasets pairing real datasets. example shown carcar dataset. plot histogram fig. report mean standard derivation table fig. compare reconstruction generation ability. models deterministic mapping higher recontruction ability show lower sampling ability. comparison reconstruction please fig. reconstruction start sample pass cycle formed mappings times. resulted reconstructions shown blue dots. reconstructed samples tends concentrated deterministic mappings. comparison sampling please fig. sampling ﬁrst draw samples domain pass mappings. generated samples colored index gaussian component comes original domain. figure comparison bidirectional models different stochastic deterministic mappings. reconstruction reconstruction rows original data point blue dots reconstruction. sampling sampling reconstruction colors generated indicate component conditions investigate effectiveness impact proposed cycle-consistency regularizer norm) datasets including dataset mnist cifar-. large range weighting hyperparameter tested. inception scores mnist datasets evaluted pre-trained perfect classiﬁers datasets respectively inception scores cifar based imagenet. results different shown figure best performance sumarized table figure impact proposed cycle-consistency regularizer. perfect performance shown solid line performance dash line. alice different levels regularization shown light blue dots best performance alice shown dark blue circle. intervals. views used. dataset split train test split train groups used domain domain samples. evaluate trained regressor classiﬁer predict azimuth angle using train set. image domain other reconstruct original domain. cycle-consistency evaluted prediction accuracy reconstructed images. table shows prediction accuracy leverage supervision different number angles. demonstrate easily control correspondence conﬁguration designing proper supervision alice enforce coherent supervsion opposite supervision respectively. supervison information used angle. translated images test using three trained models azimuth angles predicted using regressor input translated images. table show cross domain relationship discovered method. axis indicates predicted angles original transformed cars respectively. three plots results epoch. scatter points supervision concentrated diagnals plots indicates higher prediction/correlation. learning curves shown table axis indicate rmse angle prediction. weak supervision largely imporve convergence results speed. example comparison arre shown figure. reconstruction results validation dataset celeba dataset shown figure results paper alice provides faithful reconstruction input subjects. trade-off theoretical optimum practical convergence employ feature matching thus results exhibits slight bluriness characteristic. figure reconstruction alice ali. columns original samples validation even columns corresponding reconstructions. generated faces based predicted attributes real face image demonstrate potential real applications alice algorithms task sketch cartoon. built dataset collecting frames disney’s alice wonderland. large image size considered. training dataset consists domains cartoon images edges images edges created holistically-nested edge detection true cartoon images. image content either characters alice white rabbit. therefore domain exhibits modes. images collected domain. one-to-one image correspondence domain unknown goal efﬁciently generate realistic cartoon images animation based edges. cyclegan unsupervised learning algorithm. since shown equivalence ali/bigan superiority terms stability. derive weaklysupervised alice algorithm dataset cyclegan unpaired data explicit loss and/or implicit conditional loss paired samples. note pair randomly chosen character. total training iteration observed results training summarize following ated images iterations show figure generated image based slightly different edges background details character. cyclegan gets confused identifying character thus inconsistently paint dfferent colors rabbit explicit alice generate coherent color. figure comparison different algorithms iterations. bounding boxes highlight different regions. bottom real cartoon edges explicit alice implicit alice cyclegan.", "year": 2017}