{"title": "Train and Test Tightness of LP Relaxations in Structured Prediction", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Structured prediction is used in areas such as computer vision and natural language processing to predict structured outputs such as segmentations or parse trees. In these settings, prediction is performed by MAP inference or, equivalently, by solving an integer linear program. Because of the complex scoring functions required to obtain accurate predictions, both learning and inference typically require the use of approximate solvers. We propose a theoretical explanation to the striking observation that approximations based on linear programming (LP) relaxations are often tight on real-world instances. In particular, we show that learning with LP relaxed inference encourages integrality of training instances, and that tightness generalizes from train to test data.", "text": "structured prediction used areas computer vision natural language processing predict structured outputs segmentations parse trees. settings prediction performed inference equivalently solving integer linear program. complex scoring functions required obtain accurate predictions learning inference typically require approximate solvers. propose theoretical explanation striking observation approximations based linear programming relaxations often tight real-world instances. particular show learning relaxed inference encourages integrality training instances tightness generalizes train test data. many applications machine learning formulated prediction problems structured output spaces problems output variables predicted jointly order take account mutual dependencies them high-order correlations structural constraints unfortunately improved expressive power models comes computational cost indeed exact prediction learning become np-hard general. despite worst-case intractability eﬃcient approximations often achieve good performance practice. particular type approximation proved eﬀective many applications based linear programming relaxation. approach prediction problem ﬁrst cast integer integrality constraints relaxed obtain tractable program. addition achieving high prediction accuracy observed relaxations often tight practice. solution relaxed program happens optimal original hard problem particularly surprising since complex scoring functions constrained tractable family. major open question understand real-world instances behave diﬀerently theoretical worst case. paper aims address question provide theoretical explanation tightness relaxations context structured prediction. particular show approximate training objective although designed produce accurate predictors also induces tightness relaxation byproduct. analysis also suggests exact training opposite eﬀect. explain tightness test instances prove generalization bound tightness. bound implies many training instances integral test instances also likely integral. results consistent previous empirical ﬁndings knowledge provide ﬁrst theoretical justiﬁcation wide-spread success relaxations structured prediction settings training data linearly separable. many structured prediction problems represented ilps despite nphard general various eﬀective approximations proposed. include search-based methods natural relaxations hard tightness relaxations special classes problems studied extensively recent years include restricting either structure model score function. example pairwise relaxation known tight tree-structured models supermodular scores cycle relaxation known tight planar ising models external ﬁeld almost balanced models facilitate eﬃcient prediction could restrict model class tractable. example taskar learn supermodular scores meshi learn tree structures. however suﬃcient conditions mentioned means necessary indeed many score functions useful practice satisfy still produce integral solutions example martins showed predictors learned relaxation yield integral test data dependency parsing problem observed similar behavior dependency parsing number languages seen fig. phenomenon observed multi-label classiﬁcation task test integrality reached learning structured output predictors labeled data proposed various forms collins taskar tsochantaridis formulations generalize training methods binary classiﬁers perceptron algorithm support vector machines case structured outputs. learning algorithms repeatedly perform prediction necessitating approximate inference within training closely related work kulesza pereira showed approximations equally good important match inference algorithms used train test time. authors deﬁned concept algorithmic separability refers setting approximate inference algorithm achieves zero loss data set. authors studied relaxations structured learning giving generalization bounds true risk lp-based prediction. however since generalization bounds kulesza pereira focused prediction accuracy settings tightness test instances guaranteed training data algorithmically separable seldom case real-world structured prediction tasks paper’s main result hand guarantees expected fraction test instances relaxation integral close estimated training data. allows talk generalization computation. example suppose uses relaxation-based algorithms iteratively tighten relaxation sontag jaakkola sontag observes instances training data integral using pairwise relaxation tightening using cycle constraints remaining integral too. generalization bound guarantees approximately ratio hold test time finley joachims also studied eﬀect various approximate inference methods context structured prediction. theoretical empirical results also support superiority relaxations setting. martins established conditions guarantee algorithmic separability relaxed training derived risk bounds learning algorithm uses combination exact relaxed inference. finally recently globerson studied performance structured predictors grid graphs binary labels informationtheoretic point view. proved lower bounds minimum achievable expected hamming error setting proposed polynomial-time algorithm achieves error. work diﬀerent since focus relaxations approximation algorithm handle general form without making assumptions model error measure popular model class task based linear classiﬁers. setting prediction performed many applications score function particular structure. specifically assume score decomposes simpler score assignment subset variables example common decomposition assigns scores single pairs output variables corresponding nodes edges graph ijφij. viewing function consistent second solving ilps np-hard general easy obtain tractable program relaxing integrality constraints introduce fractional solutions relaxation ﬁrst level sherali-adams hierarchy provides successively tighter relaxations ilp. notice since relaxed program obtained removing constraints optimal value upper bounds optimum. order achieve high prediction accuracy parameters learned training data. supervised learning setting model labeled examples y)}m goodness measured task-speciﬁc loss structured framework empirical risk upper bounded convex surrogate called structured hinge loss yields training objective convex function hence optimized various ways. notice objective includes maximization outputs training example. loss-augmented prediction task needs solved repeatedly training makes training intractable general. fortunately prediction relaxation applied structured loss yields relaxed training objective section present main results proposing theoretical justiﬁcation observed tightness relaxations used inference models learned structured prediction training held-out data. make complementary arguments section argue optimizing relaxed training objective also eﬀect encouraging tightness training instances; section show tightness generalizes train test data. ﬁrst show relaxed training objective although designed achieve high accuracy also induces tightness relaxation. order simplify notation focus single training instance drop index denote solutions relaxed integer equality states diﬀerence scores relaxed optimum ground-truth written integrality diﬀerence scores exact optimum groundtruth simple decomposition several interesting implications. precisely relaxed training objective therefore optimizing approximate training objective minimizes upper bound integrality gap. hence driving approximate objective also reduces integrality training instances. case integrality becomes zero data algorithmically separable. case relaxed-hinge term vanishes integrality assured. however bound might sometimes loose. indeed bound discarded exact-hinge term added task-loss maximized loss-augmented objective time provides precise characterization integrality gap. speciﬁcally determined diﬀerence relaxed-hinge exact-hinge terms. implies even relaxed-hinge zero small integrality still obtained exact-hinge also large. fact large integrality setting exact-hinge much smaller relaxed-hinge. happen? point relaxed exact hinge terms upper bounded relaxed exact training objectives respectively therefore minimizing training objective also reduce corresponding hinge term using insight observe relaxed training reduces relaxed-hinge term without directly reducing exact-hinge term thereby induces small integrality gap. hand also suggests exact training actually increase integrality since reduces exact-hinge without also reducing directly relaxed-hinge term. ﬁnding consistent previous empirical evidence. speciﬁcally martins showed dependency parsing problem training relaxed objective achieved integral solutions exact training achieved integral solutions. even stronger eﬀect observed finley joachims multi-label classiﬁcation relaxed training resulted integral instances exact training attaining section provide empirical support explanation however next also show possible limitations providing counterexample. counter-example demonstrates despite training relaxed objective exact-hinge cases actually smaller relaxed-hinge leading loose relaxation. although illustrates next construct trainset ﬁrst instance second veriﬁed minimizes relaxed objective however weight vector relaxed-hinge second instance equal exact-hinge instances consequently integrality second instance relaxation loose finally note derivation holds integral ground-truth words property using integrality. indeed section verify empirically training model using random labels still attains level tightness training ground-truth labels. hand accuracy drops dramatically expected. analysis suggests tightness related accuracy predictor. finley joachims explained tightness relaxations noting fractional solutions always incur loss training. analysis suggests alternative explanation emphasizing diﬀerence scores rather loss decoupling tightness accuracy. argument section concerns tightness train instances. however empirical evidence discussed pertains test data. bridge section show train tightness implies test tightness. proving generalization bound tightness based rademacher complexity. vertices local polytope denoting sets fully-integral non-integral vertices respectively considering vertices without loss generality since linear programs always vertex optimal. next maxµ∈mi maxµ∈mf best integral fractional scores attainable respectively. convention whenever fractionality measured quantity quantity large fractional loss equals optimal fractional solution higher score optimal integral solution. notice loss ignores ground-truth expected. addition deﬁne ramp loss parameterized upper bounds fractionality loss loss zero best integral solution better best fractional solution least stronger requirement mere tightness. section give examples models guaranteed satisfy stronger requirement section also show often happens practice. point generally hard compute however interested proving tightness generalizing property worry computational eﬃciency now. ready state main theorem section. function dominating function class functions mapping y)}m independently selected according probability measure number samples probability least every satisﬁes result deﬁne class functions satisfying order obtain meaningful bound would like bound rademacher term constant satisﬁes addition weiss taskar show rest analysis holds unchanged. consequently generalization result implies likely observe similar portion instances fractional values test time training. deﬁnition relaxation called γ-tight best integral value larger best nonintegral value least focus binary pairwise models show cases model guaranteed γ-tight. proofs provided appendix ﬁrst example involves balanced models binary pairwise models supermodular scores made supermodular ﬂipping subset variables result particular interest learning structured predictors edge scores depend input. whereas could learn supermodular models enforcing linear inequalities know tractable means restricting model balanced. instead could learn full space models using relaxation. learned models balanced training data prop. together theorem tell pairwise relaxation likely tight test data well. second example regards models singleton scores much stronger pairwise scores. consider binary pairwise model minimal representation node scores ¯θij edge scores representation further variable deﬁne neighbors attractive edges ni|¯θij neighbors repulsive edges ni|¯θij figure training ‘yeast’ dataset. various quantities interest shown function training iterations. training relaxation. training ilp. integrality margin section present numerical results support theoretical analysis. experiments multi-label classiﬁcation task image segmentation task. training implemented blockcoordinate frank-wolfe algorithm structured using glpk solver. experiments standard regularizer chosen cross-validation. multi-label classiﬁcation multi-label classiﬁcation adopt experimental setting finley joachims setting labels represented binary variables model consists singleton pairwise factors forming fully connected graph labels task loss normalized hamming distance. fig. shows relaxed exact training iterations ‘yeast’ dataset plot relaxed exact hinge terms exact relaxed ssvm training objectives respectively) fraction train test instances integral solutions well test accuracy whenever fractional solution found relaxed inference simple rounding scheme applied obtain valid figure training ‘scene’ dataset. various quantities interest shown function training iterations. training relaxation. training ilp. integrality margin. prediction. first note relaxed-hinge values nicely correlated relaxed training objective likewise exact-hinge correlated exact objective second observe relaxed training relaxed-hinge exact-hinge close integrality given diﬀerence remains small hand exact training exact-hinge reduced much relaxed-hinge results large integrality indeed percentage integral solutions almost relaxed training close exact training better understanding show histogram diﬀerence optimal integral fractional model training instances seen relaxed training margin positive exact training results larger negative values. third notice train test integrality levels close other almost indistinguishable provides empirical support generalization result section next train model using random labels setting learned model obtains tight training instances supports claim integral solution used place ground-truth accuracy important tightness. finally order verify tightness coincidental proceed perform experiments ‘scene’ dataset results fig. quite similar ‘yeast’ results except behavior exact training integrality margin speciﬁcally observe case relaxed-hinge exact-hinge close value relaxed training consequence integrality small relaxation tight almost train instances. results show sometimes optimizing exact objective reduce relaxed objective well. further setting observe larger integrality margin means integral optimum strictly better fractional one. conjecture instances easy case dominance singleton scores. speciﬁcally features provide strong signal allows label assignment decided mostly based local score little inﬂuence coming pairwise terms. test conjecture repeat experiment injecting gaussian noise input features forcing model rely pairwise interactions. noisy singleton scores results indeed similar ‘yeast’ dataset large integrality observed fewer instances tight image segmentation finally conduct experiments foregroundbackground segmentation problem using weizmann horse dataset data consists images ﬁrst training rest testing. binary output variable asextract singleton pairwise features described domke fig. shows quantities multi-label setting except accuracy measure compute percentage correctly classiﬁed pixels rather observe similar behavior ‘scene’ multi-label dataset speciﬁcally relaxed exact training produce small integrality high percentage tight instances. unfigure training foreground-background segmentation weizmann horse dataset. various quantities interest shown function training iterations. training relaxation. training ilp. like ‘scene’ dataset variables satisfy condition prop. experiments learned model scores never balanced although segmentation problem believe models learned close balanced relaxed exact training. paper propose explanation tightness relaxations observed many structured prediction applications. analysis based careful examination integrality relation training objective. shows training relaxations although designed accuracy considerations mind also induces tightness relaxation. derivation also suggests exact training sometimes opposite eﬀect increasing integrality gap. explain tightness test instances show tightness generalizes train test instances. compared generalization bound kulesza pereira bound considers tightness instance ignoring label errors. thus example learning happens settle parameters tractable regime relaxation tight training instances generalization bound guarantees high probability relaxation also tight test instances. contrast kulesza pereira bound tightness test instances guaranteed training data algorithmically separable work suggests many directions study. analysis section focuses score hinge ignores task loss would interesting study eﬀect various task losses tightness relaxation training. next bound section intractable compute hardness surrogate loss therefore desirable derive tractable alternative could used obtain useful guarantee practice. upper bound integrality shown section holds convex relaxations proposed structured prediction semi-deﬁnite programming relaxations however less clear extend generalization result non-polyhedral relaxations. finally hope methodology useful shedding light tightness convex relaxations learning problems. section provide full derivations results section make extensive results weller start deﬁning model minimal representation convenient derivations follow. optimal score unique optimum integral vertex minimal form integral vertex value denote state coordinate consider computing constrained optimum holding various states. assumption integral vertex value therefore next towards contradiction suppose exists fractional vertex value fractional coordinate assumption implies contradicts therefore conclude fractional solution value possible check polynomial time model balanced unique optimum compute done computing diﬀerence value second-best. order second-best constrain variable turn diﬀer state optimal solution recompute solution; ﬁnally take maximum trials. consider variable neighbors graph. further deﬁne sets corresponding attractive repulsive edges respectively. next focus parts objective aﬀected value recall vertices half-integral figure training noisy version ‘scene’ dataset. various quantities interest shown function training iterations. training relaxation. training ilp. integrality margin section present additional experimental results ‘scene’ dataset. speciﬁcally inject random gaussian noise input features order reduce signal singleton scores increase role pairwise interactions. makes problem harder since prediction needs account global information. fig. observe exact training exact loss minimized causing exact-hinge decrease since upper bounded loss hand relaxed-hinge increase training results large integrality fewer tight instances. contrast relaxed training relaxed loss minimized causes relaxed-hinge decrease. since exact-hinge upper bounded relaxed-hinge also decreases hinge terms decrease similarly remain close other. results small integrality tightness almost instances. finally contrast settings fig. observe exact training test tightness noticeably higher train tightness contradict bound theorem since fact test fractionality even lower bound suggests. hand result entail train test tightness sometimes behave diﬀerently means might need increase size trainset order tighter bound.", "year": 2015}