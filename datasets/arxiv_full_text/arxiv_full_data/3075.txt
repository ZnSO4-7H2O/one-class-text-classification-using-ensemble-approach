{"title": "Semi-parametric Topological Memory for Navigation", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "abstract": "We introduce a new memory architecture for navigation in previously unseen environments, inspired by landmark-based navigation in animals. The proposed semi-parametric topological memory (SPTM) consists of a (non-parametric) graph with nodes corresponding to locations in the environment and a (parametric) deep network capable of retrieving nodes from the graph based on observations. The graph stores no metric information, only connectivity of locations corresponding to the nodes. We use SPTM as a planning module in a navigation system. Given only 5 minutes of footage of a previously unseen maze, an SPTM-based navigation agent can build a topological map of the environment and use it to confidently navigate towards goals. The average success rate of the SPTM agent in goal-directed navigation across test environments is higher than the best-performing baseline by a factor of three. A video of the agent is available at https://youtu.be/vRF7f4lhswo", "text": "introduce memory architecture navigation previously unseen environments inspired landmark-based navigation animals. proposed semiparametric topological memory consists graph nodes corresponding locations environment deep network capable retrieving nodes graph based observations. graph stores metric information connectivity locations corresponding nodes. sptm planning module navigation system. given minutes footage previously unseen maze sptm-based navigation agent build topological environment conﬁdently navigate towards goals. average success rate sptm agent goal-directed navigation across test environments higher best-performing baseline factor three. deep learning recently used efﬁcient approach learning navigation complex three-dimensional environments. dl-based approaches navigation broadly divided three classes purely reactive based unstructured general-purpose memory lstm employing navigation-speciﬁc memory structure based metric however extensive evidence psychology suggests traversing environments animals rely strongly metric representations rather animals employ range specialized navigation strategies increasing complexity. according strategy landmark navigation ability orient respect known object. another route-based navigation involves remembering speciﬁc sequences positions. finally map-based navigation assumes survey knowledge environmental layout need metric fact typically humans integrate experience speciﬁc routes metric cognitive navigation rather primarily depend landmark-based navigation strategy supported qualitative topological knowledge environment. paper propose semi-parametric topological memory deep-learning-based memory architecture navigation inspired landmark-based navigation animals. sptm consists components non-parametric memory graph node corresponds location environment parametric deep network capable retrieving nodes graph based observations. graph contains metric relations nodes connectivity information. exploring environment agent builds graph appending observations adding shortcut connections based detected visual similarities. network trained retrieve nodes graph based observation environment. allows agent localize graph. finally build complete sptm-based navigation agent complementing memory locomotion network allows agent move nodes graph. networks trained self-supervised fashion without manual labeling reward signal. evaluate proposed system relevant baselines task goal-directed maze navigation simulated three-dimensional environments. agent instantiated previously unseen maze given recording walk maze agent initialized location maze reach goal location maze given image goal. successful task agent must represent maze based footage seen effectively utilize representation navigation. proposed system outperforms baseline approaches large margin. given minutes maze walkthrough footage system able build internal representation environment conﬁdently navigate various goals within maze. average success rate sptm agent goal-directed navigation across test environments higher best-performing baseline factor three. qualitative results implementation method available https//sites.google.com/view/sptm. navigation animals extensively studied psychology. tolman introduced concept cognitive internal representation environment supports navigation. existence cognitive maps exact form animals including humans debated since. o’keefe nadel suggested internal representations take form metric maps. recently shown bees ants rats rely largely landmark-based mechanisms navigation. bennett mackintosh question existence cognitive maps animals. gillner mallot wang spelke argue humans rely largely landmark-based navigation. contrast navigation systems developed robotics typically based metric maps constructed using available sensory information sonar lidar rgb-d input particularly relevant work vision-based simultaneous localization mapping methods systems provide high-quality maps favorable conditions sensitive calibration issues deal well poor imaging conditions naturally accommodate dynamic environments difﬁcult scale. modern deep learning methods allow end-to-end learning sensorimotor control directly predicting control signal high-dimensional sensory observations images approaches navigation vary learning method reinforcement learning imitation learning memory representation. purely reactive methods lack explicit memory navigate well complex environments systems equipped general-purpose lstm memory episodic memory potentially store information environment. however systems demonstrated perform efﬁcient goal-directed navigation previously unseen environments empirical results indicate lstm-based systems task addressable memory ﬁrst-person-view navigation three-dimensional environments. authors demonstrate proposed memory structure supports generalization previously unseen environments. work different experiment relatively small discrete gridworld-like environments approach naturally applies large continuous state spaces. related work navigation systems specialized map-like representations. bhatti augment system metric produced standard slam algorithm. parisotto salakhutdinov spatial memory represents global environment. gupta build multi-scale metric using end-to-end trainable planning approach tamar method differs approaches aiming build global metric environment. rather topological map. allows method support navigation continuous space without externally provided camera poses ego-motion information. figure navigation agent equipped semi-parametric topological memory given inputs current observation goal observation sptm provides waypoint observation waypoint current observation locomotion network outputs action executed environment. contemporary approaches robotics dominated metric maps research topological maps long history robotics. models based topological maps applied navigation simple mazes physical systems trullier provide review biologically-inspired navigation systems including landmark-based ones. milford colleagues designed slam systems inspired computational models hippocampus reinterpret line work context deep learning. consider agent interacting environment discrete time steps. time step agent gets observation environment takes action actions experiments environment maze three-dimensional simulated world observation provided agent tuple several recent images agent’s point view. interaction agent environment stages exploration goaldirected navigation. ﬁrst stage agent presented recording traversal environment number time steps builds internal representation environment based recording. second stage agent uses internal representation reach goal locations environment. goal-directed navigation performed episodic setup episode lasting ﬁxed maximum number time steps goal reached. episode goal location provided agent observation location agent goal observation internal representation built exploration phase effectively reach goal. propose form memory suitable storing internal representations environments. refer semi-parametric topological memory consists memory graph node represents location environment deep network capable retrieving nodes graph based observations. high-level overview sptm-based navigation system shown figure sptm acts planning module given current observation goal observation generates waypoint observation lies path goal easily reached agent’s current location. current observation waypoint observation provided locomotion network responsible short-range navigation. locomotion network guides agent towards waypoint loop repeats. networks trained self-supervised fashion figure semi-parametric topological memory navigation. retrieval network localizes graph vertices corresponding current agent’s observation goal observation respectively. shortest path graph vertices computed waypoint vertex selected vertex shortest path furthest agent’s vertex still conﬁdently reached agent. output sptm corresponding waypoint observation ovw. retrieval network. network estimates similarity observations network trained environments self-supervised manner based trajectories randomly acting agent. conceptually network trained assign high similarity pairs observations temporally close similarity pairs temporally distant. cast classiﬁcation task given pair observations network predict whether temporally close not. generate training data ﬁrst random agent explore environment resulting sequence observations actions an}. automatically generate training samples trajectories. training sample triple consists observations binary label. observations considered close separated time steps negative examples pairs observations separated least steps constant factor determines margin positive negative examples. siamese architecture network akin zagoruyko komodakis input observations ﬁrst processed deep convolutional encoder based resnet outputs -dimensional embedding vector. vectors concatenated processed small -layer fully-connected network ending softmax. network trained supervised fashion cross-entropy loss. details provided supplement. memory graph. graph populated based exploration sequence provided agent. vertex graph stores denote observations sequence avoid adding trivial edges. second improve robustness visual shortcuts matching sequences observations single observations finding waypoint. navigation time sptm provide waypoints locomotion network. illustrated figure process includes three steps localization planning waypoint selection. localization step agent localizes goal graph based current observation goal observation illustrated figure experimented approaches localization. basic variant agent’s location retrieved median nearest neighbors observation memory. siamese architecture retrieval network allows efﬁcient nearest neighbor queries pre-computing embeddings observations memory. issue simple technique localization performed frame therefore result noisy susceptible perceptual aliasing inability discriminate locations similar appearance. therefore implement modiﬁed approach allowing temporally consistent self-localization inspired localization approaches robotics initially perform nearest neighbor search local neighborhood previous agent’s localization resort global search whole memory initial search fails simple modiﬁcation improves performance method also reducing search time. naive solution would waypoint ﬁxed however depending actions taken exploration sequence lead selecting waypoint either close therefore follow robust adaptive strategy. choose furthest vertex along shortest path still conﬁdently reachable sreach ﬁxed similarity threshold considering vertex reachable. practice limit waypoint search ﬁxed window output planning process observation corresponds retrieved waypoint. network trained navigate towards target observations vicinity agent. network maps pair consists current observation goal observation action probabilities r|a|. action produced either deterministically choosing probable action stochastically sampling distribution. follows stochastic policy. akin retrieval network network trained self-supervised manner based trajectories randomly acting agent. random exploration produces sequence observations actions an}. generate training samples trajectories taking pair observations separated time steps action corresponding ﬁrst observation ai). network trained supervised fashion data softmax output layer cross-entropy loss. architecture network retrieval network. possible learn useful controller based trajectories randomly acting agent? proposed training procedure leads learning conditional action distribution even though trajectories generated random actor distribution generally uniform. instance network would learn actions taken perform one-step transitions neighboring states. training data noisy still useful training signal turns sufﬁcient short-range navigation. figure sptm-based agent navigating towards goal three-dimensional maze agent aims reach goal denoted star. given current agent’s observation goal observation sptm produces waypoint observation locomotion network used navigate towards waypoint. inputs retrieval network locomotion network observations environment represented stacks consecutive images obtained environment resolution pixels. networks based resnet- note resnet- much larger networks typically used navigation agents based reinforcement learning. high-capacity architecture made possible self-supervised training model. training network size scratch pure reinforcement learning would problematic knowledge never demonstrated. training setup similar networks. generate training data online executing random agent training environment maintain replay buffer recent samples. training iteration sample mini-batch observation pairs random buffer according conditions described sections perform update using adam optimizer learning rate train networks total million mini-batch iterations respectively. details provided supplement. made sure operations sptm implemented efﬁciently. goal localization performed beginning navigation episode. shortest paths goal vertices graph therefore also computed beginning navigation. remaining computationally expensive operations nearest-neighbor queries agent self-localization graph. however thanks siamese architecture retrieval network precompute embedding vectors observations memory need evaluate small fully-connected network navigation. perform experiments using simulated three-dimensional environment based classic game doom illustration sptm agent navigating towards goal maze shown figure evaluate proposed method task goal-directed navigation previously unseen environments compare relevant baselines literature. interested agents able generalize environments. therefore used different mazes training validation testing. used textures labyrinths maze layouts different texture placement randomized. training used single labyrinth layout created versions randomized goal placements textures. addition created mazes validation mazes testing. layouts training test labyrinths shown figure validation mazes shown supplement. maze equipped goal locations marked different special objects. appearance special objects common mazes. used validation mazes tuning parameters approaches used ﬁxed parameters evaluating test mazes. overall experimental setup follows section given maze agent provided exploration sequence environment duration approximately minutes in-simulation time experiments used sequences generated human subject aimlessly exploring mazes. exploration sequences provided algorithms proposed method baselines. example exploration sequences shown project page https//sites.google.com/view/sptm. given exploration sequence agent attempts series goal-directed navigation trials. these agent positioned location maze presented image goal location. experiments used different starting locations goals maze repeated trial times resulting trials maze. trial considered successfully completed agent reaches goal within simulation steps minutes in-simulation time. hyperparameters sptm agent based evaluation validation reported table supplement. method performs well range hyperparameter values. interestingly approach robust temporal subsampling walkthrough sequence. therefore following experiments subsample walkthrough sequence factor building sptm graph. another important parameter threshold sshortcut creating shortcuts graph. threshold percentile pairwise distances observations memory words desired number shortcuts created. number follows. making visual shortcuts graph minimum shortcut distance smoothing window size threshold values waypoint selection slocal sreach minimum maximum waypoint distances hmin hmax respectively. compare proposed method baselines representative state deep-learning-based navigation. note study agent operating realistic setting continuous state space access ground-truth information depth maps egomotion. setup excludes several existing works comparison full model mirowski uses ground-truth depth maps ego-motion method gupta operates discrete grid given ground-truth ego-motion approach parisotto salakhutdinov requires knowledge ground-truth global coordinates agent. ﬁrst baseline goal-agnostic agent without memory. agent informed goal reach chance. train network training maze using asynchronous advantage actor-critic agent trained surrogate task collecting invisible beacons around labyrinth. beginning episode labyrinth populated invisible beacons random locations. agent receives reward collecting beacon otherwise. episode lasts simulation steps. train agent algorithm architecture similar mnih details provided supplement. second baseline feedforward network trained goal-directed navigation similar network gets current observation well image goal input. gets reward goal-agnostic agent collecting invisible beacons addition gets large reward reaching goal. network towards goal goal within ﬁeld view lacks memory fundamentally unable make exploration phase. network architecture ﬁrst baseline input concatenation recent frames goal image. third fourth baseline approaches goal-agnostic goal-directed agents equipped lstm memory. goal-directed lstm agent similar mirowski test time feed exploration sequence lstm agent perform goaldirected navigation without resetting lstm state. training networks follow similar protocol. first agent navigates environment steps exploration mode; rewards collecting invisible beacons without goal image given reward reaching goal. next agent given goal image spends another steps goal-directed navigation mode; goal image given high reward reaching goal reset state memory cells stages. agent learn store layout environment memory efﬁcient navigation. table shows test maze percentage navigation trials successfully completed within steps equivalent minutes real-time simulation. figure presents results test mazes detail plotting percentage completed episodes function trial duration. qualitative results available https//sites.google.com/view/sptm. proposed sptm agent superior baselines mazes. table demonstrates average success rate across test mazes three times higher best-performing baseline. figure demonstrates proposed approach successful overall agent typically reaches goal much faster baselines. difference performance feedforward lstm baseline variants generally small inconsistent across mazes. suggests standard lstm memory sufﬁcient efﬁciently make provided walkthrough footage. reason recurrent networks including lstms struggle storing long sequences duration walkthrough footage time steps beyond capabilities standard recurrent networks. sptm advantage since stores provided information design. figure walkthrough trajectory three goal-directed navigation tracks val- maze walkthrough trajectory shortcuts automatically found sptm graph shown red. goal-directed navigation trials shown tracks successful track excessively long. start positions shown green goals red. performance baseline approaches experiments signiﬁcantly weaker reported previously reason study generalization agents previously unseen environments mirowski train evaluate agents environment. generalization scenario much challenging also realistic. results indicate existing methods struggle generalization. interestingly best-performing baseline goal-agnostic goal-directed. main explanations this. first generalization performance high variance dominated spurious correlations appearance training test mazes. second even training environments goal-directed baselines necessarily outperform goal-agnostic ones since large reward reaching goal makes reinforcement learning unstable. effect observed mirowski avoid authors resort reward clipping; setting reward clipping would effectively lead ignoring goals. figure shows trajectory walkthrough provided algorithms val- maze. shortcut connections made automatically sptm graph marked red. selected conservative threshold making shortcut connections ensure false positives. still automatically discovered shortcut connections greatly increase connectivity graph instance val- maze average length shortest path goal computed nodes graph drops steps introducing shortcut connections. figure demonstrates three representative trajectories sptm agent performing goaldirected navigation. tracks agent deliberately goes goal making environment representation stored sptm. track less successful agent’s trajectory contains unnecessary loops; attribute difﬁculty vision-based self-localization large environments. table reports ablation study sptm agent validation set. removing vision-based shortcuts graph leads dramatic decline performance. agent independent per-frame localization performs quite well three mazes underperforms additional experiments reported supplement performance validation environments robustness hyperparameter settings additional ablation study evaluating performance networks compared simple alternatives experiments environments homogeneous textures experiments automated exploration. proposed semi-parametric topological memory memory architecture consists non-parametric component topological graph parametric component deep network capable retrieving nodes graph given observations environment. shown sptm planning module navigation system. navigation agent efﬁciently reach goals previously unseen environment presented minutes footage. several avenues future work. first improving performance networks directly improve overall quality system. second current system explicitly avoids using ego-motion information ﬁndings experimental psychology suggest noisy ego-motion estimation path integration useful navigation. incorporating model improve robustness. third current system size memory grows linearly duration exploration period. become problematic navigating large environments lifelong learning scenarios. possible solution adaptive subsampling retaining informative discriminative observations memory. finally would interesting integrate sptm system trainable end-to-end. cesar cadena luca carlone henry carrillo yasir latif davide scaramuzza jos´e neira reid john leonard. past present future simultaneous localization mapping toward robustperception age. ieee transactions robotics patrick william warren andrew duchon michael tarr. humans integrate routes cognitive map? mapversus landmark-based navigation novel shortcuts. journal experimental psychology michał kempka marek wydmuch grzegorz runc jakub toczek wojciech ja´skowski. vizdoom doom-based research platform visual reinforcement learning. ieee conference computational intelligence games piotr mirowski razvan pascanu fabio viola hubert soyer andrew ballard andrea banino misha denil ross goroshin laurent sifre koray kavukcuoglu dharshan kumaran raia hadsell. learning navigate complex environments. iclr volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik human-level control deep reinforcement learning. nature volodymyr mnih adri`a puigdom`enech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. icml retrieval network locomotion network based resnet- take pixel images inputs. networks initialized proposed used open resnet implementation https//github.com/raghakot/ keras-resnet/blob/master/resnet.py. network admits observations input. processed convolutional resnet- encoder. encoders produces -dimensional embedding vector. concatenated fully-connected network hidden layers units relu nonlinearities. network also admits observations contrast network processes jointly concatenating together. convolutional resnet- encoder followed single fully-connected layer outputs softmax. outputs correspond available actions nothing move forward move backward move left move right turn left turn right. implemented training keras tensorﬂow training setup similar networks. generate training data online executing random agent environment maintain replay buffer size random agent steps perform mini-batch iterations training. random agent well agents action repeat every selected action repeated times. training iteration sample mini-batch training observation pairs random buffer according conditions described sections perform update using adam optimizer learning rate momentum parameters stabilizing parameter baselines based open implementation https//github.com/pathak/ noreward-rl. used architectures mnih mirowski feedforward model consists convolutional layers fully-connected layers value policy predicted. lstm model second fully connected layer replaced lstm. input networks stack recent observed frames resized pixels. experimented using grayscale frames found baselines trained grayscale images perform better. therefore always report results baselines grayscale inputs. train baselines million action steps corresponds million simulation steps action repeat. selected snapshot used test time based training reward. layouts validation mazes shown figure plots success rate function trial duration validation maze shown figure performance sptm agent varying hyperparameters shown table experiments main paper used mazes relatively diverse textures example figure main paper. re-textured several mazes qualitatively similar mirowski mainly homogeneous textures relatively sparse inclusions discriminative textures. testing method textures retrained networks training maze similar texture distribution kept parameters method ﬁxed. experiments main paper used walkthrough sequences recorded humans exploring maze. intelligent agent able explore environment fully autonomously. effective exploration challenging task itself comprehensive study problem outside scope present paper. however ﬁrst step experiment providing method walkthrough sequences generated fully autonomously baseline agents trained reinforcement learning. possible simple mazes agents able reach goals. used best-performing baseline maze repeated exploration multiple times goals located. results reported table automatically generated trajectories leads minor decrease ﬁnal performance although qualitatively trajectories sptm agent become much noisier different texture distribution affects results more since visual self-localization becomes challenging sparser textures. method still performs quite well outperforms baselines large margin. better understand importance locomotion retrieval networks performed experiments. first substituted retrieval network simple per-pixel matching. second substituted actions predicted locomotion network actions exploration sequence note second approach uses information unavailable method actions performed walkthrough sequence. thus cannot considered proper baseline. discuss exact settings results. experiment inspired approach milford wyeth compute localization score downsample images resolution convert grayscale compute cosine distances them. experiment variants method local contrast normalization without. perform normalization split downsampled grayscale image patches size patch subtract mean divide standard deviation. table indicates per-pixel comparison baseline performs poorly. shown figure visual shortcuts made technique catastrophically wrong. local normalization makes results worse discards information absolute color intensity useful environments. able actions exploration sequence modiﬁcation method necessary. first introduce shortcut connections graph would know actions them. graph thus turns path making shortest paths longer. second allow agent move along path directions select opposite every action example opposite moving forward moving backward. finally found taking fraction completely random actions helps agent stuck diverges exploration track recorded actions useful anymore. found random actions lead good results. overall method works follows. first goal agent localized using procedure method. agent move either forward backward along exploration graph-line. forward action corresponding agent’s localized observation taken backward opposite recorded action. table suggests method works signiﬁcantly worse method even though makes extra information recorded actions. reasons this. first shortcut connections makes path goal longer. second soon agent diverges exploration trajectory actions match states more mechanism agent back track. instance imagine long corridor agent oriented small angle direction corridor inevitably crash wall. approach fail completely latter problem? likely environment forgiving allows agent slide along walls facing angle less degrees. even agent diverges exploration path break completely still makes progress towards goal. videos successful navigation trials agent found https//sites.google.com/view/sptm. table evaluation sptm navigation agent homogeneous textures automated exploration. report percentage navigation trials successfully completed steps", "year": 2018}