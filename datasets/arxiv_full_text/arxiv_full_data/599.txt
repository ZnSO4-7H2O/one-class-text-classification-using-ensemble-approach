{"title": "Learning Semantically Coherent and Reusable Kernels in Convolution  Neural Nets for Sentence Classification", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "The state-of-the-art CNN models give good performance on sentence classification tasks. The purpose of this work is to empirically study desirable properties such as semantic coherence, attention mechanism and reusability of CNNs in these tasks. Semantically coherent kernels are preferable as they are a lot more interpretable for explaining the decision of the learned CNN model. We observe that the learned kernels do not have semantic coherence. Motivated by this observation, we propose to learn kernels with semantic coherence using clustering scheme combined with Word2Vec representation and domain knowledge such as SentiWordNet. We suggest a technique to visualize attention mechanism of CNNs for decision explanation purpose. Reusable property enables kernels learned on one problem to be used in another problem. This helps in efficient learning as only a few additional domain specific filters may have to be learned. We demonstrate the efficacy of our core ideas of learning semantically coherent kernels and leveraging reusable kernels for efficient learning on several benchmark datasets. Experimental results show the usefulness of our approach by achieving performance close to the state-of-the-art methods but with semantic and reusable properties.", "text": "purpose work empirically study desirable properties semantic coherence attention mechanism kernel reusability convolution neural networks learning sentence classiﬁcation tasks. propose learn semantically coherent kernels using clustering scheme combined wordvec representation domain knowledge sentiwordnet. also suggest technique visualize attention mechanism cnns. ideas useful decision explanation purpose. reusable property enables kernels learned problem used another problem. helps efﬁcient learning additional domain speciﬁc kernels learned. experimental results demonstrate usefulness approach. performance proposed approach uses semantic re-usability properties close state-of-the-art approaches many real-world datasets. recent years convolutional neural networks proved effective achieving state-of-the-art results text-centric tasks focus learning sentence classiﬁcation tasks. sentence classiﬁcation task goal predict class label information sentences. examples tasks include classifying sentiments identifying question types. however barring limited work much discussion empirical evidence provided functional behavior learned kernels properties temporal invariance attention mechanism capabilities. furthermore aware work provides enough evidence existence properties semantic coherence reusability learned kernels. goal learn cnns semantically coherent kernels helpful explaining decision model. purpose illustration architecture proposed however core ideas presented applicable several network architectures well. classiﬁcation problem often important reason decision made model. example gain conﬁdence model know right phrases parts sentence used model predict sentiment cnns learning semantically coherent kernels helps reasoning identify kernels contribute decision. kernel semantically coherent ﬁres collection k-grams similar meaning table gives illustration k-grams kernels model obtained architecture clearly seen learned kernels semantically coherent. although learned model gives good performance becomes difﬁcult explain decision without semantic coherence. address problem propose learn semantically coherent kernels. approach involves clustering k-grams occur sentence corpus. distance function weighted combination distances wordvec sentiwordnet representation spaces resulting meaningfully polarized clusters. helps enhance semantic coherence clusters. deﬁne parametrized convolution kernel cluster jointly learn kernel parameters along weights linear softmax classiﬁer output layer. set-up imposing semantic coherence kernels also well suited obtaining solution insights identifying visualizing words sentence used kernels make decision. propose scoring scheme scores word using pooled output scores kernels simple effective visualization technique highlight words sentence express sentiment; helps visualize attentive capability cnns. suppose assume learned kernels semantic coherence. then question naturally arises reuse learned kernels application another similar application learned kernels property significant help directly kernels without learning several similar applications. however application speciﬁc semantics covered ﬁxed kernels. cases improve performance learning additional ﬁlters keeping learned kernels application ﬁxed. helps reduce training time signiﬁcantly applications. contributions propose approach learn semantically coherent kernels experimental results show that semantically coherent kernels achieve performance close state-of-the-art results. suggest novel visualization technique display words prominence demonstrate semantic coherence kernels visualization technique help reason decision. present idea reusability learned kernels similar applications test four different combinations classiﬁcation tasks. kernels learned cnns reusable signiﬁcant recnns successful image classiﬁcation problems make internal structure data structure image data convolution layers text domain cnns used variety problems including sentence modeling word embedding learning sentiment analysis making structure document data. relevant work paper work demonstrated simple layer convolution static pre-trained word vectors obtained using wordvec achieved excellent results sentiment analysis question classiﬁcation tasks. also studied multichannel representation variable size ﬁlters. kalchbrenner proposed dynamic alternated wide convolutional layers dynamic k-max pooling model sentences. sch¨utze proposed multichannel variable-size architecture sentence classiﬁcation. combines different versions word embeddings variable sized ﬁlters multiple layers convolution. mikolov proposed unsupervised framework learns continuous distributed vector representations variable-length pieces texts sentences paragraphs. vector representations learned predict surrounding context words sampled paragraph. focus learn paragraph vectors task speciﬁc features taken account. wang proposed architecture gencnn predict word sequence exploiting long/short range dependencies present history words. johnson zhang compared performance types cnns seqcnn every text region represented followed max-pooling operation produce feature output max{fj··· fjn−k+}. thus convolution layer produces dimensional feature vector using kernels. second layer linear classiﬁer layer computes class probability score weight vector corresponding class. wordvec representations learned google news data representing words. word represented dimensional real valued vector. following kernel widths experiments; note kernel widths correspond convolving -grams -grams -grams respectively sentence. unless otherwise speciﬁed kernels resulting goal design kernels kernel captures semantics call k-grams similar meaning semantically coherent. expect semantically coherent kernel produce high scores k-grams similar meaning. table gives illustration k-grams top- highest cosine similarity scores kernels using network depicted figure top- k-grams similar meaning; thus learned kernel lacks semantic coherence. therefore clear kernels represent high scoring ﬁlter outputs reason decision. learn semantically coherent kernels take two-step approach. ﬁrst step select subset k-grams sentence corpus group desired number clusters. clustering step helps group kgrams semantically coherent. associate kernel cluster weighted combination wordvec representations k-grams members cluster. zlpl denote indices k-grams constitute cluster; denote learnable weight concatenated wordvec representation associated k-gram. thus kernel parametrized learn parameters zhang applied cnns characters demonstrated deep cnns require knowledge words trained large-scale data sets. however capturing semantic information using character-level cnns difﬁcult. santos gatti designed character sentence jointly uses character-level word-level sentence-level representations perform sentiment analysis using convolutional layers. section ﬁrst brieﬂy describe architecture used work. then present notion semantic coherence make observations analyzing ﬁlters benchmark datasets. followed ideas learn semantically coherent kernels visualize ﬁlter outputs aimed helping reason decision. architecture. architecture proposed architecture convolution layer followed linear classiﬁer layer. denote d-dimensional representation word sentence. concatenate vectors represent sentence. kernel convolved sentence produce single feature output follows. representing kernel width feature produced feature value computed relu relu denotes rectiﬁed linear unit function relu table scoring k-grams match learned semantic coherent kernels dataset. clearly ﬁrst three kernels semantically coherent. last kernel coherent others. show experiments section signiﬁcant performance improvement achieved learning kernel parameters. note model built using architecture kernels vj∀j constrained parameters zl∀l subspaces spanned k-grams cj∀j. hand kernel parameters vj∀j learned optimizing free variables kim’s model using architecture. therefore expect degradation performance model; learn semantically coherent kernels easy explanation purpose. selecting k-grams. model complexity dependent number k-grams used form kernels. since number kgrams large data corpus helps control model complexity learning selected k-grams. experimented three simple selection heuristics. details given section supplemental material. clustering using domain knowledge. perform clustering need deﬁne distance function. since represent k-gram using wordvec representation captures distributional semantics using contextual information euclidean distance good distance function use. discover clusters using k-means algorithm. however visual inspection showed quality clusters good. main reason words opposite meanings similar representations tractiveunattractive.) desirable applications sentiment classiﬁcation k-grams opposite sentiments semantically coherent. therefore important form sentiment polarized clusters applications; k-grams expressing sentiment semantic occur together every cluster. form sentiment polarized clusters need additional representation k-grams capture sentiment. purpose bring domain knowledge sentiwordnet knowledge base using knowledge base assign sentiment score word explained below. sentiwordnet representation. sentiwordnet gives -tuple positive negative scores sense word. several ways assign sentiwordnet score word sentence. best sense word corresponding -tuple. simpler techniques aggregate -tuples averaging using maximum element-wise score tuples. experiments found maximum aggregation technique works well. thus sentiwordnet representation k-gram dimensional vector. forming sentiment polarized clusters. concatenate wordvec sentiwordnet representations. note representations capture semantic information derived context seen large corpus sentiment information derived task speciﬁc data corpus respectively. given joint representation modify distance function weighted combination distance functions wordvec sentiwordnet representation spaces. weights manually inspecting quality clusters. table shows kernels scoring k-grams obtained learned semantically coherent kernels dataset. kernels semantically coherent. though k-grams last column noisy improve semantic coherent quality kernels using better distance functions optimizing weights treating hyperparameters. table useful qualitatively assess semantic coherence visual inspection could make weighted distance function deﬁne computable semantic coherent score cluster follows. compute normalized average score weighted distances pairs cluster then deﬁne semantic coherence score normalization done semantic coherence score lies interval higher values indicate stronger semantic coherence. reduce computational cost computed ﬁlter using scoring k-grams. figure shows semantic coherence scores learned ﬁlters dataset. weighted k-gram model signiﬁcantly higher mass towards right compared cnn-static model example model ﬁlters score signiﬁcantly higher compared ﬁlters cnnstatic model. propose simple effective technique visualize words discovered important kernels making decision. figure present sentences words marked graded color font sizes; marked words red/dark colors larger font sizes identiﬁed important using approach learned semantically coherent kernels. generated marked sentences follows. given sentence identify k-gram ﬁres pooled output kernel. then associate respective weighted kernel output score word selected k-gram kernels; here weight linear classiﬁer feature weight depending would like visualize. finally scores word word part multiple selected k-grams normalize scores range normalized scores used intensity values graded color map. example zero intensity value represents black color highest value corresponds dark color. furthermore helps higher font sizes ease visualization. example mapped increasing font sizes equally spaced intensity range intervals generating figure several highlighted important k-grams nicely represent sentiments help reason decision also illustrates attention capability cnns. call kernel reusable kernel learned application serves useful kernel similar applications expect happen cnns k-gram models used similar k-grams appear similar applications particular since learn semantically coherent kernels expect property hold kernels represent distinct semantic notions seen table several ways learned kernels dataset. simple baseline ﬁxed kernels learn classiﬁer layer outputs. another adjust weights k-grams kernel weight regularization using previously learned weights. extend adding kernels learn either ﬁxed weight regularized reusable kernels. experiments used ﬁxed kernels learning additional ﬁlters. need additional kernels arises domain speciﬁc k-grams often present signiﬁcant improvement performance achieved using additional ﬁlters cover k-grams. advantage using reusable kernels achieve signiﬁcant reduction training time applications need learn smaller number parameters. show next section times speed-up possible real-world problems. figure figure shows histogram semantic coherence scores learned ﬁlters cnn-static models dataset. model exhibits higher semantic coherence. figure illustrates visualization technique positive negative sentences sentiment classiﬁcation task. section demonstrate efﬁcacy learning semantically coherent kernels comparing performance baselines cnn-static model emphasized earlier core ideas easily extended applied sophisticated models learning wordvec representations). also demonstrate several examples learned kernels cnns reused similar applications signiﬁcant reduction training time achieved. overall able achieve performance close state-of-the-art methods semantic reusable properties. experimental setup datasets. conducted comprehensive experiments popular benchmark datasets used sentence classiﬁcation tasks imdb sst- sst- subj ﬁrst tasks sentiment classiﬁcation tasks binary classiﬁcation tasks except sst- subj dataset binary classiﬁcation task sentences labeled subjective objective. statistics datasets given table details found supplemental material. models. compare performance several models. methods differ sentence model form i.e. feature input vector forms input classiﬁer layer. categorize methods three categories. ﬁrst category models aggregate wordvec representations words sentence convolution ﬁlters. baselines category. ﬁrst baseline uses sentence model averages wordvec representations equal weights. second baseline assign weight word vocabulary form sentence representation weighted combination; learn weights jointly classiﬁer layer weights. models learned using methods referred simple wordvec averaging weighted wordvec averaging table second category models uses convolution ﬁlter representation obtained using kgram clusters; again simple averaging weighted averaging k-gram wordvec representations form kernel representation. methods referred simple k-gram averaging weighted k-gram averaging table number k-grams extremely large. example number unique -grams imdb datasets model simple wordvec averaging weighted wordvec averaging simple k-gram averaging weighted k-gram averaging parse tree pos-tag ﬂexible ﬁlters ﬂexible ﬁlters cnn-static dcnn mv-rnn table cnn-static refers model wordvec representation. dcnn refers dynamic k-max pooling. mv-rnn refers matrix-vector recursive neural network parse trees. numbers italics reported respective papers. number obtained executing code quoted imdb dataset result reported. training settings. used regularized negative likelihood function objective function learn model parameters. here denote input sentence representation class label information respectively example class probabilities deﬁned using softmax function explained earlier; denote linear classiﬁer layer weights kernel parameters number examples respectively. kernel parameters nothing free variables trained cnn-s model. trained models different regularization constants many passes training using mini-batch adadelta learning rate updates dropout chose model gives best validation accuracy report test accuracy model. compute distance function clustering weights distance functions several experimentation visualizing quality clusters. used kernels k-grams details found supplemental material. given million respectively. therefore discussed earlier experimented three different heuristics order control complexity. ﬁrst heuristic shortlist k-grams using sentiment information available k-gram; used dictionary positive negative sentiment words k-gram contain sentiment word dictionary drop shortlisting using sentiment dictionary subj dataset since sentiment classiﬁcation problem; sampled k-grams results obtained using ﬁrst heuristic referred table goal improving coverage k-grams also identiﬁed additional k-grams obtained outputs syntactic tree parser. third heuristic selected k-grams contain least word belonging categories verbs adjectives derivatives. results obtained using heuristics reported parse tree pos-tag respectively. third category models models reported literature. emphasize intention best performance using complex network architectures; learn network models learned kernels semantically coherent helps reason inspection ﬁred features visualization technique. interesting weighted averaging wordvec representation gives reasonable performance. learning weights sentence models signiﬁcantly improves performance; seen comparing pairs results rows wordvec k-gram based models respectively. recall learn semantically coherent kernels second category models. model gives similar performance compared ﬂexible non-interpretable cnnmodel. performance model quite good even limited k-grams. performance improves signiﬁcantly several datasets bring additional k-grams using syntactic parse tree based information form clusters. still performance cnn-s model. explained earlier reason kernel parameters constrained part subspaces spanned clusters opposed treating free variables cnn-s model. addressed needed adding ﬁlters learn ﬁlters jointly semantically coherent ﬁlters. models referred respective percentage ﬂexible ﬁlters added. clear trend performance improvement ﬁlters added performance reduces significantly cnn-s model. lose reasoning capability would able explain reason added ﬁlters making decision. note tradeinterpretability improved performance accuracy controlling percentage ﬂexible ﬁlters. reasoning important requirement limited accuracy loss approach learning semantically coherent kernels signiﬁcantly useful. furthermore approach nicely discovers important words highlighted visualization technique demonstrated earlier figure overall approach learning semantically coherent kernels quite effective. kernels learned imdb datasets. another experiment reused kernels learned imdb. table decent accuracy achievable using ﬁxed kernels cnn-s models. reason behind observed signiﬁcant performance difference models ﬁxed kernels cnn-s model uses k-grams form kernel. performance significantly reduced adding additional kernels; note added number kernels cnn-s model fair comparison. improvement achieved model k-gram coverage improved significantly; cnn-s model covered larger fraction already. overall kernels learned cnns indeed reusable cnn-s models. measured training time taken full training model ﬁxed kernels sst- dataset. full training takes nearly hours takes minutes ﬁxed kernels. note classiﬁer layer needs learned ﬁxed reusable kernels. adding kernels increased training time approximately minutes. thus order magnitude improvement training time achieving similar performance. work proposed learn semantically coherent kernels using clustering scheme combined wordvec representation domain knowledge sentiwordnet. suggested effective technique visualize words discovered kernels. semantically coherent kernels identifying prominent words help reason decision. introduced kernel reusability showed kernels learned application useful similar applications achieving close state-of-the-art performance reduced training time. references stefano baccianella andrea esuli fabrizio sebastiani. sentiwordnet enhanced lexical resource sentiment analysis opinion mining. nicoletta calzolari khalid choukri bente maegaard joseph mariani odijk stelios piperidis mike rosner daniel tapias editors proceedings seventh conference international language resources evaluation valletta malta may. european language resources association peter brown peter desouza robert mercer vincent della pietra jenifer lai. class-based n-gram models natural language. computational linguistics chen jiang wang liang-chieh chen haoyuan nevatia. abc-cnn attention based convolutional neural network visual question answering. corr abs/.. deep convolutional neural maira gatti. networks sentiment analysis short texts. proceedings coling international conference computational linguistics technical papers pages dublin ireland august. dublin city university association computational linguistics. baotian zhengdong hang convolutional neural qingcai chen. network architectures matching natural language sentences. advances neural information processing systems pages yoon kim. convolutional neural networks sentence classiﬁcation. proceedings conference empirical methods natural language processing pages doha qatar october. association computational linguistics. ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural advances neural information networks. processing systems pages ankit kumar ozan irsoy jonathan james bradbury robert english brian pierce peter ondruska ishaan gulrajani richard socher. anything dynamic memory networks natural language processing. arxiv preprint arxiv.. andrew maas raymond daly peter pham huang andrew christopher potts. learning word vectors sentiment analysis. proceedings annual meeting association computational linguistics human language technologies pages portland oregon june. association computational linguistics. pang lillian lee. sentimental education sentiment analysis using subjectivity summarization based minimum cuts. proceedings annual meeting association computational linguistics page association computational linguistics. pang lillian lee. seeing stars exploiting class relationships sentiment categorization respect rating scales. proceedings annual meeting association computational linguistics pages arbor michigan june. association computational linguistics. mingxuan wang zhengdong hang wenbin jiang liu. gencnn convolutional architecture word sequence prediction. proceedings annual meeting association computational linguistics international joint conference natural language processing pages beijing china july. association computational linguistics. wenpeng hinrich sch¨utze. multichannel variable-size convoproceedings lution sentence classiﬁcation. nineteenth conference computational natural language learning pages beijing china july. association computational linguistics. xiang zhang junbo zhao yann lecun. character-level convolutional networks text classiﬁcation. advances neural information processing systems pages dragomir radev. dependency sensitive convolutional neural networks modeling proceedings sentences documents. conference north american chapter association computational linguistics human language technologies pages diego california june. association computational linguistics. richard socher brody huval christopher manning andrew semantic compositionality recur. proceedings sive matrix-vector spaces. joint conference empirical methods natural language processing computational natural language learning pages jeju island korea july. association computational linguistics. richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality proceedings sentiment treebank. conference empirical methods natural language processing pages seattle washington october. association computational linguistics. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research sheng richard socher christopher manning. improved semantic representations tree-structured long arxiv preprint short-term memory networks. arxiv.. duyu tang. sentiment-speciﬁc representation learning document-level sentiment analysis. proceedings eighth international conference search data mining pages acm. joseph turian lev-arie ratinov yoshua bengio. word representations simple general method semi-supervised learning. proceedings annual meeting association computational linguistics pages uppsala sweden july. association computational linguistics. matthew zeiler sixin zhang yann fergus. regularization neural networks using dropconnect. proceedings international conference machine learning pages sida wang christopher manning. baselines bigrams simple good sentiment topic classiﬁcation. proceedings annual meeting association computational linguistics pages jeju island korea july. association computational linguistics. movie review movie review dataset contains positive negative reviews introduced subj dataset contains subjective objective sentences introduced problem classify given sentence subjective objective.", "year": 2016}