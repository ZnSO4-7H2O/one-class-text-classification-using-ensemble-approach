{"title": "State Representation Learning for Control: An Overview", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. As the representation learned captures the variation in the environment generated by agents, this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning.  This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.", "text": "representation learning algorithms designed learn abstract features characterize data. state representation learning focuses particular kind representation learning learned features dimension evolve time inﬂuenced actions agent. representation learned captures variation environment generated agents kind representation particularly suitable robotics control scenarios. particular dimension helps overcome curse dimensionality provides easier interpretation utilization humans help improve performance speed policy learning algorithms reinforcement learning. survey aims covering state-of-the-art state representation learning recent years. reviews diﬀerent methods involve interaction environment implementations applications robotics control tasks particular highlights generic learning objectives diﬀerently exploited reviewed algorithms. finally discusses evaluation methods assess representation learned summarizes current future lines research. state representation learning dimensional embedding learning learning keywords disentangled representations disentanglement control factors robotics reinforcement learning robotics control artiﬁcial intelligence broad perspective heavily rely availability compact expressive representations sensor data. designing representations long performed manually designer deep learning provides general framework learn representations data. particularly interesting robotics multiple sensors provide high dimensional data robot objective often expressed much lower dimensional space dimensional representation frequently called state system crucial role encoding essential information discarding many irrelevant aspects original data. state representation basis classical reinforcement learning framework agent interacts environment choosing action function environment state order maximize expected reward. following framework call observation information provided several robot sensors call state compact depiction observation retains information necessary robot choose actions. deep reinforcement learning algorithms shown possible learn controllers directly observations reinforcement learning take advantage dimensional informative representations instead data solve tasks eﬃciently eﬃciency critical robotic applications experimenting action costly operation. robotics well machine learning ﬁnding deﬁning interesting states control tasks usually requires considerable amount manual engineering. therefore interesting learn features little supervision possible. goal thus avoid direct supervision using true state instead information actions performed agent consequences observation space rewards along information also generic constraints good state representation feature learning general wide domain aims decomposing data diﬀerent features faithfully characterize particular motivation deep learning automatically learn large range speciﬁc feature detectors high dimensional problems. state representation learning particular case feature learning features learn dimensional evolve time inﬂuenced actions interactions. generally framed control setup constrained favor small dimensions characterize instance environment object often semantic meaning correlates physical feature. physical feature position distance angle orientation. objective take advantage time steps actions optionally rewards transform observations states reduced vector representative features suﬃcient eﬃcient policy learning also worth distinguishing feature learning process observed learning state representation process learning agent possesses embodiment acts. latter opens broader research questions gives rise exploiting possibilities active learning artiﬁcial curiosity balancing exploration exploitation. stated above learning context performed without explicit supervision. article therefore focus learning pattern recognition regression classiﬁcation sense rather sense process model building building models exploit large objectives constraints possibly taking inspiration human learning. example infants expect inertial objects follow principles persistence continuity cohesion solidity appearance-based elements color texture perceptual goodness. time principles help guide later learnings object’ rigidity softness liquids properties. later adults reconstruct perceptual scenes using internal representations objects physically relevant properties literature make knowledge physics world interactions rewards whenever possible semi-supervision self-supervision aids challenge learning state representations without explicit supervision. figure notation illustration represents true state learned state computed observation ˆst+ estimation prediction learned state e.g. forward model. notation applies observation action review paper objective present analyze diﬀerent approaches highlight commonalities diﬀerences propose research directions. extend previously published review recent rapidly evolving literature past years focus approaches learn dimensional markovian representations without direct supervision i.e. exploiting sequences observations actions rewards generic learning objectives. works selected survey mostly evaluate algorithms simulations agents interact environment. marginally algorithms tested real settings robotics tasks e.g. manipulation exploration. remainder paper ﬁrst introduce formal framework notation present objectives used learn state representations discuss implementation aspects approaches summarizing current future lines research. formalism nomenclature deﬁnes reinforcement learning literature environment agent performs actions time step action space action makes agent transition true state ˜st+. call true state space agent obtains observation time step sensors makes possible learn interacting actions. often given state performing action agent receive reward reward given reward function agent tries maximize. reward function designed order lead agent certain behavior solves task. denote reconstruction estimate estimate notation organization shown fig. task learn representation dimension characteristics similar ˜st. formally usually ﬁnds mapping history observation current state note actions rewards also added parameters paper speciﬁcally interested particular setting mapping learned proxy objectives without exploitation true state using observations actions rewards generic objectives lead good representations. family approaches called unsupervised self-supervised. based previously deﬁned notations brieﬂy summarize common strategies used state representation learning detailed next sections. following represents parameters optimized minimizing model’s loss function. model generally implemented neural network. figure schema diﬀerent transitions projections diﬀerent spaces. represents true state learned state estimation prediction learned state. notation applies observation action schema shows respective highlighted components compared optimized loss functions. learning function possible reconstruct observation decoder minimizing reconstruction error diﬀerent constraints sparse encoding constraints etc.) learning model makes possible impose structural constraints model state representation learning. example forward model constrained linear imposing system learned state space follows simple linear dynamics. learning inverse model inverse model predicts action given observations using prior knowledge constrain state space last approach handle using speciﬁc constraints prior knowledge functioning dynamics physics world temporal continuity causality principles generally reﬂect interaction agent objects environment priors deﬁned objective loss functions applied states minimized certain condition example condition enforcing locality time proximity within states. state representation characteristics besides general idea state representation role encoding essential information discarding irrelevant aspects original data detail characteristics good state representation are. able represent true value current state well enough policy improvement. able generalize learned value-function unseen states similar futures. dimensional eﬃcient estimation. note characteristics expected state representation canused learning representation. instead later veriﬁed assessing task performance controller based learned state. note also multiple state representations verify properties given problem therefore unique solution state representation learning problem. detail problem discussing evaluation learned state space section state representation learning also linked idea learning disentangled representations clearly separate diﬀerent factors variation diﬀerent semantics. following good representation must suﬃcient eﬃcient possible minimal minimal assumption comparable simplicity prior assumes small number world properties relevant exists dimensional state representation higher level input observation. related occam’s razor prior favors state representations exclude irrelevant information encourage lower dimensionality. eﬃciency aspect representation means overlapping dimensions learned state features. unfortunately independence features alone enough assure good quality representations guarantee disentanglement factors variation. higher level abstractions however allow improve disentanglement permit easier generalization transfer. cues disentangle underlying factors include spatial temporal scales marginal independence variables controllable factors state representation learning applications main interest produce dimensional state space learning control policy eﬃcient. indeed deep reinforcement learning observation space shown spectacular results control policy learning known computationally diﬃcult requires large amount data. separation representation learning policy learning lighten complete process. approach used reviewed papers make reinforcement learning faster time and/or lighter computation. learned policies applied real robots and/or simulation settings detailed section particularly relevant multimodal observations produced several complementary sensors high dimensionality example case autonomous vehicles. dimensional representations make algorithm able take decisions hidden factors extracted complementary sensors.this instance case representation learning diﬀerent temporal signals audio images blended depth combined also used transfer learning setting taking advantage state space learned given task rapidly learn related task. example case state space related robot position learned given navigation task reused quickly learn another navigation task. also used pretraining transfer applications afterwards reinforcement learning. concrete examples application scenarios section another case could useful application evolution strategies robot control learning evolution strategies family black optimization algorithms rely gradient descent alternative techniques less adapted high-dimensional problems. optimization methods could take clear advantage lower dimension input explore much faster parameter space using data. section review objectives used learn relevant state representation. schema detailing core elements involved model’s loss function fig. highlights main approaches described here.this section touches upon machine learning reconstructing observation ﬁrst idea exploited fact true state along noise used generate observation. hypothesis noise large compressing observation retain important information contained true state. idea often exploited dimensionality reduction algorithms principal component analysis focus recent approaches speciﬁcally dealing state representation. algorithm linear transformation able compress decompress observations minimal reconstruction error. classical approaches exploited reduce dimensionality state space learning projecting images -dimensional space possible produce state used reinforcement learning algorithm reduces convergence time super mario games diﬀerent simulations swimmers mountain car. auto-encoders models learn reproduce input constraints internal representations dimensionality constraints architecture therefore used learn particular representation dimensions reconstructing simple auto-encoders used learn representation real pole images training encoding vector used learn controller balance pole. auto-encoder whose internal representation constrained represent position order learn spatial state representation serves input controller also presented proposed model learns state representation pixels robot’s hand. models based auto-encoders reconstruct observation time step however learn factors variations linked state prominent features exists. order relax assumption possible reconstruct observations time steps constraints evolution state focus reconstruction features relevant system dynamics. auto-encoder siamese encoder example project sequences images state representation space constraints fact transition linear observations several time steps order take time account representation predict future observations single decoder reconstructs ˆot+. makes model able learn representations related time random features environment ﬁltered out. idea using auto-encoder learn projection state space transitions assumed linear also used model presented embed control\" consists deep generative model learns generate image trajectories linear latent space. extending state encoders decoders trained respects transition dynamics controlled system state space time reconstruct observations compared diﬀerent types autoencoders learn visual tactile state representations representation afterwards learn manipulation task policies robot. sharing idea deep variational bayes filter extension kalman ﬁlters learn reconstruct observation based nonlinear state space using variational inference reconstruction linear state space based model inspired deep dynamical model proposed argued model adapted better training eﬃciency learn tasks complex non-linear dynamics result shows improvements pilco model learns state representation minimizing reconstruction error without constraining latent space. learning forward model last subsection discussed compressing information contained single observation results useful task learn representations. next show temporal dynamics system also help purpose. therefore present approaches rely learning forward model learn state space. general idea force eﬃciently encode information necessary make prediction next state case forward models study here model used proxy learning model ﬁrstly makes projection observation space state space obtain applies transition predict ˆst+. error computed comparing estimated next state ˆst+ value derived next observation next time step. forward models beneﬁt observation reconstruction objective presented section example works presented section belong auto-encoder category models. however predict future observations learn representations therefore well belong family forward models. method works combine forward models auto-encoders consists mapping compute transition help obtain ˆst+. ˆst+ remapped onto pixel space form vector ˆot+. error computed pixel-wise ˆot+ ot+. common assumption forward model learned state space linear transition linear combination either ﬁxed learned parameters using distributions compute ˆst+ allows kl-divergence train forward model. method also used however transition model considers kl-divergence loss reconstruction based ˆot+. nearly mandatory forward models contain enough information predict st+. however approaches need actions assuming transition allows deduce transition circumvent letting model several past states predict st+. another forward model connected intrinsic curiosity model helps agents explore discover environment curiosity extrinsic rewards sparse present proposed model reward signal computed forward model’s loss function argued incentive model learn encoding environmental features cannot inﬂuence inﬂuenced agent’s actions. learned exploration strategy agent therefore robust uncontrollable aspects environment presence distractor objects changes illumination sources noise environment related exploitation forward model idea learning controllable representation controllability prior controllable objects relevant state representation learning elements controlled robots likely relevant task. robot acts applying forces controllable things could whose accelerations correlate actions robot. accordingly loss function deﬁned minimize covariance action dimension accelerations state dimension following formula makes explicit information-theoretic capacity agent’s actuation channel inﬂuence evolution. concept empowerment related accountability agency i.e. recognizing agent responsible originating change state environment learning inverse model forward model approach turned around instead learning predict next state current next states predict action them. inverse model framework used ﬁrstly performing projection onto learned states secondly predicting action would explain transition before learning model impose constraints state representation able eﬃciently predict actions. example using inverse models learn state representation intrinsic curiosity module integrates inverse forward model surprise element action prediction used reward action selection. bypass hard problem predicting original observations since actions much lower dimension. inverse model parameters θinv trained optimize diﬀerent kind inverse model used policy gradient used learn state representation augmented auxiliary gradients called self-supervised tasks. case lack external supervision prediction error resulting interactions environment acts self-supervision. learned inverse dynamics model retrieve action performed successive time step. note connections among forward inverse models important example forward models regularize inverse dynamics models latter provide supervision construct informative features forward model predict regularize feature space inverse model. practice implemented decomposing joint loss function inverse model loss plus forward model loss another approach including forward inverse models well reconstruction observation including multimodal input using feature adversarial learning adversarial networks also used unsupervised learning state representations. generative adversarial network framework learn state representations proposed present model named infogan achieves disentanglement latent variables poses objects. described goal learn generator distribution matches real distribution pdata. instead trying explicitly assign probability every data distribution gans learn generator network samples generator distribution transforming noise variable pnoise sample noise variable components. ﬁrst randomly sampled gaussian distribution second smaller dimension sampled uniform distribution. latter used training high mutual information then sample high correlation thus considered state representation. generator trained playing adversarial discriminator network aims distinguishing samples true distribution pdata generator distribution authors succeed learn states corresponding object orientations sequences images. another example generative adversarial network presented bigan extension regular gans learn double mapping image space latent space latent space image space. allows learned feature representation useful auxiliary supervised discrimination tasks competitive unsupervised self-supervised feature learning. bigan also experimented learn state representations used reinforcement learning compared approach exploiting rewards opposed reward value compulsory. however used supplementary information help diﬀerentiating states learn task related representations. rewards helpful information disentangle meaningful information noisy distracting representation particular task. however multi-task setting approach used learn generic state representation relevant diﬀerent tasks. predictable reward prior estimates ˆrt+ given certain state action implemented similar approach integrates learned state representations architecture later used learning policy also learns predict future rewards latter case order learn policy reward used. dimensionality reduction model called reward weighted principal component analysis another using rewards state representation proposed rwpca uses data collected algorithm operates dimensionality reduction strategy takes reward account keep information compressed form. compressed data afterwards used learn policy. idea constructing task-related representation rewards supplementary information impose constraints state space topology. constraints makes space suited discriminate states diﬀerent rewards. state space particularly adapted solve given task. constraint called causality prior assumes diﬀerent rewards performing action states diﬀerentiated away representation space objective functions section present approaches assuming various speciﬁc constraints state representation learning. learning process constrained prior knowledge allow agent leverage existing common sense intuitive physics physical laws mental states others well abstract regularities compositionality causality kind priori knowledge called prior deﬁned cost functions. loss functions applied state space order impose required constraints construct model projecting observations state space. following diﬀerence states times observations. e−distance used similarity measure distance among states goes increasing distance states. note prior counter-balancing slowness prior introduced slowness alone would lead constant values. proportionality prior introduced assumes action diﬀerent states reactions action proportional amplitude eﬀect. representation vary amount equal actions diﬀerent situations. classiﬁed learned function output corrupted time step. negative samples produced incorporating observations wrong time step sequence images discriminative approach forces encode dynamics states. states learned using idea factors objects correspond ‘independently controllable’ aspects world discovered interacting environment knowing dimension state space environment transition distribution action selectivity maximal single feature changes result action. maximizing selectivity improves disentanglement controllable factors order learn good state representation. using hybrid objectives reconstruction data observation space forward models inverse models exploitation rewards objective functions presented previous sections diﬀerent approaches tackle state representation learning challenge. however approaches incompatible works often take advantage several objective functions time. instance interactively learning poke poking example empirical learning intuitive physics using current goal images respectively order predict poke action. latter composed location angle length action sets object state goal image simulations shows using inverse model jointly inverse forward models beat blob model pushing objects training data available reduced joint model outperforms inverse model performance comparable using considerably larger amount data. authors reconstruction observation slowness principle approach. combine reconstruction observation forward models. take advantage rewards causality prior several objective functions slowness principle proportionality repeatability learn state representations. illustrate example combination objective functions figure table summarizes reviewed models. shows model combination models used approach presented. model particular approach indicates proxies surrogate functions used learning reconstruction observation prediction future and/or retrieving actions kind information used action and/or rewards. section cover various implementation aspects relevant state representation learning evaluation. refer speciﬁc surrogate models loss function speciﬁcation tools strategies help constraining information bottleneck generalizing learning lowdimensional state representations learning tools ﬁrst detail models auxiliary objective function help learning state representation. several learning tools integrated broader approaches previously described. auto-encoders common tool used learn state representations widely used dimensionality reduction objective output reproduction input. architecture composed encoder decoder. encoder projects input latent space representation re-projected output afterwards decoder. problem setting input latent representation output. dimensionality latent representation chosen depending dimension state representation want learn enforcing case. automatically learn compact representation minimizing reconstruction error input output. usual loss function measure reconstruction error mean squared error input output computed pixel-wise. however norm. main issue auto-encoders risk ﬁnding easy satisfying solution minimize pixel reconstruction error. occurs decoder reconstructs kind average looking dataset. make training robust kind mean optimization solution denoising auto-encoders used. architecture adds noise input makes average\" image corrupted solution original architecture used learn visual tactile state representations. authors compared state representations learned variational auto-encoder using learned states reinforcement learning setting. found that cases state representation models gather less rewards state representations. literature also beneﬁted variational inference used variational autoencoders learn mapping observations state representations. interprets sampled distribution approximates model called approximate posterior recognition model. represents parameters model often neural network. also provides generator approximates model parameters generator. models trained optimizing error divergence normal distribution siamese networks consist identical networks share parameters i.e. exact weights. objective siamese architecture classify input data diﬀerentiate inputs kind architecture useful impose constraints latent space neural network. example used learn similarity metrics time dependencies case time-contrastive networks context siamese networks employed implement priors previously presented section example siamese networks used compute similarity loss optimize slowness principle three siamese networks compute three consecutive states time another model predicts next state. observation/action spaces section presents summary dimensions used reviewed papers. also point continuity/discreteness actions states modeled considering diﬀerent dimensions complexity learned. general higher dimensionality harder task learning state representations. practice lack solid theory dimension rarely minimal literature normally presents results presumably higher dimensionality states learned theoretically needed. dimensionality state seem obvious learning state correlate clear dimension environment. however deciding dimensionality state space often trivial learning abstract states clear dimension associated instance visually representing state associated atari game scene complex situation eﬀects capacity dimensionality observed diﬀerent application scenarios. table details dimensionality observation state action spaces well application evaluated. cardinality continuity discreetness action space also shown. best proxies assess complexity problem tackled together number dimensions observation space since well established procedure select size minimal state representation learned often models learn state space that scenarios larger dimension want data show correlation with. evaluating learned state representations section provides review validation metrics embedding quality evaluation techniques used across literature. summarized table common evaluating quality learned state space letting real robot states perform task assessing whether representation general enough transferable accomplish tasks. performance algorithm applied real target task done using reinforcement learning however approach often costly ineﬃcient terms time computation data. also various state-of-the-art algorithms applied learn policy result diﬀerent performances given state representation. uncertainty inherent therefore makes algorithms suﬃcient practical appropriate necessary condition validate particular state representation. consequence would desirable intermediate manner assess representation independent algorithm applied complete task indeed several direct ways assess learned state space. example visual assessment representation’s quality done using nearest-neighbors approach idea look nearest neighbors learned state space neighbor retrieve corresponding observation. visual inspection reveal observations indeed correspond nearest neighbors ground truth state space intend learn. nearest neighbor coherence assessed visually knn-mse quantitative metric derived qualitative information using ground truth state value every observation knn-mse measures distance value observation value nearest neighbor observations retrieved learned state space. distance means neighbor ground truth still neighbor learned representation thus local coherence conserved. characteristics good representation possess produce disentangled representation variation factors. evaluation characteristics done using selectivity prior prior cares independence among variations representation action. however applicable mainly actions known independent. another quantitatively compare degree disentanglement reached model using disentanglement metric score assumes data generated process generative factors known interpretable conditionally independent. metric measures independence interpretability representation consisting independent latent variables necessarily disentangled. order measure disentanglement uses simple low-capacity vc-dimension linear classiﬁer’s accuracy. classiﬁer’s goal predict index generative factor transfer learning object recognition. assumes generative factors known interpretable unsupervised learning manifold learning limited isometric embeddings kept ﬁxed given absolute linear diﬀerence inferred latent representations. ensures that average diﬀerence among embeddings ﬁxed latent factor lower diﬀerence among embeddings belonging diﬀerent factor. average accuracy classiﬁer predict latent factors reported disentanglement metric score. metrics area manifold learning used distortion nieqa share principle quantitative measures global quality representation representation space should much possible undistorted version original space. distortion gives insight quality representation measuring local global geometry coherence representation changes respect ground truth. designed embeddings context natural versatile paradigm solving problems metric spaces. nieqa complex evaluation distortion measures local geometry quality global topology quality representation. nieqa local part checks representation locally equivalent euclidean subspace preserves structure local neighborhoods. nieqa objectives aligned knn-mse measure assess quality representation especially locally. global nieqa measure also based idea preserving original structure representation space instead looking neighbors samples representative points whole state space. then considers preservation geodesic distance points state space. regarding assessment observation’s predictions quality using mean square error common approach measures used multi-scale similarity metrics assess realistic characteristics gan-generated images adversarial training methods concretely peak signal noise ratio structural similarity index measure image sharpness examples metrics next frame prediction assessment used authors propose image gradient diﬀerence multi-scale architecture adversarial training methods better loss functions mse. potential study could assess better quality observation reconstruction lead better quality states. evaluation scenarios datasets used validate state representation learning include varied mainly simulated environments easier reproduce generate. unlike image recognition challenges mnist digits imagenet datasets prevail state representation learning varied regular video games visuomotor tasks robotics found test suite robotics control. examples simulated environments include among others cart-pole consists inverted pendulum attached cart moves along frictionless track; system controlled applying force cart reward provided every time step pole remains upright. episode ends pole degrees vertical cart moves units center advanced test games include vizdoom levels passed reward accumulated exploration levels used evaluation metrics likewise mario benchmark platform designed reinforcement learning based super mario bros\" video game. task collect much points possible collecting coins kill enemies mushrooms ﬁreﬂowers completing many levels diﬃculty possible without getting hurt killed. possible actions move left/right/none jump/no jump run/ﬁre run/ﬁre. test suite example experimented evaluation benchmarks tested reviewed works survey include simulated octopus arms labyrinths navigation grids driving cars mountain scenarios another example bouncing ball goal learn representation bouncing ball position robotics domain benchmarks robot manipulation skills baxter pushing button grasping stabilizing poking objects balancing real pendulum nevertheless approaches achieve learn real environment scenarios instance mobile robots explore arena many latter simulated scenarios part universe openai deepmind labs benchmarking tasks used prominent state representation learning literature summarized table generally reinforcement learning literature provides benchmarks similar robotic tasks pushing sorting grasping objects pouring glass tasks involving humanoid static robots. deepmind control simulation platform control tasks test continuous version learning algorithms mujoco physics engine reinforcement learning also tackles complex problems road-crossing chicken game montezuma’s revenge atari game happens worse performance comes human-level control agents challenging games environments could tackled algorithms; require ahead thinking order learn states actions longer term planning. future work beneﬁt approaches applying section ﬁrst discuss implications autonomous agents assessment comparison reproducibility representation learned. finally explore consequences interpretability machine learning algorithms. models autonomous agents role environment exploration important dimension investigate srl. space suﬃciently explored agent acquisition varied observations exposure actions lead optimal performance hindered making learning relevant state space impossible. reviewed papers exploration assumed eﬃcient studied details autonomous agent question solved. incorporate exploration integrate curiosity intrinsic motivations algorithm collects data. overall idea approach complement extrinsic reward intrinsic reward favors states makes progress. done example intrinsic curiosity module deﬁning intrinsic reward linked forward model error encourages exploration. approach improved balancing exploratory behavior homeostatic drive also favor actions lead familiar stateaction pairs. reverse question learned state space inﬂuence performance intrinsic motivation approaches also relevant. automatic exploration designed maximize quality learned state representation ﬁeld explored order build high quality representations. another problem ultimately perform autonomously choice state representation dimension done manually reviewed approaches. challenge deciding dimensionality related bias-variance tradeoﬀ dimensionality representation constrains capacity model. unfortunately automatic process often choose larger dimension necessary. indeed increasing states dimension increases capacity model which result better reducing training error also leads overﬁtting. model chooses capacity consequently prone estimate dimension needed. avoid choosing manually state dimension possible choose automatically number features larger certain variance orthogonal done using produce features signiﬁcant ones selected respect chosen variance. also modiﬁed select reward related components although variance ﬁxed priori claimed usually easier choosing state dimension. extending technique state representation approach could interesting research direction. assessment comparison reproducibility assessment challenge two-sided. first easy certiﬁed validating learned representation. secondly evaluation diﬃculties makes fair comparison approaches diﬃcult. mentioned section common method evaluate quality representations check state representation learned used algorithm solve task. however assessment uncertain unstable. good performance cannot necessary condition guarantee quality states learned. uncertainty gives rise second problem. uncertainty assessment method comparison among approaches cannot objective. moreover comparing approaches published results particularly hard high variability environments data dimensions. illustrate diversity diﬀerent works reviewed diﬀerent settings table exhibits environments data dimensions involved nature data. baseline experiments easily reproduce approaches. reproducibility guidelines proper experimental techniques reporting procedures pointed therefore deﬁned srl. providing interpretable systems interpretability results machine learning challenging problem needs proper deﬁnition deﬁne interpretability context capacity human able link variation representation variation environment able know representation sensitive variation. designed able give level interpretability could help improving understanding learning algorithms’ output. indeed higher dimension less interpretable result humans. therefore dimensionality reduction induced could highly beneﬁcial improve understanding capacity creating visually compelling results. unfortunately assessment methods used learned states reinforcement learning gives generally better results dimension increases including overlapping factors. reason assessment method take account interpretability characteristic representation sense generally representations neural networks often composed overlapping dimensions thus prone understandable controllable humans. since often learns disentangled representation makes easier interpreted. objective representation learning providing interpretable explanations algorithmic decisions highly considered. european union regulations algorithmic decisionmaking include right explanation\" right opt-out\" discrimination\" models. artiﬁcial intelligence research thus granted opportunity provide meaningful explanations algorithms work case monitoring degree systems show thinking ﬂows humans invaluable crucial; explain human cognition works also help make better fair decisions reviewed state representation learning algorithms designed compress high-dimensional observations data meaningful dimensional space controlled systems. models require observations made system actions performed optionally reward associated task. presented various constraints used objective representation contain enough information able reproduce input observation; contain enough information dynamics environment predict future state given action reversely predict actions given states; contain information related task reward; follow various constraints based priori knowledge directly representation space e.g. using physics laws common sense priors. general trend many learning objectives possible depending available data. example could reconstruction objective linking state space observations combined predictive objective capture dynamics reward based objective apprehend causality actions performed. prior learning objective also added force state space coherent understandable humans. many models integrate several objectives proposed model currently includes together. designed automatically learn representations unlabeled observations could used future work learn evolving environments could step towards continual lifelong learning. another area explore future integration exploration strategies data collection speciﬁcally designed able improve state representation learned. research funded dream project european union’s horizon research innovation program grant agreement acknowledge olivier sigaud antonin raﬃn cynthia liem colleagues insightful detailed feedback.", "year": 2018}