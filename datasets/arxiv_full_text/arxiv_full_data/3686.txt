{"title": "Embarrassingly Parallel Variational Inference in Nonconjugate Models", "tag": ["stat.ML", "cs.AI", "cs.DC", "cs.LG", "stat.CO"], "abstract": "We develop a parallel variational inference (VI) procedure for use in data-distributed settings, where each machine only has access to a subset of data and runs VI independently, without communicating with other machines. This type of \"embarrassingly parallel\" procedure has recently been developed for MCMC inference algorithms; however, in many cases it is not possible to directly extend this procedure to VI methods without requiring certain restrictive exponential family conditions on the form of the model. Furthermore, most existing (nonparallel) VI methods are restricted to use on conditionally conjugate models, which limits their applicability. To combat these issues, we make use of the recently proposed nonparametric VI to facilitate an embarrassingly parallel VI procedure that can be applied to a wider scope of models, including to nonconjugate models. We derive our embarrassingly parallel VI algorithm, analyze our method theoretically, and demonstrate our method empirically on a few nonconjugate models.", "text": "develop parallel variational inference procedure data-distributed settings machine access subset data runs independently without communicating machines. type embarrassingly parallel procedure recently developed mcmc inference algorithms; however many cases possible directly extend procedure methods withrequiring certain restrictive exponential family conditions form model. furthermore existing methods restricted conditionally conjugate models limits applicability. combat issues make recently proposed nonparametric facilitate embarrassingly parallel procedure applied wider scope models including nonconjugate models. derive embarrassingly parallel algorithm analyze method theoretically demonstrate method empirically nonconjugate models. many large modern datasets collected stored distributed fashion multiple sensors datacollecting agents. examples include medical data recorded hospitals throughout country weather data gathered collection sensors data scraped network machines cell phone data collected users’ phones. inference algorithms operate distributed settings—by processing subsets data separately parallel—are particularly advantageous. mitigate need transfering data central location analysis reduce memory usage computation time inference allow continuous data collection independently operating agents allow sensitive data processed independently secure locations variational inference methods general procedures approximate inference bayesian models applied successfully wide variety domains paper concerned developing better methods distributed settings. major issue existing parallel methods often require synchronization between machines regular intervals communication machines synchronization greatly reduce eﬃciency procedures machine must wait information machines proceeding computation. furthermore communication requirements increase diﬃculty system implementation maintenance necessitate transfer data machines develop embarrassingly parallel algorithm data-distributed settings type parallel algorithm regular communication machines. given dataset partitioned collection machines embarrassingly parallel methods carry following steps however solution major restrictions form variational approximation model. namely form methods must tractably compute product variational approximations divided prior density methods limiting scope conditionally conjugate exponential family models using mean ﬁeld variational methods restrict variational approximation exponential family prior. attempt extend scope models embarrassingly parallel methods applied turn separate line work embarrassingly parallel mcmc methods alternative decomposition posterior distribution. subposterior density deﬁned recently progress made toward goal mean ﬁeld variational inference methods limited models certain exponential family restrictions likelihood prior distribution methods decomposition posterior takes advantage closedness properties exponential family densities products quotients. however modeling assumptions fairly restrictive decomposition cannot applied many popular models additionally approximation family typically inadequate capture multimodal densities separate line work aimed develop nonconjugate variational inference methods models without tractable exponential family conditional distributions similar methods would like general inference algorithm applied wide class bayesian models operates embarrassingly parallel setting. however variational families employed nonconjugate methods form allows apply abovementioned decomposition strategy parallelization. recent papers markov chain monte carlo literature introduced alternative decomposition posterior parallel inference involves product called subposterior densities apply decomposition nonconjugate variational inference method called nonparametric variational inference perform low-communication parallel inference general class models. particular require weak diﬀerentiability conditions joint probability. main contribution method provides perform embarrassingly parallel inference data distributed settings more-general class bayesian models without requiring conditional conjugacy exponential family assumptions model likelihood prior. following sections derive posterior decomposition used method show combine local nonparametric variational approximations form variational approximation full-data posterior density analyze computational complexity algorithms. finally demonstrate method empirically nonconjugate bayesian models. recently proposed method known nonparametric variational inference posterior approximation selected variational family densities form kid). advantages method capture multimodal posterior distributions applied many nonconjugate models eﬃcient algorithm optimize variational objective. case allows perform variational inference subposterior densities without worrying retain conjugacy necessary easily apply typical meanﬁeld approximations also allows develop method combine subposterior variational approximations produce full-data posterior variational approximation. running procedure subset data machine write inferred variational approximation subposterior distribution analytic form gives variational approximation subposterior density product particular product mixture-of-gaussians variational densities gives mixture-of-gaussians density components. write product mixture subposterior density lose conjugacy necessary requisite exponential-family-conditionals. hence easy directly apply methods approximate subposterior. therefore alternative decomposition apply broader scope models need family variational approximations general subposterior densities maintaining tractable density product analytically computed. embarrassingly parallel variational inference nonconjugate models parallel approximate bayesian inference method continuous posterior distributions. generally applicable requiring ﬁrst derivatives log-joint probability density computable. dataset partitioned machines parallel machine approximate subposterior densities; afterwards local subposterior approximations combined computing product approximates full-data posterior density. machine performs variational inference without sharing information embarrassingly parallel manner. summarize procedure algorithm figure illustration embarrassingly parallel method shown bayesian logistic regression model dataset. show ﬁrst dimensions full-data posterior density. show ﬁrst dimensions subposterior variational approximations running subset data independently. show ﬁrst dimensions combined product density recovers posterior shown subposteriors wish form variational approximation full-data posterior density taking product mixtures. however computing parameters weights components product mixture becomes infeasible grows. typically perform bayesian inference order compute expectations respect explore posterior distribution. practice common achieve sample posterior compute sample expectation; done mcmc methods methods hence instead computing product mixture solution bypass step directly generate samples product mixture. give procedure allows compute expectations respect variational approximation common sampling manner without requiring actually compute variational approximation. variational approximation algorithm prove yields correct samples. intuitive idea behind algorithm following. sample mixture ﬁrst sample component index sample chosen mixture component. therefore need sample product mixture components without ﬁrst computing component weights. solution form markov chain product mixture component indices prove stationary distribution categorical distribution probability mass values proportional product mixture component weights. hence step markov chain produce sample full variational approximation needing compute single product mixture component. note algorithm step markov chain perform simple steps sample next product mixture component select subposterior uniformly random re-draw components uniformly random speciﬁes product mixture component. compute ratio weight product mixture component previous component’s weight accept reject proposal compute parameters sampled component theorem procedure given algorithm deﬁnes markov chain whose stationary distribution categorical distribution category-probability parameter equal vector product mixture component weights. proof. note product mixture components associated m-dimensional vector hence instead sampling index categorical distribution equivalently view task sampling m-dimensional vector element space probability mass proportional associated product mixture component weight. therefore perform gibbs sampling space sample conditional distribution subposterior component index given component indices. compute sample conditional distribution could iterate possible values however could potentially expensive large instead sample values uniformly random algorithm becomes metropoliswithin-gibbs algorithm we’ve used independent metropolis proposal note dimension along take gibbs sampling step chosen line since metropolis-withingibbs algorithm shown correct stationary distribution proof complete. describe complexity algorithm section section verify algorithm achieves results taking expectations computing mixture product exactly drastically speeding-up performance. sequential subposterior subset products. cases simpler sample product mixture sequential fashion sampling product subposteriors multiple times ﬁrst sample components product groups approximations repeat process resulting mixtures formed sampled components. continues samples mixture remain. example could begin sampling components product pairs thereby forming uniformly weighted mixtures comprised sampled components. process repeated—forming pairs sampling pair product mixture— samples product mixture remaining method potentially advantageous intermediate round product mixture sampling could done parallel. however samples potentially required intermediate round generate valid samples full variational approximation ﬁnal product. compare eﬀectiveness method section consider dataset observations partitioned machines. assume approximated subposterior using components component deﬁned d-dimensional parameter. computing components product mixture exactly requires operations. computing samples product mixture approximation algorithm requires operations computing sequential subposterior subset product samples samples intermediate product requires operations overall could reduced operations single machine rounds sampling done parallel. machine learns communicates optimal variational parameters consist mean parameter vectors variance parameter scalars weight parameter scalars. total scalars communicated throughentire procedure. algorithms described paper hold posteriors distributions twice-diﬀerentiable densities ﬁnite-dimensional real spaces. method applied nonconjugate bayesian models models multimodal posteriors little restriction form model prior distribution. however certain model types method appropriate. include discrete models continuous posterior distributions simplex inﬁnite dimensional models. furthermore method well suited posteriors high correlations diﬀerent scales dimensions multimodal models suﬀering label switching. demonstrate empirically ability method signiﬁcantly speed nonconjugate models distributed setting maintaining accurate posterior approximation. particular experiments show that full-data nonparametric variational inference variational inference method designed nonconjugate models full dataset. method takes parameter number mixture components subposterior inference data subsets —the subposterior variational approximations subsets data. method takes parameter number mixture components returns approximations. compare beneﬁts general work therefore exclude comparisons alternative approximate inference methods mcmc expectation propagation deterministic dynamics inference algorithms assess performance method compute log-likelihood held-out test data given inferred variational approximation experiments conducted standard cluster system. obtained subposterior variational approximations submitting batch jobs worker since jobs independent. saved results disk worker transfered machine performed product mixture sampling algorithms. following experiments involving timing ﬁrst variational inference optimization procedures convergence afterwards added time required learning subposterior approximations time needed transfer learned parameters master machine time required product mixture sampling algorithms generalized linear models widely used variety regression classiﬁcation problems. hierarchical bayesian logistic regression model test case following experiments. model places figure experimental results hierarchical bayesian logistic regression varying numbers data show epvi sample method maintains splits mixture components consistent classiﬁcation accuracy wide range ﬁcients passed logistic transform; further gamma priors placed variance parameter coeﬃcient. notably model lacks conditional conjugacy. write generative model data. demonstrate methods susy particles dataset∗ task classify whether given signal produce supersymmetric particle. dataset observations hold evaluating test log-likelihood. report maximum time taken converge average negative log-likelihood also record time taken converge negative log-likelihood standard result. number mixture components part methods ﬁxed plot results figure epvi sample reduces time convergence order magnitude maintaining nearly test negative log-likelihood nvi. figure show performance epvi sample method suﬀer increase number machines greater range table give interpretable view performance show classiﬁcation accuracy classiﬁcation accuracy stays nearly constant approximately increase throughout range. performance varying next vary number mixture components record held-out negative log-likelihood time taken converge method. parallel methods plot results figure values epvi sample decreases time convergence nearly tenfold maintaining virtually identical test negative loglikelihood values. product mixture sampling methods. also conduct experiments judge quality product mixture sampling procedures. show methods yield similar test loglikelihoods computing expectations exact product mixture greatly decreasing computation time. demonstrate empirically range values. note that since need compare exact product restrict range values compute product mixture components. sampling methods note perform rounds epvi subset sequentially machine samples collected plot results figure sampling methods yield similar held-out negative log-likelihoods exact product values. also roughly time needed compute exact product increases substantially. additionally epvi sample appears fare slightly better epvi subset terms test log-likelihood computation time. next apply algorithm nonlinear matrix factorization model known topographic latent source analysis model viewed representing observed matrix covariate-dependent superposition latent sources. particular observed matrix rn×v assumed drawn conditioned observed matrix covariates rn×c inferred weight matrix rc×l basis matrix rl×v constructed evaluating ¯rl}. similar {¯rl written exp{λ− performance varying similar previous model ﬁrst conduct experiments showing held-out negative log-likelihood versus time varying values number data-splits mixture components results shown figure epvi sample reduces time convergence maintaining similar test negative log-likelihood nvi. also evaluate performance method diﬀerent numbers latent sources vary record held-out negative log-likelihood time taken converge positive results paper developed embarrassingly parallel algorithm bayesian inference distributed setting require models conditional conjugacy exponential family assumptions. unlike existing methods strategy uses decomposition full-data posterior involving product subposterior densities recently developed parallel mcmc literature. shown promising empirical results nonconjugate models illustrate ability method perform distributed setting provide large speed-ups maintaining accurate posterior approximation. sinead williamson avinava dubey eric xing. parallel markov chain monte carlo nonparametric mixture models. proceedings international conference machine learning pages feng ningyi yuan parallel inference latent dirichlet allocation graphics processing units. advances neural information processing systems pages zhai jordan boyd-graber nima asadi mohamad alkhouja. ﬂexible large scale topic modeling package using variational inference mapreduce. proceedings international conference world wide pages", "year": 2015}