{"title": "Learning to Navigate in Complex Environments", "tag": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "abstract": "Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.", "text": "piotr mirowski∗ razvan pascanu∗ fabio viola hubert soyer andrew ballard andrea banino misha denil ross goroshin laurent sifre koray kavukcuoglu dharshan kumaran raia hadsell learning navigate complex environments dynamic elements important milestone developing agents. work formulate navigation question reinforcement learning problem show data efﬁciency task performance dramatically improved relying additional auxiliary tasks leveraging multimodal sensory inputs. particular consider jointly learning goal-driven reinforcement learning problem auxiliary depth prediction loop closure classiﬁcation tasks. approach learn navigate sensory input complicated mazes approaching human-level performance even conditions goal location changes frequently. provide detailed analysis agent behaviour ability localise network activity dynamics showing agent implicitly learns navigation abilities. ability navigate efﬁciently within environment fundamental intelligent behavior. whilst conventional robotics methods simultaneous localisation mapping tackle navigation explicit focus position inference mapping follow recent work deep reinforcement learning propose navigational abilities could emerge by-product agent learning policy maximizes reward. advantage intrinsic end-to-end approach actions divorced representation rather learnt together thus ensuring task-relevant features present representation. learning navigate reinforcement learning partially observable environments however poses several challenges. first rewards often sparsely distributed environment goal location. second environments often comprise dynamic elements requiring agent memory different timescales rapid one-shot memory goal location together short term memory subserving temporal integration velocity signals visual observations longer term memory constant aspects environment improve statistical efﬁciency bootstrap reinforcement learning procedure augmenting loss auxiliary tasks provide denser training signals support navigation-relevant representation learning. consider additional losses ﬁrst involves reconstruction low-dimensional depth time step predicting input modality others auxiliary task concerns geometry environment aimed encourage learning representations obstacle avoidance short-term trajectory planning. second task directly invokes loop closure slam agent trained predict current location previously visited within local trajectory. figure views small maze large maze i-maze corresponding maze layouts sample agent trajectories. mazes made public different textures visual cues well exploration rewards goals address memory requirements task rely stacked lstm architecture evaluate approach using maze environments demonstrate accelerated learning increased performance proposed agent architecture. environments feature complex geometry random start position orientation dynamic goal locations long episodes require thousands agent steps also provide detailed analysis trained agent show critical navigation skills acquired. important neither position inference mapping directly part loss; therefore performance goal ﬁnding task necessarily good indication skills acquired. particular show proposed agent resolves ambiguous observations quickly localizes complex maze localization capability correlated higher task reward. rely end-to-end learning framework incorporates multiple objectives. firstly tries maximize cumulative reward using actor-critic approach. secondly minimizes auxiliary loss inferring depth observation. finally agent trained detect loop closures additional auxiliary task encourages implicit velocity integration. reinforcement learning problem addressed asynchronous advantage actor-critic algorithm relies learning policy value function given state observation policy value function share intermediate representations computed using separate linear layer topmost layer model. agent setup closely follows work refer work details details also found appendix baseline consider work agent receives input environment using either recurrent purely feed-forward model encoder input layer convolutional network. support navigation capability approach also rely agent employs two-layer stacked lstm convolutional encoder. expand observations agents include agent-relative velocity action sampled stochastic policy immediate reward previous time step. feed velocity previously selected action directly second recurrent layer ﬁrst layer receiving reward. postulate ﬁrst layer might able make associations reward visual observations provided context second layer policy computed. thus observation include image r×w×h convolutional encoder followed feedforward layer policy value function outputs; lstm layer; uses additional inputs well stacked lstm; additional outputs predict depth loop closures. figure shows augmentation different possible auxiliary losses. particular consider predicting depth convolutional layer lstm layer predicting loop closure auxiliary losses computed current frame single layer mlp. agent trained applying weighted gradients coming gradients depth prediction gradients loop closure details online learning algorithm given appendix primary input agent form images. however depth information covering central ﬁeld view agent might supply valuable information structure environment. depth could directly used input argue presented additional loss actually valuable learning process. particular prediction loss shares representation policy could help build useful features much faster bootstrapping learning. since know single frame enough predict depth know auxiliary task learnt. comparison depth input versus additional loss given appendix shows signiﬁcant gain depth loss. since role auxiliary loss build representation model necessarily care speciﬁc performance obtained nature prediction. care data efﬁciency aspect problem also computational complexity. loss useful main task converge faster compared solving problem additional computational cost minimal. achieve resolution variant depth reducing screen resolution pixels. explore different variants loss. ﬁrst choice phrase regression task natural choice. formulation combined higher depth resolution extracts information mean square error imposes unimodal distribution address possible issue also consider classiﬁcation loss depth position discretised different bands. bands non-uniformally distributed attention far-away objects motivation classiﬁcation formulation greatly reduces resolution depth ﬂexible learning perspective result faster convergence loop closure like depth valuable navigating agent since used efﬁcient exploration spatial reasoning. produce training targets detect loop closures based similarity local position information episode obtained integrating velocity time. speciﬁcally trajectory noted position agent time deﬁne loop closure label equal position agent close position earlier time order avoid trivial loop closures consecutive points trajectory extra condition intermediary position thresholds provide limits. learning predict binary loop label done minimizing bernoulli loss output single-layer output hidden representation last hidden layer model followed sigmoid activation. rich literature navigation primarily robotics literature. however focus related work deep deep q-networks breakthroughs extremely challenging domains atari recent work developed on-policy methods advantage actor-critic asynchronous training multiple agents parallel recurrent networks also successfully incorporated enable state disambiguation partially observable environments deep recently used navigation domain. kulkarni used feedforward architecture learn deep successor representations enabled behavioral ﬂexibility reward changes mazebase gridworld provided means detect bottlenecks vizdoom. used feedforward siamese actor-critic architecture incorporating pretrained resnet support navigation target discretised environment. investigated performance variety networks external memory simple navigation tasks minecraft block world environment. tessler also used minecraft domain show beneﬁt combining feedforward deep-q networks learning resuable skill modules transfer navigation tasks. trained convnet dqn-based agent using depth channel inputs obstacle avoidance environments. barron investigated well convnet predict depth channel minecraft environment depth training agent. auxiliary tasks often used facilitate representation learning recently incorporation additional objectives designed augment representation learning auxiliary reconstructive decoding pathways yielded beneﬁts large scale classiﬁcation tasks. deep settings however previous papers examined beneﬁt auxiliary tasks. speciﬁcally consider supervised loss ﬁtting recurrent model hidden representations predict next observed state context imitation learning sequences provided experts lample chaplot show performance agent ﬁrst-person shooter game vizdoom environment substantially enhanced addition supervised auxiliary task whereby convolutional network trained enemy-detection task information presence enemies weapons etc. provided game engine. contrast contribution addresses fundamental questions learn intrinsic representation space geometry movement simultaneously maximising rewards reinforcement learning. method validated challenging maze domains random start goal locations. figure rewards achieved agents different tasks static mazes ﬁxed goals static mazes comparable layout dynamic goals i-maze. results averaged random hyperparameters agent-task conﬁguration. star label indicates reward clipping. please text details. information local depth information. action space discrete allows ﬁnegrained control comprising actions agent rotate small increments accelerate forward backward sideways induce rotational acceleration moving. reward achieved environments reaching goal random start location orientation. goal reached agent respawned start location must return goal. episode terminates ﬁxed amount time expires affording agent enough time goal several times. sparse ‘fruit’ rewards serve encourage exploration. apples worth point strawberries points goals points. videos agent solving maze linked appendix static variant maze goal fruit locations ﬁxed agent’s start location changes. dynamic variant goal fruits randomly placed every episode. within episode goal apple locations stay ﬁxed episode ends. encourages explore-exploit strategy agent initially explore maze retain goal location quickly reﬁnd respawn. variants consider small large map. small mazes episodes last timesteps large mazes steps observation i-maze environment inspired classic t-maze used investigate navigation rodents layout remains ﬁxed throughout agent spawns central corridor apple rewards locate goal placed alcove four arms. goal hidden alcove optimal agent behaviour must rely memory goal location order return goal using direct route. goal location constant within episode varies randomly across episodes. different agent architectures described section evaluated training mazes. figure shows learning curves agents feedforward model recurrent model stacked lstm version velocity previous action reward input depth prediction convolution layer depth prediction last lstm layer loop closure prediction well figure left example depth predictions sampled every steps. right example loop closure prediction. agent starts gray square trajectory plotted gray. blue dots correspond true positive outputs loop closure detector; cross correspond false positives green cross false negatives. note false positives occur agent actually squares away actual loop closure. auxiliary losses considered together case experiments randomly sampled hyper-parameters mean runs well curves plotted. expert human scores established professional game player compared results. ac+d agents reach human-level performance static attain human scores random goal mnih reward clipping used stabilize learning technique employed work well. unfortunately particular tasks yields slightly suboptimal policies agent distinguish apples goals removing reward clipping results unstable behaviour base agent however seems auxiliary signal depth prediction mediates problem extent resulting stable learning dynamics clearly indicate whether reward clipping used adding asterisk agent name. figure also explores difference formulations depth prediction regression task classiﬁcation task. regression agent performs worse classiﬁcation result extends maps therefore classiﬁcation formulation results. also predicting depth last lstm layer performs better. note particular results learning curves. figure consider feedforward model versus lstm version even though navigation seems intrinsically require memory single observations could often ambiguous feedforward model achieves competitive performance static mazes. suggest might good strategies involve temporal memory give good results namely reactive policy held weights encoder learning wall-following strategy. motivates dynamic environments encourage memory general navigation strategies. figure also shows advantage adding velocity reward action input well impact using layer lstm though agent better simple architectures still relatively slow train mazes. believe mainly slower data inefﬁcient learning generally seen pure approaches. supporting adding auxiliary prediction targets depth loop closure speeds learning dramatically mazes strongest effect static mazes accelerated learning also gives substantial lasting performance increase random goal mazes. although place value task performance auxiliary losses report results loop closure prediction task. test episodes steps each within large maze ac*+dl agent demonstrated successful loop detection reaching score sample trajectory seen figure table comparison four agent architectures maze conﬁgurations including random static goals. score human averaged best hyperparameters. evaluation single best performing agent done analysis test episodes. goals gives number episodes goal reached times. position accuracy classiﬁcation accuracy position decoder. latency average time ﬁrst goal acquisition average time subsequent goal acquisitions. score mean score test episodes. order evaluate internal representation location within agent train position decoder takes representation input consisting linear classiﬁer multinomial probability distribution discretized maze locations. small mazes locations large mazes locations i-maze locations. note backpropagate gradients position decoder rest network. position decoder representation exposed model change example position decoding ac+d agent shown figure initial uncertainty position improved near perfect position prediction observations acquired agent. observe position entropy spikes respawn decreases agent acquires certainty location. additionally videos agent’s position decoding linked appendix complex mazes localization important purpose reaching goal seems position accuracy ﬁnal score correlated shown table pure feed-forward architecture still achieves accuracy static maze static goal suggesting encoder memorizes position weights small maze solvable agents sufﬁcient training time. random goal ac+d achieves best position decoding performance whereas lstm architectures approximately i-maze opposite branches maze nearly identical exception sparse visual cues. observe goal ﬁrst found ac*+dl agent capable directly returning correct branch order achieve maximal score. however linear position decoder agent accurate whereas plain lstm agent. hypothesize symmetry i-maze induce symmetric policy need sensitive exact position agent figure trajectories ac*+dl agent i-maze ac+d random goal maze course episode. beginning episode agent explores environment ﬁnds goal unknown location subsequent respawns agent consistently returns goal. value function plotted episode rises agent approaches goal. goals plotted vertical lines. figure trajectory ac+d agent random goal maze overlaid position probability predictions predicted decoder trained lstm hidden activations taken steps episode. initial uncertainty gives accurate position prediction agent navigates. desired property navigation agents random goal tasks able ﬁrst goal reliably return goal efﬁcient route subsequent re-spawns. latency column table shows ac+d agents achieve lowest latency goal goal discovered figure shows clearly agent ﬁnds goal directly returns goal rest episode. random goal none agents achieve lower latency initial goal acquisition; presumably larger challenging environment. figure shows shows trajectories traversed agent four goal locations. initial exploratory phase goal agent consistently returns goal location. visualize agent’s policy applying tsne dimension reduction cell activations step agent four goal locations. whilst clusters corresponding four goal locations clearly distinct lstm agent main clusters agent trajectories diagonally opposite arms maze represented similarly. given action sequence opposite arms equivalent suggests policy-dictating lstm maintains efﬁcient representation sub-policies critical information currently relevant goal provided additional lstm. results suggest depth prediction policy lstm yields optimal results. however several auxiliary tasks concurrently introduced thus provide comparison reward prediction depth prediction. following paper implemented additional agent architectures performing reward prediction convnet using replay buffer called ac*+r combining reward prediction convnet depth prediction lstm table suggests reward prediction improves upon plain stacked lstm architecture much depth prediction policy lstm combining reward prediction depth prediction yields comparable results depth prediction alone normalised average values respectively future work explore auxiliary tasks. figure lstm cell activations lstm ac*+dl agents i-maze collected multiple episodes reduced dimensions using tsne coloured represent goal location. policy-dictating lstm agent shown. table comparison navigation agent architectures maze conﬁgurations random static goals including agents performing reward prediction ac*+r ac+rd reward prediction implemented following report averaged best hyperparameters. proposed deep method augmented memory auxiliary learning targets training agents navigate within large visually rich environments include frequently changing start goal locations. results analysis highlight utility un/self-supervised auxiliary objectives namely depth prediction loop closure providing richer training signals bootstrap learning enhance data efﬁciency. further examine behavior trained agents ability localise network activity dynamics order analyse navigational abilities. approach augmenting deep auxiliary objectives allows end-end learning encourage development general navigation strategies. notably work auxiliary losses related independently looks data efﬁciency exploiting auxiliary losses. difference works auxiliary losses online rely form replay. also explored losses different nature. finally focus navigation domain understanding navigation emerges bi-product solving problem jaderberg concerned data efﬁciency rl-task. whilst best performing agents relatively successful navigation abilities would stretched larger demands placed rapid memory limited capacity stacked lstm regard. important future combine visually complex environments architectures make external memory enhance navigational abilities agents. further whilst work focused investigating beneﬁts auxiliary tasks developing ability navigate end-to-end deep reinforcement learning would interesting future work compare techniques slam-based approaches. would like thank alexander pritzel thomas degris joseph modayil useful discussions charles beattie julian schrittwieser marcus wainwright stig petersen environment design development amir sadik sarah york expert human game testing. charles beattie joel leibo denis teplyashin ward marcus wainwright heinrich kãijttler andrew lefrancq simon green victor valdes amir sadik julian schrittwieser keith anderson sarah york cant adam cain adrian bolton stephen gaffney helen king demis hassabis shane legg stig petersen. deepmind lab. arxiv https//arxiv.org/ abs/.. gamini dissanayake paul newman steve clark hugh durrant-whyte michael csorba. solution simultaneous localization building problem. ieee transactions robotics automation alex graves mohamed abdelrahman geoffrey hinton. speech recognition deep recurrent neural networks. proceedings international conference acoustics speech signal processing icassp alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabskabarwi´nska sergio gómez colmenarejo edward grefenstette tiago ramalho john agapiou hybrid computing using neural network dynamic external memory. nature jaderberg volodymir mnih wojciech czarnecki schaul joel leibo david silver koray kavukcuoglu. reinforcement learning unsupervised auxiliary tasks. submitted int’l conference learning representations iclr xiujun lihong jianfeng xiaodong jianshu chen deng recurrent reinforcement learning hybrid approach. proceedings international conference learning representations iclr https//arxiv.org/abs/.. volodymyr mnih adriã˘a puigdomã´lnech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. proc. int’l conf. machine learning icml arun nair praveen srinivasan blackwell cagdas alcicek rory fearon massively parallel methods deep reinforcement learning. proceedings international conference machine learning deep learning workshop icml karthik narasimhan tejas kulkarni regina barzilay. language understanding text-based games using deep reinforcement learning. proc. empirical methods natural language processing emnlp junhyuk valliappa chockalingam satinder singh honglak lee. control memory active perception action minecraft. proc. international conference machine learning icml antti rasmus mathias berglund mikko honkala harri valpola tapani raiko. semi-supervised learning ladder networks. advances neural information processing systems nips chen tessler shahar givony zahavy daniel mankowitz shie mannor. deep hierarchical approach lifelong learning minecraft. corr abs/. http//arxiv.org/abs/.. tijmen tieleman geoffrey hinton. lecture rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning volume yuting zhang kibok honglak lee. augmenting supervised neural networks unsupervised objectives large-scale image classiﬁcation. proc. international conference machine learning icml junbo zhao michaël mathieu ross goroshin yann lecun. stacked what-where auto-encoders. int’l conf. learning representations iclr http//arxiv.org/ abs/.. yuke roozbeh mottaghi eric kolve joseph abhinav gupta fei-fei farhadi. target-driven visual navigation indoor scenes using deep reinforcement learning. corr abs/. http//arxiv.org/abs/.. show behaviour ac*+dl agent videos corresponding navigation environments i-maze static maze static maze random goal maze random goal maze. video shows high-resolution video value function time layout mazes consecutive trajectories agent marked different colours output trained position decoder overlayed maze layout. introduce class neural network-based agents modular structures trained multiple tasks inputs coming different modalities implementing agent architecture simpliﬁed modular nature. essentially construct multiple networks task using shared building blocks optimise networks jointly. modules conv-net used perceiving visual inputs lstms used learning navigation policy shared among multiple tasks modules depth predictor loop closure predictor task-speciﬁc. navigation network outputs policy value function trained using reinforcement learning depth prediction loop closure prediction networks trained using self-supervised learning. within thread asynchronous training environment agent plays episode game environment therefore sees observation reward pairs takes actions different experienced agents other parallel threads. within thread multiple tasks trained schedule gradients shared parameter vector arrive. within thread ﬂag-based system subordinate gradient updates reinforcement learning procedure. experiments encoder model convolutional layers followed fully connected layer recurrent layer predict policy value function. architecture similar convolutional layers follows. ﬁrst convolutional layer kernel size stride feature maps. second layer kernel size stride feature maps. fully connected layer architecture figure hidden units lstm lstm architecture hidden units lstms figure extra inputs concatenated vector architectures ﬁrst lstm hiddens second lstm hiddens. depth predictor modules loop closure detection module single-layer mlps hidden units. depth mlps followed independent -dimensional softmax outputs loop closure followed -dimensional softmax output. illustrate figure architecture ac+d+l+dr agent. depth taken z-buffer labyrinth environment divided taken power spread values interval empirically decided following quantization ensure uniform video ac*+dl agent i-maze https//youtu.be/psijhk_bu video ac*+dl agent static maze https//youtu.be/-hsjqoiou_c video ac*+dl agent static maze https//youtu.be/khavraykbi video ac*+dl agent random goal maze https//youtu.be/ibtuadjy video ac*+dl agent random goal maze https//youtu.be/emxgbgyo figure details architecture ac+d+l+dr agent taking visual inputs past reward previous action well agent-relative velocity producing policy value function depth predictions binning across classes. previous version agent single depth prediction regressing depth pixels convnet outputs parameters modules point subset common vector parameters. optimise parameters using asynchronous version rmsprop recent example asynchronous parallel gradient updates deep reinforcement learning; case focus speciﬁc asynchronous advantage actor critic reinforcement learning procedure learning follows closely paradigm described workers rmsprop algorithm without momentum centering variance. gradients computed non-overlaping chunks episode. score point training curve average episodes model gets ﬁnish environment steps. whole experiments maximum environment step. agent action repeat means consecutive steps agent action picked beginning series. reason paper actually report results terms agent perceived steps rather environment steps. maximal number agent perceived step particular grid sample hyper-parameters categorical distributions learning rate sampled strength entropy regularization rewards scaled clipped experiments. previous experiments rewards scaled factor clipped prior back-propagation advantage actor-critic algorithm. gradients computed non-overlaping chunks steps episode. coefﬁcient depth prediction loss convnet features sampled coefﬁcient depth prediction loss lstm hiddens sampled coefﬁcient loop closure prediction loss sampled figure shows additional learning curves. particular left plot show baselines well agent without auxiliary losses perform worse without reward clipping reward clipping. seems removing reward clipping makes learning unstable absence auxiliary tasks. particular reason chose show baselines reward clipping main results. right subplot figure compares depth input versus target. note using rgbd inputs agent performs even worse predicting depth regression task general worse predicting depth classiﬁcation task. evaluated behaviour agents introduced paper well agents reward prediction introduced combination reward prediction convnet depth prediction policy lstm different maze environments non-navigation speciﬁc tasks. ﬁrst environment seek-avoid arena apples lemons disposed arena agents needs pick apples respawning; episodes last seconds. second environment stairway melon thin square corridor; direction lemon followed stairway melon direction apples dead melon visible reachable. agent spawns lemon apples random orientation. environments released deepmind environments require navigation skills shortest path planning simple reward identiﬁcation persistent exploration. figure shows major difference auxiliary tasks related depth prediction reward prediction. depth prediction boosts performance agent beyond stacked lstm architecture hinting general applicability depth prediction beyond navigation tasks. figure plot area curve rewards achieved agents across different experiments different tasks large static maze ﬁxed goals large static maze comparable layout dynamic goals i-maze. reward values computed replica; replicas experiment reward values sorted decreasing value. auxiliary tasks achieve higher results comparatively larger number replicas hinting fact auxiliary tasks make learning robust choice hyperparameters. finally compared asymptotic performance agents terms navigation terms representation policy lstm. rather visualising convolutional ﬁlters quantify change representation without auxiliary task terms position decoding following approach explained section speciﬁcally compare baseline agent navigation agent auxiliary task gets twice many gradient updates number frames seen environment task auxiliary depth prediction task. table shows performance baseline agent well position decoding accuracy signiﬁcantly increase twice number training steps (going points points reach performance position decoding accuracy ac+d agent half number training frames. reason believe auxiliary task simply accelerate training.", "year": 2016}