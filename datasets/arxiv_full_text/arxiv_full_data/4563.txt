{"title": "Efficient Reinforcement Learning Using Recursive Least-Squares Methods", "tag": ["cs.LG", "cs.AI"], "abstract": "The recursive least-squares (RLS) algorithm is one of the most well-known algorithms used in adaptive filtering, system identification and adaptive control. Its popularity is mainly due to its fast convergence speed, which is considered to be optimal in practice. In this paper, RLS methods are used to solve reinforcement learning problems, where two new reinforcement learning algorithms using linear value function approximators are proposed and analyzed. The two algorithms are called RLS-TD(lambda) and Fast-AHC (Fast Adaptive Heuristic Critic), respectively. RLS-TD(lambda) can be viewed as the extension of RLS-TD(0) from lambda=0 to general lambda within interval [0,1], so it is a multi-step temporal-difference (TD) learning algorithm using RLS methods. The convergence with probability one and the limit of convergence of RLS-TD(lambda) are proved for ergodic Markov chains. Compared to the existing LS-TD(lambda) algorithm, RLS-TD(lambda) has advantages in computation and is more suitable for online learning. The effectiveness of RLS-TD(lambda) is analyzed and verified by learning prediction experiments of Markov chains with a wide range of parameter settings. The Fast-AHC algorithm is derived by applying the proposed RLS-TD(lambda) algorithm in the critic network of the adaptive heuristic critic method. Unlike conventional AHC algorithm, Fast-AHC makes use of RLS methods to improve the learning-prediction efficiency in the critic. Learning control experiments of the cart-pole balancing and the acrobot swing-up problems are conducted to compare the data efficiency of Fast-AHC with conventional AHC. From the experimental results, it is shown that the data efficiency of learning control can also be improved by using RLS methods in the learning-prediction process of the critic. The performance of Fast-AHC is also compared with that of the AHC method using LS-TD(lambda). Furthermore, it is demonstrated in the experiments that different initial values of the variance matrix in RLS-TD(lambda) are required to get better performance not only in learning prediction but also in learning control. The experimental results are analyzed based on the existing theoretical work on the transient phase of forgetting factor RLS methods.", "text": "algorithms using linear value function approximators proposed analyzed. algorithms called rls-td fast-ahc respectively. rls-td viewed extension rls-td general multi-step temporal-difference learning algorithm using methods. convergence probability limit convergence rls-td proved ergodic markov chains. compared existing ls-td algorithm rls-td advantages computation suitable online learning. effectiveness rls-td analyzed verified learning prediction experiments markov chains wide range using methods learning-prediction process critic. performance fast-ahc also compared method using ls-td furthermore demonstrated experiments different initial values variance matrix rls-td required better performance learning prediction also learning control. recent years reinforcement learning active research area machine learning also control engineering operations research robotics provide elegant mathematical model sequential decision-making. operations research many results presented solve optimal control problem mdps model information. however reinforcement learning model information assumed unknown different methods studied operations research dynamic programming. dynamic programming elemental processes policy evaluation process policy improvement process respectively. similar processes. called learning prediction called learning control. goal learning control estimate optimal policy optimal value function without knowing model. learning prediction aims solve policy evaluation problem stationary-policy without prior model regarded sub-problem learning control. furthermore learning prediction different supervised learning. pointed sutton prediction problems supervised learning single-step prediction problems reinforcement learning multi-step prediction problems. solve multi-step prediction problems learning system must predict outcomes depend future sequence decisions. therefore theory algorithms multi-step learning prediction become important topic much research work done literature sutton presented first formal description temporaldifference methods algorithm convergence results established tabular temporal-difference learning algorithms cardinality tunable parameters state space since many real-world applications large infinite state space value function approximation methods need used cases. combined nonlinear value function approximators guarantee convergence several results regarding divergence reported literature linear function approximators also called linear algorithms several convergence proofs presented. dayan showed convergence mean linear tsitsiklis proved convergence algorithms arbitrary linear algorithms rules updating parameters similar gradient-descent methods. however gradient-learning methods step-size schedule must carefully designed guarantee convergence also obtain good performance. addition inefficient data slows convergence algorithms. based theory linear least-squares estimation brartke barto proposed temporal-difference algorithms called least-squares algorithm recursive leastsquares algorithm respectively. ls-td rls-td efficient statistical sense conventional linear algorithms eliminate design step-size schedules. furthermore convergence ls-td rls-td provided theory. algorithms viewed least-squares versions conventional linear methods. however shown literature learning algorithms update predictions based estimates multiple steps efficient monte-carlo methods well employing mechanism eligibility traces determined algorithms extract information historical data. recently class linear temporal-difference learning algorithms called ls-td proposed boyan least-squares methods employed compute value-function estimation although ls-td efficient requires much computation time-step online updates needed number state features becomes large. system identification adaptive filtering adaptive control recursive least-squares method commonly used reduce computational burden least-squares methods suitable online estimation control. although rls-td makes methods employ mechanism eligibility traces. based work tsitsiklis boyan motivated ideas class temporal-difference learning methods called rls-td algorithm proposed analyzed formally paper. rls-td superior conventional linear algorithms makes methods improve learning efficiency statistical point view eliminates step-size schedules. rls-td mechanism eligibility traces viewed extension rls-td general convergence probability rls-td proved ergodic markov chains limit convergence also analyzed. learning prediction experiments markov chains performance rls-td well ls-td compared wide range parameter settings tested. addition influence initialization parameters rls-td also discussed. observed rate convergence influenced initialization variance matrix phenomenon investigated theoretically adaptive filtering analyzed following sections benefits extension rls-td rls-td value still affect performance rls-based temporal-difference algorithms. although rls-td rate convergence mainly influenced initialization variance matrix bound approximation error dominantly determined parameter smallest error bound obtained worst bound obtained bounds suggest value selected appropriately obtain best approximation error. second benefit rls-td suitable online learning ls-td since computation time-step reduced number state features. methods actor-critic architecture used solve full reinforcement learning learning control problems. applying rls-td algorithm critic fast-ahc algorithm proposed paper. using methods critic performance learning prediction critic improved learning control problems solved efficiently. simulation experiments learning control cart-pole balancing problem swing-up acrobot conducted verify effectiveness fast-ahc method. comparing conventional methods critic demonstrated fast-ahc obtain higher data efficiency conventional methods. experiments performance comparisons methods using ls-td fast-ahc also conducted. learning control experiments also illustrated initializing constant variance matrix rls-td influences performance fast-ahc different values constant selected better performance different problems. results analyzed based theoretical work transient phase methods. paper organized follows. section introduction previous linear temporal-difference algorithms presented. section rls-td algorithm proposed convergence proved. section simulation example value-function prediction absorbing markov chains presented illustrate effectiveness rls-td algorithm different parameter settings different algorithms including ls-td studied. section fast-ahc method proposed simulation experiments learning control cart-pole balancing acrobot conducted compare fast-ahc conventional method well ls-td )-based method. simulation results presented analyzed detail. last section contains concluding remarks directions future work. consider markov chain whose states finite countable infinite space states markov chain indexed {…n} possibly infinite. although algorithms results paper applicable markov chains general state space discussion paper restricted within cases countable state space simplify notation. extension markov chains general state space requires translation matrix notation operator notation. trajectory generated markov chain denoted |t=…; s}.the dynamics markov chain described transition probability matrix whose entry denoted transition probability xt+=j given xt=i. state transition scalar reward defined. value function state defined follows eligibility trace viewed algebraic trick improve learning efficiency without recording data multi-step prediction process. trick based idea using truncated return markov chain. temporal-difference learning eligibility traces n-step truncated return defined monte-carlo return terminal state. step algorithm update rule value function estimation determined weighted average truncated returns defined above. corresponding update equation since state space markov chain usually large infinite practice function approximators neural networks commonly used approximate value function. algorithms linear function approximators popular well-studied ones. improve efficiency linear algorithms least-squares methods used linear algorithm ls-td rls-td algorithms suggested ls-td rls-td following quadratic objective function defined. boyan ls-td proposed solving directly model-based property ls-td also analyzed. however ls-td computation time-step i.e. cubic order state feature number. therefore computation required ls-td) increases fast increases undesirable online learning. next section propose rls-td algorithm making recursive least-squares methods computational burden ls-td reduced also give rigorous mathematical analysis algorithm convergence rls-td proved. well known system identification adaptive filtering control methods commonly used solve computational memory problems least-squares algorithms. sequel present rls-td algorithm based idea. first matrix inverse lemma given follows forgetting factor usually used adaptive filtering improve performance methods non-stationary environments. forgetting factor rls-td algorithm derived using similar techniques haykin detailed derivation rls-td) referred appendix rls-td algorithm absorbing markov chains weight updates absorbing states treated differently process re-initialized absorbing states transform absorbing markov chain equivalent ergodic markov chain. following convergence analysis focus ergodic markov chains. assumptions almost linear algorithms discussed tsitsiklis except assumption ergodic markov chains considered. assumption specially needed convergence rls-td) algorithm. explanations notations theorem please refer appendix discussed tsitsiklis theorem shows distance limiting function true value function bounded smallest bound approximation error obtained every bound actually deteriorates decreases. worst bound obtained although bound strongly suggests higher values likely produce accurate approximations compared ls-td) additional parameter rls-td) value initial variance matrix pointed haykin exact value initializing constant insignificant effect data length large enough. means limit final solutions obtained almost same. influence transient phase positive constant becomes large enough goes infinity transient behavior almost methods initialized relatively small value transient phases different. practice observed variable performance function initialization cases exhibit significantly faster convergence initialized relatively small positive definite matrix initialized large first effort toward direction statistical analysis soft exact initialization limits case number iterations less size estimation vector moustakides provided theoretical analysis relation algorithmic performance initialization using settling time performance measure moustakides proved well-known rule initialization relatively small matrix preferable cases high medium signal-to-noise ratio whereas relatively large matrix must selected achieving best results. following learning prediction experiments rls-td) well learning control simulation fast-ahc observed value initializing constant also plays important role convergence performance theoretical analyses provide clue explain experimental results. section illustrative example given show effectiveness proposed rls-td) algorithm. furthermore algorithmic performance influence initializing constant studied. figure state initial state trajectory state absorbing state. non-absorbing state possible state transitions transition probability state transition reward except transition state state reward thus true value function state apply linear temporal-difference algorithms value function prediction problem four-element state features basis functions chosen shown figure state features states respectively state features states obtained linearly interpolating these. simulation rls-td algorithm well ls-td) conventional linear algorithms used solve value function prediction problem without knowing model markov chain. experiments trial defined period initial state terminal state performance algorithms evaluated averaged root mean squared error value-function predictions states. parameter setting performance averaged independent monte-carlo runs. figure shows learning curves rls-td) conventional linear algorithms three different parameter settings. parameter algorithms different boyan linear algorithms applied online forms update weights every state transitions. parameter number state transitions. weights initialized zeroes. figure learning curves conventional linear algorithms step-size schedules shown curves respectively. curve averaged errors value function predictions states independent runs plotted trial. curve shows learning performance rls-td). additional parameter rls-td) initial value variance matrix experiment relatively large value. figure concluded making methods rls-td) obtain much better performance conventional linear algorithms eliminates design problem step-size schedules. experiments linear rls-td) different parameters also conducted similar results obtained initial values rls-td) large conclusion confirmed. algorithms using different initial parameters variance matrix p=.i respectively. forgetting factor µ=.. performance suggested algorithm measured averaged errors value function prediction first trials independent runs states. experiments settings parameter tested figure clearly shown performance rls-td) large initial value much better rls-td) small initial value experiments different parameter settings similar results also obtained. refer phenomenon case forgetting factor studied moustakides hop-world problem stochastic state transitions could introduce high equation corresponds additive noise large variance residuals i.e. case. discussed section forgetting factor cases relatively large initializing constant must selected better results. full understanding phenomenon found. performance rls-td) unit forgetting factor also tested experiments. although initial value effect discussed intensively effects observed empirically case shown figure experiments also found initialized small value performance sensitive values parameter case convergence speed rls-td) increases increases shown figure furthermore fixed performance rls-td) deteriorates becomes smaller shown figure figure learning curves rls-td) different initializing constants shown compared ls-td). experiment figure shown performance rls-td) approaches ls-td) becomes large. well known becomes large enough performance methods almost same. figure shows performance comparison ls-td) rls-td) large value initial variance matrix rls-td) every runs identity matrix. based experimental results concluded convergence speed rls-td mainly influenced initial value variance matrix parameter detailed discussions properties rls-td given follows relatively large effect becomes small. large enough goes infinity performance rls-td ls-td almost same discussed above. cases effect speed convergence insignificant coincides discussion boyan however described theorem value still affects ultimate error bound value function approximation. relatively small observed convergence performance rls-td) different ls-td) influenced values experiments hop-world problem results show smaller values lead slower convergence. results explained theoretical analysis transient phase forgetting factor according theory moustakides larger values needed better performance cases smaller values preferable fast convergence cases high medium snr. different values must selected faster convergence rls-td different cases. especially cases high case discussed moustakides methods small values obtain fast speed convergence. compared conventional linear algorithms rls-td algorithm obtain much better performance making methods value function prediction problems. furthermore step-size schedule needs carefully designed achieve good performance rls-td initial value variance matrix selected according criterion large small value. comparison ls-td rls-td preferable depends objective. online applications rls-td advantages computational efficiency computation step rls-td ls-td number state features. moreover seen later rls-td obtain better transient convergence performance ls-td cases. hand ls-td preferable rls-td long-term convergence performance seen figure system identification point view ls-td obtain unbiased parameter estimates face white additive noises rls-td finite would possess large parameter discrepancies. section fast-ahc algorithm proposed based results learning prediction solve learning control problems. learning control experiments conducted illustrate efficiency fast-ahc. ultimate goal reinforcement learning learning control i.e. estimate optimal policies optimal value functions markov decision processes several reinforcement learning control algorithms including q-learning sarsa-learning adaptive heuristic critic algorithm proposed. among methods method different q-learning sarsa-learning value-function-based methods. method value functions policies separately represented value-functionbased methods policies determined value functions directly. components method called critic actor respectively. actor used generate control actions according policies. critic used evaluate policies represented actor provide actor internal rewards without waiting delayed external rewards. since objective critic policy evaluation learning prediction temporal-difference learning methods chosen critic’s learning algorithms. learning algorithm actor determined estimation gradient policies. following discussion detailed introduction method given. figure shows architecture learning system based method. learning system consists critic network actor network. inputs critic network include external rewards state feedback environment. internal rewards provided critic network called temporal-difference signals. reinforcement learning methods whole system modeled denoted tuple {sapr}where state action state transition probability reward function. policy defined function probability distribution action space. objective method estimate optimal policy satisfying following equation. critic uses temporal-difference learning approximate value function current policy. linear function approximators used critic weight update equation action selection policy actor determined current state value function estimation critic. suppose neural network weight vector used actor output actor network provide internal reinforcement using temporal-difference learning algorithms efficiency temporal-different learning learning prediction greatly influence whole learning system’s performance. although policy actor changing change relatively slowly especially fast convergence learning prediction critic realized. previous sections rls-td shown better data efficiency conventional linear algorithms fast convergence speed obtained initializing constant chosen appropriately. thus applying rls-td policy evaluation critic network improve learning prediction performance critic promising enhance whole system’s learning control performance. based idea method called fast-ahc algorithm proposed paper. efficiency fast-ahc algorithm verified empirically detailed analysis results given. following complete description fast-ahc algorithm. artificial intelligence learning control inverted pendulums considered standard test problem machine learning methods especially algorithms. studied early work michie’s boxes system later barto sutton learning controllers output values berenji methods continuous outputs applied cart-pole balancing problem. paper cart-pole balancing problem continuous control values figure shows typical cart-pole balancing control system consists cart moving horizontally pole fixed cart. denote horizontal distance center cart center track negative cart left part track. variable denotes angle pole upright position amount force applied cart move towards left right. derivatives figure mass cart m=.kg mass pole m=.kg half-pole length l=.m coefficient friction cart track µc=. coefficient friction pole cart µp=.. boundary constraints state variables given follows. learning control experiments pole-balancing problem dynamics assumed unknown learning controller. addition four state variables available feedback failure signal notifies controller failure occurs means values state variables exceed boundary constraints prescribed inequalities typical reinforcement learning problem failure signal serves reward. since external reward available long sequence actions critic learning controller used provide internal reinforcement signal accomplish learning task. learning control experiments pole-balancing problem conducted using conventional method uses linear algorithms critic fast-ahc method proposed paper. solve continuous state space problem reinforcement learning class linear function approximators called cerebellar model articulation controller used. neural network model based neuro-physiological theory human cerebellarcmac first proposed albus widely used automatic control function approximation. cmac neural networks dependence adjustable parameters weights respect outputs linear. detailed discussion structure cmac neural networks refer albus sutton barto fast-ahc learning controllers cmac neural networks four inputs output used function approximators critic actor respectively. cmac tilings partitions every input. total physical memory cmac network reduce computation memory requirements hashing technique described following equations employed experiments. represents input state vector activated tile i-th element total number physical memory physical memory address corresponding state remainder divided order compare performance different learning algorithms initial parameters learning controller selected follows weights critic initialized weights actor initialized random numbers interval parameters fast-ahc algorithms experiments trial defined period initial state failure state initial state trial randomly generated state near unstable equilibrium maximum distance equation employed simulate dynamics system using euler method time step trial lasts time steps said successful learning controller assumed able balance pole. reinforcement signal problem defined forgetting factor rls-td) critic value equal close learning control experiments using conventional methods also conducted comparison. performance comparisons algorithms shown figure experiments initial variance matrixes fast-ahc algorithm p=.i. performance fast-ahc compared different numbers physical memories critic network actor network chosen respectively. parameter setting algorithms independent runs tested. performance evaluated according trial number needed successfully balance pole. learning factors actor networks manually optimized value algorithms. experiments settings tested. figure learning factors critic networks chosen respectively. found performance becomes worse. learning factors greater algorithm become unstable even algorithm becomes unstable time-varying learning factors specified performance worse constant learning factors. three settings learning factor typical near optimal algorithm. experimental results concluded using rls-td) critic network fast-ahc algorithm obtain better performance conventional algorithms. although fast-ahc requires computation step efficient less trials data needed successfully balance pole. discussed previous sections convergence performance rls-td) influenced initial value variance matrix. also case fast-ahc. learning control experiments small value selected. experiments small values performance fast-ahc satisfactory better ahc. however equal relatively large value example performance fast-ahc deteriorates significantly. since rls-td) large initializing constant similar performance ls-td) deduced method using ls-td) critic also performance cart-pole balancing problem. verify this experiments conducted using fast-ahc large initializing constant using ls-td). parameter setting independent runs tested. experiments maximum trials algorithm algorithm fails balance pole within trials performance .when using ls-td) method computational problems matrix inversion first steps learning methods tried avoid problem. usage first steps updates. actor updated early stage learning ls-td) stable. however similar results found methods. figure shows experimental results clearly verify performance fast-ahc large initializing constant similar using ls-td) much worse fast-ahc small detailed discussion phenomenon provided subsection following figure figure variations pole angle control force plotted successfully trained fast-ahc learning controller used control cart-pole system. subsection another learning control example swing-up control acrobot minimum time presented. learning control acrobot class adaptive optimal control problem difficult pole-balancing problem. investigated sutton cmac-based sarsa-learning algorithms employed solve case discrete control actions studied. experiments case continuous actions acrobot moving vertical plane shown figure first link second link respectively. control torque applied point goal swing-up control swing acrobot line higher joint amount length link. learning control problem continuous states actions. cmac-based actor-critic controller actor network critic network tilings partitions input. actor network uniform coding employed non-uniform coding used critic network. details coding parameters please refer appendix sizes physical memories actor network critic network respectively. cmac networks following hashing techniques used. please refer subsection simulation parameters acrobot chosen m=m=kg i=i=kgm lc=lc=.m l=l=m g=.m/s. time step simulation time interval learning control learning parameters k=.. trial defined period starts stable equilibrium ends goal state reached. trial state acrobot re-initialized stable equilibrium. parameter setting independent runs tested. consists trials trial actor network tested controlling acrobot alone i.e. setting action variance performance comparisons fast-ahc shown figure experiments algorithms tested different also tested different learning factors critic networks. results also shown fast-ahc achieve higher data efficiency ahc. however example relatively large used different previous cart-pole balancing example. experiments good performance obtained large initializing constant small performance deteriorates significantly. thus problem referred case moustakides large values preferable best convergence rate methods. following figure shows performance comparison fast-ahc large small value settings parameter tested algorithm. performance using ls-td) also shown. figure typical curve angle first link plotted acrobot controlled actor network fast-ahc method trials. based experimental results concluded using rls-td) algorithm critic network fast-ahc algorithm obtain better performance conventional algorithms less trials data needed converge near optimal policy. well known difficulty applications methods slow convergence especially cases learning data hard generated. fast-ahc algorithm although computation step required conventional methods serious problem number linear state features small. learning control experiments hashing techniques used reduce state features cmac networks computation fast-ahc reduced economical amount. method adaptive filtering discussed section learning control experiments cart-pole balancing problem better performance fast-ahc obtained using small values learning control acrobot higher data efficiency achieved using fast-ahc relatively large different properties fast-ahc referred different cases methods thorough theoretical analysis problem interesting topic future research. experiments performance method using ls-td) also tested. studied section initializing constant large performance rls-td) ls-td) differ much. performance using ls-td) similar fast-ahc large values appropriately. cases converge almost instantly. also verified learning prediction experiments rls-td) algorithm. applying rls-td) actor-critic learning controller although policy actor change time still assumed changing speed policy slow compared fast convergence speed rls-td). thus good performance learning prediction obtained critic. moreover since learning prediction performance critic important reinforcement learning algorithms using methods called rls-td fast-ahc respectively proposed paper. rls-td used solve learning prediction problems efficiently conventional linear algorithms. convergence probability proved rls-td limit convergence also analyzed. experimental results learning prediction problems show rls-td algorithm superior conventional algorithms data efficiency also eliminates design problem step sizes linear algorithms. rls-td viewed extension rls-td general although effect convergence speed rls-td significant cases usage still affect approximation error bound. thus needs value function estimation high precision large values preferable furthermore rlstd superior ls-td computation weight vector must updated every observations. results learning prediction learning control method called algorithm. using rls-td critic network fast-ahc achieve better performance conventional method data efficiency. simulation results learning control pole-balancing experiments found performance rls-td well fast-ahc influenced initializing constant methods. different values needed best performance different cases. also well-known phenomenon rls-based adaptive idea using rls-td critic network applied reinforcement learning methods actor-critic architectures. konda tsitsiklis actor-critic algorithm using linear function approximators proposed convergence certain conditions proved. condition convergence algorithm convergence rate critic much faster actor. thus application rls-td critic preferable order ensure convergence algorithm. theoretical empirical work problem deserves studied future. process follows. {xt} markov chain evolves according transition matrix already steady state means pr{xt=i}= given sample path markov chain define", "year": 2011}