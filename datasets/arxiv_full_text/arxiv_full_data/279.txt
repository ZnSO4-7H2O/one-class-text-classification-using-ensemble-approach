{"title": "Exploring Models and Data for Image Question Answering", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "abstract": "This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.", "text": "work aims address problem image-based question-answering models datasets. work propose neural networks visual semantic embeddings without intermediate stages object detection image segmentation predict answers simple questions images. model performs times better published results existing image dataset. also present question generation algorithm converts image descriptions widely available form. used algorithm produce order-of-magnitude larger dataset evenly distributed answers. suite baseline results dataset also presented. combining image understanding natural language interaction grand dreams artiﬁcial intelligence. interested problem jointly learning image text question-answering task. recently researchers studying image caption generation developed powerful methods jointly learning image text inputs form higher level representations models convolutional neural networks trained object recognition word embeddings trained large scale text corpora. image involves extra layer interaction human computers. model needs attention details image instead describing vague sense. problem also combines many computer vision sub-problems image labeling object detection. paper present contributions problem generic end-to-end model using visual semantic embeddings connect recurrent neural well comparisons suite models; automatic question generation algorithm converts description sentences questions; dataset generated using algorithm number baseline results dataset. work assume answers consist single word allows treat problem classiﬁcation problem. also makes evaluation models easier robust avoiding thorny evaluation issues plague multi-word generation problems. malinowski fritz released dataset images question-answer pairs dataset question answering real-world images images depth dataset taken indoor scenes. human segmentation image depth values object labeling available dataset. data sets conﬁgurations differ figure sample questions responses variety models. correct answers green incorrect red. numbers parentheses probabilities assigned top-ranked answer given model. leftmost example daquar dataset others coco-qa dataset. number object classes appearing questions mainly three types questions dataset object type object color number objects. questions easy many questions hard answer even humans. since daquar publicly available image-based dataset benchmarks evaluate models. together release daquar dataset malinowski fritz presented approach combines semantic parsing image segmentation. approach notable ﬁrst attempts image number limitations. first human-deﬁned possible predicates dataset-speciﬁc. obtain predicates algorithm also depends accuracy image segmentation algorithm image depth information. second model needs compute possible spatial relations training images. even though model limits nearest neighbors test images could still expensive operation larger datasets. lastly accuracy model strong. show simple baselines perform better. recently number parallel efforts creating datasets proposing models antol used ms-coco images created open domain dataset human generated questions answers. anto al.’s work authors also included cartoon pictures besides real images. questions require logical reasoning order answer correctly. malinowski recurrent networks encode sentence output answer. whereas malinowski single network handle encoding decoding used networks separate encoder decoder. lastly bilingual versions dataset available al.’s work. cnns extract image features sentence features fuse features together another multi-modal cnn. approach developed independently work above. similar work malinowski also experimented recurrent networks consume sequential question input. unlike formulate task classiﬁcation problem single wellaccepted metric evaluate sentence-form answer accuracy thus place focus limited domain questions answered word. also formulate evaluate range algorithms utilize various representations drawn question image datasets. methodology presented two-fold. model side develop apply various forms neural networks visual-semantic embeddings task dataset side propose ways synthesizing pairs currently available image description datasets. recent years recurrent neural networks enjoyed successes ﬁeld natural language processing long short-term memory form easier train standard rnns linear error propagation multiplicative gatings. model builds directly lstm sentence model called vis+lstm model. treats image word question. borrowed idea treating image word caption generation work done vinyals compare newly proposed model suite simpler models experimental results section. experimented several different word embedding models randomly initialized embedding dataset-speciﬁc skip-gram embedding general-purpose skip-gram embedding model word embeddings trained rest model. treat image ﬁrst word sentence. similar devise linear afﬁne transformation dimension image feature vectors dimensional vector matches dimension word embeddings. optionally treat image last word question well different weight matrix optionally reverse lstm gets content operates backward sequential fashion. currently available daquar dataset contains approximately images questions common object classes might enough training large complex models. anproblem current dataset simply guessing modes yield good accuracy. create another dataset produce much larger number pairs even distribution answers. collecting human generated pairs possible approach another synthesize questions based image labeling instead propose automatically convert descriptions form. general objects mentioned image descriptions easier detect ones daquar’s human generated questions ones synthetic based ground truth labeling. allows model rely rough image understanding without logical reasoning. lastly conversion process preserves language variability original description results human-like questions questions generated image labeling. english questions tend start interrogative words what. algorithm needs move verb well constituent front sentence. example riding horse becomes what riding? work consider following simple constraints a-over-a principle restricts movement whword inside noun phrase algorithm move wh-word contained clause constituent. question generation still open-ended topic. overall adopt conservative approach generating questions attempt create high-quality questions. consider generating four types questions below object questions first consider asking object using what. involves replacing actual object what sentence transforming sentence structure what appears front sentence. entire algorithm following stages split long sentences simple sentences; change indeﬁnite determiners deﬁnite determiners; traverse sentence identify potential answers replace what. traversal object-type question generation currently ignore prepositional phrase constituents; perform wh-movement. order identify possible answer word used wordnet nltk software package noun categories. number questions follow similar procedure previous algorithm except different identify potential answers extract numbers original sentences. splitting compound sentences changing determiners wh-movement parts remain same. color questions color questions much easier generate. requires locating color adjective noun adjective attaches. simply forms sentence what color object replaced actual noun. location questions similar generating object questions except answer traversal search within constituents start preposition also added rules ﬁlter clothing answers mostly places scenes large objects contain smaller objects. table summarizes statistics coco-qa. noted since applied pair rejection process mode-guessing performs poorly coco-qa. however coco-qa questions actually easier answer daquar human point view. encourages model exploit salient object relations instead exhaustively searching possible relations. coco-qa dataset downloaded http//www.cs.toronto.edu/˜mren/ imageqa/data/cocoqa provide brief statistics dataset. maximum question length average common answers white least common eagle tram sofa median answer across entire test overlap training questions overlap training question-answer pairs. -vis+blstm second model image feature inputs start sentence different learned linear transformations also lstms going forward backward directions. lstms output softmax layer last timestep. call second model -vis+blstm. img+bow simple model performs multinomial logistic regression based image features without dimensionality reduction bag-of-word vector obtained summing learned word vectors question. guess simple baseline predict mode based question type. example question contains many model output two. daquar modes table white coco-qa modes white room. also trained counterpart deaf model. type question train separate classiﬁcation layer note model knows type question order make performance somewhat comparable models take account words narrow answer space. however model know anything question except type. img+prior baseline combines prior knowledge object image understanding deaf model. example question asking color white bird ﬂying blue output white rather blue simply prior probability bird blue lower. denote color class object interest k-nn task image caption generation devlin showed nearest neighbors baseline approach actually performs well. whether model memorizes training data answering question include k-nn baseline results. unlike image caption generation similarity measure includes image text. bag-ofwords representation learned img+bow append image features. euclidean distance similarity metric; possible improve nearest neighbor result learning similarity metric. evaluate model performance used plain answer accuracy well wu-palmer similarity measure wups calculates similarity words based longest common subsequence taxonomy tree. similarity words less threshold score zero given candidate answer. following malinowski fritz measure models terms accuracy wups wups table summarizes learning results daquar coco-qa. daquar compare results noted daquar results portion dataset single-word answers. release paper claimed achieve better results datasets. results observe model outperforms baselines existing approach terms answer accuracy wups. vis+lstm malinkowski al.’s recurrent neural network model achieved somewhat similar performance daquar. simple average three models boosts performance outperforming models. surprising img+bow model strong datasets. limitation vis+lstm model able consume image features large dimensions time step dimensionality reduction lose useful information. tried give img+bow dim. image vector worse vis+lstm comparing blind versions lstm models hypothesize image tasks particular simple questions studied here sequential word interaction important natural language tasks. also interesting blind model lose much daquar dataset speculate likely imagenet images different indoor scene images mostly composed furniture. however non-blind models outperform blind models large margin coco-qa. three possible reasons objects ms-coco resemble ones imagenet more; ms-coco images fewer objects whereas indoor scenes considerable clutter; coco-qa data train complex models. many interesting examples space limitations show figure figure full results available http//www.cs.toronto.edu/˜mren/ imageqa/results. images added extra questions provide insight model’s representation image question information help elucidate questions models accidentally correct. parentheses ﬁgures represent conﬁdence score given softmax layer respective model. model selection using different word embedding signiﬁcant impact ﬁnal classiﬁcation results. observed ﬁne-tuning word embedding results better performance normalizing hidden image features zero-mean unit-variance helps achieve faster training time. bidirectional lstm model boost result little. object questions original trained imagenet challenge img+bow beneﬁted signiﬁcantly single object recognition ability. however challenging part consider spatial relations multiple objects focus details image. models moderately acceptable this; instance ﬁrst picture figure fourth picture figure sometimes model fails make correct decision outputs salient object sometimes blind model equally guess probable objects based question alone nonetheless full model improves accuracy compared model shows difference pure object classiﬁcation image question answering. counting daquar could observe advantage counting ability img+bow vis+lstm model compared blind baselines. coco-qa observable counting ability clean images single object type. models sometimes count six. however shown second picture figure ability fairly weak count correctly different object types present. room improvement counting task fact could separate computer vision problem own. color coco-qa signiﬁcant img+bow vis+lstm blind ones color-type questions. discovered models able recognize dominant color image sometimes associate different colors different objects shown ﬁrst picture figure however still fail number easy examples. adding prior knowledge provides immediate gain model terms accuracy color number questions. img+prior img+bow shows localized color association ability image representation. paper consider image problem present end-to-end neural network models. model shows reasonable understanding question coarse image understanding still na¨ıve many situations. recurrent networks becoming popular choice learning image text showed simple bag-of-words perform equally well compared recurrent network borrowed image caption generation framework proposed complete baselines provide potential insight developing sophisticated end-to-end image question answering systems. currently available dataset large enough developed algorithm helps collect large scale image dataset image descriptions. question generation algorithm extensible many image description datasets automated without requiring extensive human effort. hope release dataset encourage data-driven approaches problem future. image question answering fairly research topic approach present number limitations. first models answer classiﬁers. ideally would like permit longer answers involve sophisticated text generation model structured output. require automatic free-form answer evaluation metric. second focusing limited domain questions. however limited range questions allow study results depth. lastly also hard interpret models output certain answer. comparing models baselines roughly infer whether understood image. visual attention another future direction could improve results well help explain model prediction examining attention output every timestep. would like thank nitish srivastava support toronto conv extracted image features. would also like thank anonymous reviewers valuable helpful comments. klein manning accurate unlexicalized parsing chomsky conditions transformations. york academic press fellbaum wordnet electronic lexical database. cambridge london press", "year": 2015}