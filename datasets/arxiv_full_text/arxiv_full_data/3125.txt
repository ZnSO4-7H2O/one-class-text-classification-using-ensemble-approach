{"title": "Truncated Nuclear Norm Minimization for Image Restoration Based On  Iterative Support Detection", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Recovering a large matrix from limited measurements is a challenging task arising in many real applications, such as image inpainting, compressive sensing and medical imaging, and this kind of problems are mostly formulated as low-rank matrix approximation problems. Due to the rank operator being non-convex and discontinuous, most of the recent theoretical studies use the nuclear norm as a convex relaxation and the low-rank matrix recovery problem is solved through minimization of the nuclear norm regularized problem. However, a major limitation of nuclear norm minimization is that all the singular values are simultaneously minimized and the rank may not be well approximated \\cite{hu2012fast}. Correspondingly, in this paper, we propose a new multi-stage algorithm, which makes use of the concept of Truncated Nuclear Norm Regularization (TNNR) proposed in \\citep{hu2012fast} and Iterative Support Detection (ISD) proposed in \\citep{wang2010sparse} to overcome the above limitation. Besides matrix completion problems considered in \\citep{hu2012fast}, the proposed method can be also extended to the general low-rank matrix recovery problems. Extensive experiments well validate the superiority of our new algorithms over other state-of-the-art methods.", "text": "recovering large matrix limited measurements challenging task arising many real applications image inpainting compressive sensing medical imaging kind problems mostly formulated low-rank matrix approximation problems. rank operator non-convex discontinuous recent theoretical studies nuclear norm convex relaxation low-rank matrix recovery problem solved minimization nuclear norm regularized problem. however major limitation nuclear norm minimization singular values simultaneously minimized rank well approximated correspondingly paper propose multi-stage algorithm makes concept truncated nuclear norm regularization proposed iterative support detection proposed overcome limitation. besides matrix completion problems considered proposed method also extended general lowrank matrix recovery problems. extensive experiments well validate superiority algorithms state-of-the-art methods. locations corresponding observed entries. solve kind problems refer breakthrough results. nevertheless obtain suboptimal performance real applications nuclear norm good approximation rank-operator non-zero singular values rank-operator equal contribution singular values nuclear norm treated diﬀerently adding together. thus overcome weakness nuclear norm truncated nuclear norm regularization proposed matrix com. many real applications machine learning computer vision control etc. seek recover unknown low-rank matrix limited information. problem naturally formulated following model given. however usually np-hard nonconvexity discontinuous nature rank function. paper fazel et.al ﬁrstly solved rank minimization problem approximating rank function using nuclear norm moreover theoretical studies show nuclear norm tightest convex lower bound rank function matrices thus unknown low-rank matrix perfectly recovered solving optimization problem problem still non-convex local minima iterative procedure proposed review procedure details later. similar idea truncation context sparse vectors also implemented sparse signals previous work tries adaptively learn information nonzeros unknown true signal. speciﬁcally present sparse signal reconstruction method iterative support detection aiming achieve fast reconstruction reduced requirement number measurements compared classical minimization approach. alternatively calls components support detection signal reconstruction. incorrect reconstruction support detection identiﬁes index reliable support detection inexact reconstructions must take advantage features prior information true signal sparse compressible signals components fast decaying distribution nonzeros considered. best trying possible values leads high computational cost. paper motivated wang et.al propose singular value estimation method obtain best considered special implementation iterative support detection case matrices. third contribution based two. particular eﬃcient algorithmic framework proposed low-rank matrix recovery problem. name lrisd iteratively calls components solving low-rank matrix reconstruction model based tnnr. related work low-rank optimization problem attracted interests developing customized algorithms particularly lager-scale cases. brieﬂy review inﬂuential approaches problems. convex problem easily reformulated semi-deﬁnite programming problems make generic solvers sdpt sedumi based interior-point method. however interior-point approaches suﬀer limitation ineﬀectively handle large-scale problems mentioned problem also solved projected subgradient approach whose major computation concentrated singular values decomposition. method used solve large-scale cases however convergence slow especially high accuracy required. uv-parametrization based matrix factorization applied general low-rank matrix reconstruction problems. speciﬁcally low-rank matrix reduces dimensionality however rank size large computation cost also high. moreover rank known priori applications estimated dynamically adjusted might diﬃcult realize. recently augmented lagrangian method alternating direction method multipliers eﬃcient convex programming problems arising various applications. admm applied important special case problem matrix completion problem widely studied. used shrinkage operator solve nuclear norm eﬀectively. shrinkage operator applies soft-thresholding rule singular values sparse operator matrix though applied widely mangy approaches. however above-mentioned limitation nuclear norm tnnr authors studied solve special case matrix completion problems. general problems extend current state algorithms solve section section section respectively. summary procedure lrisd multistage algorithm summarized following algorithm alternately running solving corresponding tnnr models iterative scheme converge solution tnnr model whose solution expected better plain nuclear norm minimization model following content explain implementation step details extend existing algorithms nuclear norm regularized models tnnr based models procedure step singular value estimate subsection mainly focus step lrisd i.e. describing process estimate number largest singular values. feasible best trying possible done procedure computationally eﬃcient. thus quickly give estimate best known low-rank matrices images singular values often feature fast decaying distribution common convex relaxation model idea make truncated nuclear norm regularization deﬁned variant ones passively possible values number largest singular values proposed actively estimate value addition consider general low-rank recovery problems beyond matrix completion problem. speciﬁcally solve three models equality-model unconstrained-model inequality-model general framework lrisd iterative procedure starts initial i.e. solving plain nuclear norm minimization problem estimates based recovered result. based estimated solve resulted tnnr model using recovered result update value solve tnnr model algorithm iteratively calls estimation solver tnnr model. solving following idea speciﬁcally simple eﬃcient iterative procedure adopted decouple initial guess l-th iteration ﬁrst compute described based update solving following problems respectively showed take advantage feature extend previous work detecting large components sparse vectors large singular values low-rank matrices. particular nothing speciﬁc implementation support detection cases low-rank matrices acquire estimation true however paper unlike original paper propose apply last signiﬁcant jump rule absolute values ﬁrst order diﬀerence i.e. instead speciﬁcally look largest speciﬁcally computed obtain jump sizes count change neighboring components then reﬂect stability jumps diﬀerence need considered because largest singular values jump actively small singular values would change much. cut-oﬀ threshold determined certain heuristic methods experiments synthetic real visual data sets. note subsection present reliable rule determining threshold value tended verison admm named tnnr-admm original admm corresponding nuclear norm regularized low-rank matrix recovery model denoted lr-admm. addition deduce resulting subproblems simple enough closed-form solutions easily achieved high precision. start section preliminaries convenient presentation algorithms later. present process eﬀectiveness sve. noted showed algorithm repeated several times stable estimate obtained. time given reference image obtain singular value vector svd. natural positions true large singular values based considered estimate singular value vector true matrix based thresholding fast decaying property singular values. choice case-dependent. spirit called last signiﬁcant jump rule threshold value detect large singular values minimized false detections assume components sorted large small. straightforward apply last signiﬁcant jump rule look largest analysis subproblems according analysis above computation iteration tnnr-admm approach dominated solving subproblems elaborate strategies solving subproblems based abovementioned preliminaries. idea admm decompose minimization task three easier smaller subproblems involved variables minimized separately altering order. particular apply admm solve obtain following iterative scheme tnnr-apgl section consider model attracted attention certain multi-task learning problems tnnr-admm applied solve model preferred noiseless problems. simple version model i.e. based common nuclear norm regularization many accelerated gradient techniques based proposed. among them accelerated proximal gradient line search method proposed beck extended solve tnnr based matrix completion model paper extend apgl solve general tnnr based low-rank recovery problem applied tnnr-apgl solve problem obtain closed-form solutions. addition convergence apgl well studied convergence rate paper also omit convergence analysis. tnnr-admm usually eﬃcient solving tnnr based models convergence could become slower constraints inspired alternating direction method multipliers adaptive penalty applied reduce constrained conditions adaptive penalty used speed convergence. resulted algorithm named tnnr-admmap whose subproblems also closed-form solutions. sample set. correspondingly ﬂexibly change form matrices vectors calculation process. here provide idea process tnnr-admmap. match relevant function following results overall tnnr-admm tnnr-apgl applied solve generated subproblems closed-form solutions. mentioned before tnnr-admmap used speed convergence many constraints. problem solved simultaneously three algorithms tnnr-admmap general eﬃcient case matrix completion test problems. experiments results section present numerical results validate eﬀectiveness lrisd. summary parts certiﬁed following experiments. hand illustrate eﬀectiveness real visual synthetic data sets. hand also illustrate eﬀectiveness lrisd solves tnnr based low-rank matrix recovery problems synthetic real visual data sets. since space limited discuss model using lrisd-admm experiments. necessary refer extensive numerical results understand admmap much faster apgl admm without sacriﬁcing recovery accuracy cases matrix completion. similarly low-rank matrix recovery conclusion according experiments. since main paper present eﬀectiveness lrisd here omit detailed explanation. ﬁrst cases illustrate eﬀectiveness sve. compared algorithm lrisd-admm proposed name tnnr-admmtry matrix completion problem. main diﬀerence lrisd-admm tnnr-admmtry determining best former estimate best latter possible values pick best performance. last show better recovery quality lrisd-admm compared solution common nuclear norm regularized low-rank recovery models example whose corresponding algorithm denoted lr-admm above. empirically works quit well tested problems. parameters tnnradmm default values besides psnr evaluate quality image. color images three channels psnr se/t error blue total number missing pixels. grayscale images psnr similar deﬁnition. comparison lrisd-admm tnnr-admm-try matrix completion subsection evaluate eﬀectiveness compare proposed lrisd-admm tnnradmm-try well ld-admm matrix completion problems. better recovery quality tnnradmm-try lr-admm matrix completion problem demonstrated show ﬁnal estimated lrisd-admm close tnnr-admm-try. test three real clear images present input images masked images results calculated three diﬀerent algorithms lr-admm tnnradmm-try lrisd-admm. recovery images showed numerical value comparison time psnr shown table compared lr-admm tnnr-admmtry lrisd-admm achieve better recovery quality expected. tnnr-admm-try achieves best recovery quality expected trying every possible value running time extremely longer lr-admm. proposed lrisd-admm achieve almost recovery quality tnnradmm-try signiﬁcant reductions computation cost. fact best precision expected estimated lrisd-admm reference search best around reasonably eﬀectiveness analysis twodimensional partial synthetic data subsection showed estimated lrisd-admm close best tnnradmm-try matrix completion problems. conﬁrm eﬀectiveness proposed lrisd-admm conducting experiments results below denote rank matrix estimated rank sample ratio taken respectively. noise level sample ratios choose tests. reason setting want well illustrate robustness noise. next compare diﬀerent settings. scenario generated model times report results. shown proposed performs rationality eﬀectiveness estimate step algorithm even much noise method still valid namely equivalent real rank. proposed pretty robust corruption noise sample data. practice achieve ideal results diﬀerent settings. save space illustrate eﬀectiveness using above-mentioned situations. comparison lrisd-admm lr-admm synthetic data subsection compare proposed lrisdadmm lr-admm partial data general rank matrix recovery cases. illustrate numerical results show advantages proposed lrisd-admm terms better recovery quality. easy noise level increases total reer becomes larger. even lrisd-admm achieve much better recovery performance lr-admm. lrisd model better approximate rank function nuclear norm. thus illustrate lrisd-admm robust noise deals low-rank matrices. increasing sample ratio total reer reduces generally lrisd-admm better lr-admm lrisd-admm approximately recover rank matrix showed comparison lrisd-admm lr-admm real visual data subsection test three images door window compare recovery images lrisdadmm general lr-admm partial operator. tests process diﬀerent stages obtain depicted three images generate lrisd-admm. moreover shows recovery results algorithms. illustrated easy returns stable merely three iterations. estimated good estimate number largest singular values. seen lrisd-admm outperforms general lr-admm terms smaller psnr. important using eyeballs better ﬁdelity recoveries lrisd-admm true signals terms better recovering sharp edges. fig. comparisons results lr-admm tnnr-admm-try lrisd-admm lrisd-admm-adjust three images here. ﬁrst column original images. second column masked images. masked images obtained covering pixels original image test. third column depicts images recovered lr-admm. fourth fig. comparison results lrisd-admm lr-admm synthetic data. diﬀerent noise levels shown gives results diﬀerent sample ratios. shows recovery ranks diﬀerent estimated rank equivalent best rank. addition extend tnnr matrix completion general low-rank cases synthetic real visual data sets discussed. notice limited thresholding. eﬀective support detection guarantees good performance lrisd. therefore future research includes studying speciﬁc signal classes developing eﬀective support detection methods.", "year": 2014}