{"title": "Learning with Group Invariant Features: A Kernel Perspective", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We analyze in this paper a random feature map based on a theory of invariance I-theory introduced recently. More specifically, a group invariant signal signature is obtained through cumulative distributions of group transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of $N$ points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.", "text": "analyze paper random feature based theory invariance introduced speciﬁcally group invariant signal signature obtained cumulative distributions group-transformed random projections. analysis bridges invariant feature learning kernel methods show feature deﬁnes expected haar-integration kernel invariant speciﬁed group action. show non-linear random feature approximates group invariant kernel uniformly points. moreover show deﬁnes function space dense equivalent invariant reproducing kernel hilbert space. finally quantify error rates convergence empirical risk minimization well reduction sample complexity learning algorithm using invariant representation signal classiﬁcation classical supervised learning setting. encoding signals building similarity kernels invariant action group problem unsupervised learning reduces complexity learning task mimics brain represents information invariantly symmetries various nuisance factors convolutional neural networks achieve state performance many computer vision speech recognition tasks require large amount labeled examples well augmented data reﬂect symmetries world virtual examples obtained applying identitypreserving transformations shearing rotation translation etc. training data. work adopt approach representation signal designed reﬂect invariant properties model world symmetries group actions. ultimate bridge unsupervised learning invariant representations invariant kernel methods tools classical supervised learning easily address statistical consistency sample complexity questions indeed many invariant kernel methods related invariant kernel networks proposed. refer reader related work section review start showing accomplish invariance group-invariant haarintegration kernels show random features derived memory-based theory invariances introduced approximate kernel. start reviewing group-invariant haar-integration kernels introduced binary classiﬁcation problem. section highlights conceptual advantages kernels well practical inconvenience putting perspective advantage approximating explicit invariant random feature maps. invariant haar-integration kernels. consider subset hypersphere dimensions sd−. measure consider kernel radial basis function kernel. group acting normalized haar measure assumed compact unitary group. deﬁne invariant kernel haar-integration follows integrating entire group easy that hence haar-integration kernel invariant group action. symmetry obvious. moreover positive deﬁnite kernel follows positive deﬁnite well haar-integration kernel framework another form data augmentation since produce group-transformed points order compute kernel. invariant decision boundary. turning binary classiﬁcation problem assume given labeled training {±}}n order learn decision function minimize following empirical risk induced l-lipschitz convex loss function minf∈hk restrict belong hypothesis class induced invariant kernel called reproducing kernel hilbert space representer theorem shows solution problem since hence decision boundary f∗is group-invariant well have reduced sample complexity. shown group-invariant kernel induces groupinvariant decision boundary translate sample complexity learning algorithm? answer question assume input following structure {z|z {e}} identity group element. structure implies function invariant rkhs have label posteriors. assume ρy∀g natural assumption since label unchanged given group action. assume endowed measure also group-invariant. group-invariant decision function consider expected risk induced loss deﬁned follows hence given invariant kernel group action identity preserving sufﬁcient minimize empirical risk core generalizes samples imagine ﬁnite cardinality |x|; cardinality core small fraction cardinality α|x| hence sample training points maximum size training α|x| yielding reduction sample complexity. reviewed group-invariant haar-integration kernel. summary group-invariant kernel implies existence decision function invariant group action well reduction sample complexity sampling training points reduced a.k.a core kernel methods haar-integration kernels come expensive computational price training test time computing kernel computationally cumbersome integrate group produce virtual examples transforming points explicitly group action. moreover training complexity kernel methods scales cubicly sample size. practical considerations make usefulness kernels limited. contributions paper three folds ﬁrst show non-linear random feature derived memorybased theory invariances introduced induces expected group-invariant haarintegration kernel ﬁxed points have satisﬁes show johnson-lindenstrauss type result holds uniformly points assess concentration random feature around expected induced kernel. sufﬁciently large uniformly points set. random feature space sampling points core generalizes unseen points reducing sample complexity. moreover show features deﬁne function space approximates dense subset invariant rkhs assess error rates empirical risk minimization using random features. random variable drawn according normalized haar measure random template whose distribution deﬁned later. deﬁne following truncated cumulative distribution function product section study geometric information captured kernel stating explicitly similarity computes. remark main advantage feature outlined store transformed templates order compute wanted compute invariant kernel type would need explicitly transform points. latter computationally expensive. storing transformed templates computing signature much efﬁcient. falls category memory-based learning biologically plausible section present main results proofs given supplementary material theorem shows random feature deﬁned previous section corresponds expectation group-invariant haar-integration kernel moreover computes average pairwise distance points orbits orbit deﬁned collection group-transformations given point theorem deﬁne distance orbits theorem sense invariant johnson-lindenstrauss type result show product deﬁned random feature concentrated around invariant expected kernel uniformly data points given sufﬁciently large number templates large number sampled group elements large number error naturally decomposes numerical error statistical errors sampling templates group elements respectively. probability putting together theorems following corollary shows group-invariant random feature captures invariant distance points uniformly dataset points. corollary log. number bins group elements universal numeric constants have integral interesting consider different distributions domain-speciﬁc templates assess number templates needed approximate kernels. also interesting optimal templates achieve minimum distortion equation data dependent address points future work. section show learning linear model invariant random feature space training sampled reduced core expected risk generalizes unseen test points generated distribution architecture proof follows ideas recall given l-lipschitz convex loss function minimize expected risk given equation denote empirical ihgitxi≤τ distribution templates ψψpdtdτ probability least training choice templates group elements. proof theorem given appendix theorem shows learning linear model invariant random feature space deﬁned expected risk. importantly risk arbitrarily close optimal risk achieved inﬁnitedimensional class functions namely training sampled reduced core invariant learning generalizes unseen test points generated distribution hence reduction sample complexity. recall dense rkhs haar-integration invariant kernel expected risk achieved linear model invariant random feature space attainable invariant rkhs. note error decomposes terms. ﬁrst statistical depends training sample complexity governed approximation error contributions perspective outlining previous work invariant kernels approximating kernels random features. approximating kernels. several schemes proposed approximating non-linear kernel explicit non-linear feature conjunction linear methods nystr¨om method random sampling techniques fourier domain translation-invariant kernels features fall random sampling techniques where unlike previous work sample projections group elements induce invariance integral representation. note relation random features quadrature rules thoroughly studied sharper bounds error rates derived apply setting. invariant kernels. focused paper haar-integration kernels since integral representation hence represented random features invariant kernels proposed authors introduce transformation invariant kernels unlike general setting analysis concerned dilation invariance. multilayer arccosine kernels built composing kernels integral representation explicitly induce invariance. closely related work kernel descriptors built visual recognition introducing kernel view histogram gradients corresponds case cumulative distribution group variable. explicit feature maps obtained kernel features obtained random sampling. finally convolutional kernel network builds sequence multilayer kernels integral representation convolution considering spatial neighborhoods image. future work consider composition haar-integration kernels convolution applied spatial variable group variable akin paper speciﬁcally theorems showed random group-invariant feature captures invariant distance points learning linear model trained invariant random feature space generalize well unseen test points. section validate claims three experiments. claims theorem nearest neighbor classiﬁer theorem rely regularized least squares classiﬁer simplest algorithms supervised learning. proofs focus norm-inﬁnity regularization corresponds tikhonov regularization square loss. speciﬁcally performing t−way classiﬁcation batch training points summarized data matrix rn×d label matrix rn×t perform regularization parameter feature representation described paper pooling data projected onto group-transformed random templates. experiments paper completed gurls toolbox three datasets explore perm artiﬁcial dataset consisting sequences length whose elements come alphabet characters. want learn function assigns positive value sequence contains target characters regardless position. thus function label globally invariant permutation project data onto permuted versions random template sequences. mnist seek local invariance translation rotation random templates translated pixels directions rotated degrees. tidigits subset tidigits consisting speakers reading digits isolation datapoint waveform single word. seek local invariance pitch speaking rate random templates pitch shifted cents warped play half double speed. task -way classiﬁcation class-per-digit. detail. acknowledgements stephen voinea acknowledges support nuance foundation grant. work also supported part center brains minds machines funded award figure classiﬁcation accuracy function training size averaged random training samples size. refers random feature bins templates. templates random feature outperforms features bag-ofwords representation even approaches classiﬁer haar-integration kernel. error bars removed plot clarity. supplement. figure left plot) mean classiﬁcation accuracy function number bins templates averaged random sets templates. right plot) classiﬁcation accuracy function training size averaged random samples training size. examples class achieve accuracy figure mean classiﬁcation accuracy function number bins templates averaged random sets templates. speaker dataset test unseen speakers gender dataset test gender giving extreme train/test mismatch. ≥+εzxz involve maximum correlated gaussian variables rotation invariance gaussians g′−z correlated random gaussian variables correllation coefﬁcient note =g−x g−z. following lemma gives expectation variance maximum gaussians correllation coefﬁcient lemma correlated gaussians correllation coefﬁcient deﬁne µx−µy mean variance probability training samples. function deﬁned lemma approximates lemma know that probability choice templates sampled group elements. optimality hence union bound probability training templates group elements have ﬁrst experiment created artiﬁcial dataset designed exploit permutation invariance providing ﬁnite group complete access. dataset xperm consists sequences length element sequence taken alphabet characters giving total data points. characters randomly chosen designated targets sequence xperm labeled positive contains position characters sequence matter. likewise sequence contain characters labeled negative. provides binary classiﬁcation problem label preserved permutations sequence indices i.e. sequences belong orbit permuted versions another. character encoded -dimensional vector every position sequence xperm formed concatenating vectors representing characters resulting binary vector length build permutation-invariant representation project binary sequences onto equal-length sequence consisting standard-normal gaussian vectors well permutations pool projections cdf. baseline also used bag-of-words representation xperm encoded -dimensional vector element equal count many times character appears note representation also invariant permutations share many beneﬁts feature map. classiﬁcation results points randomly chosen xperm form training even split positive points negative points. remaining points formed test set. know theorem expected risk dependent number templates used encode data number bins used cdf-pooling step. right panel figure shows classiﬁcation accuracy xperm different numbers templates bins. that ﬁxed number templates increasing number bins improve accuracy ﬁxed number bins adding templates improve accuracy. also know dependence number transformation samples group left panel figure shows classiﬁcation accuracy ﬁxed number training points bins templates depends number transformation access curve rather graceful degradation performance. figure include sample complexity plot error bars added. figure left) classiﬁcation accuracy random invariant features function number sampled group elements xperm. right) classiﬁcation accuracy random invariant features function number templates sizes xperm. figure classiﬁcation accuracy function training size. refers random feature bins templates. training size accuracy averaged random training samples. enough templates/bins random feature outperforms features well bag-of-words representation also train classiﬁer haar-invariant kernel naturally gives best performance. however increasing number templates come close matching performance random feature maps. figure mean classiﬁcation accuracy function number templates bins accuracy averaged random template samples error bars displayed. utterance dataset train test speakers test contains utterances digit. easiest dataset representing intraspeaker variability performance quite good even small number bins.", "year": 2015}