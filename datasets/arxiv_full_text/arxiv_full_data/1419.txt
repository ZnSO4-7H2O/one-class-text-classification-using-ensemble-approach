{"title": "Exploring the Imposition of Synaptic Precision Restrictions For  Evolutionary Synthesis of Deep Neural Networks", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "A key contributing factor to incredible success of deep neural networks has been the significant rise on massively parallel computing devices allowing researchers to greatly increase the size and depth of deep neural networks, leading to significant improvements in modeling accuracy. Although deeper, larger, or complex deep neural networks have shown considerable promise, the computational complexity of such networks is a major barrier to utilization in resource-starved scenarios. We explore the synaptogenesis of deep neural networks in the formation of efficient deep neural network architectures within an evolutionary deep intelligence framework, where a probabilistic generative modeling strategy is introduced to stochastically synthesize increasingly efficient yet effective offspring deep neural networks over generations, mimicking evolutionary processes such as heredity, random mutation, and natural selection in a probabilistic manner. In this study, we primarily explore the imposition of synaptic precision restrictions and its impact on the evolutionary synthesis of deep neural networks to synthesize more efficient network architectures tailored for resource-starved scenarios. Experimental results show significant improvements in synaptic efficiency (~10X decrease for GoogLeNet-based DetectNet) and inference speed (>5X increase for GoogLeNet-based DetectNet) while preserving modeling accuracy.", "text": "deep neural networks considered successful biology-inspired machine learning approaches shown signiﬁcant improvements accuracy machine learning approaches wide range challenges ranging image classiﬁcation segmentation speech recognition gene sequencing. contributing factor incredible feats signiﬁcant rise proliferation massively parallel computing power allowing researchers greatly increase size depth deep neural networks leading signiﬁcant improvements modeling accuracy. such much research deep neural networks focused design deeper larger complex deep neural networks less attention focus exploring notion synaptogenesis formation synapses neurons deep neural networks. however exploration synaptogenesis deep neural networks signiﬁcant beneﬁts formation efﬁcient deep neural network architectures important resource-starved scenarios computational complexity memory limited energy requirements strict. motivated explore synaptogenesis forming highly efﬁcient deep neural networks shaﬁee mishra wong took inspirations nature stochastic behaviour exhibited synaptogenesis observed hill wang riachi sch¨urmann markram speciﬁc functional connectivity neocortical neural microcircuits wistar rats evolutionary regression neurological functionality energy preservation observed eyeless mexican caveﬁsh evolved lose vision system generations high metabolic cost vision. leveraging inspirations evolutionary synthesis strategy proposed synthesize increasingly efﬁcient effective deep neural networks successive generations stochastic manner resulting evolved deep neural networks operate well resource-starved scenarios. differing neuroevolution approaches forgoes classical methods like genetic algorithms evolutionary programming instead introduces probabilistic generative modeling strategy. factor explored within evolutionary synthesis framework synaptic precision signiﬁcant impact computational complexity. particular observation made number researchers baldassi gerace lucibello saglietti zecchina that consistent recent biological ﬁndings near-optimal performance achieved deep neural networks limited synaptic precision motivated observation study explore imposition synaptic precision restrictions impact evolutionary synthesis deep neural networks. evolutionary synthesis strategy proposed shaﬁee progressively efﬁcient deep neural networks synthesized successive generations stochastic manner. genetic encoding deep neural network represented synaptic probability model viewed ‘dna’ network used mimic notion heredity. offspring network synthesized stochastically based synaptic probability model computational environmental conditions establish generation. offspring network trained reach modeling capability. evolution process repeated generations desired traits met. combination genetic information previous generation environmental conditions stochastic synthesis process encourages newly synthesized offspring networks possess diverse effective network architectures. instance imposed environmental conditions conﬁgured favor efﬁcient network architectures within stochastic synthesis process formation synapses offspring networks inherently sparser following random behavior mimics stochastic neuro-plasticity behaviour brain observed hill such random environmentallyinﬂuenced nature synaptogenesis evolved deep neural networks compensates reduction synapses compared previous generations. mathematically genetic encoding scheme formulated network architecture generation inﬂuenced synaptic strengths generation random variables binary random variable states speciﬁes whether synapse exists generation not. offspring network synthesized stochastically synthesis probability combines synapse probability model environmental factors imposed figure overview evolutionary synthesis deep neural networks imposed synaptic precision restrictions offspring network synthesized stochastically using ‘dna’ previous generation environmental factors mimic heredity natural selection random mutation. trained synaptic precision restriction imposed. process repeated generations. explore notion imposing synaptic precision restrictions impact evolutionary synthesis deep neural networks extend aforementioned process enforcing strong synaptic precision constraint generation training offspring network enforcing synaptic precision constraint trained offspring network effectively inﬂuences synaptogenesis behaviour offspring deep neural networks. study half-precision synaptic precision constraint enforced generation training performed full precision resulting offspring networks half-precision synaptic precision. successful impositions synaptic precision restrictions result evolved deep neural networks reduce computational requirements needed inference. study inﬂuence imposing synaptic precision restrictions evolutionary synthesis process examine performance evolved offspring deep neural networks generations within context object detection using parse-k dataset jetson within caffe framework tensorrt supports accelerated half-precision operations. figure demonstrates inference speed number synapses generations evolved deep neural networks using evolutionary synthesis process synaptic precision restriction impositions. seen number synapses decreases signiﬁcantly generations inference speed increased generations. shown figure precision recall largely consistently generations indicates modeling performance retained offspring deep neural networks. results show imposition synaptic precision restrictions within evolutionary synthesis process result evolved deep neural networks signiﬁcantly reduced number synapses also reduced precision requirements maintaining modeling inference performance signiﬁcant beneﬁts used resource-starved environments limited computational memory energy requirements.", "year": 2017}