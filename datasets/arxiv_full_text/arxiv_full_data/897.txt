{"title": "Rotational Unit of Memory", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks (RNN) to state-of-the-art performance in a variety of sequential tasks. However, RNN still have a limited capacity to manipulate long-term memory. To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms. In this paper we propose a novel RNN model that unifies the state-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM is its rotational operation, which is, naturally, a unitary matrix, providing architectures with the power to learn long-term dependencies by overcoming the vanishing and exploding gradients problem. Moreover, the rotational unit also serves as associative memory. We evaluate our model on synthetic memorization, question answering and language modeling tasks. RUM learns the Copying Memory task completely and improves the state-of-the-art result in the Recall task. RUM's performance in the bAbI Question Answering task is comparable to that of models with attention mechanism. We also improve the state-of-the-art result to 1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) task, which is to signify the applications of RUM to real-world sequential data. The universality of our construction, at the core of RNN, establishes RUM as a promising approach to language modeling, speech recognition and machine translation.", "text": "concepts unitary evolution matrices associative memory boosted ﬁeld recurrent neural networks state-of-the-art performance variety sequential tasks. however still limited capacity manipulate long-term memory. bypass weakness successful applications external techniques attention mechanisms. paper propose novel model uniﬁes state-of-the-art approaches rotational unit memory core rotational operation naturally unitary matrix providing architectures power learn long-term dependencies overcoming vanishing exploding gradients problem. moreover rotational unit also serves associative memory. evaluate model synthetic memorization question answering language modeling tasks. learns copying memory task completely improves state-of-the-art result recall task. rum’s performance babi question answering task comparable models attention mechanism. also improve state-of-the-art result bits-per-character loss character level penn treebank task signify applications real-world sequential data. universality construction core establishes promising approach language modeling speech recognition machine translation. recurrent neural networks widely used variety machine learning applications language modeling machine translation speech recognition ﬂexibility taking inputs dynamic length makes particularly useful tasks. however traditional models long short-term memory gated recurrent unit exhibit weaknesses prevent achieving human level performance limited memory–they remember hidden state usually occupies small part model; gradient vanishing/explosion training–trained backpropagation time models fail learn long-term dependencies. several ways address problems known. solution soft local attention mechanisms crucial modern applications rnn. nevertheless researchers still interested improving basic cell models process sequential data better. numerous works associative memory span large memory space. example practical implement associative memory weight matrices trainable structures change according input instances training. furthermore recent concept unitary orthogonal evolution matrices jing also provides theoretical empirical solution problem memorizing long-term dependencies. here propose novel cell resolves simultaneously weaknesses basic rnn. rotational unit memory modiﬁed gated model whose rotational operation acts associative memory strictly orthogonal matrix. tested model several benchmarks. able solve synthetic copying memory task traditional lstm fail. synthetic recall task exhibits stronger ability remember sequences hence outperforming state-of-the-art models fastweight weinet using achieve state-of-the-art result real-world character level penn treebank task. also outperforms basic models babi question answering task. performance competitive memory networks take advantage attention mechanisms. lstm designed solve problem gradient clipping still required training. recently restraining hidden-to-hidden matrix orthogonal unitary many models overcome problem exploding vanishing gradients. theoretically unitary orthogonal matrices keep norm gradient absolute value eigenvalues equals one. several approaches successfully developed applications unitary orthogonal matrix recurrent neural networks. arjovsky jing parameterizations form unitary spaces. wisdom applies gradient projection onto unitary manifold. vorontsov uses penalty terms regularization restrain matrices unitary hence accessing long-term memorization. learning long-term dependencies sufﬁcient powerful rnn. jing ﬁnds combination unitary/orthogonal matrices gated mechanism improves performance beneﬁts forgetting ability. jing also points optimal unitary/gated combination unitary/orthogonal matrix appear reset gate followed modrelu activation. implement orthogonal operation place construction matrix completely different instead parameterizing kernel encode natural rotation generated inputs hidden state. limited memory truly shortage. adding external associative memory natural solution. instance neural turing machine many models shown power using technique. expands accessible memory space technique signiﬁcantly increases size model therefore making process learning many parameters harder. brieﬂy describe concept associative memory. basic hidden state time step input data step. trainable parameters ﬁxed model. recent approach replaces dynamic matrix serve memory state. thus memory size increases hidden size. particular determined part multi-layer hopﬁled net. treating weights memory determined current input data larger memory size provided less trainable parameters required. signiﬁcantly increases memorization ability rnn. model also falls category associative memory rotational design orthogonal matrix. operation rotation efﬁcient encoder orthogonal operation acts unit memory. rotation computes orthogonal operator rnh×nh represents rotation non-collinear vectors two-dimensional subspace span euclidean space distance consequence kernel hidden state formally propose function practical advantage rotation orthogonal differentiable. hand composition differentiable sub-operations enables learning backpropagation. hand preserves norm hidden state hence yield stable gradients. motivated differentiable implementations unitary operations existing toolkits deep learning. conclusion rotation implemented various frameworks utilized deep learning architectures. indeed rotation constrained parameterize unitary structure instead produces orthogonal matrix simple components cell makes useful experimentation. implement rotation together action hidden state efﬁciently. need compute matrix rotate. instead directly apply equation hidden state. hence memory complexity algorithm determined figure rotation universal differentiable operation enables advantages architecture. rotation plane deﬁned acts hidden state cell rotation encodes kernel matrix acts thus keeps norm hidden state. note trainable vectors generate orthogonal weights rnh×nh means model degrees freedom single unit memory. likewise time complexity thus rotation universal operation enables implementations suitable neural network model backpropagation. propose recurrent unit memory ﬁrst example application rotation recurrent cell. figure sketch connections cell. consists update gate function gru. instead reset gate however model learns memory target variable rnh. also learns embed input vector yield hence rotation encodes rotation embedded input target accumulated associative memory unit rnh×nh non-negative integer hyper-parameter model. here orthogonal acts state produce evolved hidden state finally obtains hidden state gru. equations follows initial update gate memory target; activation update gate; embedded input rotation; rotational associative memory; unbounded evolution hidden state; hidden state time normalization introduced time subscripts demonstrate recurrence relations. kernels dimensions given rnx×nh rnh×nh ˜wxh rnx×nh. biases variables rnh. norm scalar hyper-parameter model. orthogonal matrix conceptually takes place kernel acting hidden state gru. efﬁcient place introduce orthogonal operation gated orthogonal recurrent unit experiments suggest. difference goru cell goru parameterizes learns kernel orthogonal matrix parameterize rotation instead learns together determines orthogonal matrix keeps norm vectors experiment relu activation instead conventional tanh gated mechanisms. even though orthogonal element norm stable relu activation. therefore suggest normalizing hidden state norm call technique time normalization usually feed mini-batches learning shape size batch length sequence feed time normalization happens along sequence dimension opposed batch dimension batch normalization. choosing appropriate model stabilizes learning ensures eigenvalues kernels bounded above. turn means smaller reduce effect exploding gradients. finally even though uses update gate standard gated mechanism reset gate. instead suggest utilizing additional memory target vector feeding inputs adapts encode rotations align hidden states desired locations without changing norm believe unit memory gives advantage gated mechanisms lstm gru. firstly test rum’s memorization capacity copying memory task. secondly signify superiority obtaining state-of-the-art result associative recall task. thirdly show even without external memory achieves comparable state-of-the-art results babi question answering data set. finally utilize rum’s rotational memory reach character level penn treebank. standard evaluate memory capacity neural network test performance copying memory task henaff arjovsky follow setup jing objective remember information received time steps earlier results task demonstrate utilizes different representation memory outperforms lstm gru; solves task completely despite update gate allow information encoded hidden stay pass through. gated model successful copying goru. figure reveals lstm predictable baseline equivalent random guessing. falls bellow baseline subsequently learns task achieving zero loss thousands iterations. figure orthogonal operation rotation enables solve copying memory task. delay times models except models training models rmsprop optimization learning rate decay rate batch size quickly case requiring norm time though training entails ﬂuctuations. hence believe choosing ﬁnite normalize hidden state important tool stable learning. moreover necessary task paper character level predictions large hidden sizes left unnormalized make cross entropy loss blow also observe beneﬁts tuning associative rotational memory. indeed smaller hidden size learns much quickly rum. possible accumulation phase enable faster long-term dependence learning case. either models overcome vanishing/exploding gradients eventually learn task completely. another important synthetic task test memory ability recurrent neural network associative recall. task requires remember whole sequence data perform extra logic sequence. follow setting zhang zhou modify original task test longer sequences. detail sequence characters e.g. asdfg??d. supposed output character based located sequence. needs look back sequence retrieve next character. example correct answer section details data. experiment compare lstm fast-weight recent successful weinet models hidden state different lengths batch size optimizer rmsprop learning rate lstm fails learn task lack sufﬁcient memory capacity. fast-weight fail longer tasks means cannot learn manipulate memory efﬁciently. table gives numerical summary results ﬁgure appendix compares graphically lstm. table comparison models convergence validation accuracy. recent weinet able successfully solve associative recall task hidden state signiﬁcantly less parameters. question answering remains important applicable tasks nlp. almost stateof-the-art performance achieved means attention mechanisms. works done improve performance developing stronger rnn. here tested babi question answering data demonstrate ability memorize reason without attention. task train sub-tasks jointly model. section detailed experimental settings results sub-task. compare model several baselines simple lstm end-to-end memory network goru. outperforms signiﬁcantly lstm goru achieves competitive result memnn attention mechanism. summarize results table emphasize sub-tasks table require large memory outperforms models attention mechanisms table question answering task babi dataset. test accuracy lstm memnn goru rum. signiﬁcantly outperforms lstm/goru performance close memnn uses attention mechanism. rotational unit memory natural architecture learn long-term structure data avoiding signiﬁcant overﬁtting. perhaps best demonstrate unique property among models test real world character level tasks. corpus collection articles wall street journal text english vocabulary consists words. split data train validation test sets according mikolov train feeding mini-batches size consist sequences consecutive characters. incorporate state-of-the-art high-level model fast-slow fs-rnn-k architecture consists hierarchical layers fast layer connects cells series; slow layer consists single cell organization roughly follows receives input mini-batch feeds state feeds state output probability distribution predicted character. table outlines performance fs-rnn models along results data present improved test bpc. mujika achieve record fslstm- setting lstm. authors paper suggest slow cell function capturing long-term dependencies data. hence natural given memorization advantages. particular experiment fs-rum- lstm. additionally test performance simple two-layer rum. models prone overﬁtting models follow experimental settings regularization mujika presented section techniques work particularly well combination rotational structure rum. speciﬁcally fs-rum- needs epochs converge following suitable learning rate pattern fs-rum- generalizes better gated models lstm learns efﬁcient patterns activation kernels. skill useful large penn treebank data special diagonal structure cell fs-rum- activates almost neurons hidden state. discuss representational advantage section advantage rotational unit memory allows model encode information phase hidden state. order demonstrate structure behind learning look kernels generate target memory model. figure visualization recall task demonstrates diagonal structure generates contrasted less). interpret importance diagonal contrast neuron hidden state plays important role learning since table fs-rum- achieve state-of-the-art test result penn treebank task. additionally non-extensive grid search vanilla models yields comparable results zoneout lstm. element diagonal activates distinct neuron. therefore seems utilizes capacity hidden state almost completely. reason might consider architecture close theoretical optimum representational power models. moreover diagonal structure task speciﬁc. example figure observe particular target penn treebank task. interpret meaning diagonal structure combined off-diagonal activations probably encode grammar vocabulary well links various components language. figure kernel generating target memory following diagonal activation pattern signiﬁes sequential learning model. temperature values variables model learned. task associative recall model without time normalization. interpretation function diagonal off-diagonal activations rum’s kernel tasks. task character level penn treebank model section additional examples. natural view rotational unit memory many approaches using orthogonal matrices fall category phase-encoding architectures phase information matrix. instance parameterize orthogonal matrix according efﬁi= block diagonal matrix containing numbers -by- rotations. component one-by- parameter vector. therefore rotational memory equation model rotational memory phase vectors time represents phases generated operation rotation correspondingly. note element matrix multiplication depends element each. means that cancel element model needs learn express negation thus concept phase-encoding simply special sampling manifolds generated special orthogonal group hidden size. extend current model allow real number associative memory equation rotation. expand representational power rotational unit. difﬁculty mathematically deﬁne raising matrix real power equivalent deﬁning logarithm matrix. again rotations prove natural choice since elements logarithms correspond elements vector space algebra associatied future work model applied higher-level structures. instance section already showed successfully embed fs-rnn achieve stateof-the-art results. examples include recurrent highway networks hypernetwork structures etc. fusion architectures could lead state-of-the-art results sequential tasks. proposed novel architecture rotational unit memory. model takes advantage unitary associative memory concepts. outperforms many previous state-of-the-art models including lstm goru synthetic benchmarks copying memory associative recall tasks. additionally rum’s performance real-world tasks question answering language modeling competetive advanced architectures include attention mechanisms. claim rotational unit memory serve benchmark model absorbs advantages existing models scalable way. indeed rotational operation applied many ﬁelds limited convolutional generative adversarial neural networks. would like thank konstantin rangelov supply computational power used research. grateful yichen shen charles roques-carmes peter rawn henry fidel cano-renteria rumen hristov fruitful discussions. many thanks pamela siska irina tomova comments paper. work partially supported army research ofﬁce institute soldier nanotechnologies contract wnf-d national science foundation grant ccf- semiconductor research corporation grant -ep--b. jimmy geoffrey hinton volodymyr mnih joel leibo catalin ionescu. using fast weights attend recent past. advances neural information processing systems kyunghyun bart merri¨enboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. geoffrey hinton deng dong george dahl abdel-rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine jing yichen shen tena dubcek john peurifoy scott skirlo yann lecun tegmark marin soljaˇci´c. tunable efﬁcient unitary neural networks application rnns. proceedings international conference machine learning volume pmlr david krueger tegan maharaj j´anos kram´ar mohammad pazeshki nicolas ballas rosemary anirudh goyal aaron courville chris pal. zoneout regularizing rnns randomly preserving hidden activations. arxiv preprint arxiv. tom´aˇs mikolov anoop sutskever ilya deoras hai-son stefan kombrink ˇcernock´y. subword language modeling neural networks. preprint http//www.fit. vutbr.cz/˜imikolov/rnnlm/char.pdf. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research eugene vorontsov chiheb trabelsi samuel kadoury chris pal. orthogonality learning recurrent networks long term dependencies. proceedings international conference machine learning volume proceedings machine learning research jason weston antoine bordes sumit chopra alexander rush bart merrinboer armand joulin tomas mikolov. towards ai-complete question answering prerequisite tasks. arxiv preprint arxiv. scott wisdom thomas powers john hershey jonathan roux atlas. full-capacity unitary recurrent neural networks. advances neural information processing systems julian georg zilly rupesh kumar srivastava koutn´ık j¨urgen schmidhuber. recurrent highway networks. proceedings international conference machine learning volume pmlr alphabet input consists symbols {ai} ﬁrst represent data copying remaining form blank marker symbols respectively. experiment data copying ﬁrst symbols input. expectation model output blank marker appears input output sequentially initial data steps. sequences training randomly generated consist pairs character number elements. always character. size character equal half length sequence size number equal therefore total category size task train models jointly sub-task. data divided training validation. ﬁrst tokenize words data combine story question simply concatenating sequences. different length sequences ﬁlled blank beginning end. words sequence embedded dense vectors sequential manner. model outputs answer prediction question softmax layer. batch size subsets. model trained adam optimizer learning rate subset trained epochs regularization applied. figure associative memory provided rotational operation rotation enables solve associative recall task. input sequences models training models rmsprop optimization learning rate decay rate batch size observe necessary tune associative memory since learn task. single supporting fact supporting facts three supporting facts arg. relations three arg. relations yes/no questions counting lists/sets simple negation indeﬁnite knowledge basic coreference conjunction compound coref. time reasoning basic deduction basic induction positional reasoning size reasoning path finding agent’s motivations mean performance table question answering task babi dataset. test accuracy lstm memnn goru rum. signiﬁcantly outperforms lstm/goru performance close memorynn uses attention mechanism. cells apply layer normalization cells lstm gates rum’s update gate target memory zoneout recurrent connections dropout fs-rnn. training adam optimization apply gradient clipping maximal norm gradients equal table lists hyper-parameters models. embed inputs higher-dimensional space. output models passes softmax layer; probabilities evaluated standard cross entropy loss function. bits-per-character loss simply cross entropy binary logarithm. hyper-parameters non-recurrent dropout cell zoneout hidden zoneout fast cell size associative power time normalization slow cell size length mini-batch size input embedding size initial learning rate epochs", "year": 2017}