{"title": "Post-Inference Prior Swapping", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "abstract": "While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice, convenient priors that allow for computationally cheap or tractable inference are commonly used. In this paper, we investigate the following question: for a given model, is it possible to compute an inference result with any convenient false prior, and afterwards, given any target prior of interest, quickly transform this result into the target posterior? A potential solution is to use importance sampling (IS). However, we demonstrate that IS will fail for many choices of the target prior, depending on its parametric form and similarity to the false prior. Instead, we propose prior swapping, a method that leverages the pre-inferred false posterior to efficiently generate accurate posterior samples under arbitrary target priors. Prior swapping lets us apply less-costly inference algorithms to certain models, and incorporate new or updated prior information \"post-inference\". We give theoretical guarantees about our method, and demonstrate it empirically on a number of models and priors.", "text": "bayesian methods praised ability incorporate useful prior knowledge practice convenient priors allow computationally cheap tractable inference commonly used. paper investigate following question given model possible compute inference result convenient false prior afterwards given target prior interest quickly transform result target posterior? potential solution importance sampling however demonstrate fail many choices target prior depending parametric form similarity false prior. instead propose prior swapping method leverages pre-inferred false posterior efﬁciently generate accurate posterior samples under arbitrary target priors. prior swapping lets apply less-costly inference algorithms certain models incorporate updated prior information post-inference. give theoretical guarantees method demonstrate empirically number models priors. carnegie mellon university machine learning department pittsburgh school computer science. correspondence willie neiswanger <williecs.cmu.edu>. simple parametric priors allow computationally cheap density queries maximization sampling reduce costs iterative inference algorithms gradient-based mcmc sequential monte carlo reasons might hope infer result convenient-but-unrealistic prior afterwards attempt correct result. generally given inference result might wish incorporate updated prior information result under different prior assumptions without re-run costly inference algorithm. leads main question paper given model possible convenient false prior infer false posterior afterwards given target prior interest efﬁciently accurately infer associated target posterior? potential strategy involves sampling false posterior reweighting samples importance sampling however depending chosen target prior—both parametric form similarity false prior—the resulting inference inaccurate high inﬁnite variance estimates instead devise method yields accurate inferences arbitrary target priors. furthermore like want make pre-inferred false posterior without simply running standard inference algorithms target posterior. note standard inference algorithms iterative data-dependent parameter updates iteration involve data computational cost quality update depends amount data used. hence running inference algorithms directly target posterior costly defeats purpose using convenient false prior. paper propose prior swapping iterative dataindependent method generating accurate posterior samples arbitrary target priors. prior swapping uses pre-inferred false posterior perform efﬁcient updates is-based methods developed task prior sensitivity analysis goal determine posterior varies sequence priors existing work proposed inferring single posterior under prior using methods infer further posteriors sequence strategy effective subsequent priors similar enough breaks priors sufﬁciently dissimilar ill-matched parametric families illustrate example below. note that general ˆµis alsurely. however estimates still fail practice ˆµis high inﬁnite variance. variance weights large lead inaccurate estimates. case variance ˆµis broad class satisﬁed exists given preπ inferred false prior accuracy thus depends target prior interest. example heavier tails variance ˆµis inﬁnite many intuitively expect variance higher dissimilar show concrete example fig. consider normal model data standard normal false prior yields closedform false posterior also normal. suppose we’d like estimate posterior expectation laplace target prior mean variance test function draw false posterior samples compute weights esti{˜θt}t mate ˆµis slows signiﬁcantly fig. ˆµis increases maintains high error even made large. analyze issue theoretically. suph since know pose want ˆµis normal compute lower bound number false posterior samples would needed depend data thus proceeds quickly. therefore advocate breaking difﬁcult inference problems easier steps ﬁrst inference using computationally convenient prior given model then future priors interest prior swapping. following sections demonstrate pitfalls using describe proposed prior swapping methods different types false posterior inference results give theoretical guarantees methods. finally show empirical results heavy-tailed sparsity priors bayesian generalized linear models relational priors components mixture topic models. methodology suppose dataset vectors chosen family models likelihood function parameterized suppose prior distribution space model parameters probability density function likelihood prior deﬁne joint model bayesian inference interested computing posterior distribution joint model interested following task given false posterior inference result exact approximate pdf) choose arbitrary target prior efﬁciently sample associated target posterior p—or generally compute expectation test function respect target posterior. importance sampling prior sensitivity begin describing initial strategy existing work related task known prior sensitivity analysis. suppose false posterior samples {˜θt}t importance sampling samples importance distribution used estimate expectation test function respect target distribution. straightforward idea false posterior example fig. hence we’d need samples note bound actually nothing parametric form π—it based solely normal false posterior distance target posterior mean however even distance small importance estimate would still inﬁnite variance laplace target prior. further note situation signiﬁcantly worsen higher dimensions false posterior lower variance. we’d like method work well even false target priors signiﬁcantly different different parametric families performance worsen priors made dissimilar. redoing inference target posterior costly especially data size large because per-iteration cost standard inference algorithms scales many iterations needed accurate inference. includes mcmc sequential monte carlo algorithms per-iteration cost still scales variance estimates still inﬁnite subsequent distributions ill-matched. note however depending represent much simpler analytic representation typically deﬁned likelihood function causes inference algorithms costs scale data size speciﬁcally low-complexity evaluated constant time respect data size general strategy surrogate standard mcmc optimization procedures yield data-independent algorithms constant cost iteration. intuitively likelihood information captured false posterior—we make instead likelihood function costly evaluate. concretely iteration standard inference algorithms must evaluate data-dependent function associated posterior density. example evaluate function proportional metropolis-hastings gradient-based mcmc methods hamiltonian monte carlo optimization procedures yield point estimate. prior swapping instead evaluate gradient optimization estimate here iteration requires evaluating simple analytic expressions thus complexity respect data size. instead leverage inferred false posterior more-efﬁciently compute future target posterior. begin deﬁning prior swap density suppose false posterior inference algorithm returned density function distance parametric form could produce high inﬁnite variance estimates. issue wanted procedure work well however performance depends similarity —and using false posterior samples estimate well approximates additionally prove certain choices guarantee ﬁnite variance estimate. note variance ˆµpsis bound this sufﬁcient show exists ﬁnite variance) satisfy condition propose certain parametric note that maintain prior swapping profamily evalucedure cost want ated constant time. general fewer terms yield faster procedure. mind propose following family densities. deﬁnition. parameter density sample markov chain involve data done constant time respect compute fig. draw samples {θt}t sample estimate ˆµps compare true value ˆµps converges relatively small number samples previous method applicable false posterior inference result here develop prior swapping methods setting access samples {˜θt}tf propose following procedure describe methods applying correction samples—one involving importance sampling involving semiparametric density estimation. additionally discuss forms guarantees forms optimize choice particular argue methods fail dissimilar ill-matching parametric forms. prior swap importance sampling. ﬁrst proposal applying correction prior swap samples involves aft= estimating sampling {θt}t treat {θt}t importance samples compute estimate adjusts samples generated. particular density estimate viewed product parametric density estimate nonparametric correction function density estimate consistent number samples instead correcting prior swap samples importance sampling correct updating nonparametric correction function continue generate false posterior samples. given samples {˜θt}tf parametric false posterior estimate hence prior swap density proportional product densities parametric prior swap density correction density. estimate expectations follow alg. before respect replace weight function ﬁnal estimate advantage strategy computing weights doesn’t require data—it thus constant cost respect data size additionally importance sampling prove procedure yields exact estimate asymptotically converges rate consistent showing theorem given false posterior samples {˜θt}tf tent i.e. mean-squared error satisﬁes inspired true form false posterior constant-time evaluation however estimate parameter using samples {˜θt}tf furthermore following guarantees. theorem deﬁned then exists corollary {θt}t test function satisﬁes varp variance estimate µpsis note know normalization constant issue prior swapping since need access function proportional mcmc algorithms. however still need estimate issue because unknown normalization constant function fortunately method score matching estimate given density unknown normalization constant. found optimal parameter draw compute samples weights samples compute estimate ˆµpsis give pseudocode full prior swap importance sampling procedure alg. semiparametric prior previous method chose parametric form general even optimal yield inexact approximation here incorporate methods return increasingly exact estimate given false posterior samples {˜θt}tf idea nonparametric kernel density estimate plug however nonparametric density estimates yield inaccurate density tails fare badly high dimensions. help mitigate problems turn semiparametric estimate begins parametric estimate show empirical results bayesian generalized linear models sparsity heavy tailed priors latent factor models relational priors factors demonstrate empirically prior swapping efﬁciently yields correct samples cases allows apply certain inference algorithms more-complex models previously possible. following experiments refer following procedures assess performance choose test function compute euclidean distance estimate returned procedure. denote performance metric posterior error ˆµh. since typically available analytically single chain mcmc target posterior million steps samples ground truth compute timing plots assess error method given time point collect samples drawn time point remove ﬁrst quarter burn time takes compute corrections. sparsity-encouraging regularizers gained high level popularity past decade ability produce models greater interpretability parsimony. example norm used induce sparsity great effect shown equivalent mean-zero independent laplace prior bayesian setting inference given sparsity prior difﬁcult often requires computationally intensive method posterior approximations make factorization parametric assumptions propose cheap accurate solution ﬁrst inference result more-tractable prior prior swapping quickly convert result posterior given sparsity prior. exp{−|θi|./σ} priors. here normal conjugate allows exact false posterior inference. second experiments bayesian logistic regression models write bern logistic ...n. pair heavy tailed priors hierarchical target prior gamma. experiments also normal however false prior longer conjugate mcmc sample linear regression yearpredictionmsd data set* regression used predict year associated song logistic regression miniboone particle identiﬁcation data set† binary classiﬁcation used distinguish particles. fig. compare prior swapping methods order show prior swapping procedures yield accurate posterior estimates compare speeds convergence. plot posterior error wall time method’s estimate posterior mean sparsity target priors linear logistic regression. linear regression since normal conjugate allows compute closed form prior swap exact method however also sample compute ˜pα∗ therefore compare methods prior swap parametric correction methods. logistic regression closed form here compare methods make samples fig. prior swapping methods quickly converge nearly zero posterior error. additionally linear regression prior swap parametric using ˜pα∗ yields similar posterior error prior swap exact uses figure comparison prior swapping methods bayesian linear logistic regression laplace verysparse target priors. prior swapping methods quickly converge posterior errors. figure prior swapping fast inference bayesian linear models sparsity heavy-tailed priors convergence plots showing prior swapping performs accurate inference faster comparison methods robust changing inferred density marginals prior sparsity increased. prior swapping results variety different sparsity priors. fig. show prior swapping used fast inference bayesian linear models sparsity heavy-tailed priors. plot time needed ﬁrst compute false posterior prior swapping target posterior compare algorithm directly target posterior. show convergence plots prior swapping performs faster inference direct plot reduce variance target prior; hurts accuracy false posterior prior swapping still quickly converges zero error. show density marginals increase prior sparsity show prior swapping results various sparsity priors. appendix also include results logistic regression hierarchical target prior well results synthetic data able compare timing posterior error tune many latent variable models machine learning—such mixture models topic models probabilistic matrix factorization others—involve latent factors often we’d like priors encourage interesting behaviors among factors. example might want dissimilar factors diversity-promoting prior factors show sort sparsity pattern inference models often computationally expensive designed case-by-case basis however conjugate priors placed factor parameters collapsed gibbs sampling applied. method factor parameters integrated leaving subset variables; these conditional figure latent factor models prior swapping results relational target priors components mixture model. prior swapping diversity-promoting target prior topic model separate redundant topic clusters; words topic shown. show wall times initial inference prior swapping. distributions computed analytically allows gibbs sampling variables. afterwards samples collapsed factor parameters computed. hence propose following strategy ﬁrst assign prior factor parameters allows collapsed gibbs sampling; afterwards reconstruct factor samples apply prior swapping complex relational priors factors. thus perform convenient inference collapsed model apply more-sophisticated priors variables uncollapsed model. ﬁrst show results gaussian mixture model written {µm}m ...n. using normal {µm}m allows collapsed gibbs sampling. also show results topic model text data here using dirichlet topics allows collapsed gibbs sampling. mixture models generate synthetic data model topic models simple english wikipedia‡ corpus topics. fig. show results mixture topic models. show inferred posteriors components number relational target priors deﬁne apply diversity-promoting target prior separate redundant topics. here show topic clusters separated distinct thematically-similar topics prior swapping. also show wall times inference methods. conclusion given false posterior inference result arbitrary target prior studied methods accurately compute associated target posterior efﬁciently leveraging pre-inferred result. argued shown empirically strategy effective even false target posteriors quite dissimilar. believe strategy shows promise allow wider range inference alorithms applied certain models allow updated prior information more-easily incorporated models without re-incurring full costs standard inference algorithms. bornn luke doucet arnaud gottardo raphael. efﬁcient computational approach prior sensitivity analysis cross-validation. canadian journal statistics metropolis nicholas rosenbluth arianna rosenbluth marshall teller augusta teller edward. equation state calculations fast computing machines. journal chemical physics minka thomas expectation propagation approxproceedings sevimate bayesian inference. enteenth conference uncertainty artiﬁcial intelligence morgan kaufmann publishers inc. smith adrian roberts gareth bayesian computation gibbs sampler related markov chain monte carlo methods. journal royal statistical society. series newman david welling max. collapsed variational bayesian inference algorithm latent dirichlet allocation. advances neural information processing systems wang chong blei david collaborative topic modeling recommending scientiﬁc articles. proceedings sigkdd international conference knowledge discovery data mining prior swapping pseudocode give pseudocode prior swapping procedure given false posterior inference result using prior swap functions π−∇θ described sec. alg. show prior swapping metropolis-hastings algorithm makes repeated alg. show prior swapping hamiltonian monte carlo makes repeated special case alg. occurs number simulation steps prior swapping langevin dynamics. here prove theorems stated sec. throughout analysis assume samples {˜θt}tf false-posterior denotes bandwidth semiparametric false-posterior density estimator ˜psp h¨older class deﬁned times differentiable functions whose derivative satisﬁes proof. prove mean-square consistency semiparametric prior swap density estimator give bound mean-squared error show tends zero increase number samples drawn false-posterior. prove this bound bias variance estimator bound mse. following avoid cluttering notation drop subscript ﬁrst bound bias semiparametric prior swap estimator. write bias show empirical results logistic regression model hierarchical target prior given gamma. synthetic data able compare timing posterior error different methods tune experiment assume given samples false posterior want mostefﬁciently compute target posterior prior addition prior swapping methods standard iterative inference algorithms mcmc variational inference target posterior comparisons. following experiments show that data size grows large enough prior swapping methods become efﬁcient standard inference algorithms. also show held-out test error prior swapping matches standard inference algorithms. experiments also prior swap method called prior swapping method involves making approximation using prior swapping allows whether test error similar standard inference algorithms compute approximation posterior. finally show results range target prior hyperparameter values show prior swapping maintains accuracy full range. show results fig. vary number observations prior swapping constant wall time wall times mcmc increase prior swapping methods achieve test error standard inference methods. vary number dimensions case methods increasing wall time test errors match. vary prior hyperparameter prior swapping infer single mcmc applied compute hyperparameter results using demonstrates prior swapping quickly infer correct results range hyperparameters. here prior swapping semiparametric method matches test error mcmc slightly better parametric method. figure bayesian hierarchical logistic regression wall time test error comparisons varying data size increased wall time remains constant prior swapping grows standard inference methods. wall time test error comparisons varying model dimensionality wall time test error comparisons inferences prior hyperparameters here single false posterior used prior swapping hyperparameters.", "year": 2016}