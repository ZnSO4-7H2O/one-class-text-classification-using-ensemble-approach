{"title": "Accelerating Innovation Through Analogy Mining", "tag": ["cs.CL", "cs.AI", "stat.ML"], "abstract": "The availability of large idea repositories (e.g., the U.S. patent database) could significantly accelerate innovation and discovery by providing people with inspiration from solutions to analogous problems. However, finding useful analogies in these large, messy, real-world repositories remains a persistent challenge for either human or automated methods. Previous approaches include costly hand-created databases that have high relational structure (e.g., predicate calculus representations) but are very sparse. Simpler machine-learning/information-retrieval similarity metrics can scale to large, natural-language datasets, but struggle to account for structural similarity, which is central to analogy. In this paper we explore the viability and value of learning simpler structural representations, specifically, \"problem schemas\", which specify the purpose of a product and the mechanisms by which it achieves that purpose. Our approach combines crowdsourcing and recurrent neural networks to extract purpose and mechanism vector representations from product descriptions. We demonstrate that these learned vectors allow us to find analogies with higher precision and recall than traditional information-retrieval methods. In an ideation experiment, analogies retrieved by our models significantly increased people's likelihood of generating creative ideas compared to analogies retrieved by traditional methods. Our results suggest a promising approach to enabling computational analogy at scale is to learn and leverage weaker structural representations.", "text": "legal precedents innovation spurred analogy well analogy bicycle allowed wright brothers design steerable aircra. whether architecture design technology mathematics ability apply paerns domains fundamental human achievement explosion available online data represents unprecedented opportunity analogies accelerate human progress across domains. example patent database full text million patents issued present. innocentive contains business social policy scientic technical problems solutions. irky company assists inventors development process million product idea submissions. openideo receives hundreds solutions variety social problems. millions scientic papers legal cases searchable google scholar. believe data form treasure trove analogies accelerate problem solving innovation discovery. striking recent example mechanic invented simple device ease dicult childbirths drawing analogy extracting cork wine bole discovered youtube video. award-winning device could save millions lives particularly developing countries. imagine future people could search data based deep analogical similarity rather simple keywords; lawyers legal scholars could legal precedents sharing similar systems relations contemporary case; product service designers could mine myriad potential solutions problem. however siing massive data sources relevant useful analogies poses serious challenge humans machines. humans memory retrieval highly sensitive surface similarity favoring near within-domain analogs share object aributes structurally similar analogs share object relations analogical processing also incurs heavy cognitive load taxing working memory even relations required processed searching datasets thousands millions items structurally similar ones daunting prospect. finding analogies challenging machines well based understanding deep relational similarity entities dierent terms surface aributes example chrysippus’ analogy sound waves water waves required ignoring many dierent surface features recent advances data abstract availability large idea repositories could signicantly accelerate innovation discovery providing people inspiration solutions analogous problems. however nding useful analogies large messy realworld repositories remains persistent challenge either human automated methods. previous approaches include costly handcreated databases high relational structure sparse. simpler machinelearning/information-retrieval similarity metrics scale large natural-language datasets struggle account structural similarity central analogy. paper explore viability value learning simpler structural representations specically problem schemas specify purpose product mechanisms achieves purpose. approach combines crowdsourcing recurrent neural networks extract purpose mechanism vector representations product descriptions. demonstrate learned vectors allow analogies higher precision recall traditional information-retrieval methods. ideation experiment analogies retrieved models signicantly increased people’s likelihood generating creative ideas compared analogies retrieved traditional methods. results suggest promising approach enabling computational analogy scale learn leverage weaker structural representations. introduction ability useful analogies critical driving innovation variety domains. many important discoveries science driven analogies example analogy bacteria slot machines helped salvador luria advance theory bacterial mutation. analogical reasoning forms foundation eectiveness argument dependent permission make digital hard copies part work personal classroom granted without provided copies made distributed prot commercial advantage copies bear notice full citation page. copyrights components work owned others must honored. abstracting credit permied. copy otherwise republish post servers redistribute lists requires prior specic permission and/or fee. request permissions permissionsacm.org. august halifax canada acm. ----//.... mining information retrieval include variety natural language techniques words parts speech language feature-based vector representations order calculate similarity measures examples include word embedding models like wordvec vector-space models like latent semantic indexing probabilistic topic modeling approaches like latent dirichlet allocation approaches excel detecting surface similarity unable detect similarity documents whose word distributions disparate. problem especially acute source target domains dierent another approach nding analogies structural similarity sentences texts using coupled clustering detecting structural correspondence text however approaches typically require rich data sets clear substructures whereas descriptions problems ideas existing online databases short sparse lack consistent structure. current methods focus narrow analogy tasks four-term analogy problems particular short strings contrast wish analogies real world data involve complex representations diverse analogical relations. paper interested automatically discovering analogies large unstructured data sets. particular focus corpus product innovations. insights behind approach believe make problem tractable despite longstanding status holy grail cognitive science first rather trying solve problem fully structured analogical reasoning instead explore idea retrieving practically useful analogies weaker structural representations learned reasoned scale ease extraction structure expressivity). specically investigate weaker structural representation idea’s purpose mechanism useful analogies. second insight advances crowdsourcing made possible harvest rich signals analogical structure help machine learning models learn ways would possible existing datasets alone. paper combines ideas contribute technique computationally nding analogies unstructured text datasets beyond surface features. high level approach uses behavioral traces crowd workers searching analogies identifying purpose mechanisms ideas developing machine learning models develop similarity metrics suited analogy mining. demonstrate learning purpose mechanism representations allows analogies higher precision recall traditional information-retrieval methods based tf-idf glove challenging noisy settings. furthermore similarity metrics automatically analogies products high purpose similarity mechanism similarity. user study show able inspire participants generate innovative ideas alternative baselines increasing relative proportion positively-rated ideas least representations expressive notoriously dicult obtain. section investigate weaker structural representation. goal come representation learned still expressive enough allow analogical mining. analogies product ideas intricately related purpose mechanism. informally think product’s purpose what does used product’s mechanism works. importance product’s purpose mechanism core components analogy theoretically rooted early cognitive psychology work schema induction dene core components schema goal proposed solution recently practical value dening problem schema purpose mechanism demonstrated empirical benets nding using analogies augment idea generation separating idea purpose mechanisms enables core analogical innovation processes re-purposing given product purpose nding another assume product vectors representing product’s purpose mechanism respectively. using representation able apply rich queries corpus products decomposition products purpose mechanism also draws inspiration engineering functional models ontologies describing products although common denition functions much research functionality conducted areas functional representation engineering design value engineering. scope ontologies however highly mechanistic engineering-oriented many cases observe product data purpose product naturally understood others term whether entertainment leisure serious purposes target user forth. scale collection purpose mechanism labels microtasks crowdsourcing markets specically show amazon mechanical turk crowd workers product description asking annotate parts text consider purposes product parts related mechanisms. frame problem simple terms guiding workers look words/phrases/chunks text talking what product does good works components seen figure juxtapose copies product text side-by-side ease cognitive load encourage workers give purpose mechanism tags similar overlapping thus capturing potentially richer distinct signal. corpus consisted products. product annotated four workers. collecting analogies. previous preliminary work explored crowdsourcing label analogies collecting labeled examples analogies metric-learning deep learning model. showing promising results process collecting labeled analogies proved expensive requiring considerable cognitive eort workers thus time limiting number labels realistically collected. addition work deep learning model blind rich structures purpose mechanism hope recovering automatically relative data scarcity. paper take dierent approach focusing resources collecting purpose mechanism annotations crowd collecting small number curated labeled analogies strictly purpose evaluation extracting purpose mechanism vectors. section describe approach learning extract purpose mechanism product representations. begin training product texts variablelength sequence tokens document collect purpose annotations mechanism annotations number workers annotate document. dene purpose annotation binary vector importantly dataset product descriptions contains noisy texts wrien informally non-professional people. texts product descriptions lacking detail ill-dened. automatically describe product terms formal functional model would require inordinate amount meticulous data annotation collection professional engineers large number product descriptions. thus resort soer approach hoping compromise level detail enable datadriven methods automatically extract useful representations product purpose mechanism. finally also make note potentially wider applicability automatically extracting representations real-word product descriptions. identifying components functions products could conceivably improve search capabilities internal external product databases perhaps enhance recommender systems beer understanding user looking product product oers. last idea connected line work product dimensions shown implicitly identifying properties products book wizards) helps improving recommendations. authors propose method combines ratings data textual product reviews hoping implicitly recover topics text inform recommendations. look product dimensions target abstract broad directly learn supervised fashion annotated data. data innovation corpus. test approach corpus product descriptions irky.com online crowdsourced product innovation website. irky representative kinds datasets interested large unstructured covers variety domains makes cross-domain analogies possible. following example illustrates typical length messiness product ideas dataset water bowl/dispenser vehicle holder. spill catch water optional sleeve larger holders optional floor base valve water cant flow bottle small reservoir reservoir acts backsplash water bottle attachment holds water vehicle cupholder foldable handle unit holder dishwasher safe optional sleeve larger holders collecting purpose mechanism data. addition irky innovation corpus needed collect analogy-specic data train model. previous approaches creating structured representations items analogical computation example predicate logic extremely heavyweight take tens person-hours complex items instead develop lightweight task avoids complex structure instead relies cognitive expertise intuitions people able separate purpose product mechanism. next purpose mechanism weight shared representation docmatrices respectively. ument transform vectors forming purpose mechanism predictions product parameters network tuned minimize loss averaged scenarios care predicting either purpose mechanism case could incorporate weight term loss function giving weight either purpose mechanism vector interpretations. here give intuition kinds representations extracted ability interpret simple tools. compute held-out product texts. approach interpreting purpose mechanism predictions glove word vectors similar among vectors appear vocabulary. second approach recover word vectors sparse linear combination approximately gives formally spirit sparse coding approach consider collection word vectors vocabulary stack matrix solve following optimization problem table display examples applying simple methods product texts test data product yogurt maker machine used concentrating yogurt heat reduce time energy. observe words selected related purpose vector representation include food produce concentrate making energy reduce also words typical language used describing advantages products data especially whole enough much. mechanism words indeed overall much mechanical nature including liquid heat cooling pump steel machine. examples observe paern words selected closely-related purpose mechanism representations using simple techniques empirically appear reect corresponding properties product text language deeper meaning. surface seing appears lend naturally sequence-to-sequence learning important dierences. dierence seing problem interest learn recover latent exactly unseen products rather extract form representation captures overall purpose mechanism. care semantic meaning context representation captures respect product purposes mechanisms rather predicting individual words. additionally sequence-tosequence models typically involve heavier machinery work well large data sets suitable scenario thousands tagged examples. technical note instead sequence output simple solution aggregate annotations example taking union intersection annotations considering token positively annotated least positive labels. richer aggregations also used. considering focus capture overall representation however resort simple aggregation annotations. simple terms look words annotated take tf-idfweighted average word vectors. sequence glove word vectors representing select word vectors concatenate sequence. compute tf-idf scores tokens sequence tokens tf-idf scores take tf-idf-weighted average corresponding glove vectors. denote resulting weightedaverage vectors purpose mechanism annotations respectively. consider target vectors predict unseen texts. embedding texts weighted-average word vectors lead surprisingly good results across many tasks furthermore case simple weightedaverage several advantages. next lends straightforward machine learning seing suitable modestly-sized data objective nding overall vector representation used multiple ways chiey computation purpose-wise mechanism-wise distances products. additionally concatenating annotations weighting tf-idf naturally give weight average vector words frequently annotated thus giving higher impact words considered important annotators respect purpose/mechanism. learning purpose mechanism. training product texts corresponding target tuples represent pre-trained glove vectors goal learn function predicts model recurrent neural network follows. network takes input variable-length sequence sequence product small yogurt maker machine concentrating yogurt heat vacuum. round base drum customized scooper washable stainless steel drum parts. reduce time energy used. cover placed truck protect hail. elastic perimeter prevent wind blowing cover. snap velcro slits open door without removing cover. strong aachment wont blow away. inatable baes cover front windshield side. leash accessory removable compartments phone cards cash keys poop bags treats bowl. walk carry essentials without pockets purse bag. purpose words concentrate enough similar food even much especially reduce produce whole sparse coding making energy yogurt drum concentrate vacuum heavy foods aches service similar storm hail rain roofs doors wind front winds walls sparse coding roof hail padded obstructing defenses diesel windshield wets mechanism words liquid heat cooling similar pump steel machine water heating electric sparse coding vacuum cooled drum heavy ingredients design renewable stainless vending similar roof cover lining zipper boom hood plastic rubber sparse coding front cover insulation hail buckle sling watertight cuer blowing evaluation analogies typically done context learning document representations approach quantitative evaluation down-stream task document classication. evaluate predicted context ability capture distances reect analogies primary focus paper. create dataset analogies non-analogies. collecting analogies crowdsourcing crowdsourced analogy nding within irky products. crowd workers used search interface collect analogies pairs products seed documents. search task powered simple word-matching approach. deal word variants added lemmas word bagof-words associated product. search query also expanded lemmas associated query term. search results ranked descending order number matching terms. median completion time seed minutes further deal potential data quality issues recruited workers seed pairs tagged matches became positive examples analogy dataset. however coming negative examples dicult. borrowing information retrieval assume people read search results sequentially treat implicitly rejected documents negatives. important remember documents necessarily real negatives. increase chance document actually read restrict top- results. challenges. geing workers understand concept analogies avoiding tagging products supercially similar analogies proved challenge. address this scaolded search task requiring workers generate schema ern) describe core purpose mechanism product concrete terms abstract terms figure workers instructed products matched abstract schema created. found scaolded workow reduced number supercial matches; non-negligible portion pairs labeled positive either supercial matches near analogies likely strong tendency towards surface features analogical retrieval further products multifaceted search results implicitly rejected even analogous seed matching schema dierent initially identied worker. antitative results table present precision recall results. rank pairs test data based distances according various metrics including own. summary across levels approach outperformed baselines despite challenging noisy seing. considerable portion test product pairs tagged workers analogies despite surface similarity creating mislabeled positive examples favor surface-based baselines. addition ranking purpose-only mechanism-only also concatenate representations vector product observe overall improvement results although one-dimensional either purpose mechanism alone still beats baselines. using considerably beer results looking precision perhaps indicating tendency workers mechanism-based analogies. evaluation ideation analogy since major application enhanced search retrieval capabilities analogy enhanced creativity evaluate usefulness algorithms. examine degree model’s retrieved output improves people’s ability generate creative ideas compared methods. standard ideation task participants redesign existing product given inspirations help either approach tf-idf baseline random baseline. assumption approach provide participants useful examples similar purpose provide diverse mechanisms help explore diverse parts design space generating ideas. hypothesize approach lead beer results tf-idf baseline random baseline generate inspirations redesign task start using learned purpose mechanism representations document apply rich queries corpus products. particular assuming vectors normalized unit euclidean norm pairs products high mi·mi type reasoning discussed above core element analogical reasoning. take idea step forward clustering purpose diversifying mechanism. detail take products seen training follow simple intuitive procedure follows. denote corpus test-set products. denote number seed products wish experiment. denote number inspirations wish produce seed clustering purpose. first groups products similar purpose clustering purpose representation. k-means based vectors cluster compute intra-distance measure mse. prune clusters less instances. rank clusters descending order pick call clusters ktop-purpose corresponding cluster centers cluster ktop-purpose select product whose result diversication mechanism. seed products corresponding cluster products similar purposes. next need pick inspirations seed. purpose cluster. empirically observe purpose-clusters ktop-purpose generate vectors highly similar seed respect mechanism less order generate far-mechanism results seed candidate turn diversication results. problem extracting well-diversied subset results larger candidates seen work prominently context information retrieval ing). case assume found relevant results according purpose metric diversify mechanism metric many ways diversify results mainly diering objective function constraints. canonical measures max-min max-avg dispersion problems former subset maximized. words max-min problem subset products distance nearest products maximized. max-avg problem subset average distance pairs maximized. problems admit simple greedy algorithms constant-factor approximations choose max-min problem since want avoid displaying too-similar results even user solve problem using algorithm mentioned iteration selects candidate minimum distance already-selected product largest among remaining candidates measure distance according mechanism metric experiments seeds matches each respectively. figure overview excerpts ideation experiment. seed product. workers asked solve problem dierent way. middle inspirations conditions. note tf-idf baseline returns results domain method returns broader range products. bottom ideas generated users exposed dierent conditions. experiment design recruited workers redesign existing product common creative task design ensure robustness eects experiment included dierent seed products. participants paid participation. baseline surface participants receive product inspirations retrieved using tf-idf nding products similar seed. baseline meant simulate current search engines. order conditions counterbalanced prevent order effects. ensure unbiased permutations used fisher-yates shue assign seeds conditions every seed would seen conditions erent users). since prior work shown people benet analogies receive ideation begun ideation task proceeded phases generating ideas unassisted minute receiving inspirations generating ideas minutes. inspirations laid four pages inspirations page users could freely browse them. figure provides overview experiment excerpt data. original task redesign existing product case cell phone charger case. surface baseline retrieves products phone-related contrast algorithm retrieves diverse results human pulley-powered electricity generator suit. boom gure shows ideas generated users condition. interestingly results measures. interested ability approach enhance people’s ability generate creative ideas. following measured creative output rate participant generates good ideas. recruited graduate students judge idea generated participants good not. denition good follows standard denition creativity literature combination novelty quality feasibility judge instructed judge idea good satised following criteria uses dierent mechanism/technology original product proposes mechanism/technology would achieve purpose original product could implemented using existing technology defy physics agreement judges substantial fleiss kappa lending measure creativity acceptable interrater reliability. measure whether idea good computed thresholding number votes good least judges rated good. report results liberal strict seings evaluation. total ideas collected ideas judged good measure. mentioned above fisher-yates shue assign seeds conditions. take conservative approach step look seeds appeared across three conditions conditions another. slicing data good ideas. proportion good ideas condition next random baseline nally tf-idf baseline achieved results proportion test thus observe signicant terms absolute number positively-rated ideas terms proportions approach able generate considerably large relative positive eect leading beer ideas. total ideas collected ideas judged good. again start looking seeds appeared across three conditions leaves good ideas. proportion good ideas condition next-up random baseline nally tf-idf baseline achieved looking conservative majority-vote threshold observed eect method increases. looking seeds appeared across conditions basic make sure cancel possible confounding factors. rened aempting model eects condition them follows. interested likelihood given idea good function inspiration condition. however ideas independent participant generated multiple ideas ideas proposed dierent seeds. failing account dependencies would lead inaccurate estimates eects inspirations participants beer generating ideas others seeds might easy/dicult others. erefore used generalized linear mixed model figure showing proportion estimates random-eect logistic regression participants signicantly likely generate good ideas redesign ideation task given inspirations analogy approach compared baseline-surface baseline-random approaches resulting model eect inspiration condition) yields signicant reduction variance compared null model eects likelihood ratio model also yields reduction akaike information criterion null model indicating improved data overing. figure shows method signicantly higher probability good ideas. condence interval condition. tf-idf random advantages analogy condition baseline substantial statistically signicant tfidf random. tf-idf random tf-idf random. note condence intervals probability estimates relatively wide replications experiment possibly data could yield results somewhere between precise estimates true size eect. main take-away study approach yields reliable increase participants’ creative ability. discussion conclusion paper sought develop scalable approach nding analogies large messy real-world datasets. explored potential learning leveraging weak structural representation product descriptions. leverage crowdsourcing techniques construct training dataset purpose/mechanism annotations learn purpose mechanism vectors product. demonstrate learned vectors allow analogies higher precision traditional information-retrieval similarity metrics like tf-idf glove lda. received inspirations sampled analogy approach erent mechanism) compared traditional baseline random sampling approach. psychological perspective benets inspirations likely approach’s superior ability sample diverse still structurally similar inspirations since diversity examples known robust booster creative ability tf-idf approach yielded inspirations likely relevant also likely redundant homogeneous random sampling approach yields diversity relevance. moving weak structural representation based purpose mechanism signicantly increased feasibility analogy-nding extensions necessary generalize domains besides product descriptions. example purpose mechanism vectors distinguish higher lower level purposes/mechanisms core/peripheral purposes/ mechanisms also encode dependencies particular purposes/mechanisms. potentially fruitful areas future work especially important moving relatively simple product descriptions complex data scientic papers purposes mechanisms exist multiple hierarchical levels generally believe exploring tradeos degree structure learnability utility augmenting innovation could lead interesting points design space could theoretical practical value. acknowledgments. authors thank anonymous reviewers helpful comments. work supported grants chs- iis- iis- carnegie mellon’s initiative bosch google grant alon grant. dafna shahaf harry&abe sherman assistant professor. references sanjeev arora yuanzhi yingyu liang tengyu andrej risteski. linear algebraic structure word senses applications polysemy. arxiv preprint arxiv. danushka bollegala yutaka matsuo mitsuru ishizuka. measuring similarity implicit semantic relations web. proceedings international conference world wide web. recovery noise. ieee transactions information eory joel chan hope dafna shahaf aniket kiur. scaling analogy crowdsourcing machine learning. workshop computational analogy iccbr. joel chan christian schunn. importance iteration creative conceptual combination. cognition darren dahl page moreau. inuence value analogical thinking product ideation. journal marketing research deerwester susan dumais geroge furnas omas landauer. julie hirtz robert stone daniel mcadams simon szykman kristin wood. functional basis engineering design reconciling evolving previous eorts. research engineering design douglas hofstadter melanie mitchell others. copycat project model mental uidity analogy-making. advances connectionist neural computation theory consumer choice. journal consumer psychology julian mcauley jure leskovec. hidden factors hidden topics understanding rating dimensions review text. proceedings conference recommender systems. masanori ookubo yusuke koji munehiko sasajima yoshinobu kitamura riichiro mizoguchi. towards interoperability functional taxonomies using ontology-based mapping. proc. iced ekaterina shutova. models metaphor nlp. proceedings annual meeting association computational linguistics. association computational linguistics public legal eory research paper ilya sutskever oriol vinyals sequence sequence learning neural networks. advances neural information processing systems. tseng jarrod moss jonathan cagan kenneth kotovsky. role timing analogical similarity stimulation idea generation design. design studies ullman. mechanical design process york swaroop vaam bryan wiltgen michael helms ashok goel jeannee yen. dane fostering creativity biologically inspired design. design creativity lixiu aniket kiur robert kraut. encouraging outside-thebox inking crowd innovation rough identifying domains expertise. proceedings conference computer-supported cooperative work social computing. lixiu robert kraut aniket kiur. distributed analogical idea generation multiple constraints. proceedings conference computer-supported cooperative work social computing.", "year": 2017}