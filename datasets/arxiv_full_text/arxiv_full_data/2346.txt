{"title": "Learning the Dimensionality of Hidden Variables", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. Detecting hidden variables poses two problems: determining the relations to other variables in the model and determining the number of states of the hidden variable. In this paper, we address the latter problem in the context of Bayesian networks. We describe an approach that utilizes a score-based agglomerative state-clustering. As we show, this approach allows us to efficiently evaluate models with a range of cardinalities for the hidden variable. We show how to extend this procedure to deal with multiple interacting hidden variables. We demonstrate the effectiveness of this approach by evaluating it on synthetic and real-life data. We show that our approach learns models with hidden variables that generalize better and have better structure than previous approaches.", "text": "mod­ serious problem learning probabilistic presence hidden variables. variables observed sev­ eral observed variables. variables relations termining able. paper address latter problem context bayesian networks. approach utilizes ative state-clustering. allows efficiently range cardinalities show extend procedure deal multiple interacting hidden variables. demonstrate evaluating show approach learns models hid­ variables better bet­ structure observed variables. portant role improving quality learned model understanding main. crucial problem question deter­ issue mine dimensionality relevant cases learning gorithm attempts introduce sig­ model also nificant effect performance com­ complexity. example figure demonstrates phenomenon states parent variable merged children dependent given consequence might networks needed describe domain. phenomenon pronounced variable also parents. approach determining ables. approach starts \"maximal\" number states possible merges iteration instance \"hard\" assignment thus score data using complete functions standard dure progresses lead best improvement score. steps repeated states merged state. based scores intermedi­ stages choose cardinality able. show networks learned intermediate stages also good initial starting fine-tune parameters. variables. cations single-variable procedure tions several hidden variable. method structural detection elidan show leads learning bet­ performing given maximum likelihood estimate parameters. challenging learn structure proach problem introduce respect train­ evaluates candidate networks data search best network accord­ score. commonly used scoring function learn bayesian networks bayesian metric denote scoreane-this scoring metric uses balance likelihood learned model complexity representation. learning problem different training data incomplete states training data missing learn network contains hid­ variables computationally order learn parameters expectation search maximum likelihood posteriori) structures marginal likelihood cheeseman­ tions. commonly used approximation stutz score combines likelihoods parameters found estimate penalty term associated friedman extends idea algorithm realm structure uses e-step part structure rent model structure well parameters used computing expected sufficient statistics didate structures. based expected sufficient gorithm moves candidate apply structure expected sufficient statistics \"local\" maximum. tures. algorithm converges search space algorithm contains many conver­ gence points care taken choosing initialization ables finding structural work learned observed variables. show \"signature\" variable clique children how­ ever reconstructing miss edges. thus instead searching cliques mate cliques previous section ation number states instances agglomerative variable. repeat procedure. reexamine hidden variable markov blanked changed. thus continue procedure hidden variable changed cardi­ nality state assignment. evaluate applicability various learning tasks. start evaluating algorithm cardinality datasets hid. sampled instances dataset. manually variable network evaluated gave algorithm figure ability reconstruct shows typical behavior scoresde number variables states. alarm network. sets cardinality gested complete collapse single state. equivalent close look probabilities variables thus indeed seem almost redundant. confirm claim points find best scoring network. variables best score achieved vari­ able collapsed example takes seconds. tion procedure initial cardinality computationally early stopping time seconds also resulted worse made agglomerative time saved using agglomerative strawman hidden variables. structural expect learning algorithm quickly summarized states respectively binary. able effectively able. cardinality extra states lations structure importance ables suggested log-loss eration model learned binary values. alarm network. real-life data sets stock data dataset daily change major technology years represent message vector containing news­ group attributes removed common stop words frequency included group designator words. trained messages ran­ domly selected test data. base line original supplied input find­ without hidden variable hidden. solid diamonds score network hidden variable able arbitrarily network hidden variable method applied. cases network suggested origi­ network. network learned using agglomeration formed better learned network agglomera­ tion paper proposed agglomerative approach determining ables. compared method setting cardinality showed successfulness models. importance agglomeration algorithm nificant computational bust number instances also able deal effectively variables. finally hidden variable thetic real-life refine­ thors examined operations ment bayesian networks works mostly concerned ramifications erations inference cardinality tion. although data observable discretized variable. example friedman goldszmidt corporated process learning bayesian networks. like approach composable complexity tion. approach discretizing variables evant works stolcke omohundro works learn hidden markov models probabilistic agglomeration. ning possible using information viewed generalization general bayesian networks combining hidden variable aimed toward learning non-trivial data. incorporation variables essential improving prediction gain understanding domain. plan continue research thank noam lotner iftach nachman comments earlier drafts paper. work supported part israel science foundation friedman also supported ship harry sherman senior lectureship computer science. experiments equipment funded basic", "year": 2013}