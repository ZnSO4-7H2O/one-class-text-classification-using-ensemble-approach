{"title": "Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector  Machine Classifiers", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This work proposes a new algorithm for training a re-weighted L2 Support Vector Machine (SVM), inspired on the re-weighted Lasso algorithm of Cand\\`es et al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In particular, the margin required for each training vector is set independently, defining a new weighted SVM model. These weights are selected to be binary, and they are automatically adapted during the training of the model, resulting in a variation of the Frank-Wolfe optimization algorithm with essentially the same computational complexity as the original algorithm.  As shown experimentally, this algorithm is computationally cheaper to apply since it requires less iterations to converge, and it produces models with a sparser representation in terms of support vectors and which are more stable with respect to the selection of the regularization hyper-parameter.", "text": "additional cost algorithm. moreover shown experimentally proposed approach needs less iterations converge standard frank–wolfe resulting svms sparser much robust respect changes regularization hyper-parameter retaining comparable accuracy. follows deﬁnition weighted model inspired weighted lasso connection lasso svm. deﬁnition extended reweighted based iterative scheme deﬁne weights. proposal modiﬁcation frank–wolfe algorithm based re-weighting scheme train svm. algorithm results sparser model coincides model obtained using standard training algorithm automatically-selected subsample original training data. numerical comparison proposed model standard number diﬀerent datasets. experiments show proposed algorithm requires less iterations providing sparser model also stable modiﬁcations regularization parameter. remaining paper organized following way. section summarizes results regarding connection lasso. weighted re-weighted introduced section whereas proposed modiﬁed frank– wolfe algorithm presented section performance algorithm tested numerical experiments section section ends paper conclusions pointers work. notation denotes number training patterns number dimensions. data matrix denoted rn×d correspond transpose diﬀerent pattern corresponding vector targets denotes label i-th pattern. identity matrix dimension denoted rn×n. work proposes algorithm training reweighted support vector machine inspired re-weighted lasso algorithm cand`es equivalence lasso shown recently jaggi. particular margin required training vector independently deﬁning weighted model. weights selected binary automatically adapted training model resulting variation frank–wolfe optimization algorithm essentially computational complexity original algorithm. shown experimentally algorithm computationally cheaper apply since requires less iterations converge produces models sparser representation terms support vectors stable respect selection regularization hyper-parameter. regularization essential mechanism machine learning usually refers techniques attempt improve estimates biasing away samplebased values towards values deemed physically plausible practice often used avoid overﬁtting prior knowledge problem hand induce desirable properties resulting learning machine. properties called sparsity roughly deﬁned expressing learning machines using part training information. advantages terms interpretability model manageability also preventing over-ﬁtting. representatives type models support vector machines lasso model based inducing sparsity diﬀerent levels. hand svms sparse representation terms training patterns means model characterized subsample original training dataset. hand lasso models induce sparsity level features sense model deﬁned function subset inputs hence performing implicit feature selection. recently jaggi showed equivalence optimization problems corresponding classiﬁcation ‘-svm constrained regression lasso. explored work connection useful transfer ideas ﬁeld other. particular looking sparser svms paper reweighted lasso approach cand`es taken basis deﬁne ﬁrst weighted ‘-svm simple adjusting iteratively weights leads modiﬁed frank–wolfe algorithm. adaptation weights idea coeﬃcient small could correspond zero ground-truth model hence pushed zero. side coeﬃcient large likely diﬀerent zero ground-truth model hence penalization decreased order bias value. approach based constrained formulation allow training errors since resulting model always satisfy possible implementation idea problem without strong assumption following towards sparser important remark regarding rw-lasso reweighting scheme breaks equivalence explained section i.e. cannot simply apply rwlasso approach solve problem order sparsity instead analogous scheme directly included formulation section below. speciﬁcally shown fig. connection between lasso suggests apply weighting scheme also svm. order weights iterative procedure seems natural step although would require solve complete problem iteration. finally online procedure determine weights adapted directly optimization algorithm lead modiﬁcation frank–wolfe algorithm. relation level optimization problem means ‘-svm model trained using approach training lasso model around cannot extended prediction phase since lasso model solving regression problem whereas solves classiﬁcation one. moreover number dimensions number patterns transforming problem other. nevertheless illustrated paper connection valuable inspire ideas. re-weighted lasso re-weighted lasso proposed approach approximate norm using norm re-weighting coeﬃcients particular approach initially designed approximate problem another illustration fig. shows small example three patterns allows represent feasible dimensions convex hull three vertices. value weight changed whereas weights kept ﬁxed before increasing weight pushes solution towards corresponding pattern. moreover last fig. shows example three dimensional representation clear eﬀect decreasing feasible basically lengthening triangle increasing angle respect horizontal plane point triangle becomes unbounded rectangle completely vertical. taking consideration solution unconstrained problem origin decreasing moving away ﬁrst vertex unconstrained solution thus making less likely assign non-zero coeﬃcient point unless really decreases objective function. parallelism original rw-lasso considering case relation weight corresponding optimal multiplier directly proportional following iterative approach namely re-weighted arises naturally weighted order transfer weighting scheme rw-lasso framework natural idea directly change constraint problem introduce scaling factors results following weighted-svm dual optimization problem therefore eﬀect increasing scaling factor w-svm dual formulation equivalent increasing margin required i-th pattern primal formulation. thus intuitively increase facilitate i-th pattern become support vector. inﬂuence numerically illustrated fig. value weight varied analyse inﬂuence corresponding multiplier binary classiﬁcation problem weights ﬁxed equal solving problem vector normalized maximum still equal order preserve scale. experiment done three diﬀerent values weights corresponding maximum minimum intermediate value multiplier standard svm. clearly present proportional figure example feasible region solution problem three patterns diﬀerent values weighting vector plot value shown boldface. three rows correspond changes respectively weighted probability simplex represented convex hull three vertices. fourth corresponds changes -dimensional representation keeping aspect ratio axis also including limit case upper bounded. solution constrained optimization problem shown approach main drawbacks. ﬁrst select function fmon. also implies selecting minimum maximum values weights saturate trivial task inﬂuence behaviour model. second drawback approach requires solve problem iteration means training completely w-svm model iteration hence overall computational cost much larger. although fact aﬀordable drawback objective solely approach norm case original paper rw-lasso case sparser models order reduce complexity resulting model improve performance specially large datasets hence make sense need several iterations. section proposes training algorithm sparser svms based online modiﬁcation weighting vector w-svm model. particular basis proposal frank–wolfe optimization algorithm. initial vertex. initial point. initial gradient. main loop. select forward node. select away node. build update direction. gap. compute step length. point update. gradient update. stopping criterion. active vectors weight inactive vectors weight proposed m-fw start initial active vector iteration inactive vector smaller negative gradient activated. that coeﬃcients active vectors updated using standard pair-wise step. intuition behind algorithm following. standard algorithm applied training activate coeﬃcient certain vector partial derivative better already active coeﬃcients i.e. vector less others. side m-fw activate coeﬃcient partial derivative negative i.e. vector somehow good itself. algorithm proposed m-fw algorithm train summarized alg. algorithm similar alg. except initialization control active lines search forward away nodes lines stopping criterion line convergence regarding convergence m-fw algorithm following lemma states algorithm provide model equivalent standard model trained subsample training patterns. lemma algorithm converges certain vector particular noticed that sparse nature expressed terms support vectors. nevertheless proposed provides trained subsample training although vectors subsample become support vectors. frank–wolfe algorithm frank–wolfe algorhtm ﬁrst order optimization method constrained convex optimization. several versions algorithm particular basis work pairwise frank–wolfe roughly speaking based using iteration linear approximation objective function select vertices target towards current estimate solution move another vertex solution move away updating solution direction goes away node forward using optimal step length. linear approximation boils selecting node corresponding smallest partial derivative forward node largest derivative away node. general algorithm used many diﬀerent contexts particular succesfully applied training svms speciﬁcally case problem following deﬁnitions results employed. truncating optimal step needed order remain convex hull nodes i.e. satisfy constraints problem straightforwardly problem solved simply taking derivative respect making equal zero noticed gradient point thus need compute moreover direction diˆki requires compute columns kernel matrix corresponding updated variables particular pairwise columns forward away nodes used determine keep gradient updated. modiﬁed frank–wolfe algorithm idea proposed modiﬁed frank–wolfe modify weights i.e. margin required training pattern directly inner iteration algorithm hence overall cost similar original particular since according figs. relation weight resulting coeﬃcient seems directly proportional incremental procedure binary weights deﬁned leading training algorithm svm. speciﬁcally training vectors divided groups active converged corresponding coeﬃcients satisfy hence aﬀect neither objective function gradient. therefore remaining iterations m-fw reduces standard algorithm considering vertices converge solution problem subset training patterns. worth mentioning that although proposed m-fw algorithm converges model trained subsample training data subsample depend initial point algorithm. section proposed m-fw algorithm compared standard algorithm several classiﬁcation tasks. particular binary datasets used experiments described table includes size training test sets number dimensions percentage majority class belong libsvm repository except mgamma miniboone belong repository standard model trained using model resulting proposed m-fw algorithm compared terms accuracies number support vectors number iterations needed achieve convergence training algorithm. diﬀerent kernels used linear ones. respect hyper-parameters models value bandwidth obtained -fold cross validation mgamma whereas largest dataset ijcnn tuned ﬁxed kernel hyper-parameters tuned models used predict test sets. stopping criterion used test results summarized table looking ﬁrst accuracies models svmm-fw practically equivalent three four experiments diﬀerences insigniﬁcant whereas ijcnn linear kernel accuracy higher case svmm-fw. regarding number support vectors svmm-fw gets sparser models ijcnn linear kernel mgamma kernel whereas experiments models comparable sparsity. finally respect convergence training algorithms svmm-fw shows advantage dealing linear kernels whereas ones approaches practically equivalent. noticed that larger datasets execution done dataset kernel hence diﬃcult solid conclusions. hence interesting analyse performance models phase done below. robustness w.r.t. hyper-parameter evolution respect parameter accuracy number support vectors number training iterations shown fig. proposed svmm-fw. kernel curves correspond optimum value svm. observing plots accuracy svmm-fw turns much stable getting accuracy almost optimal larger wide range values moreover accuracy achieved smaller number support vectors less training iterations. point value large enough svmm-fw perform since support vectors also become active vectors training svmm-fw algorithms m-fw provide model. stability svmm-fw concerning value regularization parameter suggests beforehand order tuning parameter. option explored next bunch experiments. exhaustive experiments following experiments smaller datasets second block table used compare exhaustively three models proposed svmm-fw alternative svmm-fw model ﬁxed regularization parameter results results detailed table includes three models mean standard deviation accuracy number support vectors number training iterations partitions. colours represent rank models dataset kernel rank used signiﬁcant diﬀerence models. results averaged summary table included percentage respect reference svm. table shows svmm-fw allows reduce number support vectors training iterations whereas accuracy drops moreover using svmm-fw approach allows avoid tuning reducing support vectors iterations drop accuracy accuracy. using wilcoxon signed rank test zero median signiﬁcance austral breast diabete german heart ionosph mushroo sonar austral breast diabete german heart ionosph mushroo sonar austral breast diabete german heart ionosph mushroo sonar figure evolution validation results ijcnn mgamma using linear kernel optimum standard proposed svmm-fw. striped regions represent range minimum maximum partitions whereas lines middle represent average values. legend model possible initial point trained many models training patterns partition. results ﬁrst measure dependence initialization diﬀerences sets support vectors models. table shows second column overlap sets support vectors every pair models diﬀerent initializations quantiﬁed percentage support vectors shared models total number support vectors. easiest datasets iris mushrooms show smallest overlaps hence highest dependence initialization. surprising since example iris dataset many hyperplanes separate classes perfectly. remaining datasets show overlap datasets therefore inﬂuence initialization depend strongly particular dataset. nevertheless looking accuracies included table speciﬁcally comparing results svmm-fw considering possible initializations seems noticeable diﬀerence them. particular reducing table single measure set-up experiment kernel used one. order hyper-parameters -fold applied small subsample patterns. although approach seem quite simplistic provides good enough parameters convergence comparison goal experiment. case svmm-fw ﬁxed optimal directly used instead tuning validation done model. selected models trained whole training iterations. process intermediate models extracted every iterations simulating diﬀerent selections stopping criterion intermediate models used predict test thus allow analyse evolution test accuracy function number iterations. observed standard starts higher accuracy rapidly matched svmm-fw later svmm-fw. nevertheless models ﬁnally comparable stable accuracy reach approximately number iterations main diﬀerence seen evolution number support vectors. ﬁrst iterations models introduce support vector iteration ﬁrst svmm-fw second svmm-fw saturate number presenting ﬁnal alﬂat phase. contrary although reduces slightly rate growth number support vectors continues adding patterns solution whole training. means that stopping criterion carefully chosen model much support vectors needed corresponding increase complexity. side svmm-fw svmm-fw limit successfully number support vectors providing sparser models accuracy svm. connection lasso support vector machines used propose algorithmic improvement frank–wolfe algorithm used train svm. modiﬁcation based re-weighted lasso enforce sparsity computationally requires additional conditional check iteration overall complexity algorithm remains same. convergence analysis modiﬁed frank–wolfe algorithm shows provides exactly model would obtain applying original algorithm subsample training set. several numerical experiments shown m-fw leads models comparable terms accuracy sparsity requiring less iterations trained much robust respect regularization parameter extent allowing parameter beforehand thus avoiding validation. possible lines extension work explore formulations example based loss allow even sparsity. m-fw algorithm could also applied training machine learning algorithms non-negative lasso even general optimization problems permit certain relaxation original formulation. authors would like thank following organizations. research leading results received funding european research council european union’s seventh framework programme datadrive-b paper reﬂects authors’ views union liable made contained information. research council goa// manet pfv// bil/t; phd/postdoc grants. flemish government phd/postdoc grants. phd/postdoc grants. iminds medical information technologies belgian federal science policy oﬃce iuap", "year": 2017}