{"title": "Detecting and Correcting for Label Shift with Black Box Predictors", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets), cause symptoms (observations), we focus on label shift, where the label marginal $p(y)$ changes but the conditional $p(x|y)$ does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution $p(y)$. BBSE exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. While better predictors give tighter estimates, BBSE works even when predictors are biased, inaccurate, or uncalibrated, so long as their confusion matrices are invertible. We prove BBSE's consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classifiers. Experiments demonstrate accurate estimates and improved prediction, even on high-dimensional datasets of natural images.", "text": "faced distribution shift training test wish detect quantify shift correct classiﬁers without test labels. motivated medical diagnosis diseases cause symptoms focus label shift label marginal changes conditional not. propose black shift estimation estimate test distribution bbse exploits arbitrary black predictors reduce dimensionality prior shift correction. better predictors give tighter estimates bbse works even predictors biased inaccurate uncalibrated long confusion matrices invertible. prove bbse’s consistency bound error introduce statistical test uses bbse detect shift. also leverage bbse correct classiﬁers. experiments demonstrate accurate estimates improved prediction even high-dimensional datasets natural images. assume august train pneumonia predictor. covariates consist chest x-rays administered previous year labels binary indicators whether physician diagnoses patient pneumonia. train convnet predict pneumonia given x-ray image. assume training patients pneumonia. deploy clinic several months reliably predicts roughly positive. fast-forward january running last week’s data patients predicted pneumonia remains ﬁxed shift must change marginal violating familiar assumption. absent familiar guarantees wonder still accurate? what’s real current rate pneumonia? shouldn’t classiﬁer trained obsolete prior underestimate pneumonia uncertain? thus might suspect real prevalence greater given labeled training data unlabeled test data desire detect distribution shift quantify correct model perform well data. absent assumptions changes task impossible. covariate shift assumption posits case covariate shift suggests distribution radiologic ﬁndings changes conditional probability pneumonia remains constant. it’s easy wrong feature cough? normally cough might indicate pneumonia. epidemic might substantially. instead posit labels shift i.e. manifestations disease change prevalence disease does distinction well-known schölkopf observe covariate shift corresponds causal learning label shift corresponds anticausal learning focus label shift motivated diagnosis recognition tasks pneumonia outbreak might rise manifestations disease might change. moreover sometimes purpose learning detect changes examples include monitoring public health wildlife populations despite importance label shift comparatively under-investigated given covariates quantifying seem intuitive. introduce black shift estimation estimate label shift using black predictor bbse estimates ratios label requiring expected confusion matrix invertible. estimate solving linear system confusion matrix training data average output calculated test data. make following contributions consistency error bounds bbse. apply bbse statistical tests detecting distribution shift bayesian inference bbse oﬀers following advantages despite wide applicability learning label shift unknown remains curiously under-explored. noting diﬃculty problem storkey proposes placing prior inferring posterior distribution test covariates. requires explicitly estimating feasible high-dimensional datasets. chan infer using method also requires estimating schölkopf articulates connections label shift anti-causal learning zhang extend kernel mean matching approach label shift problem. known label shift simpliﬁes problem changing base rates covariate shift also called sample selection bias well-studied rosenbaum rubin introduce propensity scoring design unbiased experiments. shimodaira earliest propose correcting models weighting examples q/p. later works estimate importance weights available data e.g. gretton propose kernel mean matching re-weight training points. earliest relevant work comes econometrics addresses non-random samples estimate behavior. heckman addresses sample selection bias investigates estimating parameters under choice-based endogenous stratiﬁed sampling cases analogous shift label distribution. crucially unlike setting assume test marginal distributions known. also related work foundational research propensity scoring finally note connection cognitive science work demonstrating humans classify items diﬀerently depending unlabeled items appear previous work requires estimating often relying kernel methods scale poorly dataset size underperform high-dimensional data. contrast bbse uses black-box predictors reduce dimensionality. exploits advances discriminative modeling degrade either accuracy tractability data feature dimensions grow. denote feature label variables. simplicity assume discrete domain equivalent source target distributions deﬁned denote probability density function probability mass function associated respectively. random variable interest clear context. example p.m.f. p.d.f. moreover short respectively denotes probability event denotes expectation. subscripts operators make referenced distribution clear. standard supervised learning learner observes training data drawn training distribution denote collection feature vectors rn×d label domain adaptation learner additionally observes collection samples drawn test distribution objective predict well samples drawn comment assumptions. corresponds anti-causal learning. assumption strong reasonable many practical situations including medical diagnosis diseases cause symptoms. also applies classiﬁers trained non-representative class distributions note visions systems commonly trained balanced classes true class distribution real tasks rarely uniform. assumption addresses identiﬁability requiring target label distribution’s support subset training distribution’s. discrete simply means training data contain examples every class. assumption requires expected predictor outputs class linearly independent. assumption holds typical case classiﬁer predicts class often given images actually belong given images class practice could neural network boosted decision-tree classiﬁer trained holdout training data set. verify training time empirical estimated normalized confusion matrix invertible. assumption generalizes naturally soft-classiﬁers outputs probability distribution supported thus bbse applied even confusion matrix degenerate. objective wish estimate every training data unlabeled test data predictor estimate enables techniques importance-weighted using q/p. label shift assumption importance weight q/p. task isn’t straightforward don’t observe samples estimate using data source distribution unlabeled test data drawn target distribution leads novel method-of-moments approach consistent estimation shifted label distribution weights without loss generality assume denote ˆµˆy moments plug-in estimates deﬁned proposition assumption true proof uses first borel-cantelli lemma show probability entire sequence empirical confusion matrices data size ...∞ simultaneously invertible converges thereby enabling continuous mapping theorem applying strong large numbers component. bounds give practical insights square error depends sample size proportional also term reﬂects diﬀerent source target distributions are. addition σmin reﬂects quality given classiﬁer example perfect classiﬁer σmin miny∈y cannot distinguish certain classes cˆyy low-rank σmin technique invalid expected. parse error bound ﬁrst term required even observe importance weight exactly. second term captures additional error fact estimate predictor note small uniform. note correctly classiﬁes class probability e.g. νy/σ proof theorem assumption ensures provide high probability bound euclidean norm operator norm give operator norm bound assumption yield high probability bound square estimation error. figure label-shift detection mnist. pane illustrates type error correctly controlled absent label shift. pane illustrates high power mild label-shift. pane shows increased power better classiﬁers. compare kernel two-sample tests oracle sample test directly tests samples each. proposed test beats directly testing high-dimensions nearly matches oracle. application results formally detection cast hypothesis testing problem null hypothesis alternative hypothesis recall observe neither samples however observe unlabeled data target distribution predictor proposition assumption classiﬁer satisfying thus weak assumptions test running two-sample tests readily available samples examples include kolmogorov-smirnoﬀ test anderson-darling maximum mean discrepancy. tests asymptotic distributions known almost perfectly control type error. power test depends classiﬁer’s performance distribution thereby allowing leverage recent progress deep learning attack classic problem detecting non-stationarity data distribution. could also test whether label-shift assumption implied advantage testing distribution instead need deal one-dimensional distribution. theory experiments two-sample tests high dimensions exponentially harder. figure estimation error correction accuracy dataset size mnist data compared dirichlet shift bbse conﬁdence interval runs runs computation; largest feasible experiment. note classes occur rarely test bbse produce negative importance weights. ﬂipped sign would cause maximize loss unbounded above. thus clip negative weights owing eﬃcacy generality approach serve default tool deal domain adaptation. ﬁrst things even label-shift assumption doesn’t hold. contrast heuristic method using logistic-regression construct importance weights lacks theoretical justiﬁcation estimated weights correct. even simpler problem average treatment eﬀect estimation it’s known using estimated propensity lead estimators large variance issue applies supervised learning. prefer live biased solution unweighted rather suﬀer high variance unbiased weighted erm. proposed approach oﬀers consistent low-variance estimator label shift. experimentally demonstrate power bbse real data simulated label shift. organize results three categories shift detection bbsd weight estimation bbse classiﬁer correction bbsc. bbse-hard denotes method yields classiﬁcations. bbse-soft outputs probabilities. label shift simulation simulate distribution shift experiments adopt following protocols first split original data train validation test sets. then given distributions generate sampling replacement appropriate split. knock-out shift knock fraction data points given class training validation sets. tweak-one shift assign probability classes rest mass spread evenly among classes. dirichlet shift draw dirichlet distribution concentration parameter uniform dirichlet shift bigger smaller label-shift detection conduct nonparametric two-sample tests described section using mnist handwritten digits data set. simulate label-shift randomly split training data training validating test data points apply knock-out shift class note diﬀer increasingly grows large making shift detection easier. obtain training two-layer relu-activated multilayer perceptron neurons training epochs. conduct two-sample test whether distribution using kolmogorov-smirnov test. results summarized figure demonstrate bbsd produces p-value distributes uniformly provides power state-of-the-art kernel two-sample test discriminates gets better train black-box predictor even more. weight estimation label-shift correction evaluate bbse mnist simulating label shift datasets various sizes. speciﬁcally split training data randomly using ﬁrst half train second half estimate full training weighted erm. before two-layer mlp. fair comparisons baselines full training data used throughout evaluate estimator ground truth prediction accuracy bbsc test set. cover variety diﬀerent types label-shift take uniform distribution generate dirichlet shift label-shift correction cifar next extend experiments cifar dataset using time allowing train epochs. consider tweak-one dirichlet shift compare bbse unweighted classiﬁer varying degrees shift tweak-one experiment averaging results choices tweaked label plotting variance. dirichlet experiments sample every choice range kernel-based baselines cannot handle datasets size dimensionality compare unweighted erm. kernel mean matching baselines compare bbse state-of-the-art kernel mean matching methods. detection experiments baseline kernel b-test extension kernel mean discrepancy test gretton boasts nearly linear-time computation little loss power. compare bbse approach zhang solves operator cx|y e|ψ] function denote kernel embedding respectively. note label-shift assumption cx|y also note since discrete simply one-hot representation deﬁnition cx|y must estimated ﬁnite data. proposal involves constrained optimization solving gaussian process regression automatic hyperparameter choices marginal likelihood. fair comparison used original authors’ implementations baselines also used median trick adaptively tune kernel’s hyperparameter. diﬀerence bbse matches distribution rather distribution like learn supervised learning rather specifying feature choosing kernel front. note like many kernel methods requires construction inversion gram matrix complexity hinders application real-life machine learning problems often thousands. experiments largest feasibly code roughly unfortunately stop mnist experiment. reason cannot cifar experiments. curves figure estimating suggest convergence rate slower bbse polynomial factor bbse better handles large datasets. constructing training error bounds estimates depend norm true vector q/p. conﬁrms common sense absent assumption given ability select class-conditioned examples annotations build dataset uniform it’s always possible apply bbse successfully test time correct sporadic shift settings might change sporadically. cases label shift occurs applying bbsc might damage classiﬁer. cases prose combine detection estimation correcting classiﬁer shift likely occurred. using known predictor experiments trained using random split data makes bbseto perform worse baseline data extremely small. practice especially context services could natural predictor currently deployed whose training data legacy little distributions trying distinguish. case lose factor suﬀer variance training small amount data. could allow detect mild shift distributions short period time. making suitable applications ﬁnancial market prediction. bbse degenerate confusion matrices practice sometime confusion matrices degenerate. instance class rare features partially predictive might cases straightforward variations black method still work first analysis focuses confusion matrices easily extends operator soft probabilities. class even never argmax example long soft confusion matrix invertible. even produce operator invertible confusion matrix options remain merge classes together yielding invertible confusion matrix. might able estimate frequencies classes estimate others accurately. another possibility compute pseudo-inverse. future work next step plan extend methodology streaming setting. practice label distributions tend shift progressively presenting challenge apply bbse trailing windows face trade-oﬀ. looking back increases lowering estimation error estimate less fresh. propensity weights makes bbse amenable doubly-robust estimates typical bias-variance tradeoﬀ related techniques common covariate shift correction.", "year": 2018}