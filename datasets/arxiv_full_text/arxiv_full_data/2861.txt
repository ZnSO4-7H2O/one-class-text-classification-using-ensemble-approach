{"title": "Sever: A Robust Meta-Algorithm for Stochastic Optimization", "tag": ["cs.LG", "cs.AI", "cs.DS", "stat.ML"], "abstract": "In high dimensions, most machine learning methods are brittle to even a small fraction of structured outliers. To address this, we introduce a new meta-algorithm that can take in a base learner such as least squares or stochastic gradient descent, and harden the learner to be resistant to outliers. Our method, Sever, possesses strong theoretical guarantees yet is also highly scalable -- beyond running the base learner itself, it only requires computing the top singular vector of a certain $n \\times d$ matrix. We apply Sever on a drug design dataset and a spam classification dataset, and find that in both cases it has substantially greater robustness than several baselines. On the spam dataset, with $1\\%$ corruptions, we achieved $7.4\\%$ test error, compared to $13.4\\%-20.5\\%$ for the baselines, and $3\\%$ error on the uncorrupted dataset. Similarly, on the drug design dataset, with $10\\%$ corruptions, we achieved $1.42$ mean-squared error test error, compared to $1.51$-$2.33$ for the baselines, and $1.23$ error on the uncorrupted dataset.", "text": "high dimensions machine learning methods brittle even small fraction structured outliers. address this introduce meta-algorithm take base learner least squares stochastic gradient descent harden learner resistant outliers. method sever possesses strong theoretical guarantees also highly scalable—beyond running base learner itself requires computing singular vector certain matrix. apply sever drug design dataset spam classiﬁcation dataset cases substantially greater robustness several baselines. spam dataset corruptions learning presence outliers ubiquitous challenge machine learning; nevertheless machine learning methods sensitive outliers high dimensions. focus work designing algorithms outlier robust remaining competitive terms accuracy running time. highlight motivating applications. ﬁrst biological data mislabeling measurement errors create systematic outliers require painstaking manual eﬀort remove detecting outliers settings often important either outlier observations interest might contaminate downstream statistical analysis. second motivation machine learning security outliers introduced data poisoning attacks adversary inserts fake data training recent work shown high-dimensional datasets even small fraction outliers substantially degrade learned model ∗supported award ccf- sloan research fellowship. †supported award ccf- ccf- ccf- n---. ‡supported award ccf- sloan research fellowship. §supported award ccf- ccf- google faculty research award figure illustration sever pipeline. ﬁrst machine learning algorithm model data. then extract gradients data point learned parameters take singular value decomposition gradients. compute outlier score data point. detect outliers remove re-run learning algorithm; otherwise output learned parameters. crucially biological security settings above outliers random instead highly correlated could complex internal structure diﬃcult model. leads following conceptual question underlying present work design training algorithms robust presence ε-fraction arbitrary outliers? estimation presence outliers prototypical goal robust statistics systematically studied since pioneering work tukey nevertheless recently known eﬃcient estimators suﬀered large errors presence small fraction outliers even basic highdimensional tasks common machine learning. recent work theoretical computer science resulted eﬃcient robust estimators classical problems linear classiﬁcation mean covariance estimation clustering regression despite recent algorithmic progress promise eﬃcient high-dimensional robust estimation realized; indeed aforementioned results generally suﬀer shortcomings—either sophisticated convex optimization algorithms scale large datasets tailored speciﬁc problems interest speciﬁc distributional assumptions data hence good accuracy real data. work address shortcomings. propose algorithm sever robust handle arbitrary outliers small increase error even high dimensions. general applied common learning problems including regression classiﬁcation high level algorithm simple plugin outlier detector—ﬁrst whatever learning procedure would normally then consider matrix gradients optimal parameters compute singular vector matrix. finally remove points whose projection onto singular vector large despite simplicity algorithm possesses strong theoretical guarantees long data heavy-tailed sever provably robust outliers—see section detailed statements theory. time show algorithm works well practice outperforms number natural baseline outlier detectors. line original motivating biological security applications implement method tasks—a linear regression task predicting protein activity levels spam classiﬁcation task based e-mails enron corporation even small fraction outliers baseline methods perform extremely poorly datasets; instance enron spam dataset fraction outliers baseline errors range sever incurs error similarly drug design context supervised learning gave ﬁrst eﬃcient algorithms robustly learning origincentered linear separators respect loss assumption distribution good data isotropic log-concave. subsequently obtained improved robust algorithm achieves nearly tight error bounds learning problem. context unsupervised learning concurrent works gave ﬁrst provably robust eﬃciently computable estimators several basic high-dimensional unsupervised tasks including robust mean covariance estimation. since dissemination ﬂurry research activity algorithmic robust estimation variety high-dimensional settings including learning graphical models understanding computation-robustness tradeoﬀs establishing connections learning tolerating noise outputting list hypotheses robust estimation discrete structures robust estimation sum-of-squares despite recent theoretical progress prior works focus designing specialized algorithms speciﬁc settings rather designing general algorithms. exception provides robust meta-algorithm stochastic convex optimization similar setting ours. however algorithm requires solving large semideﬁnite program incurs signiﬁcant loss performance relative standard training even absence outliers. hand provide practical implementation robust mean covariance estimation algorithms clear extend approach broader class learning tasks. concrete settings regression classiﬁcation considered experiments substantial number related works. number papers proposed eﬃcient algorithms type robust linear regression. however works consider restrictive corruption model allows adversarial corruptions responses hand studies linear regression broadly generalized linear models under robustness model similar considered here. main issues algorithm requires running ellipsoid method crucially assumes gaussianity covariates unlikely hold practice. related direction study outlier detectors linear classiﬁcation provide method either numerically certifying given outlier detector imparts robustness learning algorithm providing attack learner robust. show outlier detector consider brittle high dimensions motivating need robust algorithms presented current work. concurrent works. contemporaneous work independent work obtained robust algorithm stochastic convex optimization combining gradient descent robust mean estimation. case linear regression provide eﬃcient robust algorithms near-optimal error guarantees various distributional assumptions establish matching computational-robustness tradeoﬀs. formal setting consider stochastic optimization tasks true distribution functions goal parameter vector minimizing def= ef∼p∗ assume space possible parameters. example consider linear regression deﬁnition given distribution functions data generated follows ﬁrst clean samples drawn then adversary allowed inspect samples replace arbitrary samples. resulting points given algorithm. call samples ε-corrupted ε-contamination model adversary allowed remove points. theoretical results hold strong robustness model. experimental evaluation uses corrupted instances adversary allowed corrupted points. additive corruptions essentially correspond huber’s contamination model robust statistics. essentially deﬁnition means value cannot decreased much changing input locally staying within domain. condition enforces moving direction noted outlined figure algorithm works post-processing gradients black-box learning algorithm. basic intuition follows want ensure outliers large eﬀect learned parameters. intuitively outliers eﬀect corresponding gradients large magnitude systematically pointing speciﬁc direction. detect singular value decomposition–if hold outliers responsible large singular value matrix gradients allows detect remove them. theoretical guarantees. ﬁrst theoretical result says long data heavy-tailed sever approximate critical point true function even presence outliers. theorem suppose functions bounded closed domain suppose satisfy following deterministic regularity conditions exists igood |igood| take-away theorem error guarantee dependence underlying dimension contrast natural algorithms incur error grows hence poor robustness high dimensions. proposition closed bounded diameter distribution functions ef∼p∗ suppose unit vector ef∼p∗ appropriate lipschitz smoothness assumptions high ω)/) ε-corrupted functions drawn i.i.d. probability satisfy conditions theorem general holds even non-convex loss functions might general hope approximate critical point. particular convex problems guarantee approximate global minimum. follows corollary theorem practical considerations. theory hold need randomized ﬁltering algorithm shown algorithm ﬁlter stopping condition line algorithm satisﬁed. however practice found following simpler algorithm worked well iteration simply remove fraction outliers according scores instead using speciﬁc stopping condition simply repeat ﬁlter iterations total. version sever experiments section concrete applications also provide several concrete applications general theorem particularly involved optimization problems related learning generalized linear models. setting given pairs set. tries vector minimizes appropriate loss function example standard hinge-loss max). similarly logistic loss function log)). cases show approximate minimizer empirical loss theorem distribution ×{±} many values near hyperplane. ε-corrupted samples either hinge loss logistic loss function. exists polynomial time algorithm probability returns vector minimizes e∼dxy additive error. theorem distribution independent mean variance assume furthermore bounded fourth moments. exists algorithm given ε-corrupted samples computes value high probability problem well-understood absence corruptions mild assumptions take suﬃciently many samples average approximates pointwise high probability. hence standard methods convex optimization approximate minimizer turn serve approximate minimizer robust setting stochastic optimization becomes quite challenging even basic special cases problem single adversarially corrupted sample substantially change location minimum moreover naive outlier removal methods tolerate negligible fraction corruptions ﬁrst idea around obstacle following consider standard gradient descent method used minimum algorithm would proceed repeatedly computing gradient appropriate points using update current location. issue adversarial corruptions completely compromise algorithm’s behavior since substantially change gradient chosen points. observation approximating gradient given point given access ε-corrupted samples viewed robust mean estimation problem. thus robust mean estimation algorithm succeeds fairly mild assumptions good samples. assuming covariance matrix bounded summary ﬁrst algorithmic idea robust mean estimation routine black-box order robustly estimate gradient iteration gradient descent. yields simple robust method stochastic optimization polynomial sample complexity running time general setting ready describe sever main insight behind roughly speaking sever calls robust mean estimation routine outlier removal) time algorithm reaches approximate critical point main motivations approach first empirically observed iteratively ﬁlter samples keeping subset samples removed iterations ﬁlter remove points. second iteration ﬁlter subroutine expensive iteration gradient descent. therefore advantageous many steps gradient descent current corrupted samples consecutive ﬁltering steps. idea improved using stochastic gradient descent rather computing average step. important feature analysis sever robust mean estimation routine black box. contrast take advantage performance guarantees ﬁltering algorithm. main idea analysis follows suppose reached approximate critical point step apply ﬁltering algorithm. performance guarantees latter algorithm cases either ﬁltering algorithm removes corrupted functions certiﬁes gradient close gradient ﬁrst case make progress produce cleaner functions. second case certiﬁcation implies point also approximate critical point done. section apply sever regression classiﬁcation problems. base learners used ridge regression respectively. implemented latter quadratic program using gurobi backend solver yalmip modeling language. cases base learner extracted gradients data point learned parameters. centered gradients matlab’s svds method compute singular vector removed fraction points largest outlier score computed squared magnitude projection onto repeated iterations total. classiﬁcation centered gradients separately class improved performance. nodefense points removed. remove points covariate large distance mean. loss remove points large loss gradient remove points large gradient -norm). gradientcentered remove points whose gradients mean gradient -norm. note gradientcentered similar method except removes large gradients terms -norm rather terms projection onto singular vector. before classiﬁcation compute metrics separately class. ridge regression single hyperparameter optimized based uncorrupted data kept ﬁxed throughout experiments. addition since data already outliers added varying amounts outliers process described detail below. test points. drug discovery dataset obtained chembl database originally curated consists data points dimensions; split training points test points. centering found centering data points decreased error noticeably drug discovery dataset scaling coordinate variance decreased error small amount synthetic data. center presence outliers used robust mean estimation algorithm adding outliers. devised method generating outliers fools baselines still inducing high test error. high level outliers cause ridge regression output check unique minimizer ridge regression perturbed dataset. tuning obtain attacks fool baselines damaging model increase error also found useful perturb individual xbad small amount gaussian noise. figure test error baselines sever synthetic data drug discovery dataset. left middle ﬁgures show sever continues maintain statistical accuracy attacks able defeat previous baselines. right ﬁgure shows attack parameters chosen increase test error sever drug discovery dataset much possible. despite this sever still relatively small test error. figure representative histograms scores baselines sever synthetic data drug discovery dataset. left right scores defense drug discovery dataset scores loss synthetic data scores sever drug discovery dataset addition outliers. scores true dataset blue scores outliers red. baselines scores outliers inside bulk distribution thus hard detect whereas scores outliers assigned sever clearly within tail distribution easily detectable. results. figure compare test error defense baselines increase fraction added outliers. avoid cluttering ﬁgure show performance loss gradientcentered sever; performance remaining baselines qualitatively similar baselines figure baselines algorithms iterate defense times time removing fraction points largest score. consistency results defense value defense times fresh attack points display median test errors. understand baselines fail detect outliers figure show representative sample histograms scores uncorrupted points overlaid scores outliers synthetic data drug discovery dataset base learner. scores outliers well within distribution scores uncorrupted points. thus would impossible baselines remove without also removing large fraction uncorrupted points. interestingly small methods improve upon uncorrupted test error drug discovery data; appears presence small number natural outliers data methods successfully remove. next describe experimental results svms; tested method synthetic gaussian dataset well spam classiﬁcation task. similarly before synthetic data consists observations independent standard gaussian entries signxi .zi) also gaussian true parameters spam dataset comes enron corpus consists training points test points dimensions. generate attacks used data poisoning algorithm presented authors provided improved version algorithm circumvent loss baselines partially circumvents gradient baselines well. highest-scoring points iterations number positive negative training points respectively. expression chosen order account class imbalance extreme case enron dataset attacker plants outliers smaller class smaller value would remove points even perfect detection method. figure versus test error loss baseline sever synthetic data. left ﬁgure demonstrates sever accurate outliers manage defeat loss. right ﬁgure shows result attacks increased test error sever. even case sever performs much better baselines. synthetic results. considered fractions outliers ranging performing sweep across hyperparameters attack generated distinct sets attacks value figure show results attack loss baselines worst well attack method worst. attacks eﬀective loss sever substantially outperforms nearly matching test accuracy uncorrupted data loss performs worse error fraction injected outliers. even attacks eﬀective sever still outperforms loss achieving test error note baselines behaved qualitatively similarly loss results displayed section spam results. results enron used values considered distinct hyperparameters attack. single attack simultaneously defeated baselines figure show attacks well diﬀerent sets baselines well attack performs best method. investigate further looked attacks found attacks error never exceeded attacks error substantially higher. figure shows happening. leftmost ﬁgure displays scores assigned sever figure versus test error baselines sever enron spam corpus. left middle ﬁgures attacks perform best baselines right ﬁgure performs best sever. though baselines perform well certain cases sever consistently accurate. exception certain attacks which shown figure require three rounds outlier removal method obtain reasonable test error plots defenses perform rounds. figure illustration multiple rounds ﬁltering necessary. histograms scores assigned sever three subsequent iterations outlier removal. inliers blue outliers early iterations signiﬁcant fraction outliers hidden correctly classiﬁed iteration. however previous outliers removed points become incorrectly classiﬁed thus signiﬁcantly degrading quality solution simultaneously becoming evident sever. ﬁrst iteration bars indicate outliers. outliers assigned extremely large scores thus detected several outliers correctly classiﬁed thus gradient. however remove ﬁrst outliers outliers previously correctly classiﬁed large score displayed middle ﬁgure. another iteration process produces rightmost ﬁgure almost remaining outliers large score thus removed sever. demonstrates outliers hidden outliers removed necessitating multiple iterations. paper presented algorithm sever strong theoretical robustness properties presence outliers performs well real datasets. sever based idea learning often cast problem ﬁnding approximate stationary point loss turn cast robust mean estimation problem allowing leverage existing techniques eﬃcient robust mean estimation. number directions along sever could improved ﬁrst could extended handle general assumptions data; second could strengthened achieve better error bounds terms fraction outliers; ﬁnally could imagine automatically learning feature representation sever performs well. discuss ideas detail below. general assumptions. main underlying assumption sever rests singular value gradients data small. appeared hold true datasets considered common occurence practice large singular values together many small singular values. would therefore desirable design version sever take advantage phenomena. addition would worthwhile detailed empirical analysis across wide variety datasets investigating properties enable robust estimation could provide template ﬁnding properties). dependence error fraction stronger robustness outliers. outliers without stronger assumptions likely possible improve practice would prefer dependence closer therefore would also useful improve sever o-dependence stronger realistic assumptions. unfortunately existing algorithms robust mean estimation achieve error better either rely strong distributional assumptions gaussianity else require expensive computation involving e.g. sum-of-squares optimization improving robustness sever thus requires improvements robust mean estimation algorithm sever uses primitive. learning favorable representation. finally note sever performs best features small covariance strong predictive power. situation particular holds many approximately independent features predictive true signal. would interesting learn representation property. could done instance training neural network cost function encourages independent features issue learn representation robustly; idea learn representation dataset known free outliers hope representation useful datasets application domain. beyond speciﬁc questions view general investigation robust methods important step machine learning moves forwards. indeed machine learning applied increasingly many situations increasingly automated ways important attend robustness considerations machine learning systems behave reliably avoid costly errors. bulk recent work highlighted vulnerabilities machine learning optimistic practical algorithms backed principled theory ﬁnally patch vulnerabilities lead truly reliable systems.", "year": 2018}