{"title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions:  Towards Unifying Bayesian Optimization, Active Learning, and Beyond", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.RO"], "abstract": "This paper presents a novel nonmyopic adaptive Gaussian process planning (GPP) framework endowed with a general class of Lipschitz continuous reward functions that can unify some active learning/sensing and Bayesian optimization criteria and offer practitioners some flexibility to specify their desired choices for defining new tasks/problems. In particular, it utilizes a principled Bayesian sequential decision problem framework for jointly and naturally optimizing the exploration-exploitation trade-off. In general, the resulting induced GPP policy cannot be derived exactly due to an uncountable set of candidate observations. A key contribution of our work here thus lies in exploiting the Lipschitz continuity of the reward functions to solve for a nonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real time, we further propose an asymptotically optimal, branch-and-bound anytime variant of epsilon-GPP with performance guarantee. We empirically demonstrate the effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian optimization and an energy harvesting task.", "text": "paper presents novel nonmyopic adaptive gaussian process planning framework endowed general class lipschitz continuous reward functions unify active learning/sensing bayesian optimization criteria offer practitioners ﬂexibility specify desired choices deﬁning tasks/problems. particular utilizes principled bayesian sequential decision problem framework jointly naturally optimizing exploration-exploitation trade-off. general resulting induced policy cannot derived exactly uncountable candidate observations. contribution work thus lies exploiting lipschitz continuity reward functions solve nonmyopic adaptive \u0001-optimal policy. plan real time propose asymptotically optimal branch-and-bound anytime variant \u0001-gpp performance guarantee. empirically demonstrate effectiveness \u0001-gpp policy anytime variant bayesian optimization energy harvesting task. fundamental challenge integrated planning learning design autonomous agent plan actions maximize expected total rewards interacting unknown task environment. recent research efforts tackling challenge progressed simple markov models assuming discrete-valued independent observations rich class bayesian nonparametric gaussian process models characterizing continuous-valued correlated observations order represent latent structure complex possibly noisy task environments higher ﬁdelity. challenge posed following important problems machine learning among others active learning/sensing context environmental sensing trafﬁc sensing objective select informative observations predicting spatially varying environmental ﬁeld modeled subject sampling budget constraints rewards agent deﬁned based formal measure predictive uncertainty entropy mutual information criterion. resolve issue sub-optimality faced greedy algorithms recent developments made nonmyopic computationally tractable provable performance guarantees further investigated performance advantage adaptivity proposing nonmyopic adaptive observation selection policies depend past observations. bayesian optimization objective select gather informative observations ﬁnding global maximum unknown highly complex objective function modeled given sampling budget rewards agent deﬁned using improvement-based expected improvement currently found maximum) entropybased upper conﬁdence bound acquisition function limitation algorithms myopic. overcome limitation approximation algorithms nonmyopic adaptive proposed performances theoretically guaranteed. general tasks/problems. practice types rewards need speciﬁed agent plan operate effectively given realworld task environment modeled detailed section shall elucidated later similarities structure problems motivate consider whether possible tackle overall challenge devising nonmyopic adaptive planning framework general class reward functions unifying criteria affording practitioners ﬂexibility specify desired choices deﬁning tasks/problems. integrated planning learning framework address exploration-exploitation trade-off common problems agent faces dilemma gathering observations maximize expected total rewards given current possibly imprecise belief task environment improve belief learn environment paper presents novel nonmyopic adaptive gaussian process planning framework endowed general class lipschitz continuous reward functions unify criteria discussed earlier offer practitioners ﬂexibility specify desired choices deﬁning tasks/problems particular utilizes principled bayesian sequential decision problem framework jointly naturally optimizing exploration-exploitation trade-off consequently allowing planning learning integrated seamlessly performed simultaneously instead separately general resulting induced policy cannot derived exactly uncountable candidate observations. contribution work thus lies exploiting lipschitz continuity reward functions solve nonmyopic adaptive \u0001-optimal policy given arbitrarily userspeciﬁed loss bound plan real time propose asymptotically optimal branch-andbound anytime variant \u0001-gpp performance guarantee. finally empirically evaluate performances \u0001-gpp policy anytime variant energy harvesting task simulated real-world environmental ﬁelds ease exposition rest paper described assuming task environment environmental ﬁeld agent mobile robot coincide setup experiments. notations preliminaries. domain environmental ﬁeld corresponding sampling locations. time step robot deterministically move previous location visit location observes taking corresponding realized ﬁeld measurement denotes ﬁnite sampling locations reachable previous location single time step. state robot initial starting location represented prior observations/data available planning denote respectively vectors comprising locations visited/observed corresponding ﬁeld measurements taken robot prior planning last component similarly time step state robot current location represented observations/data denote respectively vectors comprising locations visited/observed corresponding ﬁeld measurements taken robot time step denotes vector concatenation. time step robot also receives reward deﬁned later. modeling environmental fields gaussian processes used model spatially varying environmental ﬁeld follows ﬁeld assumed realization location associated latent ﬁeld measurement {ys}s∈s denote every ﬁnite subset multivariate gaussian distribution then fully speciﬁed prior mean covariance cov] latter characterizes spatial correlation structure environment ﬁeld deﬁned using covariance function. common choice squared exponential covariance function signal variance controlling intensity measurements diagonal matrix length-scale components governing degree spatial correlation similarity measurements respective horizontal vertical directions ﬁelds experiments. ﬁeld measurements taken robot assumed corrupted gaussian white noise i.e. time steps model perform probabilistic regression using predict noisy measurement unobserved location well provide predictive uncertainty using gaussian predictive distribution following posterior mean variance respectively vector mean components every location σst+st vector covariance components kst+s every location σstst+ transpose σst+st γstst σstst covariance matrix components every pair locations important property model that unlike µst+|dt independent problem formulation. frame nonmyopic adaptive gaussian process planning bayesian sequential decision problem adaptive policy deﬁned sequentially decide next location observed time step using observations ﬁnite planning horizon time steps/stages value adaptive policy deﬁned expected total rewards achieved selected observations starting prior observations following thereafter computed using following h-stage bellman equations possible expected total rewards respect possible induced sequences future gaussian posterior beliefs discussed next. formally involves choosing adaptive policy call policy maximize plugging maxst+∈a stages policy jointly naturally optimizes exploration-exploitation trade-off selected location maxst+∈a time step affects immediate expected reward e)|dt given current belief well gaussian posterior belief π∗st zt+)) next time step latter inﬂuences expected future rewards usually cannot evaluated closed form uncountable candidate measurements except degenerate cases like independent overcome difﬁculty show section later lipschitz continuity reward functions exploited theoretically guaranteeing performance proposed nonmyopic adaptive \u0001-optimal policy expected total rewards achieved selected observations closely approximates within arbitrarily user-speciﬁed loss bound lipschitz continuous reward functions. r+r+r user-deﬁned reward functions satisfy conditions below lipschitz continuous lipschitz constant lipschitz continuous denotes convolution; deﬁne well-deﬁned evaluated closed form computed arbitrary precision reasonable time lipschitz continuous lipschitz constant depends locations visited/observed robot time step independent realized measurement used represent sampling motion costs explicitly consider exploration deﬁning function reward function independent measurements past states and/or exploiting maximum likelihood observations planning provable performance guarantee. proof appendix lemma used prove lipschitz continuity later. this consider lipschitz continuous reward functions deﬁned unify criteria discussed section used deﬁning tasks/problems. active learning/sensing setting yields well-known plans/decides locations maximum entropy observed minimize posterior entropy remaining unobserved areas ﬁeld. since independent expectations away thus making non-adaptive hence straightforward search algorithm plagued issue uncountable candidate measurements. such focus degenerate case. degeneracy vanishes environment ﬁeld instead realization log-gaussian process. then becomes adaptive reward function represented lipschitz continuous reward functions setting gσst+|st identity functions log. greedy algorithm srinivas utilizes selection criterion µst+|dt βσst+|st approximately optimize global objective total ﬁeld measurements taken robot equivalently minimize total regret. represented lipschitz continuous reward functions setting gσst+|st identity functions βσst+|st e|dt st+] µst+|dt βσst+|st. particular derived policy maximizes expected total ﬁeld measurements taken robot hence optimizing exact global objective srinivas expected sense. unlike greedy nonmyopic framework explicitly consider additional weighted exploration term reward function jointly naturally optimize exploration-exploitation trade-off explained earlier. nevertheless stronger exploration behavior desired ﬁne-tuned. different nonmyopic algorithm marchant ramos sanner using ucb-based rewards proposed nonmyopic \u0001-optimal policy need impose extreme assumption maximum likelihood observations planning importantly provides performance guarantee including extreme assumption made nonmyopic ucb. framework differs nonmyopic algorithm osborne garnett roberts every selected observation contributes total ﬁeld measurements taken robot instead considering expected improvement last observation. usually expend given sampling budget global maximum. general tasks/problems. practice necessary reward function complex ones speciﬁed formed identity function ﬁeld measurement. example consider problem placing wind turbines optimal locations maximize total power production. though average wind speed region modeled power output linear function steady-state wind speed. fact power production requires certain minimum speed known cutspeed. threshold power output increases eventually plateaus. assuming cut-in speed effect modeled logarithmic reward function gives value otherwise best knowledge hσst+|st closed-form expression. appendix present interesting reward functions like unit step function gaussian distribution represented used real-world tasks. lipschitz continuous reward functions lipschitz continuous lipschitz constant deﬁned below deﬁnition dekey idea constructing proposed nonmyopic adaptive \u0001-gpp policy approximate expectation terms every stage using form deterministic sampling illustrated ﬁgure below. speciﬁcally measurement space ﬁrst partitioned intervals intervals equally spaced within bounded gray region speciﬁed user-deﬁned width parameter intervals span inﬁnitely long tails. note requires partition valid. sample measurements selected setting upper limit interval lower limit interval centers respective gray intervals ζn−. next weights corresponding sample measurements deﬁned areas respective intervals gaussian predictive distribution example partition given appendix selected sample measurements corresponding weights exploited approximating lipschitz continuous reward functions using following h-stage bellman equations stages resulting induced \u0001-gpp policy jointly naturally optimizes exploration-exploitation trade-off similar manner policy explained section interesting note setting yields µst+|dt equivalent selecting single sample measurement µst+|dt corresponding weight identical special case maximum likelihood observations planning extreme assumption used nonmyopic sampling gain time efﬁciency. performance guarantee. difﬁculty theoretically guaranteeing performance \u0001-gpp policy lies analyzing values width parameter deterministic sampling size chosen satisfy user-speciﬁed loss bound discussed below. ﬁrst step prove approximates closely chosen values relies lipschitz continuity value standard normal cdf. theorem suppose given. expand tree. construct minimal sub-tree rooted node sampling possible next locations median sample measurements recursively full height backpropagation. backpropagate bounds leaves newly constructed sub-tree node reﬁned bounds expanded nodes used inform bounds unexpanded siblings exploiting lipschitz continuity explained appendix backpropagate bounds root partially constructed tree similar manner. section empirically evaluates online planning performance time efﬁciency \u0001-gpp policy anytime variant limited sampling budget energy harvesting task simulated wind speed ﬁeld simulated plankton density ﬁeld real-world logpotassium concentration ﬁeld broom’s barn farm simulated ﬁeld spatially distributed region discretized grid sampling locations. ﬁelds assumed realizations gps. wind speed ﬁeld simulated using hyperparameters hyperparameters lg-k ﬁeld learned using maximum likelihood estimation robot’s initial starting location near center simulated ﬁeld randomly selected lg-k ﬁeld. move adjacent grid locations time step tasked maximize total rewards time steps performances \u0001-gpp policy anytime variant compared state-of-the-art nonmyopic greedy three performance metrics used total rewards achieved evolved time steps maximum reward achieved experiment search tree size terms nodes experiments linux machine intel core ghz. energy harvesting task simulated wind speed field. robotic rover equipped wind turbine tasked harvest energy/power wind exploring polar region driven logarithmic reward function described ‘general tasks/problems’ section fig. shows results performances \u0001-gpp policy anytime variant averaged independent realizations wind speed ﬁeld. observed pling budget locations increasing deterministic sampling size increasing reduces hence allows reduced well. width parameter mixed effect error bound note proportional upper bound error incurred extreme sample measurements shown appendix increasing reduces unfortunately raises order reduce increasing complemented raising fast enough keep increasing. allows reduced well. remark feasible choice satisfying expressed analytically terms given hence computed prior planning shown appendix remark σst+|st computed prior planning depend reachable locations measurements. using theorem next step bound performance loss \u0001-gpp policy relative policy policy \u0001-optimal theorem given user-speciﬁed loss bound substituting choice stated remark above. proof appendix observed theorem tighter bound error achieved decreasing sampling budget locations increasing deterministic sampling size effect width parameter error bound error bound explained remark above. anytime \u0001-gpp. unlike policy \u0001-gpp policy derived exactly since incurred time independent size uncountable candidate measurements. however expanding entire search tree \u0001-gpp incurs time containing term always necessary achieve \u0001-optimality practice. mitigate computational difﬁculty propose anytime variant \u0001-gpp produce good policy fast improve approximation quality time brieﬂy discussed detailed pseudocode appendix intuition expand sub-trees rooted promising nodes highest weighted uncertainty corresponding values improve estimates. represent uncertainty encountered node upper lower heuristic bounds maintained like partial construction entire tree maintained expanded incrementally iteration anytime \u0001gpp incurs linear time comprises steps node selection. traverse partially constructed tree repeatedly selecting nodes largest difference between upper lower bounds discounted weight preceding sample measurement figure graphs rewards tree size \u0001-gpp policies online planning horizon varying varying time steps energy harvesting task. plot uses anytime variant maximum tree size nodes plot effectively assumes maximum likelihood observations planning like nonmyopic gradients achieved total rewards increase time indicate higher obtained reward increasing number time steps robot exploit environment effectively exploration previous time steps. gradients eventually stop increasing robot enters perceived high-reward region. exploration deemed unnecessary unlikely another preferable location within time steps; robot remains near-stationary remaining time steps. also observed incurred time much higher ﬁrst time steps. expected posterior variance σst+|st decreases increasing time step thus requiring decreasing deterministic sampling size satisfy initially \u0001-gpp policies achieve similar total rewards robots begin starting location. time \u0001-gpp policies lower user-speciﬁed loss bound longer online planning horizon achieve considerably higher total rewards cost incurred time. particular observed robot assuming maximum likelihood observations planning like nonmyopic using greedy policy performs poorly quickly. former case gradient total rewards stops increasing quite early indicates perceived local maximum reached prematurely. interestingly observed fig. \u0001-gpp policy incurs time despite latter achieving higher total rewards. suggests trading tighter loss bound longer online planning horizon especially small turn requires large consequently incurs signiﬁcantly time. real-world log-potassium concentration field. agricultural robot tasked peak lg-k measurement exploring broom’s barn farm driven ucb-based reward function described ‘bo’ section fig. shows results performances \u0001-gpp policy anytime variant nonmyopic greedy averfigure graphs total normalized rewards \u0001-gpp policies using ucb-based rewards varying varying varying time steps real-world ﬁeld. plot uses anytime variant maximum tree size nodes plot effectively assumes maximum likelihood observations during planning like nonmyopic ucb. aged randomly selected robot’s initial starting location. observed figs. gradients achieved total normalized rewards generally increase time. particular fig. nonmyopic assuming maximum likelihood observations during planning obtains much less total rewards \u0001gpp policies anytime variant time steps ﬁnds maximum lg-k measurement least worse time steps. performance anytime variant comparable best-performing \u0001gpp policy fig. greedy policy performs much poorly nonmyopic \u0001-gpp counterparts ﬁnds maximum lg-k measurement lower greedy lack exploration. increasing \u0001-gpp policies outperform greedy naturally jointly optimize explorationexploitation trade-off. interestingly fig. shows \u0001-gpp policy achieves highest total rewards time steps indicates need slightly stronger exploration behavior explained small length-scale lg-k ﬁeld thus requiring exploration peak measurement. increasing beyond larger spatial correlation expect diminishing role βσst+|st term. also observed aggressive exploration hurts performance. results tree size \u0001-gpp policy anytime variant appendix paper describes novel nonmyopic adaptive \u0001-gpp framework endowed general class lipschitz continuous reward functions unify criteria used deﬁning tasks/problems. particular jointly naturally optimize explorationexploitation trade-off. theoretically guarantee performances \u0001-gpp policy anytime variant empirically demonstrate effectiveness energy harvesting task. future work plan scale \u0001-gpp anytime variant data using parallelization online learning stochastic variational inference extend handle unknown hyperparameters acknowledgments. work supported singaporemit alliance research technology subaward agreement r----.", "year": 2015}