{"title": "Implicit Discourse Relation Classification via Multi-Task Neural  Networks", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Without discourse connectives, classifying implicit discourse relations is a challenging task and a bottleneck for building a practical discourse parser. Previous research usually makes use of one kind of discourse framework such as PDTB or RST to improve the classification performance on discourse relations. Actually, under different discourse annotation frameworks, there exist multiple corpora which have internal connections. To exploit the combination of different discourse corpora, we design related discourse classification tasks specific to a corpus, and propose a novel Convolutional Neural Network embedded multi-task learning system to synthesize these tasks by learning both unique and shared representations for each task. The experimental results on the PDTB implicit discourse relation classification task demonstrate that our model achieves significant gains over baseline systems.", "text": "yang sujian xiaodong zhang zhifang laboratory computational linguistics peking university china collaborative innovation center language ability xuzhou jiangsu china pairs occurring sentence pairs useful since extent represent semantic relationships sentences earlier studies researchers word pairs help classifying discourse relations. however strange useful word pairs composed stopwords. rutherford point counter-intuitive phenomenon caused sparsity nature word pairs. employ brown clusters alternative abstract word representation result intuitive cluster pairs achieve better performance. another problem discourse parsing coexistence different discourse annotation frameworks different kinds corpora tasks created. wellknown discourse corpora include penn discourse treebank rhetorical structure theory discourse treebank annotation complexity size corpus large enough. further corpora different annotation frameworks usually used separately discourse relation classiﬁcation also main reason sparsity discourse relation classiﬁcation. however different annotation frameworks strong internal connections. example elaboration joint relations rst-dt similar sense expansion relation pdtb. based this consider design multiple discourse analysis tasks according frameworks synthesize tasks goal classifying implicit discourse relations ﬁnding precise representations sentence pairs. inspired work done regard implicit explicit relation classiﬁcation pdtb framework tasks design multi-task learning method obtain higher performance. paper propose general multi-task learning system implicit discourse relation classiﬁcation synthesizing discourse analysis tasks within different corpora. represent sentence pairs construct convolutional neural networks derive vector representations dimensional latent space replacing sparse lexical features. combine different discourse analysis tasks embed cnns multi-task neural network learn unique shared reprewithout discourse connectives classifying implicit discourse relations challenging task bottleneck building practical discourse parser. previous research usually makes kind discourse framework pdtb improve classiﬁcation performance discourse relations. actually different discourse annotation frameworks exist multiple corpora internal connections. exploit combination different discourse corpora design related discourse classiﬁcation tasks speciﬁc corpus propose novel convolutional neural network embedded multi-task learning system synthesize tasks learning unique shared representations task. experimental results pdtb implicit discourse relation classiﬁcation task demonstrate model achieves significant gains baseline systems. discourse relations support sentences form coherent text. automatically identifying discourse relations help many downstream tasks question answering automatic summarization. certain circumstances relations form explicit markers like because relatively easy identify. prior work shows explicit markers exist relation types disambiguated scores higher however without explicit marker rely classifying implicit discourse relations much difﬁcult. fact implicit relations outnumber explicit ones naturally occurring text makes classiﬁcation types challenge discourse analysis. major line research work approaches implicit relation classiﬁcation problem extracting informed features corpus designing machine learning algorithms obvious challenge classifying discourse relations features appropriate representing sentence pairs. intuitively word copyright association advancement artiﬁcial intelligence rights reserved. sentations sentence pairs different tasks reﬂect differences connections among tasks. multi-task neural network multiple discourse tasks trained simultaneously optimize connections. stated above improve implicit discourse relation classiﬁcation make full combination different discourse corpora. work choose three kinds discourse corpora pdtb rst-dt natural text discourse connective words. section brieﬂy introduce corpora. pdtb penn discourse treebank known largest discourse corpus composed wall street journal articles. pdtb adopts predicate-argument structure predicate discourse connective arguments text spans around connective. pdtb relation explicit explicit discourse connective presented text; otherwise implicit. pdtb relations hierarchically organized top-level classes expansion comparison contingency temporal further divided types subtypes. work mainly experiment top-level classes previous work rst-dt rst-dt based rhetorical structure theory proposed composed articles. corpus text represented discourse tree whose leaves non-overlapping text spans called elementary discourse units since mainly focus discourse relation classiﬁcation make discourse dependency structure converted tree structures extracted pairs labeled rhetorical relations them. rst-dt relations classiﬁed classes. choose highest frequent classes relations. text connective words exists large amount text connective words know connective words serve natural means connect text spans. thus text connective words somewhat similar explicit discourse relations pdtb without expert judgment also used special discourse corpus. work adopt york times corpus million news articles. extract sentence pairs around commonly-used connective words generate discourse corpus relations removing connective words. corpus veriﬁed human contains noise since connective words reﬂect discourse relations connective words different meanings different contexts. however still help training better model certain scale instances. motivation overview different discourse corpora closely related though under different annotation theories. table list instances similar discourse relations nature annotated differently different corpora. second belongs elaboration relation rst-dt. third fourth expansion relations pdtb implicit explicit connective particular. ﬁfth corpus directly uses word particularly denote discourse relation sentences. instances reﬂect similar discourse relation second argument gives details ﬁrst argument. intuitive classiﬁcation performance instances boosted appropriately synthesize them. idea propose adopt multi-task learning method design speciﬁc discourse analysis task corpus. according principle multi-task learning related tasks powerful multi-task learning method based this design four discourse relation classiﬁcation tasks. task implicit pdtb discourse relation classiﬁcation task explicit pdtb discourse relation classiﬁcation task rst-dt discourse relation classiﬁcation task connective word classiﬁcation ﬁrst tasks classify relation arguments pdtb framework. third task predict relations edus using processed rst-dt corpus. last designed predict correct connective word sentence pair using corpus. deﬁne task classifying implicit pdtb relations main task tasks auxiliary tasks. means focus learning tasks improve performance main task system. noted call text spans tasks arguments convenience. next introduce tackle tasks. work propose convolutional neural networks representing argument pairs. then embed cnns multi-task neural network learn shared unique properties tasks. cnns modeling argument pairs figure illustrates proposed method modeling argument pairs. associate word vector representation usually pre-trained large unlabeled corpora. view argument sequence word vectors vector word argument then argument pair represented argument reserves amount total lessdeveloped-country exposure revenue rose billion billion particular slashing rate income taxation particularly show-offs butterﬂy weeds boneset baptisia generally xii+j relate concatenation word vectors xi+··· xi+j. convolution operation involves ﬁlter applied window words produce feature. speciﬁc task capturing relation arguments time take words arguments concatenate vectors apply convolution operation window pair. example feature generated window pair composed words jj+h− bias term non-linear function paper tanh. ﬁlter applied possible window pair arguments produce feature twodimensional matrix. since arguments different lengths operation called dynamic pooling capture salient features generating ﬁxedsize matrix rnp×np. order this matrix divided roughly equal parts. every maximal value rectangular window selected form grid. process matrix lose information compared original matrix however approach capture global structure. example upper left part constituted word pair features reﬂecting relationship beginnings arguments. property useful discourse parsing prior research pointed word position argument important identifying discourse relation. multiple ﬁlters like this argument pairs modeled three-dimensional tensor. ﬂatten vector rnp×np×nf representation argument pair number ﬁlters. multi-task learning kind machine learning approach trains main task auxiliary tasks simultaneously shared representation learning commonality among tasks. work embed convolutional neural networks multi-task learning system synthesize four tasks mentioned above. argument pairs different tasks low-dimensional vector representations proposed cnn. guarantee principle tasks optimize withbringing much noise task owns unique representation argument pairs meanwhile special shared representation connecting tasks. architecture multi-task learning system shown figure clarity diagram depicts tasks. aware number tasks limited two. acquiring several additional surface-level features proven useful bunch existing work notate feature vector task then concatenate name since tasks related classiﬁcation dimension output vector task predeﬁned class number next take input generate output vector tmax operation weight matrix bias network architecture various good properties. shared representation makes sure tasks effectively learn other. meanwhile multiple cnns modeling argument pairs give ﬂexibility assign different hyper-parameters task. example pdtb built sentences rst-dt elementary discourse units usually shorter sentences. under proposed framework assign larger window size pdtb related tasks smaller window size rst-dt related task better capturing discourse relations. additional features classifying discourse relations consider several surface-level features supplemental automatically generated representations. different features task considering speciﬁc properties. features include ﬁrst last words arguments production rules extracted constituent parse trees model training deﬁne ground-truth label vector instance task binary vector. instance belongs class i-th dimension dimensions mtnn model tasks classiﬁcation problems adopt cross entropy loss mini-batch stochastic gradient descent train parameters referring training procedure select task epoch update model according task-speciﬁc objective. avoid over-ﬁtting different learning rates train neural network parameters word embeddings denoted make tasks expect reach best performance roughly time. order achieve this assign different regulative ratio different tasks adjusting learning rates task update rules since main goal conduct implicit discourse relation classiﬁcation table summarizes statistics four top-level implicit discourse relations pdtb. follow setup previous studies splitting dataset training development test set. sections used train classiﬁers sections develop feature sets tune models section test systems. task explicit relations sections pdtb used. table shows distribution explicit relations four classes. task convert rstdt trees discourse dependency trees according direct relations edus task standford parser segment sentences. select frequent connectives pdtb extract instances containing connectives corpus based patterns manually compile rules remove noisy instances short arguments. finally obtain corpus instances random sampling. space limitation list frequent connective words corpus table model conﬁguration word embeddings provided glove dimension embeddings ﬁrst train four tasks separately roughly hyper-parameters. then carefully tune multi-task learning system based performance main task development set. learning rates task hyper-parameters including window size pooling size number ﬁlters dimension task-speciﬁc representation regulative ratios tasks share window size pooling size number ﬁlters learning shared representation denoted detailed settings shown table evaluation analysis mainly evaluate performance implicit pdtb relation classiﬁcation seen -way classiﬁcation task. relation class adopt commonly used metrics precision recall performance evaluation. evaluate whole system metrics accuracy macro-averaged analysis model first evaluate combination different tasks. table shows detailed results. relation ﬁrst conduct main task implementing model show results ﬁrst row. combine main task three auxiliary tasks results next three rows. ﬁnal gives performance using four tasks general synthesizing tasks system achieve best performance. speciﬁcally tasks different inﬂuence different discourse relations. task classiﬁcation explicit pdtb relations slight even negative impact relations except temporal relation. result consistent conclusion reported exists difference explicit implicit discourse relations corpus explicit relations deﬁnitely boost performance implicit ones. task classiﬁcation connective words besides similar effects observed greatly helpful identifying contingency relation. contingency covers wide range subtypes ﬁne-grained connective words corpus give hints identifying relation. contrary training task classifying rstdt relations result gets better comparison however improvement relations less obvious using tasks. possible reason deﬁnitions contrast comparison rstdt similar comparison pdtb tasks easily learn classes. importantly synthesizing tasks model except result comparison relation experiences slight deterioration classiﬁcation performance generally gets better. comparison systems compare general performance model state-of-the-art system terms accuracy macro-average table rutherford elaborately select combination various lexical features production rules brown cluster pairs feeding maximum entropy classiﬁer. also propose gather weakly labeled data based discourse connectives classiﬁer achieve state-of-the-art results -way classiﬁcation task. proposed system achieves higher performance accuracy macro-averaged also compare general performance system single-task learning system trained task result shows raises accuracy improvements signiﬁcant one-tailed t-test direct comparison previous results also conduct experiments based setting task four binary classiﬁers. results presented table three additional systems used baselines. park cardie design traditional featurebased method promote performance optimizing feature set. eisenstein used recursive neural networks syntactic parse tree induce representation arguments entity spans. zhou ﬁrst predict connective words unlabeled corpus predicted connectives features recognize discourse relations. results show multi-task learning system especially helpful classifying contingency temporary relation. increases performance temporary relation achieving substantial improvement. probably relation suffers lack training data learn better representations argument pairs help auxiliary tasks. comparison relation beneﬁts least mtl. previous work suggests relation relies syntactic information arguments. features captured upper layer model optimized multiple tasks. generally system achieves state-of-the-art performance three discourse relations supervised method often approaches discourse analysis classiﬁcation problem pairs sentences/arguments. ﬁrst work tackle task pdtb selected several surface features train four binary classiﬁers top-level pdtb relation classes. although features proved useful word pairs major contributor classiﬁers. interestingly found training features pdtb useful training external corpus. extending work identiﬁed four different feature types representing context constituent parse trees dependency parse trees text respectively. addition park cardie promoted performance optimizing feature set. recently mckeown biran tried tackle feature sparsity problem aggregating features. rutherford used brown cluster replace word pair features achieving state-of-the-art classiﬁcation performance. eisenstein used recursive neural networks represent arguments entity spans combination representations predict discourse relation. also exist semi-supervised approaches exploit labeled unlabeled data discourse relation classiﬁcation. hernault bollegala ishizuka proposed semi-supervised method exploit cooccurrence features unlabeled data. found method especially effective improving accuracy infrequent relation types. zhou presented method predict missing connective based language model trained unannotated corpus. predicted connective used feature classify implicit relation. interesting work done designed multi-task learning recent years neural network-based methods gained prominence ﬁeld natural language processing multi-task neural networks proposed. example collobert designed single sequence labeler multiple tasks part-of-speech tagging chunking named entity recognition. recently proposed representation learning algorithm based multi-task objectives successfully combining tasks query classiﬁcation search. previous studies implicit discourse relation classiﬁcation always face problems sparsity argument representation. solve problems propose different kinds corpus design multi-task neural network synthesize different corpus-speciﬁc discourse classiﬁcation tasks. mtnn model convolutional neural networks dynamic pooling developed model argument pairs. then different discourse classiﬁcation tasks derive unique shared representations argument pairs optimize without bringing useless noise. experiment results demonstrate system achieves state-ofthe-art performance. future work design system based syntactic tree enabling task share structural information. thank anonymous reviewers insightful comments paper. work partially supported national basic research program china national natural science foundation china correspondence author paper sujian", "year": 2016}