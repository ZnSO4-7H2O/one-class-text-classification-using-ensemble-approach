{"title": "Introduction to Nonnegative Matrix Factorization", "tag": ["cs.NA", "cs.CV", "cs.LG", "math.OC", "stat.ML"], "abstract": "In this paper, we introduce and provide a short overview of nonnegative matrix factorization (NMF). Several aspects of NMF are discussed, namely, the application in hyperspectral imaging, geometry and uniqueness of NMF solutions, complexity, algorithms, and its link with extended formulations of polyhedra. In order to put NMF into perspective, the more general problem class of constrained low-rank matrix approximation problems is first briefly introduced.", "text": "paper introduce provide short overview nonnegative matrix factorization several aspects discussed namely application hyperspectral imaging geometry uniqueness solutions complexity algorithms link extended formulations polyhedra. order perspective general problem class constrained low-rank matrix approximation problems ﬁrst brieﬂy introduced. constrained low-rank matrix approximation becoming popular able extract pertinent information large data sets; example recent survey clrma equivalent linear dimensionality reduction. given data points goal basis vectors correk= problem equivalent low-rank column data point column basis vector column provides coordinates corresponding column basis words column approximated linear combination columns leads principal component analysis solved using singular value decomposition surprisingly show optimization problem variables spurious local minima explains solved eﬃciently despite error nonconvex. note resulting problem reformulated semideﬁnite program using -k-norm constraints factors satisfy. constraints depend application hand allow meaningful interpretation factors. example k-means equivalent requiring factor single nonzero entry column equal columns cluster centroids. another widely used variant sparse requires factors sparse thus yielding compact easily interpretable decomposition imposing componentwise nonnegativity factors leads nonnegative matrix factorization example document analysis column corresponds document nonnegativity constraints allow interpret columns factor topics columns factor indicate proportion document discusses topic paper focus particular variant clrma. clrma problems heart many ﬁelds applied mathematics computer science including statistics data analysis machine learning data mining signal image processing graph theory numerical linear algebra systems theory control good news optimization community clrma models lead wide variety theoretical algorithmic challenges optimizers solve problems? conditions? appropriate model given application? algorithm situation? type guarantees provide? complexity. soon norm frobenius norm feasible domain constraints problem becomes diﬃcult cases. example wlra robust sparse hard active direction research developing approximation algorithms problems; example norm wlra componentwise matrix satisﬁes conditions depending model optimal solution guaranteed recover solution original problem; examples include missing data robust know approaches drawbacks. first input matrix satisfy required conditions often case practice unclear whether quality solution satisfactory. second number variables much larger namely large-scale problems even ﬁrst-order methods might costly. possible handle large positive semideﬁnite matrix factor product matrices; sometimes referred burer-monteiro approach fact many cases stationary point guaranteed global minimum also survey. currently active area research trying identify nonconvex problems optimal solutions guaranteed computed eﬃciently nonconvex approaches. tackle many ways using standard nonlinear optimization schemes. straightforward popular two-block coordinate descent method simple scheme implemented diﬀerent ways. subproblems usually solved high precision; example steps gradient method used. methods general guaranteed converge stationary point sophisticated schemes include riemannian optimization techniques many methods based randomization also developed recently; surveys alternating local minimization shown lead optimal solutions assumptions similar needed convexiﬁcation-based approaches; example missing data sparse robust pca. recently showed missing data spurious local minima goal paper provide exhaustive survey rather provide brief introduction focusing several aspects particular address application hyperspectral imaging geometric interpretation complexity issues algorithms nonnegative rank link extended formulations polyhedra. mentioned introduction nonnegativity constraints allow interpreting basis elements data nonnegativity allows interpreting weights activation coeﬃcients. describe detail next section particular application namely blind hyperspectral unmixing nonnegativity physical interpretation. nonnegativity constraints also naturally lead sparse factors. fact ﬁrst-order optimality conditions problem type minx≥ hence stationary points expected zero entries. property enhances interpretability provides better compression compared unconstrained variants. grayscale image image value pixel single sample. image three channels allows color image reconstructed perceived human eye. hyperspectral image image usually pixel channels corresponding reﬂectance diﬀerent wavelengths. wavelengths measured hyperspectral image depend camera used usually chosen depending application hand. advantage hyperspectral images contain much information blind human allows identify characterize materials present scene much precisely; figure illustration. numerous applications include agriculture care food processing mineralogy surveillance physics astronomy chemical imaging environmental science; example https//en.wikipedia.org/wiki/hyperspectral_imaging http//sciencenordic.com/lengthy-can-do-list-colour-camera. figure bottom image four plants identify artiﬁcial one? grayscale image wavelength blind naked allows identifying artiﬁcial plant analysis image allows ﬁnding small target lego ﬁgure within plants. source sciencenordic.com photo torbjørn skauli ffi. assume scene imaged hyperspectral imager using wavelengths pixels. construct matrix rp×n reﬂectance pixel wavelength. column therefore corresponds so-called spectral signature pixel corresponds vectorized image given wavelength. given image important goal practice identify constitutive materials present image called endmembers classify pixels accordingly identify pixels contain materials quantity. fact resolution hyperspectral images hence pixels contain several materials. library dictionary spectral signatures materials present image available problem referred blind hyperspectral unmixing goal identify endmembers quantify abundances endmembers pixel. simplest popular model linear mixing model assumes spectral signature pixel equals weighted linear combination spectral signatures endmembers contains weight given abundances. physically reﬂectance pixel proportional materials contains example pixel contains aluminum copper spectral signature equal times spectral signature aluminum plus times spectral signature copper. practice model figure illustration decomposition hyperspectral image three endmembers left hyperspectral image middle spectral signatures three endmembers columns matrix right abundances material pixel using standard algorithm algorithm tries solve general lead sought decomposition. reason solution highly nonunique discussed later. practice meaningful solution achieved usually using additional conk= sparsity piecewise smoothness columns spatial coherence rows numerous constrained variants exist discuss here; example references therein. nice geometric interpretation crucial consider order understand nonuniqueness solutions. discussed subsequently also allows develop eﬃcient algorithms closely related extended formulations polyhedra. conv therefore exact problem equivalent ﬁnding polytope conv nested given polytopes conv unit simplex dimension inner polytope conv rank dimension outer polytope dimension nested polytope conv known advance. three polytopes dimension problem well known computational geometry referred nested polytope problem rank rank column spaces must coincide outer polytope restricted case inner nested outer polytopes dimension. impose explicitly additional constraint rank) exact problem prove restricted variant exact equivalent reduced another restricted exact problem involves nested hexagons facet outer polytope corresponds facet nonnegative orthant nonnegativity constraint. inner hexagon smaller outer ratio inner hexagon twice small outer triangle rank+ figure rank+ triangle hexagons. inner hexagon smaller outer rectangle rank+ figure implies rank+ general unique solution example four triangles polytopes would even worse since would inﬁnite number solutions. reason practitioners often additional constraints model identify meaningful solution problem example details uniqueness nmf. nonnegative rank increase neighborhood given matrix; nonnegative rank upper semicontinuous nonnegative matrix without zero columns rank+ exists ball centered radius rank+ quantiﬁer elimination theory unfortunately know cannot used practice even small matrices developing eﬀective code exact small matrices important direction research. note developed code based heuristics allows solving exact matrices dozen rows columns recently shitov independently chistikov answered important open problem showing nonnegative rank reals might diﬀerent nonnegative rank rationals implying nonnegative rank computation since size output bounded size input. section brieﬂy describe main classes algorithms. mentioned introduction exist best knowledge successful convexiﬁcation approach opposed low-rank models. note however exist convexiﬁcation approach compute lower bounds nonnegative rank explanation cannot work directly low-rank approximation nuclear norm even given best nonnegative approximation nonnegative rank general recovering exact would diﬃcult. writing directly convexiﬁcation variables seems diﬃcult symmetry problem breaking symmetry seems nontrivial; discussion tentative formulation. interesting direction research. componentwise product matrices. extremely popular simplicity proposed paper seung launched research nmf. however converges slowly; cannot modify zero entries; guaranteed converge stationary point. note interpreted rescaled gradient descent; example methods solve subproblems exactly referred alternating nonnegative least squares; among these active methods seem eﬃcient dedicated codes implemented haesun park collaborators; references therein. practice method seems work extremely well apply steps coordinate descent nnls subproblems subblocks columns rows reason subproblems solved closed form. fact optimal column given although usually provide satisfactory results practice methods described preceding section come guarantee. paper complexity arora also identify subclass matrices problem much easier. so-called separable matrices deﬁned follows. requires column basis matrix decomposition present input matrix equivalently requires matrix decomposition contain identity matrix submatrix. separable problem problem identify subset minimized). geometrically exact case normalization columns separability assumption equivalent conv conv. therefore so-called separable problem reduces identify vertices convex hull columns relatively easy geometric problem. becomes tricky noise added separable matrix many recent works tried quantify level noise tolerate still able recover vertices error. algorithms separable based geometric interpretation many developed within blind community recently however robustness noise algorithms analyzed. simplest algorithm often referred successive projection algorithm closely related modiﬁed gram-schmidt algorithm column pivoting discovered several times discussion polytope strongly convex function always maximized vertex used identify vertex column conv separability assumption). column identiﬁed project columns onto orthogonal complement column amounts applying linear transformation polytope. full rank vertices project onto steps recursively. approach greedy method identify subset columns maximum volume algorithm proved robust noise made robust noise using strategies separable exist index size nonnegative matrix equivalently exists n-by-n nonnegative matrix nonzero rows solving separable therefore formulated diagonal entry largest goal minimize number nonzero entries diagonal optimal solution contain nonzero diagonal entries hence nonzero rows. using norm another convex model improved models presented turn essentially equivalent main drawback computational cost since models variables. example hyperspectral imaging number pixels typically order millions; hence solving problems challenging natural approach therefore ﬁrst select subset good candidates among columns optimize subset rows main advantage approach resulting models provably robust separable intuitively reason model focuses identifying example subset columns large volume also requires data points well approximated selected vertices reason also much less sensitive outliers geometric approaches. example. extension complexity regular n-polygons result used approximate second-order cone program linear program particular seen extension complexity regular hexagon equation figure recent results. several recent important results understanding limits linear programming solving combinatorial problems based theorem constructing lower bounds nonnegative rank usually based sparsity pattern slack matrix survey. particular rothvoß showed recently prefect matching problem cannot written polynomially many constraints convex cone leads clrma problems. example cone positive semideﬁnite matrices rows columns required vectorized matrices. smallest extension given equal so-called rank slack matrix; recent survey ideas example recently allowed hamza fawzi prove code cannot represented using second-order cone proof relies fact second-order cone rank cone -by- matrices inﬁnite. used many applications. tightly connected diﬃcult geometric problems; hence developing fast reliable algorithms challenge. although important challenges remain tackled even challenges exist generalizations nmf. particular mentioned cone factorizations recent problems explored full extent. acknowledgments. author would like thank editors siam activity group optimization’s views news stefan wild jennifer erway providing insightful feedback helped improve presentation paper. paper appeared siag/opt views news author also acknowledges support f.r.s.-fnrs", "year": 2017}