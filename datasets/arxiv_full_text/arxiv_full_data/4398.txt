{"title": "Hard Mixtures of Experts for Large Scale Weakly Supervised Vision", "tag": ["cs.CV", "stat.ML"], "abstract": "Training convolutional networks (CNN's) that fit on a single GPU with minibatch stochastic gradient descent has become effective in practice. However, there is still no effective method for training large CNN's that do not fit in the memory of a few GPU cards, or for parallelizing CNN training. In this work we show that a simple hard mixture of experts model can be efficiently trained to good effect on large scale hashtag (multilabel) prediction tasks. Mixture of experts models are not new (Jacobs et. al. 1991, Collobert et. al. 2003), but in the past, researchers have had to devise sophisticated methods to deal with data fragmentation. We show empirically that modern weakly supervised data sets are large enough to support naive partitioning schemes where each data point is assigned to a single expert. Because the experts are independent, training them in parallel is easy, and evaluation is cheap for the size of the model. Furthermore, we show that we can use a single decoding layer for all the experts, allowing a unified feature embedding space. We demonstrate that it is feasible (and in fact relatively painless) to train far larger models than could be practically trained with standard CNN architectures, and that the extra capacity can be well used on current datasets.", "text": "data gets bigger expect able scale models well better features; data means refined models less overfitting. however even today’s state convolutional models cannot keep size today’s weakly supervised data. current optimization technology hardware images posted photo sharing sites passed training pipeline standard state convolutional architectures. furthermore evidence work architectures already underfitting datasets scale hundreds millions images. well established approach scaling models straightforward mixture architectures model acts gater routing data points expert classifiers update final decision example work make contributions. first propose particularly simple mixture architecture expert associated cluster feature space trained acts gater. also describe variant experts share decoder allowing feature space experts meaningful transfer tasks. second give evidence setting weakly supervised prediction images large datasets available today standard models underfitting. hand show despite approach’s simplicity setting give significant benefits test accuracy allowing efficient training much powerful models. denote x...xn labeled training images target outputs y...yn. basic idea mixture experts model expert classifiers h...hk gating classifier evaluate model input processed outputting probability vector coordinates. output model training convolutional networks single minibatch stochastic gradient descent become effective practice. however still effective method training large cnn’s memory cards parallelizing training. work show simple hard mixture experts model efficiently trained good effect large scale hashtag prediction tasks. mixture experts models past researchers devise sophisticated methods deal data fragmentation. show empirically modern weakly supervised data sets large enough support naive partitioning schemes data point assigned single expert. experts independent training parallel easy evaluation cheap size model. furthermore show single decoding layer experts allowing unified feature embedding space. demonstrate feasible train larger models could practically trained standard architectures extra capacity well used current datasets. large annotated image datasets revolutionized computer vision. rise data hungry machine learning methods like convolutional neural networks facilitated training sets millions labeled images especially imagenet machine learning methods proven successful solving training tasks also finding good features many image tasks; large extent models perform better tasks like imagenet recognition challenge give features better tasks however hand annotation laborious. imagenet dataset small compared hundreds millions images posted social media every day. recent works shown possible build vision models weakly supervised instead hand-annotated data open possibility using truly gigantic datasets. here convolutional networks moreover consider simple situation nonzero coordinate. models pick single expert instead distribution experts models hard mixtures. work train models end. attempt directly optimize assignment expert minimizes final classification loss. instead build follows first train standard supervised convolutional network layers produce satisfied optimization construct take output last hidden layer decoder training point k-means clustering obtaining cluster centers c..ck. model thus local architecture sense location feature space defined corresponds choice expert classifier. note make attempt balance number images cluster. given input outputs expert possible methods building rest model. simplest version outputs distribution leading model figure case optimize independently share training data. model useful test time thing care predicting labels however often case training model want features last layer decoder rather label predictions. case keep independent instead outputting probabilities labels outputs feature vector append shared decoder model. figure note models output expert distribution full possible labels. training model shared decoder involved model independent decoders large number classes gradients relatively sparse. setting machine hold machine models proposed important scalability advantages compared standard times many feature maps. efficient terms wall clock time parameter train test trained independently easier parallelize. furthermore independent number evaluation time cost finding output input cost computing plus cost computing single whereas large times many feature maps naive forward could cost much times much. although many methods exist compressing layers make evaluation faster less success training models compressed form would difficult current technology train cnn’s large ones discuss here. hand models described inefficient compared standard terms modeling power parameter terms data usage. acts independently others parameters interact parameters contrast standard parameter interact other. moreover training data split amongst experts parameter sees fraction data trained accurately data. situations quickly runs hardware algorithmic limitations training standard serial stochastic gradient descent hard mixtures experts become attractive. particular make times many parameters infeasible scale number feature maps making expert hard mixture model practical. advantages liabilities end-to-end however scale discussed paper end-to-end training mixture models continues exceptional engineering endeavor computing infrastructure necessary make work widespread hand techniques described work simple used expert trains independently. experts train much faster trunk total time even trained serial small multiple training trunk. concurrently work described distributed end-to-end mixture experts system training rnn’s settings. shows possible train mixture models end-to-end scale; show valuable take simpler approach. recent works shown noisy tags captions large image collections effective source supervision e.g. references therein. works pave using web-scale training sets learning image features. work framework these show improve results larger capacity models. model propose particularly simple mixture experts. models introduced differs many following works hard mixture efficiency scale rather soft probabilistic assignment. similar approach however work instead using multiple rounds optimizing gating classifiers fixed optimizing classifiers single round k-means feature outputs classifier expert assignments. furthermore gater experts convolutional networks instead svm’s. recently several works using mixture experts type models cnn’s vision tasks roughly models differ deal routing data experts parts model shared results experts combined. work similar works data routed experts agglomeration classes abstract superclasses. gater sends image expert based superclass gater thinks image work output final hidden states gater applied training data clustered directly opposed class labels. suitable prediction tasks focus here image multiple different tags. certainly true certain tags co-occur others still case would like send image expert based likely tags rather particular tag; focus scaling model want look experts appearing union possible tags image. even single-label image categorization setting clustering image embedding rather classes partially mitigate errors gater makes unrecoverable mistake. found good results image mapped single expert. different methods combining output authors propose experts proposed. solving optimization evaluation time match output distributions generalist experts. expert networks taken parallel layers mapped directly onto outputs generalist. output model distribution given taking weighted distributions experts distribution given generalist coarse classes. model figure considered simple form sense generalist outputs delta simply take output relevant expert output. model somewhat different these expert outputs feature vector shared decoder. concurrent work also puts hard mixture model component deep learning architecture large scale language modeling. work work different problem domain simpler gating scheme. discuss three sets experiments. first train test category labeled imagenet second experiments models trained prediction following training procedures report prediction results. final experiments models trained prediction model except last layer fixed. last layer trained labeled datasets well weakly supervised features used transfer learning. following datasets prediction yfccm yfccm contains million images //www.flickr.com/ model resnet- resnet- resnet- resnet- ×feature size resnet- ensemble- resnet- moe- resnet- moe- resnet- moe- resnet- moe- resnet- moe- resnet- moe- shared decoder resnet- moe- oracle table yfccm hash prediction results. computed equations respectively. resnet- refers resnet layers moe-a refers model experts. base model parameters model experts parameters. ×feature size model hiden layers times many hidden units normal resnet. resnet- moe- shared decoder model times many parameters base model. cost twice much evaluate base models perform significantly better. examples predictions shown figure oracle model uses best possible choice expert using true test label; compute notion best involved. table flickr transfer results; numbers test accuracies datasets except imagenet reported number test accuracy validation set. model experts. models trained yfccm decoder layer removed decoder retrained training labels dataset. resnet- resnet- models dimensional feature representations alexnet dimensional feature representation. instagram food also collected milinstagram contain words relating food hashtags captions. dictionary words used select images obtained starting seed words finding images contained words captions hashtags. co-occurring words kept based tf-idf score model train architecture gater experts choose alexnet resnet; resnet well. dataset first train error plateaus. centroids k-means computation. running clustering project first dimensions. table visualization clusters feature space resnet- model. corresponds cluster; images randomly sampled cluster. first cluster peak porsche table cluster images train set. next rows respectively cluster peak zebra images train set; cluster peak park images train set; cluster peak keyboard images train set. distribution sizes clusters shown figure train using stochastic gradient descent minibatch size weight decay momentum training define epoch million images. start learning rate divide every epochs. train manner without momentum divide learning rate every epochs. corroborating reports found experts train faster original model. yfccm gater takes days train gpus; takes less hours gpu. thus experts trained parallel total training time less days. roughly train generalist epochs expert around means even without large number gpus possible train mixture model tens experts without small multiple total training time. table imagenet classification results. mixture experts model uses experts. error results reported validation data improvements accuracy scale. displaying results need define statistics describing model accuracy. first given vector define vector largest values entries given test images true targets model outputs define imagenet hand annotated dataset million color images roughly evenly taken categories. include imagenet experiments show simple architectures improve accuracies scale note however combining results experts care authors able improve results imagenet. results true tags convention vegas nevada centre construction predicted construction condo crane cranes uptown reconstruction skyscraper completion apartments elevated table show results prediction yfccm show qualitative results figure addition reporting defined above also report average test loss sampled uniformly dictionary image sampled uniformly conditioned tag. shared decoders independent decoders base architecture mixture experts model significantly better test loss better better base model. true test sampling schemes even though model trained first one. moreover increase number experts test increases. evidence base models underfitting train loss almost test loss; test loss much worse larger capacity mixture models. trends visible table base models seem underfitting larger models able significantly better. another view figure plot number parameters various architectures test accuracies. shared decoder incur loss accuracy compared independent decoders. possible shared decoder model less powerful independent decoder model another possibility arises fact independent decoder model early stopping expert individually based validation loss; shared decoder model easy individual early stopping. last table include oracle mixture experts model gets test tags choose experts. gives sense much lost training end-to-end presumably end-to-end training could allow accurate choice experts. room accurately choose experts. note upper bound end-to-end training also allow experts efficiently specialize. however experience information assignments experts inputs recovering end-to-end model figure number parameters test accuracy many models yfccm. accuracy measured orange curve corresponds resnet- models. except ensemble model less accurate parameter models roughly curve test accuracy increases model size. approach allows training large models take advantage rich data. finally warn reader yfccm instagram food datasets train test validation splits random subsets images. therefore image-sets single user divided across splits allows over-fitting without penalty however view datasets larger need models capacity over-fit data size standard models cannot moreover many settings kind over-fitting feature. table select tags plot distribution tags clusters features resnet- model. clusters cover different kinds vocabulary. words mostly concentrated clusters words spread many. words appear cluster. figure sparsity cluster distribution accuracy. represents distribution clusters axis ||c||/||c|| axis tag. roughly models accurate predicting tags sparser distributions clusters. table display random images random clusters give qualitative sense composition various clusters. supplemental display random samples clusters. test quality features learned shared decoder model inputs linear classifiers following test datasets indoor scenes dataset scene dataset birds dataset oxford flowers dataset stanford action recognition table instagram food hash prediction results. computed equations respectively. resnet- refers resnet layers moe-a refers model experts. base model parameters model experts parameters costs twice much time evaluate. table distribution tags among clusters. subplot corresponds specific word dictionary. histogram corresponds specific cluster height corresponds number times word appears cluster. numbers bars label identity cluster represents. tags specific clusters others uniformly spread out. clusters feature maps resnet- model projected dimensions yfccm dataset train feature extractor. training prediction task follow standard practice layers except decoder. output layer prior decoder taken features image datasets listed above. train linear classifier dataset using features dataset’s labels supervision. results transfer experiment shown table clear pattern improved performance prediction translating improved performance transfer. note also using single model trained imagenet better models trained yfccm nevertheless performance gain across datasets mixture model especially datasets require fine-grained classification particular results training feature extractor yfccm transferring imagenet encouraging. barring breakthrough optimization radical change computer hardware order train convolutional models datasets many billions images without under-fitting need specialized model architectures designed scale. work showed simple scalable hard mixture experts model significantly raise prediction accuracy large weakly supervised image collections million million images. model image routed single expert evaluation twice computational cost base model; moreover easy train experts parallel. showed encouraging results version model experts share decoder allowing features used transfer tasks.", "year": 2017}