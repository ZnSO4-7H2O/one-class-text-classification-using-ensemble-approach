{"title": "Lattice Rescoring Strategies for Long Short Term Memory Language Models  in Speech Recognition", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Recurrent neural network (RNN) language models (LMs) and Long Short Term Memory (LSTM) LMs, a variant of RNN LMs, have been shown to outperform traditional N-gram LMs on speech recognition tasks. However, these models are computationally more expensive than N-gram LMs for decoding, and thus, challenging to integrate into speech recognizers. Recent research has proposed the use of lattice-rescoring algorithms using RNNLMs and LSTMLMs as an efficient strategy to integrate these models into a speech recognition system. In this paper, we evaluate existing lattice rescoring algorithms along with new variants on a YouTube speech recognition task. Lattice rescoring using LSTMLMs reduces the word error rate (WER) for this task by 8\\% relative to the WER obtained using an N-gram LM.", "text": "recurrent neural network language models long short term memory variant shown outperform traditional n-gram speech recognition tasks. however models computationally expensive n-gram decoding thus challenging integrate speech recognizers. recent research proposed lattice-rescoring algorithms using rnnlms lstmlms efﬁcient strategy integrate models speech recognition system. paper evaluate existing lattice rescoring algorithms along variants youtube speech recognition task. lattice rescoring using lstmlms reduces word error rate task relative obtained using n-gram increased resulting large size poorly trained sparsity higher order n-gram counts. neural network language models recently emerged promising alternative n-gram embedding words continuous space nnlm provides better smoothing unknown well rare words relative n-gram addition recurrent nnlm ability model long-range contexts effectively making entire word history results improved transcription long form speech content long short term memory variant rnnlm suffer problem vanishing exploding gradients shown effective speech recognition basic rnnlm lstmlms make long range word history predicting word probabilities challenging integrate ﬁrst pass speech recognition system typically k-best list recognition hypotheses generated initial recognition pass rescored using lstmlm. however transcribing long form content kbest list represents tiny subset possible hypotheses. alternative perform rescoring lattices contain many hypotheses relative k-best lists. computationally prohibitive lstmlm uses full word history computing probability next word. recent work demonstrated efﬁcient strategies lattice rescoring. extend work compare existing algorithms variants n-gram traditionally language model choice speech recognition systems efﬁcient train decoding. n-gram makes assumption word depends previous words efﬁciency n-gram stems small ngram orders typically used estimate models. makes n-gram powerful tasks voice-search short-range contexts sufﬁce perform well tasks transcription long form speech content require modeling longrange contexts order n-gram model inference time computation lstmlm takes place softmax layer. multiple schemes proposed reduce computation softmax layer self-normalization strategy whereby objective function training lstmlm modiﬁed adding penalty normalizers training data. selfnormalization yields normalizers approximate enables obtain large savings computation skipping softmax computation inference time. however training model using scheme slow computing normalizer involves summation words full vocabulary. speed training compute normalizer randomly drawn sample words within training mini-batch optimal value chosen grid search. values range work well practice because provide good trade-off achieving selfnormalization staying close original likelihood objective function. perform lattice rescoring lstmlms transcribing videos youtube. also present techniques speeding training inference lstmlms reducing memory requirements inference. continuous representation word obtained using embedding matrix logistic sigmoid function. input forget output gates. forget input gates coupled limit range internal state cell. tuple represents internal cell states lstm state. denote full weight matrices diagonal weight matrices biases respectively. represents element-wise multiplication. architecture uses peep-hole connections internal cells gates learn precise timings outputs recurrent projection layer reduces dimension recurrent state thus allowing matrices remain relatively small retaining large memory capacity. large vocabulary speech recognition task typically employs vocabulary several hundreds thousands words. large vocabulary softmax layer computationally expensive training test times. reduce computation training approximate softmax full vocabulary sampled softmax words vocabulary sorting words descending order frequencies training corpus draw negative samples log-uniform distribution. sampled softmax probability word given history previous words given acoustic language model probabilities respectively. reduce computation involved integrating lstmlm ﬁrst pass recognizer generate lattices using initial recognition pass incorporates acoustic model ngram paper goal evaluate lstmlms speech recognition long form content videos average segment length words. focus lattice rescoring lstmlms allows search many hypotheses k-best list requires approximation. weights computed efﬁciently using forward-backward algorithm compare weighting schemes simpler scheme assigns uniform probability predecessor nodes. i.e. push-forward algorithm selects best lstm state lattice node uses lstm state scoring outgoing arcs node. propose arcbeam variant allows independently select best lstm state among predecessor nodes state scoring word arc. algorithm possible arcs starting lattice node select different lstm states scoring denotes nodes lattice pred nodes immediately precede arcs arcs starting node a.am a.lm a.word a.next refer acoustic model costs language model costs word destination node respectively. node denoted fig. push-forward algorithm lattice. lstm state shown -dimensional vector. lattice node lstm state lowest cumulative cost propagated outgoing arcs. algorithm describes push-forward algorithm adapted lattices generated speech recognition system. algorithm extends lstm state words lattice arcs retaining lstm states lattice node. lstm states sorted based cumulative acoustic language model probabilities leading lattice node. algorithm constructs lattice lstmlm costs arcs. special case original lattice structure preserved leading efﬁcient algorithm however approximated lstm states leading lattice node single-best lstm state. make approximation less severe expanding initial lattice speciﬁed n-gram order prior applying push-forward algorithm. expansion ensures paths leading lattice node words. approach similar recombination pruning n-gram history based clustering approach next study variant rescoring algorithm proposed whereby original structure lattice preserved instead keeping likely lstm state lattice node lstm state given lattice node computed weighted combination lstm states predecessor nodes. variant differs lattices contain arcs non-speech symbols representing pauses sentence boundaries. however lstmlm trained tokens. prevent train-test mismatch modify lattice rescoring algorithms copy lstm state across tokens. strategy also employed speech recognition task vocabulary words crucial train lstmlm large embedding softmax layers achieve good performance. vocabulary size words embedding dimension nodes parameters. similarly input softmax dimensions softmax layer parameters. lstmlm parameters model part either embedding softmax layers contribute substantially overall size model. compress lstmlm focused shrinking embedding softmax matrices. scalar quantization shown effective compression neural network models however resulting compression ratios adequate task. alternative strategy vector quantization lead larger compression. however work well high dimensional settings consisting dimensions. product quantization hybrid strategy allows apply higher dimensions. prior work applied compress fully connected convolutional layers neural networks. contrast apply compress embedding softmax layers lstmlm. others also applied compress word embedding layers text classiﬁcation models. applying embedding matrix divide embedding dimensions equal sized chunks perform chunk. result need store indices centers codebook. using chunk size either able shrink embedding softmax matrices factor without loss performance lstmlm. matrix-matrix multiplication softmax layer efﬁciently performed using lookup table. performed subsequent model training. conducted experiments youtube speech recognition task youtube video sharing website billion users. speech recognition enabled youtube improve accessibility users providing automatic captions. language models trained billion word tokens derived anonymized user-generated video captions. evaluation consisted videos sampled randomly google preferred channels youtube contained categories ranging science&education video games total duration hours word tokens. divided audio portion video segments using gaussian mixture model based speech-silence endpointer. segment generated initial word lattice using context dependent phone level acoustic model trained using connectionist temporal classiﬁcation objective function described n-gram recognition vocabulary words. rescored lattice using lstmlm. investigate effect order n-gram used lattice generation performance lstmlm lattice rescoring algorithms experimented bigram -gram consisting n-grams. used subset ﬁrst pass vocabulary consisting frequent words training lstmlm. lstmlm parameters shown figure training words training present vocabulary mapped special token. decoding word lattice present vocabulary assigned probability plstm nunk number unique words mapped training. using hyperparameters trained model convergence using adagrad optimizer using learning rate without dropout unrolled rnns steps used batch size clipped gradients lstm weights norm bounded training used workers asynchronous gradient updates. training performed using tensorflow comparison also trained larger -gram consisting billion n-grams vocabulary used training lstmlm. next report recognition results terms word error rate rescoring lattices generated using either bigram -gram lstmlm obtain results rescoring best list unique recognition hypotheses generated initial lattice applying push-forward lattice rescoring algorithm probability computed lstmlm used as-is interpolated probability ﬁrst pass used language model scale factor word insertion penalty lattices pruned density using forward-backward pruning prior rescoring. using k-best rescoring lstmlm yielded tiny improvement relative. contrast lattice rescoring lstmlm gave relative improvement ﬁrst pass conﬁrms best list tiny space hypotheses video segment. expected lstmlm gives much larger relative improvement bigram ﬁrst pass next present results using lattice rescoring algorithms presented section ﬁrst varied maximum hypotheses lattice node push-forward algorithm increasing reduced relative -gram lattices showed variation -gram lattices. push-forward algorithm results severe approximation -gram lattices. hence increasing lstm hypotheses lattice node results better wer. next expanded lattice different n-gram orders maximum prior applying push-forward algorithm -gram lattices beneﬁtted considerably lattice expansion less gain -gram lattices already contained unique -gram histories lattice nodes prior expansion. lstm state pooling gives similar performance push-forward algorithm max-prob sum-prob weighting schemes performed better assigning uniform probability predecessor nodes. finally beam algorithm performed equivalently push-forward algorithm paper presented comparison algorithms rescoring speech recognition lattices using lstm language model. obtain relative improvement ﬁrst pass n-gram speech recognition youtube videos. push-forward algorithm variants explored previously work novel compare algorithms lstm state pooling proposed spoken language understanding evaluated speech recognition. pooling lstm states predecessor nodes gives similar performance choosing single best lstm state lattice node. indicates terminal lstm states paths ending lattice node similar. propose arc-beam algorithm novel variant push-forward algorithm selects best predecessor lstm state independently lattice arc. lattice rescoring algorithms substantially improve upon rescoring recognition hypotheses. shows crucial explore large space hypotheses rescoring task youtube speech recognition average length utterance words. prior lattice rescoring approaches experiment history vector based clustering shown give equivalent results n-gram based history clustering which turn similar applying push-forward algorithm lattice expanded speciﬁed n-gram order. lattices accurate start times lattice arcs decoder optimization therefore explore lookahead based techniques make arc-level start times paper also provides directions lstmlm model optimization. earliest show lstmlm trained using self-normalization sampled softmax loss allows efﬁcient training well inference especially critical language models large vocabularies used commerical speech recognizers second product quantization effective strategy help compress embedding softmax layers lstmlm factor without loss performance. leads large reduction overall size lstmlm embedding softmax layers contribute bulk model size. expect lattice rescoring model optimization strategies useful speech recognition related tasks handwriting recognition long-span language models proven useful. thank jitong chen michael riley brian roark hagen soltau david rybach ciprian chelba chris alberti felix stahlberg rafal jozefowicz helpful suggestions. ebru arisoy tara sainath brian kingsbury bhuvana ramabhadran deep neural network language models proceedings naacl-hlt workshop ever really replace n-gram model? future language modeling stroudsburg association computational linguistics. martin sundermeyer hermann ralf schl¨uter from feedforward recurrent lstm neural networks language modeling ieee trans. audio speech language processing vol. xunying chen yongqiang wang mark gales philip woodland efﬁcient lattice rescoring methods using recurrent neural network language models ieee trans. audio speech language processing vol. babak damavandi shankar kumar noam shazeer antoine bruguier nn-grams unifying neural network n-gram language models speech recognition interspeech tomas mikolov anoop deoras daniel povey lukas burget cernock`y sanjeev khudanpur strategies training large scale neural network language models asru hasim andrew senior franc¸oise beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling interspeech mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems arxiv preprint arxiv. chen xunying yongqiang wang mark gales philip woodland efﬁcient training evaluation recurrent neural network language models automatic speech recognition ieee transactions audio speech language processing vol. jacob devlin rabih zbib zhongqiang huang thomas lamar richard schwartz john makhoul fast robust neural network joint models statistical machine translation acl. john duchi elad hazan yoram singer adaptive subgradient methods online learning stochastic optimization journal machine learning research vol. frank wessel ralf schl¨uter klaus macherey hermann conﬁdence measures large vocabulary continuous speech recognition ieee transactions speech audio processing vol. yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey jeff klingner apurva shah melvin johnson xiaobing lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean google’s neural machine translation system bridging human machine translation corr vol. abs/. armand joulin edouard grave piotr bojanowski matthijs douze jegou h´erv´e tomas mikolov fasttext.zip compressing text classiﬁcation models arxiv preprint arxiv. hank liao mcdermott andrew senior large scale deep neural network acoustic modeling semisupervised training data youtube video transcription asru. ieee", "year": 2017}