{"title": "Audio Visual Speech Recognition using Deep Recurrent Neural Networks", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "In this work, we propose a training algorithm for an audio-visual automatic speech recognition (AV-ASR) system using deep recurrent neural network (RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal Classification (CTC) objective function. The frame labels obtained from the acoustic model are then used to perform a non-linear dimensionality reduction of the visual features using a deep bottleneck network. Audio and visual features are fused and used to train a fusion RNN. The use of bottleneck features for visual modality helps the model to converge properly during training. Our system is evaluated on GRID corpus. Our results show that presence of visual modality gives significant improvement in character error rate (CER) at various levels of noise even when the model is trained without noisy data. We also provide a comparison of two fusion methods: feature fusion and decision fusion.", "text": "abstract. work propose training algorithm audiovisual automatic speech recognition system using deep recurrent neural network .first train deep acoustic model connectionist temporal classiﬁcation objective function. frame labels obtained acoustic model used perform non-linear dimensionality reduction visual features using deep bottleneck network. audio visual features fused used train fusion rnn. bottleneck features visual modality helps model converge properly training. system evaluated grid corpus. results show presence visual modality gives signiﬁcant improvement character error rate various levels noise even model trained without noisy data. also provide comparison fusion methods feature fusion decision fusion. audio-visual automatic speech recognition case multi-modal analysis modalities complement recognize speech. incorporating visual features speaker’s movements facial expressions automatic speech recognition systems shown improve performances especially noisy conditions. several methods proposed traditionally include variants gmm/hmm models. recently av-asr methods based deep neural networks models proposed. end-to-end speech recognition methods based rnns trained objective function come fore recently shown give performances comparable dnn/hmm. trained directly learns mapping audio feature frames character/phoneme sequences. method eliminates need intermediate version accepted international workshop multimodal pattern recognition social signals human computer interaction satellite event international conference pattern recognition work design evaluate audio-visual system using deep recurrent neural network objective function. design av-asr system includes tasks visual feature engineering audio-visual information fusion. figure shows av-asr pipeline test time. work mainly deals visual feature extraction processing steps training protocol fusion model. proper visual features important especially case rnns rnns diﬃcult train. bottleneck features used tandem audio features known improve performance employ similar idea improve discriminatory power video features. show helps converge properly compared features. finally compare performances feature fusion decision fusion methods. paper organized follows section presents prior work avasr. bi-directional training using objective function discussed section section describes feature extraction steps audio visual modalities. section diﬀerent fusion models explained. section explains training protocols experimental results. finally summarize work diﬀerences various av-asr systems chieﬂy methods employed visual feature extraction audio-visual information fusion. visual feature extraction methods types appearance based features pixel mouth region speaker considered informative. usually transformation applied reduce dimensions. additional feature processing mean normalization intra-frame inter-frame applied shape based features utilize geometric features height width area region build statistical model contours whose parameters used features. combination appearance shape based features. fusion methods broadly divided types feature fusion decision fusion. feature fusion models perform level integration audio visual features involves single model trained fused features. feature fusion include simple concatenation features feature weighting usually followed dimensionality reduction transformation like lda. decision fusion applied cases output classes modalities same. various decision fusion methods based variants hmms proposed. multistream emission probability state audio-visual system obtained linear combination loglikelihoods individual streams state. parameters hmms individual streams estimated separately jointly. multistream assumes state level synchrony streams methods coupled allow asynchrony streams. detailed survey based av-asr systems refer readers application deep learning multi-modal analyses presented describes multi-modal cross-modal shared representation learning applications av-asr. deep belief networks explored. authors train separate networks audio visual inputs fuse ﬁnal layers networks build third fused features. addition presents architecture bilinear soft-max layer improves performance. deep de-noising auto-encoder used learn noise robust speech features. auto-encoder trained mfcc features noisy speech input reconstructs clean features. outputs ﬁnal layer auto-encoder used audio features. trained images mouth region input phoneme labels output. ﬁnal layers networks combined train multi-stream hmm. following notations adopted paper. utterance length denote observation sequences audio visual frames assume equal frame rates audio visual inputs ensured experiments means interpolation. features time utterance corresponding label sequence given directional hidden layer components corresponding forward backward connections. given input sequence output network calculated follows forward pass forward component hidden layer given instant given corresponds hidden-to-hidden weights forward components forward component bias. non-linearity depending choice hidden layer unit. similarly forward pass backward component hidden layer given rnns trained using back-propagation time algorithm. training algorithm suﬀers vanishing gradients problem overcome using special unit hidden layer called long short term memory. dnns used systems frame-level classiﬁers i.e. frame input sequence requires class label order trained. framelevel labels usually states obtained ﬁrst training gmm/hmm model forced alignment input sequences states. objective function obviates need alignments enables network learn possible alignments. input sequence corresponding label sequence employs soft-max output layer containing node element l∪{φ}. number output units additional symbol represents blank label meaning network produced output input frame. additional blank label output allows deﬁne alignment length containing elements example alignments length label sequence accordingly many deﬁned generates label sequence quence given observation sequence. equation computationally intractable since number alignments increases exponentially number labels. eﬃcient computation forward-backward algorithm used. sampling rate audio data converted khz. frame speech signal duration ﬁlter-bank features dimensions extracted. ﬁlter-bank features mean normalized features appended. ﬁnal dimensional features used audio features. video frame rate increased match rate audio frames interpolation. av-asr visual features region surrounding speaker’s mouth. frame converted gray scale face detection performed using viola-jones algorithm. region extracted detecting landmark points speakers face cropping surrounding speakers mouth chin. dimensional features extracted roi. several experiments training features found training either exploded converged poorly. order improve discriminatory power visual features perform non-linear dimensionality reduction features using deep bottleneck network. bottleneck features obtained training neural network hidden layers relatively small dimensions. trained using cross-entropy cost function character labels output. frame-level character labels required training obtained ﬁrst training acoustic model obtaining outputs ﬁnal soft-max layer conﬁguration given opdim work fusion models character based rnns trained using objective function i.e. english alphabet including blank label. fusion models shown figure feature fusion technique single trained concatenating audio visual features using objective function. test phase instant concatenated features forward propagated network. decoding step posterior probabilities obtained soft-max layer converted pseudo log-likelihoods modality. example higher levels noise audio input value preferred. work adapt parameter utterance based kl-divergence measure posterior probability distributions divergence posterior probability distributions expected vary noise audio modality increases. kl-divergence scaled value using logistic sigmoid. parameter determined empirically validation dataset. system trained tested grid audio-visual corpus. grid corpus collection audio video recordings speakers uttering sentences. utterance ﬁxed length approximately seconds. total number words vocabulary syntactic structures sentences similar shown below. corpus obtained video recordings speaker available. addition utterances various speakers could processed various errors. dataset eﬀect consisted utterances used training cross validation remaining data used test set. training test data contain utterances speakers. models trained tested using kaldi speech recognition tool kaldi+pdnn eesen framework. na-acoustic model contains bi-directional lstm hidden layers.input network -dimensional vector containing ﬁlter-bank coefﬁcients along features. model parameters randomly initialized within range initial learning rate learning rate adaption performed follows improvement accuracy cross-validation successive epochs falls learning rate halved.the halving continues subsequent epoch training stops increase frame level accuracy less deep bottleneck network training protocol similar followed train bottleneck network. input video features mean normalized spliced. cross-entropy loss function minimized using mini-batch stochastic gradient descent frames shuﬄed randomly epoch. batch size initial learning rate learning rate adaptation similar acoustic model employed. nv-lip reader trained bottleneck network features input. network architecture training procedure figure depicts learning curves trained bottleneck features features. ﬁgure shows bottleneck features helpful proper convergence model. cients audio modality bottleneck features visual modality respective features. initialization learning rate adaption similar acoustic model training. however learning rate adaptation employed minimum number epochs completed. utterance epoch ﬁrst present fused audio-visual fused input sequence followed input sequence audio input values. prevents over-ﬁtting audio inputs. thus eﬀective number sequences presented network given epoch twice total number training utterances training features train network epochs audio utterances obtained turning visual modality. audio-visual model tested three levels babble noise clean audio. noise added test data artiﬁcially mixing babble noise clean audio .wav ﬁles. order show importance visual modality noisy environment model tested either audio video inputs turned token wfst used paths corresponding label sequences. token wfst obtains mapping removing blanks repeated labels. character error rate obtained decoded expected label sequences calculating edit distance them. results shown table observe clean audio input audio performs signiﬁcantly better compared audio-visual however audio becomes noisy performance deteriorates signiﬁcantly whereas performance remains relatively stable. noisy conditions feature fusion model behaves receiving input audio modality. table also gives comparison feature fusion model decision fusion model. feature fusion model performs better decision fusion model cases except clean audio conditions. poor model indicates frame level predictions synchronous. however fusion models provide signiﬁcant gains noisy audio inputs. large diﬀerence models clean inputs believe diﬀerence nature dataset reduce larger datasets. work presented audio-visual system using deep rnns trained objective function. described feature processing step visual features using deep bottleneck layer showed helps faster convergence model training. presented training protocol either modalities turned training order avoid dependency single modality. results indicate trained model robust noise. addition compared fusion strategies feature level decision level. bottleneck features visual modality helps training requires frame level labels involves additional step training audio rnn. therefore system end-to-end. experiments visual feature engineering unsupervised methods like multi-modal auto-encoder produce remarkable results. future work intend explore unsupervised methods visual feature extraction canonical correlation analysis.", "year": 2016}