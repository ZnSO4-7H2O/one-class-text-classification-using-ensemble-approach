{"title": "Learning Activation Functions to Improve Deep Neural Networks", "tag": ["cs.NE", "cs.CV", "cs.LG", "stat.ML"], "abstract": "Artificial neural networks typically have a fixed, non-linear activation function at each neuron. We have designed a novel form of piecewise linear activation function that is learned independently for each neuron using gradient descent. With this adaptive activation function, we are able to improve upon deep neural network architectures composed of static rectified linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%), CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs boson decay modes.", "text": "artiﬁcial neural networks typically ﬁxed non-linear activation function neuron. designed novel form piecewise linear activation function learned independently neuron using gradient descent. adaptive activation function able improve upon deep neural network architectures composed static rectiﬁed linear units achieving state-of-theart performance cifar- cifar- benchmark high-energy physics involving higgs boson decay modes. deep learning artiﬁcial neural networks enabled rapid progress applications engineering basic science usually parameters linear components learned data nonlinearities pre-speciﬁed logistic tanh rectiﬁed linear max-pooling function. sufﬁciently large neural network using common nonlinear functions approximate arbitrarily complex functions ﬁnite networks choice nonlinearity affects learning dynamics network’s expressive power. designing activation functions enable fast training accurate deep neural networks active area research. rectiﬁed linear activation function saturate like sigmoidal functions made easier quickly train deep neural networks alleviating difﬁculties weight-initialization vanishing gradients. another recent innovation maxout activation function achieved state-of-the-art performance multiple machine learning benchmarks maxout activation function computes maximum linear functions property approximate convex function input. springenberg riedmiller replaced function probabilistic function gulcehre explored activation function replaces function norm. however type activation function signiﬁcant impact learning space possible functions hardly explored. explore space learn activation function training. previous efforts largely focused genetic evolutionary algorithms attempt select activation function neuron pre-deﬁned set. recently turner miller combined strategy single scaling parameter learned training. paper propose powerful adaptive activation function. parametrized piecewise linear activation function learned independently neuron using gradient descent represent convex non-convex functions input. experiments demonstrate like piecewise linear activation functions works well training deep neural networks obtain state-of-the-art performance multiple benchmark deep learning tasks. result piecewise linear activation function. number hinges hyperparameter learned using standard gradient descent advance variables training. variables determine locations hinges. number additional parameters must learned using units total number hidden units network. number small compared total number weights typical networks. figure shows example functions note unlike maxout class functions learned single unit includes non-convex functions. fact large enough approximate arbitrarily complex continuous functions subject conditions theorem implies reconstruct piecewise-linear function subset real line conditions constrain behavior linear gets large small. ﬁrst condition less restrictive seem. neural networks generally interest input linear function linear function effectively restores degrees freedom eliminated constraining rightmost segment unit slope bias figure sample activation functions obtained changing parameters. notice ﬁgure shows activation function also non-convex. asymptotically activation functions tend plots. ﬁrst term slope range elsewhere. element summation term equation slope range elsewhere. last three terms together slope elsewhere. continuous slopes match almost everywhere easily veriﬁed thus conclude observe maxout units network-in-network learn nonlinear activation function units require many parameters difference allows units applied different ways maxout network-in-network nonlinearities small number parameters needed tune unit makes practical train convolutional networks apply different nonlinearities point feature would completely impractical either maxout networks network-in-network approaches. incorporating multiple linear functions increases expressive power maxout units allowing approximate arbitrary convex functions allowing difference pair maxout units approximate arbitrary functions. networks maxout units particular weight-tying scheme reproduce output unit. terms equation positive coefﬁcients term) convex function terms negative coefﬁcients concave function. could approximate convex part maxout unit concave part another maxout unit standard maxout network however vectors tied. implementing units using maxout network would require learning times many parameters size maxout layer’s input vector. whenever expressive power unit sufﬁcient using complex maxout units therefore waste computational modeling power. network-in-network. proposed replacing simple rectiﬁed linear activation convolutional networks fully connected network whose parameters learned data. mlpconv layer couples outputs ﬁlters applied patch permits arbitrarily complex transformations inputs. depth-m mlpconv layer produces output vector input patch series transformations function maps hinge output indices ﬁlter indices coefﬁcient aggressive weight-tying scheme dramatically reduces number parameters used mlpconv layer. waste computational modeling power network-in-network wherever unit would sufﬁce. however network-in-network things units cannot—in particular efﬁciently couples summarizes outputs multiple ﬁlters. beneﬁts architectures replacing rectiﬁed linear units mlpconv layer units. experiments performed using software package caffe hyperparameter controls complexity activation function determined using validation parameters regularized penalty scaled dataset. without penalty optimizer free choose large values balanced small weights would lead numerical instability. found adding penalty improved results. model ﬁles solver ﬁles available https//github.com/forestagostinelli/learnedactivation-functions-source/tree/master. cifar- cifar- datasets color images classes respectively. training images test images. images preprocessed subtracting mean values pixel training image. network cifar- loosely based network used convolutional layers ﬁlters respectively. kernel size padded pixels side. convolutional layers followed max-pooling average-pooling average-pooling layer respectively; kernel size stride fully connected layers units each. applied dropout network well. found applying dropout pooling layer increased classiﬁcation accuracy. probability unit dropped pooling layer pooling layers. probability dropped pooling layers respectively. probability unit dropped fully connected layers layers. ﬁnal layer softmax classiﬁcation layer. cifar- difference second pooling layer max-pooling instead average-pooling. baseline used rectiﬁed linear activation functions. using units cifar- cifar- table shows adding units improved baseline case cifar- almost case cifar-. terms relative difference decrease error rate respectively. also network-in-network architecture cifar- cifar- cifar-. improves performance datasets. also method augmented version cifar- cifar-. image around four pixel border zeros. training take random crops image randomly horizontal ﬂips. testing take center image. best knowledge results report data augmentation using network-in-network architecture best results reported cifar- cifar- method. equal compare leaky relus method different values pick best value one. possible values positive negative standard convolutional neural network architecture cifar- cifar-. network-in-network architecture cifar- cifar-. units consistently outperform leaky relu units showing value tuning nonlinearity table error rates cifar- cifar- without data augmentation. includes standard convolutional neural networks network-in-network architecture networks trained times using different random initializations report mean followed standard deviation parenthesis. best results bold. higgs boson decay higgs-to-τ decay dataset comes ﬁeld high-energy physics analysis data generated large hadron collider dataset contains million collision events characterized real-valued features describing momenta energies collision products. supervised learning task distinguish types physical processes higgs boson decays leptons background process produces similar measurement distribution. performance measured terms area receiver operating characteristic curve test million examples terms discovery signiﬁcance units gaussian using signal events background events relative uncertainty. baseline experiment layer neural network architecture whose architecture training hyperparameters optimized using spearmint algorithm used architecture training parameters except dropout used hidden layers reduce overﬁtting. units used table shows single network units achieves state-of-the-art performance increasing performance dropout-trained baseline ensemble neural networks table performance higgs boson decay dataset terms expected discovery signiﬁcance. networks trained times using different random initializations report mean followed standard deviation parenthesis. best results bold. table shows effect varying cifar- benchmark. also tested whether learning activation function important tried freezing activation functions random initialized positions allowing learn. results show learning activations opposed keeping ﬁxed results better performance. diversity adaptive piecewise linear functions visualized plotting sample neurons. figures show adaptive piecewise linear functions cifar- higgs→ experiments along random initialization function. ﬁgure layer activation functions plotted. greater variance learned activations cifar- cifar-. greater variance learned activations higgs→ cifar-. case higgs→ trend seen variance decreases higher layers. introduced novel neural network activation function neuron computes independent piecewise linear function. parameters neuron-speciﬁc activation function learned gradient descent along network’s weight parameters. experiments demonstrate learning activation functions lead signiﬁcant performance improvements deep neural networks without signiﬁcantly increasing number parameters. furthermore networks learn diverse activation functions suggesting standard one-activation-function-ﬁts-all approach suboptimal. agostinelli supported fellowship. work done internship adobe. also wish acknowledge support nvidia corporation donation tesla used research grant iis- google faculty research award baldi thanks yuzo kanomata computing support. glorot xavier bordes antoine bengio yoshua. deep sparse rectiﬁer networks. proceedings international conference artiﬁcial intelligence statistics. jmlr w&cp volume volume gulcehre caglar kyunghyun pascanu razvan bengio yoshua. learned-norm pooling deep feedforward recurrent neural networks. machine learning knowledge discovery databases springer hannun awni case carl casper jared catanzaro bryan diamos greg elsen erich prenger ryan satheesh sanjeev sengupta shubho coates adam andrew deep hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. jarrett kevin kavukcuoglu koray ranzato lecun yann. best multi-stage architecture object recognition? computer vision ieee international conference ieee yangqing shelhamer evan donahue jeff karayev sergey long jonathan girshick ross guadarrama sergio darrell trevor. caffe convolutional architecture fast feature embedding. arxiv preprint arxiv. krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems lusci alessandro pollastri gianluca baldi pierre. deep architectures deep learning chemoinformatics prediction aqueous solubility drug-like molecules. journal chemical information modeling srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research stollenga marijn masci jonathan gomez faustino schmidhuber j¨urgen. deep networks internal selective attention feedback connections. advances neural information processing systems", "year": 2014}