{"title": "A Stable Multi-Scale Kernel for Topological Machine Learning", "tag": ["stat.ML", "cs.CV", "cs.LG", "math.AT"], "abstract": "Topological data analysis offers a rich source of valuable information to study vision problems. Yet, so far we lack a theoretically sound connection to popular kernel-based learning techniques, such as kernel SVMs or kernel PCA. In this work, we establish such a connection by designing a multi-scale kernel for persistence diagrams, a stable summary representation of topological features in data. We show that this kernel is positive definite and prove its stability with respect to the 1-Wasserstein distance. Experiments on two benchmark datasets for 3D shape classification/retrieval and texture recognition show considerable performance gains of the proposed method compared to an alternative approach that is based on the recently introduced persistence landscapes.", "text": "particularly popular method since captures birth death times topological features e.g. connected components holes etc. multiple scales. information summarized persistence diagram multiset points plane. feature persistent homology stability small changes input data lead small changes wasserstein distance associated persistence diagrams considering discrete nature topological information existence well-behaved summary perhaps surprising. note persistence diagrams together wasserstein distance form metric space. thus possible directly employ persistent homology large class machine learning techniques require hilbert space structure like pca. obstacle typically circumvented deﬁning kernel function domain containing data turn deﬁnes hilbert space structure implicitly. wasserstein distance naturally lead valid kernel show possible deﬁne kernel persistence diagrams stable w.r.t. -wasserstein distance. main contribution paper. contribution. propose multiscale kernel persistence diagrams kernel deﬁned l-valued feature based ideas scale space theory show feature lipschitz continuous respect -wasserstein distance thereby maintaining stability property persistent homology. scale parameter kernel controls robustness noise tuned data. investigate detail theoretical properties kernel demonstrate applicability shape classiﬁcation/retrieval texture recognition benchmarks. methods leverage topological information computer vision medical imaging methods roughly grouped categories. ﬁrst category identify previous work directly utilizes topological information address speciﬁc problem topologyguided segmentation. second category identify approaches indirectly topological information. topological data analysis oﬀers rich source valuable information study vision problems. lack theoretically sound connection popular kernel-based learning techniques kernel svms kernel pca. work establish connection designing multi-scale kernel persistence diagrams stable summary representation topological features data. show kernel positive deﬁnite prove stability respect -wasserstein distance. experiments benchmark datasets shape classiﬁcation/retrieval texture recognition show considerable performance gains proposed method compared alternative approach based recently introduced persistence landscapes. many computer vision problems data piped complex processing chains order extract information used address high-level inference tasks recognition detection segmentation. extracted information might form low-level appearance descriptors e.g. sift higher-level nature e.g. activations speciﬁc layers deep convolutional networks recognition problems instance customary feed consolidated data discriminant classiﬁer popular support vector machine kernelbased learning technique. substantial progress extracting encoding discriminative information recently people started looking topological structure data additional source information. emergence topological data analysis computational tools eﬃciently identifying topological structure become readily available. since then several authors demonstrated capture characteristics data methods often fail provide figure visual data analyzed using persistent homology roughly speaking persistent homology captures birth/death times topological features form persistence diagrams. contribution deﬁne kernel persistence diagrams enable theoretically sound summary representations framework kernel-based learning techniques popular computer vision community. representative ﬁrst category skraba adapt idea persistence-based clustering segmentation method surface meshes shapes driven topological information persistence diagram. persistence information restore called handles i.e. topological cycles already existing segmentations left ventricle extracted computed tomography images. diﬀerent segmentation setup chen propose directly incorporate topological constraints random-ﬁeld based segmentation models. second category approaches chung pachauri investigate problem analyzing cortical thickness measurements surface meshes human cortex order study developmental neurological disorders. contrast persistence information used directly rather descriptor discriminant classiﬁer order distinguish normal control patients patients alzheimer’s disease/autism. step training classiﬁer topological information typically done rather adhoc manner. instance persistence diagram ﬁrst rasterized regular grid kernel-density estimate computed eventually vectorized discrete probability density function used feature vector train using standard kernels however unclear resulting kernel-induced distance behaves respect existing metrics properties stability aﬀected. approach directly uses well-established distances persistence diagrams recognition recently proposed besides bottleneck wasserstein distance authors employ persistence landscapes corresponding distance experiments. results expose complementary nature persistence information combined traditional bag-offeature approaches. empirical study sec. inspired primarily focus development kernel; combination methods straightforward. order enable persistence information machine learning setups adcock propose compare persistence diagrams using feature vector motivated algebraic geometry invariant theory. features deﬁned using algebraic functions birth death values persistence diagram. conceptual point view bubenik’s concept persistence landscapes probably closest ours another kind feature persistence diagrams. persistence landscapes explicitly designed machine learning algorithms draw connection work sec. show fact admit deﬁnition valid positive deﬁnite kernel. moreover persistence landscapes well approach represent computationally attractive alternatives bottleneck wasserstein distance require solution matching problem. persistence diagrams. persistence diagrams concise description topological changes occuring growing sequence shapes called ﬁltration. particular growth shape holes diﬀerent dimension appear disappear. intuitively k-dimensional hole born time ﬁlled time gives rise point persistence diagram. persistence diagram thus multiset points formally persistence diagram natural metric associated persistence diagrams called bottleneck distance. loosely speaking distance diagrams expressed minimizing largest distance corresponding points bijections diagrams. formally persistence diagrams augmented adding point diagonal countably inﬁnite multiplicity. bottleneck distance ranges bijections individual elements individual elements note taking limit yields bottleneck distance therefore deﬁne following result bounding p-wasserstein distance terms distance note that strictly speaking stability result sense lipschitz continuity since establishes h¨older continuity. moreover gives constant upper bound wasserstein distance kernels. given function kernel exists hilbert space called feature space called feature equivalently kernel symmetric positive deﬁnite kernels allow apply machine learning algorithms operating hilbert space applied general settings strings graphs case persistence diagrams. figure function persistence diagram local minima create connected component corresponding sublevel local maxima merge connected components. pairing birth death shown persistence diagram. note every hole disappear ﬁltration. holes give rise essential features naturally represented points form diagram. essential features therefore capture topology ﬁnal shape ﬁltration. present work consider features part persistence diagram. moreover persistence diagrams assumed ﬁnite usually case persistence diagrams coming data. filtrations functions. standard obtaining ﬁltration consider sublevel sets function deﬁned domain easy sublevel sets indeed form ﬁltration parametrized denote resulting persistence diagram fig. illustration. example consider grayscale image rectangular domain image grayscale value point domain sublevel would thus consist pixels value certain threshold another example would piecewise linear function triangular mesh popular heat kernel signature another commonly used ﬁltration arises point clouds embedded considering distance function minp∈p sublevel sets function unions balls around computationally usually replaced equivalent constructions called alpha shapes. stability. crucial aspect persistence diagram function stability respect perturbations fact stability guarantees infer information function persistence diagram presence noise. formally consider metric spaces deﬁne stability lipschitz continuity map. requires choices metrics functions mirrored diagonal. shown restricting solution extended problem yields solution original equation. given convolving initial condition gaussian kernel denote persistence diagrams augmented points diagonal. note augmenting diagrams points diagonal change values seen since unaugmented persistence diagrams assumed ﬁnite matching achieves inﬁmum deﬁnition wasserstein distance kernel induces pseudometric distance feature space. call kernel stable w.r.t. metric constant note equivalent lipschitz continuity feature map. stability kernel particularly useful classiﬁcation problems assume exists separating hyperplane classes data points margin data points perturbed still separates classes margin persistence scale-space kernel motivate deﬁnition point persistence diagrams i.e. multisets points possess hilbert space structure however persistence diagram uniquely represented dirac delta distributions point since dirac deltas functionals hilbert space embed persistence diagrams hilbert space adopting point view. unfortunately induced metric take account distance points diagonal therefore cannot robust perturbations diagrams. motivated scale-space theory address issue using dirac deltas initial condition heat diﬀusion problem dirichlet boundary condition diagonal. solution partial diﬀerential equation function chosen scale parameter following paragraphs deﬁne persistence scale space kernel derive simple formula evaluating prove stability w.r.t. -wasserstein distance. deﬁnition denote space diagonal denote dirac delta centered point given persistence diagram consider solution partial diﬀerential equation evaluate kernel proposed sec. investigate conceptual diﬀerences persistence landscapes sec. consider performance context shape classiﬁcation/retrieval texture recognition sec. comparison persistence landscapes bubenik introduced persistence landscapes representation persistence diagrams functions banach space construction mainly intended statistical computations enabled vector space structure hilbert space structure construct kernel analogously purpose work refer kernel persistence landscape kernel denote corresponding feature map. kernel-induced distance denoted dkl. bubenik shows stability w.r.t. weighted version wasserstein distance summarized theorem persistence diagrams better understanding stability results given theorems present discuss thought experiments. ﬁrst experiment diagrams point points move away diagonal increasing maintaining euclidean distance other. consequently asymptotically approach constant contrast grows order particular unbounded. means emphasizes points high persistence diagrams reﬂected weighting term second experiment compare persistence diagrams data samples ﬁctive classes illustrated fig. ﬁrst consider dkl). seen previous experiment dominated variations points high persistence. similarly also dominated points long suﬃciently large. hence instances classes would inseparable nearest neighbor setup. contrast overemphasize points high persistence thus allow distinguish classes refer left-hand side persistence scale space distance note right hand side decreases increases. adjusting accordingly allows counteract inﬂuence noise input data causes increase sec. tuning data beneﬁcial overall performance machine learning methods. natural question arising theorem whether stability result extends answer question ﬁrst note kernel additive call kernel persistence diagrams additive choosing additive kernel trivial next theorem establishes theorem sharp sense non-trivial additive kernel stable w.r.t. p-wasserstein distance range time parameters increasing value. speciﬁc choice obtain piecewise linear function surface mesh object. discussed sec. compute persistence diagrams induced ﬁltrations dimensions texture classiﬁcation compute clbp descriptors results reported rotationinvariant versions clbp-single clbp-magnitude operator neighbours radius operators produce scalar-valued response image interpreted weighted cubical cell complex lower star ﬁltration used compute persistence diagrams; details. types input data persistence diagrams obtained using dipha directly handle meshes images. standard soft margin c-svm classiﬁer implemented libsvm used classiﬁcation. cost factor tuned using ten-fold cross-validation training data. kernel cross-validation includes kernel scale tables list classiﬁcation results shrec results averaged crossvalidation runs using random training/testing splits roughly equal class distribution. report results -dimensional features only; -dimensional features lead comparable performance. real synthetic data observe leads consistent improvements choices gains even range cases improvements relatively small. explained fact varying time essentially varies smoothness input data. scale allows compensate—at classiﬁcation stage—for unfavorable smoothness settings certain extent sec. contrast capability essentially relies suitably preprocessed input data. choices fact lead classiﬁcation accuracies close however using carefully adjust time parameter corresponding changes inreport results vision tasks persistent homology already shown provide valuable discriminative information shape classiﬁcation/retrieval texture image classiﬁcation. purpose experiments outperform state-of-the-art problems would rather challenging exclusively using topological information demonstrate advantages dkl. datasets. shape classiﬁcation/retrieval shrec benchmark fig. consists synthetic real shapes given meshes. synthetic part data contains meshes humans diﬀerent poses; real part contains meshes humans diﬀerent poses. meshes full resolution i.e. without mesh decimation. classiﬁcation objective distinguish diﬀerent human models i.e. -class problem shrec -class problem shrec texture recognition outex benchmark downsampled pixel images. benchmark provides predeﬁned training/testing splits classes equally represented images training testing. data. undesirable situations since computation meshes large number vertices quite time-consuming sometimes might even access meshes directly. improved classiﬁcation rates indicate using additional degree freedom fact beneﬁcial performance. addition classiﬁcation experiments report shape retrieval performance using standard evaluation measures allows assess behavior kernel-induced distances dkl. brevity nearest-neighbor performance listed table using shape query shape once nearestneighbor performance measures often top-ranked shape retrieval result belongs class query. study eﬀect tuning scale column lists maximum nearest-neighbor performance achieved range scales. results similar classiﬁcation experiment. however speciﬁc settings time performs better dkσ. noted sec. explained changes smoothness input data induced diﬀerent times another observation nearest-neighbor performance quite unstable around result respect example drops shrec results context existing works shape retrieval table also lists three entries benchmark. real synthetic data ranks among entries. indicates topological persistence alone rich source discriminative information particular problem. addition since assess time parameter time performance could potentially improved elaborate fusion strategies. texture recognition results averaged training/testing splits outex benchmark. table lists performance classiﬁer using -dimensional features higher-dimensional features informative problem. comparison table also lists performance trained normalized histograms clbp-s/m responses using kernel. first table evident performs better large margin gains accuracy. second also apparent that problem topological information alone competitive svms using simple orderless operator response histograms. however results show combination persistence information conventional bag-of-feature representations leads stateof-the-art performance. indicates complementary nature topological features also suggests kernel combinations could lead even greater gains including proposed kernel assess stability cross-validation strategy select speciﬁc fig. illustrates classiﬁcation performance function latter. given smoothness performance curve seems unlikely parameter selection cross-validation sensitive finally remark tuning drawbacks case shape classiﬁcation experiments. while principle could smooth textures clbp response images even tweak radius clbp operators strategies would require changes beginning processing pipeline. contrast adjusting scale done pipeline classiﬁer training. shown theoretically empirically proposed kernel exhibits good behavior tasks like shape classiﬁcation texture recognition using svm. moreover ability tune scale parameter proven beneﬁcial practice. possible direction future work would address computational bottlenecks order enable application large scale scenarios. could include leveraging additivity stability order approximate value kernel within given error bounds particular reducing number distinct points summation -wasserstein distance well established proven useful applications hope improve understanding stability persistence diagrams w.r.t. wasserstein distance beyond previous estimates. result would extend stability kernel persistence diagrams underlying data leading full stability proof topological machine learning.", "year": 2014}