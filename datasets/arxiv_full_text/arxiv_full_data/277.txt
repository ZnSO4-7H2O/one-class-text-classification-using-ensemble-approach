{"title": "A Supervised Approach to Extractive Summarisation of Scientific Papers", "tag": ["cs.CL", "cs.AI", "cs.NE", "stat.AP", "stat.ML"], "abstract": "Automatic summarisation is a popular approach to reduce a document to its main arguments. Recent research in the area has focused on neural approaches to summarisation, which can be very data-hungry. However, few large datasets exist and none for the traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents. In this paper, we introduce a new dataset for summarisation of computer science publications by exploiting a large resource of author provided summaries and show straightforward ways of extending it further. We develop models on the dataset making use of both neural sentence encoding and traditionally used summarisation features and show that models which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods.", "text": "automatic summarisation popular approach reduce document main arguments. recent research area focused neural approaches summarisation data-hungry. however large datasets exist none traditionally popular domain scientiﬁc publications opens challenging research avenues centered encoding large complex documents. paper introduce dataset summarisation computer science publications exploiting large resource author provided summaries show straightforward ways extending further. develop models dataset making neural sentence encoding traditionally used summarisation features show models encode sentences well local global context perform best signiﬁcantly outperforming well-established baseline methods. automatic summarisation task reducing document main points. streams summarisation approaches extractive summarisation copies parts document form summary abstractive summarisation reads document generates summary contain phrases appearing document. abstractive summarisation difﬁcult task useful domains sentences taken context good basis forming grammatical coherent summary like novels. here concerned summarising scientiﬁc publications. since scientiﬁc publications technical domain fairly regular explicit language task extractive summarisation. although work summarisation scientiﬁc publications before existing datasets small consisting tens documents small datasets sufﬁcient learn supervised summarisation models relying neural methods sentence document encoding usually trained many thousands documents paper introduce dataset automatic summarisation computer science publications used abstractive extractive summarisation. consists documents easily extended automatically additional domains. dataset created exploiting existing resource sciencedirect many journals require authors submit highlight statements along manuscripts. using highlight statements gold statements proven good gold standard news documents dataset offers many exciting research challenges best encode large technical documents largely ignored current research. paper title statistical estimation names https servers domain name graphs highlights present domain name graph formal expression keep track cname chains characterize dynamic diverse nature mechanisms deployments. develop framework called service-ﬂow works dng.sfmap estimates hostname https server given pair client server addresses. statistically estimate hostname even associating queries unobserved caching mechanisms extensive analysis using real packet traces demonstrate sfmap framework establishes good estimation accuracies outperform state-of-the technique called dnhunter. also identify optimized setting sfmap framework. experiment results suggest success sfmap lies fact complement incomplete information leveraging graph structure. cope large-scale measurement data introduce techniques make sfmap framework scalable. validate effectiveness approach using large-scale trafﬁc data collected gateway point internet access links summary statements highlighted context section main text contributions work present novel methodology aims infer hostnames https ﬂows given three research challenges shown above. contributions work summarized follows. present domain name graph formal expression keep track cname chains characterize dynamic diverse nature mechanisms deployments develop framework called service-ﬂow works dng. sfmap estimates hostname https server given pair client server addresses. statistically estimate hostname even associating queries unobserved caching mechanisms extensive analysis using real packet traces demonstrate sfmap framework establishes good estimation accuracies outperform state-of-the technique called dn-hunter also identify optimized setting sfmap framework. experiment results suggest success sfmap lies fact complement incomplete information leveraging graph structure. cope large-scale measurement data introduce techniques make sfmap framework scalable. validate effectiveness approach using large-scale trafﬁc data collected gateway point internet access links. remainder paper organized follows section summarizes related work. taking inspiration previous work summarising scientiﬁc literature introduce metric feature abstractrouge used extract summaries exploiting abstract paper benchmark several neural well traditional summarisation methods dataset simple features model global context summary statement contribute overall score compare best performing system several well-established baseline methods elaborate methods model global context show best performing model outperforms extractive summarisation expect research documented paper relevant beyond document summarisation community tasks space automatically understand scientiﬁc publications keyphrase extraction semantic relation extraction topic classiﬁcation scientiﬁc articles release novel dataset extractive summarisation comprised computer science publications. publications obtained sciencedirect publications grouped domains computer science them. such dataset could easily extended domains. example document shown table paper dataset guaranteed title abstract author written highlight statements author deﬁned keywords. highlight statements sentences effectively convey main takeaway paper good gold summary keyphrases topics paper. abstract highlights thought summary paper. since highlight statements unlike sentences abstract generally dependencies them gold summary statements developing summarisation models following hermann nallapati approaches news summarisation. problem formulation shown sentences good summaries even taken context surrounding sentences. highlights characteristic relying previous subsequent sentences make sense. consequently frame extractive summarisation task binary sentence classiﬁcation task assign sentence document label training data therefore list sentences sentence features encode context label stored randomly ordered list. creation training testing data used papers create different datasets cspubsum cspubsumext cspubsumext cspubsum extended highlightrouge. number training items given table cspubsum dataset’s positive examples highlight statements paper. equal number negative examples sampled randomly bottom sentences worst summaries paper measured rouge-l resulting training instances. cspubsum test formed full papers rather randomly ordered list training sentences. used measure summary quality summariser accuracy trained models. cspubsumext cspubsum dataset drawbacks order magnitude behind comparable large summarisation datasets labels sentences context main body paper. generate additional training examples paper highlightrouge ﬁnds sentences similar highlights. results instances cspubsumext train instances cspubsumext test. cspubsumext test used test accuracy trained models. trained models used summarisers whose quality tested cspubsum test rouge-l metric highlightrouge method used generate additional training data dataset using similar approach input takes gold summary body text ﬁnds sentences within text give best rouge-l score relation highlights like oracle summariser would sentences represent ideal sentences extract paper extractive summary. select sentences give highest rouge-l score highlights paper positive instances combine highlights give positive examples paper. equal number negative instances sampled lowest scored sentences match. generating data using highlightrouge sentences abstracts papers included training examples. abstract already summary; goal extract salient sentences main paper supplement abstract preexisting summary. abstractrouge used feature summarisation. metric presented work exploits known structure paper making abstract preexisting summary. idea abstractrouge sentences good summaries abstract also likely good summaries highlights. abstractrouge score sentence simply rouge-l score sentence abstract. intuition comparing sentences abstract often used summarising scientiﬁc literature e.g. however authors generally encode sentences abstract tf-idf vectors compare them rather directly comparing evaluation metric. seem somewhat like cheating scientiﬁc papers guaranteed abstract makes sense exploit much possible. summariser features sentences dataset randomly ordered readily available context sentence surrounding sentences provide local global context features used sentence described below. contextual features contribute achieving best performances. recent work summarisation uses many features choose minimal features focus learning data feature engineering although could potentially improve results. location authors kavila radhika chose summary sentences abstract introduction conclusion thinking salient summaries; show certain sections within paper relevant summaries others therefore assign sentences integer location different sections highlight abstract introduction results discussion analysis method conclusion else. location features used ways previous work summarising scientiﬁc literature; visser wieling extract sentence location features based headings occurred beneath teufel moens divide paper equal parts assign sentence location based segment occurred attempt capture distinct zones paper. scientiﬁc papers features used title score. feature differs slightly visser wieling main paper title whereas visser wieling section headings. calculate feature non-stopwords sentence contains overlap title paper counted. keyphrase score authors sp¨arck jones refer keyphrase score useful summarisation feature. feature uses author deﬁned keywords counts many keywords sentence contains idea important sentences contain keywords. tf-idf term frequency inverse document frequency measure relevant word document takes account frequency word current document frequency word background corpus documents; word frequent document infrequent corpus likely important document. tfidf calculated word sentence averaged sentence give tf-idf score sentence. stopwords ignored. document tf-idf document tf-idf calculates metric tf-idf uses count words sentence term frequency count words rest paper background corpus. gives representation important word sentence relation rest document. sentence length teufel created binary feature sentence longer threshold. simply include length sentence feature; attempt capture intuition short sentences unlikely good summaries cannot possibly convey much information longer sentences. sentence encoded rnn. vector representation abstract paper created averaging word vectors every non-stopword word abstract. since abstract already summary models containing neural network multiple hidden layers. models ending ensemble. non-linearity functions rectiﬁed linear units chosen faster training time recent popularity wordvec word vector models wordvecaf single layer networks. wordvec takes input sentence represented averaged word vector numbers. wordvecaf takes sentence average vector abstract average vector handcrafted features giving -dimensional vector classiﬁcation. lstm-rnn method snet takes input ordered words sentence represented dimensional vectors feeds bi-directional long-short term memory cells hidden units dropout prevent overﬁtting. dropout probability thought near optimal many tasks output forwards backwards lstms concatenated projected classes. lstm features sfnet sfnet processes sentence lstm previous paragraph passes output fully connected layer dropout. handcrafted features treated separate inputs network passed fully connected layer. outputs lstm features hidden layer concatenated projected classes. sections paper generally represent good summaries take sections summary paper. precisely kavila radhika constructing summaries abstract introduction conclusion. approach works intuition certain sections relevant summaries. understand much section contributes gold summary compute rouge-l score sentence compared gold summary average sentence-level rouge-l scores section. rouge-type metrics metrics determine relevant sentence summary. throughout data approximately occurrences authors directly copying sentences within main text highlight statements. recording sections paper sentences came determine sections authors frequently copy sentences highlights relevant summary. referred copy/paste score paper. figure shows average rouge score section papers normalised copy/paste score. title highest rouge score relation gold summary intuitive title convey information research single line. surprising result introduction third-lowest rouge score relation highlights. hypothesis introduction would ranked highest abstract title designed give reader basic background problem. indeed introduction second highest copy/paste score sections. reason introduction rouge score high copy/paste score likely length. introduction tends longer sections still relatively simple level compared method thus potential sentences author highlights giving high copy/paste score. however would also sentences good summaries thus reduce overall average rouge score introduction. figure comparison average rouge scores section normalised copy/paste score section detailed section wider bars ascending order rouge scores section thinner overlaid bars copy/paste count. figure shows comparisons best model developed well-established external baseline methods. model seen signiﬁcantly outperform methods including graph-based methods take account global context lexrank textrank probabilistic methods klsum methods based singular value decomposition simple methods based counting sumbasic encouraging result showing methods combine neural sentence encoding simple features representing global context positional information effective modelling extractive summarisation problem. figure shows performance models developed work measured terms accuracy rouge-l cspubsumext test cspubsum test respectively. architectures combination sentence encoding additional features performed best measures. lstm encoding outperforms models based averaged word embeddings accuracy rouge points. shows ordering words sentence clearly makes difference deciding sentence summary sentence. particularly interesting result shows encoding sentence superior simple arithmetic provides alternative recursive autoencoder proposed performed worse vector addition. another interesting result highest accuracy cspubsumext test translate best rouge score cspubsum test although strongly correlated safnet achieved highest accuracy cspubsumext test however worse abstractrouge summariser cspubsum test. likely imperfections training data. small fraction sentences training data mislabelled examples highlights exacerbated highlightrouge method. leads confusion summarisers capable learning complex enough representations classify mislabelled data correctly. manually examined sentences cspubsumext incorrectly classiﬁed safnet. those mislabelled examples. primary cause false positives lack context long range dependency important causes false positives mislabelled data failure recogfigure comparison accuracy model cspubsumext test rouge-l score cspubsum test. rouge scores given percentage oracle summariser score highest score achievable extractive summariser papers. wider bars ascending order rouge scores. statistically signiﬁcant difference performance four summarisers highest scoring nise mathematically intense sentences good summaries lack context sentences require information sentences immediately make sense. example sentence performance systems commonly evaluated using data matrix classiﬁed positive make sense context clear systems sentence referring long-range dependency sentences refer entity described elsewhere paper e.g. sentences referring ﬁgures. likely classiﬁed summary statements using models trained automatically generated training data highlightrouge large overlap summary. primary cause false negatives mislabelled data failure recognise entailment observation conclusion mislabelled data usually caused presence sentences highlights form approach clear without context. sentences labelled positive part multi-line summaries difﬁcult determine automatically. failure recognise entailment observation conclusion sentence form entity seems small effect example summariser learnt information useful summary possibly occluded mislabelled data. safnet sfnet achieve high accuracy automatically generated cspubsumext test dataset though lower rouge score simpler methods fnet cspubsum test. likely overﬁtting simpler summarisation models less prone option solve would manually improve cspubsumext labels change form training data. rather using randomised list sentences trying learn objectively good summaries training example could sentences order paper classiﬁed either summary summary. best summary sentences within paper would chosen using highlightrouge used training data approach similar nallapati could used read whole paper sequentially solve issue long-range dependencies context. issue faced safnet affect ensemble methods much predictions weighted hyperparameter tuned cspubsum test rather cspubsumext. ensemblers ensure good performance test sets models adapted perform better different examples. summary model performances show that reading sentence sequentially superior averaging word vectors simple features model global context positional information effective high accuracy automatically generated test guarantee high rouge-l score gold test although correlated. likely caused models overﬁtting data small signiﬁcant proportion mislabelled examples byproduct generated automatically. cspubsum train fnet summariser sfnet suffer statistically signiﬁcant drops performance using unexpanded dataset although interestingly safnet suggesting stable model two. drops performance however show using method described increase amount available training data improves model performance summarisation. work suggested abstractrouge metric feature figure compares performance models trained without shows things abstractrouge metric improve performance summarisation techniques based feature engineering; learning representation sentence directly text done safnet sfnet well learning features results stable model. model still able make good predictions even abstractrouge available training meaning models need rely presence abstract. extractive summarisation methods early work extractive summarisation focuses exclusively easy compute statistics e.g. word frequency location document tf-idf supervised learning methods classify sentences document binarily summary sentences soon became popular exploration cues sentence position sentence length words title presence proper nouns word frequency event cues followed. setting since documents large computationally challenging read whole publication neural sequence encoder. work therefore encode target sequence global context simpler features. leave fully neural approaches encoding publications future work. paper introduced dataset summarisation computer science publications substantially larger comparable existing datasets exploiting existing resource. showed performance several extractive summarisation models dataset encode sentences global context position signiﬁcantly outperform well-established summarisation methods. introduced metric abstractrouge show increases summarisation performance. finally show dataset extended automatically increases performance. remaining challenges better model global context summary statement better capture crosssentence dependencies. figure comparison rouge scores features only safnet sfnet models trained without abstractrouge evaluated cspubsum test. fnet classiﬁer suffers statistically signiﬁcant decrease performance without abstractrouge metric. document understanding conference text analysis conference proposed single-document summarisation whereas datasets multi-document summarisation datasets contain roughly documents. existing datasets summarisation scientiﬁc documents aware small. kupiec used publications cl-scisumm contains publications. ronzano saggion used papers kupiec used visser wieling used papers. largest known scientiﬁc paper dataset used teufel moens used subset papers larger corpus articles. dataset introduce paper knowledge large dataset extractive summarisation scientiﬁc publications. size dataset enables training data-intensive neural methods also offers exciting research", "year": 2017}