{"title": "Unsupervised Feature Learning with C-SVDDNet", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "In this paper, we investigate the problem of learning feature representation from unlabeled data using a single-layer K-means network. A K-means network maps the input data into a feature representation by finding the nearest centroid for each input point, which has attracted researchers' great attention recently due to its simplicity, effectiveness, and scalability. However, one drawback of this feature mapping is that it tends to be unreliable when the training data contains noise. To address this issue, we propose a SVDD based feature learning algorithm that describes the density and distribution of each cluster from K-means with an SVDD ball for more robust feature representation. For this purpose, we present a new SVDD algorithm called C-SVDD that centers the SVDD ball towards the mode of local density of each cluster, and we show that the objective of C-SVDD can be solved very efficiently as a linear programming problem. Additionally, traditional unsupervised feature learning methods usually take an average or sum of local representations to obtain global representation which ignore spatial relationship among them. To use spatial information we propose a global representation with a variant of SIFT descriptor. The architecture is also extended with multiple receptive field scales and multiple pooling sizes. Extensive experiments on several popular object recognition benchmarks, such as STL-10, MINST, Holiday and Copydays shows that the proposed C-SVDDNet method yields comparable or better performance than that of the previous state of the art methods.", "text": "method maps input data feature representation simply associating data point nearest cluster center. parameter involved k-means based method i.e. number clusters hence model easy practice. coates shows k-means based feature learning network capable achieve superior performance compared sparse autoencoder sparse however k-means based feature representation terse take non-uniform distribution cluster size account intuitively clusters containing data likely part features higher inﬂuential power compared smaller ones. paper proposed svdd based method address issues. idea method svdd measure density cluster resulted k-means clustering based robust feature representation built. actually k-means algorithm lacks robust deﬁnition size clusters since nearest center principle robust noise outliers commonly encountered real world applications. advocate svdd could good address issue. actually svdd widely used tool minimal closed spherical boundary describe data belonging target class therefore given cluster data expecting svdd generate ball containing normal data except outliers. performing procedure clusters k-means ﬁnally obtain svdd balls representation built. addition take cluster size account distance data ball’s surface instead center feature. possible problem method however come instability svdd’s center fact position mainly determined support vectors boundary noise data deviate center mode hence resulting svdd ball consistent data’s distribution used feature representation. address issue constraint original svdd objective function make model align better data. addition show modiﬁed svdd solved efﬁciently linear programming problem instead quadratic one. usually need compute hundreds clusters linear programming solution thus save large amounts time. proposed method extended adopting receptive ﬁelds different sizes capture multiscale information ranging detailed edge-like features part-level features. preliminary version work appeared feasibility effectiveness proposed c-svdd-based method abstract—in paper investigate problem learning feature representation unlabeled data using single-layer k-means network. k-means network maps input data feature representation ﬁnding nearest centroid input point attracted researchers’ great attention recently simplicity effectiveness scalability. however drawback feature mapping tends unreliable training data contains noise. address issue propose svdd based feature learning algorithm describes density distribution cluster kmeans svdd ball robust feature representation. purpose present svdd algorithm called csvdd centers svdd ball towards mode local density cluster show objective csvdd solved efﬁciently linear programming problem. additionally traditional unsupervised feature learning methods usually take average local representations obtain global representation ignore spatial relationship among them. spatial information propose global representation variant sift descriptor. architecture also extended multiple receptive ﬁeld scales multiple pooling sizes. extensive experiments several popular object recognition benchmarks stl- minst holiday copydays shows proposed c-svddnet method yields comparable better performance previous state methods. learning good feature representation unlabeled data make progress recognition classiﬁcation tasks attracted great attention interest academia industry recently. representative method deep learning approach goal learn multiple layers abstract representations data. among others typical method called convolutional neural network consists multiple trainable stages stacked other followed supervised classiﬁer many variations convnet network proposed well different vision tasks great success. methods layers representation usually obtained greedily training layer time lower level using unsupervised learning algorithm. hence performance single-layer learning effect ﬁnal representation. neural network based singlelayer methods autoencoder widely used usually many parameters adjust timeconsuming practice. motivates simple efﬁcient methods single-layer feature learning. among others k-means clustering algorithm commonly used unsupervised learning dong wang xiaoyang department computer science technology nanjing university aeronautics astronautics p.r. china. corresponding author xiaoyang coding local patch vector vk]t vlad simpliﬁed version difference signal patch ﬁlter deﬁned difference signals concatenated kdim vector feature representation. obviously vlad encode much richer information hence discriminative subsequent tasks object classiﬁcation. coates al.’s method best knowledge work ﬁrst deep unsupervised learning method based k-means method hence close connection aforementioned vlad methods. particularly learning ﬁlter bank instead using basin attraction like references calculating difference vectors utilized generate series feature maps ﬁlter. least potential advantages compared vlad encoded information even rich; feature maps preserve spatial information well hence whole procedure could repeated leading deep unsupervised learning architecture. x−ck mean elements activation function outputs feature average distance centroid model leads less sparse representation note triangle encoding strategy essentially allows learn distributed representation using simple k-means method instead complicated network-based methods hence saving much time training. coastes shows strategy actually leads comparable performance better than based network methods. however method take characteristics cluster consideration. actually number data point cluster usually different distribution data points cluster. believe differences would make difference feature representation well. unfortunately aforementioned k-means feature mapping scheme completely ignores uses position center feature encoding. shown fig. although data point distance centers clusters assigned different score remaining parts paper organized follows section preliminaries provided regarding unsupervised feature learning representation detail improved feature learning method section iii. section investigate performance method empirically several popular datasets. conclude paper section goal unsupervised feature learning automatically discover useful hidden patterns/features large datasets without relying supervisory signal learnt patterns utilized create representations facilitate subsequent supervised learning compared supervised learning unsupervised learning unique characteristics advantages. among others important used learn consistent patterns unlabelled data often free easy obtain. patterns distinguish noise since deﬁnition noise thought random variations presented data. implies many potential applications unsupervised learning e.g. transfer knowledge domain another related domain regularize behavior supervised algorithm represent data compact effective manner. reasons unsupervised learning regarded future deep learning many kinds unsupervised learning methods computer vision words vector linearly aggregated descriptors fisher vector typical pipeline unsupervised feature learning includes three steps. ﬁrst step train local ﬁlters unlabeled training data. usually done running k-means lots local patches sampled dataset using centers clusters ﬁlter bank. second step partition given image patches encode feature vectors using learnt ﬁlter bank. feature vectors ﬁnally combined normalized feature representation input image. follows give brief review methods. words variants simple basic unsupervised feature learning method model. model local ﬁlters usually centers clusters k-means. ﬁlters looked bins serves pool local patches nearest them. regarded hard voting method preserve enough local spatial information subsequent processing. compared improved feature encoding method named c-svdd adopt architecture relatively small dictionary. different learn ﬁlter banks feature encoding sift-based post-pooling processing procedure onto network essentially projects responses pooling operation compact robust representation space. assume dataset contains data objects {xi} ball described center radius goal svdd closed spherical boundary around given data points. order avoid inﬂuence outliers svdd actually faces tradeoff conﬂicting goals i.e. minimizing radius covering many data points possible. formulated following objective slack variable represents penalty related deviation i-th training data point outside ball user deﬁned parameter controlling degree regularization imposed objective. i.e. center ball linear combination data dual function svdd method understood type one-class boundary solely determined support vectors points. svdd allows summarize group data points nice robust way. hence natural svdd ball model cluster k-means thereby combining strength models. particular given data point ﬁrst compute distance surface svdd ball following modiﬁed triangle encoding method feature representation since former cluster much bigger latter. practice unequal clusters uncommon k-means method reliably grasp size clusters existence outliers. propose svdd based method describe density distribution cluster robust feature representation. section presenting overview proposed method give details centered-svdd method feature encoding compare k-means triangle encoding method. describe sift-based postpooling layer discuss extend method extract multi-scale information. typical single-layer network contains several components input image ﬁrst mapped feature maps using ﬁlter banks subjected pooling/subsampling operation condense information contained feature maps. finally pooled feature maps concatenated feature vector serves representation subsequent classiﬁcation/cluster tasks. several design options procedure size ﬁlter bank pooling grids major tradeoff make. generally speaking bigger ﬁlter banks help sample nearby representative points accurately cost yielding high-dimensional representation hence crude pooling/subsampling needed reduce dimensionality. overall type architecture emphasizes global aspects samples local ones actually coates show kind network able yield state results several challenging datasets hand works smaller ﬁlter banks highlight importance detailed local information constructing representation usually based complicated feature encoding strategy done pcanet fisher vector xj))n×n xi)n× objective function linear thus solved efﬁciently linear programming algorithm. since model centered towards mode distribution data points cluster named method c-svdd fig. shows difference svdd c-svdd left svdd right c-svdd. model aligns better density data points expected. also worth mentioning normalization parameter plays important role model larger value fig. using svdd ball cover clusters k-means svdd balls cover clusters different sizes respectively. test point encode feature using distance surface svdd ball. calculated subtracting length radius ball distance ball center hence svdd balls different size encoded features point would different. although svdd ball provides robust describe cluster data unwelcome property ball align well distribution data points cluster. illustrated fig. although svdd ball covers cluster well center biased region density. avoided since actually gives suboptimal estimates distribution cluster data. address issue inspired observation centers k-means always located corresponding mode local density propose shift svdd ball centroid data better distribution data cluster. objective function informative atom tends large major advantage c-svdd-based strategy capable exploit characteristic dictionary atoms effective feature encoding shown ﬁrst three rows fig. partially explains superior performance proposed c-svdd method compared k-means counterpart useful take brief discussion difference kinds feature maps i.e. k-meansbased triangle encoding c-svdd-based pilot experiment conducted. particularly learn small dictionary containing atoms using face images clustering zca-whitened patches randomly sampled faces take feature encoding. fig. illustrates face images used dictionary learning learnt atoms feature maps face images encoded k-means encoding method c-svdd encoding method respectively shown fig. fig. corresponding dictionary atom next column corresponding face. comparing feature maps shown fig. fig. c-svdd-based ones contain detailed information k-means feature maps ﬁrst three atoms responses last atoms largely suppressed method understand phenomenon plot entropy atom fig. ﬁgure shows entropy last atoms much smaller ﬁrst three ones indicates local appearance patterns captured last atoms much simpler ﬁrst three. hence atoms tend widely used many faces resulting reduced discriminative capability distinguishing different subjects. sense useful suppress responses also useful inspect distribution local facial patches attracted atoms. fig. gives results. seen distribution uniform number local patches attracted fourth atom signiﬁcantly larger atoms. result k-means encoding method feature maps yielded atom show much rich details others potentially indicating could play important roles others subsequent classiﬁcation task. however explained above since atom actually contains much less information ﬁrst three atoms really good over-emphasize importance feature encoding. drawback k-means feature mapping largely bypassed c-svdd-based scheme. shown fig. fourth atom actually represents small cluster. fact radius c-svdd ball corresponding traditional unsupervised methods like words model usually generate global feature representation simply histogramming local codings ignoring spatial relationship local patches. problem preserving spatial information feature representation huge dimension feature maps. suppose size receptive ﬁeld size input image densely extracting patches encoding them would obtain feature maps ﬁlter size particularly small images small dictionary size size ﬁlter resulting dimension feature maps nearly large many applications. methods average pooling pooling reduce size feature maps. sized pooling blocks size feature example reduced dimension becomes still concatenating maps. however choose bigger pooling window spatial information lost. paper proposed variant sift-representation address issues. sift widely used descriptor computer vision helpful suppress noise improve invariant properties ﬁnal feature representation. sift representation general extracts -bit sift-descriptors densely. also cause high dimensionality. example extract dimensional sift-descriptors densely feature maps size pixel dimension obtained representation vector high address issue ﬁrst divide feature blocks extract -bit gradient histogram block sift does. results feature representation dimension signiﬁcantly reduce dimensionality preserving rich information subsequent task. next extend method exploit multi-scale information better feature learning. multi-scale method describe objects interest different sizes context. would useful since patches ﬁxed size seldom characterize object well actually capture local appearance information limited size. example size small information edges fig. illustration feature maps face images using k-means c-svdd respectively based local dictionary atoms maps corresponding atom next column corresponding face. response values feature darker lower. evaluate performance proposed c-svddnet conduct extensive experiments four datasets including object classiﬁcation datasets minst image retrieval datasets inria copydays images undergo whitening preprocessing feeding network. whitening operation linearly transforms data covariance matrix becomes unit sphere hence justifying euclidean distance k-means clustering procedure. unless otherwise noted parameter settings listed table.i apply experiments. inﬂuence important parameters number ﬁlters investigated detail subsequent sections. single scale network receptive ﬁeld default across datasets recommended multi-scale version receptive ﬁelds three scales shown table.i. c-svdd ball regularization parameter set. parameter allows control amount noise willing tolerant seen e.q. small value encourages tight ball. default datasets except noisy background furthermore centers c-svdd k-means safely ignore effect initialization k-means. throughout experiments coates’ k-means triangle encoding method baseline direct counterpart method simply replacing triangle encoding c-svdd encoding denoted ‘c-svdd’. furthermore denote proposed single layer network ‘c-svddnet’ multi-scale version ‘msrv c-svddnet’. addition re-evaluate baseline method within proposed network replacing component c-svdd k-means-based encoding denoted ‘k-meansnet’. first conduct extensive experiments dataset investigate behavior proposed method. stl- large image dataset popularly used evaluate algorithms unsupervised feature learning self-taught learning. besides unlabeled images contains labeled images object classes among images partitioned training remaining could captured information combine meaningful patterns motifs parts poselets object lost information entities different levels valuable discriminative complementary well. popular manually designed feature descriptors sift address problem extend pooling image gradients edglets-like features still unclear example assemble edglets motifs using methods. convolutional neural network provides simple comprehensive solution issue automatically learn hierarchies features ranging edglets objects. however procedure information high-level patterns found becomes ambiguous. c-svddnet single-layer network difﬁcult learn multi-scale information hierarchical way. instead take naive obtain multi-scale information using receptive ﬁelds different sizes. particular fetch patches squares size training images train dictionary atoms corresponding size k-means. fig. shows examples atoms learnt face dataset. feature extractors similar learnt using typical convnet. speciﬁcally increasing window size learnt features become understandable example shown fig. using receptive ﬁeld size face images successfully learned facial parts eyes mouth smaller receptive ﬁeld gives oriented ﬁlters shown fig. scale train several networks different pooling window. advantage method efﬁcient learn effective capture salient features multi-scale context. however tell bigger patterns explained smaller ones information would useful generative angle. learnt multi-scale information classiﬁcation train separate classiﬁer output layer corresponding network according different receptive sizes different pooling sizes combine boosting framework. particularly assume total number categories scales learn output nodes. nodes corresponding multi-class classiﬁers. denote parameter classiﬁer rd×c weight vector k-th category. ﬁrst train parameters using series one-versus-rest l-svm classiﬁers normalize outputs classiﬁer using soft function images testing. images color images pixels size. pre-deﬁned overlapped folds training images images fold. fold classiﬁer trained training images tested testing images. consistence report average accuracy across folds. unsupervised feature learning randomly select unlabeled data. size spatial pooling hence size feature maps sift representation multi-scale receptive voting scale perform spatial pooling sizes ranging really need large number local features? number features mean number ﬁlters used feature extraction equal number dictionary atoms. major conclusions coates al.’s series controlled experiments single layer unsupervised feature learning network compared choice particular learning algorithm parameters deﬁne feature extraction pipeline especially number features much deep impact performance. using k-means network features example able achieve surprisingly good performance several benchmark datasets even better much deeper architectures deep boltzmann machine sparse auto-encoder feature maps) condense resulting feature maps otherwise dimensionality ﬁnal feature representation could prohibitively high. example pooling feature maps size would lead total number features hence ﬁrst question investigate whether large number features really needed time? fig. gives performance curves according varying number features different methods stl- dataset. besides aforementioned methods ﬁgure also give results random dictionary combination random dictionary sift representation seen increasing number features performance k-means c-svdd methods rises consistent results coates possible explanation since k-means encoding c-svdd encoding learnt dictionary extract non-linear features dictionary atoms help disentangle factors variations images. opinion capability learn large number atoms relatively computational cost major advantages k-means based methods unsupervised feature learning algorithms gaussian mixture model sparse coding rbm. example difﬁcult learn dictionary atoms hand large dictionary increase redundancy decrease efﬁciency. hence desirable reduce number features hurting performance much. fig. shows c-svdd encoding method consistently works better k-means encoding different number features combining csvdd encoding sift-based representation dramatically reduces needs large dictionary without scarifying performance. actually table.ii show using c-svdd encoding sift feature representation dictionary size reduces times performance improves .%). random dictionary interesting number atoms small random atoms perform much worse ﬁnetuned k-means. size dictionary increases performance difference random dictionary k-means dictionary begins reduce. example features using random atoms gives performance slightly worse k-means performance randomnet also close k-meansnet however performance random methods much lower c-svdd based methods. effect pooling size investigate effect different pooling sizes performance using proposed method conduct series experiments stl- dataset. particularly original image receptive algorithm selective receptive fields trans. invariant simulated visual ﬁxation convnet discriminative sum-prod. hierarchical matching pursuit deep feedforward networks vlad k-means c-svdd k-meansnet c-svddnet msrv+k-meansnet msrv+c-svddnet combining leads better performance. shows representations captured different receptive ﬁelds pooling sizes complementary other. contribution components illustrate contributions individual stages proposed method conduct series experiments stl- dataset removing three main stages turn leaving remaining stages place fig. gives results. general stage beneﬁcial results cumulative stages sift stage seems contribute performance improvement. suggests taking spatial information global representation importance. stl- dataset table.ii gives results stl- dataset. major challenges dataset images captured wild cluttered background objects various scales poses. before compared method several feature learning methods state performance. scale c-svdd fig. detailed performance different representations ensemble stl- dataset. representations obtained combining different receptive ﬁeld size pooling size indicates receptive ﬁeld denotes pooling block pixel. yielded accuracy. fig. gives results different settings. ﬁgure generally layer k-means-based network need bigger block sizes improved translation invariance adding robust sift encoding layer pooling effectively reduces needs large pooling size obtaining better performance. possible reason tends characterize detailed information objects represented. effect multi-scale receptive ﬁeld voting fig. gives detailed accuracy representations using sizes receptive ﬁelds sizes pooling blocks. different representation leads different prediction accuracy algorithm deep boltzmann machines convolutional deep belief networks multi-column deep neural networks network network maxout networks regularization neural networks pcanet deeply-supervised nets k-means c-svdd k-meansnet c-svddnet msrv+k-meansnet msrv+c-svddnet network obtains accuracy using ﬁltering dictionary atoms outperforms several feature encoding methods words vector linearly agregated descriptors fisher vector unsupervised deep learning methods selective receptive fields discriminative sum-product networks also indicates spatial information preserving using sift indeed useful unsupervised feature learning. also note replacing proposed c-svdd encoding k-means encoding leads nearly performance loss fusing multi-scale information gives improvement accuracy exceeding current best performer challenging dataset. minst dataset mnist popular datasets pattern recognition. consists grey valued images handwritten digits training examples test examples size-normalized centered ﬁxed-size image pixel. training dictionary atoms feature mapping pooling/subsampling break feature blocks extract sift features. multi-scale receptive voting types receptive ﬁelds combined settings pooling sizes different views/representations obtained image dataset. table.iii gives experimental results minst dataset. well-known deep learning achieved great success task digit recognition. example among test digits misclassiﬁed deep boltzmann machines convolutional deep belief networks maxout networks respectively reduce number simple single layer network achieves error highly competitive complex methods using deep architecture. fig. shows misclassiﬁed digits method misclassiﬁed digits confusing even human beings. compared original k-means network proposed method reduces error rate much smaller number ﬁlters. fig. misclassiﬁed handwritten digits among test examples method. small digit white square ground truth label corresponding image green square prediction made method. reveals least dataset clean background beneﬁcial focus representation details image rather emphasizing much global aspects using large number ﬁlters large pooling size. holiday dataset inria holiday dataset consists images personal holiday photos. queries ground truth images. employed measure retrieval accuracy. resize images training dictionary atoms feature mapping pooling/subsampling break feature blocks extract sift features. thus dimension ﬁnal representation also dimensionality reduction multi-scale receptive voting types receptive ﬁelds combined four settings pooling sizes different views/representations obtained image dataset. note image retrieval task train classiﬁers concatenate views’ representations combine multi-scale information. retrieval stage euclidean distance nearest neighbor searching facilitating fair comparison various feature representation methods task. table.iv gives experimental results dataset. compare method vlad different dimension takes sized ﬁlter bank lowest replacing k-means triangle encoding improves still needs large ﬁlter bank respectively. take small ﬁlters size arandjelovic combines vlad adaptive ﬁlter bank normalization achieve accuracy proposed c-svddnet ﬁlters well. outperforms vlad vlad+adapt+innorm even reduce dimension smaller sizes consistently achieves best performance among compared ones. also note replacing k-means encoding c-svdd encoding results signiﬁcant improvement concatenating multi-scale representation views able achieve highest without using supervision information. copydays dataset inria copydays dataset designed evaluate near-duplicate detection dataset contains original images. obtain query images relevant copy detection scenario image dataset transformed three different types transformation image resizing cropping strong transformations total transformed images single matching image database. images resized types receptive ﬁelds together three pooling sizes result different views. challenge ourself experiments also merge database images does. imental results dataset. cropped circumstance c-svddnet ﬁlters robust enough achieve ﬁlters. reducing dimension bits still performs second best. strong transformation setting c-svddnet achieves outperforms vlad nearly furthermore c-svdd encoding allows c-svddnet improve upon k-meansnet terms map. reduced bits c-svddnet achieves difﬁcult cases strong transformation outperforms compared methods multi-scale version improves paper propose simple one-layer neural network termed c-svddnet unsupervised feature learning. major advantages proposed method allows effective feature representation many applications object classiﬁcation image retrieval exploiting unlabeled data often cheap readily available. show properly combined sift descriptors representation could made even efﬁcient discriminant. extensive experiments several challenging object classiﬁcation datasets image retrieval datasts demonstrate proposed method signiﬁcantly outperforms previous state unsupervised feature learning methods word vlad j´egou perronnin douze s´anchez p´erez schmid image descriptors compact codes pattern aggregating local analysis machine intelligence ieee transactions vol. chatﬁeld lempitsky vedaldi zisserman devil details evaluation recent feature encoding methods yuan huang optimal mean robust principal component analysis proceedings international conference machine learning douze j´egou sandhawalia amsaleg schmid evaluation gist descriptors web-scale image search proceedings international conference image video retrieval. grosse ranganath convolutional deep belief networks scalable unsupervised learning hierarchical representations proceedings annual international conference machine learning. rich information feature preserve compact encoding signiﬁcantly reduces computational cost. last least show multi-scale information improve performance without training many layers networks training several shallow networks much easier training deep one. carneiro nascimento freitas segmentation left ventricle heart ultrasound data using deep learning architectures derivative-based search methods image processing ieee transactions vol. ling nuryani nguyen evolvable rough-blockbased neural network biomedical application hypoglycemia detection system. cybernetics ieee transactions vol.", "year": 2014}