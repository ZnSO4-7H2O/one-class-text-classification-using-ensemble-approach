{"title": "Bayesian Optimization with Automatic Prior Selection for Data-Efficient  Direct Policy Search", "tag": ["cs.RO", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "One of the most interesting features of Bayesian optimization for direct policy search is that it can leverage priors (e.g., from simulation or from previous tasks) to accelerate learning on a robot. In this paper, we are interested in situations for which several priors exist but we do not know in advance which one fits best the current situation. We tackle this problem by introducing a novel acquisition function, called Most Likely Expected Improvement (MLEI), that combines the likelihood of the priors and the expected improvement. We evaluate this new acquisition function on a transfer learning task for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has to learn to walk on flat ground and on stairs, with priors corresponding to different stairs and different kinds of damages. Our results show that MLEI effectively identifies and exploits the priors, even when there is no obvious match between the current situations and the priors.", "text": "abstract— interesting features bayesian optimization direct policy search leverage priors accelerate learning robot. paper interested situations several priors exist know advance best current situation. tackle problem introducing novel acquisition function called likely expected improvement combines likelihood priors expected improvement. evaluate acquisition function transfer learning task planar possibly damaged -legged robot learn walk ground stairs priors corresponding different stairs different kinds damages. results show mlei effectively identiﬁes exploits priors even obvious match current situations priors. reinforcement learning could allow robots adapt tasks contexts adaptation happens minutes contrary simulated worlds thousands simulations evaluated number trials robotics hardware limited energetic autonomy robot need perform task soon possible useful among different approaches data-efﬁcient bayesian optimization promising approach because work continuous action state spaces contrary classic algorithms scales well dimension state space contrary modelbased policy search algorithms blackdrops example successfully used learn walking policies quadruped -legged compass walker originally conceived black-box optimization algorithm expensive functions however robot often possible prior knowledge behavior system. instance simulator intact robot help learn policy damaged robot guide search algorithm promising areas knowledge acquired solving previous tasks make faster solve *corresponding author jean-baptiste.mouretinria.fr authors following afﬁliations inria villers-lès-nancy france cnrs loria vandœuvre-lès-nancy france université lorraine loria vandœuvre-lès-nancy france work received funding european research council european union’s horizon research innovation programme european commission project andy task used direct policy search priors reward function added using non-constant mean function model modeling difference observations prior instead modeling observations directly paper interested using several priors available know beforehand prior corresponds current context. typical situation robot knows solve task context needs learn solve context knowing whether closer tasks perception system might recognize right context many others observations reward function allow robot determine prior plausible. instance walking robot could learn surface slippery observing matches predictions correspond prior slippery ﬂoors often difﬁcult predict slipperiness surface looking main insight compare priors computing likelihood combination \"prior model\" select prior matches best observations. second insight prior selection elegantly incorporated acquisition function procedure select next point test balancing expected improvement likelihood model used compute expected improvement. demonstrate approach simple simulated problem whose goal reach target simulated physical -legged robot faces different damage conditions different environments. direct policy search successful approach robotics scales well high dimensional continuous state-action spaces instead trying predict expected returns future events valuefunction based learning learning direct policy search algorithms look optimal parameters parameterized policies. essentially differ policy updated techniques ranging gradient estimation ﬁnite differences advanced optimization algorithms covariance matrix adaptation evolutionary strategy trust region policy optimization deep deterministic policy gradient algorithms. make learning tractable successful experiments model-based policy search alternative direct policy search aims improving data-efﬁciency minimize number required trials model-based policy search algorithms choose next policy performing episode robot learning dynamical model system using data acquired optimizing policy according model using direct policy search algorithm. algorithms scale well dimensionality policy policy optimization performed model; sensitive dimension state-space need learn predict accurately next state given current one. difﬁculty scale makes challenging systems complex basic control benchmarks instead modeling dynamics system bayesian optimization directly models reward function promising parameters policy maximize expected reward. episode updates model allows improve predictions next iteration. core bayesian optimization made main components model reward function acquisition function uses model deﬁne utility point search space. vast majority experiments bayesian optimization gaussian processes model. acquisition function expected improvement upper conﬁdence bound probability improvement experimental results tend show perform better artiﬁcial objective functions recent experiment gait learning physical robot suggested outperform real situations direct policy search approach depend dimensionality state space makes effective learning policies robots complex dynamics instance bayesian optimization successfully used learn policies quadruped robot quadcopter small biped compass robot pocket-sized soft tensegrity robot cases least order magnitude data-efﬁcient competing methods. different domain also becoming successful approaches tune hyper-parameters machine learning algorithms like robotics evaluating quality parameters takes long time. beneﬁt using modeling method easily include prior knowledge data. common select particular mean function roughly corresponds what predicted value data?. early work bayesian optimization robotics focused constant mean functions user-deﬁned constant). noted overestimating mean function make real data appear mediocre leads excessive exploration whereas underestimating prior lead greedy exploration since real observations look promising recent work proposed priors come simulators simpliﬁed models non-constant priors. particular intelligent trial error algorithm ﬁrst creates repertoire highperforming policies stores low-dimensional robot needs adapt bayesian optimization algorithm searches best policy low-dimensional uses reward stored mean function algorithm allowed legged walking robot adapt several damage conditions less minutes whereas used simulator intact robot generate prior. instead generating prior ﬁrst also possible choose querying simulator real robot point different conﬁdence level depending obtained. last recent article proposed simulator learn kernel used instead using simulator deﬁne mean function priors simulation also successfully used model learning instead learning dynamical model robot scratch possible learn residual model difference simulated real robot approach instance successfully demonstrated pilco algorithm model-based policy search learning model optimal control contributions show using well-chosen priors promising approach data-efﬁcient learning previous algorithms assume know right prior advance. often strong assumption means robot recognizes current situation; also often critical assumption misleading prior substantially slow learning process. present paper relax assumption allowing algorithm choose prior likely help learning process. instance priors correspond different typical situations algorithm choose automatically relevant many situations prior knowledge objective function available starting optimization. case write information prior function update equations accordingly next point objective function evaluated found maximizing acquisition function function leverages model predict promising point. function often used expected improvement mt)) model. however would risk select overpessimistic prior beginning optimization because ﬁrst observations likely low-performing random points likely high-performing would need learning. essence observed highperforming solutions likeliest prior prior every solution low-performing. therefore need balance likelihood prior potential high-performing solutions. words good expected improvement according unlikely model ignored; conversely likely model expected improvement might pessimistic helpful. model likely enough lets expect good improvement might helpful maximum objective function takes discrete values case likelihood probability. considering observations introduce indicator function ...f equals predictions match exactly observations deﬁne expected improvement prior ...f ...f predicted value depends samples observations deterministic function independent original distribution thus factors inside expectation independent variables split function extended afterwards case takes continuous values likelihood becomes density probability function still deﬁned product expected improvement likelihood mlei acquisition function used like acquisition function algorithm. please note likelihood evaluated model every point c++- limbo library implementation ﬁrst evaluate mlei acquisition function kinematic simulation planar robotic reach target point effector degrees freedom link measures reward function distance effector target point robot position controlled joints take positions policy parametrized target angles. pre-deﬁned priors using function gives position effector given angular positions forward kinematics none priors corresponds right target instance second prior selected robot knows reach target setup seen simple transfer learning example robot knows reach targets solve tasks robot needs learn reach novel target given knowledge previous targets. fig. robotic experiment. -dof planar touch given target end-effector. circles correspond target points available priors whereas green cross indicates actual target. solution found mlei shown. comparison mlei acquisition function without prior constant prior mean function random selection priors. replicates experiment carried consisting iterations replicated experiment times gather statistics. results show mlei ﬁnds policy reaches target episodes whereas random selection priors prior need overall mlei clearly outperforms three baselines. evaluate mlei acquisition function similar context cully -legged robot either damaged unknown introduced unknown environment used alternative walking gait works spite unforeseen situation. however cully used single prior introduce many priors correspond either potential damages different terrains test learning algorithm priors corresponding actual situation also situations fully covered prior. robot policy robot identical legs dofs controls horizontal movements whereas others control elevation leg. dofs controlled open-loop best parameter vector. initialization done mapelites iterates simple loop randomly select occupied cells random variation parameter vector simulate behavior insert parameter vector grid performs better ends empty cell map-elites straightforwardly parallelized large clusters deploying robot. successfully used create behaviors legged robots wheeled robots designs airfoils morphologies walking soft robots adversarial images deep neural networks mapelites also extended effectively handle tasks spaces arbitrary dimensionality behavior descriptors proposed cully body orientation captures often body robot tilted direction. formally simulating policy leads -dimensional vector contains proportion time body robot positive negative pitch roll duration gait robot divided intervals pitch roll torso robot respectively indicator function returns argument true angles ignored. quantity rounded take values body orientation factors ﬁnite contains elements organized map. experiments behaviorperformance maps created possible environments maps created map-elites algorithm hours -core xeon computer. used sferes library kernel chosen squared exponential experiment adaptation stairs simulation ﬁrst experiments intact robot needs adapt unknown environments. generated behaviorperformance maps four following environments fig. hexapod stairs real robot oscillator deﬁned parameters amplitude phase duty cycle second vertical angle constrained take values inferior member remains vertical angle vertical line. thus whole gait robot deﬁned parameters. simulations robot performed dynamic animation robotics toolkit world gravity simulated robot similar intact physical hexapod. reward function experiments reward function distance covered -legged robot virtual corridor width soon robot gets corridor evaluation stopped; also stopped robot stays corridor. compared traditional reward functions instance distance covered seconds reward function encourages robot follow straight line even means gait slower. similar results however obtained average walking speed reward. priors -dimensional behavior-reward maps computed simulated -legged robot different environments damaged robot behavior-reward maps created beforehand using map-elites algorithm recent evolutionary algorithm designed generate thousands different high-performing control policies. map-elites assumes deﬁne low-dimensional behavior descriptor lowdimensional vector captures main difference behaviors. given n-dimensional behavior space mapelites deﬁnes n-dimensional grid divided cells attempts cells high-quality solutions. starts random policy parameters simulates robot parameters records position robot behavior space performance. cell free algorithm stores policy parameters cell; already occupied algorithm compares reward values keeps fig. comparison simulation mlei acquisition functions choices prior -legged robot learning climb stairs and/or recover damages iterations replicates experiment. robot unknown stairs damage real stairs among priors robot unknown stairs unknown damages priors stairs damages robot ground unknown damages real damage among priors number stars indicates p-value obtained using mann-whitney-wilcoxon test respectively. mlei random priors experiments randomly choose priors possible environment leading unique priors mlei experiment experiment randomly chosen priors. please note several priors correspond situation interesting maps might higher-quality others even generated environment. test situations results show mlei allows robot learn high-performing gaits stairs even stairs used learning experiments present priors right prior accessible mlei ﬁnds accessible still leverage priors good behavior using priors. tested cases mlei clearly outperforms random selection priors method using ground prior means mlei selects priors correctly priors help learning process. surprisingly mlei also outperforms perfect prior mlei access priors hard stairs therefore select best them whereas experiment access single prior considered stairs relatively good performance random selection priors likely stem fact algorithm access much higher diversity behaviors ground prior makes likely behavior works tested situation. experiment adaptation stairs damages simulation second experiment evaluate robot adapt unforeseen damage conditions without stairs without priors damage conditions. legged removed generated priors map-elites leading priors like previous experiments available priors made random maps situation. adaptation damage priors damage conditions ground actual damage among priors priors select from. results show mlei compensatory gaits stairs using priors computed intact robot. real stairs among priors mlei outperforms right stairs since robot damaged helpful prior always prior corresponds correct stairs like before mlei access priors makes likely policy compensate damage. actual stairs priors mlei still outperforms approaches mlei also take advantage priors damage condition whether damage included priors actual damage conditions among priors mlei leads higher-performing solutions intact robot prior; damage condition among prior mlei performs intact robot prior. results consistent shows intact robot effective prior adapt damage. experiment adaptation damage physical experiment robot priors damage conditions allow physical -legged robot adapt. simulation perfect learning algorithm compensate reality damage. robot tracked external motion capture system episodes seconds. like before consider situations damage among priors not. replicate experiment times. like simulation mlei takes advantage priors higher-performing gaits single prior used priors correspond actual damage condition mlei clearly outperforms single prior ﬁnds high-performing gaits less episodes; mlei also ﬁnds better gaits actual damage condition fig. comparison mlei standard single prior coming simulated undamaged robot. real experiment carried physical damaged -legged robot walking ground iterations replicates experiment. damage used missing rear shortened rear among priors likely come fact mlei take inspiration priors compensate damage well-chosen priors guide high-performing solution constraining search pre-designed solutions. however learning algorithms useful robot environment partially known therefore often challenging design single prior would help possible situations. likely expected improvement allows relax assumption making capable selecting useful prior ignore others. therefore makes possible beneﬁt faster convergence speed given priors assuming much robot environment. paper demonstrated acquisition function leads powerful adaptation algorithm systems planar manipulator -legged robot. latter case robot capable discovering compensatory behaviors dozen trials damaged even priors correspond intact robot faced unknown stairs even without prior actual stairs. overall mlei substantially increases potential uses priors often guess could useful robot cannot sure advance given prior useful future. based direct observation rewards robot discovers works attempt know behaviors work not. approach well theory embodied cognition suggests robots need explicit representation world act. classic sense-planact architecture would assume existence accurate model world act; spectrum learning algorithms assuming little knowledge possible robot environment. automatic selection prior effective middle ground prior knowledge perception system guide direct policy search needed ignore previous knowledge still effective act. brochu freitas tutorial bayesian optimization expensive cost functions application active user modeling hierarchical reinforcement learning corr vol. abs/. papaspyros chatzilygeroudis vassiliades j.-b. mouret safety-aware robot damage recovery using constrained bayesian optimization simulated priors bayesopt workshop nips marco berkenkamp hennig schoellig krause schaal trimpe virtual real trading simulations physical experiments reinforcement learning bayesian optimization proc. icra duarte gomes oliveira christensen evolution repertoire-based control robots complex locomotor systems ieee transactions evolutionary computation vassiliades chatzilygeroudis j.-b. mouret using centroidal voronoi tessellations scale multi-dimensional archive phenotypic elites algorithm ieee trans. evolutionary computation", "year": 2017}