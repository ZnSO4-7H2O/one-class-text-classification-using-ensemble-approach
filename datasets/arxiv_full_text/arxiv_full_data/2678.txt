{"title": "Safe Model-based Reinforcement Learning with Stability Guarantees", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.SY"], "abstract": "Reinforcement learning is a powerful paradigm for learning optimal policies from experimental data. However, to find optimal policies, most reinforcement learning algorithms explore all possible actions, which may be harmful for real-world systems. As a consequence, learning algorithms are rarely applied on safety-critical systems in the real world. In this paper, we present a learning algorithm that explicitly considers safety, defined in terms of stability guarantees. Specifically, we extend control-theoretic results on Lyapunov stability verification and show how to use statistical models of the dynamics to obtain high-performance control policies with provable stability certificates. Moreover, under additional regularity assumptions in terms of a Gaussian process prior, we prove that one can effectively and safely collect data in order to learn about the dynamics and thus both improve control performance and expand the safe region of the state space. In our experiments, we show how the resulting algorithm can safely optimize a neural network policy on a simulated inverted pendulum, without the pendulum ever falling down.", "text": "reinforcement learning powerful paradigm learning optimal policies experimental data. however optimal policies reinforcement learning algorithms explore possible actions harmful real-world systems. consequence learning algorithms rarely applied safety-critical systems real world. paper present learning algorithm explicitly considers safety deﬁned terms stability guarantees. speciﬁcally extend control-theoretic results lyapunov stability veriﬁcation show statistical models dynamics obtain high-performance control policies provable stability certiﬁcates. moreover additional regularity assumptions terms gaussian process prior prove effectively safely collect data order learn dynamics thus improve control performance expand safe region state space. experiments show resulting algorithm safely optimize neural network policy simulated inverted pendulum without pendulum ever falling down. reinforcement learning algorithms achieved impressive results games example atari platform rarely applied real-world physical systems outside academia. main reason algorithms provide optimal policies long-term intermediate policies unsafe break system harm environment. especially true safety-critical systems affect human lives. despite this safety remained largely open problem consider example self-driving car. desirable algorithm drives improve time policy applied system guarantee safe driving. thus possible learn system random exploratory actions almost certainly lead crash. order avoid problem learning algorithm needs consider ability safely recover exploratory actions. particular want able recover safe state example driving reasonable speed middle lane. ability recover known asymptotic stability control theory speciﬁcally care region attraction closed-loop system policy. subset state space forward invariant state trajectory starts within stays within times converges goal state eventually. paper present algorithm continuous state-action spaces provides kind high-probability safety guarantees policies. particular show starting initial safe policy expand estimate region attraction collecting data inside safe region adapt policy increase region attraction improve control performance. related work safety active research topic different deﬁnitions safety exist discrete markov decision processes class tractable models analyzed. risk-sensitive speciﬁes risk-aversion reward example deﬁne risk probability driving agent known undesirable states. similarly robust mdps maximize rewards transition probabilities uncertain introduce algorithms safely explore mdps agent never gets stuck without safe actions. methods require accurate probabilistic model system. continuous state-action spaces model-free policy search algorithms successful. update policies without system model repeatedly executing task setting introduces safety guarantees terms constraint satisfaction hold expectation. high-probability worst-case safety guarantees available methods based bayesian optimization together gaussian process models cost function. algorithms provide high-probability safety guarantees parameter evaluated real system. methods used safely optimize parametric control policy quadrotor. however resulting policies task-speciﬁc require system reset. model-based setting research focused safety terms state constraints. priori known safe global backup policies used learns switch several safe policies. however clear policies ﬁrst place. approaches model predictive control constraints model-based technique control actions optimized online. example models uncertain environmental constraints uses approximate uncertainty propagation dynamics along trajectories. setting robust feasability constraint satisfaction guaranteed learned model bounded errors using robust model predictive control method uses reachability analysis construct safe regions state space. theoretical guarantees depend solution partial differential equation approximated. theoretical guarantees stability exist tractable stability analysis veriﬁcation ﬁxed control policy. control stability known system veriﬁed using lyapunov function similar approach used deterministic unknown dynamics modeled allows provably safe learning regions attraction ﬁxed policies. similar results shown stochastic systems modeled bayesian quadrature compute provably accurate estimates region attraction. approaches update policy. contributions introduce novel algorithm safely optimize policies continuous state-action spaces providing high-probability safety guarantees terms stability. moreover show possible exploit regularity properties system order safely learn dynamics thus improve policy increase estimated safe region attraction without ever leaving speciﬁcally starting policy known stabilize system locally gather data informative safe points improve policy safely based improved model system prove exploration algorithm gathers data points reaches natural notion full exploration. show theoretical results transfer practical algorithm safety guarantees apply simulated inverted pendulum stabilization task. states control actions discrete time index true dynamics consist parts known prior model obtained ﬁrst principles represents priori unknown model errors. model errors unknown obtain noisy measurements driving system state taking action want system behave certain e.g. driving road. need specify control policy that given current state determines appropriate control action drives system goal state origin without loss generality encode performance requirements drive system origin positive cost associated states actions policy aims minimize cumulative discounted costs starting state. goal safely learn dynamics measurements adapt policy performance without encountering system failures. speciﬁcally deﬁne safety constraint state divergence occurs leaving region attraction. means adapting policy allowed decrease region attraction exploratory actions learn dynamics allowed drive system outside region attraction. region attraction known priori implicitly deﬁned system dynamics choice policy. thus policy deﬁnes performance typical also determines safety obtain measurements. model assumptions general kind safe learning impossible without assumptions. example discontinuous system even slight change control policy lead drastically different behavior. moreover expand safe need generalize learned knowledge dynamics states visited. restrict general practically relevant class models lipschitz continuous. typical assumption control community additionally ensure closed-loop system remains lipschitz continuous control policy applied restrict policies rich class lπ-lipschitz continuous functions also contains certain types neural networks assumption dynamics lhlg lipschitz continuous respect -norm. considered control policies functions lπ-lipschitz continuous respect -norm. enable safe learning require reliable statistical model. commit exploration analysis safety suitable well-calibrated model applicable. assumption denote posterior mean covariance matrix functions statistical model dynamics conditioned noisy measurements. trace) exists probability least holds βnσn. assumption ensures build conﬁdence intervals dynamics that scaled appropriate constant cover true function high probability. introduce speciﬁc statistical model fulﬁlls assumptions certain regularity assumptions sec. lyapunov function satisfy speciﬁed safety constraints safe learning require tool determine whether individual states actions safe. control theory safety deﬁned region attraction computed ﬁxed policy using lyapunov functions lyapunov functions continuously differentiable functions idea behind using lyapunov functions show stability system similar gradient descent strictly quasiconvex functions show that given policy applying dynamics state maps strictly smaller values lyapunov function state eventually converges equilibrium point origin particular assumptions theorem imply strictly quasiconvex within region attraction dynamics lipschitz continuous. result step decrease property states within level guarantees eventual convergence origin. theorem lyapunov function lipschitz continuous dynamics policy. within level region attraction implies limt→∞ convenient characterize region attraction level lyapunov function since replaces challenging test convergence one-step decrease condition lyapunov function. theoretical analysis paper assume lyapunov function given determine region attraction. ease notation also assume ∂v/∂x ensures level sets connected since lyapunov functions continuously differentiable lv-lipschitz continuous compact general easy suitable lyapunov functions. however physical models like prior model energy system good candidate lyapunov function. moreover recently shown possible compute suitable lyapunov functions experiments exploit fact value functions lyapunov functions costs strictly positive away origin. follows directly deﬁnition value function v)). thus obtain lyapunov candidates by-product approximate dynamic programming. initial safe policy lastly need ensure exists safe starting point learning process. thus assume initial policy renders origin system asymptotically stable within small states example policy designed using prior model since models locally accurate deteriorate quality state magnitude increases. policy explicitly safe throughout state space theory section assumptions safe reinforcement learning. start computing region attraction ﬁxed policy statistical model. next optimize policy order expand region attraction. lastly show possible safely learn dynamics additional assumptions model system’s reachability properties approach expands estimated region attraction safely. consider idealized algorithm amenable analysis convert practical variant sec. fig. illustrative algorithm examples sets deﬁned below. region attraction start computing region attraction ﬁxed policy. extension method discrete-time systems. want lyapunov decrease condition theorem guarantee safety statistical model dynamics. however posterior uncertainty statistical model dynamics means step predictions uncertain too. account constructing high-probability conﬁdence intervals lvβnσn−]. assumption together lipschitz property know contained probability least exploration analysis need ensure safe state-actions cannot become unsafe; initial safe remains safe intersect conﬁdence intervals initialized l∆vτ otherwise. note contained probability assumption upper lower bounds deﬁned maxcn mincn. given high-probability conﬁdence intervals system stable according theorem however intractable verify condition directly continuous domain without additional restrictive assumptions model. instead consider discretization state space cells holds here denotes point smallest distance given discretization bound decrease variation lyapunov function states lipschitz continuity generalize continuous state space theorem assumptions lvlf discretization holds l∆vτ holds probability least region attraction policy proof given appendix theorem states that given conﬁdence intervals statistical model dynamics sufﬁcient check stricter decrease condition theorem discretized domain guarantee requirements region attraction continuous domain theorem bound theorem becomes tight discretization constant zero. thus discretization constant trades computation costs accuracy approaches obtain measurement data posterior model uncertainty dynamics √βnσn decreases. conﬁdence intervals corresponding estimated region attraction seen bottom half fig. policy optimization focused estimating region attraction ﬁxed policy. safety property states ﬁxed policy. means policy directly determines figure example application algorithm input constraints system becomes unstable large states. start initial local policy small safe region attraction fig. algorithm selects safe informative state-action pairs within evaluated without leaving region attraction current policy gather data uncertainty model decreases update policy lies within fulﬁlls lyapunov decrease condition. algorithm converges largest safe fig. improves policy without evaluating unsafe state-action pairs thereby without system failure. states safe. speciﬁcally form region attraction states discretizaton within level lyapunov function need fulﬁll decrease condition theorem depends policy choice. state-action pairs fulﬁll decrease condition given fig. order estimate region attraction based need commit policy. speciﬁcally want pick policy leads largest possible region attraction according theorem requires discrete state corresponding state-action pair policy must thus optimize policy according region attraction corresponds optimized policy according given fig. largest level lyapunov function state-action pairs correspond discrete states within contained means state-action pairs fulﬁll requirements theorem region attraction true system policy following theorem thus direct consequence theorem theorem true region attraction policy probability least thus optimize policy subject constraint estimated region attraction always inner approximation true region attraction. however solving optimization problem intractable general. approximate policy update step sec. collecting measurements given stability guarantees natural might obtain data points order improve model thus efﬁciently increase region attraction. question difﬁcult answer general since depends property statistical model. particular general statistical models often clear whether conﬁdence intervals contract sufﬁciently quickly. following make additional assumptions model reachability within order provide exploration guarantees. assumptions allow highlight fundamental requirements safe data acquisition safe exploration possible. assume unknown model errors bounded norm reproducing kernel hilbert space corresponding differentiable kernel class weights decay sufﬁciently fast assumption ensures satisﬁes lipschitz property assumption moreover models dynamics fulﬁll assumption state fully observable measurement noise σ-sub-gaussian information capacity. corresponds amount mutual information obtained measurements measure size function class encoded model. information capacity sublinear dependence common kernels upper bounds computed efﬁciently details model given appendix order quantify exploration properties algorithm consider discrete action space deﬁne exploration number state-action pairs safely learn without leaving true region attraction. note despite discretization policy takes values continuous domain. moreover instead using conﬁdence intervals directly consider algorithm uses lipschitz constants slowly expand safe set. analysis quantify ability generalize beyond current safe set. practice nearby states sufﬁciently correlated model enable generalization using suppose given state-action pairs learn safely. speciﬁcally means policy that state-action pair apply action state apply actions according policy state converges origin. constructed using initial policy sec. starting want update policy expand region attraction according theorem conﬁdence intervals states inside determine state-action pairs fulﬁll decrease condition. thus redeﬁne exploration analysis formulation equivalent except uses lipschitz constant generalize safety. given region attraction committing policy according order expand region attraction effectively need decrease posterior model uncertainty dynamics collecting measurements. however ensure safety outlined sec. restricted states within also need ensure state taking action safe; dynamics state back region attraction lipschitz constant order determine contains state-action pairs safely evaluate current policy without leaving region attraction fig. remains deﬁne strategy collecting data points within effectively decrease model uncertainty. speciﬁcally focus high-level requirements exploration scheme without committing speciﬁc method. practice exploration strategy aims decrease model uncertainty driving system speciﬁc states used. safety ensured picking actions according whenever exploration strategy reaches boundary safe region backup policy exploration. high-level goal exploration strategy shrink conﬁdence intervals state-action pairs order expand safe region. speciﬁcally exploration strategy visit state-action pairs uncertain dynamics; conﬁdence interval largest gathered enough information order expand implicitly assumes state within reached exploration policy achieves high-level goal exploration algorithm aims reduce model uncertainty. practice safe exploration scheme limited unreachable parts state space. compare active learning scheme oracle baseline starts initial safe knows accuracy within safe set. oracle also uses knowledge lipschitz constants optimal policy iteration. denote baseline manages determine safe provide detailed deﬁnition appendix theorem assume σ-sub-gaussian measurement noise model error rkhs norm smaller assumptions theorem measurements collected according cq|+) log. smallest positive integer true region attraction policy following holds jointly probability least theorem states that selecting data points according estimated region attraction contained true region attraction current policy selected data points cause system leave region attraction. means exploration method considers safety constraint able safely learn system without leaving region attraction. last part theorem states ﬁnite number data points achieve least exploration performance oracle baseline classify unsafe state-action pairs safe. means algorithm explores largest region attraction possible given lyapunov function residual uncertaint smaller details comparison baseline given appendix. practice means exploration method manages reduce maximal uncertainty dynamics within able expand region attraction. example repeatedly evaluating one-dimensional state-space shown fig. seen that selecting data points within current estimate region attraction algorithm efﬁciently optimize policy expand safe region time. previous section given strong theoretical results safety exploration idealized algorithm solve section provide practical variant theoretical algorithm previous section. particular retain safety guarantees sacriﬁce exploration guarantees obtain practical algorithm. summarized algorithm policy optimization problem intractable solve considers safety rather performance metric. propose approximate policy update maximizes approximate performance providing stability guarantees. proceeds optimizing policy ﬁrst computes region attraction ﬁxed policy. impact safety since data still collected inside region attraction. moreover optimization fail region attraction decrease always revert previous policy guaranteed safe. figure optimization results inverted pendulum. fig. shows initial safe policy green region represents estimated region attraction optimized neural network policy. contained within true region attraction fig. shows improved performance safely learned policy policy prior model. experiments approximate dynamic programming capture performance policy. given policy parameters compute estimate cost-to-go mean dynamics based cost state γ-discounted rewards encountered following policy goal adapt parameters policy minimum cost measured ensuring safety constraint worst-case decrease lyapunov function theorem violated. lagrangian formulation constrained optimization problem ﬁrst term measures long-term cost lagrange multiplier safety constraint theorem experiments value function lyapunov function candidate case corresponds high-probability upper bound cost-to-go given uncertainty dynamics. similar worst-case performance formulations found robust mdps consider worst-case value functions given parametric uncertainty transition model. moreover since depends lipschitz constant policy simultaneously serves regularizer parameters verify safety conﬁdence intervals directly also conﬁdence compute active learning scheme algorithm line practice need compute entire solve global optimization method even random sampling scheme within suitable state-actions. moreover measurements actions away current policy unlikely expand fig. optimize gradient descent policy changes locally. thus achieve better data-efﬁciency restricting exploratory actions close constant computing region attraction verifying stability condition discretized domain suffers curse dimensionality. however necessary update policies real time. particular since policy returned algorithm provably safe within level policies used safely arbitrary number time steps. scale method higher-dimensional system would consider adaptive discretization veriﬁcation experiments python implementation algorithm experiments based tensorflow gpﬂow available https//github.com/befelix/safe_learning. verify approach inverted pendulum benchmark problem. true continuous-time dynamics given angle mass gravitational constant torque applied pendulum. control torque limited pendulum necessarily falls beyond certain angle. model discrete-time dynamics mean dynamics given linearized discretized model true dynamics considers wrong lower mass neglects friction. result optimal policy mean dynamics perform well small region attraction underactuates system. combination linear matérn kernels order capture model errors result parameter integration errors. policy neural network hidden layers neurons relu activations each. compute conservative estimate lipschitz constant standard approximate dynamic programming quadratic normalized cost xtqx utru positive-deﬁnite compute cost-to-go jπθ. speciﬁcally piecewiselinear triangulation state-space approximate allows quickly verify assumptions made lyapunov function sec. using graph search. practice function approximators. optimize policy stochastic gradient descent theoretical conﬁdence intervals model conservative. enable data-efﬁcient learning corresponds high-probability decrease condition per-state rather jointly state space. moreover local lipschitz constants lyapunov function rather global one. affect guarantees greatly speeds exploration. initial policy approximate dynamic programming compute optimal policy prior mean dynamics. policy unstable large deviations initial state poor performance shown fig. initial suboptimal policy system stable within small region state-space fig. starting initial safe algorithm proceeds collect safe data points improve policy. uncertainty dynamics decreases policy improves estimated region attraction increases. region attraction data points shown fig. resulting contained within true safe region optimized policy time control performance improves drastically relative initial policy seen fig. overall approach enables safe learning dynamic systems data points collected learning safely collected current policy. shown classical reinforcement learning combined safety constraints terms stability. speciﬁcally showed safely optimize policies give stability certiﬁcates based statistical models dynamics. moreover provided theoretical safety exploration guarantees algorithm drive system desired state-action pairs learning. believe results present important ﬁrst step towards safe reinforcement learning algorithms applicable real-world problems. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature martin pecka tomas svoboda. safe exploration techniques reinforcement learning overview. modelling simulation autonomous systems pages springer jens schreiter nguyen-tuong mona eberts bastian bischoff heiner markert marc toussaint. safe exploration active learning gaussian processes. machine learning knowledge discovery databases number pages springer international publishing yanan alkis gotovos joel burdick andreas krause. safe exploration optimization gaussian processes. proc. international conference machine learning pages felix berkenkamp angela schoellig andreas krause. safe controller optimization quadrotors gaussian processes. proc. ieee international conference robotics automation pages alexander hans daniel schneegaß anton maximilian schäfer steffen udluft. safe exploration reinforcement learning. proc. european symposium artiﬁcial neural networks pages chris ostafew angela schoellig timothy barfoot. robust constrained learningbased nmpc enabling reliable mobile robot path tracking. international journal robotics research anayo akametalu shahab kaynama jaime fisac melanie zeilinger jeremy gillula claire tomlin. reachability-based safe learning gaussian processes. proc. ieee conference decision control pages ruxandra bobiti mircea lazar. sampling approach ﬁnding lyapunov functions nonlinear discrete-time systems. proc. european control conference pages felix berkenkamp riccardo moriconi angela schoellig andreas krause. safe learning regions attraction nonlinear systems gaussian processes. proc. conference decision control pages julia vinogradska bastian bischoff nguyen-tuong henner schmidt anne romer peters. stability controllers gaussian process forward models. proceedings international conference machine learning pages christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. proc. international conference learning representations huijuan lars grüne. computation local lyapunov functions discrete-time systems linear programming. journal mathematical analysis applications bernhard schölkopf. learning kernels support vector machines regularization optimization beyond. adaptive computation machine learning. press cambridge mass niranjan srinivas andreas krause sham kakade matthias seeger. gaussian process optimization bandit setting regret experimental design. ieee transactions information theory martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg mane rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viegas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous distributed systems. arxiv. alexander matthews mark wilk nickson keisuke fujii alexis boukouvalas pablo león-villagrá zoubin ghahramani james hensman. gpﬂow gaussian process library using tensorflow. journal machine learning research scott davies. multidimensional triangulation interpolation reinforcement learning. proc. conference neural information processing systems pages last three inequalities follow assumptions last inequality follows lemma result holds probability least deﬁnition discretization policy class grid cell proof. proof analogous lemma follows assumptions corollary holds probability least proof. direct consequence fact lemma holds jointly probability least lemma show decrease lyapunov function discrete grid close continuous domain given conﬁdence intervals establish region attraction using theorem theorem assumptions lvlf discretization holds l∆vτ holds probability least region attraction policy proof. using lemma holds continuous states probability least since discrete states fulﬁll theorem thus theorem conclude region attraction theorem true region attraction policy probability least proof. following deﬁnition clear constraint optimization problem holds equivalently −l∆vτ result follows theorem note initialization conﬁdence intervals ensures decrease condition always fulﬁlled initial policy. particular assumption satisﬁes lipschitz continuity allows model dynamics model errors live reproducing kernel hilbert space corresponding differentiable kernel rkhs norm smaller theoretical analysis assumption prove exploration guarantees. distribution well-behaved smooth functions parameterized mean function covariance function encodes assumptions functions case mean given prior model kernel corresponds rkhs. given noisy measurements dynamics locations corrupted independent gaussian noise posterior distribution mean knt−yn covariance knt−kn) variance vector contains observed noisy deviations mean contains covariances test input data points rn×n entries identity matrix. dimensional output output dimension indexed allows standard deﬁnitions rkhs norm model. case deﬁne posterior distribution µn]t ≤i≤q unusual deﬁnition standard deviation used lemma given previous assumptions follows dynamics lipschitz continuous lipschitz constant depends properties kernel. moreover construct high-probability conﬁdence intervals dynamics fulﬁll assumption using model. lemma assume σ-sub-gaussian noise model error rkhs norm bounded choose then probability least holds βnσn−. proof. follows µn−| βnσn holds probability least following remark model multi-output function single-output function extended parameter space. thus result directly transfers deﬁnition norm deﬁnition multiple output dimensions maximum mutual information could gained dynamics samples. information capacity sublinear dependence many commonly used kernels linear squared exponential matérn kernels efﬁciently accurately approximated note explicitly account measurements states remark model assumes gaussian noise lemma considers σ-sub-gaussian noise. moreover consider functions bounded rkhs norm rather samples lemma thus states even though make different assumptions model conﬁdence intervals conservative enough capture true function high probability. safe exploration remark following assume deﬁned baseline baseline consider class algorithms know lipschitz continuity properties addition learn arbitrary statistical accuracy visiting state obtaining measurement next state applying action face safety restrictions deﬁned sec. suppose given state-action pairs learn safely. speciﬁcally means policy that state-action pair apply action state apply actions according policy state converges origin. constructed using initial policy sec. goal algorithm expand states learn safely. thus need estimate region attraction certifying state-action pairs achieve −l∆vτ decrease condition theorem learning state-action pairs generalize gained knowledge unseen states exploiting lipschitz continuity rdec learn accuracy within speciﬁcally include allow initial policies safe meet strict decrease requirements theorem given states rdec fulﬁll requirements theorem estimate corresponding region attraction committing control policy estimating largest safe level lyapunov function. rdec operator encodes operation. optimizes safe policies determine largest level state-action pairs discrete states level fulﬁll decrease condition theorem result rlev) estimate largest region attraction given \u0001-accurate knowledge state-action pairs based increased region attraction states safely learn about. speciﬁcally lipschitz constant statistical accuracy determine states back region attraction rlev rlev thus contains state-action pairs visit learn system. repeatedly applying operator leads largest state-action pairs safe algorithm knowledge restricted policies could hope reach. speciﬁcally state-action pars discrete grid algorithm could hope classify safe without leaving safe set. moreover rlev) largest corresponding region attraction algorithm classify safe given lyapunov function. proofs following implicitly assume assumptions lemma hold deﬁned speciﬁed within lemma moreover ease notation assume level lyapunov function lemma rlev maxx∈rlev proof. directly deﬁnition compare remark lemma allows write proofs entirely terms operators rather deal explicit policies. following algorithm replace according lemma moves deﬁnitions closer baseline makes easier comparison. roughly follow proof strategy deal additional complexity safe sets deﬁned difﬁcult non-trivial safe sets carefully designed order ensure algorithm works general nonlinear systems. start listing fundamental properties sets deﬁned below. lemma holds given properties ﬁrst consider happens safe expand collecting data points. results later conclude safe must either expand maximum level reached. denote proof. modify results lemma different deﬁnition even though goal different ours still apply reasoning bound amplitude conﬁdence interval dynamics. particular βnσn− according lemma means rdec) since therefore holds induction hypothesis. conclude rlev rlev)) concludes proof. lemma smallest integer |r|nn∗. then exists sn+nn holds probability proof. contradiction. assume contrary sn+nn. lemma know sn+nn. since increasing nn∗. thus must holds |sjtn∗| particular ﬁrst inequality follows corollary second lipschitz continuity lemma deﬁnition rlev follows rlev∩xτ moreover rlev level lyapunov function deﬁnition. thus result follows theorem lemma proof. holds deﬁnition. deﬁtion exists theorem assume σ-sub-gaussian measurement noise model error rkhs norm smaller assumptions theorem measurements collected according cq|+) log. smallest positive integer true region attraction policy following holds jointly probability least proof. lemmas respectively. part direct consequence corollary lemma", "year": 2017}