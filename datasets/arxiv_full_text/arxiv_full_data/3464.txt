{"title": "Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Deep neural networks (DNNs) have achieved unprecedented performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. This has caused a recent surge of interest in methods for rendering modern neural systems more interpretable. In this work, we propose to address the interpretability problem in modern DNNs using the rich history of problem descriptions, theories and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.", "text": "last half-decade deep learning signiﬁcantly improved performance variety tasks however deep neural network solutions remain poorly understood leaving many think models black boxes question whether understood opacity obstructs basic research seeking improve models applications models real world problems recent pushes aimed better understand dnns tailor-made loss functions architectures produce interpretable features output-behavior analyses unveil previously opaque operations networks parallel work neuroscience-inspired methods activation visualization ablation analysis activation maximization also applied. altogether line research developed promising tools understanding dnns paper producing glimmer insight. here propose another tool leveraging methods inspired neuroscience instead psychology. cognitive psychologists long wrestled problem understanding another opaque intelligent system human mind. contend search better understanding dnns proﬁt rich heritage problem descriptions theories experimental tools developed cognitive psychology. test belief performed proof-ofconcept study state-of-the-art dnns solve particularly challenging task one-shot word learning. specifically investigate matching networks state-of-the-art one-shot learning performance imagenet investigate inception baseline model following approach used cognitive psychology began hypothesizing inductive bias model solve word learning task. research developmental psychology shows learning words humans tend assign name similarly shaped deep neural networks achieved unprecedented performance wide range complex tasks rapidly outpacing understanding nature solutions. caused recent surge interest methods rendering modern neural systems interpretable. work propose address interpretability problem modern dnns using rich history problem descriptions theories experimental methods developed cognitive psychologists study human mind. explore potential value tools chose well-established analysis developmental psychology explains children learn word labels objects applied analysis dnns. using datasets stimuli inspired original cognitive psychology experiments state-of-the-art shot learning models trained imagenet exhibit similar bias observed humans prefer categorize objects according shape rather color. magnitude shape bias varies greatly among architecturally identical differently seeded models even ﬂuctuates within seeds throughtraining despite nearly equivalent classiﬁcation performance. results demonstrate capability tools cognitive psychology exposing hidden computational properties dnns concurrently providing computational model human word learning. items rather items similar color texture size. test hypothesis dnns discover shape bias probed models using datasets experimental setup based original shape bias studies results follows inception networks trained imagenet indeed display strong shape bias. high variance bias inception networks initialized different random seeds demonstrating otherwise identical networks converge qualitatively different solutions. also strong shape bias bias closely mimics bias inception model provides input emulating shape bias observed children models provide candidate computational account human one-shot word learning. altogether results show technique testing hypothesized biases using probe datasets yield expected surprising insights solutions discovered trained dnns. behavioral probes understand neural network function extensively applied within psychology itself neural networks employed effectively models human cognitive function contrast present work advocating application behavioral probes along associated theories hypotheses cognitive psychology address interpretability problem modern deep networks. spite widespread adoption deep learning methods recent years knowledge work applying behavioral probes dnns machine learning purpose quite limited; aware zoran goodfellow used psychophysics-like experiments better understand image processing models. delve speciﬁcs shape bias one-shot word learning describe approach general context inductive biases probe datasets statistical learning. suppose data xi}n goal build model data optimize loss function measuring ||yi g||. perhaps data images imagenet objects classiﬁed images histology tumors classiﬁed statistical learner minimize discovering properties input predictive labels discovered predictive properties effect properties trained model inductive bias. examples properties include shape imagenet objects number nodes tumor particular constellation blood test values often precedes exacerbation pneumonia symptoms. critically real-world datasets these discovered properties unlikely correspond single feature input instead correspond complex conjunctions features. could describe properties using function which example returns shape focal object given imagenet image number nodes given scan tumor. indeed articulate difﬁculty understanding dnns often can’t intuitively describe conjunctions features although often numerical representations intermediate layers they’re often arcane interpret. advocate addressing problem using following hypothesis-driven approach first propose property model using. critically it’s necessary function evaluated using automated method. instead intention function humans intuitively evaluate. property believed relevant problem object shape number tumor nodes. proposing property next step generate predictions model behave given various inputs fact uses bias respect property then construct carry experiment wherein predictions tested. order execute experiment typically necessary craft probe examples cover relevant portion range example variety object shapes. results experiment either support fail support hypothesis model uses solve task. process especially valuable situations little training data available important regions input space practitioner needs know trained model behave region. psychologists developed repertoire hypotheses experiments effort understand human mind. explore application theory-experiment pairs state one-shot learning discussions one-shot word learning psychological literature inevitably begin philosopher w.v.o. quine broke problem described computationally challenging components enormous number tenable hypotheses learner explain single observed example. make point quine penned now-famous parable ﬁeld linguist gone visit culture whose language entirely different linguist trying learn words helpful native rabbit runs past. native declares gavagai linguist left infer meaning word. quine points linguist faced abundance possible inferences including gavagai refers rabbits animals white things speciﬁc rabbit undetached parts rabbits. quine argues indeed inﬁnity possible inferences made uses conclusion bolster assertion meaning cannot deﬁned terms internal mental events. contrary quine’s intentions example introduced developmental psychology community macnamara spurred give idea internal meaning instead posit test cognitive biases enable children eliminate broad swaths hypothesis space variety hypothesis-eliminating biases proposed including whole object bias children assume word refers entire object components taxonomic bias children assume word refers basic level category object belongs mutual exclusivity bias children assume word refers object category shape bias concerned variety others biases tested empirically experiments wherein children adults given object along novel name asked whether name apply various objects. taken whole work yielded computational level account word learning whereby people make biases eliminate unlikely hypotheses inferring meaning words. contrasting complementary approaches explaining word learning exist psychological literature including association learning bayesian inference leave application theories deep learning models future work focus determining insight gained applying hypothesis elimination theory methodology. begin present work knowledge part hypothesis elimination theory correct models surely kind inductive biases since statistical learning machines successfully model mapping images object labels. however several questions remain open. predictive properties dnns ﬁnd? properties? properties interpretable humans? properties children use? biases change course training? address questions carry experiments analogous landau enables test whether shape bias human interpretable feature used children learning language visible behavior inception networks. furthermore able test whether models well different instances them display bias. next section describe detail one-shot word learning problem inception networks solve one-shot word learning task label novel data example novel class label single example. speciﬁcally given support images associated labels unlabelled probe image one-shot learning task identify true label probe image support labels distance function. function parameterised inception best performing imagenet classiﬁcation models speciﬁcally returns features last layer pre-trained inception classiﬁer inception classiﬁer trained using rms-prop described szegedy section features input cosine distance distance function classiﬁer equation achieves accuracy one-shot classiﬁcation imagenet dataset henceforth call inception classiﬁer together nearest-neighbor component inception baseline model. also investigate state-of-the-art one-shot learning architecture called matching nets fully differentiable neural network architecture state-of-the-art shot learning performance imagenet cosine distance provide context-dependent embeddings embedding bi-directional lstm support provided input sequence. embedding lstm read-attention mechanism operating entire embedded support set. input lstm given penultimate layer features pre-trained deep convolutional network speciﬁcally inception baseline model described train proceed follows step training model given small support images associated labels. addition support model unlabelled probe image model parameters updated improve classiﬁcation accuracy probe image given support set. parameters updated using stochastic gradient descent learning rate update labels training randomly re-assigned image classes critical step. prevents learning consistent mapping category label. usually classiﬁcation want one-shot learning want train model classiﬁcation viewing single in-class example support set. formally objective function cognitive psychology probe data consists images objects images arranged triples consisting probe image shape-match image color-match image dataset triples shown different backgrounds giving total triples. images generously provided cognitive psychologist linda smith. images photographs stimuli used previously shape bias experiments conducted cognitive development indiana university. potentially confounding variables background content object size controlled dataset. also assembled real-world dataset consisting images objects collected using google image search. again images arranged triples consisting probe shape-match colour-match. probe image chose images real objects unlikely appear standard image datasets imagenet. data contains irregularity figure example images cognitive psychology dataset data consists image triples containing colour match image shape match image probe image triples calculate shape bias reporting proportion times model assigns shape match image class probe image. dataset supplied cognitive psychologist linda smith designed control object size background. real world also probing models’ properties outside image space covered training data. shape-match image chose object similar shape colourmatch image chose object similar colour example triple consists silver tuning fork probe silver guitar capo colour match black tuning fork shape match. photo dataset contains single object white background. collected data strengthen conﬁdence results obtained cogpsych dataset demonstrate ease probe datasets constructed. authors crafted dataset solely using google image search span roughly days’ work. results dataset especially fact bias pattern time matches results well established cogpsych dataset support contention practitioners collect effective probe datasets minimal time expenditure using readily available tools. provide features nearest-neighbour one-shot classiﬁer probed model using cogpsyc dataset. speciﬁcally given probe image loaded shape-match image corresponding label along colour-match image corresponding label memory support calculated using equation model assigned either probe image. estimate shape bias calculated proportion shape labels assigned probe experiments using euclidean cosine distance distance function. found results distance functions qualitatively similar report results euclidean distance. found shape bias similarly shape bias using real-world dataset together results strongly suggest trained imagenet stronger bias towards shape colour. note that expected shape bias model qualitatively similar across datasets quantitatively different largely datasets quite different. indeed datasets chosen quite different could explore broad space possibilities. particular cogpsyc dataset backgrounds much larger variability real-world dataset backgrounds real-world dataset objects much greater variability cogpsyc dataset objects. next probed using similar procedure. used trained previous section provide input features described section then following training procedure outlined section trained one-shot word learning imagenet achieving state-of-the-art performance reported then repeating analysis above found shape bias using cogpsyc dataset bias using real-world dataset. interesting note bias values similar bias values. figure shape bias across models different initialization seeds within models training calculated using cogpsyc dataset. shape bias inception models calculated throughout training strong shape bias emerges across models. bias value indicates shape bias indicates colour bias. examples highlighted clarity. shape bias ﬂuctuates strongly within models training three standard deviations. distribution bias values calculated start middle training. bias variability high start training. here distributions calculated using kernel density estimates shape bias measurements models within indicated window. answer questions extended shape bias analysis described calculate shape bias population models population models different random initialization ﬁrst calculated dependence shape bias initialization surprisingly observed strong variability depending initialization. cogpsyc dataset average shape bias standard deviation training real-world dataset average shape bias next calculated dependence shape bias model performance. cogpsych dataset correfigure scatter plot showing matching network bias function inception bias. receives input inception model. point scatter plot bias bias inception model providing input particular total bias values models plotted lation bias classiﬁcation accuracy pone tail real-world dataset correlation pone tail therefore ﬂuctuations bias cannot accounted ﬂuctuations classiﬁcation accuracy. surprising classiﬁcation accuracy models similar training shape bias variable. demonstrates models variable behaviour along important dimensions performance measured another figure shape bias across models different initialization seeds within models training calculated using real-world dataset. shape bias inception models calculated throughout training strong shape bias emerges across models. examples highlighted clarity. shape bias ﬂuctuates strongly within models training. distribution bias values calculated start middle training. bias variability high start training. finally compare shape bias within models training models training. during training shape bias within ﬂuctuates signiﬁcantly contrast shape bias ﬂuctuate training instead model inherits shape bias characteristics start training provides input embeddings shape-bias remains constant throughout training. moreover evidence corresponding bias values different note ﬁne-tune inception model providing input training observe shape-bias properties independent model properties. psychology-inspired approach understanding dnns produced number insights. firstly found trained imagenet display strong shape bias. important result practitioners routinely models especially applications known priori colour important shape. illustrative example practitioner planned build one-shot fruit classiﬁcation system proceed caution plan pre-trained imagenet models like inception fruit often deﬁned according colour features rather shape. second surprising ﬁnding large variability shape bias within models training across models depending randomly chosen initialisation model. variability arise models explicitly optimised shape biased categorisation. important result shows models created equally models stronger preference shape others even though architecturally identical almost identical classiﬁcation accuracy. third ﬁnding retain shape bias statistics downstream inception network demonstrates possibility biases propagate across model components. case shape bias propagates inception model memory modules. result another cautionary observation; combining multiple modules together must aware contamination unknown properties across modules. indeed bias benign module might detrimental effect combined later modules. natural question immediately arises results remove unwanted bias induce desirable bias? biases consideration properties architecture dataset synthesized together optimization procedure. such observation shapebias partly result statistics natural imagelabellings captured imagenet dataset partly result architecture attempting extract statistics. therefore discovering unwanted bias practitioner either attempt change model architecture explicitly prevent bias emerging attempt manipulate training data. neither possible example appropriate data manipulation expensive bias cannot easily suppressed architecture possible zero-th order optimization models. example perform post-hoc model selection either using early stopping selecting suitable model initial seeds. important caveat note behavioral tools often provide insight neural mechanisms. case mechanism whereby model parameters input images interact give rise shape bias elucidated expect happen. indeed cognitive psychology often neuroscience computational level insights provide starting point research mechanistic level. example future work would interesting gradient-based visualization neuron ablation techniques augment current results identifying mechanisms underlying shape bias. convergence evidence introspective methods current behavioral method would create richer account models’ solutions one-shot word learning problem. one-shot learning case study demonstrated utility leveraging techniques cognitive psychology understanding computational properties dnns. wide ranging literature cognitive psychology describing techniques probing spectrum behaviours humans. work leads study artiﬁcial cognitive psychology application techniques better understand dnns. example would useful apply work massive literature episodic memory recent ﬂurry episodic memory architectures apply techniques semantic cognition literature recent models concept formation generally rich psychological literature become increasingly useful understanding deep reinforcement learning agents learn solve increasingly complex tasks. work demonstrated techniques cognitive psychology leveraged help better understand dnns. case study measured shape bias powerful poorly understood dnns inception mns. analysis revealed previously unknown properties models. generally work leads future exploration dnns using rich body techniques developed cognitive psychology. would like thank linda smith charlotte wozniak providing cognitive psychology probe dataset; charles blundell reviewing paper prior submission; oriol vinyals daan wierstra peter dayan daniel zoran osband karen simonyan helpful discussions; james besley legal assistance; deepmind team support. previous attempts model human word learning cognitive science literature however none models capable one-shot word learning scale real-world images. solve task scale emulate hallmark experimental ﬁndings propose computational-level account human one-shot word learning. another feature results supports contention model shape bias increases dramatically early training similarly humans show shape bias much strongly adults children older children show bias strongly younger children good cognitive model should dnns make testable predictions word-learning humans. speciﬁcally current results predict shape bias vary across subjects well within subject course development. also predict humans adult-level one-shot word learning abilities correlation shape bias magnitude oneshot-word learning capability. another promising direction future cognitive research would probe additional biases order predict novel computational properties humans. probing model much faster running human behavioural experiments wider range hypotheses blundell charles uria benigno pritzel alexander yazhe ruderman avraham leibo joel jack wierstra daan hassabis demis. model-free arxiv preprint arxiv. episodic control. caruana rich gehrke johannes koch paul sturm marc elhadad noemie. intelligible models healthcare predicting pneumonia risk hospital proceedings -day readmission. sigkdd international conference knowledge discovery data mining graves alex wayne greg reynolds malcolm harley danihelka grabska-barwi´nska agnieszka colmenarejo sergio g´omez grefenstette edward ramalho tiago agapiou john hybrid computing using neural network dynamic external memory. nature gregor karol besse frederic rezende danilo jimenez danihelka wierstra daan. towards concepadvances neural information tual compression. processing systems higgins irina matthey loic glorot xavier arka uria benigno blundell charles mohamed shakir lerchner alexander. early visual concept learnarxiv preprint unsupervised deep learning. arxiv. kourou konstantina exarchos themis exarchos konstantinos karamouzis michalis fotiadis dimitrios machine learning applications cancer prognosis prediction. computational structural biotechnology journal plaut david mcclelland james seidenberg mark patterson karalyn. understanding normal impaired word reading computational principles quasiregular domains. psychological review zoran daniel isola phillip krishnan dilip freeman william learning ordinal relationships mid-level vision. proceedings ieee international conference computer vision raposo david santoro adam barrett david g.t. pascanu razvan lillicrap timothy battaglia peter. discovering objects relations entangled scene representations. arxiv preprint arxiv. santoro adam bartunov sergey botvinick matthew wierstra daan lillicrap timothy. meta-learning memory-augmented neural networks. proceedings international conference machine learning schilling savannah sims clare colunga eliana. taking development seriously modeling interactions emergence different word learning biases. cogsci szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. proceedings going deeper convolutions. ieee conference computer vision pattern recognition szegedy christian vanhoucke vincent ioffe sergey shlens jonathon wojna zbigniew. rethinking inception architecture computer vision. arxiv preprint arxiv.", "year": 2017}