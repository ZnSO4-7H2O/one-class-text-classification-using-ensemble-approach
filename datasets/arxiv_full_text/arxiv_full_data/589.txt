{"title": "How Transferable are Neural Networks in NLP Applications?", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain. It is particularly important to neural networks, which are very likely to be overfitting. In some fields like image processing, many studies have shown the effectiveness of neural network-based transfer learning. For neural NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an illuminating picture on the transferability of neural networks in NLP.", "text": "transfer learning aimed make valuable knowledge source domain help model performance target domain. particularly important neural networks likely overﬁtting. ﬁelds like image processing many studies shown effectiveness neural network-based transfer learning. neural however existing studies casually applied transfer learning conclusions inconsistent. paper conduct systematic case studies provide illuminating picture transferability neural networks nlp. transfer learning sometimes known domain adaptation plays important role various natural language processing applications especially large enough datasets task interest scenarios would like transfer adapt knowledge domains mitigate problem overﬁtting improve model performance traditional feature-rich kernel-based models researchers developed variety elegant methods domain adaptation; examples include easyadapt parameters source task initialize network target task; alternatively also train tasks simultaneously parameters shared. performance veriﬁed empirical experiments. existing studies already shown evidence transferability neural features. example image processing low-level neural layers closely resemble gabor ﬁlters color blobs transferred well different tasks. donahue suggest high-level layers also transferable general visual recognition; yosinski investigate transferability neural layers different levels abstraction. although transfer learning promising image processing conclusions appear less clear applications. image pixels low-level signals generally continuous less related semantics. contrast natural language tokens discrete word well reﬂects thought humans neighboring words share much information pixels images previous neural studies casually applied transfer techniques results consistent. collobert weston apply multi-task learning obtain .–.% error reduction bowman contrary improve natural language inference task accuracy initializing parameters additional dataset samples. therefore systematic studies needed shed light transferring neural networks ﬁeld nlp. transferring knowledge semantically similar/equivalent task different dataset; transferring knowledge task semantically different shares neural topology/architecture neural parameters indeed transferred. distinguish transfer methods using parameters trained initialize multi-task learning i.e. training simultaneously. study mainly focuses following research questions acronyms refer semantic role labeling named entity recognition part-of-speech tagging chunking respectively. here quote accuracies obtained using unsupervised pretraining word embeddings. highest performance paper; using pretrained word embeddings also common practice literature. conducted extensive experiments datasets classifying sentences sentence pairs. leveraged widely-used convolutional neural network long short term memory -based recurrent neural network models. based experimental results following main observations unexpected. whether neural network transferable depends largely semantically similar tasks different consensus image processing. rest paper organized follows. section introduces datasets neural models transferred across; section details neural architectures experimental settings. describe approaches transfer learning section present experimental results sections concluding remarks section dvd-cca appealed state supreme court. appealed decision u.s. supreme court. earnings share recurring operations cents cents. beat company’s april earnings forecast cents share. semantically different transfer examples also illustrated table demonstrate semantic relatedness. noticed image speech processing input neural networks pretty much consists signals; hence low-level feature detectors almost always transferable even yosinski manually distinguish artiﬁcial objects natural ones image classiﬁcation task. relatedness—which emerges layers either word embeddings successive hidden layer—is speciﬁc also insight paper. shall sections transferability neural networks sensitive semantics image processing. group used single neural model solve three problems uniﬁed manner. neural architecture among three datasets makes possible investigate transfer learning regardless whether tasks semantically equivalent. concretely neural models follows. sentence according sentiment question type recurrent neural network long short term memory units softmax layer added last word’s hidden state classiﬁcation. siamese architecture classify relation sentences. ﬁrst apply convolutional neural network window size model local context pooling layer gathers information ﬁxed-size vector. sentence vectors concatenated hidden layer softmax output. table accuracy without transfer. also include related models comparison showing achieved comparable results thus ready investigate transfer learning. models source domains could transfer particular model instead average several models. applied stochastic gradient descent minibatch size optimization. setting tuned hyperparameters follows learning rate power decay learning rate {fast moderate low} regularized network dropout rate note might nonsensical settings e.g. larger dropout rate network already underﬁtting report test performance associated highest validation accuracy. setup baseline trained models withtransfer times different random parameter initializations achieved reasonable performance comparable similar models reported literature datasets. therefore implementation fair suitable study transfer learning. transfer learning aims knowledge source domain target domain. neural networks usually trained incrementally gradient descent straightforward gradient information source target domains optimization accomplish knowledge transfer. depending samples source target domains scheduled main approaches neural network-based transfer learning parameter initialization init approach ﬁrst trains network directly uses tuned parameters initialize network transfer parameters target domain i.e. training performed labeled data available would better ﬁne-tune autoencoders approaches parameters trained unsupervised transferred initialize model supervised task however paper focuses supervised pretraining means transfer knowledge labeled source domain. individual cost function domain. hyperparameter balancing domains. nontrivial optimize equation practice gradient-based methods. take partial derivative thus goes learning rate model vulnerable likely blow large learning rates stuck local optima small ones. collobert weston alternatively choose data sample either domain certain probability take derivative particular data sample. domain transfer independent learning rates able fully entire dataset large. adopted latter approach experiment simplicity. formally multi-task learning strategy follows. combination ﬁrst pretrain source domain parameter initialization train simultaneously. theoretical perspective init mult work different ways. mult approach source domain regularizes model aliasing error surface target domain; hence neural network less prone overﬁtting. init error surface remains intact. training target dataset parameters initialized meaningful contain additional knowledge source domain. however extreme case error surface convex init ineffective parameters reach global optimum regardless initialization. practice deep neural networks usually highly complicated non-convex error surfaces. properly initializing parameters knowledge reasonably expect parameters better catchment basin init approach transfer knowledge results transferring init ﬁrst analyze init behaves nlp-based transfer learning. addition different transfer scenarios regarding semantic relatedness described section evaluated settings ﬁne-tuning parameters freezing parameters transfer existing evidence shows frozen parameters would generally hurt performance setting provides direct understanding transferable features therefore included experiments. moreover transferred parameters layer layer answer second research question. subsections initialized parameters ones corresponding highest validation accuracy subsection investigated parameters ready transferred training overall performance table shows main results init. quick observation that groups transfer learning semantically equivalent tasks appears successful improvement results surprising also reported bowman imdb→qc snli→msrp however improvement transferring hidden layers namely lstm-rnn units feature maps. setting yields slight degradation .–.% std. incapability transferring also proved locking embeddings hidden layers formance even worse majority-class guess msrp. examining training accuracy respectively conclude extracted features lstm-rnn models almost irrelevant ultimate tasks although previous studies researchers mainly drawn positive conclusions transfer learning negative result similar upon careful examination collobert weston unfortunately results somewhat misinterpreted. paper authors report transferring pretrained word embeddings improves task accuracy gain mainly word embeddings. settings pretrained word embeddings together improve accuracy results rather frustrating indicating neural networks transferable tasks different semantics. transfer learning prone semantics image processing domain even highlevel feature detectors almost always transferable layer-by-layer analysis answer next analyze transferability layer. first freeze embeddings hidden layers even semantically equivalent settings freeze output layer performance imdb→mr snli→sick drops randomly initializing output layer’s parameters obtain similar higher result compared baseline ho). ﬁnding suggests output settings eho) imdb→mr experiment suggests embeddings hidden layer play important role improving accuracy snli→sick however main improvement lies hidden layer. plausible explanation sentiment classiﬁcation tasks information emerges input i.e. sentiment lexicons thus embeddings natural language inference tasks address semantic compositionality thus hidden layers important. bowman suggest transferring large learning rate damage knowledge stored parameters; paper transfer learning rate information addition parameters. although rule thumb choose hyperparameters—including learning rate—by validation curious whether conjecture holds. estimating rough range sensible hyperparameters ease burden model selection; also provides evidence better understand transfer learning actually works. plot learning curves different learning rates figure large learning rate like accuracy increases fast peaks earlier epochs. training small learning rate slow peak performance comparable large learning rates iterated epochs. learning curves figure similar classic speed/variance trade-off following additional discovery init transferring learning rate information necessarily useful. large learning rate damage knowledge stored pretrained hyperparameters accelerates training process large extent. need perform validation choose learning rate computational resources available. figure learning curves accuracies parameters transferred certain epoch training dotted lines refer non-transfer equivalently viewed transferring training i.e. epoch note x-axis shares across different subplots. experiments transfer parameters achieve highest validation performance straightforward intuitive practice. however imagine parameters well-tuned source dataset speciﬁc i.e. model overﬁts thus underﬁt another advantage early transfer lies computational concerns. manage transfer model parameters epochs save much time especially large. therefore made efforts studying neural model ready transferred. figures plot learning curves source tasks. accuracy increases sharply epochs later reaches plateau still growing slowly. transferred parameters different stages training target tasks accuracies plotted imdb→mr source performance transferring performance align well. snli→sick experiment however produces interesting unexpected results. using second epoch snli’s training yields highest transfer performance sick i.e. snli performance comparatively later transfer performance decreases gradually ∼.%. results experiments inconsistent lack explanation. beddings hidden layers analogous init. combining mult init used pretrained parameters embeddings hidden layers initialize multi-task training visually represented mult mult+init hyperparameter balancing source target tasks tuned granularity friendly reminder refers using only; refers using only. ﬁnding small yields high performance mult imdb+mr snli+sick experiments tuned ﬁnegrained granularity racy target tasks inability transfer cross-checked init method section sick hand transferability neural model also consistently positive supporting conclusion neural transtransfer performance mult+init remains high different values parameters given init already conveyed sufﬁcient information source task mult+init consistently outperforms non-transferring large margin. peak performance however higher mult init. summary answer follows experiments mult init generally comparable; obtain gain combining mult init. paper addressed problem transfer learning neural network-based applications. conducted series experiments datasets showing transferability neural models depends largely semantic relatedness source target tasks different domains like image processing. analyzed behavior different neural layers. also experimented transfer methods parameter initialization multi-task learning besides reported additional studies sections paper provides insight transferability neural models; results also help better understand neural features general. transferable conclusions paper? concede empirical studies subject variety factors conclusions vary different scenarios. paper tested results groups experiments involving datasets neural models models tasks widely studied literature chosen deliberately. results mostly consistent along analyzing experimental data also collected related results previous studies serving additional evidence answering research questions. future work. work also points future directions research. example would like analyze effect different mult strategies. efforts also needed developing effective robust method multi-task learning. thank reviewers constructive comments bowman helpful suggestion vicky discussion manuscript. research supported national basic research program china under grant national natural science foundation china grant nos.", "year": 2016}