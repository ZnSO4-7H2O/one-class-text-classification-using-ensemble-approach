{"title": "Picture It In Your Mind: Generating High Level Visual Representations  From Textual Descriptions", "tag": ["cs.IR", "cs.CL", "cs.CV", "cs.NE"], "abstract": "In this paper we tackle the problem of image search when the query is a short textual description of the image the user is looking for. We choose to implement the actual search process as a similarity search in a visual feature space, by learning to translate a textual query into a visual representation. Searching in the visual feature space has the advantage that any update to the translation model does not require to reprocess the, typically huge, image collection on which the search is performed. We propose Text2Vis, a neural network that generates a visual representation, in the visual feature space of the fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis optimizes two loss functions, using a stochastic loss-selection method. A visual-focused loss is aimed at learning the actual text-to-visual feature mapping, while a text-focused loss is aimed at modeling the higher-level semantic concepts expressed in language and countering the overfit on non-relevant visual components of the visual loss. We report preliminary results on the MS-COCO dataset.", "text": "paper tackle problem image search query short textual description image user looking for. choose implement actual search process similarity search visual feature space learning translate textual query visual representation. searching visual feature space advantage update translation model require reprocess typically huge image collection search performed. propose textvis neural network generates visual representation visual feature space fc-fc layers imagenet short descriptive text. textvis optimizes loss functions using stochastic loss-selection method. visual-focused loss aimed learning actual text-to-visual feature mapping text-focused loss aimed modeling higherlevel semantic concepts expressed language countering overﬁt non-relevant visual components visual loss. report preliminary results ms-coco dataset. using textual query retrieve images common cross-media search task text eﬃcient media describe kind image user searching for. actual retrieval process implemented number ways depending shared search space text images deﬁned. search space based permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copyrights third-party components work must honored. uses contact owner/author. neu-ir sigir workshop neural information retrieval july pisa italy copyright held owner/author. using textual features common solution specially scale. image associated textual features extracted context eventually enriched means classiﬁers assign textual labels related presence certain relevant entities abstract properties image. textual search space model exploit actual visual content image classiﬁers concepts interest available thus requiring relevant number classiﬁers; also requires reprocess entire image collection whenever classiﬁer made available. side visual joint search spaces represent image visual features extracted actual content. method propose paper adopts visual space search model. textual query converted visual representation visual space search performed similarity. advantage model improvement text representation model conversion visual features immediate beneﬁts image retrieval process without requiring reprocess whole image collection. joint space model requires instead reprocessing images whenever textual model updated since projection images joint space inﬂuenced also textual model part. also requires managing storing additional joint space representations used cross-media search. paper present preliminary results learning textvis neural network model converts textual descriptions visual representations space extracted deep convolutional neural networks imagenet textvis achieves goal using stochastic loss choice separate loss functions textual representations autoencoding visual representations generation. preliminary results show produced visual representations capture high level concepts expressed textual description. deep learning deep convolutional neural networks particular recently shown impressive performance number multimedia information retrieval tasks deep learning methods learn representations data multiple levels abstraction. result activation hidden layers used context transfer learning content-based image retrieval high-level representations visual content. somewhat similarly distributional semantic models produced wordvec glove found useful modeling semantic similarities among words establishing correlation word meaning position vector space. order perform cross-media retrieval feature spaces become comparable typically learning properly diﬀerent sources. problem attempted different manners could roughly grouped three main variants depending whether mapping performed common space textual space visual space. mapping common space idea comparing texts images shared space investigated means cross-modal factor analysis canonical correlation analysis similar vein corr-ae proposed cross-modal retrieval allowing search performed directions i.e. text-to-image viceversa idea train autoencoders image domain another textual domain imposing restrictions two. seen architecture presenting bears resemblance architectures investigated so-called correspondence full-modal autoencoder contrarily multimodal architectures though apply stochastic criterion jointly optimize modals thus refraining combining parametric single loss. trains deep neural network images directly bag-of-words space cosine similarity bows representations used generate ranking somehow similarly dedicated area related research focused generating captions describing salient information image important examples along lines devise conse methods build upon higher layers convolutional neural network main diﬀerence lies methods treat last layer net. whereas devise replaces last layer linear mapping conse side directly takes outputs last layer learns projection textual embedding space. mapping visual space proposal textvis belongs group where best knowledge example method dubbed wordvisualvec reported recently. fundamental points method diﬀer though. hand wordvisualvec takes combinations wordvec-like vectors starting point thus reducing dimensionality input space; directly start bag-of-words vector encoding textual space observed improvement pre-training textual part. other build deep network textual representation. shall seen textvis much shallower found capable mapping textual vectors visual space quite eﬃciently provided model properly regularized; issue focused attention. section describe architecture textvis network. idea textual descriptions highlevel visual representations. visual space used layers hybrid network trained imagenet places datasets). tested vectorial representations textual descriptions textvis uses simple bag-of-words vectors mark value positions relative words appear textual description leave zero others; textvis adds text structure info considering also n-grams selection part-of-speech patterns. textvis ﬁrst approach modeling text structure input vectorial representation differentiates task search detailed/complex textual description traditional keyword search. also investigated pre-trained word embeddings representing textual description average embeddings words composing description observed improvement. generating word embeddings additional cost ﬁtness embeddings task depends type documents learned from. example improvement reported learning embedding flickr tags compared learning wikipedia pages. direct bagof-words vectors textvis removes variable selecting appropriate document collection learn embedding learning cost. described following textvis actually learns description embedding space able reconstruct original description visual description. reach this started simple regressor model trained directly predict visual representation image associated textual input. observed strong tendency overﬁt thus degrading applicability method unseen images. explained overﬁtting fact visual representation keeps track every element appears image regardless semantic relevance within image textual description likely focused visually relevant information disregarding secondary content image shown figure learning iterations proceed simple regressor model starts capturing secondary elements images relevant main represented concept somecharacteristic training data. http//image-net.org http//places.csail.mit.edu/index.html considered part-of-speech patterns ‘nounverb’ ‘verbprt’ ‘verb-verb’ ‘num-noun’ ‘noun-noun’. model parameters independent branch. optimization problem objectives iteration random choice decides optimized. call heuristic stochastic loss optimization. iteration. tout text-to-text branch autoencoder. pieces text semantically equivalent text-to-text branch might reminiscent skip-gramcbow like architectures. text-to-image branch case regressor. causes model co-regularized. notwithstanding since ﬁnal goal project textual descriptor visual space text-to-text branch might though regularization visual reconstruction responds constrains linguistic nature. used microsoft coco dataset mscoco originally proposed image recognition segmentation caption generation. although datasets image retrieval exist oriented keyword-based queries. believe mscoco scenario want explore since captions associated images expressed natural language thus semantically richer short list keywords composing query. mscoco contains training images validation images test images corresponding diﬀerent competitions mscoco proposed caption generation captions accessible textvis proposal contrast overﬁtting text-to-text autoencoding branch hidden layer forcing model satisfy losses visual linguistic linguistic loss works higher level abstraction visual acting regularization constraint model preventing conﬁrmed experiments overﬁtting visual loss detailed next section implemented losses stochastic process iteration selected optimization. textvis consists overlapped feedforward neural nets shared hidden layer. shared hidden layer causes regularization eﬀect combined optimization; i.e. hidden state constrained good representation accomplish diﬀerent goals. feedforward computation described following equations represents bag-of-words encoding textual descriptor given input hidden representation visual textual predictions respectively obtained hidden representation bi}i∈{} model parameters relu max{ predictions confronted expected outputs visual representation corresponding layers textual descriptor tout semantically equivalent tin. used mean squared error loss function cases model thus multi-objective many alternative strategies could followed point order parameters criteria jointly minimized. rather propose much simpler eﬀective carrying optimization search consists considering branches independent randomly deimage retrieval performed similarity search visual space using euclidean distance l-normalized visual vectors generate ranking images sorted closeness. measure retrieval eﬀectiveness visual representations produced textual descriptions textvis network means discounted cumulative gain deﬁned estimate using metric. evaluation measures mscoco caption generation competition compute reli query caption captions associated retrieved image rank captionto-caption relevance model thus aimed measuring much concepts expressed query appear relevant parts retrieved images. compared performance textvis textvis models against rrank lower bound baseline produces random ranking images query; vissim direct similarity method computes euclidean distances using original features image associated query caption mscoco; visreg text-to-image regressor described figure table reports averaged scores obtained compared methods. results show signiﬁcant improvement proposal respect compared methods. using visual space textvis obtains relative improvement respect vissim visreg. improvements textvis respectively using visual representation image taken layer hybrid network tout textual descriptors representing input output descriptors model respectively. training tout uniformly chosen random note number training instances could extract given amounts increases solve optimization problems equations using adam method stochastic optimization default parameters note inlt lection probability diﬀerent distributions size training batch examples. maximum number iterations apply early stop model starts overﬁtting training shuﬄed time complete pass images completed. vocabulary size textvis removing terms appearing less captions. textvis considered uni-grams n-grams appearing least captions. since number units hidden output layers respectively total number parameters models amount textvis textvis minimized single optimizer compared approaches case equal relevance losses better optimizes losses less prone overﬁt. deem allows model natural relative relevance various losses combined i.e. selecting losses proportion assigned relevance whereas numeric aggregation aﬀected relative values losses diﬀerences variation optimization also computationally lighter aggregated loss updates part model iteration. figure show samples highlight diﬀerences results three compared methods. cases results vissim method dominated main visual features images face ﬁrst query content screen second query outdoor image light lower part plants people third one. text based methods obtains results figure cumulative probability distribution diﬀerence performance textvis respect vissim visreg positive diﬀerences mean textvis obtained better ranking score vissim visreg addition averaged performance also investigated often ranking produced textvis relevant produced vissim visreg. figure indicates cases ranking textvis found relevant vissim happens cases comparing textvis visreg. often contain elements description. ﬁrst query textvis retrieves four relevant images visreg. queries results pretty similar textvis placing second position image perfect match query visreg places ﬁfth position. preliminary experiments indicate method produces relevant rankings produced similarity search directly visual features query image. indication text-to-image mapping produces better prototypical representations desired scene representation sample image itself. simple explanation result textual descriptions strictly emphasize relevant aspects scene user mind whereas visual features directly extracted query image keeping track information contained image causing similarity search potentially confused secondary elements scene. textvis model also improved smaller margin visreg model showing auto-enconding branch network useful avoid overﬁtting visual features. also found combing losses stochastic fashion rather numerically improves eﬀectiveness eﬃciency system. future plan compare textvis recently proposed wordvisualvec model. also intend improve modeling word order information textvis likely adding recurrent component network architecture. costa pereira coviello doyle rasiwasia lanckriet levy vasconcelos. role correlation abstraction cross-modal multimedia retrieval. pattern analysis machine intelligence ieee transactions x.-s. yang wang wang wang clickage towards bridging semantic intent gaps mining click logs search engines. proceedings international conference multimedia pages summaries. marie-francine moens editor text summarization branches proceedings acl- workshop pages barcelona spain july association computational linguistics. t.-y. maire belongie hays perona ramanan doll´ar zitnick. microsoft coco common objects context. computer vision–eccv pages springer carlsson. features oﬀ-the-shelf astounding baseline recognition. proceedings ieee conference computer vision pattern recognition workshops pages", "year": 2016}