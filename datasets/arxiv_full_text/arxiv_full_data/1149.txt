{"title": "Training Convolutional Networks with Noisy Labels", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings manual annotation of the data is impractical; instead our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark.", "text": "availability large labeled datasets allowed convolutional network models achieve impressive recognition results. however many settings manual annotation data impractical; instead data noisy labels i.e. freely available label image accurate. paper explore performance discriminatively-trained convnets trained noisy data. introduce extra noise layer network adapts network outputs match noisy label distribution. parameters noise layer estimated part training process involve simple modiﬁcations current training infrastructures deep networks. demonstrate approaches several datasets including large scale experiments imagenet classiﬁcation benchmark. recent years convolutional networks shown impressive results image classiﬁcation tasks however achievement relies availability large amounts labeled images e.g. imagenet labeling images hand laborious task impractical many problems. alternative approach labels obtained easily user tags social network sites keywords image search engines. catch labels reliable contain misleading information subvert model training. given abundance tasks noisy labels available important understand consequences training convnet them contributions paper. image classiﬁcation real-world settings types label noise dominate label ﬂips example erroneously given label another class within dataset outliers image belong classes consideration mistakenly labels. fig. shows examples cases. consider scenarios explore variety noise levels datasets. contrary expectations standard convnet model proves surprisingly robust types noise. inevitably high noise levels signiﬁcant performance degradation occurs. consequently propose novel modiﬁcation convnet enables effectively trained data high level label noise. modiﬁcation simply done adding constrained linear noise layer softmax layer adapts softmax output match noise distribution. demonstrate model handle label outlier noise. linear layer rest model trained end-to-end conventional backpropagation thus automatically learning noise distribution without supervision. model also easy implement existing convnet libraries readily scale imagenet-sized problems. figure classiﬁcation example classes illustrating types label noise encountered real datasets. label case images belong classes sometimes labels confused them. outlier case images unrelated classiﬁcation task possess labels. classiﬁcation model degradation performance inevitable noise training data especially noise labels harmful noise input features label noise complex phenomenon. several types noise labels also noise source different. example label noise caused unreliable labeling cheap fast framework amazon mechanical turk noise introduced labels intentionally protect people privacy simple approach handle noisy labels data preprocessing stage labels suspected incorrect removed corrected however weakness approach difﬁculty distinguishing informative hard samples harmful mislabeled ones instead paper focus models robust presence label noise. effects label noise well studied common classiﬁers robust variants proposed recently natarajan proposed generic unbiased estimator binary classiﬁcation noisy labels. employed surrogate cost function expressed weighted original cost functions gave theoretical bounds performance. considering recent success deep learning relatively little work application noisy data. mnih hinton larsen noise modeling incorporated neural network proposed model. however binary classiﬁcation considered mnih hinton larsen assumed symmetric label noise therefore single noise parameter tuned cross-validation. paper consider multi-class classiﬁcation assume realistic asymmetric label noise makes impossible cross-validation adjust noise parameters unsupervised pre-training deep models received much attention particularly relevant work auto-encoders layer-wise pre-train models. however performance eclipsed purely discriminative convnet models trained large labeled paradigm consider data noisy labels unknown fraction trustworthy. assume availability clean labels e.g. provided human. contrasts semi-supervised learning fraction data high quality labels rest either unlabeled unreliable labels. although closely related fact approaches complementary another. given large data noisy labels requires annotate subset. ones choose? absence external information forced pick random. however inefﬁcient labeler resources since fraction points near decision boundaries random sample unlikely contain many them. even settings non-uniform prior labels picking informative examples label challenging. example taking high ranked images returned image search engine might seem good strategy likely result prototypical images rather borderline cases. light this approach regarded natural precursor ssl. ﬁrst applying method noisy data train convnet identify subset difﬁcult examples presented human annotator. moreover practical settings several drawbacks make impractical apply unlike method. many popular approaches based spectral methods complexity problematic datasets range consider. fergus efﬁcient spectral approach make strong independence assumptions unrealistic practice. nystrom methods scale large drastically sub-sampling ﬁrst resulting loss structure within problem. contrast approach complexity number classes since model aggregate noise statistics classes rather estimating perexample weights. label flip noise denotes ﬁrst describe scenario label ﬂip. given training data parametrized probability transition matrix thus assume label ﬂips independent however model capacity model asymmetric label noise distributions opposed uniform label ﬂips models example model image likely mislabeled tree. probability input labeled noisy data computed using modify classiﬁcation model using probability matrix modiﬁes prediction match label distribution noisy data. prediction probability true labels classiﬁcation model. then prediction combined model given combined model consist parts base model parameterized noise model parameterized combined model trained maximizing cross entropy noisy labels model prediction given eqn. cost function minimize figure label noise modeled constrained linear layer inserted softmax cost layers. noise distribution becomes weight matrix layer. changes output probabilities base model distribution better matches noisy labels. training sequence learning noisy data. noise matrix initially identity base model trained inadvertently learning noise data. start updating also captures noise properties data leaving model make clean predictions. number training samples. however ultimate goal predict true labels noisy labels achieved make base model predict true labels accurately. quantify confusion matrix {cij} deﬁned training samples true label manage make equal identity means base model perfectly predicts true labels training data. note prediction modiﬁed noise model. also deﬁne confusion matrix {˜cij} combined model using eqn. follows note cannot actually measure reality unless know true labels. show minimizing training objective eqn. forces predicted distribution combined model close possible noisy label distribution training data asymptotically. objective eqn. becomes know true noise distribution non-singular eqn. setting would force converge identity. therefore training predict noisy labels using combined model parameterized directly forces base model predict true labels. base model convnet network softmax output layer noise model linear layer constrained probability matrix sits softmax layer shown figure role noise layer implement eqn. output softmax layer. therefore noise layer linear layer bias weights matrix since modiﬁcation network still perform back-propagation training. learning noise distribution previous section showed setting noise model optimal making base model accurately predict true labels. practice true noise distribution often unknown case infer noisy data itself. fortunately noise model constrained linear layer network means weights updated along weights network. done back-propagating cross-entropy loss matrix base model. taking gradient step model weights project back subspace probability matrices represents conditional probabilities. unfortunately simply minimizing loss eqn. give desired solution. follows training progresses confusion matrix combined model true noise distribution data. however alone cannot guarantee example given enough capacity base model start learning noise distribution hence implies actually inﬁnitely many solutions order force regularizer probability matrix forces diffuse trace norm ridge regression. regularizer effectively transfers label noise distribution base model noise model encouraging converge regularization reminiscent blind deconvolution algorithms. indeed noise distribution acts system diffusing predictions base model. diffusion kernel unknown necessary regularize ill-posed inverse problem pushing estimates away trivial identity kernel shows lower bound equality hold true therefore minimizing sensible make base model accurately predict clean labels. although proof true strong assumptions show empirically works well practice. also weight decay instead minimizing practice since already implemented deep learning packages similar effect diffusing ﬁnally describe learning procedure illustrated figure beginning training different general confusion matrix necessarily large elements diagonal. makes learning difﬁcult. therefore start training. illustrated left part figure point base model could learned noise training data want. therefore start updating along rest network using weight decay push away identity towards starts diffuse starts absorbing noise base model thus making base model accurate. however large weight decay would make diffused true hurt performance. case clean data cannot tune weight decay parameters validation. experiments paper weight decay parameter want make prediction test model clear data noise layer removed outlier noise another important setting case training samples belong existing signal classes. case create additional outlier class enables apply previously described noise model. number existing classes. then base network output probabilities last represents probability sample outlier. labels given outlier samples uniformly distributed across classes corresponding noise distribution unfortunately matrix singular would different network outputs exact point. simple solution problem extra outlier images label training data would make nonsingular noise distribution becomes note setting learning matrix ﬁxed given eqn. fraction outliers training required compute hyper-parameter must manually however experimentally demonstrate algorithm sensitive exact value. section empirically examine robustness deep networks without noise modeling. first perform controlled experiments deliberately adding types label noise clean datasets label noise outlier noise. show realistic experiments using datasets inherent label noise know true distribution noisy labels. three different image classiﬁcation datasets experiments. ﬁrst dataset google street-view house number dataset consists images house number digits captured google streetview. images training images testing. second challenging dataset cifar subset million tiny images dataset natural images labeled object categories. training images test images. last dataset imagenet large scale dataset images labeled classes. datasets preprocessing step mean subtraction except svhn also perform contrast normalization. imagenet data augmentation taking random crop random locations well horizontal ﬂips probability svhn cifar- model architecture hyperparameter settings given cudaconv conﬁguration layers-pct.cfg implements network three convolutional layers. settings kept ﬁxed experiments using datasets. imagenet model architecture described krizhevsky synthesize noisy data clean data stochastically changing labels original label randomly changed ﬁxed probability figure shows noise distribution used experiments. alter distribution changing probability diagonal generate datasets different overall noise levels. labels test images left unperturbed. svhn training noise model svhn data identity ﬁrst epochs. thereafter updated weight decay epochs. figure shows test errors different training sizes different noise levels. plots show normal model coping noise degrading badly. contrast addition noise layer allows model operate beyond this method breaks false labels overwhelm correct ones. overall addition noise model consistently achieves better accuracy compared normal deep network. ﬁgure also shows error rates noise model trained using true noise distribution performs well learned showing proposed method learning noise distribution data effective. figure shows example learned alongside ground truth used generate noisy data. difference negligible. figure shows effects label noise detail. color ﬁgure shows test errors contour lines indicates accuracy. without noise model performance drops quickly number incorrect labels increases. contrast convnet noise model shows greater robustness incorrect labels. cifar- perform experiments svhn cifar- dataset varying training data size noise level. identity ﬁrst epochs training another epochs updating weight decay results shown figure again using noise model robust label noise compared unmodiﬁed model. difference especially large high noise levels large training sets shows scalability noise model. figure test errors svhn dataset noise level differing overall training sizes. test errors trained samples noise level varies. note performance learned close model trained ﬁxed true noise distribution ground truth noise distribution learned noisy data figure noise model compared baseline model different amount training data varying noise levels. plots show test errors brighter color indicates better accuracy. imagenet experiment deliberately half training labels imagenet dataset test scalability noise model class problem. explore different noise distributions random adversarial applied imagenet training set. random case labels ﬂipped non-uniform probability using pre-deﬁned matrix around thousand non-zero values random off-diagonal locations. class labels correct distributed randomly chosen classes. adversarial case noise distribution labels changed classes likely confused true class first train normal convnet clean data measure confusion matrix. matrix gives good metric classes likely confused. matrix used constructing labels randomly ﬂipped similar labels. using alexnet architecture train three models noise case standard model noise layer; model learned matrix model ﬁxed ground truth table shows top- classiﬁcation error imagenet validation models trained random noise distribution. clear noise hurts performance signiﬁcantly. model learned shows clear gain unaltered model still behind model used ground truth learned model superior training unaltered model subset clean labels showing noisy examples carry useful information accessed model. table shows errors adversarial noise situation. here overall performance worse random images considered outliers tiny images dataset covers classes thus chance belonging cifar classes small. training data consists random inlier images true labels outlier images random labels described section outlier model requires small known outlier images. case examples randomly picked tiny images. testing original cifar test data. figure shows classiﬁcation performance model trained different amounts inlier outlier images. interestingly large amount outlier noise signiﬁcantly reduce accuracy normal model without noise modeling. nevertheless noise model used effect outlier noise reduced particularly small training sets. experiment hyper-parameter eqn. using true number outliers shown fig. model sensitive precise value. figure explores ability trained models distinguish inlier outlier heldnoisy test set. normal model entropy softmax output proxy outlier conﬁdence. outlier model conﬁdence class output. ﬁgure shows precision recall curves models trained varying training sizes outliers. average precision outlier model consistently superior normal model. imagenet added outlier noise imagenet dataset using images randomly chosen entire imagenet fall release outlier images randomly labeled categories added training set. number inlier images increase number outlier images nout mixed training data. training noise model added outlier images labeled outlier training data. trained normal alexnet model data along version using outlier model. used three different values experiments. used using true percentage outliers training data. runs perturbed value explore sensitivity noise model hyper-parameter. results shown fig. differing amounts outliers. although normal deep network quite robust large outlier noise using noise model reduces effect noise. figure cifar- outlier experiments. left effect outlier noise classiﬁcation performance cifar without noise model. here nout number inlier outlier images training data respectively. right precision recall curve detecting inliers test data. subset taken classes cifar- dataset extracted. data contains inlier images well totally unrelated outlier images. cursory examination data estimated outlier fraction using ratio along known outliers evaluation used cifar- test set. training data using unmodiﬁed convnet produced test error second model trained outlier layer gave test error relative gain images imagenet also apply approach challenging noisy real world problem based around classes used imagenet. collected noisy image dataset using thousand category names input internet image search engines. available images class downloaded taking care delete also appeared imagenet dataset. dataset averages examples/class total images. noise highly ranked images signiﬁcant later examples class. precise noise level unknown browsing images assuming images outlier. trained alexnet model without noise adaption layer image dataset evaluated performance imagenet validation set. complication distribution inliers images differs somewhat imagenet evaluation creating problem domain shift. reduce effect added imagenet training images data. ensures model learns representation class consistent test data. table shows three alexnet models applied data unaltered model model learned label-ﬂip noise matrix model outlier noise matrix results show label-ﬂip model boosting performance paper explored convolutional networks trained data noisy labels. proposed simple models improving noise robustness focusing different types noise. explored approaches variety settings small large-scale datasets well synthesized real label noise. former case approaches gave signiﬁcant performance gains standard model. real data gains smaller. however approaches implemented minimal effort existing deep learning implementations little overhead training procedure. references barandela ricardo gasca eduardo. decontamination training samples supervised pattern recognition methods. advances pattern recognition volume lecture notes computer science springer bootkrajang jakramate kabn ata. label-noise robust logistic regression applications. machine learning knowledge discovery databases volume lecture notes computer science springer deng dong socher li-jia fei-fei imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference june erhan dumitru bengio yoshua courville aaron manzagol pierre-antoine vincent pascal bengio samy. unsupervised pre-training help deep learning? mach. learn. res. march fergus weiss yair torralba antonio. semi-supervised learning gigantic image collections. bengio schuurmans lafferty j.d. williams c.k.i. culotta advances neural information processing systems ipeirotis panagiotis provost foster wang jing. quality management amazon mechanical turk. proceedings sigkdd workshop human computation hcomp yangqing shelhamer evan donahue jeff karayev sergey long jonathan girshick ross guadarrama sergio darrell trevor. caffe convolutional architecture fast feature embedding. arxiv preprint arxiv. larsen nonboe hintz-madsen hansen l.k. design robust neural network classiﬁers. acoustics speech signal processing proceedings ieee international conference volume vol. honglak grosse roger ranganath rajesh andrew convolutional deep belief networks scalable unsupervised learning hierarchical representations. proceedings annual international conference machine learning icml netzer yuval wang coates adam bissacco alessandro andrew reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning pechenizkiy tsymbal puuronen pechenizkiy class noise supervised learning medical domains effect feature extraction. computer-based medical systems cbms ieee international symposium sermanet pierre eigen david zhang xiang mathieu michael fergus lecun yann. overfeat integrated recognition localization detection using convolutional networks. international conference learning representations april taigman yaniv yang ming ranzato marc’aurelio wolf lior. deepface closing humanlevel performance face veriﬁcation. conference computer vision pattern recognition torralba fergus freeman w.t. million tiny images large data nonparametric object scene recognition. pattern analysis machine intelligence ieee transactions", "year": 2014}