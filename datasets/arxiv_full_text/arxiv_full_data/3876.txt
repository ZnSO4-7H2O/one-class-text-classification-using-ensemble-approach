{"title": "Complete Dictionary Recovery over the Sphere", "tag": ["cs.IT", "cs.CV", "cs.LG", "math.IT", "math.OC", "stat.ML", "68P30, 58C05, 94A12, 94A08, 68T05, 90C26, 90C48, 90C55"], "abstract": "We consider the problem of recovering a complete (i.e., square and invertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb R^{n \\times p}$ with $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is sufficiently sparse. This recovery problem is central to the theoretical understanding of dictionary learning, which seeks a sparse representation for a collection of input signals, and finds numerous applications in modern signal processing and machine learning. We give the first efficient algorithm that provably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per column, under suitable probability model for $\\mathbf X_0$. In contrast, prior results based on efficient algorithms provide recovery guarantees when $\\mathbf X_0$ has only $O(n^{1-\\delta})$ nonzeros per column for any constant $\\delta \\in (0, 1)$.  Our algorithmic pipeline centers around solving a certain nonconvex optimization problem with a spherical constraint, and hence is naturally phrased in the language of manifold optimization. To show this apparently hard problem is tractable, we first provide a geometric characterization of the high-dimensional objective landscape, which shows that with high probability there are no \"spurious\" local minima. This particular geometric structure allows us to design a Riemannian trust region algorithm over the sphere that provably converges to one local minimizer with an arbitrary initialization, despite the presence of saddle points. The geometric approach we develop here may also shed light on other problems arising from nonconvex recovery of structured signals.", "text": "theoretical understanding dictionary learning seeks sparse representation collection input signals ﬁnds numerous applications modern signal processing machine learning. give ﬁrst eﬃcient algorithm provably recovers nonzeros column suitable probability model contrast prior results column constant algorithmic pipeline centers around solving certain nonconvex optimization problem spherical constraint hence naturally phrased language manifold optimization. show apparently hard problem tractable ﬁrst provide geometric characterization high-dimensional objective landscape shows high probability spurious local minima. particular geometric structure allows design riemannian trust region algorithm sphere provably converges local minimizer arbitrary initialization despite presence saddle points. geometric approach develop also shed light problems arising nonconvex recovery structured signals. keywords. dictionary learning nonconvex optimization spherical constraint trust region method escaping saddle point manifold optimization function landscape second-order geometry inverse problem structured signal nonlinear approximation mathematics subject classiﬁcation. acknowledgement. thank boaz barak pointing inaccurate comment made overcomplete dictionary learning using sos. thank henry columbia university discussions related project. thanks family private foundation generous support. work partially supported grants n--- funding moore sloan foundations. introduction theoretical algorithmic challenges intriguing numerical experiment real images dictionary recovery results main ingredients innovations nonconvex formulation glimpse high-dimensional function landscape second-order algorithm manifold riemannian trust region method prior arts connections notations organization reproducible research riemannian trust-region algorithm sphere main convergence results useful technical results proof ideas orthogonal dictionaries basic facts sphere steps towards proof proof proposition proof proposition proof proposition proof pointwise concentration results proof lipschitz results proofs theorem proofs section theorem given signal samples i.e. possible construct dictionary much smaller coeﬃcient matrix nonzeros possible? words model dictionary learning problem seeks concise representation collection input signals. concise signal representations play central role compression also prove useful many important tasks signal acquisition denoising classiﬁcation. traditionally concise signal representations relied heavily explicit analytic bases constructed nonlinear approximation harmonic analysis. constructive approach proved highly successfully; numerous theoretical advances ﬁelds summary relevant results) provide ever powerful representations ranging classic fourier modern multidimensional multidirectional multiresolution bases including wavelets curvelets ridgelets however challenges confront practitioners adapting results domains function class best describes signals hand consequently representation appropriate. challenges coupled function classes known good analytic bases rare. around neuroscientists olshausen field discovered sparse coding principle encoding signal atoms learned dictionary reproduces important properties receptive ﬁelds simple cells perform early visual processing discovery spurred ﬂurry algorithmic developments successful applications past decades spanning classical image processing visual recognition compressive signal acquisition also recent deep architectures signal classiﬁcation review development). learning approach particularly relevant modern signal processing machine learning deal data huge volume great variety proliferation problems data seems preclude analytically deriving optimal representations class data timely manner. hand datasets grow learning dictionaries directly data looks increasingly attractive promising. armed suﬃciently many data samples signal class solving model problem would expect obtain dictionary allows sparse representation whole class. hope borne number successful examples theories donoho ...in eﬀect uncovering optimal codebook structure naturally occurring data involves challenging empirical questions ever solved empirical work mathematical sciences. theoretical algorithmic challenges contrast empirical successes theoretical study dictionary learning still developing. applications dictionary learning applied hands-free manner desirable eﬃcient algorithms guaranteed perform correctly input data admit sparse model. several important recent results direction review section sketching main results. nevertheless obtaining algorithms provably succeed broad realistic conditions remains important research challenge. understand diﬃculties arise consider model formulation attempt obtain dictionary coeﬃcients best trade-oﬀ sparsity ﬁdelity observed data |xij| promotes sparsity coeﬃcients trades level coeﬃcient here sparsity quality approximation imposes desired structures dictionary. formulation nonconvex admissible typically nonconvex daunting nonconvexity comes objective value conceptual formulation permutation matrix denotes matrix transpose. thus diagonal matrix diagonal entries expect problem combinatorially many global minima. multiple isolated global minima problem appear amenable convex relaxation contrasts sharply problems sparse recovery compressed sensing simple convex relaxations often provably eﬀective hope obtain global solutions problem? intriguing numerical experiment real images provide empirical evidence support positive answer question. speciﬁcally learn orthogonal bases real images patches. orthobases interest typical hand-designed dictionaries discrete cosine wavelet bases orthogonal orthobases seem competitive performance applications image denoising compared overcomplete dictionaries example nonlinear approximation harmonic analysis orthonormal basis frames preferred; scale ambiguity discussed text common practice require column-normalized. obvious reason believe convexifying constraint sets would leave optima unchanged. example symmetry breaking constraints convex objective function tends minimizers inside ball obviously orthogonal matrices. ideas lifting play together objective function yield tight relaxations semideﬁnite programming lifting useful general strategy convexify bilinear inverse problems e.g. however problems general nonlinear constraints unclear whether lifting always yield tight relaxation consider e.g. again. figure alternating direction method uncompressed real images seems always produce solution image resolution encoded uncompressed format image evenly divided non-overlapping image patches patches vectorized stacked columns data matrix bottom given solve times independent randomized initialization plots show values across independent repetitions. virtually relative diﬀerences less divide given greyscale image non-overlapping patches converted -dimensional vectors stacked column-wise data matrix specializing setting obtain optimization problem derive concrete algorithm deploy alternating direction method i.e. alternately minimizing objective function respect variable ﬁxing other. iteration sequence actually takes simple form denotes well-known soft-thresholding operator acting elementwise matrices i.e. figure shows obtained using simple algorithm independent randomized initializations observation implies heuristic algorithm always converge global minimizer equally surprising phenomenon observed real images. imagine random data typically favorable structures; fact almost existing theories pertain random data dictionary recovery results paper take step towards explaining surprising eﬀectiveness nonconvex optimization heuristics focus dictionary recovery setting given data matrix deﬁne reasonably simple structured problem make following assumptions target dictionary complete i.e. square invertible particular class includes orthogonal dictionaries. admittedly overcomplete dictionaries tend powerful modeling allow sparser representations. nevertheless classic hand-designed dictionaries common orthogonal. orthobases competitive performance certain tasks image denoising admit faster algorithms learning encoding. prove following result theorem given complete dictionary ∼i.i.d. polynomial time algorithm recovers high probability whenever ﬁxed polynomial condition number parameter cn−/ ﬁxed positive numerical constant obviously even known needs make identiﬁcation problem well posed. particular probabilistic model simple coupon collection argument implies technically converge global solutions surprising even convergence critical points atypical e.g. references therein. section includes detailed discussions point. actually phenomenon also observed simulated data coeﬃcient matrix obeys bernoulligaussian model deﬁned later. result real images supports previously claimed empirical successes decades non-incidental. empirically systematic evidence supporting overcomplete dictionaries strictly necessary good performance published applications argues necessity neuroscience perspective). ideas tools developed complete dictionaries also apply certain classes structured overcomplete dictionaries tight frames. section relevant discussion. parameter controls sparsity level intuitively recovery problem easy small becomes harder large perhaps surprising eﬃcient algorithm succeed constant i.e. linear sparsity compared case known constant sparsity level deal with. result gives ﬁrst eﬃcient algorithm provably recovers complete nonzeros column appropriate probability model. section provides detailed comparison result recent recovery results complete overcomplete dictionaries. nonconvex formulation since complete denotes space matrix) hence rows sparse vectors known subspace fact ﬁrst recover rows subsequently recover solving system linear equations. fact ∼i.i.d. rows sparsest vectors w.h.p. whenever thus might recover rows solving objective discontinuous domain open set. particular homogeneous constraint nonconventional tricky deal with. since recovery scale remove homogeneity ﬁxing scale known relaxations scale indeed small enough columns predominately -sparse directly observes scaled versions atoms fully dense corresponding recovery never possible easily another complete figure dictionary learning tractable? assume target dictionary orthogonal. left large sample objective function local minima columns negatives. center function visualized height plane right around optimum function exhibits small region positive curvature region large gradient ﬁnally region direction away direction negative curvature. inﬁnitely diﬀerentiable controls smoothing level. spherical constraint nonconvex. hence a-priori unclear whether admits eﬃcient algorithms attain global optima. surprisingly simple descent algorithms exhibit striking behavior many practical numerical examples appear produce global solutions. next section uncover interesting geometrical structures underlying phenomenon. remarkably spurious local minima. better illustrate point take particular case project upper hemisphere equatorial plane projection bijective equivalently deﬁne reparameterization figure plots graph obviously local minimizers ±e±e also global minimizers. moreover apparent nonconvex landscape interesting structures around moving away sees successively strongly convex region nonzero gradient region region point always direction negative curvature shown schematically figure geometry implies nonoptimal point always least direction descent. thus algorithm take advantage descent directions likely converge global minimizer irrespective initialization. challenges stand implementing idea. geometry show similar structure exists general complete high dimensions number observations ﬁnite algorithms need able fact nothing special choice believe valid smooth approximation would work yield qualitatively similar results. also preliminary results showing latter geometric picture remains certain nonsmooth functions modiﬁed version huber function though analysis involves handling diﬀerent technical subtleties. algorithm also needs additional modiﬁcations. geometry orthogonal landscape simply rotated version i.e. hence focus case among symmetric sections centered around signed basis vectors work symmetric section around example. result carry sections argument; together provides complete characterization function sn−. invoke projection trick described above time onto equatorial plane respectively three consecutive regions moving away origin corresponding three regions figure particular typical expectation-concentration style argument show exists positive constant respective regions w.h.p. conﬁrming low-dimensional observations described above. particular favorable structure observed persists high dimensions w.h.p. even large ﬁnite case orthogonal. moreover local minimizer close within distance geometry complete general complete dictionaries hope function retains nice geometric structure discussed above. ensure preconditioning output looks generated certain orthogonal matrix possibly plus matrix small magnitude. simple perturbation argument shows constant shrunk suﬃciently large. thus qualitative aspects geometry changed perturbation. second-order algorithm manifold riemannian trust region method know ahead time algorithm needs take advantage structure described without knowledge intuitively seems possible descent direction space appears also local descent direction sphere. another issue although optimization problem spurious local minima many saddle points second-order information guarantee escape saddle points. derive algorithm based riemannian trust region method sphere purpose. deﬁned measure progress typically radius updated dynamically according adapt local function behavior. detailed introductions classical found texts additional issue compared euclidean setting vector tangent space additive update leads point outside sphere. resort natural exponential pull tangent vector point sphere using geometric characterizations prove w.h.p. algorithm converges local minimizer parameter suﬃciently small. particular show trust region step induces least ﬁxed amount decrease objective value negative curvature nonzero gradient region; trust region iterate sequence eventually move stay strongly convex region converge local minimizer contained region asymptotic quadratic rate. short geometric structure implies initialization iterate sequence converges close approximation target solution polynomial number steps. prior arts connections ambitious include comprehensive review exciting developments algorithms applications pioneer work refer reader chapter book survey paper summaries relevant developments image analysis visual recognition. following focus reviewing recent developments theoretical side dictionary learning draw connections problems techniques relevant current work. theoretical dictionary learning. theoretical study recovery setting started recently. ﬁrst provide algorithmic procedure correctly extract generating dictionary. algorithm requires exponentially many samples exponential running time; also subsequent work studied target dictionary local optimum natural recovery criteria. meticulous analyses show polynomially many samples suﬃcient ensure local correctness natural assumptions. however results imply design eﬃcient algorithms obtain desired local optimum hence dictionary. initiated on-going research eﬀort provide eﬃcient algorithms globally solve showed recover complete dictionary solving certain sequence linear programs sparse random matrix nonzeros column. give eﬃcient algorithms provably recover overcomplete incoherent dictionaries based combination {clustering nonzeros column. recent work provides ﬁrst polynomial-time algorithm provably recovers nice overcomplete dictionaries nonzeros column constant however proposed algorithm runs super-polynomial time sparsity level goes similarly also proposes super-polynomial time algorithm guarantees recovery nonzeros column. comparison give ﬁrst polynomial-time algorithm provably recovers complete dictionary nonzeros column. finding sparse vectors linear subspace. followed cast core problem ﬁnding sparsest vectors given linear subspace also independent interest. planted sparse model shows solving sequence linear programs similar recover sparse vectors sparsity sublinear vector dimension. improved recovery limit solving nonconvex spherical constrained problem similar algorithm. idea seeking rows sequentially solving core problem sees precursors blind source separation matrix sparsiﬁcation. also proposed nonconvex optimization similar employed nonconvex optimization problems. nonconvex optimization problems recovery structured signals including low-rank matrix completion/recovery phase retreival tensor recovery mixed regression structured element pursuit recovery simultaneously structured signals numerical linear algebra optimization initialization plus local reﬁnement strategy adopted theoretical also crucial nearness target solution enables exploiting local geometry target analyze local reﬁnement. comparison provide complete characterization global geometry admits eﬃcient algorithms without special initialization. idea separating geometric analysis algorithmic design also prove valuable nonconvex problems discussed above. sparse vector embedded otherwise random subspace. diﬀerence chose work huber function proxy function. this body recent work studying nonconvex recovery statistical precision including e.g. manifold settings. refer reader monographs survey developments ﬁeld. particular developed newton conjugate-gradient methods stiefel manifolds spherical manifold special case. generalized trust-region methods riemannian manifolds. cannot however adopt existing convergence results concern either global convergence local convergence particular geometric structure forces piece together diﬀerent arguments obtain global result. figure asymptotic function landscapes rows independent. w.l.o.g. assume ∼i.i.d. columns i.i.d. gaussian vectors obeying symmetric diagonal i.i.d. oﬀ-diagonal entries distributed similarly ∼i.i.d. columns i.i.d. vectors generated ∼i.i.d. uniform. comparison ∼i.i.d. ∼i.i.d. uniform. denote elementwise product objective function still based cosh function independent component analysis matrix factorization problems. also considered general framework matrix factorization problems encompass classic principal component analysis clustering recent problems nonnegative matrix factorization multi-layer neural nets problems np-hard. identifying tractable cases practical interest providing provable eﬃcient algorithms subject on-going research endeavors; e.g. recent progresses learning deep neural nets model implies rows independent aligning problem perfectly problem. interestingly cosh objective analyze proposed generalpurpose contrast function thoroughly analyzed algorithm analysis another popular contrast function fourth-order cumulants indeed overlap considerably interesting connection potentially helps port analysis fundamental question playing vital role sparsity independence. figure helps shed light direction plot asymptotic objective landscape natural reparameterization section left central panels evident even without independence sparse columns induces familiar geometric structures figure structures broken sparsity level becomes large. believe later analyses generalized correlated cases experimented with. hand right panel seems independence function landscape undergoes transition sparsity level grows target solution goes minimizers objective maximizers objective. without adequate knowledge true sparsity unclear whether would like minimize maximize objective. suggests sparsity instead independence makes current algorithm work. nonconvex problems similar geometric structure besides discussed above turns handful practical problems arising signal processing machine learning induce spurious minimizers saddles second-order structure natural setting including eigenvalue problem generalized phase retrieval tensor decomposition linear neural nets learning gave review problems discussed methodology developed companion paper generalized solve problems. notations organization reproducible research bold capital small letters denote matrices vectors respectively. small letters reserved scalars. several speciﬁc mathematical objects frequently work with orthogonal group order unit sphere unit ball matrix transposition causing confusion work entirely real ﬁeld. superscript index rows matrix i-th matrix subscript index columns vectors norms usual norm vector operator norm matrix; norms indexed subscript example frobenius norm matrices element-wise max-norm mean random variable distributed nevertheless objective functions apparently diﬀerent. moreover provided complete geometric characterization objective contrast believe geometric characterization could provide insight algorithm also help improve algorithm terms stability also ﬁnding components. showed results model here seems structure persists even approaches suspect phase transition landscape occurs diﬀerent points diﬀerent distributions gaussian outlying case transition occurs according denote gaussian law. means standard gaussian vector. similarly ∼i.i.d. mean elements independently identically distributed according fact equivalent ∼i.i.d. particular distribution interest paper bernoulli-gaussian rate also write compactly frequently indexed numerical constants stating proving technical results. scopes constants local unless otherwise noted. standard notations cases exceptions clariﬁed locally. rest paper organized follows. section present major technical results complete characterization geometry sketched section similarly section present necessary technical machinery results convergence proof riemannian trust-region algorithm sphere corresponding section section discuss whole algorithmic pipeline recovering complete dictionaries given present main theorems. presenting simple simulation corroborate theory section wrap main content section discussing possible improvement future directions work. major proofs geometrical algorithmic results deferred section section respectively. section augments main results. appendices cover recurring technical tools auxiliary results proofs. though unique local minimizer exactly recovers last near hence resulting produces close approximation note contains points maxi∈± q∗ei. characterize graph function vicinity signed basis vector simply changing plane times obtain characterization entirety sn−. result captured next corollary. corollary suppose hence exist positive constant probability local minimizers sphere sn−. particular bijective minimizers signed basis vectors {±ei}i corresponding local minimizer {±ei}i satisfy proof theorem unique local minimizer. suppose not. exist satisfying since mapping √n-lipschitz satisfying implying local minimizer diﬀerent contradiction. straightforward calculation shows fact possible pull detailed geometry captured back sphere also; analysis riemannian trust-region algorithm later part these. stick simple global version here. repeating argument times vicinity signed basis vectors gives local minimizers indeed symmetric sections cover sphere certain overlaps simple calculation shows local minimizer lies overlapped regions extra local minimizer local minimizer contained least symmetric sections resulting diﬀerent local minimizers section contradicting uniqueness result obtained above. though isolated local minimizers diﬀerent objective values equally good sense produces close approximation certain discussed section cases orthobasis landscape simply rotated version characterized above. probability least minimizers sphere sn−. particular bijective minimizers signed basis vectors {±ei}i corresponding local minimizer {±ei}i satisfy useful technical lemmas proof ideas orthogonal dictionaries proof theorem conceptually straightforward shows claimed properties proves quantities interest concentrates uniformly expectation. detailed calculations nontrivial. next three propositions show expected function landscape successively strongly convex region nonzero gradient region directional negative curvature region moving away zero depicted figure sketched section note case prove hold qualitatively ﬁnite i.e. function need ﬁrst prove ﬁxed quantity interest concentrate expectation w.h.p. function nice enough extend results discretization argument. next three propositions provide desired pointwise concentration results. proposition suppose extending complete dictionaries hinted section instead proving things scratch build results obtained orthogonal dictionaries. particular work preconditioned data matrix crucial problem left eﬃciently obtain local minimizers. presence saddle points motivated develop riemannian trustregion algorithm sphere; existence descent directions nonoptimal points drives trust-region iteration sequence towards minimizers asymptotically. prove modeling assumptions algorithm eﬃciently produces accurate approximation minimizers. throughout exposition basic knowledge riemannian geometry assumed. keep technical requirement minimal possible; reader consult excellent monograph relevant background details. function euclidean space typical starts initialization produces sequence iterates repeatedly minimizing quadratic approximation objective function ball centered current iterate. here interested restriction unit sphere sn−. instead directly approximating function form quadratic approximations tangent space sn−. recall tangent space sphere point tqsn− i.e. vectors orthogonal consider tqsn− deﬁnes smooth curve sphere satisﬁes function obviously smooth expect taylor expansion around good approximation function least vicinity taylor’s theorem gives accurate mean achieve arbitrary numerical accuracy reasonable amount time. running time algorithm order target accuracy polynomial problem parameters. motivated hence algorithm intuitive taylor approximation function sn−. understand properties useful interpret riemannian trust-region method manifold sn−. class algorithm discussed detail monograph particular quadratic approximation obtained noting region subproblem i.e. minimizing quadratic function subject single quadratic constraint solved polynomial time either root ﬁnding methods semideﬁnite programming root ﬁnding methods numerically suﬀer so-called hard case deploy approach here. introduce choice trust region size important convergence theory practical eﬀectiveness trms. following standard recommendations backtracking approach modiﬁes iteration iteration based accuracy recover optimal solution computing extract subvector ﬁrst coordinates principal eigenvectoru approximation whole algorithmic procedure described pseudocode algof main convergence results using general results riemannian diﬃcult prove iterates produced algorithm converge critical point objective sn−. section show probabilistic assumptions claim strengthened. particular algorithm guaranteed produce accurate approximation local minimizer objective function number iterations polynomial problem size. arguments described section show high probability every local minimizer produces close approximation taken together implies algorithm eﬃciently produces close approximation convergence result shows target accuracy algorithm terminates within polynomially many steps. estimate number steps pessimistic analysis assumed ﬁxed step size running time relatively large degree polynomial adaptive step size described algorithm produces accurate solution relatively iterations. nevertheless goal stating results provide tight analysis prove riemannian algorithm ﬁnds local minimizer polynomial time. nonconvex problems entirely trivial results show general np-hard local minimum nonconvex function. useful technical results proof ideas orthogonal dictionaries reason algorithm successful derives geometry depicted figure formalized theorem basically sphere divided three regions. near local minimizer function strongly convex algorithm behaves like standard algorithm applied strongly convex function particular exhibits quadratic asymptotic rate convergence. away local minimizers function always exhibits either strong gradient direction negative curvature riemannian aglorithm capable exploiting quantities reduce objective value least constant iteration. total number iterations spent away vicinity local minimizers bounded comparing constant initial objective value. proofs follow exactly line make various quantities precise. denotes parallel translation operator translates tangent vector tangent vector parallel manner. sequel identify following matrix whose restriction tqsn− parallel translation operator denote inverse matrix restriction tγsn− inverse parallel translation operator note matrix hess rank nonzero obviously null space. hess rank null direction tangent space. thus case acts tangent space steps towards proof note orthogonal words established fact function landscape rotated version thus local minimizer rotated minimizer also algorithm generates iteration sequence upon initialization generate iteration sequence w.l.o.g. adequate prove convergence results case section write mean partition sphere three regions label riii corresponding strongly convex nonzero gradient negative curvature regions respectively consists union spherical caps radius centered around signed standard basis vector ±ei. consist diﬀerence union spherical caps radius centered around standard basis vectors finally riii covers rest sphere. trust-region step takes step current iterate similarly riii steps. since geometric structures derived theorem corollary conditions proof page section next lemma says trust-region step size small enough riemannian trust-region step reduces objective value certain amount descent direction. show decrease objective value riii enough exhibit descent direction point regions. next lemmas help almost accomplish goal. convenience choose state results canonical section vicinity projection w)/] idea similar statements hold symmetric sections. lemma suppose trust region size vector tqsn− proof page section take shown theorem take lipschitz results section log/ w.h.p. lemma repeat argument symmetric regions conclude w.h.p. objective value decreases least constant amount. next proposition summarizes results. similarly region riii proposition theorem lemma w.h.p. log/ /µ-lipschitz upper bounded −cθ. lemma lemma trust-region step decreases objective value least analysis slightly trickier. region near local minimizer objective function strongly convex. still expect trust-region step decreases objective value. hand unlikely provide universal lower bound amount decrease iteration sequence approaches local minimizer movement expected diminishing. nevertheless close minimizer trust-region algorithm takes unconstrainted steps. constrained steps show reduction objective value least ﬁxed amount; unconstrained step show distance iterate nearest local minimizer drops rapidly. combining estimate lemma lemma obtain concrete lower bound reduction objective value constrained step. proposition assume constrained trust-region step reduces objective value least proof strategy sketched lemma expect iteration sequence ultimately always takes unconstrained steps moves near local minimizer. show following true small enough iteration sequence starts take unconstrained step take consecutive unconstrained steps afterwards. takes steps show this upon unconstrained step next iterate stay obvious make ensure next iterate stays rii. strengthen result gradient information. theorem expect magnitudes gradients lower bounded; hand points near local minimizers continuity argument implies magnitudes gradients upper bounded. show small enough bounds implying next iterate stays small enough step fact unconstrained. state result canonical section canonical mapping. next lemma exhibits absolute lower bound magnitudes gradients rii. lemma satisfying proof page section assuming theorem gives w.h.p. ∇g/w thus w.h.p grad rii. next lemma compares magnitudes gradients taking unconstrained step. crucial providing upper bound magnitude gradient next iterate also establishing ultimate sequence convergence. obviously make upper bound small tuning combining lower bound grad conclude small next iterate stays another application optimality condition gives conditions guarantees next trust-region step also unconstrained. detailed argument found proof following proposition. proposition assume w.h.p trust-region algorithm takes unconstrained step always takes unconstrained steps provided positive numerical constants w.h.p. next trust-region step also unconstrained step. noting made deﬁnition make claimed simpliﬁcation completes proof. finally want show ultimate unconstrained iterates actually converges nearby local minimizer rapidly. lemma established gradient diminishing. next lemma shows magnitude gradient serves good proxy distance local minimizer. last inequality used lemma hence combining observation lemma derive asymptotic sequence convergence result follows. proposition assume conditions lemma k-th step ﬁrst unconstrained step unique local minimizer connected component contains w.h.p. positive integer proof geometric characterization theorem corollary separated local minimizers located within distance signed basis vectors {±ei}i∈. moreover obvious consists disjoint connected components. consider symmetric component vicinity claims carry others symmetry. small enough numerical constants deﬁned theorem lemma respectively constant value) veriﬁed conditions propositions satisﬁed. since local minimizers contained relative interior connected component deﬁne threshold value overline denotes closure. obviously well-deﬁned function continuous sets riii compact. also local minimizers holds four propositions above step either riii constrained step decreases objective value least certain ﬁxed amount unconstrained step future steps unconstrained sequence converges local minimizer quadratically. hence regardless initialization whole iteration sequence consists consecutive type steps followed consecutive type steps. depending initialization either type phase type phase absent. case ﬁnite number steps function value must drops future iterates stay indeed function value never drops continuity whole sequence must entirely type whereby either ﬁnite-length sequence converges local minimizer every iterate inﬁnite sequence steadily decreases objective value least ﬁxed amount either case objective value ever drop ﬁnitely many steps; hence contradiction arises. function value drops type future steps decreases objective value deﬁnition iterates stay within type future steps unconstrained steps obviously keep subsequent iterates three possibilities objective value drop future iterates stay assume unique local minimizer connected component current iterate sequence always take constrained steps hits exactly ﬁnitely many steps; sequence takes constrained steps reaching certain point deﬁned proposition since constrained step must decrease objective value least next future steps must unconstrained steps sequence converges sequence starts take unconstrained steps certain point local minimizer proposition proposition proposition number iterations obtain ε-near solution grossly bounded words function landscape rotated version thus local minimizer rotated minimizer also algorithm generates iteration sequence upon initialization generate iteration sequence w.l.o.g. adequate prove convergence u∗ξ. constant made arbitrarily small setting constant suﬃciently large. whole proof quite similar orthogonal case last section. sketch major changes below. distinguish corresponding quantities last section proposition quantities involved determining βgrad modiﬁed constant multiplicative factors changed respective tilde version conclude algorithm always takes unconstrained step taking provided orthogonal dictionaries theorem corollary know minimizers away respective nearest target certain theorem shown w.h.p. riemannian algorithm produces algorithm away exact recovery simple linear programming rounding procedure guarantees exactly produce optimizer deﬂation sequentially recover rows overall w.h.p. dictionary sparse coeﬃcient exactly recovered sign permutation orthogonal dictionaries. summarize relevant technical lemmas main results section procedure used recover complete dictionaries though analysis slightly complicated; present results section overall algorithmic pipeline recovering orthogonal dictionaries sketched follows. since orthogonal ∼i.i.d. another instance orthogonal dictionary learning problem reduced dimension. keep parameter settings theorem conditions theorem theorem cases reduced dimensions still proof sketch explains recursive plus rounding works. overall failure probability obtained simple union bound simpliﬁcations exponential tails inverse polynomials corroborate theory experiment dictionary recovery simulated data. simplicity focus recovering orthogonal dictionaries declare success single coeﬃcient matrix recovered. slightly diﬀerent bernoulli-gaussian model assumed analysis. reasonably large models produce similar behavior. sparsity surrogate deﬁned parameter implement algorithm adaptive step size instead ﬁxed step size analysis. allowable sparsity level varies dimension theory primarily about vary dictionary dimension sparsity every pair repeat simulations independently times. optimal solutions signed coordinate vectors {ei}n reconstruction error near target target likely recovered rounding described figure shows phase transition plane orthogonal case. obvious algorithm work well linear region whenever analysis tight logarithm factors also polynomial dependency theory polynomial demands recently improved almost matching lower bound sample complexity stated theorem obviously much higher. interesting whether growth complexity intrinsic working linear regime. though experiments seemed suggest necessity even orthogonal case could eﬃcient algorithms demand much less. tweaking three points likely improve complexity proxy. derivative hessians cosh function adopted entail tanh function amenable eﬀective approximation aﬀects sample complexity; geometric characterization algorithm analysis. seems working directly sphere could simplify possibly improve certain parts analysis; treating complete case directly rather using bounds treat perturbation orthogonal case. particularly general linear transforms change space signiﬁcantly preconditioning comparing orthogonal transforms eﬃcient proceed. possible extend current analysis dictionary settings. geometric structures algorithms allow plug-and-play noise analysis. nevertheless believe stable dealing noise directly extract whole dictionary i.e. consider geometry optimization orthogonal group. require additional nontrivial technical work likely feasible thanks relatively complete knowledge orthogonal group substantial leap forward would extend methodology recovery structured overcomplete dictionaries tight frames. though natural elimination variable consider marginalization objective function coeﬃcients work hidden functions. coeﬃcient model alluded section analysis results likely carried coeﬃcients statistical dependence physical constraints. connection discussed section suggests geometric characterization algorithms modiﬁed problem. likely provide theoretical insights computational schemes ica. surge theoretical understanding nonconvex heuristics initialization plus local reﬁnement strategy mostly diﬀers practice whereby random initializations seem work well analytic techniques developed mostly fragmented highly specialized. analytic algorithmic developed hold promise provide coherent account problems. interesting extent streamline generalize framework. motivating experiment real images section remains mysterious. believe real image data nice objective spurious local minima either surprising would escape critical points predicted classic modern theories. reasonable place start look gradient descent algorithms generic initializations escape local maxima saddle points recent work showed randomly perturbing iterate help gradient this recent work overcomplete used similar idea. marginalization taken near global optimum variable function well-behaved. studying global properties marginalization introduce additional challenges. algorithm escape saddle points high probability. would interesting know whether similar results obtained gradient descent algorithms random initialization. continuous counterpart seems well understood; e.g. discussions morse-bott theorem gradient convergence. section provide complete proofs technical results stated section that introduce notations common results used later throughout section. since deal random variables random vectors often convenient write vector explicitly i.i.d. bernoulli random variables i.i.d. standard normal. particular realization random vector denote support particular coordinate often refer subset rn−. lemma hard proofs section proof proposition proof involves delicate analysis particularly polynomial approximation function naturally induced tanh function. next lemma characterizes polynomial approximation obtain last line invoked association inequality lemma coordinatewise nonincreasing w.r.t. index set. substituting upper bound noting also noting fact proof pointwise concentration results avoid clutter notations subsection write mean similarly k-th column function means ﬁrst establish useful comparison lemma random i.i.d. bernoulli random vectors random i.i.d. normal random vectors. lemma suppose ﬁxed vector holds proof lipschitz results avoid clutter notations subsection write mean similarly k-th column function means need following lemmas prove lipschitz results. lemma suppose l-lipschitz normed space normed space l-lipschitz normed space composition ll-lipschitz. lemma rn−. assume l-lipschitz l-lipschitz bounded i.e. constants function l-lipschitz takes certain delicate work prove lemma basically discretization argument degree continuity hessian needed. tricky part continuity need compare hessian operators diﬀerent points hessian operators deﬁned respective tangent planes. place parallel translation comes play. next lemmas compute spectral bounds forward inverse parallel translation operators. thus enough take suﬃciently large make overall failure probability small enough lower bound holds. proof proof similar first assume dictionary used perturbation bound matrix inverse bound ﬁrst term lemma enough upper bound largest principal angle subspaces span]) spanned write short bound ﬁrst line used fact full column rank matrix orthogonal projection onto column span obtain ﬁfth lines invoked matrix inverse perturbation bound again. facts proof type bounds obtained integration parts proper truncations. type upper bound obtained integration parts lower bound obtained considering function noticing always nonnegative. type bounds mentioned reproduced systematic approach developed therein third line used result corollary i.e. fact last line relies exchange inﬁnite summation expectation justiﬁed bounded spectral radius. repeating argument backwards xp−··· lemma consider linear subspaces dimension spanned orthonormal bases respectively. suppose principal angles holds minq∈ok iii) proof proof similar theorem w.l.o.g. assume canonical bases respectively. references alekh agarwal animashree anandkumar prateek jain praneeth netrapalli rashish tandon learning sparsely used overcomplete dictionaries alternating minimization arxiv preprint arxiv. alekh agarwal animashree anandkumar praneeth netrapalli exact recovery sparsely used overcomplete dictionaries arxiv preprint arxiv. absil christopher baker kyle gallivan trust-region methods riemannian manifolds foundations computational mathematics hédy attouch jérôme bolte patrick redont antoine soubeyran proximal alternating minimization projection methods nonconvex problems approach based kurdykalojasiewicz inequality mathematics operations research michal aharon michael elad alfred bruckstein uniqueness overcomplete dictionaries practical retrieve them linear algebra applications anima anandkumar rong majid janzamin analyzing tensor power method dynamics applications learning overcomplete latent variable models arxiv preprint arxiv. animashree anandkumar rong majid janzamin guaranteed non-orthogonal tensor decomposition alternating rank- updates arxiv preprint arxiv. sanjeev arora rong ravindran kannan ankur moitra computing nonnegative matrix factorization–provably proceedings forty-fourth annual symposium theory computing sanjeev arora rong ankur moitra algorithms learning incoherent overcomplete dictionaries arxiv preprint arxiv. sparse coding arxiv preprint arxiv. sanjeev arora rong ankur moitra sushant sachdeva provable unknown gaussian noise implications gaussian mixtures autoencoders advances neural information processing systems anima anandkumar prateek jain yang u.n. niranjan tensor matrix methods robust tensor decomposition block sparse perturbations arxiv preprint arxiv. transitions convex programs random data information inference iau. pierre-antoine absil robert mahoney rodolphe sepulchre optimization algorithms matrix manifolds princeton university press ahmed benjamin recht justin romberg blind deconvolution using convex programming information theory ieee transactions chenglong jian-feng fast sparsity-based orthogonal dictionary learning image restoration computer vision ieee international conference ieee pierre baldi kurt hornik neural networks principal component analysis learning examples without local minima neural networks chenglong yuhui quan zuowei shen norm based dictionary learning proximal methods global convergence computer vision pattern recognition ieee conference ieee chenglong zuowei shen convergence analysis iterative data-driven tight frame construction scheme applied computational harmonic analysis afonso bandeira christopher kennedy amit singer approximating little grothendieck problem orthogonal unitary groups arxiv preprint arxiv. boaz barak jonathan kelner david steurer dictionary learning tensor decomposition sum-of-squares method arxiv preprint arxiv. srinadh bhojanapalli anastasios kyrillidis sujay sanghavi dropping convexity faster semi-deﬁnite optimization arxiv preprint arxiv. stéphane boucheron gábor lugosi pascal massart concentration inequalities nonasymptotic theory independence oxford university press chenglong yuhui quan convergent incoherent dictionary learning algorithm sparse coding computer vision–eccv springer briët oded regev tight hardness non-commutative grothendieck problem arxiv preprint arxiv. jérôme bolte shoham sabach marc teboulle proximal alternating linearized minimization nonconvex nonsmooth problems mathematical programming dimitri bertsekas john tsitsiklis parallel distributed computation numerical methods vol. prentice hall englewood cliﬀs stephen boyd lieven vandenberghe convex optimization cambridge university press york sivaraman balakrishnan martin wainwright statistical guarantees algorithm population sample-based analysis arxiv preprint arxiv. emmanuel candès ties computational harmonic analysis approximation theory approximation theory emmanuel candès mathematics sparsity proceedings international congress mathematicians seoul south korea yuxin chen emmanuel candes solving random quadratic systems equations nearly easy solving linear systems arxiv preprint arxiv. journal emmanuel candès xiaodong mahdi soltanolkotabi phase retrieval wirtinger theory algorithms information theory ieee transactions sunav choudhary urbashi mitra identiﬁability scaling laws bilinear inverse problems arxiv preprint arxiv. pierre comon independent component analysis concept? signal processing venkat chandrasekaran benjamin recht pablo parrilo alan willsky convex geometry linear inverse problems foundations computational mathematics emmanuel candes thomas strohmer vladislav voroninski phaselift exact stable signal recovery magnitude measurements convex programming communications pure applied mathematics yudong chen martin wainwright fast low-rank estimation projected gradient descent general statistical algorithmic guarantees arxiv preprint arxiv. ronald devore nonlinear approximation acta numerica ronald devore nonlinear approximation applications multiscale nonlinear adaptive approximation springer david donoho matan gavish andrea montanari phase transition matrix recovery gaussian measurements matches minimax matrix denoising proceedings national academy sciences laurent demanet paul hand scaling recovering sparsest element subspace information inference david donoho jared tanner observed universality phase transitions high-dimensional geometry implications modern data analysis signal processing philosophical transactions royal society mathematical physical engineering sciences harmonic analysis information theory ieee transactions alan edelman tomás arias steven smith geometry algorithms orthogonality constraints siam journal matrix analysis applications michael elad sparse redundant representations theory applications signal image processing springer alan frieze mark jerrum ravi kannan learning linear transformations ieee annual symposium foundations computer science ieee computer society gerald folland real analysis modern techniques applications john wiley sons simon foucart holger rauhut mathematical introduction compressive sensing springer charles fortin henry wolkowicz trust region subproblem semideﬁnite programming* optimization methods software rong furong huang yang yuan escaping saddle points—online stochastic gradient tensor decomposition arxiv preprint arxiv. remi gribonval rodolphe jenatton francis bach martin kleinsteuber matthias seibert sample complexity dictionary learning matrix factorizations arxiv preprint arxiv. rémi gribonval rodolphe jenatton francis bach sparse spurious dictionary learning noise outliers arxiv preprint arxiv. lee-ad gottlieb tyler neylon matrix sparsiﬁcation sparse null space problem approximation randomization combinatorial optimization. algorithms techniques springer rémi gribonval karin schnass dictionary identiﬁcation sparse matrix-factorization minimization ieee transactions information theory quan geng john wright local correctness -minimization dictionary learning submitted ieee transactions information theory preprint http//www.columbia. edu/~jw. theodore harris lower bound critical probability certain percolation process mathematical proceedings cambridge philosophical society vol. cambridge univ press moritz hardt understanding alternating minimization matrix completion foundations computer science ieee annual symposium ieee nicholas higham functions matrices society industrial applied mathematics elad hazan tomer koren linear-time algorithm trust region problems arxiv preprint arxiv. karhunen hyvärinen independent component analysis john wiley sons. christopher hillar friedrich sommer dictionary learning uniquely recover sparse data subsamples? arxiv preprint arxiv. moritz hardt mary wootters fast matrix completion without condition number proceedings conference learning theory aapo hyvarinen fast robust ﬁxed-point algorithms independent component analysis ieee trans. neural networks prateek jain sham kakade praneeth netrapalli computing matrix squareroot convex local search arxiv preprint arxiv. prateek jain praneeth netrapalli fast exact matrix completion ﬁnite samples arxiv preprint arxiv. prateek jain praneeth netrapalli sujay sanghavi low-rank matrix completion using alternating minimization proceedings forty-ﬁfth annual symposium theory computing prateek jain sewoong provable tensor factorization missing data advances neural information processing systems raghunandan keshavan andrea montanari sewoong matrix completion entries information theory ieee transactions sylvain lesage rémi gribonval frédéric bimbot laurent benaroya learning unions orthonormal bases thresholded singular value decomposition proceedings ieee international conference acoustics speech signal processing vol. ieee po-ling statistical consistency asymptotic normality high-dimensional robust m-estimators arxiv preprint arxiv. livni shai shalev-shwartz ohad shamir computational eﬃciency training neural networks advances neural information processing systems kyle dictionary learning samples matrix concentration arxiv preprint arxiv. po-ling martin wainwright high-dimensional regression noisy missing data provable guarantees non-convexity advances neural information processing systems arxiv. kiryung yihong yoram bresler near optimal compressed sensing sparse rank-one matrices sparse power factorization arxiv preprint arxiv. julien mairal francis bach jean ponce sparse modeling image vision processing foundations trends computer graphics vision nishant mehta alexander gray sparsity-based generalization bounds predictive sparse coding proceedings international conference machine learning relaxations tensor recovery arxiv preprint arxiv. katta murty santosh kabadi np-complete problems quadratic nonlinear programming mathematical programming jianwei gerlind plonka review curvelets recent applications ieee signal processing magazine andreas maurer massimiliano pontil k-dimensional coding schemes hilbert spaces information theory ieee transactions jorge moré danny sorensen computing trust region step siam journal scientiﬁc statistical computing michael mccoy joel tropp sharp recovery bounds convex demixing applications foundations computational mathematics praneeth netrapalli prateek jain sujay sanghavi phase retrieval using alternating minimization advances neural information processing systems praneeth netrapalli niranjan sujay sanghavi animashree anandkumar prateek jain non-convex robust advances neural information processing systems behnam neyshabur rina panigrahy sparse matrix factorization arxiv preprint arxiv. jorge nocedal stephen wright numerical optimization springer bruno olshausen david field emergence simple-cell receptive ﬁeld properties learning sparse code natural images nature samet oymak babak hassibi null space results recovery thresholds matrix rank minimization arxiv preprint arxiv. qing john wright finding sparse vector subspace linear sparsity using alternating directions advances neural information processing systems franz rendl henry wolkowicz semideﬁnite framework trust region subproblems applications large scale minimization mathematical programming hanie sedghi anima anandkumar provable methods training neural networks sparse connectivity arxiv preprint arxiv. karin schnass local identiﬁcation overcomplete dictionaries arxiv preprint arxiv. geometric analysis phase retreival preparation nonconvex problems scary? arxiv preprint arxiv. gilbert stewart ji-guang matrix perturbation theory academic press proceedings annual conference learning theory stephen ross boczar mahdi soltanolkotabi benjamin recht low-rank solutions linear matrix equations procrustes arxiv preprint arxiv. vladimir temlyakov nonlinear methods approximation foundations computational mathematics joel tropp user-friendly tail bounds sums random matrices foundations computational mathematics paul tseng convergence block coordinate descent method nondiﬀerentiable minimization journal optimization theory applications constantin udriste convex functions optimization methods riemannian manifolds vol. springer science business media daniel vainsencher shie mannor alfred bruckstein sample complexity dictionary learning journal machine learning research zhaoran wang quanquan yang ning high dimensional expectationmaximization algorithm statistical optimization asymptotic normality arxiv preprint arxiv. zhaoran wang huanran nonconvex statistical optimization minimax-optimal sparse polynomial time arxiv preprint arxiv. chris white rachel ward sujay sanghavi local convexity solving quadratic equations arxiv preprint arxiv. siqi local identiﬁability -minimization dictionary learning suﬃcient almost necessary condition arxiv preprint arxiv. xinyang constantine caramanis sujay sanghavi alternating minimization mixed linear regression arxiv preprint arxiv. yinyu shuzhong zhang results quadratic minimization siam journal optimization qinqing zheng john laﬀerty convergent gradient descent algorithm rank minimization semideﬁnite programming random linear measurements arxiv preprint arxiv. michael zibulevsky barak pearlmutter blind source separation sparse decomposition signal dictionary neural computation", "year": 2015}