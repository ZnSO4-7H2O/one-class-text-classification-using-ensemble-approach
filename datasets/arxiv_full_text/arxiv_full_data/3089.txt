{"title": "A Privacy-Aware Bayesian Approach for Combining Classifier and Cluster  Ensembles", "tag": ["cs.LG", "cs.CV", "stat.ML", "I.5.4"], "abstract": "This paper introduces a privacy-aware Bayesian approach that combines ensembles of classifiers and clusterers to perform semi-supervised and transductive learning. We consider scenarios where instances and their classification/clustering results are distributed across different data sites and have sharing restrictions. As a special case, the privacy aware computation of the model when instances of the target data are distributed across different data sites, is also discussed. Experimental results show that the proposed approach can provide good classification accuracies while adhering to the data/model sharing constraints.", "text": "abstract—this paper introduces privacy-aware bayesian approach combines ensembles classiﬁers clusterers perform semi-supervised transductive learning. consider scenarios instances classiﬁcation/clustering results distributed across different data sites sharing restrictions. special case privacy aware computation model instances target data distributed across different data sites also discussed. experimental results show proposed approach provide good classiﬁcation accuracies adhering data/model sharing constraints. extracting useful knowledge large distributed data repositories difﬁcult task data cannot directly centralized uniﬁed single database variety constraints. recently emphasis obtain high quality information distributed sources statistical modeling simultaneously adhering restrictions nature data models shared data ownership privacy issues. much work appeared moniker privacy-preserving data mining. three popular approaches privacy-preserving data mining techniques query restriction solve inference problem databases subjecting individual records attributes privacy preserving randomization operation subsequent recovery original data using cryptographic techniques secure two-party multi-party communications meanwhile notion privacy expanded substantially years. approaches k-anonymity l-diversity focused privacy terms indistinguishableness record others allowable queries. recent approaches differential privacy notion privacy impact statistical model. larger body distributed data mining techniques developed focused simple classiﬁcation/clustering algorithms mining association rules allowable data partitioning also limited typically vertically partitioned horizontally partitioned data techniques typically speciﬁcally address privacy issues encryption also true earlier data-parallel methods susceptible privacy breaches also need central planner dictates algorithm runs site. paper introduce privacy-aware bayesian approach combines ensembles classiﬁers clusterers effective semisupervised transductive learning. know topic addressed literature. combination multiple classiﬁers generate ensemble proven useful compared individual classiﬁers analogously several research efforts shown cluster ensembles improve quality results compared single clusterer e.g. references therein. motivations combining ensembles classiﬁers clusterers similar hold standalone either classiﬁer cluster ensembles. however additional nice properties emerge combination. instance unsupervised models provide supplementary constraints classifying data thereby improve generalization capability resulting classiﬁer. motivation mind bayesian approach combine cluster classiﬁer ensembles privacy-aware setting presented. consider collection instances clustering/classiﬁcation algorithms reside different data sites. idea combining classiﬁcation clustering models introduced algorithms described however algorithms deal privacy issues. probabilistic framework provides alternative approach combining class labels cluster labels conditions sharing individual records across data sites permitted. soft probabilistic notion privacy based quantiﬁable information-theoretic formulation discussed detail consider classiﬁer ensemble previously induced training data employed generate class labels every instance target data. also cluster ensemble applied target data provide sets cluster labels. class/cluster labels provide inputs bayesian combination classiﬁer cluster ensembles algorithm. theory inference estimation proposed model could performed maximizing log-likelihood using expectation maximization family algorithms however coupling makes exact computation summation classes intractable general therefore inference estimation performed using variational expectation maximization inference obtain tractable lower bound observed log-likelihood specify fully factorized distribution approximate true posterior hidden variables multinomial {φnm}} variational parameters corresponding instance. further components corresponding vectors made explicit. using jensen’s inequality lower bound observed log-likelihood derived −eq]] entropy variational distribution expectation inequality w.r.t non-negative divergence true posterior hidden variables. distributions fully factorized form given optimal distribution produces tightest possible lower bound thus given where wnmj cluster label instance clustering wnmj otherwise. since multinomial distribution updated values components normalized unity. similarly optimal value {γni} satisﬁes given classiﬁcation models produced class labels every instance similarly consider cluster ensemble comprised clustering algorithms generated cluster labels every instance target set. note cluster labeled given data partition align cluster numbered another partition none clusters correspond class given class cluster labels objective come reﬁned class probability distributions {θn}n target instances. assume classes denoted {ci}k observed class cluster labels denoted {{wnl} {wnm}} class label instance classiﬁer cluster label assigned instance clusterer. generative model proposed explain observations instance underlying mixed-membership different classes. denote latent mixed-membership vector assumed discrete probability distribution classes sampled dirichlet distribution parameter also classes different base clusterings assume multinomial distribution cluster labels. base clustering clusters dimension βmij generative model instance sampled class base clustering cluster label sampled multinomial distribution βmi. modeling classiﬁcation results different classiﬁers instance straightforward observed class labels assumed sampled latent mixedmembership vector essence posteriors {θn} expected accurate effort explain classiﬁcation clustering results framework. derives inspiration mixedmembership na¨ıve bayes model denote hidden variables {{znm} {θn}}. model parameters conveniently represented {βmi}}. therefore joint distribution hidden observed variables written subsets subsets located different data sites. data site access accordingly respective class cluster labels instances. similarly data site access instances class/cluster labels. data site update variational parameters {ζn} similarly data site update variational parameters {ζn} variational parameters updated e-step server gathers information sites updates model parameters. here primary requirement class cluster labels instances different data sites available server. broken follows ﬁrst second terms calculated data sites separately sent server terms added βmij updated variational parameters {φnmj} available sever thus aggregated information values {wnm} sent server. also observe number instances given data site difﬁcult becomes retrieve cluster labels individual clients. also practice server know many instances present data site makes recovery cluster labels even difﬁcult. also note approach adopted splits central computation multiple tasks based data distributed. therefore performance proposed model data single place always performance distributed data assuming information loss data transmission node another. summary server updating m-step sends individual clients. clients updating variational parameters e-step send partial summation results form shown server. server node helpful conceptual understanding parameter update sharing procedures. practice however real need server. client nodes take place server provided computations carried separate time windows proper order. column arbitrarily distributed ensembles illustrated figs. respectively. analogous distributed inference estimation frameworks derived cases without sharing cluster/class labels among different data sites. however detailed discussion avoided space constraints. estimation estimation maximize optimized inference w.r.t lower bound obtained variational free model parameters taking partial derivative lower bound w.r.t have again since multinomial distribution updated values components normalized unity. however direct analytic form update exists details) numerical method optimization needs resorted part objective function depends given inference estimation using allows performing computation without explicitly revealing class/cluster labels. visualize instances along class/cluster labels arranged matrix form data site contains subset matrix entries. depending matrix entries distributed across different sites three scenarios arise distributed ensemble column distributed ensemble iii) arbitrarily distributed ensemble. distributed ensemble framework target partitioned different subsets assumed different locations. instances subset d=xd. assumed class denoted cluster labels available i.e. already generated classiﬁcation clustering algorithms. objective reﬁne class probability distributions instances without sharing class/cluster labels across data sites. careful look e-step equations reveals update variational parameters corresponding instance given iteration independent instances given model parameters previous iteration. suggests maintain client-server based framework server updates model parameters clients update variational parameters instances e-step. instance consider situation target dataset partitioned would gotten information originally distributed across different data sites available single data site. therefore assess learning capabilities using benchmark datasets stored single location. semi-supervised approaches useful labeled data limited benchmarks created evaluating supervised methods. therefore small portions training data build classiﬁer ensembles. remaining data used target labels removed. adopt classiﬁers clustering hierarchical single-link k-means algorithms. achieved results presented table best component indicates accuracy best classiﬁer ensemble. also compare related algorithms bgcm deal privacy issues. observe that besides privacy-preserving property presents competitive accuracies respect counterparts. indeed friedman test followed nemenyi post-hoc test pairwise comparisons algorithms shows signiﬁcant statistical difference among accuracies bgcm.", "year": 2012}