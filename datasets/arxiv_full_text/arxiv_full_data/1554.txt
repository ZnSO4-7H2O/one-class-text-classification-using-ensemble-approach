{"title": "Sequential Dialogue Context Modeling for Spoken Language Understanding", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Spoken Language Understanding (SLU) is a key component of goal oriented dialogue systems that would parse user utterances into semantic frame representations. Traditionally SLU does not utilize the dialogue history beyond the previous system turn and contextual ambiguities are resolved by the downstream components. In this paper, we explore novel approaches for modeling dialogue context in a recurrent neural network (RNN) based language understanding system. We propose the Sequential Dialogue Encoder Network, that allows encoding context from the dialogue history in chronological order. We compare the performance of our proposed architecture with two context models, one that uses just the previous turn context and another that encodes dialogue context in a memory network, but loses the order of utterances in the dialogue history. Experiments with a multi-domain dialogue dataset demonstrate that the proposed architecture results in reduced semantic frame error rates.", "text": "figure example semantic parse utterance slot domain intent annotations following representation slot values. logue system components. example semantic frame shown restaurant reservation related query figure complexity task supported dialogue system increases need increased back forth interaction user agent. example restaurant reservation task might require user specify restaurant name date time number people required reservation. additionally based reservation availability user might need negotiate date time attribute agent. puts burden parsing in-dialogue contextual user utterances language understanding module. complexity increases system supports task user allowed goals spanning multiple domains within dialogue. natural language utterances often ambiguous context previous user system turns could help resolve errors arising ambiguities. paper explore approaches improve dialogue context modeling within recurrent neural network based spoken language understanding system. propose novel model architecture improve dialogue context modeling spoken language understanding spoken language understanding component goal oriented dialogue systems would parse user utterances semantic frame representations. traditionally utilize dialogue history beyond previous system turn contextual ambiguities resolved downstream components. paper explore novel approaches modeling dialogue context recurrent neural network based language understanding system. propose sequential dialogue encoder network allows encoding context dialogue history chronological order. compare performance proposed architecture context models uses previous turn context another encodes dialogue context memory network loses order utterances dialogue history. experiments multi-domain dialogue dataset demonstrate proposed architecture results reduced semantic frame error rates. goal oriented dialogue systems help users accomplishing tasks like making restaurant reservations booking ﬂights interacting natural language. capability understand user utterances break task speciﬁc semantics requirement systems. accomplished spoken language understanding module typically parses user utterances semantic frames composed domains intents slots processed downstream diamulti-domain dialogue dataset. proposed architecture extension hierarchical recurrent encoder decoders combine query level encodings representation current utterance feeding session level encoder. compare performance model tagger injected previous turn context single memory network uses attention weighted combination dialogue context furthermore describe dialogue recombination technique enhance complexity training dataset injecting synthetic domain switches create better match mixed domain dialogues test dataset. principle multi-turn extension grammars synthetically enhance single turn text combine single domain dialogue sessions multi-domain dialogues provide richer context training. task understanding user utterance typically broken tasks domain classiﬁcation intent classiﬁcation slot-ﬁlling modern approaches spoken language understanding involve training machine learning models labeled training data recently recurrent neural network based approaches shown perform exceedingly well spoken language understanding tasks based approaches also applied successfully tasks dialogue systems like dialogue state tracking policy learning system response generation parallel joint modeling tasks addition contextual signals shown result performance gains several applications. modeling domain intent slots joint model shown result reduction overall frame error rates joint modeling intent classiﬁcation language modeling showed promising improvements intent recognition especially presence noisy speech recognition similarly models incorporating context dialogue history semantic context frame tend outperform models without context shown potential greater generalization spoken language understanding related tasks. show improved performance informational dialogue agent incorporating knowledge base context dialogue system. using dialogue context shown boost performance dialogue next utterance prediction next sections describe proposed model architecture dataset dialogue recombination approach. followed experimental results analysis. figure architecture dialogue context encoder cosine similarity based memory network. u...ut} time step trying output parse user utterance given utterance sequence tokens given divide model components context encoder acts produce vector representation dialogue context denoted tagger takes dialogue context encoding current utterance input produces domain intent slot annotations output. context encoder architectures section describe architectures context encoders used experiments. compare performance different architectures encode varying levels dialogue context. previous utterance encoder baseline context encoder architecture. feed embeddings corresponding tokens previous system utterance {xt− nt−} single bidirectional layer gated recurrent unit cells dimensions embeddings shared tagger. ﬁnal state context encoder used dialogue context. memory network architecture identical approach described encode dialogue context utterances u...ut−} memory vectors denoted ...mt−} using bidirectional encoder dimensions temporal context dialogue history utterlet matrix given obtain cosine similarity memory vector context vector softmax similarity used attention distribution memory attention weighted used produce dialogue context vector conceptually depicted figure sequential dialogue encoder network enhance memory network architecture described adding session encoder temporally combines joint representation current utterance encoding memory vectors m...mt−} combine context vector memory vector concatenating passing feed forward layer produce dimensional context encodings denoted g...gt−} crowd sourced multi-turn dialogue sessions tasks buying movie tickets searching restaurant reserving tables restaurant. data collection process comprises steps generating user-agent interactions comprising dialog acts slots based interplay simulated user rule based dialogue policy. using crowd sourcing platform elicit natural language utterances align semantics generated interactions. goal spoken language understanding module dialogue system user utterance frame based semantics processed downstream components. tables describing intents slots present dataset found appendix. stochastic agenda-based user simulator interplay rule based system policy. user goal speciﬁed terms tuple slots denote user constraints. constraints might unspeciﬁed case user indifferent value slots. given turn simulator samples user dialogue acceptable actions based user goal agenda includes slots still need speciﬁed randomly chosen user proﬁle previous user figure architecture sequential dialogue encoder network. feed-forward networks share weights across memories. mensional bigru layer. ﬁnal state session encoder represents dialogue context encoding tagger architecture experiments stacked birnn tagger jointly model domain classiﬁcation intent classiﬁcation slot-ﬁlling similar approach described feed learned dimensional embeddings corresponding current utterance tokens tagger. ﬁrst layer uses cells dimensions equation token embeddings token level inputs ﬁrst layer produce token level outputs second layer uses long short term memory cells dimensions lstm based second layer since improved slot-ﬁlling performance validation architectures. apply dropout outputs layers. initial states forward backward lstms second tagger layer initialized dialogue encoding equation token level outputs ﬁrst layer input system actions. based chosen user dialogue rule based policy might make backend call inquire restaurant movie availability. based user backend response system responds back dialogue combination dialogue acts based hand designed rule based policy. generated interactions translated natural language counterparts sent crowdworkers paraphrasing natural language human-machine dialogues. simulator policy also extended handle multiple goals spanning different domains. set-up user goal simulator would include multiple tasks slot values could conditioned previous task example simulator would booking table after movie search restaurant near theater. slots supported simulator enumerated table collected dialogues restaurant reservation dialogues ﬁnding restaurants dialogues buying movie tickets. single domain datasets used training. multi-domain simulator used collect dialogues training validation test set. since natural language dialogues paraphrased versions known dialogueact slot combinations automatically labeled. labels veriﬁed expert annotator turns missing annotations manually annotated expert. described previous section train models large single domain dialogue datasets small multi-domain dialogues. models evaluated test composed multi-domain dialogues user attempts fulﬁll multiple goals spanning several domains. results distribution drift might result performance degradation. counter drift training-test data distributions device dialogue recombination scheme generate multi-domain dialogues single domain training datasets. dialogue tickets inferno. sure booking around tomorrow night. theatre mind? find italian restaurants mountain view price range looking cheap ristorante giovanni nice italian restaurant mountain view. works. thanks. idea behind recombination approach conditional independence sub-dialogues aimed performing distinct tasks exploit presence task intents intents denote switch primary task user trying perform since strong indicator switch focus dialogue. exploit independence sub-dialogue following intents previous dialogue context generate synthetic dialogues multi-domain context. recombination process described follows dialogue deﬁned sequence laannotations) bels three single domain datasets. sample dialogue generated using procedure described table drop utterances dialogue following insertion point recombined dialogue since turns become ambiguous confusing absence preceding context. sense approach partial dialogue recombination. compare domain classiﬁcation intent classiﬁcation slot-ﬁlling performances overall frame error rates encoder-decoder memory network sequential dialogue encoder network dataset described above. frame error rate system percentage utterances makes wrong prediction i.e. domain intent slot predicted incorrectly. trained models rmsprop training steps batch size started learning rate decayed factor every steps. gradient norms clipped exceed magnitude model optimization hyper-parameters chosen based grid search minimize validation frame error rates. table dialogue test predictions encoder decoder recombined data memory network recombined data sequential dialogue encoder network dialogue recombination .tokens italicized dialogue vocabulary replaced special tokens. columns right dialogue history detail attention distributions. sden+dr magnitude change session state proxy attention distribution. attention weights might non-zero attention history padding. restrict model vocabularies contain tokens occurring times training prevent over-ﬁtting training entities. digits replaced special token allow better generalization unseen numbers. dialogue history padded utterances batch processing. report results without recombined dataset table encoder decoder model trained previous turn context performs worst almost metrics irrespective presence recombined data. explained worse performance in-dialogue utterances previous turn context isn’t sufﬁcient accurately identify domain several cases intents slots utterance. memory network best performing model absence recombined data indicating model able encode additional context effectively improve performance tasks even small amount multi-domain data available. sequential dialogue encoder network performs slightly worse memory network absence recombined data. could explained model over-ﬁtting single domain context seen training failure utilize context effectively multi-domain setting. presence recombined dialogues outperforms implementations. apart increasing noise dialogue context adding recombined dialogues training increases average turn length training data bringing closer test dialogues. augmentation approach spirit extension data recombination described conversations. hypothesize presence synthetic conutterance hello hello need tickets cinemark redwood downtown movie want time date didn’t understand that. please tell movie time date movie movie queen katwe today number tickets tickets showing bought tickets showing queen katwe cinemark redwood downtown thank want brazilian restaurant fogo brazilian steakhouse espetus churrascaria mateo fogo would prefer fogo brazilian steakhouse table dialogue test predictions encoder decoder recombined data memory network recombined data sequential dialogue encoder network dialogue recombination tokens italicized dialogue vocabulary replaced special tokens. columns right dialogue history detail attention distributions. sden+dr magnitude change session state proxy attention distribution. attention weights might non-zero attention history padding. text regularization-like effect models. similar effects observed training longer syntheticallyaugmented utterances resulted improved semantic parsing performance simpler test set. also supported observation performance improvements obtained addition recombined data increase complexity model increases. table demonstrates example dialogue test along gold model annotations models. observe encoder decoder sequential dialogue encoder network able successfully identify domain intent slots memory network fails identify movie name. looking attention distributions notice attention diffused whereas sden focusing recent last utterances directly identify domain presence movie slot ﬁnal user utterance. also able identify presence movie ﬁnal user utterance previous utterance context. displays another example table sden model outperforms constrained previous utterance unable correctly identify domain user utterance. model correctly identiﬁes domain using strong focus task-intent bearing utterance unable identify presence restaurant user utterance. highlights failure combine context multiple history utterances. hand indicated attention distribution ﬁnal utterances sden able successfully combine context dialogue correctly identify domain restaurant name user utterance despite presence several outof-vocabulary tokens. examples hint sden performs better scenarios multiple history utterances encode complementary information could useful interpret user utterances. usually case natural goal oriented dialogues several tasks tasks focus conversation hand also observed sden performs signiﬁcantly worse absence recombined data. complex architecture much larger parameters sden prone over-ﬁtting data scenarios. paper collect multi-domain dataset goal oriented human-machine conversations analyze compare performance multiple neural network based model architectures encode varying amounts context. experiments suggest encoding context dialogue enabling model combine contextual information sequential order results reduction overall frame error rate. also introduce data augmentation scheme generate longer dialogues richer context empirically demonstrate results performance improvement multiple model architectures. would like thank pararth shah abhinav rastogi anna khasin georgi nikolov help user-machine conversation data collection labeling. would also like thank anonymous reviewers insightful comments. references ankur bapna gokhan t¨ur dilek hakkani-t¨ur larry heck. towards zero-shot frame semantic parsing domain scaling. proceedings interspeech. stockholm sweden. junyoung chung caglar gulcehre kyunghyun yoshua bengio. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv. bhuwan dhingra lihong xiujun jianfeng yun-nung chen faisal ahmed deng. end-to-end reinforcement learning dialogue agents information access. arxiv preprint arxiv. hahn dinarelli raymond lefevre lehnen mori moschitti riccardi. comparing stochastic approaches spoken language understanding multiple languages. ieee transactions audio speech language processing hakkani-t¨ur celikyilmaz y.-n. chen deng y.-y. wang. multidomain joint semantic frame parsing using bidirectional rnn-lstm. proceedings interspeech. francisco matthew henderson. machine learning dialog state tracking review. proceedings first international workshop machine learning spoken language processing. mesnil dauphin bengio deng hakkani-t¨ur heck using recurrent neural networks slot ﬁlling spoken language understanding. ieee transactions audio speech language processing jost schatzmann blaise thomson karl weilhammer steve young. agenda-based user simulation bootstrapping pomdp dialogue system. human language technologies conference north american chapter association computational linguistics; companion volume short papers. association computational linguistics pages iulian vlad serban alessandro sordoni yoshua bengio aaron courville joelle pineau. hierarchical neural network generative models movie dialogues. corr abs/.. http//arxiv.org/abs/.. alessandro sordoni yoshua bengio hossein vahabi christina lioma jakob grue simonsen jian-yun nie. hierarchical recurrent encoder-decoder generative context-aware query suggestion. international conference information knowledge management. york cikm pages https//doi.org/./.. pei-hao david vandyke milica gasic nikola mrksic tsung-hsien steve young. reward shaping recurrent neural networks speeding on-line policy learning spoken dialogue systems. arxiv preprint arxiv. tsung-hsien milica gasic nikola mrksic lina rojas-barahona pei-hao david vandyke steve young. multi-domain neural network language generation spoken dialogue systems. arxiv preprint arxiv. tsung-hsien milica gasic nikola mrksic peihao david vandyke steve young. semantically conditioned lstm-based natural language generation spoken dialogue systems. arxiv preprint arxiv. table supported intents list intents dialogue acts supported user simulator descriptions representative examples. acts parametrized slot instantiated attribute supported within domain. intent descriptions generic afﬁrmation expressing failure understand system utterance generic negation expressing dialogue expressing gratitude greeting request alternatives system offer expressing value acceptable given attribute explicit intent movie tickets explicit intent reserve table restaurant explicit intent restaurants implicit intent continuing context also used place inform intents supported dialogue system table sample dialogue sample dialogue generated using crowd working platform. consists instructions shown crowd workers based dialog interactions user simulator rule based policy. describes natural language dialog generated crowd workers. theatre name provide preference time number tickets time dontcare provide preference movie movie power rangers found following date march time disagree found following date march time accept purchase conﬁrmed given details theatre name date march time number tickets movie power rangers thank natural language paraphrase movie tickets date theatre would like make reservation theatre reservation for? tickets time movie would like power rangers movie found tickets available march", "year": 2017}