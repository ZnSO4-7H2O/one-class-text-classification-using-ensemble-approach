{"title": "Robust Kronecker Component Analysis", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Dictionary learning and component analysis models are fundamental in learning compact representations that are relevant to a given task (feature extraction, dimensionality reduction, denoising, etc.). The model complexity is encoded by means of specific structure, such as sparsity, low-rankness, or nonnegativity. Unfortunately, approaches like K-SVD - that learn dictionaries for sparse coding via Singular Value Decomposition (SVD) - are hard to scale to high-volume and high-dimensional visual data, and fragile in the presence of outliers. Conversely, robust component analysis methods such as the Robust Principle Component Analysis (RPCA) are able to recover low-complexity (e.g., low-rank) representations from data corrupted with noise of unknown magnitude and support, but do not provide a dictionary that respects the structure of the data (e.g., images), and also involve expensive computations. In this paper, we propose a novel Kronecker-decomposable component analysis model, coined as Robust Kronecker Component Analysis (RKCA), that combines ideas from sparse dictionary learning and robust component analysis. RKCA has several appealing properties, including robustness to gross corruption; it can be used for low-rank modeling, and leverages separability to solve significantly smaller problems. We design an efficient learning algorithm by drawing links with a restricted form of tensor factorization, and analyze its optimality and low-rankness properties. The effectiveness of the proposed approach is demonstrated on real-world applications, namely background subtraction and image denoising and completion, by performing a thorough comparison with the current state of the art.", "text": "abstract—dictionary learning component analysis models fundamental learning compact representations relevant given task model complexity encoded means speciﬁc structure sparsity low-rankness nonnegativity. unfortunately approaches like k-svd learn dictionaries sparse coding singular value decomposition hard scale high-volume high-dimensional visual data fragile presence outliers. conversely robust component analysis methods robust principle component analysis able recover low-complexity representations data corrupted noise unknown magnitude support provide dictionary respects structure data also involve expensive computations. paper propose novel kronecker-decomposable component analysis model coined robust kronecker component analysis combines ideas sparse dictionary learning robust component analysis. rkca several appealing properties including robustness gross corruption; used low-rank modeling leverages separability solve signiﬁcantly smaller problems. design efﬁcient learning algorithm drawing links restricted form tensor factorization analyze optimality low-rankness properties. effectiveness proposed approach demonstrated real-world applications namely background subtraction image denoising completion performing thorough comparison current state art. methods traced back principal component analysis since rich statistical models algorithms tailored learn compact representations data developed. building block methods sort structured matrix tensor factorization allowing learn components representations relevant given task e.g. feature extraction dimensionality reduction clustering classiﬁcation denoising etc. model complexity encoded means low-rankness over-completeness physical level constraints enforced imposing speciﬁc structure sparsity nonnegativity. importance learned components representations cannot overstated neither efﬁciency dramatically improving machine perception. prominent examples convolutional neural networks hierarchical feature extraction build ad-hoc representations enabling state performance wide range problems work study models optimization algorithms unsupervised learning compact representations data. particular proposed robust kronecker component analysis method offers bridge robust sparse dictionary learning perspective robust lowrank tensor factorization. robust sparse dictionary learning assuming data samples represented columns matrix structured matrix factorization seeks decompose meaningful components given structure solving regularization problem form approximation data respect loss possibly non-smooth regularizer encourages desired structure regularization parameter balancing terms. popular instances include principal component analysis variants e.g. sparse robust well sparse dictionary learning concretely taken factorized form i.e. obtain range different models depending choice regularization properties instance assuming low-rank approximation yields imposing sparsity sparse obtained. handle data corrupted sparse noise large magnitude rpca assumes observation matrix low-rank matrix sparse matrix collects gross errors outliers. model actually special instance ||a||∗+λ||e|| frobenius norm. here ||·||∗ denotes low-rank promoting nuclear norm ||·|| denotes norm enforces sparsity. matrix rpca extended tensors multiple ways relying varying deﬁnitions tensor rank. refer overview cp-based tucker-based tensor rpca models section speciﬁcs models compared paper. rest manuscript organized follows. section dedicated deriving rkca model relating rkca separable dictionary learning deriving rkca missing values. section discuss optimality guarantees formulating rkca equivalent factorization duplicated factors. section dedicated ladmm variant improved scalability. perspectives low-rank promoting properties found section discussion computational cost implementation details methods section finally present section experimental evidence effectiveness rkca synthetic real-world data. model derivation throughout paper matrices denoted uppercase boldface letters e.g. denotes identity matrix compatible dimensions. column denoted tensors considered multidimensional equivalent matrices vectors denoted bold calligraphic letters e.g. order tensor number indices needed address elements. consequently element th-order tensor addressed indices i.e. ii...im sets real integer numbers denoted respectively. th-order real-valued tensor deﬁned tensor space ri×i×···×im mode-n product tensor ri×i×...×im matrix rj×in denoted deﬁned element-wise finally deﬁne tensor tucker rank vector ranks mode-n unfoldings tensor multi-rank vector ranks frontal slices. details tensors deﬁnitions tensor slices mode-n unfoldings found example. ||·||f frobenius norm ||·|| pseudonorm counting number non-zero elements. k-svd variants problem solved iterative manner alternates sparse coding data samples current dictionary process updating dictionary atoms better data using singular value decomposition procedure suffers high computational burden preventing applicability method high-dimensional large scale data. overcome issue scalability dictionary learning separable structure dictionary enforced. instance separable dictionary learning considers samples matrix form namely admitting sparse representations bases kronecker product constructs dictionary. separable dictionary learning casted regularizers promote sparsity representations mutual-coherence dictionary respectively. here constrained orthogonal columns i.e. pair shall product manifold product sphere manifolds. different approach taken separable dictionary learnt two-step strategy similar k-svd. matrix observation represented aribt. ﬁrst step sparse representations found orthogonal matching pursuit second step decomposition performed tensor residuals regularized alternating least squares solve minabr b||f outline contributions here propose novel methods separable dictionary learning based robust tensor factorizations learn simultaneously dictionary sparse representations. seek overcompleteness rather promote lowrankness pair dictionaries sparsity codes learn low-rank representation input tensor. regard methods combine ideas sparse dictionary learning robust well tensor factorizations. solvers based alternating direction multipliers method problem jointly convex convex component individually. resort alternatingdirection method propose non-convex admm procedure operates frontal slices. minimizing ||b⊗ a||f presents challenge product high-dimensional bases coupled loss non-smooth. ||.||p denotes schatten-p norm. using identity b||p ||a||p||b||p remarking ||b||f||a||f ||a|| minimize simpler upper bound. resulting sub-problems smaller therefore easy solve computationally. order obtain exact proximal steps encodings introduce split variable thus solve recognize equation stein equation solved cubical time quadratic space solvers discrete-time sylvester equations hessenberg-schur method instead naive time space solution vectorizing equation size linear system. obtain algorithm respectively low-rank sparse components deﬁne regularization functions possibly non-smooth non-convex meant promote structural properties case low-rankness sparsity robust kronecker component analysis obtained assuming factorizes restricted form tucker factorization deﬁning combination penalties factors. speciﬁcally assume construct promote low-rankness ﬁrst modes therefore frontal slices. work discuss three different choices depending positive-homogeneity degree full regularization. deﬁnition function positively homogeneous degree notably holds. efﬁcient algorithm degree regularizer section derive efﬁcient algorithm degree -regularized problem. discussion serve basis regularizers require minor modiﬁcations algorithm. tensor completion rkca present extension original problem assume given incomplete possibly grossly-corrupted observations. given ni×i×...in deﬁne sampling operator ri×i×...in ri×i×...in projection space n-way tensors whose non-zero entries indexed n-tuples i.e. ri×i×...in ||e|| ||πω|| ¯ω|| without loss generality follow assume ¯ω+π implies seek recover possible corruption missing values directly missing element. problem therefore equivalent rkca global optimality work suggests global optimality achieved initialization tensor factorization models given factorization regularization mappings match certain ways. summarize main results work show model regularizer respects conditions global optimality. robust separable dictionary learning section show rkca learns structured robust dictionaries tensor factorization. seek overcompleteness rather promote sparsity dictionary learn low-rank representation input tensor. regard methods combine ideas sparse dictionary learning tensor robust choice kronecker product explained. suppose observations obtained vectorizing two-dimensional data images i.e. rm×n. preferable keep observations matrix form preserves spatial structure images without loss generality factorization function main challenge adapting model framework factorization function expressed elemental mappings. problem aslend formulation even though factors form size-r factors core tensor size last dimension. note however frontal slices size-r factors formulation doesn’t satisfy proposition immediately obvious deﬁne concatenation factors verify convexity factorization-regularization function proposition omitted brevity). additionally results proved case single last dimension factors factorization special case tucker decomposition naturally described next seek transformation involve cross-terms. idea unfold tucker factorization duplicating elements factors express form size-s factors since involves elements clear unfolding must too. since respectively columns deﬁnition size-r factors deﬁned tensors ﬁnal dimension tensor equal interpreted .×r. deﬁnition elemental mapping mapping positively homogeneous degree r-element factorization mapping deﬁned order apply results case describe reduction factorization allows framework. arguments develop easily extended general case tucker factorization. established factorization function reformulated within framework check degree regularizers introduced section compatible. recall regularizers form case nuclear norm simply observe numbers linearly independent columns clearly transformation preserves ranks target matrices regularization function simply clear newly deﬁned positively homogeneous degree form nondegenerate pair. property also holds. hence argue rkca product norms regularization enjoys optimality guarantees presented substitution method used section effective practice comes additional cost limits scalability. notably cost solving stein equation frontal slice cannot neglected practice. additionally added parameters make tuning difﬁcult. linearization provides alternative approach scale better large dimensions large datasets section show applied models. state linearized updates development directly applied solving problem variants simple change shrinkage factor. detailed derivations found appendix augmented factors dimension last mode. duplicate column times concatenating copies column dimension. column copied times before stacking next column resulting matrix concatenated times. deﬁned low-rank solutions seeing models perspective robust seeks low-rank representation dataset minimize rank low-rank tensor precisely show theorem simultaneously penalize tucker rank multi-rank theorem rkca encourages mode- mode- rank thus low-rankness frontal slice suitable choices parameters proof. minimize either λ||e|| α||r|| b||f λ||e|| α||r||||a b||f λ||e|| α||r||||a b||∗. equivalence norms ﬁnite-dimensions +||a b||∗ k||a b||f. case frober∗ nius norm choose reduce problem nuclear norm. cases penalize rank rankrank. given rank non-negative integer rank rank decreases necessarily. therefore minimize mode- mode- ranks additionally rank rank rank). ters. show rm×n know rn||x|| another perspective rank minimization present alternative interpretation low-rank properties methods. authors show frobenius norm nuclear norm regularizations either lead optimal solution even presence noise dictionary provides enough representative power equivalent sense describe different bases column space dictionary. explain behavior algorithms showing subproblems rank-minimization problems. mode- matricization line concatenation columns frontal slices matrix products lines captures interactions across rows input images. conversely permute) ||ddt mode- matricization line concatenation lines frontal slices matrix products lines captures interactions across columns input images. rkca learns low-rank dictionaries section explain low-rank promoting behavior rkca complementary perspective. first show regularizers deﬁned equations directly provide upper bounds mode- mode- ranks low-rank component thus rank frontal slices. perspective ﬁrst presented second approach explaining models’ properties studies optimization sub-problems associated bases based recent work show sub-problems equivalent rank-minimization problems admit closed-form solutions involve forms singular value thresholding. optimal solutions interpretation according optimal solution min||c||∗ assuming feasible solutions exist vrvt skinny showed also optimal solution min||c||f therefore easy show optimal solution min||ct||∗ similar conditions urut proof. letting uσvt vσut. sake brevity shall describe interpretation holds symmetry. noted matrix orthogonal projection onto basis. thus optimal projection onto principal components. remembering product code matricized line concatenation columns partial reconstructions rkbt columns lines rkbt seen time space complexity iteration algorithm minr since terms asymptotically negligible practice useful know computational requirements scale size dictionary. similarly initialization procedure cost time needs quadratic space slice assuming standard algorithm used switching ladmm update eliminates need solving costly stein equations. soft-shrinkage operator applied tensor elements computed remembering space complexity update updating frobenius nuclear norm penalty requires computating proximal operator also solving linear system admm substitution. focus computation proximal operator section substitution adds additional time space complexity several steps algorithms summations independent terms trivially distributed mapreduce way. proximal operators separable nature therefore parallelizable. consequently highly parallel distributed implementations possible computational complexity reduced adaptively adopting sparse linear algebra structures algorithms. frobenius nuclear norm penalties equation used upper bound ||a||f||b||f obtain smooth sub-problems. degree -regularized case must keep positive homogeneous degree therefore cannot apply bound. given nonsmoothness frobenius norm must resort approaches substitution linearization thus choose penalize ranks directly ||a||∗ ||b||∗. section showed frobenius nuclear norm penalties yield optimal solutions case would equivalent solving sub-problems associated therefore natural wonder choose other practical implications choice are. shown proximal operator schatten-p norm closed form expression requires computation matrix computing costly typical algorithm requiring ﬂoating-point operations. storing resulting triple requires space. although faster algorithms developed scalability remains concern. however frobenius norm also element-wise norm matrix whose proximal operator projection unit ball doesn’t require costly matrix decomposition. choice frobenius norm therefore justiﬁed high dimensional setting. parameter tuning adaptive admm concern practical implementation admm ladmm algorithms parameter tuning initialization exacerbated introduction additional variables constraints. constraint form translates additional terms augmented lagrangian problem tensor lagrange multipliers dimension penalty parameter. standard admm initialized updated sequence bounded non-decreasing generally simple update ρµt. initial value value directly inﬂuence speed convergence quality reconstructed images. constraints added penalty parameters introduced must tuned updated properly possibly independently another. although literature admm adaptive penalty updates suggests adaptive updates variables solutions also come added complexity possibly parameters tune. case ladmm practitioner must also ensure adequate choice value made upper bound lipschitz constants gradients update. sensitivity admm ladmm algorithms parameter tuning found algorithm provided beneﬁts practice best reconstructions simplest tune. explains choice algorithm ladmm variant experiments. convergence initialization problems non-convex therefore global convergence priori guaranteed. recent work studies convergence admm non-convex possibly non-smooth objective functions linear constraints. here constraints linear. proposed problem based global convergence could theoretically attained local descent algorithm. however problem doesn’t offer guarantees cases admm scheme employed necessarily converge local minimum. section provide experimental results algorithm discuss initialization strategy implemented variants. bases core performing observation uisivt initialize dualvariables constraint aribt take scaling coefﬁcient chosen practice chose ||ri||f similarly applicable. correspond averaging initial values individual slice corresponding constraint. convergence criterion corresponds primal-feasibility problem given errr errrec maxi maxi ladmm versions max. empirically obtained systematic convergence algorithm good solution linear convergence rate shown figure similar results found admm substitution algorithm problem ladmm variants problems. experimental evaluation ﬁrst provide experimental veriﬁcation correctness algorithm synthetic data. compared performance rkca degree regularization range state-of-the-art tensor decomposition algorithms four low-rank modeling computer vision benchmarks image denoising background subtraction. baseline report performance matrix robust implemented inexact non-negative robust dictionary learning chose following methods include recent representatives various existing approaches low-rank modeling tensors singleton version higher-order robust optimizes tucker rank tensor nuclear norms unfoldings. authors consider similar model robust m-estimators loss functions either cauchy loss welsh loss support hard soft thresholding; tested softthresholding models non-convex tensor robust adapts tensors matrix non-convex rpca finally tensor rpca algorithms work slightly different deﬁnitions tensor nuclear norm convex surrogate tensor multi-rank. tuning tractable. criteria heuristics choosing parameters provided authors chose search space around value obtained them. cases tuning process explored wide range parameters maximize performance. performance method signiﬁcantly worse other result reported clutter text case separable dictionary learning whose drastically different nature renders unsuitable robust low-rank modeling compared completeness. reason compare method k-svd finally provide tensor completion experiments validation synthetic data generated synthetic data following rkca’s assumptions ﬁrst sampling random bases known ranks gaussian slices core forming ground truth modeled additive random sparse laplacian noise tensor whose entries probability equal probability otherwise. generated data leading noise density respectively measured reconstruction error density varying values model achieved near-exact recovery exact recovery density suitable values evidence presented figure noise case. algorithm appears robust small changes suggests value lead optimal results simple criterion provides consistently good reconstruction derived robust noise case observe increase density increases error order background subtraction background subtraction common task computer vision tackled robust low-rank modeling static mostly static background video sequence effectively represented low-rank tensor foreground forms sparse component outliers. video sequence cars travelling highway; background completely static. kept gray-scale images re-sized pixels. second airport hall dataset chosen challenging benchmark since background fully static scene richer. used excerpt frames kept frames original size pixels. results provide original ground truth recovered frames figure hall experiment table presents scores algorithms ranked order mean performance benchmarks. matrix methods rank high benchmarks half tensor algorithms match outperform baseline. proposed model matches best performance highway dataset provides signiﬁcantly higher performance challenging hall benchmark. visual inspection results show rkca method doesn’t fully capture immobile people background therefore achieves best trade-off foreground detection background-foreground contamination. image denoising many natural artiﬁcial images exhibit inherent lowrank structure suitably denoised low-rank modeling algorithms. section assess performance cohort datasets chosen popularity typical cases represent. clear differences appeared noise level demonstrated quantitative metrics visual inspection figure overall performance markedly lower level methods started lose much details. visual inspection results conﬁrms higher reconstruction quality rkca. invite reader look texture skin white reﬂection light subject’s skin pupil. latter particular close nature white pixel corruption salt pepper noise. methods rkca provided best reconstruction quality algorithm removed noise aforementioned details distinguishable reconstruction. noise level method scored markedly higher competitors image quality metrics seen figure table visualizing reconstructions conﬁrms difference image recovered rkca noise level comparable output competing algorithms noise level. color image denoising benchmark facade image rich details lighting makes interesting assess reconstruction. geometric nature building’s front wall strong correlation bands indicate data modeled low-rank -way tensor frontal slice color channel. pepper) noise introduced separately frontal slices observation tensor three different levels simulate medium high gross corruption. experiments value noise levels level. report quantitative metrics designed measure aspects image recovery. peak signal noise ratio used indicator elementwise reconstruction quality signals feature similarity index evaluates recovery structural information. quantitative metrics perfect replacements subjective visual assessment image quality; therefore present sample reconstructed images veriﬁcation. measure choice determining images compare visually fsim higher correlation human evaluation psnr. monochromatic face images face denoising experiment uses extented yale-b dataset different subject different lighting conditions. according face images subject various illuminations approximately -dimensional subspace therefore suitable low-rank modeling. used pre-cropped images ﬁrst subject kept full resolution. resulting collection images constitutes -way tensor images size mode corresponds respectively columns rows images illumination component. three expected low-rank spatial correlation within frontal slices correlation images subject different illuminations. present comparative quantitative performance methods tested figure provide visualizations reconstructed ﬁrst image noise level figure report metrics averaged images. noise level nearly every method provided good excellent recovery original images. therefore omit noise level hand methods notable exception rkca trpca trpca failed provide acceptable reconstruction gross corruption case. thus present tensor completion showcase tensor completion capabilities algorithm implemented ladmm method solve problem α||r|| provide comparison robust tensor completion model matrix robust missing values yale-b noise missing values ﬁrst experiment extends section investigate case apart corruption values missing. generated data ﬁrst introducing salt pepper noise removing pixels random. seen table rkca markedly outperformed models terms fsim translates natural reconstruction. horpca-s achieved similar psnr many details lost rpca removed much image’s details left corruption. values metrics shown figure previous benchmark results appendix noise level cauchy exhibited highest psnr trpca scored best fsimc metric. rkca second highest psnr among highest fsimc scores. details provided figure clear differences visible best seen details picture black iron ornaments light coming window. method best preserved dynamics lighting sharpness details provided reconstruction visually closest original. competing models tend oversmooth image make light dimmer; indicating substantial losses highfrequency dynamic information. rkca appears also provide best color ﬁdelity. present table occluded frames video dataset completed frames obtained rkca horpca-s rpca. since corruption present dataset parameters ﬁxed high value algorithms behaved tensor matrix completion models. rkca bounded rank reconstruction performed grid-search values parameter. clear table rkca provides completion. horpca-s able complete sparse missing values center image misses selfocclusions right. matrix rpca failed complete frames benchmark. conclusion work presented rkca framework robust low-rank representation learning separable dictionaries expresses problem tensor factorization. proposed three different regularizers derived steps admm ladmm solvers. showed factorization assumed expressed equivalent gives optimality guarantees coupled regularizer positively-homogeneous degree discussed low-rank properties models practical implementation. reached conclusion model degree regularizer achieved good trade-off experimental effectiveness parameter tuning difﬁculty. future work seek develop supervised variant rkca could applied tasks. another extension leave future work consider nonconvex regularizers well-known letting element-wise scatten-p norms recover respectively pseudo-norm rank functions. refer example matrix rpca case. acknowledgments mehdi bahri partially funded department computing imperial college london. work panagakis partially supported european community horizon grant agreement zafeiriou partially funded epsrc project ep/n/ papamakarios bahri panagakis zafeiriou robust low-rank tensor modelling using tucker decomposition european signal processing conference special session component analysis computer vision harshman foundations parafac procedure models conditions explanatory multimodal factor analysis ucla working papers phonetics vol. carroll j.-j. chang analysis individual differences multidimensional scaling n-way generalization eckartyoung decomposition psychometrika vol. bahri panagakis zafeiriou robust kroneckerdecomposable component analysis low-rank modeling ieee international conference computer vision haeffele young vidal structured low-rank matrix factorization optimality algorithm applications image processing proceedings international conference machine learning jebara xing eds. jmlr workshop conference proceedings haeffele vidal global optimality tensor factorization deep learning beyond corr vol. abs/. zhang aeron kilmer novel methods multilinear data completion de-noising based tensor-svd proceedings ieee computer society conference computer vision pattern recognition feng chen tensor robust principal component analysis exact recovery corrupted lowrank tensors convex optimization ieee conference computer vision pattern recognition shang cheng cheng robust principal component analysis missing data proceedings international conference conference information knowledge management cikm hong razaviyayn convergence analysis alternating direction method multipliers family nonconvex problems ieee international conference acoustics speech signal processing april wang zeng global convergence admm nonconvex nonsmooth optimization arxiv e-prints nov. chen augmented lagrange multiplier method exact recovery corrupted low-rank matrices arxiv. yang feng suykens robust low-rank tensor recovery regularized redescending m-estimator ieee transactions neural networks learning systems vol. anandkumar jain niranjan tensor matrix methods robust tensor decomposition block sparse perturbations proceedings international conference artiﬁcial intelligence statistics aistats gretton christian eds. cadiz spain jmlr.org goyette jodoin porikli konrad ishwar changedetection.net change detection benchmark dataset ieee computer society conference computer vision pattern recognition workshops zhao zhou zhang cichocki s.-i. amari bayesian robust tensor factorization incomplete multiway data ieee transactions neural networks learning systems vol. georghiades belhumeur kriegman from many illumination cone models face recognition variable lighting pose ieee transactions pattern analysis machine intelligence vol. ramamoorthi hanrahan efﬁcient representation irradiance environment maps proceedings annual conference computer graphics interactive techniques siggraph vol. york york press sagonas tzimiropoulos zafeiriou pantic faces in-the-wild challenge ﬁrst facial landmark localization challenge proceedings ieee international conference computer vision mehdi bahri student stefanos zafeiriou department computing imperial college london. received diplˆome d’ing´enieur applied mathematics hongrenoble institute technology ensimag advanced computing distinction imperial college london published master’s research iccv spent year industry. research interests include representation learning geometric deep learning bayesian learning. yannis panagakis research fellow department computing imperial college lonlecturer middlesex university london. received degrees department informatics aristotle university thessaloniki b.sc. degree informatics telecommunication national kapodistrian university athens greece. yannis received various scholarships awards studies research including prestigious marie-curie fellowship current research interests include machine learning signal processing mathematical optimization applications computer vision human behaviour analysis music information research. stefanos zafeiriou currently reader pattern recognition/statistical machine learning computer vision department computing imperial college london london distinguishing research fellow university oulu finish distinguishing professor programme. recipient prestigious junior research fellowships imperial college london start independent research group. recipient presidents medal excellence research supervision received various awards doctoral post-doctoral studies. currently serves associate editor ieee transactions cybernetics image vision computing journal. guest editor journal special issues co-organised nine workshops/special sessions face analysis topics venues cvpr/fg/iccv/eccv citations work h-index general chair bmvc journal latex class files vol. august appendix proximal operators selective shrinkage operator prove result general d-dimensional tensor proof. proximal operator ||πω|| satisﬁes appendix updating bases substitution case degree three regularizers substitution method tackle non-smoothness frobenius norm nuclear norm regardless method used core tensor present derivation corresponding updates. case recommend direct calculation constant instead using backtracking procedure matrices often small size computation performed partial cases. mean performance yale dataset yale-b experiment measurements present average values images ﬁrst subject. therefore uncertainty measures wish address here. additional denoising results yale present supplementary denoising results. addition ﬁrst illumination show reconstruction obtained third illumination include output algorithms. comparison sedil present experiment assess performance sedil yale-b benchmark. chose experiment presented original paper sedil designed denoising grayscale images. tuned sedil training dictionary parameter used fista denoising shall denote original paper authors choose dimension square image patches σnoise tuned parameters grid-search choosing kept patch sizes extracted random patches training images ﬁrst subject. followed procedure described paper denoising extracted patches sliding window step size denoised patches fista reconstructed denoised image averaging denoised patches.", "year": 2018}