{"title": "Quasi-Recurrent Neural Networks", "tag": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.", "text": "james bradbury∗ stephen merity∗ caiming xiong richard socher salesforce research palo alto california {james.bradburysmeritycxiongrsocher}salesforce.com recurrent neural networks powerful tool modeling sequential data dependence timestep’s computation previous timestep’s output limits parallelism makes rnns unwieldy long sequences. introduce quasi-recurrent neural networks approach neural sequence modeling alternates convolutional layers apply parallel across timesteps minimalist recurrent pooling function applies parallel across channels. despite lacking trainable recurrent layers stacked qrnns better predictive accuracy stacked lstms hidden size. increased parallelism times faster train test time. experiments language modeling sentiment classiﬁcation character-level neural machine translation demonstrate advantages underline viability qrnns basic building block variety sequence tasks. recurrent neural networks including gated variants long short-term memory become standard model architecture deep learning approaches sequence modeling tasks. rnns repeatedly apply function trainable parameters hidden state. recurrent layers also stacked increasing network depth representational power often accuracy. applications natural language domain range sentence classiﬁcation wordcharacter-level language modeling rnns also commonly basic building block complex models tasks machine translation question answering unfortunately standard rnns including lstms limited capability handle tasks involving long sequences document classiﬁcation character-level machine translation computation features states different parts document cannot occur parallel. convolutional neural networks though popular tasks involving image data also applied sequence encoding tasks models apply time-invariant ﬁlter functions parallel windows along input sequence. cnns possess several advantages recurrent models including increased parallelism better scaling long sequences often seen character-level language data. convolutional models sequence processing successful combined layers hybrid architecture traditional maxaverage-pooling approaches combining convolutional features across timesteps assume time invariance hence cannot make full large-scale sequence order information. present quasi-recurrent neural networks neural sequence modeling. qrnns address drawbacks standard models like cnns qrnns allow parallel computation across timestep minibatch dimensions enabling high throughput good scaling long sequences. like rnns qrnns allow output depend overall order elements sequence. describe qrnn variants tailored several natural language tasks including document-level sentiment classiﬁcation language modeling character-level machine translation. models outperform strong lstm baselines three tasks dramatically reducing computation time. figure block diagrams showing computation structure qrnn compared typical lstm architectures. signiﬁes convolutions matrix multiplications; continuous block means computations proceed parallel. blue signiﬁes parameterless functions operate parallel along channel/feature dimension. lstms factored linear blocks elementwise blocks computation timestep still depends results previous timestep. layer quasi-recurrent neural network consists kinds subcomponents analogous convolution pooling layers cnns. convolutional component like convolutional layers cnns allows fully parallel computation across minibatches spatial dimensions case sequence dimension. pooling component like pooling layers cnns lacks trainable parameters allows fully parallel computation across minibatch feature dimensions. given input sequence rt×n n-dimensional vectors convolutional subcomponent qrnn performs convolutions timestep dimension bank ﬁlters producing sequence rt×m m-dimensional candidate vectors order useful tasks include prediction next token ﬁlters must allow computation given timestep access information future timesteps. ﬁlters width depends xt−k+ concept known masked convolution implemented padding input left convolution’s ﬁlter size minus one. apply additional convolutions separate ﬁlter banks obtain sequences vectors elementwise gates needed pooling function. candidate vectors passed tanh nonlinearity gates elementwise sigmoid. pooling function requires forget gate output gate timestep full computations convolutional component then convolution ﬁlters larger width effectively compute higher n-gram features timestep; thus larger widths especially important character-level tasks. suitable functions pooling subcomponent constructed familiar elementwise gates traditional lstm cell. seek function controlled gates states across timesteps acts independently channel state vector. simplest option balduzzi ghifary term dynamic average pooling uses forget gate term three options -pooling fo-pooling ifo-pooling respectively; case initialize zero. although recurrent parts functions must calculated timestep sequence simplicity parallelism along feature dimensions means that practice evaluating even long sequences requires negligible amount computation time. single qrnn layer thus performs input-dependent pooling followed gated linear combination convolutional features. convolutional neural networks qrnn layers stacked create model capacity approximate complex functions. motivated several common natural language tasks long history work related architectures introduce several extensions stacked qrnn described above. notably many extensions recurrent convolutional models applied directly qrnn combines elements model types. regularization important extension stacked qrnn robust regularization scheme inspired recent work regularizing lstms. need effective regularization method lstms dropout’s relative lack efﬁcacy applied recurrent connections development recurrent dropout schemes including variational inference–based dropout zoneout schemes extend dropout recurrent setting taking advantage repeating structure recurrent networks providing powerful less destructive regularization. variational inference–based dropout locks dropout mask used recurrent connections across timesteps single pass uses single stochastic subset recurrent weights. zoneout stochastically chooses subset channels zone timestep; channels network copies states timestep next without modiﬁcation. qrnns lack recurrent weights variational inference approach apply. thus extended zoneout qrnn architecture modifying pooling function keep previous pooling state stochastic subset channels. conveniently equivalent stochastically setting subset qrnn’s gate channels applying dropout thus pooling function need modiﬁed all. note using off-theshelf dropout layer context important remove automatic rescaling functionality implementation present. many experiments also apply ordinary dropout layers including word embeddings ﬁrst qrnn layer. densely-connected layers also extend qrnn architecture using techniques introduced convolutional networks. sequence classiﬁcation tasks found helpful skip-connections every qrnn layer technique termed dense convolution huang traditional feed-forward convolutional networks connections between subsequent layers densenet layers feed-forward convolutional connections every pair layers total improve gradient convergence properties especially deeper networks although requires parameter count quadratic number layers. qrnn layer’s input output along channel dimension feeding state next layer. output last layer alone used overall encoding result. encoder–decoder models demonstrate generality qrnns extend model architecture sequence-to-sequence tasks machine translation using qrnn encoder modiﬁed qrnn enhanced attention decoder. motivation modifying decoder simply feeding last encoder hidden state decoder’s recurrent pooling layer analogously conventional recurrent encoder–decoder architectures would allow encoder state affect gate update values provided decoder’s pooling layer. would substantially limit representational power decoder. instead output decoder qrnn layer’s convolution functions supplemented every timestep ﬁnal encoder hidden state. accomplished adding result convoz rt×m) broadcasting linearly projected copy layer lution layer last encoder state tilde denotes encoder variable. encoder–decoder models operate long sequences made signiﬁcantly powerful addition soft attention removes need entire input representation ﬁxed-length encoding vector. experiments computed attentional encoder’s last layer’s hidden states. used products encoder hidden states decoder’s last layer’s un-gated hidden states applying softmax along encoder timesteps weight encoder states attentional decoder timestep. context decoder state linear layer followed output gate ﬁrst step attention procedure quadratic sequence length practice takes signiﬁcantly less computation time model’s linear convolutional layers simple highly parallel dot-product scoring function. model nbsvm-bi layer sequential ensemble rnns nb-svm -layer lstm residual -layer bi-lstm models densely-connected -layer lstm densely-connected -layer qrnn densely-connected -layer qrnn table accuracy comparison imdb binary sentiment classiﬁcation task. models units layer; layers ﬁrst layer whose ﬁlter width vary ﬁlter width train times reported single nvidia gpu. exclude semi-supervised models conduct additional training unlabeled portion dataset. evaluate performance qrnn three different natural language tasks document-level sentiment classiﬁcation language modeling character-based neural machine translation. qrnn models outperform lstm-based models equal hidden size three tasks dramatically improving computation speed. experiments implemented chainer evaluate qrnn architecture popular document-level sentiment classiﬁcation benchmark imdb movie review dataset dataset consists balanced sample positive negative reviews divided equal-size train test sets average document length words compare results make additional unlabeled data best performance held-out development achieved using four-layer denselyconnected qrnn units layer word vectors initialized using -dimensional cased glove embeddings dropout applied layers used regularization optimization performed minibatches examples using rmsprop learning rate small batch sizes long sequence lengths provide ideal situation demonstrating qrnn’s performance advantages traditional recurrent architectures. observed speedup imdb train time epoch compared optimized lstm implementation provided nvidia’s cudnn library. speciﬁc batch sizes sequence lengths speed gain possible. figure provides extensive speed comparisons. figure visualize hidden state vectors ﬁnal qrnn layer part example imdb dataset. even without post-processing changes hidden state visible interpretable regards input. consequence elementwise nature recurrent pooling function delays direct interaction different channels hidden state computation next qrnn layer. replicate language modeling experiment zaremba ghahramani benchmark qrnn architecture natural language sequence prediction. experiment uses standard preprocessed version penn treebank mikolov implemented gated qrnn model medium hidden size layers units layer. qrnn layers convolutional ﬁlter width timesteps. medium models used work consist units figure visualization ﬁnal qrnn layer’s hidden state vectors imdb task timesteps along vertical axis. colors denote neuron activations. initial positive statement this movie simply gorgeous timestep triggers reset hidden states phrase exactly story timestep recommend movie everyone even you’ve never played game hidden units recover. layer computationally convenient multiple penn treebank relatively small dataset preventing overﬁtting considerable importance major focus recent research. obvious advance many regularization schemes would perform well applied qrnn. tests showed encouraging results zoneout applied qrnn’s recurrent pooling layer implemented described section experimental settings largely followed medium setup zaremba optimization performed stochastic gradient descent without momentum. learning rate epochs decayed subsequent epoch total epochs. additionally used regularization rescaled gradients norm zoneout applied performing dropout ratio forget gates qrnn without rescaling output dropout function. batches consist examples timesteps. comparing results gated qrnn zoneout results lstms ordinary variational dropout table qrnn highly competitive. qrnn without zoneout strongly outperforms medium lstm medium lstm zaremba recurrent dropout even competitive variational lstms. limited computational capacity qrnn’s pooling layer relative lstm’s recurrent weights providing structural regularization recurrence. without zoneout early stopping based upon validation loss required qrnn would begin overﬁtting. applying small amount zoneout early stopping required qrnn achieves competitive levels perplexity variational lstm ghahratable single model perplexity validation test sets penn treebank language modeling task. lower better. medium refers two-layer network hidden units layer. qrnn models include dropout embeddings layers. refers monte carlo dropout averaging test time. figure left training speed two-layer -unit batch examples timesteps. softmax include forward backward times optimization overhead includes gradient clipping regularization computations. right inference speed advantage -unit qrnn layer alone equal-sized cudnn lstm layer data given batch size sequence length. training results similar. mani variational inference based dropout applied recurrently. best performing variation also used monte carlo dropout averaging test time different masks making computationally expensive run. training dataset nvidia found qrnn substantially faster standard lstm even comparing optimized cudnn lstm. figure provide breakdown time taken chainer’s default lstm cudnn lstm qrnn perform full forward backward pass single batch training ptb. lstm implementations running time dominated computations even highly optimized cudnn implementation. qrnn implementation however layers longer bottleneck. indeed diminishing returns optimization qrnn softmax optimization overhead take equal greater time. note softmax vocabulary size words relatively small; tasks larger vocabularies softmax would likely dominate computation time. also important note cudnn library’s primitives natively support form recurrent dropout. running lstm uses state-of-the-art regularization scheme cudnn-like speeds would likely require entirely custom kernel. evaluate sequence-to-sequence qrnn architecture described challenging neural machine translation task iwslt german–english spoken-domain translation applying fully character-level segmentation. dataset consists sentence pairs parallel training data transcribed tedx presentations mean sentence length characters german english. remove training sentences characters english german uniﬁed vocabulary unicode code points. best performance development achieved using four-layer encoder– decoder qrnn units layer dropout regularization gradient rescaling maximum magnitude inputs supplied encoder reversed encoder convolutions masked. ﬁrst encoder layer used convolutional ﬁlter width encoder layers used optimization performed epochs minibatches examples using adam decoding performed using beam search beam width length normalization modiﬁed log-probability ranking criterion provided appendix. results using architecture compared equal-sized four-layer encoder–decoder lstm attention applying dropout optimized using adam; hyperparameters equal values qrnn beam search procedure applied. table shows qrnn outperformed character-level lstm almost matching performance word-level attentional baseline. table translation performance measured bleu train speed hours epoch iwslt german-english spoken language translation task. models trained in-domain data only negative log-likelihood training criterion. models trained epochs. qrnn model uses layers ﬁrst encoder layer. exploring alternatives traditional rnns sequence tasks major area current research. quasi-recurrent neural networks related several recently described models especially strongly-typed recurrent neural networks introduced balduzzi ghifary motivation constraints described work different balduzzi ghifary concepts learnware ﬁrmware parallel discussion convolution-like pooling-like subcomponents. fully connected layer recurrent connections violates constraint strong typing strongly-typed architectures also quasi-recurrent. however qrnn models strongly typed. particular t-rnn differs qrnn described paper ﬁlter size -pooling absence activation function similarly t-grus t-lstms differ qrnns ﬁlter size foifo-pooling respectively lack tanh tanh rather sigmoid qrnn also related work hybrid convolutional–recurrent models. zhou apply cnns word level generate n-gram features used lstm text classiﬁcation. xiao also tackle text classiﬁcation applying convolutions character level stride reduce sequence length feeding features bidirectional lstm. similar approach taken character-level machine translation. model’s encoder uses convolutional layer followed max-pooling reduce sequence length four-layer highway network bidirectional gru. parallelism convolutional pooling highway layers allows training speed comparable subword-level models without hard-coded text segmentation. qrnn encoder–decoder model shares favorable parallelism path-length properties exhibited bytenet architecture character-level machine translation based residual convolutions binary trees. model constructed achieve three desired properties parallelism linear-time computational complexity short paths pair words order better propagate gradient signals. intuitively many aspects semantics long sequences context-invariant computed parallel aspects require long-distance context must computed recurrently. many existing neural network architectures either fail take advantage contextual information fail take advantage parallelism. qrnns exploit parallelism context exhibiting advantages convolutional recurrent neural networks. qrnns better predictive accuracy lstm-based models equal hidden size even though fewer parameters substantially faster. experiments show speed accuracy advantages remain consistent across tasks word character levels. extensions cnns rnns often directly applicable qrnn model’s hidden states interpretable recurrent architectures channels maintain independence across timesteps. believe qrnns serve building block long-sequence tasks previously impractical traditional rnns. james bradbury richard socher. metamind neural machine translation system proceedings first conference machine translation berlin germany. association computational linguistics kalchbrenner lasse espeholt karen simonyan aaron oord alex graves koray kavukcuoglu. neural machine translation linear time. arxiv preprint arxiv. david krueger tegan maharaj j´anos kram´ar mohammad pezeshki nicolas ballas rosemary anirudh goyal yoshua bengio hugo larochelle aaron courville zoneout regularizing rnns randomly preserving hidden activations. arxiv preprint arxiv. ankit kumar ozan irsoy peter ondruska mohit iyyer james bradbury ishaan gulrajani victor zhong romain paulus richard socher. anything dynamic memory networks natural language processing. icml gr´egoire mesnil tomas mikolov marc’aurelio ranzato yoshua bengio. ensemble generative discriminative techniques sentiment analysis movie reviews. arxiv preprint arxiv. tijmen tieleman geoffrey hinton. lecture .-rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine transarxiv preprint lation system bridging human machine translation. arxiv. length normalization parameter output character ttrg target length equal source sentence length plus characters. reduces ordinary beam search probabilities", "year": 2016}