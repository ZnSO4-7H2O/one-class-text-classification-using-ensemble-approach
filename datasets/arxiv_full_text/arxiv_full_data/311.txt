{"title": "Collaborative Deep Learning for Recommender Systems", "tag": ["cs.LG", "cs.CL", "cs.IR", "cs.NE", "stat.ML"], "abstract": "Collaborative filtering (CF) is a successful approach commonly used by many recommender systems. Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance. To address this sparsity problem, auxiliary information such as item content information may be utilized. Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information. Nevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse. To address this problem, we generalize recent advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix. Extensive experiments on three real-world datasets from different domains show that CDL can significantly advance the state of the art.", "text": "icant role individuals using allows make eﬀective information. besides many companies using extensively target customers recommending products services. existing methods roughly categorized three classes content-based methods collaborative ﬁltering based methods hybrid methods. content-based methods make user proﬁles product descriptions recommendation. cf-based methods past activities preferences user ratings items without using user product content information. hybrid methods seek best worlds combining content-based cf-based methods. privacy concerns generally diﬃcult collect user proﬁles past activities. nevertheless cf-based methods limitations. prediction accuracy often drops signiﬁcantly ratings sparse. moreover cannot used recommending products receive rating information users. consequently inevitable cf-based methods exploit auxiliary information hence hybrid methods gained popularity recent years. according whether two-way interaction exists rating information auxiliary information divide hybrid methods sub-categories loosely coupled tightly coupled methods. loosely coupled methods like process auxiliary information provide features models. since information one-way rating information cannot provide feedback guide extraction useful features. sub-category improvement often rely manual tedious feature engineering process. contrary tightly coupled methods like allow two-way interaction. hand rating information guide learning features. hand extracted features improve predictive power models two-way interaction tightly coupled methods automatically learn features auxiliary information naturally balance inﬂuence rating auxiliary information. tightly coupled methods often outperform loosely coupled ones collaborative topic regression recently proposed tightly coupled method. probabilistic graphical model seamlessly integrates topic model latent dirichlet allocation model-based method probabilistic matrix factorization collaborative ﬁltering successful approach commonly used many recommender systems. conventional cf-based methods ratings given items users sole source information learning make recommendation. however ratings often sparse many applications causing cf-based methods degrade signiﬁcantly recommendation performance. address sparsity problem auxiliary information item content information utilized. collaborative topic regression appealing recent method taking approach tightly couples components learn diﬀerent sources information. nevertheless latent representation learned eﬀective auxiliary information sparse. address problem generalize recent advances deep learning i.i.d. input non-i.i.d. input propose paper hierarchical bayesian model called collaborative deep learning jointly performs deep representation learning content information collaborative ﬁltering ratings matrix. extensive experiments three real-world datasets diﬀerent domains show signiﬁcantly advance state art. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copyrights components work owned others must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. request permissions permissionsacm.org. kdd’ august sydney australia. acm. isbn ----// ..... http//dx.doi.org/./.. appealing method produces promising interpretable results. nevertheless latent representation learned often eﬀective enough especially auxiliary information sparse. representation learning problem focus paper. hand deep learning models recently show great potential learning eﬀective representations deliver state-of-the-art performance computer vision natural language processing applications. deep learning models features learned supervised unsupervised manner. although appealing shallow models features learned automatically inferior shallow models capturing learning similarity implicit relationship items. calls integrating deep learning performing deep learning collaboratively. unfortunately attempts made develop deep learning models uses restricted boltzmann machines instead conventional matrix factorization formulation perform extends work incorporating user-user item-item correlations. although methods involve deep learning actually belong cf-based methods incorporate content information like crucial accurate recommendation. uses low-rank matrix factorization last weight layer deep network signiﬁcantly reduce number model parameters speed training classiﬁcation instead recommendation tasks. music recommendation directly conventional deep belief networks assist representation learning content information deep learning components models deterministic without modeling noise hence less robust. models achieve performance boost mainly loosely coupled methods without exploiting interaction content information ratings. besides linked directly rating matrix means models perform poorly ratings sparse shown following experiments. address challenges above develop hierarchical bayesian model called collaborative deep learning novel tightly coupled method ﬁrst present bayesian formulation deep learning model called stacked denoising autoencoder this present model tightly couples deep representation learning content information collaborative ﬁltering ratings matrix allowing two-way interaction two. experiments show signiﬁcantly outperforms state art. note although present using sdae feature learning component actually general framework also admit deep learning models deep boltzmann machines recurrent neural networks convolutional neural networks simultaneously extract eﬀective deep feature representation content capture similarity implicit relationship items learned representation also used tasks recommendation. riori estimates also derive sampling-based algorithm bayesian treatment which interestingly turns bayesian generalized version back-propagation. chical bayesian model bridge stateof-the-art deep learning models besides bayesian nature easily extended incorporate auxiliary information boost performance. similar work recommendation task considered paper takes implicit feedback training test data. entire collection items represented j-by-s matrix bag-of-words vector xcj∗ item based vocabulary size users deﬁne i-by-j binary rating matrix example dataset citeulike-a user article personal library otherwise. given part ratings content information problem predict ratings note although focus movie recommendation article recommendation like paper model general enough handle recommendation tasks matrix plays role clean input sdae noise-corrupted matrix also j-by-s matrix denoted output layer sdae denoted j-by-kl matrix. similar denoted xlj∗. weight matrix bias vector respectively layer wl∗n denotes column number layers. convenience denote collection layers weight matrices biases. note l/-layer sdae corresponds l-layer network. ready present details model. ﬁrst brieﬂy review sdae give bayesian formulation sdae. followed presentation hierarchical bayesian model tightly integrates ratings content information. sdae feedforward neural network learning representations input data learning predict clean input output shown figure usually hidden layer middle i.e. ﬁgure constrained bottleneck input layer corrupted version clean input data. figure left graphical model cdl. part inside dashed rectangle represents sdae. example sdae shown. right graphical model degenerated cdl. part inside dashed rectangle represents encoder sdae. example sdae shown right note although still decoder sdae vanishes. prevent clutter omit variables except graphical models. hyperparameters conﬁdence parameter similar note middle layer serves bridge ratings content information. middle layer along latent oﬀset enables simultaneously learn effective feature representation capture similarity relationship items similar generalized sdae computational eﬃciency also take inﬁnity. function. model degenerate bayesian formulation sdae. call generalized sdae. note ﬁrst layers network encoder last layers decoder. maximization note generation clean input part generative process bayesian sdae generation noise-corrupted input artiﬁcial noise injection process help sdae learn robust feature representation. fourth term equivalent sdae minimizing reconstruction error. seeing view neural networks approaches positive inﬁnity training probabilistic graphical model figure would degenerate simultaneously training neural networks overlaid together common input layer diﬀerent output layers shown figure note second network much complex typical neural networks involvement rating matrix. ratio λn/λv approaches positive inﬁnity degenerate two-step model latent representation learned using sdae directly ctr. another extreme happens λn/λv goes zero decoder sdae essentially vanishes. right figure graphical model degenerated λn/λv goes zero. demonstrated experiments predictive performance suﬀer greatly extreme cases. based model above parameters could treated random variables fully bayesian methods markov chain monte carlo variational approximation methods applied. however treatment typically incurs high computational cost. besides since primary baseline comparison would fair reasonable take approach analogous used ctr. consequently devise em-style algorithm obtaining estimates encoder function takes corrupted content vector item input computes encoding item function also takes input computes encoding reconstructed content vector item example number layers output third layer output sixth layer. alternating update local optimum several commonly used techniques using momentum term used alleviate local optimum problem. completeness also provide samplingbased algorithm appendix. feedback speciﬁcally zero entry fact user interested item user aware existence. such precision suitable performance measure. like recommender systems sort predicted ratings candidate items recommend items target user. recallm user deﬁned experiments ﬁrst validation optimal hyperparameters svdfeature deepmusic. regularization hyperparameters latent factors diﬀerent contexts grid search performs best weights rating matrix content matrix sparse setting. dense setting weights respectively. svdfeature best performance achieved regularization hyperparameters users items learning rate equal deepmusic best performance achieved using convolutional layers. also best tune hyperparameters. achieve good prediction performance directly perform grid search hyperparameters grid search split training data -fold cross validation. masking noise noise level corrupted input clean input layer sdae dropout rate achieve adaptive regularization. terms network architecture number hidden units extensive experiments conducted three real-world datasets diﬀerent domains demonstrate eﬀectiveness model quantitatively qualitatively. datasets three datasets diﬀerent real-world domains citeulike netﬂix experiments. ﬁrst datasets collected diﬀerent ways speciﬁcally diﬀerent scales diﬀerent degrees sparsity mimic diﬀerent practical situations. ﬁrst dataset citeulike-a mostly second dataset citeulike-t collected independently ﬁrst one. manually selected seed tags collected articles least tags. similar users fewer articles included. result citeulike-a contains users items. citeulike-t numbers citeulike-t contains users items citeulike-a. also citeulike-t much sparser useritem matrix entries contain ratings citeulike-a ratings user-item matrix entries. last dataset netﬂix consists parts. ﬁrst part ratings movie titles netﬂix challenge dataset. second part plots corresponding movies collected imdb similar order consistent implicit feedback setting ﬁrst datasets extract positive ratings training testing. removing users less positive ratings movies without plots users movies ratings ﬁnal dataset. follow procedure preprocess text information extracted titles abstracts articles plots movies. removing stop words discriminative words according tf-idf values chosen form vocabulary dataset similar randomly select items associated user form training rest dataset test set. evaluate compare models sparse dense settings respectively experiments. value repeat evaluation times diﬀerent randomly selected training sets average performance reported. code data available www.wanghao.in citeulike allows users create collections articles. abstract title tags article. details citeulike data found http//www.citeulike.org. http//www.imdb.com figures show results compare deepmusic svdfeature using three datasets sparse dense settings. strong baseline beats deepmusic svdfeature datasets even though deepmusic deep architecture. sparse setting outperforms svdfeature time sometimes even achieves performance comparable ctr. deepmusic performs poorly lack ratings overﬁtting. dense setting svdfeature signiﬁcantly better citeulike-a citeulikeinferior netﬂix. deepmusic still slightly worse reasons mentioned section focus speciﬁcally comparing citeulike-a -layer outtable shows models sparse settings. almost twice ctr. tables show recall results diﬀerent numbers layers applied three datasets sparse dense settings. citeulike-t netﬂix recall increases number layers increases. citeulike-a starts overﬁt exceeds layers. since note results somewhat diﬀerent ﬁrst datasets although domain. diﬀerent ways datasets collected discussed above. speciﬁcally text information rating matrix citeulike-t much sparser. seamlessly integrating deep representation learning content information rating matrix handle sparse rating matrix figure shows results diﬀerent values using citeulike-t dense setting. similar phenomena observed number layers value varied omitted space constraints. mentioned previous section extremely large λn/λv approach positive inﬁnity degenerates separate models. case latent item representation learned sdae unsupervised manner directly ctr. consequently interaction bayesian sdae collaborative ﬁltering component based matrix factorization hence prediction performance suﬀer greatly. extreme extremely small λn/λv approach zero degenerates figure decoder bayesian sdae component essentially vanishes. encoder bayesian sdae component easily overﬁt latent item vectors learned simple matrix factorization. figure prediction performance degrades signiﬁcantly gets large small. recallm already close result pmf. gain better insight ﬁrst take look example users citeulike-t dataset represent proﬁle using three matched topics. examine recommended articles returned -layer ctr. models trained sparse setting table speculate user might computer scientist focus recommendation clearly indicated ﬁrst topic second ctr. correctly recommends many articles tagging systems focuses social networks instead. digging data rated article training data ‘what drives content tagging case photos flickr’ article talks impact social networks tagging behaviors. explain focuses recommendation social networks. hand better understand points article make appropriate recommendation accordingly. consequently precision respectively. rectly captures user proﬁle achieves precision however recommends quite articles astronomy instead. examining data rated article returned ‘simulating deformable particle suspensions using coupled lattice-boltzmann ﬁnite-element method’. expected article deformable particle suspension blood cells. might misinterpreted article focusing recommendation words like ‘ﬂows’ ‘formation’ separately. explains recommends articles like ‘formation versus destruction evolution star cluster population galaxy mergers’ ‘macroscopic eﬀects spectral structure turbulent ﬂows’ result precision next present another case study netﬂix dataset dense setting case study choose user vary number ratings training given user partition training test data remains users. examine recommendation adapts user expresses preference movies. table shows recommendation lists number training samples training samples movies user likes ‘moonstruck’ ‘true romance’ romance movies. precision close samples added precision boosted remains unchanged movies ‘johnny english’ ‘american beauty’ belong action drama movies. successfully captures user’s change taste gets recommendations right fails similar phenomena observed number training samples increases case study sensitive enough changes user taste hence provide accurate recommendation. following update rules paper computational complexity updating dimensionality learned representation number items. complexity number users size vocabulary dimensionality output ﬁrst layer. note third term cost computing output encoder dominated computation ﬁrst layer. user search image query images queries tagging index tags searching social online internet communities sharing networking facebook friends ties participation collaborative optimization ﬁltering recommendation contextual planning items preferences structure collaborative tagging systems usage patterns collaborative tagging systems folksonomy complex network tagging paper taxonomy flickr academic article read tagging systems work information retrieval folksonomies search ranking tagging communities vocabulary evolution complex dynamics collaborative tagging improved annotation blogosphere autotagging hierarchical clustering collaborative tagging tripartite network user social online internet communities sharing networking facebook friends ties participation search image query images queries tagging index tags searching feedback event transformation wikipedia indicators vitamin indirect taxonomy tagging paper taxonomy flickr academic article read structure evolution online social networks group formation large social networks membership growth evolution measurement analysis online social networks face crowd social searching social browsing strength weak ties flickr recommendation based collective knowledge computer-mediated communication network social capital self-esteem online social network sites longitudinal analysis increasing participation online communities framework human-computer interaction user cloud codes matter boundary lattice particles galaxies ﬂuid galaxy mobile membrane wireless sensor mobility lipid traﬃc infrastructure monitoring hybrid orientation stress ﬂuctuations load temperature centrality mechanical two-dimensional heat modeling dense suspensions deformable particles three dimensions simpliﬁed particulate model coarse-grained hemodynamics simulations lattice boltzmann simulations blood non-newtonian rheology clotting processes genome-wide association study celiac disease identiﬁes risk variants eﬃcient accurate simulations deformable particles multiscale model thrombus development multiphase hemodynamic simulation pulsatile coronary artery lattice boltzmann modeling thrombosis giant aneurysms lattice boltzmann simulation clotting stented aneursysms predicting dynamics rheology blood user cloud codes matter boundary lattice particles galaxies ﬂuid galaxy transition equations dynamical discrete equation dimensions chaos transitions living trust mobile membrane wireless sensor mobility lipid traﬃc infrastructure monitoring multiphase hemodynamic simulation pulsatile coronary artery metallicity evolution star-forming galaxies redshift formation versus destruction evolution star cluster population galaxy mergers clearing globular clusters macroscopic eﬀects spectral structure turbulent ﬂows wigglez dark energy survey lattice-boltzmann simulation blood digitized vessel networks global properties ’ordinary’ early-type galaxies proteus direct forcing method simulations particulate ﬂows analysis mechanisms platelet near-wall excess arterial blood conditions experiments conducted servers intel cpus nvidia tesla gpus each. using matlab implementation gpu/c++ acceleration epoch takes seconds takes epochs ﬁrst datasets. netﬂix takes seconds epoch needs much fewer epochs satisfactory recommendation performance. since netﬂix much larger datasets shows scalable. expect changing implementation pure c++/cuda would signiﬁcantly reduce time cost. demonstrated paper state-of-the-art performance achieved jointly performing deep representation learning content information collaborative ﬁltering ratings matrix. know ﬁrst hierarchical bayesian model bridge state-of-the-art deep learning models terms learning besides algorithm attaining estimates also derive sampling-based algorithm bayesian treatment bayesian generalized version back-propagation. among possible extensions could made bag-of-words representation replaced powerful alternatives bayesian nature also provides potential performance boost side information incorporated besides movies training moonstruck true romance johnny english american beauty princess bride double platinum rising dead poets society waiting guﬀman swordﬁsh fish called wanda terminator clockwork orange sling blade bridget jones’s diary raising arizona streetcar named desire untouchables full monty remarked above actually provides framework also admit deep learning models sdae. promising choice convolutional neural network model which among things explicitly take context order words account. performance boost possible using deep learning models. rithm bayesian treatment cdl. turns bayesian generalized version well-known back-propagation learning algorithm. space constraints list results without detailed derivation. interestingly goes inﬁnity adaptive rejection metropolis sampling used sampling turns bayesian generalized version speciﬁcally figure shows getting gradient loss function point next sample would drawn region line equivalent probabilistic version sample curve loss function tangent line would added better approximate distribution corresponding loss function. that samples would drawn region lines. sampling besides searching local optima using gradients algorithm also takes variance consideration. call bayesian generalized back-propagation. p.-a. manzagol. stacked denoising autoencoders learning useful representations deep network local denoising criterion. jmlr wager wang liang. dropout training adaptive regularization. nips pages wang blei. collaborative topic modeling", "year": 2014}