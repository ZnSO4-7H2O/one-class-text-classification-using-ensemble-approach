{"title": "LSTM Neural Reordering Feature for Statistical Machine Translation", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Artificial neural networks are powerful models, which have been widely applied into many aspects of machine translation, such as language modeling and translation modeling. Though notable improvements have been made in these areas, the reordering problem still remains a challenge in statistical machine translations. In this paper, we present a novel neural reordering model that directly models word pairs and alignment. By utilizing LSTM recurrent neural networks, much longer context could be learned for reordering prediction. Experimental results on NIST OpenMT12 Arabic-English and Chinese-English 1000-best rescoring task show that our LSTM neural reordering feature is robust and achieves significant improvements over various baseline systems.", "text": "artiﬁcial neural networks powerful models widely applied many aspects machine translation language modeling translation modeling. though notable improvements made areas reordering problem still remains challenge statistical machine translations. paper present novel neural reordering model directly models word pairs alignment. further utilizing lstm recurrent neural networks much longer context could learned reordering prediction. experimental results nist openmt arabic-english chinese-english -best rescoring task show lstm neural reordering feature robust achieves signiﬁcant improvements various baseline systems. statistical machine translation language model translation model reordering model three important components. among models reordering model plays important role phrase-based machine translation still remains major challenge current study. lexicalized conditions reordering probabilities current phrase pairs. according orientation determinants lexicalized reordering model classiﬁed word-based phrase-based hierarchical phrase-based paper propose novel neural reordering feature including longer context predicting orientations. utilize long short-term memory recurrent neural network directly models word pairs predict probable orientation. experimental results nist openmt arabic-english chinese-english translation show neural reordering model achieves signiﬁcant improvements various baselines -best rescoring task. feed-forward neural language model ﬁrst proposed bengio breakthrough language modeling. mikolov proposed recurrent neural network language modeling include much longer context history predicting next word. experimental results show rnn-based language model signiﬁcantly outperform standard feed-forward language model. equation represents phrase orientations. example commonly used msd-based orientation type takes three values stands monotone swap discontinuous. deﬁnition msd-based orientation shown equation order include context information determining reordering propose recurrent neural network shown perform considerably better standard feed-forward architectures sequence prediction however conventional backpropagation training suffers gradient vanishing issues later long short-term memory proposed solving gradient vanishing problem could catch longer context standard rnns sigmoid activation functions. paper adopt lstm architecture training neural reordering model. devlin proposed neural network joint model conditioning source target language context target word predicting. though network architecture simple feed-forward neural network results shown signiﬁcant improvements state-of-the-art baselines. also forward neural translation model utilizing lstm-based bidirectional rnn. introducing bidirectional rnns target word conditioned history also future source context forms full source sentence predicting target words. proposed recursive autoencoder phrase pairs continuous vectors handle reordering problems classiﬁer. also suggested including current previous phrase pairs determine phrase orientations could achieve improvements reordering accuracy noticed ﬁrst time lstm-rnn reordering model. could include much longer context information determine phrase orientations using architecture. furthermore utilizing lstm units network able capture much longer range dependencies standard rnns. need record ﬁxed length history information decoding step utilize lstm-rnn reordering model feature -best rescoring step. word alignments known generating n-best list possible lstm-rnn reordering model score hypothesis. traditional statistical machine translation lexicalized reordering models widely used considers alignments current previous phrase pairs determine orientation. formally given source language sentence target language sentence phrase alignment lexicalized reordering model illustrated equation conditions output layer composed orientation types. example condition output layer contains units right orientation. finally apply softmax function obtain normalized probabilities orientation. current source/target word one-to-many alignment judge orientation considering ﬁrst aligned target/source word aligned target/source words annotated ollow reordering type means word pairs inherent orientation previous word pair. openmt parallel dataset. sama tokenizer arabic word tokenization in-house segmenter chinese words. english part parallel data tokenized lowercased. development test sets references segment. statistics development test sets shown table baseline systems built opensource phrase-based toolkit moses word alignment phrase extraction done giza++ l-normalization grow-diag-ﬁnal reﬁnement rule monolingual part training data used train -gram language model using srilm parameter tuning done k-best mira guarantee result stability tune every system times independently take average bleu score translation quality evaluated case-insensitive bleu- metric statistical signiﬁcance test also carried paired bootstrap resampling method intervals models evaluated -best rescoring step features -best list well lstm-rnn reordering feature retuned k-best mira algorithm. neural network training parallel text baseline training. trade-off between computational cost performance projection layer hidden layer enough task initial learning rate standard optimization without momentum. trained model total epochs crossentropy criterion. input output vocabulary respectively out-ofvocabulary words mapped token. results different orientation types ﬁrst test neural reordering model baseline contains word-based reordering model orientation. results shown table that among various orientation types model could give consistent improvements baseline system. overall bleu improvements range arabic-english chinese-english systems. neural results signiﬁcantly better baselines meantime also left-right based orientation methods mslr consistently outperform msd-based orientations. caused non-separability problem means msd-based methods vulnerable change context weak resolving reordering ambiguities. similar conclusion found results different reordering baselines also test approach various baselines either contains word-based phrase-based hierarchical phrase-based reordering model. show results mslr orientation relatively superior others according results section ar-en system baseline +nrm mslr baseline +nrm mslr baseline hier +nrm mslr zh-en system baseline +nrm mslr baseline +nrm mslr baseline hier +nrm mslr table results various baselines arabic-english chinese-english system. word-based; phrasebased; hier hierarchical phrase-based reordering model. results signiﬁcantly better baselines table though strong hierarchical phrase-based reordering model baseline model still bring maximum gain bleu score suggest model applicable robust various circumstances. however noticed gains arabic-english system relatively greater chinese-english system. probably because hierarchical reordering features tend work better chinese words thus model bring little remedy baseline. present novel work build reordering model using lstm-rnn much sensitive change context introduce rich context information reordering prediction. furthermore proposed model purely lexicalized straightforward easy realize. experimental results -best rescoring show neural reordering feature robust could give consistent improvements various baseline systems. future planning extend wordbased lstm reordering model phrase-based reordering model order dissolve much ambiguities improve reordering accuracy. further-", "year": 2015}