{"title": "Structural-RNN: Deep Learning on Spatio-Temporal Graphs", "tag": ["cs.CV", "cs.LG", "cs.NE", "cs.RO"], "abstract": "Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatio-temporal graphs are a popular tool for imposing such high-level intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks~(RNNs). We develop a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.", "text": "figure st-graph s-rnn example problem. shows example activity modeling problems requires spatial temporal reasoning. stgraph capturing spatial temporal interactions human objects. schematic representation structural-rnn architecture automatically derived st-graph. captures structure interactions st-graph rich scalable manner. spatio-temporal nature. example cooking activity humans interact multiple objects space time. similarly parts human body individual functions work concert generate physically sensible motions. hence bringing high-level spatio-temporal structures rich sequence modeling capabilities together particular importance many applications. notable success rnns proven capability many end-to-end learning tasks however lack high-level intuitive spatio-temporal structure though shown successful modeling long sequences therefore augmenting high-level structure learning capability rnns leads powerful tool best worlds. spatio-temporal graphs popular general tool representing high-level spatio-temporal structures. nodes graph typically represent problem components edges capture spatio-temporal interactions. achieve deep recurrent neural network architectures though remarkably capable modeling sequences lack intuitive high-level spatio-temporal structure. many problems computer vision inherently underlying high-level structure beneﬁt spatiotemporal graphs popular tool imposing highlevel intuitions formulation real world problems. paper propose approach combining power high-level spatio-temporal graphs sequence learning success recurrent neural networks develop scalable method casting arbitrary spatiotemporal graph rich mixture feedforward fully differentiable jointly trainable. proposed method generic principled used transforming spatio-temporal graph employing certain well deﬁned steps. evaluations proposed approach diverse problems ranging modeling human motion object interactions shows improvement state-of-the-art large margin. expect method empower approaches problem formulation high-level spatio-temporal graphs recurrent neural networks. world live inherently structured. comprised components interact space time leading spatio-temporal composition. utilizing structures problem formulation allows domainexperts inject high-level knowledge learning frameworks. incentive many efforts computer vision machine learning logic networks graphical models structured svms structures span space time particular interest computer vision robotics communities. primarily interactions between humans environment real world inherently goal develop generic tool transforming arbitrary st-graph feedforward mixture rnns named structural-rnn figure schematically illustrates process sample spatio-temporal problem shown bottom corresponding st-graph representation shown middle mixture counterpart st-graph shown top. high-level steps given arbitrary st-graph ﬁrst roll time decompose contributing factor components. factors identify independent components collectively determine decision derived edges nodes st-graph. semantically group factor components represent group using results desired mixture. main challenges transformation problem making mixture rich possible enable learning complex functions keeping mixture scalable respect size input st-graph. order make resulting mixture rich liberally represent spatio-temporal factor using rnn. hand keep overall mixture scalable lose essential learning capacity utilize factor sharing allow factors similar semantic functions share rnn. results rich scalable feedforward mixture rnns equivalent provided st-graph terms input output spatiotemporal relationships. mixture also fully differentiable therefore trained jointly entity. proposed method principled generic transformation based well deﬁned steps applicable problem formulated st-graphs several previous works attempted solving speciﬁc problems using collection rnns almost unanimously task-speciﬁc. also utilize mechanisms similar factorization factor sharing devising architecture ensure richness scalability. s-rnn also modular enjoying underlying high-level structure. enables easy high-level manipulations basically possible unstructured rnns evaluate proposed approach diverse spatio-temporal problems show significant improvements state problem. also study complexity convergence properties s-rnn provide experimental insights visualizing memory cells reveals cells interestingly represent certain semantic operations. code entire framework accepts st-graph input yields output mixture available contribution paper generic method casting arbitrary st-graph rich scalable jointly trainable mixture defence structured approaches show s-rnn signiﬁcantly outperforms unstructured counterparts defence rnns show several diverse spatio-temporal problems modeling structure s-rnn outperforms non-deep learning based structured counterparts. give categorized overview related literature. general three main characteristics differentiate work existing techniques generic restricted speciﬁc problem providing principled method transforming st-graph scalable rich feedforward mixture jointly trainable. spatio-temporal problems. problems require spatial temporal reasoning common robotics computer vision. examples include human activity recognition segmentation videos context-rich human-object interactions modeling human motion etc. spatio-temporal reasoning also ﬁnds application assistive robots driver understanding object recognition fact daily activities spatio-temporal nature. growing interests rich interactions robotics form reasoning become even important. evaluate generic method three context-rich spatio-temporal problems human motion modeling human-object interaction understanding driver maneuver anticipation mixtures deep architectures. several previous works build multiple networks wire together order capture complex structure problem promising results applications activity detection scene labeling image captioning object detection however architectures mostly hand designed speciﬁc problems though demonstrate beneﬁt using modular deep architecture. recursive neural networks hand generic feedforward architectures problems recursive structure parsing natural sentences scenes work remedy problems expressed spatio-temporal graphs. spatiotemporal problem hand practitioner needs express intuition problem st-graph. deep learning graphical models. many works addressed deep networks graphical models structured prediction tasks. bengio combined cnns hand writing recognition. tompson jointly train human pose estimation. chen similar approach image classiﬁcation general mrf. recently several works figure example spatio-temporal graph human activity. st-graph capturing human-object interaction. unrolling st-graph edges nodes edges labelled feature vectors associated them. factor graph parameterization st-graph. node edge st-graph corresponding factor. addressed end-to-end image segmentation fully connected several works follow two-stage approach decouple deep network crf. applied multiple problems including image segmentation pose estimation document processing etc. works advocate well demonstrate beneﬁt exploiting structure problem together rich deep architectures. however largely address spatio-temporal problems proposed architectures task-speciﬁc. conditional random fields model dependencies outputs learning joint distribution them. applied many applications including st-graphs commonly modeled spatio-temporal approach adopt st-graphs general graph representation embody using mixture architecture. unlike approach probabilistic meant model joint distribution outputs. s-rnn instead learns dependencies outputs structural sharing rnns outputs. section describe approach building structural-rnn architectures. start st-graph decompose factor components represent factor using rnn. rnns interconnected resulting architecture captures structure interactions st-graph. representation spatio-temporal graphs many applications require spatial temporal reasoning modeled using st-graphs represent st-graph whose structure unrolls time edges figure shows example st-graph capturing human-object interactions activity. nodes edges st-graph repeats time. particular figure shows st-graph unrolled time. unrolled st-graph nodes given time step connected undirected spatio-temporal edge nodes adjacent time steps congiven st-graph feature vectors associated shown figure goal nodes predict node labels time step instance human-object interaction node features represent human object poses edge features relative orientation; node labels represent human activity object affordance. affected node interactions nodes leading overall complex system. interactions commonly parameterized factor graph conveys function st-graph factorizes simpler functions derive s-rnn architecture factor graph representation st-graph. factor graph representation factor function node pairwise factor edge. figure shows factor graph corresponding st-graph sharing factors nodes. factor stgraph parameters needs learned. instead learning distinct factor node semantically similar nodes optionally share factors. example object nodes {uw} st-graph share node factor parameters. modeling choice allows enforcing parameter sharing similar nodes. gives ﬂexibility handle st-graphs nodes without increasing number parameters. purpose partition nodes semantically similar nodes node factor ψvp. figure re-draw st-graph assign color nodes sharing node factors. partitioning nodes semantic meanings leads natural semantic partition edges edges whose nodes form semantic pair. therefore edges share edge factor ψem. example human-object note adopted factor graph tool capturing interactions modeling overall function. factor graphs commonly used probabilistic graphical models factorizing joint probability distributions. consider general st-graphs establish relations probabilistic function decomposition properties. figure example st-graph s-rnn. st-graph figure redrawn colors indicate sharing nodes edge factors. nodes edges color share factors. overall distinct factors node factors edge factors. s-rnn architecture factor. edgernns nodernns connected form bipartite graph. parameter sharing human object nodes happen edgernn forward-pass human node involve rnns figure show detailed layout forward-pass. input features human-object edge features xvw. forward-pass object node involve rnns forward-pass edgernn processes edge feature xvw. edges modeled edge factor. sharing factors based semantic meaning makes overall parametrization compact. fact sharing parameters necessary address applications number nodes depends context. example human-object interaction number object nodes vary environment. therefore without sharing parameters object nodes model cannot generalize environments objects. modeling ﬂexibility edge factors shared across edges hence figure object-object temporal edge colored differently object-object spatio-temporal edge. input output s-rnn graph semantically partition edges find factor components {ψvp ψem} represent nodernn represent edgernn connect {rem} {rvp} form bipartite graph. s.t. return order predict label node consider node factor edge factors connected factor graph. deﬁne node factor edge factor neighbors jointly affect label node st-graph. formally node factor edge factor neighbors exist node connects factor graph. deﬁnition building s-rnn architecture captures interactions st-graph. structural-rnn spatio-temporal graphs derive s-rnn architecture factor graph representation st-graph. factors st-graph operate temporal manner time step factors observe features perform computation features. s-rnn represent factor rnn. refer rnns obtained node factors nodernns rnns obtained edge factors edgernns. interactions represented st-graph captured connections nodernns edgernns. denote rnns corresponding node factor edge factor respectively. order obtain feedforward network connect edgernns nodernns form bipartite graph particular edgernn connected nodernn factors neighbors st-graph i.e. jointly affect label node st-graph. summarize algorithm show steps constructing s-rnn architecture. figure shows s-rnn human activity represented figure nodernns combine outputs edgernns connected predict node labels. predictions nodernns interact edgernns edgernn handles speciﬁc semantic interaction nodes connected st-grap models interactions evolve time. next section explain inputs outputs training procedure s-rnn. training structural-rnn architecture order train s-rnn architecture node st-graph features associated node architecture. forward-pass node input edgernn temporal sequence edge edge inciedge features dent node st-graph. nodernn time step concatenates node feature outputs edgernns connected predicts node label. time training errors prediction backpropagated nodernn edgernns involved forward-pass. s-rnn non-linearly combines node edge features associated nodes order predict node labels. forward-pass. forward-pass involves edgernns since human node interacts object nodes {uw} pass summation edge features input summation features opposed concatenation important handle variable number object nodes ﬁxed architecture. since object count varies environment challenging represent variable context ﬁxed length feature vector. empirically adding features works better mean pooling. conjecture addition retains object count structure st-graph mean pooling averages number edges. nodernn concatenates node features outputs edgernns predicts activity time step. parameter sharing structured feature space. important aspect s-rnn sharing parameters across node labels. parameter sharing node labels happen common forward-pass. example figure edgernn common forward-pass human node object nodes. furthermore parameters gets updated back-propagated gradients object human nodernns. affects human object node labels. since human node connected multiple object nodes input edgernn always linear combination human-object edge features. imposes structure features processed formally input inner product feature matrix storing edge features s.t. vector captures structured feature space. entries depending node forward-passed. example human node object node figure applications include modeling human motion motion capture data human activity detection anticipation maneuver anticipation real-world driving data human body good example separate well related components. motion involves complex spatiotemporal interactions components resulting sensible motion styles like walking eating etc. experiment represent complex motion humans st-graphs learn model s-rnn. show structured approach outperforms state-of-the-art unstructured deep architecture motion forecasting motion capture data. several approaches based gaussian processes restricted boltzmann machines rnns proposed model human motion. recently fragkiadaki proposed encoderrnn-decoder gets state-of-the-art forecasting results mocap data s-rnn architecture human motion. s-rnn architecture follows st-graph shown figure according st-graph spine interacts body parts arms legs interact other. st-graph automatically transformed s-rnn following section resulting s-rnn three nodernns type body part four edgernns modeling spatio-temporal interactions them three edgernns temporal connections. edgernns nodernns fcfc-lstm lstm-fc-fcfc architectures respectively skip input output connections outputs nodernns skeleton joints different body parts concatenated reconstruct complete skeleton. order model human motion train s-rnn predict mocap frame time given frame time similar gradually noise mocap frames training. simulates curriculum learning helps keeping forecasted motion close manifold human motion. node features joint values expressed exponential edge features concatenation figure forecasting eating activity test subject. aperiodic activities lstm-lr struggle model human motion. s-rnn hand mimics ground truth short-term generates human like motion long term. without edgernns motion freezes mean standing position. video node features. train rnns jointly minimize euclidean loss predicted mocap frame ground truth. supplementary material project page training details. evaluation setup. compare s-rnn stateof-the-art architecture mocap data also compare layer lstm architecture baseline. motion forecasting follow experimental setup downsample train subjects test subject forecast ﬁrst feed architectures seed mocap frames forecast future frames. following consider walking eating smoking activities. addition three also consider discussion activity. forecasting specially challenging activities complex aperiodic human motion. data signiﬁcant parts eating smoking discussion activities aperiodic walking activity mostly periodic. evaluation demonstrates beneﬁts underlying structure three important ways present visualizations quantitative results complex aperiodic activities evaluates periodic walking motion); forecast human motion almost twice longer state-of-the-art challenging aperiodic activities; ﬁnally show s-rnn interestingly learns semantic concepts demonstrate modularity generating hybrid human motion. unstructured deep architectures like offer modularity. qualitative results motion forecasting. figure shows forecasting human motion eating activity subject drinks walking. s-rnn stays close ground-truth short-term generates human like motion long-term. removing edgernns parts human body become independent stops interacting parameters. hence without edgernns skeleton freezes mean position. lstm-lr suffers drifting problem. many test examples drifts mean position walking human made similar observations lstm-lr). motion generated stays human-like short-term drifts away non-human like motion long-term. common outcome complex aperiodic activities unlike s-rnn. furthermore produced human motion non-smooth many test examples. video project page examples quantitative evaluation. follow evaluation metric fragkiadaki present angle error forecasted mocap frame ground truth table qualitatively models human motion better lstm-lr. however short-term mimic ground-truth well lstm-lr. fragkiadaki also note trade-off lstm-lr. hand s-rnn outperforms lstm-lr short-term motion forecasting activities. s-rnn therefore mimics ground truth short-term generates human like motion long term. well handles short long term forecasting. stochasticity human motion longterm forecasts signiﬁcantly differ ground truth still depict human-like motion. reason long-term forecast numbers table fair representative algorithms modeling capabilities. also observe discussion challenging figure s-rnn memory cell visualization. cell nodernn ﬁres putting forward. cell nodernn ﬁres moving hand close face. visualize cell eating smoking activities. aperiodic activity algorithms. user study. asked users rate motions likert scale s-rnn performed best according user study. supplementary material results. summarize unstructured approaches like lstm-lr struggles model long-term human motion complex activities. s-rnn’s good performance attributed structural modeling human motion underlying st-graph. s-rnn models body part separately nodernns captures interactions edgernns order produce coherent motions. going deeper structural-rnn visualization memory cells. investigated srnn memory cells represent meaningful semantic submotions. semantic cells earlier studied problems ﬁrst present vision task human motion. figure show cell nodernn learns semantic motion moving forward. cell ﬁres positive forward movement negative backward movement. subject walks cell alternatively ﬁres right left leg. longer activations right corresponds longer steps taken subject right leg. similarly cell nodernn learns concept moving hand close face shown figure cell ﬁres whenever subject moves hand closer face during eating smoking. cell remains active long hand stays close face. video generating hybrid human motion. demonstrate ﬂexibility modular architecture generating novel meaningful motions data set. modularity interest explored generate diverse motion styles result underlying high-level structure approach allows exchange rnns s-rnn architectures trained different motion styles. leverage create novel s-rnn architecture generates hybrid figure generating hybrid motions demonstrate ﬂexibility s-rnn generating hybrid motion human jumping forward leg. train test error. s-rnn generalizes better smaller test error. motion human jumping forward shown figure experiment modeled left right different nodernns. trained independent s-rnn models slower human faster human swapped left nodernn trained models. resulting faster human slower left jumps forward left keep twice faster right leg. unstructured architectures like offer kind ﬂexibility. figure examines test train error iterations. s-rnn converge similar training error however s-rnn generalizes better smaller test error next step prediction. discussion supplementary. human activity detection anticipation section present s-rnn modeling human activities. consider cad- data activities involve rich human-object interactions. activity consist sequence sub-activities objects affordance evolves activity progresses. detecting anticipating sub-activities affordance enables personal robots assist humans. however problem challenging involves complex interactions humans interact multiple objects activity objects also interact makes particularly good evaluating method. koppula represents rich spatio-temporal interactions st-graph shown figure models spatio-temporal crf. experiment show modeling st-graph s-rnn yields superior results. node edges features figure shows s-rnn architecture model st-graph. since number objects varies environment factor sharing object nodes human-object edges becomes crucial. s-rnn handles object nodes human-object edges respectively. allows ﬁxed s-rnn architecture handle varying size st-graphs. edgernns single layer lstm size nodernns lstm-softmax. time step human nodernn outputs sub-activity label object nodernn outputs affordance table maneuver anticipation miles real-world driving data. s-rnn derived st-graph shown figure jain st-graph models probabilistic frame aio-hmm. table shows average precision recall time-to-maneuver. time-to-maneuver interval time algorithm’s prediction start maneuver. algorithms compared features table results cad- s-rnn architecture derived st-graph figure outperforms koppula models st-graph probabilistic framework. s-rnn multi-task setting improves performance. observed st-graph upto time goal detect sub-activity affordance labels current time also anticipate future labels time step detection train s-rnn labels current time step. anticipation train architecture predict labels next time step given observations upto current time. also train multi-task version s-rnn softmax layers nodernn jointly train anticipation detection. table shows detection anticipation f-scores averaged classes. s-rnn signiﬁcantly improves koppula anticipation detection anticipating object affordance s-rnn f-score detection. s-rnn markov assumptions like spatio-temporal therefore better models long-time dependencies needed anticipation. table also shows importance edgernns handling spatio-temporal components. edgernn transfers information human objects helps predicting object labels. therefore s-rnn without edgernns poorly models objects. signiﬁes importance edgernns also validates design. finally training s-rnn multi-task manner works best majority cases. figure show visualization eating activity. show representative frame sub-activity corresponding predictions. s-rnn complexity. terms complexity discuss aspects function underlying st-graph number rnns mixture; complexity forward-pass. number rnns depends number semantically similar nodes st-graph. overs-rnn architecture compact edgernns shared nodernns number semantic categories usually context-rich applications. furthermore factor sharing number rnns increase semantically similar nodes added st-graph. forward-pass complexity figure qualitative result eating activity cad-. shows multi-task s-rnn detection anticipation results. sub-activity time labels anticipated time depends number rnns. since forward-pass edgernns nodernns happen parallel practice complexity depends cascade neural networks driver maneuver anticipation ﬁnally present s-rnn another application involves anticipating maneuvers several seconds happen. jain represent problem stgraph shown figure model st-graph probabilistic bayesian network stgraph represents interactions observations outside vehicle driver’s maneuvers observations inside vehicle model st-graph s-rnn architecture using node edge features jain table shows performance different algorithms task. s-rnn performs better state-of-the-art aio-hmm every setting. supplementary material discussion details conclusion proposed generic principled approach combining high-level spatio-temporal graphs sequence modeling success rnns. make factor graph factor sharing order obtain mixture scalable applicable problem expressed st-graphs. mixture captures rich interactions underlying st-graph. demonstrated significant improvements s-rnn three diverse spatiotemporal problems including human motion modeling; human-object interaction; driver maneuver anticipation. visualizing memory cells showed s-rnn learns certain semantic sub-motions demonstrated modularity generating novel human motion. references bengio lecun henderson. globally trained handwritten word recognizer using spatial representation convolutional neural networks hidden markov models. nips chen papandreou kokkinos yuille. semantic image segmentation deep convolutional nets fully connected crfs. arxiv. chen schwing yuille urtasun. learn chen zitnick. mind’s recurrent visual representation image caption generation. cvpr donahue hendricks guadarrama rohrbach venugopalan saenko darrell. long-term recurrent convolutional networks visual recognition description. cvpr girshick. fast r-cnn. iccv goller kuchler. learning task-dependent distributed representations backpropagation structure. neural networks ieee volume tang. semantic image segmentation deep parsing network. iccv mccallum schultz singh. factorie probabilistic programming imperatively deﬁned factor graphs. nips taylor hinton roweis. modeling human motion using binary latent variables. nips tompson stein lecun perlin. real-time continuous pose recovery human hands using convolutional networks.", "year": 2015}