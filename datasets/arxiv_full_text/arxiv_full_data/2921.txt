{"title": "Conditional generation of multi-modal data using constrained embedding  space mapping", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "We present a conditional generative model that maps low-dimensional embeddings of multiple modalities of data to a common latent space hence extracting semantic relationships between them. The embedding specific to a modality is first extracted and subsequently a constrained optimization procedure is performed to project the two embedding spaces to a common manifold. The individual embeddings are generated back from this common latent space. However, in order to enable independent conditional inference for separately extracting the corresponding embeddings from the common latent space representation, we deploy a proxy variable trick - wherein, the single shared latent space is replaced by the respective separate latent spaces of each modality. We design an objective function, such that, during training we can force these separate spaces to lie close to each other, by minimizing the distance between their probability distribution functions. Experimental results demonstrate that the learned joint model can generalize to learning concepts of double MNIST digits with additional attributes of colors,from both textual and speech input.", "text": "present conditional generative model maps low-dimensional embeddings multiple modalities data common latent space hence extracting semantic relationships them. embedding speciﬁc modality ﬁrst extracted subsequently constrained optimization procedure performed project embedding spaces common manifold. individual embeddings generated back common latent space. however order enable independent conditional inference separately extracting corresponding embeddings common latent space representation deploy proxy variable trick wherein single shared latent space replaced respective separate latent spaces modality. design objective function that training force separate spaces close other minimizing distance probability distribution functions. experimental results demonstrate learned joint model generalize learning concepts double mnist digits additional attributes colorsfrom textual speech input. humans capable using information multiple sources modalities learn concepts relate together. previous research also shown perceiving relating multiple modalities input data component efﬁcient learning. multi-modal data here refers information multiple sources. multi-modal data cognition every-day life. however artiﬁcial learning systems relating modality independent concepts still remains primary reason difﬁculty model relationships machines since generalize well associate concepts unseen objects training set. failure generalize novel concepts attributed main reasons humans constantly learning multi-modal data since birth results much better model learning mapping modalities compared machine learning systems typically access small ﬁnite subset sample data. machine learning algorithms effective learning meaningful concepts generative manner training examples. thus multi-modal learning area active research. paper speciﬁcally target cross-modality concept learning case text speech images depicted ﬁgure recent work achieved success direction i.e. jointly learn generative models capable generating modality another. instance ngiam proposed deep learning framework using restricted boltzmann machines deep belief networks learn efﬁcient features audio video modalities. illustrated multi-modal learning results better performance compared unimodal case. case learning generate images text modality recent work mansimov show using attention-based models generating images text captions results higher quality samples. furthermore claimed leads better generalization towards previously novel captions. works reed proposed deep convolutional generative adversarial networks combined natural language image embeddings order produce compelling synthetically generated images. recently also work ﬁeld cross-domain feature learning images. coupled generative models generates pairs images different domains sharing weights higher level feature extracting layers. similarly methods like disco-gan conditional vaes learn transfer style rise challenge problem jointly learning distribution multiple modalities data using learned generative models low-dimensional embeddings high dimensional natural data. approach consists ﬁrst projecting high dimensional data lowdimensional manifold latent space separately learn generative models embedding space. order together additional constraint learning objective make latent representations generative model close possible. inference time latent representations generative model used proxy allowing combined conditional generative model multi-modal data. using text speech image modalities show proposed method successfully learn generate images modiﬁed mnist datasets text captions speech snippets seen training. figure illustration proposed method learns mapping embedding spaces reducing distance latent representations also ensuring inverse functions learns generate embeddings back reducing reconstruction errors problem statement section formulate problem mapping between multiple modalities data learn semantic concepts. consider random variables representing instances input data modalities. given data ideally would want create parametric joint distribution model optimal parameters perform conditional inference order data modalities i.e. proposed method learning constrained embedding mapping original high dimensional data points ﬁrst mapped semantically meaningful low-dimensional manifold deterministic stochastic functions dimensions original high dimensional space dimensional embedding. embeddings represented respectively. also assume original high-dimensional data recovered embeddings generative models. that explicitly learn mapping between deterministic manner ensuring mapping reconstruct decode original embeddings common latent space. take inspiration authors proposed similar mapping transfer learned skills robots extend learn meaningful mappings among embedding spaces. instead modeling original high dimensional joint distribution model joint distribution embedding space learning common latent variable embeddings pθpdl. however shared latent representation modalities data would required predict latent space inference resolve problem employ proxy-variable trick that separately learn generative models low-dimensional embeddings using auto-encoder architecture parameterized network parameters respectively. introduce additional constraint minimizes distance latent representations auto-encoder structure. such within framework reducing original high dimensional data low-dimensional embedding subspaces ensure important purpose since embedding spaces semantically meaningful intuition mapping subspaces lead better generalization novel data points using original high dimensional data. learning mapping embedding space allows exploit generative capabilities separate models trained plethora unlabeled data. furthermore since simple autoencoder model reduction data dimension leads better faster convergence shallower networks. using proposed conditional generative model generate images corresponding text speech representations. example text seven also corresponding speech representation independently learn generate images modalities. generating embedding space images evaluate method using multi-layered perceptron based variational auto-encoders convolutional networks based variational auto-encoders based auto-encoders convolutional auto-encoders generating word embeddings wordvec proposed trained wikipedia word corpus. speech signals mel-frequency cepstral coefﬁcients features speech embeddings. generating back text speech embeddings nearest neighbor retrieve closest data point query. image embedding case normalization function forward function a−µx corresponding unnormalization function inverse function mean standard deviation image embedding training set. word speech embeddings represent non-linear mapping functions neural networks. speciﬁcally implementation simple fully connected networks containing hidden layer encoder decoder units. loss used components objective function. adam optimizer used default parameters minimizing objective function equation since generated images different attributes images test dataset difﬁcult directly compare arbitrary instances test set. thus ﬁrst image test closest peak signal noise ratio generated image compared image given respective embeddings generated back. additionally latent representations constrained equal minimizing distance facilitates multi-modal mapping inference step. explain section combining three loss functions optimal parameters auto-encoders learned minimizing following objective function ﬁgure provide outline training process. networks trained rearrange individual components order create generative mapping model discussed following sub-section. table quantitative comparison proposed method using different generative models compared baseline directly predicting images text speech embeddings convolutional network. baseline method direct mapping word embedding space image pixel space using convolutional neural networks similar discriminator network dcgan architecture. method directly regressing image word embeddings involve generation latent variables thus learns mean image representations word embedding demonstrated later section. experimental evaluation performed generate mnist images textual speech embeddings. four cases dimensional image embedding images corresponding word embeddings dimension word. speech embeddings dimension used digits audio double digit numbers created concatenating mnist digit images horizontally. training image auto-encoder randomly remove two-digit combinations train remaining images. thus autoencoder explicitly never trained generate images. word speech embeddings simply concatenate respective embeddings digit. learning mapping image word speech embeddings hide mapping unseen images mentioned learn mapping remaining images-word combination. testing give word embeddings unseen -digit numbers generate corresponding images. figure shows generated double digit images text combinations unseen training. mapping using convolution vae) image auto-encoder produces clear images word embeddings. produces images comparatively blurry compared conv-vae case. direct regression) network learns generate mean images digits training dataset seen digits like ﬁve. noted direct regression learns associate word embeddings blurry mean images encountered training proposed method ﬁnds mappings semantically meaningful image embedding space subsequently generates clearer images hence producing higher quality images. similar results colored double mnist data-set ﬁgure convolutional produces better generalization novel color digit combinations compared baseline direct regression method. similar results seen image generation speech ﬁgure show supplementary materials. quantitatively evaluate performance algorithm take mean psnr value double digcolored-double digit combination computed equation table shows psnr values experiments using different image encoder-decoder methods direct regression text embeddings speech features. psnr values show convolutional vaes produce highest psnr value experiments text embeddings double mnist speech features whereas convolutional auto-encoder produces best psnr values image generation speech data colored mnist double digits. direct methods suffers psnr blurry nature digits cause high divergence closest image test set. propose multi-modal mapping model generate images even unseen captions. core proposed algorithm explicitly learn separate generative models low-dimensional embeddings multi-modal data. thereby enforcing equality constraint beradford alec metz luke chintala soumith. unsupervised representation learning deep convolutional generative adversarial networks. arxiv preprint arxiv. reed scott akata zeynep xinchen logeswaran lajanugen schiele bernt honglak. generative adversarial proceedings international conference international conference machine learning volume icml’ jmlr.org http//dl.acm.org/citation. cfm?id=.. tween latent representations data enables two-way generation information. showcase validity proposed model performing experimental evaluation generating two-digit mnist images colored digit numbers word embeddings speech features seen training. future hope extend method generating images complex captions using higher dimensional natural images. references gupta abhishek devin coline yuxuan abbeel pieter levine sergey. learning invariant feature spaces transfer skills reinforcement learning. corr abs/. http//arxiv. org/abs/.. kingma diederik mohamed shakir rezende danilo jimenez welling max. semi-supervised advances learning deep generative models. neural information processing systems section provide details network architecture ensure reproducibility paper. networks adam optimizer used default values keras neural network library. image auto-encoders binary crossentropy used reconstructions loss. mapping embeddings latent space mean squared error used loss function. convoutional variational auto-encoder encoder used convolution layers ﬁlters size max-pooling followed dense layers size dense layers dimension mean standard deviation image embedding space. decoder used dense layer size followed convolution layer ﬁlters size upsampling convolution layer size single channel output produce image back. layers non-linearity relu used except last convolution layer decoder used sigmoid non-linearity. variational auto-encoder encoder used hidden dense layers size dense layers dimension mean standard deviation image embedding space. decoder used hidden dense layers size dense layers dimension produce image back. layers non-linearity relu used except last dense layer decoder used sigmoid non-linearity. convoutional auto-encoder encoder used convolution layers ﬁlters size max-pooling followed another convolution layers ﬁlters size max-pooling followed dense layers dimension image embedding space. decoder used dense layer size followed convolution layer ﬁlters size up-sampling another convolution layer ﬁlters size up-sampling. finally convolution layer size single channel output used produce image back. layers non-linearity relu used except last convolution layer decoder used sigmoid non-linearity. auto-encoder encoder used hidden dense layers size dense layers dimension image embedding space. decoder used hidden dense layers size dense layers dimension produce image back. layers non-linearity relu used except last dense layer decoder used sigmoid non-linearity. image embedding normalization function forward function a−µx corresponding un-normalization function inverse funcx mean tion standard deviation image embedding training set. words force shared latent space normalized image embedding space implementation. word embeddings represent non-linear mapping functions neural networks. encoder word-embedding common latent space contains hidden dense layers size dense layers dimension common latent space. mnist digit images horizontally. -digit combination images generated. during training image auto-encoder randomly remove sixteen two-digit combinations train remaining images. thus image embeddings explicitly never trained create images. text speech embeddings simply concatenate embeddings digit. learning mapping image text speech embeddings hide mapping between images learn mapping images- combination. testing give text speech embeddings sixteen -digit numbers generate corresponding images. colored double mnist digits experiment similar experiment addition color attribute increase complexity mapping. colors green blue only. -digit combination images generated randomly juxtaposing images digits random color. training image auto-encoder randomly remove sixteen colored two-digit combinations train remaining images. text speech embeddings simply concatenate embeddings word digit learning mapping image text speech embeddings hide mapping images learn mapping images. testing give text speech embeddings sixteen two-digit numbers color combinations generate corresponding images. present additional results image generation text embeddings ﬁgure using various types autoencoders image generators. qualitative inspection reveals convolutional produces clearest best-looking results accordance psnr values table image generation speech embeddings please refer ﬁgure qualitative inspection reveals proposed method produce clear good-looking results compared direct regression baseline accordance psnr values table experimental evaluation performed mnist dataset four cases speech text data. noted that speech text data embeddings ﬁxed class. thus class double digits mnist case colored double digit mnist singleton speech signal text embedding. double mnist digits experiment attempt learn concept double digits testing algorithm generate novel images double digit combinations text embeddings. double digit numbers created concatenat-", "year": 2017}