{"title": "Continual Learning in Generative Adversarial Nets", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Developments in deep generative models have allowed for tractable learning of high-dimensional data distributions. While the employed learning procedures typically assume that training data is drawn i.i.d. from the distribution of interest, it may be desirable to model distinct distributions which are observed sequentially, such as when different classes are encountered over time. Although conditional variations of deep generative models permit multiple distributions to be modeled by a single network in a disentangled fashion, they are susceptible to catastrophic forgetting when the distributions are encountered sequentially. In this paper, we adapt recent work in reducing catastrophic forgetting to the task of training generative adversarial networks on a sequence of distinct distributions, enabling continual generative modeling.", "text": "developments deep generative models allowed tractable learning high-dimensional data distributions. employed learning procedures typically assume training data drawn i.i.d. distribution interest desirable model distinct distributions observed sequentially different classes encountered time. although conditional variations deep generative models permit multiple distributions modeled single network disentangled fashion susceptible catastrophic forgetting distributions encountered sequentially. paper adapt recent work reducing catastrophic forgetting task training generative adversarial networks sequence distinct distributions enabling continual generative modeling. deep generative models gained widespread tractable model high-dimensional data distributions. recently introduced frameworks generative adversarial networks variational autoencoders noise distribution data space producing realistic samples. capturing high-dimensional data distributions makes possible variety downstream tasks e.g. semi-supervised learning sampling reconstruction/denoising. models also amenable conditional training conditional input guides sampling process test time manually selected conditional inputs lead samples corresponding conditional distribution. assuming distributions share underlying structure vastly reduces capacity required model distributions compared modeling conditional distribution separate network. however standard training regime deep generative models assumes training data drawn i.i.d. distribution interest. example setting dataset contains multiple classes data representing every class used concurrently training. many real-world scenarios data actually arrive sequentially available short time period. notably number potential applications deep generative models encounter exactly constraint. reason widespread interest unsupervised learning vastly unlabeled data available labeled data. many cases agent exploring environment learning visual representation camera feed training generative model text twitter ﬁrehose unlikely practical save data lifetime system. online streaming approach preferable. another area gained recent interest exhibits constraint private learning learning differentially private deep classiﬁer many private datasets currently requires many rounds communication queries models trained locally dataset private learning deep generative models still open research problem differentially private deep generative model would allow generation synthetic data good feature representations downstream tasks without need repeated communication. given model would desirable efﬁciently update private data sources sequence without access previous data sources again. wish update trained generative model capture newly observed distribution naively training model solely data result previously learned distributions forgotten general susceptibility neural networks catastrophic forgetting trained multiple tasks sequentially instead standard training regime requires train data simultaneously. approach scalable requires previously observed data stored synthetic data representative previous observations regenerated round training. work propose scalable approach training generative adversarial nets prominent deep generative model multiple datasets sequentially training data previous dataset assumed inaccessible. leverage recent work discriminative models synaptic plasticity reduced network parameters determined critical previously encountered tasks experimental evaluation approach demonstrates gans extended settings observed data distribution changes time enabling continual learning novel generative tasks. generative modeling wish learn distribution pdata e.g. distribution natural images. deep generative model pdata approximated neural network. generative models structured allow explicit estimation likelihood directly approximating pdata function models include variational autoencoders variants. generative models allow simulation data sampling trainable onedirectional mapping ﬁxed prior latent variables generator’s output distribution case induced density pgpz models notably include generative adversarial nets work present method continual learning gans believe similar routes taken generative modeling frameworks. standard framework pits generator discriminator given random noise input drawn prior deterministically generates sample generator’s goal fool discriminator discriminator cannot determine generated samples come current learned distribution pdata true data distribution. trained according following two-player minimax objective conditional framework quite ﬂexible terms input enters generator discriminator networks. example range discrete one-hot vector representing class label partially complete sample case image inpainting. furthermore simply included input ﬁrst layers input every layer. considered architectural hyperparameter certain ways incorporating effective others depending speciﬁc problem domain. neural network trained multiple tasks sequentially process training later tasks tends decrease performance earlier tasks. phenomenon known catastrophic forgetting ﬁrst explored classic experiment work consisted ﬁrst training multilayer perceptron backpropagation ones addition tasks learned correctly compute sums. then network trained twos addition tasks without access training data loss function ones addition. network learned perform twos addition forgot performs ones addition. phenomenon observable across diverse domains neural networks widely used including computer vision speech recognition natural language processing. network weights adjust without restriction meet objective current task previous task performance severely disrupted. knowledge existing literature preventing catastrophic forgetting neural networks focused discriminative/classiﬁcation settings. approaches directly preserve functional input/output mapping network previous tasks. example task training data arrives softmax activations previous task output layers recorded training examples. then similar process knowledge distillation softmax activations serve targets output layers training task. approaches make dramatic architectural changes reduce interference different tasks. example distinct columns added network task freezing parameters previous columns. lateral connections allow sharing features tasks. forgetting prevented number parameters grows quadratically number tasks. work assume setting even linear growth number parameters undesirable. kirkpatrick recently proposed approach dubbed elastic weight consolidation computationally inexpensive require adding capacity network. parameters found critical performance achieved previous tasks protected additional loss term restricting movement. given network parameterized trained task training data posterior distribution parameters approximated gaussian distribution mean variance given inverse diagonal fisher information matrix thus parameters associated higher fisher diagonal elements considered certain thus less ﬂexible. network begins training task without access task training data loss function task augmented standard loss task diagonal element fisher information matrix hyperparameter denoting relative importance tasks. experimental results permuted mnist tasks atari games validated approach classiﬁcation tasks supervised reinforcement learning settings. similar method proposed rather compute weight saliency task’s training session online measure takes account weight’s total contribution global change loss training trajectory. case weights higher contributions critical. moving subsequent tasks quadratic penalty used similar approaches connections second derivative objective function respect network parameters combination parameter magnitude used estimate saliency. saliency parameters pruned leading reduced network size. rather completely removing saliency parameters allow parameters greater ﬂexibility learning task. assume generator trained simulate distribution interest. observed data distribution changes wish additionally model distribution without interfering old. could certainly train entirely similar distributions approach prevents leveraging shared underlying structure. addition storing separate models task scalable limited capacity settings. might simply continue training model according standard objective using data distribution. network trains according setup however quickly undergoes catastrophic forgetting without exposed real examples previously encountered data learns determine inputs real examples sampled distribution only. therefore resources directed towards fooling solely task leading inability sample previously learned distributions. simple expensive solution forgetting problem regenerate training previously encountered datasets route unavailable discriminate learning setting. assuming trained well samples generates realistic enough serve real inputs standard training proceed simultaneously data old. route reduces extent catastrophic forgetting efﬁcient. observed distribution must regenerate training sets previous distributions. addition must retrain regenerated data addition newly observed data. learning sequence observed distributions would time complexity instead approach ultimately take requiring regeneration training data. propose replace portion standard minimax objective augmented loss function discouraging weights critical accurately modeling previously observed distribution undergoing drastic changes value. however unconditional framework constraint would attempt preserve mapping single noise prior previous data distribution objective would encourage mapping noise prior data distribution. observed able overcome arbitrarily large ﬁxed constraints ultimately resulting forgetting previously learned distribution. conditional framework distinct observed data distributions associated distinct conditional inputs maintaining well-deﬁned desired mapping joint distribution conditional input data space. settings pdata change time does datasets corresponding class observed sequentially directly apply constraints conditional gan. settings conditional inputs available observed distribution changes arbitrarily time artiﬁcially assign distinct conditional input distributions observed different times. portion standard minimax objective conditional gans relevant determining parameter updates practice follow standard approach using generator’s cost function instead provide better gradients early training. trained well initial conditional inputs leading optimal parameters task want subsequent task training sessions signiﬁcantly damage performance achieved. intuitively distribution greatly altered task estimating fisher information respect parameters allows determine critical fooling correspondingly penalize movement parameters. constraining movement parameters high fisher information previous tasks introduced discriminative models elastic weight consolidation deﬁne penalty weighting parameter parameters that perturbed would high effect discriminator’s output high value corresponding element diagonal fisher information matrix considered especially salient. note actually corresponds approximation known empirical fisher information efﬁcient compute objective augmented explores method similar constraining movement salient parameters. unlike computation parameter importance performed online require between-task phase estimating fisher information. approach could alternatively used here. evaluate approach class-conditional gans setting image classes observed time. here classes digit categories mnist svhn datasets conditional input one-hot encoding class label. unlike typical concurrent training regime class-conditional continual learning setup distinct conditional distributions encountered sequentially previously observed data unavailable. proposed framework assumes generator network used model distribution require discriminator used. here experiment extreme setting where similar previously observed data previously trained discriminators assumed inaccessible. architecturally setup either interpreted elements appended training session corresponding initialization connections element simply always zero corresponding class part current task. adam optimization algorithm re-initialize optimizer parameters task. thus updates occur weights zero gradient making implementations equivalent. mnist test approach simple multilayer perceptron svhn deep convolutional mnist ﬁrst employ simple architecture two-layer mlps. hidden layers networks size relu activation. output layer size sigmoid activation corresponding generated image. single output sigmoid activation corresponding estimated probability input example drawn pdata. figure samples produced generator ﬁrst trained concurrently digits digit encountered next training session generator’s objective augmented previous distributions forgotten. contrast catastrophic forgetting evident figure generator trains digit standard figure conditionally sampled images digit digit training standard objective sampled images digits training conditional input digit using standard conditional objective generator forgets sample previously learned distributions. figure conditionally sampled images digit digit training standard objective sampled images digits training conditional input digit using ewc-augmented objective generator longer forgets produce samples previous categories. objective. figure samples generator ﬁrst trained concurrently digits trained sequentially digits augmented objective. every displayed image sampled ﬁnal training session clear previously learned distributions forgotten. catastrophic forgetting signiﬁcantly reduced using augmented objective small amount peripheral noise evident generated samples pdata. learns distinguish real examples recently generated samples. images neither realistic similar current generated distribution meaningfully classiﬁed evident visualizing saliency output layer parameters critical parameters fooling corresponding structure digit. matures enough training peripheral noise longer produced need devote resources detecting noise pixels. limitation could potentially addressed training experience replay adding noise proportion fake images trained practice higher capacity networks less susceptible limitation. figure shows deformation image sampled previously learned distribution ﬁxed ﬁrst epoch training class without ewc-augmented objective. training digit standard objective forgets produce visually coherent images digit digit image transitions skinny training progresses. deformation apparent augmented objective used. figure visualization digit-speciﬁc fisher information. conditional trained digits concurrently pixel-wise mean fisher information output computed conditional input. figure images sampled generator training different classes sequentially. digits ﬁrst trained concurrently remaining digit classes encountered time sequentially. samples drawn last training session svhn conditional dcgan architecture three convolutional layers fully connected layer append input trainable linear transformation added output ﬁrst convolutional layer. figure shows outputs dcgan model trained digits zero four concurrently model continuing training digits nine model continuing training augmented loss digits nine. forgetting clearly present visual quality digits zero four similar quality digits visual quality digits nine similar quality digits mnist svhn dcgan observed results applying largely invariant magnitude reported results mnist used reported results svhn used svhn also trained models observed little difference visual quality difference diversity ﬁnal results. argument could made principled parameter interpreting taylor expansion previous task’s loss. however small values generator parameters tended rapidly deviate previous task values beginning figure sampling ﬁxed training conditional input. images digit sampled ﬁxed training digit standard objective ewc-augmented objective catastrophic forgetting visible standard objective. euclidean distance current sampled image original image shown left training objectives. figure overcoming forgetting svhn dataset. column represents ﬁxed represents ﬁxed conditionally sampled images generator trained digits images sampled generator continuing train digits standard objective. images sampled generator training ewc-augmented objective digits training task resulting poor quality samples gradually pulled back visual quality increasing course training. high values observed loss visual ﬁdelity beginning training task rapid convergence model produced good samples classes. introduced approach continual learning generative adversarial nets. experimental results demonstrate sequential training different sets conditional inputs utilizing ewcaugmented loss counteracts catastrophic forgetting previously learned distributions. approach general applicable setting observed distribution conditional inputs changes time conditional input representing time data capture appended data. promise deep generative modeling ability learn model world vast sources data; fully realize potential necessary learn data point capture rather storing future use. work step towards goal. references martin abadi andy goodfellow brendan mcmahan ilya mironov kunal talwar zhang. deep learning differential privacy. proceedings sigsac conference computer communications security goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems james kirkpatrick razvan pascanu neil rabinowitz joel veness guillaume desjardins andrei rusu kieran milan john quan tiago ramalho agnieszka grabska-barwinska demis hassabis claudia clopath dharshan kumaran raia hadsell. overcoming catastrophic forgetting neural networks. corr abs/. brendan mcmahan eider moore daniel ramage seth hampson blaise aguera arcas. communication-efﬁcient learning deep networks decentralized data. proceedings international conference artiﬁcial intelligence statistics yuval netzer wang adam coates alessandro bissacco andrew reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning andrei rusu neil rabinowitz guillaume desjardins hubert soyer james kirkpatrick koray kavukcuoglu razvan pascanu raia hadsell. progressive neural networks. corr abs/. kihyuk sohn honglak xinchen yan. learning structured output representation using deep conditional generative models. advances neural information processing systems", "year": 2017}