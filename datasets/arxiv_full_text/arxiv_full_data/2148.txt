{"title": "Dynamic Collaborative Filtering with Compound Poisson Factorization", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Model-based collaborative filtering analyzes user-item interactions to infer latent factors that represent user preferences and item characteristics in order to predict future interactions. Most collaborative filtering algorithms assume that these latent factors are static, although it has been shown that user preferences and item perceptions drift over time. In this paper, we propose a conjugate and numerically stable dynamic matrix factorization (DCPF) based on compound Poisson matrix factorization that models the smoothly drifting latent factors using Gamma-Markov chains. We propose a numerically stable Gamma chain construction, and then present a stochastic variational inference approach to estimate the parameters of our model. We apply our model to time-stamped ratings data sets: Netflix, Yelp, and Last.fm, where DCPF achieves a higher predictive accuracy than state-of-the-art static and dynamic factorization models.", "text": "model-based collaborative ﬁltering analyzes user-item interactions infer latent factors represent user preferences item characteristics order predict future interactions. collaborative ﬁltering algorithms assume latent factors static although shown user preferences item perceptions drift time. paper propose conjugate numerically stable dynamic matrix factorization based compound poisson matrix factorization models smoothly drifting latent factors using gamma-markov chains. propose numerically stable gamma chain construction present stochastic variational inference approach estimate parameters model. apply model time-stamped ratings data sets netﬂix yelp last.fm dcpf achieves higher predictive accuracy state-of-the-art static dynamic factorization models. collaborative ﬁltering popular framework recommender systems personalize item recommendations customer based previous item interactions; approaches used large online platforms user decision-making inﬂuenced item recommendations including amazon spotify netﬂix latent factor models recently become research focus within models represent user preferences item attributes terms latent factors exploit low-dimensional latent structure impute missing user–item interactions context poisson factorization models demonstrated better accuracy predicting held-out user–item ratings models probabilistic matrix factorization gaussian factorization model. poisson factorization models generally assume user preferences item attributes static time however real-world scenarios user interests item attributes drift time items’ popularity reception continuously evolve items categories emerge global environments change. customers’ preferences needs change time leading interact items time-varying way. however static models mistakenly recommend sports documentaries current horror movies suggest rock songs electronic music. similarly might recognize statistical physics paper relevant machine learning audience. thus static assumption traditional models well-aligned true generative model ratings real-world scenarios. temporal dynamics explored context collaborative ﬁltering frameworks. early approaches underweighted older user–item interactions timesvd++ proposed extension singular value decomposition augmenting matrix factorization user–item–time bias term user-evolving factors ignored drifting nature item attributes nevertheless timesvd++ winning algorithms netﬂix prize motivating need work temporal dynamics models. bayesian poisson tensor factorization proposes independent latent factor time factorizes user–item–time tensor approach models user item factors time index independently previous ones makes evaluating trends speciﬁc user item factors impossible. several dynamic extensions also proposed leveraging gaussian state space model represent evolving user item latent factors however conjugate gamma-poisson structure poisson factorization models ensures computationally tractable approaches high-dimensional data using variational inference additionally long tailed gamma priors powerfully capture overdispersed user item behavior sparse matrices leads better predictive results—in terms precision recall—than latent dirichlet allocation among approaches shown gopalan paper address problem evolving user item factors within context poisson factorization. propose novel dynamic model dynamic compound-poisson factorization dcpf novel dynamic probabilistic model represents user item latent factors independent smoothly-evolving gamma-markov chains. recent dynamic extension attempt replacing gamma priors gaussian state space model however approach compromises conjugacy model leading computational intractability alternatively inaccurate numerical approximations posterior inference. fact lack conjugacy prevents simple closed-form updates consequently prevents convenient extensions model. furthermore gaussian priors time step fail capture empirical response distribution long-tailed gamma distributions demonstrated gopalan however conjugacy cannot attained simple gamma chains poisson factorization model. therefore introduce auxiliary gamma chains chain user item factors guarantee conjugacy user item chains within model. also introduce compound poisson generating distribution poisson compounding allows rescale non-negative multiplicative gamma chains would otherwise grow uncontrollably thus lead numerical instability inference. also provides ﬂexibility ratings data types. section propose conjugate gamma chain construction dampened positively correlated states section describe generative model dcpf stochastic variational inference approach allows inference dcpf scale large sparse data sets. section present experimental setup including data sets static dynamic baselines comparison metrics hyperparameter initialization choices discuss results. conclude potential improvements. approach incorporating smooth temporal dynamics poisson factorization preserved gamma-poisson conjugate structure placing gamma-markov chains priors latent factors associated user item. conjugacy guarantees closed form updates infer model scalable fashion combine conjugate models future. additionally long tail gamma priors able capture diverse sets users items terms activity popularity allows better represent data users might proactive reviewers majority rarely contribute example. assume latent factors evolve smoothly time remain non-negative requires gamma chain states positively correlated therefore constructed gamma chain conditioning state every time step inverse state previous time step gamma scale parameters. different acharya approach chaining shape parameter consideration temporal smoothness used chains substantially different problem factor analysis. simple intuitive construction chain would draw component user latent factor time follows umkt here gamma distribution ﬁxed shape parameter rate parameter umkt−. however construction non-conjugate full conditional distribution latent factor value includes terms conjugate density distribution family however easily check gamma density cannot expressed terms. therefore auxiliary variables zmkt chains preserving positive correlation umkt umkt− ﬁrst suggested cemgil dikmen chain constructed follows component user latent factor construction differs prior work parametrization gamma chains. separate parameter transition chaining parameters contrast cemgil dikmen decoupling parameters allows capture changes user preferences time independently initial state values. allows model static interests temporal corrections. however contribution limited change gamma construction also incorporate chains model ensure stability addressed previous works despite obvious problem poses inference task. first note fact correlation thus always positive whereas skewness depends ratio ω·\u0001. ratio probability mass shifted towards region chain exhibit systematic positive drift thus increases increase monotonically depending ratio raised power time index. ratio signﬁcantly larger smaller chains grow large small experienced ﬁrst attempting simple poisson factorization. ensure numerical stability model inference routine leverage update compound poisson hyper-parameters consequently model rescales contribution latent factors towards generating distributions ensure chains don’t grow uncontrollably time. time step represent user vector latent preferences item vector latent attributes. auxiliary gamma variables chains respectively. variables parametrize initial states user item chains. contributions usage compound-poisson generating distribution instead introducing parameter solely rescale gamma chains independently generating distribution opted compound poisson already includes parameter leverage purpose addition providing ﬂexibility type observation data handle. compound poisson distribution deﬁned i.i.d. random variables element distribution here i.i.d. variables drawn exponential dispersion models poisson-distributed random variable mean virtue additivity property distributions compound poisson variable distribution also distribution scale parameter natural parameter examples compound poisson include zero-truncated poisson gamma-poisson. accordingly generating compound poisson distribution density generative process dcpf model detailed algorithm main computational problem model posterior inference parameters model given matrix observations indexed user item time. posterior estimated dcpf generative model recommend items users based latent factors given time point using expectation score provided given matrix user ratings items across time wish compute posterior distribution user preferences umkt item attributes vnkt. many bayesian models exact posterior poisson factorization computationally intractable therefore variational inference proposes family distributions latent variables indexed variational parameters variational inference estimates variational parameters place variational distributions posterior closest true posterior kullback-liebler divergence inference problem thus becomes optimization problem scaled large sample sizes using stochastic optimization leading stochastic variational inference latent variables model user preferences item attributes corresponding auxiliary chain variables rate parameters initial states chain mean-ﬁeld variational distribution thus given mkt)qq. optimize variational parameters coordinate ascent iteratively optimizes variational parameter holding others ﬁxed previous approximations assert convergence change predictive likelihood validation set. given dcpf’s conjugacy derived closed-form updates parameters shown algorithm note learning rate parameters stochastic optimization computation update user item parameter independent users items respectively. accordingly inference routine easily parallelizable. runtime per-iteration complexity linear terms non-missing entries time step train non-missing instead full matrix gamma chain stability variational parameters depend update simply equals observation large. explained previously change time cause chain grow unstable. address issue deliberate model choices. first compounding contribution variational parameter towards observations controlled stay small reﬂects updates variational parameters interaction latent factors). second carefully initialize hyperparameters control skewness chain least ﬁrst iteration posterior update. results consider types time-stamped ratings data sets explicit ratings implicit ratings model data sets ﬁxed number time windows predict ratings held-out time windows. compare prediction accuracy versus hcpf tracks/bands last.fm dataset binary ratings indicate whether last.fm user played song given time period also consider bands items instead tracks. dataset dimensions found table split data time windows using time stamps rating. kept duration time window around months based previous works judgment average pace change ratings data set. speciﬁcally music tastes might evolving weeks songs albums come out. movie tastes often cyclical months movies move assign ratings last time windows data held-out test order quantify performance. choice number windows hold depends number ratings windows guarantees reasonable test size. randomly sample assign ratings within test validation set. hcpf recent accurate poisson factorization model hcpf dynamic model remove timestamps training test validation sets remove repeated ratings across different time periods since hcpf cannot user-item interaction every user-item pair. particular ﬁlter duplicates data keeping ﬁrst instance user-item interaction signals user’s initial interest item since dcpf performs compound poisson factorization compare different variants compound poisson generating distributions across dcpf hcpf. recent dynamic algorithm which prior work proven outperform state-of-the-art dynamic algorithms speciﬁcally bayesian probabilistic tensor factorization timesvd++ binarizes ratings default also number latent factors hcpf dcpf compromise tractability predictive accuracy. fact experimented wide range differences negligible datasets. initialize parameters compound poisson distribution based maximum likelihood estimates computed small sample non-zero ratings also initialize rate parameter initial states chains assuming contribution latent factor components user item equal. therefore initialize variance gaussian distributions suggested default values fact attempted tune initialization found signiﬁcant difference. also tried understand quality model calculate area curve variants dcpf hcpf zero-truncated poisson gamma-poisson gaussian-poisson poisson-poisson example curve seen fig. compares probability given user-item rating test exists binarizing numeric ratings represent whether item rated all. also report test likelihood hcpf dcpf variants full matrix similarly comparisons basbug engelhardt prior work computes ranking metrics typical collaborative ﬁltering models implicit data therefore compare dcpf generate items recommend user based predicted scores. user proceed compute precision also calculate normalized discounted cumulative gain table dcpf best performing algorithm terms datasets outperforming dcpf lfmbands lfmtracks. since datasets implicit compare dcpf datasets based ranking metrics. hcpf outperformed dcpf every single dataset validates dynamic hypothesis regarding user item factors. conﬁrm hypothesis compare test likelihood dcpf hcpf variant. possible variants hcpf outperform dynamic counterparts. static model outperforms dynamic model lfmsumbands. additionally signiﬁcant difference values active netﬂix variants. rare lack improvement static baseline linked imbalance terms number ratings across time windows. fact active netﬂix times fewer ratings ﬁrst window ﬁnal one. similarly lfmsumbands times fewer ratings ﬁrst window compared window second half data. contrast active yelp balanced addition active subset displays much greater improvement. expected weakness dcpf since emerging users items display inadequate chain updates. great difference initialization update sudden shift interest activity cannot perfectly captured smooth transition. additionally kind shift rarely repeated time reason dynamic approaches active\" subset data sets however positive correlation might bias future updates towards higher values match shift even though shift usually occurs ﬁrst observing user item. compare performances static dynamic variants based test likelihood. allows evaluate binary accuracy also general model relevant non-binary datasets. accordingly dcpf substantially outperforms hcpf every compound-poisson generating distribution active netﬂix active yelp data sets according table similar margin improvement seen datasets suggests dcpf effectively exploit data repeated ratings time–lastfm repeated user-item ratings across time windows. repeated structure allows accurate estimate user interests item attributes multiple opportunities estimate parameters based non-zero observations along chain given user-item pair. reported values dcpf lower lfmbands lfmtracks. expected binary datasets since samples non-missing entries specialized task binary prediction. advantage sampling whole matrix extensively discussed basbug engelhardt however binary accuracy translate well predicting rankings matter ﬁtting implicit ratings datasets. cases care modeling scale interest user item. therefore computed ndcg precision values reported table allow compare well model predicts items user test set. seen table dcpf outperforms datasets based metrics. contrast comparison fact model binarizes observations thus ignores strongly user favors item rated. additionally long-tailed gamma priors proven better capture user item factors settings gaussian priors difference clearest datasets conﬁrms advantage scaled ratings assessing much user would rate speciﬁc items. case number times user plays song scaled indicator future interest song similar songs. paper proposed novel conjugate computationally stable dynamic matrix factorization models smoothly changing latent factors time using gamma-markov chains. preserving gamma-poisson structure allows leverage long-tailed gamma prior’s better user item factors sparse matrices. conjugate chain construction guarantees straightforward scalable inference potentially enables combination conjugate structures models. also offset main computational concern non-negative multiplicative gamma chains numerical instability poisson compounding. allows provide ﬂexibility observation data control gamma chains’ growth. applied dcpf different time-stamped ratings data sets showed dcpf achieved higher predictive performance state-of-the-art static factorization model terms test likelihood. demonstrated improvement terms ranking accuracy also illustrated drawback approach relative static model presence imbalanced recommendations across time emerging inactive users items. addressed active\" subset data sets already used mitigating effects datasets. additionally approach lacks ﬂexibility classic time series techniques. smoothness assumption allows model dynamic latent factors gamma-markov chains ignores cyclical seasonal behavior prevalent shopping music listening habits. however believe conjugate stable scalable potentially modular framework open doors complicated dynamic bayesian models. taylan cemgil onur dikmen. conjugate gamma markov random ﬁelds modelling nonstationary sources. independent component analysis signal separation pages springer freddy chong chua richard oentaryo ee-peng lim. modeling temporal adoptions using dynamic matrix factorization. data mining ieee international conference pages ieee prem gopalan laurent charlin david blei. content-based recommendations poisson factorization. advances neural information processing systems pages xingquan ruijiang chengqi zhang xiangyang xindong crossdomain collaborative ﬁltering time. proceedings twenty-second international joint conference artiﬁcial intelligence-volume volume three pages aaai press ruslan salakhutdinov andriy mnih. bayesian probabilistic matrix factorization using markov chain monte carlo. proceedings international conference machine learning pages john kush varshney karthik subbian. dynamic matrix factorization state space approach. acoustics speech signal processing ieee international conference pages ieee liang xiong chen tzu-kuo huang jeff schneider jaime carbonell. temporal collaborative ﬁltering bayesian probabilistic tensor factorization. volume pages siam xiaowei martin ester hans-peter kriegel. feature weighting instance selection collaborative ﬁltering information-theoretic approach*. knowledge information systems", "year": 2016}