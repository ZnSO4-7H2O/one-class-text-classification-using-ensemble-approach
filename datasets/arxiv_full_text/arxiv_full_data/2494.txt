{"title": "On the Computational Efficiency of Training Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks.", "text": "well-known neural networks computationally hard train. hand practice modern neural networks trained efﬁciently using variety tricks include different activation functions over-speciﬁcation regularization. paper revisit computational complexity training neural networks modern perspective. provide positive negative results yield provably efﬁcient practical algorithms training certain types neural networks. signiﬁcant recent developments machine learning resurgence deep learning usually form artiﬁcial neural networks. combination algorithmic advancements well increasing computational power data size breakthrough effectiveness neural networks used obtain impressive practical performance variety domains neural network described graph vertex graph corresponds neuron edge associated weight. neuron calculates weighted outputs neurons connected passes resulting number activation function outputs resulting number. focus feed-forward neural networks neurons arranged layers output layer forms input next layer. intuitively input goes several transformations higher-level concepts derived lower-level ones. depth network number layers size network total number neurons. perspective statistical learning theory specifying neural network architecture obtain hypothesis class namely prediction rules obtained using network architecture changing weights network. learning class involves ﬁnding speciﬁc weights based training examples yields predictor good performance future examples. studying hypothesis class usually concerned three questions sample complexity many examples required learn class. expressiveness type functions expressed predictors class. training time much computation time required learn class. simplicity ﬁrst consider neural networks threshold activation function otherwise) boolean input space single output sample complexity neural networks well understood known dimension grows linearly number edges also easy matter activation function long represent weight network using constant number bits dimension bounded constant times number edges. implies empirical risk minimization ﬁnding weights small average loss training data effective learning strategy statistical point view. expressiveness networks easy neural networks depth sufﬁcient size express functions however also possible show happen size network must exponential functions express using network polynomial size? theorem shows boolean functions calculated time also expressed network depth size theorem every functions implemented turing machine using operations. exist constants every network architecture depth size threshold activation function resulting hypotesis class contains proof theorem follows directly relation time complexity programs circuit complexity fact simulate standard boolean gates using ﬁxed number neurons. statistical perspective neural networks form excellent hypothesis class; hand every runtime using depth contain predictors time hand sample complexity resulting class depends polynomially main caveat neural networks training time. existing theoretical results mostly negative showing successfully learning networks computationally hard worst case. example neural networks depth contain class intersection halfspaces reduction k-coloring shown ﬁnding weights best training np-hard shown even ﬁnding weights result close-to-minimal empirical error computationally infeasible. hardness results focus proper learning goal nearly-optimal predictor ﬁxed network architecture however goal good predictor reason limit predictors particular architecture. instead example network different architecture almost good best network architecture example powerful concept improper learning often proved useful circumventing computational hardness results. unfortunately hardness results showing even improper learning even data generated exactly small depth- neural network efﬁcient algorithms predictor performs well test data. particular shown case learning intersections halfspaces using cryptographic average case complexity assumptions. related note recently showed positive results learning data generated neural network certain architecture randomly connected weights. however assumptions used strong unlikely hold practice. despite theoretical pessimism practice modern-day neural networks trained successfully many learning problems. several tricks enable successful training changing activation function threshold activation function zero derivative almost everywhere. therefore cannot apply gradient-based methods activation function. circumvent problem consider activation functions. widely known sigmoidal activation e.g. forms smooth approximation threshold function. another recent popular activation function rectiﬁed linear unit function max{ note subtracting shifted relu relu yields approximation threshold function doubling number neurons approximate network threshold activation network relu activation. goal paper revisit re-raise question neural network’s computational efﬁciency modern perspective. challenging topic pretend give deﬁnite answers. however provide several results positive negative. although appeared literature contexts. contributions follows make simple observation sufﬁciently over-speciﬁed networks global optima ubiquitous general computationally easy ﬁnd. although holds extremely large networks overﬁt seen indication computational hardness learning decrease amount over-speciﬁcation. also demonstrated empirically sec. motivated idea changing activation function consider quadratic activation function networks quadratic activation compute polynomial functions input hence call polynomial networks. main ﬁndings networks follows networks quadratic activation expressive networks threshold activation. constant depth networks quadratic activation learned polynomial time. sigmoidal networks depth regularization approximated polynomial networks depth follows sigmoidal networks regularization learned polynomial time well. aforementioned positive results interesting theoretically lead impractical algorithms. provide practical provably correct algorithm training depth- polynomial networks. networks also learned using linearization trick algorithm efﬁcient returns networks whose size depend data dimension. algorithm follows forward greedy selection procedure step greedy selection procedure builds neuron solving eigenvalue problem. generalize algorithm depth- forward greedy step involves efﬁcient approximate solution tensor approximation problem. algorithm learn rich sub-class depth- polynomial networks. begin considering idea over-speciﬁcation make observation sufﬁciently over-speciﬁed networks optimization problem associated training generally quite easy solve global optima sense ubiquitous. interesting contrast note small networks associated optimization problem generally hard exhibit exponentially many local minima emphasize observation holds extremely large networks overﬁt reasonable scenario point possible spectrum computational cost decreases amount over-speciﬁcation. present result matrix training examples think network composed mappings. ﬁrst maps matrix number neurons whose outputs connected output layer. second mapping linear mapping maps neurons output layer. finally loss function we’ll assume convex assesses quality prediction entire data denote weights affect mapping denote function maps optimization problem associated learning network therefore minwv function generally non-convex local minima. however reasonable assume rank) large probability non-linear nature function computed neural networks. case simply solve minw computationally tractable example consider function computed ﬁrst layer sigmoid assumed convex. since full rank solution problem corresponds global optima hence global optima original optimization problem. thus sufﬁciently large networks ﬁnding global optima generally easy sense ubiquitous. review several known hardness results apply learning setting. simplicity throughout section focus model binary classiﬁcation case boolean cube realizable case ﬁxed target accuracy. every dimension input space hypothesis class functions {±}. often omit subscript clear context. learning algorithm access oracle samples according unknown distribution returns unknown target hypothesis objective algorithm return classiﬁer probability least efﬁcient runs time poly function returns also evaluated instance time poly. efﬁciently learnable. context neural networks every network architecture deﬁnes hypothesis class ntnσ contains target functions implemented using neural network layers neurons activation function immediate question ntnσ efﬁciently learnable. ﬁrst address question threshold activation function otherwise. observing depth- networks threshold activation function implement intersections halfspaces rely following hardness results theorem {±}d different complexity assumption showed similar result even mentioned before neural networks depth activation function express intersections halfspaces example ﬁrst layer consists neurons computing halfspaces second layer computes conjunction mapping trivially class efﬁciently learnable class containing also efﬁciently learnable. thus obtain following corollary corollary every class ntnσ efﬁciently learnable happens change activation function? particular widely used activation functions neural networks sigmoidal activation function σsig rectiﬁed linear unit activation function σrelu max{z ﬁrst observation note σsig data domain discrete boolean cube hence allow weights network arbitrarily large ntnσ ntnσsig. similarly function σrelu−σrelu equals every result without restricting weights simulate threshold activated neuron relu activated neurons implies ntnσ ntnσrelu. hence corollary applies sigmoidal networks relu networks well long regularize weights network. while focus realizable case ﬁxed accuracy conﬁdence since dealing hardness results results trivially apply agnostic case learning arbitrarily small accuracy conﬁdence parameters. happens regularize weights? ntnσl target functions implemented using neural network depth size activation function restrict input weights neuron argue many real world distributions difference classes ntnσl ntnσ small. roughly speaking distribution density around decision boundary neurons sigmoidal neurons able effectively simulate threshold activated neurons. practice sigmoid relu activation functions advantageous threshold activation function since trained using gradient based methods. empirical successes turned formal guarantees? unfortunately closer examination thm. demonstrates learning nnσsigl nnσrelul still hard. formally apply networks binary classiﬁcation follow standard deﬁnition learning margin assumption assume learner receives examples form real-valued function comes hypothesis class assume |f∗| even margin assumption following corollary every classes ntnσsigl ntnσrelul efﬁciently learnable proof provided appendix. happens much smaller? later paper show positive results constant depth ﬁxed. results obtained using polynomial networks study next section. previous section shown several strong negative results learning neural networks threshold sigmoidal relu activation functions. circumvent hardness results considering another activation function. maybe simplest non-linear function squared function call networks activation function polynomial networks since compute polynomial functions inputs. previous section denote ntnσl class functions implemented using neural network depth size squared activation function bound norm input weights neuron. whenever specify refer polynomial networks unbounded weights. study expressiveness computational complexity polynomial networks. note algorithms efﬁciently learning sparse low-degree polynomials studied several previous works however rely strong distributional assumptions data instances uniform log-concave distribution interested distribution-free setting. ﬁrst show that similarly networks threshold activation polynomial networks polynomial size express functions implemented efﬁciently using turing machine. theorem thm. exist constants every class ntnσl log) contains another relevant expressiveness result later shows polynomial networks approximate networks sigmoidal activation functions theorem ˜ox∞< proof relies approximation sigmoid function based chebyshev polynomials done given appendix. turn computational complexity learning polynomial networks. ﬁrst show hard learn polynomial networks depth indeed combining thm. corollary obtain following corollary class ntnσ efﬁciently learnable. side constant-depth polynomial networks learned polynomial time using simple linearization trick. speciﬁcally class polynomial networks constant depth contained class multivariate polynomials total degree class represented ds-dimensional linear space vector coefﬁcient vector polynomial. therefore class polynomial networks depth learned time mapping instance vector monomials learning linear poly. particular constant therefore polynomial networks constant depth efﬁciently learnable. another learn class using support vector machines polynomial kernels. interesting application observation depth- sigmoidal networks efﬁciently learnable sufﬁcient regularization formalized result below. contrasts corollary provides hardness result without regularization. theorem class nnσsigl learned accuracy time poly idea proof follows. suppose obtain data nnσsigl. based thm. nbtnbnσ approximates ﬁxed accuracy deﬁned thm. learn nbtnbnσ considering class polynomials total degree applying linearization technique discussed above. since assumed separate data margin )|f∗| separates data margin enough establishing accuracy sample time depends polynomially interesting theoretically results practical since time sample complexity grow fast depth network. section describe practical provably correct algorithms special case depth- depth- polynomial networks additional constraints. although networks learned polynomial time explicit linearization runtime resulting network size scales quadratically cubically data dimension contrast algorithms guarantees much milder dependence networks corresponds hidden layer containing neurons squared activation function restrict input weights neurons network bounded norm also allow direct linear dependency input layer output layer. uses polynomial kernels time sample complexity small margin assumptions feature space corresponding given kernel. note however large margin space different assumption make here namely network small number hidden neurons works well data. loss function. we’ll assume β-smooth convex. basic idea algorithm gradually hidden neurons hidden layer greedy manner decrease loss function data. deﬁne functions implemented hidden neurons. every afﬁne function plus weighted functions algorithm starts minimizer afﬁne functions. greedy step search minimizes ﬁrst order approximation vector hidden neuron network. finally minimize w.r.t. weights hidden layer output layer following theorem follows directly theorem provides convergence guarantee geco. observe theorem gives guarantee learning allow output over-speciﬁed network. theorem assume loss function convex β-smooth. iterations outputs network geco algorithm minf∗∈pk next consider hypothesis class consisting third degree polynomials subset -layer polynomial networks hidden neurons functions class hypothesis class consider basic idea algorithm -layer networks. however -layer case could implement efﬁciently greedy step solving eigenvalue problem face following tensor approximation problem greedy step general hard optimization problem approximate luckily approximate greedy step sufﬁces success greedy procedure. procedure given figure based approximate eigenvector computation. guarantee quality approximation given appendix leads following theorem whose proof given appendix. theorem assume loss function convex β-smooth. geco algorithm iterations iteration relies approximation procedure given fig. probability outputs network minf∗∈pk also possible approximate solution eigenvalue problem still retain performance guarantees since approximate eigenvalue found time using power method obtain runtime geco depends linearly demonstrate practicality geco train neural networks real world problems considered pedestrian detection problem follows. collected training examples image patches size pixels containing either pedestrians hard negative examples examples images above. used half examples training half test set. calculated features images. trained using geco depth- polynomial network resulting features. used neurons hidden layer. comparison trained network architecture sgd. also trained similar network relu activation function. implementation tried following tricks speed convergence heuristics initialization weights learning rate rules mini-batches nesterov’s momentum dropout. test errors function number iterations depicted plot figure side. also mark performance geco straight line seen error geco slightly better sgd. also noted perform large number iterations obtain good solution runtime geco much faster. indicates geco valid alternative approach training depth- networks. also apparent squared activation function slightly better relu function task. second plot side ﬁgure demonstrates beneﬁt over-speciﬁcation sgd. generated random examples passed random depth- network contains hidden neurons relu activation function. tried network data over-speciﬁcation factors clearly seen converges much faster over-specify network. acknowledgements research supported intel also supported grant marie-curie career integration grant. also supported center knowledge recipient google europe fellowship learning theory research supported part google fellowship. thank itay safran spotting mistake previous version sec. james martens helpful discussions.", "year": 2014}