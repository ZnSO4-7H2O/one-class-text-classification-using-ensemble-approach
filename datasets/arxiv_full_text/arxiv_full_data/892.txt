{"title": "Unsure When to Stop? Ask Your Semantic Neighbors", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "In iterative supervised learning algorithms it is common to reach a point in the search where no further induction seems to be possible with the available data. If the search is continued beyond this point, the risk of overfitting increases significantly. Following the recent developments in inductive semantic stochastic methods, this paper studies the feasibility of using information gathered from the semantic neighborhood to decide when to stop the search. Two semantic stopping criteria are proposed and experimentally assessed in Geometric Semantic Genetic Programming (GSGP) and in the Semantic Learning Machine (SLM) algorithm (the equivalent algorithm for neural networks). The experiments are performed on real-world high-dimensional regression datasets. The results show that the proposed semantic stopping criteria are able to detect stopping points that result in a competitive generalization for both GSGP and SLM. This approach also yields computationally efficient algorithms as it allows the evolution of neural networks in less than 3 seconds on average, and of GP trees in at most 10 seconds. The usage of the proposed semantic stopping criteria in conjunction with the computation of optimal mutation/learning steps also results in small trees and neural networks.", "text": "abstract iterative supervised learning algorithms common reach point search induction seems possible available data. search continued beyond point risk overing increases signicantly. following recent developments inductive semantic stochastic methods paper studies feasibility using information gathered semantic neighborhood decide stop search. semantic stopping criteria proposed experimentally assessed geometric semantic genetic programming semantic learning machine algorithm experiments performed real-world high-dimensional regression datasets. results show proposed semantic stopping criteria able detect stopping points result competitive generalization gsgp slm. approach also yields computationally ecient algorithms allows evolution neural networks less seconds average trees seconds. usage proposed semantic stopping criteria conjunction computation optimal mutation/learning steps also results small trees neural networks. reference format gonc¸alves sara silva carlos fonseca mauro castelli. unsure stop? semantic neighbors. proceedings gecco berlin germany july pages. hp//dx.doi.org/./. permission make digital hard copies part work personal classroom granted without provided copies made distributed prot commercial advantage copies bear notice full citation page. copyrights components work owned others must honored. abstracting credit permied. copy otherwise republish post servers redistribute lists requires prior specic permission and/or fee. request permissions permissionsacm.org. gecco berlin germany acm. ----//.... hp//dx.doi.org/./. introduction supervised learning refers task inducing general paern provided examples. common issue supervised learning possibility resulting models could simply learning provided examples instead learning underlying paern. model incurring behavior commonly said overing. genetic programming extensively applied supervised learning tasks. despite this uncommon early years approaches aimed improving generalization resulting models. kushchu mentioned point issue generalization received aention deserved. fact even uncommon measure performance unseen data. notably koza problems presented separate training unseen data performance never evaluated unseen cases eiben jelasity also mentioned point uncommon report unseen data results larger evolutionary computation area. interest studying generalization overing recently increasing geometric semantic genetic programming also contributed rising interest dening variation operators shown perform eectively corresponding standard operators variation operators known geometric semantic operators. reasoning behind geometric semantic operators used create equivalent operators representations computational models. case feedforward neural networks gonc¸alves derived original gsgp mutation operator applicable neural networks. operator incorporated neural network construction algorithm named semantic learning machine algorithm shares similar properties gsgp. within context geometric semantic methods paper explores feasibility using information gathered geometric semantic operators decide stop search process. selection suitable stopping point potentially avoid overing issue. paper organized follows. section contextualizes gsgp slm. section presents proposed semantic stopping criteria. section describes experimental methodology. section presents discusses results section concludes. geometric semantic methods geometric semantic genetic programming gsgp formulations valid assumptions. assumption task hand supervised learning task i.e. given data known targets goal build individual learns underlying paerns data. second assumption error individual computed distance known targets. gsgp derives name fact operators follow particular geometric properties semantic space. context semantics individual dened outputs individual data instances. gsgp individual seen point semantic space. interesting property semantic space associated tness landscape always unimodal supervised learning problem. implies local optima i.e. exception global optimum every point search space least neighbor beer tness neighbor reachable application variation operators. type landscape eliminates local optima issue potentially much favorable terms search eectiveness eciency. moraglio specied geometric semantic operators three domains boolean arithmetic conditional rules. since paper based realvalued semantics geometric semantic operators presented ones arithmetic domain. denition geometric semantic crossover given parent functions geometric semantic crossover returns real function +·t) random real function whose output values range interval denition geometric semantic mutation given parent function geometric semantic mutation mutation step returns real function +ms· random real functions. important consideration geometric semantic mutation reach point semantic space starting point. hand order geometric semantic crossover produce ospring beer parents target semantics must semantics parents. eoretically means mutation operator robust. conrmed practice works also empirically conrmed hill climbing strategy indeed ecient standard population-based strategy. since local optima search focused around best individual without conceivable disadvantage. moraglio named hill climbing strategy semantic stochastic hill climber common hill climber sshc keeps best individual uses geometric semantic mutation advance search process. generation sample osprings produced best selected current best osprings survive next generation. sshc gsgp variant used paper ecient standard population-based strategy. important question raised original gsgp work issue generalization. moraglio measure performance individuals unseen data. erefore generalization ability individuals produced gsgp unknown. gonc¸alves showed resulting gsgp generalization greatly dependent particular detail geometric semantic mutation. mutation operator generates without particular concern structure trees result considerable overing. dierentiate mutation version named unbounded mutation. hand generated bounding function root node outcome competitive generalization. mutation version named bounded mutation. applying bounding function results bounding semantic variation also unseen data. logistic function +e−x used bounding function output random subtree ranges interval consequently output resulting subtracting random subtrees ranges interval applying mutation step semantic variation added parent always ranges interval itself guarantee competitive generalization. detailed discussion issue reader referred since bounded mutation yields beer generalizations used paper. semantic learning machine formulated assumptions gsgp. also follows hill climbing strategy sshc. similarly case gsgp mutation equivalent geometric semantic mutation neural networks also relatively simple combination nns. simplify description mutation operator referred gsm-nn geometric semantic mutation neural networks. operator originally specied single hidden layer subsequently extended applicable number hidden layers paper assesses single hidden layer variant. such following gsm-nn description applies single hidden layer case. already existing gsm-nn generates random joins resulting step gsm-nn create random case random created simplest possible i.e. single hidden neuron. weights input layer hidden neuron randomly generated uniform probability. weight hidden neuron output neuron learning step learning step equivalent mutation step gsgp mutation. inuences amount semantic variation application operator. second step gsm-nn join parent random case single hidden layer join operation rather simple since parent random inuence directly. semantics simply combined output neuron produce semantics resulting gsmnn application. this gsm-nn always perform incremental evaluation mutation application. words regardless size evaluation needs occur part contribution rest already computed order guarantee equivalent comparison sshc hidden neurons added application gsmnn hyperbolic tangent activation function. guarantees that application operator semantic variation always ranges interval represents learning step. results fact outputs hyperbolic tangent range interval similarly sshc variant considered semantic variation always ranges interval represents mutation step. important remark gsm-nn operator excludes need backpropagation adjust weights network. gsm-nn operator allows algorithm eectively explore space nns. details reader referred semantic stopping criteria semantic stopping criteria proposed intended assess feasibility using semantic neighborhood source information detect suitable stopping points semantic supervised learning methods. suitable stopping point point within search induction possible available data. common outcome continuing search beyond point overing training data. best case scenario search enters generalization plateau generalization achieved stabilizes. either case best decision stop search. within context term semantic neighborhood used dene models reachable given reference model given semantic mutation operator applied. reference model considered always best model terms training data performance. sampling semantic neighborhood performed iteration/generation decision stop based information semantic sample. since semantic methods considered paper follow hill climbing strategy semantic sampling already performed iteration/generation order advance search. erefore semantic stopping criteria studied require additional sampling. underlying idea stopping criteria assess trend within search based decide suitable stopping point avoid overing and/or reduce computational time search method. error deviation variation error deviation variation criterion based variation error deviation within semantic neighborhood. context term error deviation used refer sample standard deviation absolute errors given model training instances. criterion concerned models improve current best model given iteration/generation. models criterion measures percentage models reduce error deviation comparison error deviation current best model. words neighbors beer current best criterion measures percentage lower error deviation. information allows determine training error reduction starts conducted less uniformly across training instances. indicate overing starting occur. search stopped criterion measure drops given stopping threshold since criterion based error deviation prevent algorithm nding models large semantic deviation actually desired given target semantics. criterion also prevent next best model larger error deviation previous best exibility could important learning process. search stopped considerable majority models improving training performance expense larger error deviations. training improvement eectiveness training improvement eectiveness criterion based measuring eectiveness semantic variation operator used perform sampling. context eectiveness operator dened percentage times operator able produce neighbor superior current best model. iteration/generation eectiveness measured respect sample considered. criterion search stopped eectiveness operator drops given stopping threshold. reasoning underlying criterion that training error improvements harder possibly improvements forced expense resulting generalization. experimental methodology experimental methodology based gonc¸alves since work recently provided results sshc three datasets used paper. datasets bioavailability plasma protein binding median lethal dose respectively instances features; instances features; instances features. real-world high-dimensional regression datasets describe relationships pharmaceutical drugs pharmacokinetics parameters. table provides experimental parameters. furthermore errors computed root mean squared error outputs model targets dataset. error unseen data referred generalization error. randomly selected data partition performed. method uses data partition equivalent runs. notice sample size refers number models generated initialization iteration/generation sshc. term step used simplication refer learning step mutation step sshc. dierent step strategies studied proposed stopping criteria fixed step optimal step name implies variants step throughout runs variants compute optimal step application mutation operator using moore-penrose inverse. computation optimal steps algorithm explored time paper. gonc¸alves assessed algorithm steps. slm-fs sshc-fs step datasets step dataset. higher step used dataset explained higher order magnitude errors. claims statistical signicance based mann-whitney tests bonferroni correction considering signicance level non-parametric test used data guaranteed follow normal distribution. evolution plots presented next section based median values runs particular measure. median preferred average robust outliers. training error evolution plots based training error best model generation. generalization error evolution plots based generalization error best model selected according training error. experimental study help description following simplications used slmfs describes slm-fs using criterion; slm-fs describes slm-fs using criterion; slm-os describes slm-os using criterion; sshc-fs describes sshc-fs using criterion; sshc-fs describes sshc-fs using criterion; sshc-os describes sshc-os using criterion. criterion applicable variants eectiveness mutation operator case empirically conrmed almost always means stopping occurs variants criterion reasonable stopping thresholds. semantic learning machine initial assessment gure presents evolution measures used stopping criteria complementary measures slm-fs. plots show medians value throughout runs semantic stopping criteria applied. presents measures related criterion second presents measures related criterion. blue solid lines represent measures directly used criteria determine stopping point. dashed lines respective complementary measures. starting criterion row) datasets beginning runs algorithm eective generating models superior current best time reduce error deviation shown line labeled decrease. around iteration values measure usually datasets. iterations quick decrease measure occurs well consequent increase complementary measure decrease measure drops increase measure increases indicates algorithm mostly nding models superior current best expense increasing error deviation. clear ahead area quick disruption occurs indeed signal induction seems reliably possible. dataset decrease measure also starts high values. however similar quick disruption apparent median values presented. clear ahead quick disruption also occurs dataset. fact disruption happens considerably dierent iterations dierent runs makes median values misleading. criterion clear paern occurs across datasets. initially eectiveness variation operator around words generating model superior current best approximately likely generating model inferior current best. expected step version mutation operator approximately models generated direction target semantics generated opposite direction. similarly occurs criterion disruption scenario happens iterations datasets iteration dataset. particularly noticeable datasets fact disruption criterion occurs iterations later criterion. disruption eectiveness variation operator drops across datasets. evolution measures used stopping criteria variants tested presents similar behavior. remaining results shown given space restrictions. next step assess robustness stopping criteria respect associated threshold. intuitively middle values robust sampling variations extreme stopping thresholds lead larger outcome variations. paper considers default stopping threshold criteria also experimentally assessing outcomes table presents generalization errors number iterations training errors resulting application stopping criteria slm-fs slm-os dierent stopping thresholds. performance outcome presented median average standard deviation results show dierent stopping thresholds result remarkably similar outcomes corresponding variant. results reect similar stopping points. desirable outcome tuning stopping threshold seems required. erefore stopping criteria simply shiing burden deciding stop separate search dierent threshold values. remaining analysis considers results default stopping threshold criteria variants avoid overing result competitive generalizations even though variants result considerably dierent stopping points. expected computation optimal steps results considerably faster training error reduction. table median average standard deviation results generalization error number iterations training error resulting application stopping criteria slm-fs slm-os dierent stopping thresholds results earlier stopping points across datasets. slm-os stops signicantly faster slm-fs slm-fs terms generalization case variant signicantly superior variants dataset slm-fs generalizes beer slm-fs slm-os within variants slm-fs also generalizes beer slm-fs dataset statistically signicant dierences found terms generalization remaining comparisons. overall slm-fs robust variant terms generalization. interestingly almost always stops variants. case semantic stochastic hill climber similarly table table presents generalization errors number generations training errors resulting application stopping criteria sshc-fs sshc-os dierent stopping thresholds. results relatively consistent avoiding overing although present slightly bigger variation comparison results. negative outliers exist terms generalization particularly sshc-os edv. instance dataset average standard deviation considerably inuenced single generalization error without outlier average generalization error threshold would standard deviation would behavior might related semantic distributions generated random tree initializations. empirically shown gonc¸alves even otherwise equivalent algorithms random initializations used within geometric semantic mutation operator signicantly inuence outcomes. also discussed moraglio investigation inuence dierent random tree initializations within gsgp/sshc might clarify issue. remaining analysis considers results default stopping threshold criteria case computation optimal steps sshc also results earlier stopping points across datasets. sshc-os stops signicantly faster sshc-fs sshcfs datasets sshc-fs achieves signicantly superior generalizations sshc-fs sshc-os statistically signicant dierences found terms generalization dataset. across datasets best sshc variant always achieves competitive generalization. case criterion applied conjunction step yields robust generalizations. generalizations also result later stops case. additional considerations respect direct comparisons equivalent sshc variants using stopping criterion slmfs generalizes beer sshc-fs datasets. slm-fs generalizes beer sshc-fs dataset slm-os generalizes beer sshc-os datasets. statistically signicant dierences found terms generalization remaining equivalent comparisons. general results show variants robust equivalent sshc variants. previously mentioned might related possibly less smooth semantic distributions generated random tree initializations. order assess proposed stopping criteria stop early variant extended iterations/generations determine induction indeed possible. figure presents evolution plots generalization row) training errors stopping applied. vertical lines represent median stopping point variant. stopping criteria apply stopping point represents stop single exception occurring dataset slm-fs stops earlier slm-fs edv. clear trend across datasets variants that least best stopping point corresponding variant either starts overt simply stabilizes generalization error. hints induction improvement possible available data. even generalization error stabilizes across iterations/generations advantage continuing search would increase model size computational time. notice also stopping points corresponding variant still able eectively decrease training error. particularly important consider step variants criterion means step simply high search eective. erefore variants indeed detecting risky overing regions. table presents number hidden neurons resulting neural networks variant. notice since hidden neuron added successful iteration total number hidden neurons number iterations plus initial random node. resulting neural networks relatively small particularly slm-os hidden neurons average. results number tree nodes sshc variant presented table reasoning applies sshc-os producing small trees particular datasets. notice despite considerable size trees produced sshc-fs variant able surpass sshc variants terms generalization three datasets. note computational time variants tested. tables present computational time seconds run. usage proposed semantic stopping criteria application incremental evaluation mutation operator results demonstrably ecient search procedures. computational times presented computed without form implicit explicit parallelization. results show possible evolve neural networks competitive generalizations less seconds average. variants take around seconds compute. eciency particularly important turning widely used supervised learning method given sometimes perceived relatively slow. conclusions paper showed feasible information gathered semantic neighborhood determine search stopping points result competitive generalizations. semantic stopping criteria proposed directly applicable gsgp algorithm. besides achieving competitive generalizations proposed approach also yields computationally ecient algorithms table median average standard deviation results generalization error number generations training error resulting application stopping criteria sshc-fs sshc-os dierent stopping thresholds figure generalization row) training errors evolution plots median stopping points represented vertical lines corresponding variant. stopping criteria apply stopping point represents stop single exception occurring dataset slm-fs stops earlier slm-fs allows evolution neural networks less seconds average trees seconds. paper also explored time computation optimal learning steps algorithm. results successful evolution neural networks iterations. resulting networks also small number hidden neurons. gsgp usage proposed semantic stopping criteria conjunction computation optimal mutation steps also results small trees. future work extended experimental study help assess general applicability proposed stopping criteria. extended experimental assessment include regression datasets well classication datasets also include comparisons well-established non-evolutionary supervised learning methods. acknowledgments work partially funded project perseids bioisi unit uid/multi// funded fct/mctes/piddac portugal. funding work also provided conacyt project fc--/ aprendizaje evolutivo gran escala. references chen bing shang mengjie zhang. improving generalisation genetic programming symbolic regression structural risk minimisation. proceedings genetic evolutionary computation conference. agoston eiben m´ark jelasity. critical note experimental research methodology proceedings congress evolutionary computation vol. gonc¸alves sara silva. balancing learning overing genetic programming interleaved sampling training data. genetic programming. springer gonc¸alves sara silva carlos fonseca. generalization ability geometric semantic genetic programming. genetic programming. springer gonc¸alves sara silva carlos fonseca. semantic learning machine feedforward neural network construction algorithm inspired geometric semantic genetic programming. progress articial intelligence. lecture notes computer science vol. springer gonc¸alves sara silva joana melo jo˜ao carreiras. random sampling technique overing control genetic programming. genetic programming. springer gonc¸alves. exploration generalization overing genetic programming standard geometric semantic approaches. ph.d. dissertation. department informatics engineering university coimbra portugal. gonc¸alves sara silva. experiments controlling overing genetic programming. proceedings portuguese conference articial intelligence progress articial intelligence michael kommenda michael aenzeller bogdan burlacu gabriel kronberger stephan winkler. genetic programming data migration symbolic regression. proceedings companion publication annual conference genetic evolutionary computation. john koza. genetic programming programming computers means natural selection press. ibrahim kushchu. evaluation evolutionary generalisation genetic programming. artif. intell. rev. issue", "year": 2017}