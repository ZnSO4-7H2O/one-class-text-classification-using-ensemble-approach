{"title": "Face2Text: Collecting an Annotated Image Description Corpus for the  Generation of Rich Face Descriptions", "tag": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "The past few years have witnessed renewed interest in NLP tasks at the interface between vision and language. One intensively-studied problem is that of automatically generating text from images. In this paper, we extend this problem to the more specific domain of face description. Unlike scene descriptions, face descriptions are more fine-grained and rely on attributes extracted from the image, rather than objects and relations. Given that no data exists for this task, we present an ongoing crowdsourcing study to collect a corpus of descriptions of face images taken `in the wild'. To gain a better understanding of the variation we find in face description and the possible issues that this may raise, we also conducted an annotation study on a subset of the corpus. Primarily, we found descriptions to refer to a mixture of attributes, not only physical, but also emotional and inferential, which is bound to create further challenges for current image-to-text methods.", "text": "past years witnessed renewed interest tasks interface vision language. intensively-studied problem automatically generating text images. paper extend problem speciﬁc domain face description. unlike scene descriptions face descriptions ﬁne-grained rely attributes extracted image rather objects relations. given data exists task present ongoing crowdsourcing study collect corpus descriptions face images taken wild’. gain better understanding variation face description possible issues raise also conducted annotation study subset corpus. primarily found descriptions refer mixture attributes physical also emotional inferential bound create challenges current image-to-text methods. keywords face images vision language image-to-text natural language generation crowdsourcing paper describes annotation project conducted cross-disciplinary group researchers university malta rival group create corpus human face images annotated rich textual descriptions. initial goal project investigate general users describe images human faces ultimately create resource could used train system generate descriptions potentially varied rich possible thus moving state-of-the-art automatic description generation images faces feature-based process takes advantage complex textual descriptions. report preliminary version corpus focussing collected evaluated. automatic image description research rely wide range image-description datasets. datasets consist images depicting various objects actions associated descriptions typically collected crowdsourcing. descriptions verbalise objects events relations shown images different degrees granularity. example widely-used image captioning datsets flickrk flickrk vltk coco contain images familiar scenes descriptions restricted ‘concrete conceptual’ level mentioning visible minimising inferences drawn visual information. datasets somewhat specialised. example caltechucsd birds oxford flowers- contain ﬁne-grained visual descriptions images birds ﬂowers respectively datasets also contain captions different languages case multik extensive overview image caption datasets found bernardi although images faces included datasets none speciﬁcally targets face descriptions. several datasets faces widely used image processing community including megaface ijb-c datasets datasets however labelled attributes. lfwa celeba datasets hand contain images labelled features mainly referring physical facial attributes skin colour hair style also attributes person e.g. gender. number datasets also focus speciﬁcally emotion recognition rendering images faces expressions typically either acquired speciﬁc emotions elicited photographed subjects posed actors subsequently validated asking annotators image emotion labels. none datasets pairs images text beyond simple feature labels. facetext dataset preliminary version described paper aims combine characteristics several existing datasets novel re-using collection images human faces collected wild order collect rich textual descriptions varying semantic granularity syntactic complexity refer physical attributes emotions well inferred elements necessarily directly observable image itself. number reasons face descriptions criminal longish face receding hairline although rest carefully combed parting person’s left. groomed mustache. could middleeastern arab world. double chin unhappy face. serious looking. middle aged women blond well groomed seems defending/justifying crowd/audience. face remorse/regret something done. attractive woman lovely blonde hair style looks pretty seductive lips. looks like fashion queen age. interesting domain vision-language research. descriptions faces frequent human communication example seeks identify individual distinguish another person. also pervasive descriptive narrative text. ability adequately describe person’s facial features give rise humanlike communication artiﬁcial agents potential applications conversational agents interactive narrative generation well forensic applications faces need identiﬁed textual spoken descriptions. hence primary motivation collecting facetext corpus promises provide resource break ground problem automatic description retrieval images beyond current focus naturalistic images scenes common objects. corpora target speciﬁc domain descriptions solicited tend fairly detailed nuanced. raises possibility in-depth investigation language speciﬁc domain including identiﬁcation description salient distinguishing features investigation conditions tend feature human descriptions. indeed descriptions faces produced humans often feature-based focussing distinguishing physical characteristics face and/or transient emotional states. alternatively involve inference analogy. examples descriptions seen figure nevertheless assuming existence appropriate dataset architectures generating face descriptions likely share many challenges familiar image description task. hence worth brieﬂy outlining ways latter approached. approaches image description generation based either caption retrieval direct generation generation-by-retrieval approach human authored descriptions similar images stored database image-description pairs. given input image described database queried similar images input image descriptions images returned. descriptions either copied directly synthesized extracted phrases. examples retrieval visual space; approaches rely retrieval multimodal space hand direct generation attempts generate novel descriptions using natural language generation techniques. traditionally achieved using computer vision detectors applied generate list image content classical pipeline produces textual description verbalising salient aspects image. early examples systems. state image description makes deep learning approaches usually relying neural language model generate descriptions based image analysis conducted pre-trained convolutional network systems currently state suffer tendency generate repetitive descriptions generating signiﬁcant amount descriptions found as-is training suggests datasets trained repetitive lack diversity. state image captioning requires large datasets training testing. datasets exist scene descriptions data currently available face description task despite existence annotated image datasets. following section describe addressed lacuna initiating ongoing crowd-sourcing exercise create large dataset face descriptions paired images annotated physical features. long-term extend scope current imageto-text technology ﬁner-grained attribute-focussed descriptions speciﬁc types entities akin birds ﬂowers rather objects relations scenes. descriptions facetext dataset currently collected custom-made crowd-sourcing website. screen shot page requesting description face shown figure pilot study prior launching crowd-sourcing interface conducted small pilot study among approximately individuals asked write descriptions sample faces dataset. used provide participants examples expected data images selected faces wild dataset collecting many descriptions possible image. close-up images faces public personalities taken naturalistic contexts selected images evenly divided pictures males females. procedure system designed allow participant describe many images wish. stage process participant shown image selected randomly among given fewest descriptions far. long-term equalises number descriptions image. participants given minimal instructions order encourage much variation possible done because precise nature face descriptions distribution various features knowledge underresearched. hence rather explicit instructions regarding meant write participants shown four examples faces accompanied three different descriptions collected preliminary pilot study. main points instructions given following reading instructions participants requested enter basic demographic information speciﬁcally gender bracket country origin proﬁciency english. latter self-rated forced choice among options native speaker non-native ﬂuent ﬂuent. respondent rated non-ﬂuent. participants could interrupt study point. however system saved session variables meaning limited time period participants could revisit online interface resume description exercise left off. participation dataset participation voluntary ﬁnancial incentives offered participants. date crowdsourcing experiment advertised among staff students university malta well social media. total descriptions collected participants. images described least times approximately images descriptions. annotation agreement shown figure considerable variation descriptions people write. majority include physical features also emotional descriptors well analogical descriptions inferred characteristics addition data collection exercises raise potential ethical concern insofar individuals take advantage anonymity crowdsourcing exercise produce texts racially sexually offensive. note however taking steps identify weed ethically problematic descriptions prior dissemination data intend exclude descriptions simply grounds describe nationality ethnicity participant described figure ‘middle eastern’. indeed examples raise interesting questions salience features different individuals function instance come including country origin gender among demographic details request participants hope able address questions informed manner. annotation gain better understanding issues well potential challenges annotating data conducted annotation exercise subset data. study conducted among nine members rival team. eight designated annotators ninth acted control annotator. annotators assigned random descriptions. these random descriptions description garbage? description contain elements inferred external image? description refer emotional state face? description include physical attributes? description contain hate speech table questions used annotation exercise overall proportion positive responses across descriptions. column gives overall agreement using fleiss’s kappa among descriptions shared among participants including control annotator. column gives mean agreement annotators control annotator standard deviations parentheses. percentages questions exclude descriptions question answered positively least annotator. shared among eight annotators. used compute overall agreement. control annotator’s data consisted descriptions consisting shared subset descriptions; descriptions annotators’ set. participants viewed description corresponding image responded yes/no questions shown left panel table ﬁrst intended weed descriptions relationship image. descriptions annotators answered question positively included subsequent analyses. last question table intended identify potentially racist sexually discriminatory descriptions modulo provisions made concerning ethnic characteristics used offensive manner. shown table description identiﬁed potentially containing hate speech. overall proportions positive responses suggest majority descriptions focus physical attributes expected substantial proportion also incorporate inferred characteristics and/or emotional elements table also gives agreement annotators shared descriptions estimated using fleiss’s kappa appropriate multiple annotators. shown table questions high levels agreement exception second question agreement falls indicates ‘inferred’ open interpretation annotators viewing features nationality even person’s inferred others not. discussions annotators exercise completed became clear sometimes viewed physical agreement sanity check also computed agreement annotators control annotator. recall that addition descriptions shared among participants latter annotated descriptions consisting annotators’ cases. agreement results shown ﬁnal column table exception question since none descriptions shared control annotator classiﬁed individual participant containing hate speech. case agreement ﬁgures generally lower means distinct values. perhaps notable drop question deals descriptions containing emotion mean agreement control annotator drops note however also case variance highest important reason drop case annotator seems interpreted question differently control annotator resulting negative agreement ﬁgure increasing variance considerably mean without outlier goes discussion overall distribution sub-corpus included annotation study conforms expectations majority descriptions incorporating physical attributes sizeable proportion including emotional possibly inferred attributes. agreement annotators appear reliably identiﬁed descriptions falling categories interest possible exception inference clearly needs precise deﬁnition. agreement control annotator generally lower across board consequences preliminary study better placed predict face description dataset contain challenges automatic face description particular descriptions bound refer mixture attributes physical also emotional. latter probably challenging identify current tools also raise interesting questions expressed relation physical characteristics. person smiling qualify happy? clearly expressed also depend purpose descriptions intended achieve though present crowd-sourcing study specify particular purpose since cast wide possible view gaining better understanding ways people describe faces. identify main distribution types descriptions extent annotators expect agree types. current work focussing extending crowdsourcing study produce dataset sufﬁciently large support non-trivial machine learning work automated description faces. extend reach current image-to-text systems domain focus necessarily less scene descriptions involving objects relations ﬁne-grained descriptions using physical attributes. based annotation reported paper intend ﬁlter irrelevant descriptions and/or ethically problematic ones prior dissemination. apart extending crowd-sourcing exercise elicit human-authored descriptions actively exploring semi-automatic data augmentation techniques. particularly promising avenue existing imagedescription pairs facetext corpus harvest similar publicly available images. done goal mine text surrounding images portions text similar descriptions produced contributors corpus. second challenge address possible differences purposes descriptions produced. possibilities wide-ranging describing face accurately enough recognition gamiﬁed humorous contexts descriptions might need rely analogy inference. medium-term goals undertake ﬁne-grained annotation exercise existing data view identifying portions descriptions pertain particular features agreement statistics already indicate identiﬁed reasonably high reliability; explicitly annotating data hope able develop techniques automatically future descriptions high-level semantic information. based this possible undertake ﬁne-grained evaluation corpus example types images certain attributes tend recur people’s descriptions. correlate trends demographic data collect participants view constructing model predict aspects face salient enough warrant explicit mention given describer’s characteristics background. thank three anonymous reviewers helpful comments paper. work partially funded endeavour scholarship scheme scholarships part-ﬁnanced european union european social fund operational programme cohesion policy bernardi akici elliott erdem erdem ikizler-cinbis keller muscat plank automatic description generation images survey models datasets evaluation measures. journal artiﬁcial intelligence research devlin cheng fang gupta deng zweig mitchell language models image captioning quirks works. annual meeting association computational linguistics. elliott keller image description using visual dependency representations. proceedings conference empirical methods natural language processing labeled faces wild database studying face recognition unconstrained environments. technical report university massachusetts amherst october. kemelmacher-shlizerman seitz miller brossard megaface benchmark milproceedings lion faces recognition scale. ieee conference computer vision pattern recognition pages klare klein taborsky blanton cheney allen grother burge jain pushing frontiers unconstrained face detection recognition iarpa janus benchmark ieee conference computer vision pattern recognition pages june. kulkarni premraj dhar choi berg berg baby talk understanding generating simple image descriptions. ieee conference computer vision pattern recognition. learned-miller huang roychowdhury labeled faces wild survey. michal kawulok editors advances face detection facial image analysis pages springer international publishing berlin heidelberg. mitchell dodge goyal yamaguchi stratos mensch berg berg daume midge generating image descriptions computer vision detections. proceedings conference european chapter association computational linguistics pages avignon france. association computational linguistics. socher karpathy manning grounded compositional semantics finding describing images sentences. transactions association computational linguistics tottenham tanaka leon mccarry nurse hare marcus westerlund casey nelson nimstim facial expressions judgments untrained research participants. psychiatry research. vinyals toshev bengio erhan show tell neural image caption generator. ieee conference computer vision pattern recognition institute electrical electronics engineers jun. kiros courville salakhutdinov zemel bengio show attend tell neural image caption generation visual attention. proceedings international conference machine learning volume abs/. pages", "year": 2018}