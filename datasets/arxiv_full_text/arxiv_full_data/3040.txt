{"title": "One-Shot Visual Imitation Learning via Meta-Learning", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "abstract": "In order for a robot to be a generalist that can perform a wide range of jobs, it must be able to acquire a wide variety of skills quickly and efficiently in complex unstructured environments. High-capacity models such as deep neural networks can enable a robot to represent complex skills, but learning each skill from scratch then becomes infeasible. In this work, we present a meta-imitation learning method that enables a robot to learn how to learn more efficiently, allowing it to acquire new skills from just a single demonstration. Unlike prior methods for one-shot imitation, our method can scale to raw pixel inputs and requires data from significantly fewer prior tasks for effective learning of new skills. Our experiments on both simulated and real robot platforms demonstrate the ability to learn new tasks, end-to-end, from a single visual demonstration.", "text": "abstract order robot generalist perform wide range jobs must able acquire wide variety skills quickly efﬁciently complex unstructured environments. high-capacity models deep neural networks enable robot represent complex skills learning skill scratch becomes infeasible. work present meta-imitation learning method enables robot learn learn efﬁciently allowing acquire skills single demonstration. unlike prior methods one-shot imitation method scale pixel inputs requires data signiﬁcantly fewer prior tasks effective learning skills. experiments simulated real robot platforms demonstrate ability learn tasks end-to-end single visual demonstration. enabling robots generalists capable performing wide variety tasks many objects presents major challenge current methods. learning-based approaches offer promise generic algorithm acquiring wide range skills. however learning methods typically require fair amount supervision experience task especially learning complex skills sensory inputs using deep neural network models. moreover methods provide mechanism using experience previous tasks quickly solve tasks. thus learn many skills training data would need collected independently every task. reusing data across skills robots able amortize experience signiﬁcantly improve data efﬁciency requiring minimal supervision skill. paper consider question leverage information previous skills quickly learn behaviors? propose combine meta-learning imitation enabling robot reuse past experience result learn skills single demonstration. unlike prior methods take task identity demonstration input contextual policy approach learns parameterized policy adapted different tasks gradient updates effectively learning imitation learn. result skills learned ﬂexible using fewer overparameters. ﬁrst time demonstrate visionbased policies ﬁne-tuned end-to-end demonstration using meta-learning pre-training procedure uses demonstrations diverse range environments. primary contribution paper demonstrate approach one-shot imitation learning pixels. evaluate approach simulated planar reaching domains simulated pushing tasks visual placing tasks real robot approach able learn visuomotor policies adapt task variants using visual demonstration including settings video demonstration available without access controls applied demonstrator. employing parameter-efﬁcient meta-learning method approach requires relatively modest number demonstrations meta-learning scales pixel inputs. result method successfully applied real robotic systems. present method combines imitation learning meta-learning one-shot learning visual demonstrations. efﬁcient imitation small number demonstrations successful scenarios state environment poses objects known work focus settings state environment unknown must instead learn sensory inputs. removes need pre-deﬁned vision systems also making method applicable vision-based non-prehensile manipulation unknown dynamic environments. imitation learning pixels widely studied context mobile robotics however learning demonstrations primary challenges applied real-world settings. ﬁrst widely-studied issue compounding errors address paper. second need large number demonstrations task. latter limitation major roadblock developing generalist robots learn wide variety tasks imitation. inverse reinforcement learning reduce number demonstrations needed inferring reward function underlying demonstrations. however requires additional robot experience optimize reward experience typically comes form trial-and-error learning data learning model. work drastically reduce number demonstrations needed individual task sharing data across tasks. particular goal learn task single demonstration task using dataset demonstrations many tasks meta-learning. sharing information across tasks means idea e.g. using task-to-task mappings gating shared features multi-task robotic learning methods consider problem generalization tasks speciﬁcation task. common approach often referred contextual policies provide task input policy value function task represented goal demonstration another approach train controllers variety tasks learn mapping task representations controller parameters work instead meta-learning enable robot quickly learn tasks gradient-based policy updates. essence learn policy parameters that ﬁnetuned demonstration task immediately learn perform task. enables robot learn tasks end-to-end extreme efﬁciency using demonstration without requiring additional mechanisms contexts learned update functions. section introduce visual meta-imitation learning problem vision-based policy must adapt task single demonstration. also summarize prior meta-learning method extend meta-imitation learning algorithm section goal learn policy quickly adapt tasks single demonstration task. remove need large amount task-speciﬁc demonstration data propose reuse demonstration data number tasks enable efﬁcient learning tasks. training adaptation across tasks meta-learning effectively treats entire tasks datapoints. amount data available individual task relatively small. context robotics precisely want developing generalist robots ability provide small amount supervision task robot perform. section formally deﬁne one-shot imitation learning problem statement introduce notation. consider policy maps observations predicted actions meta-learning policy trained adapt large number tasks. formally imitation task consists demonstration data generated loss function used imitation. feedback provided loss function expert policy might mean squared error continuous actions cross-entropy loss discrete actions. meta-learning scenario consider distribution tasks one-shot learning setting policy trained learn task drawn demonstration generated meta-training task sampled policy trained using demonstration expert determine training test error according loss policy improved considering test error demonstration changes respect parameters. thus test error sampled demonstration serves training error meta-learning process. meta-training tasks sampled meta-performance measured policy’s performance learning demonstration. tasks used meta-testing held meta-training. approach visual meta-imitation learning meta-learning train fast adaptation across number tasks extending model-agnostic meta-learning metaimitation learning visual inputs. previously maml applied few-shot image recognition reinforcement learning. maml algorithm aims learn weights model standard gradient descent make rapid progress tasks drawn without overﬁtting small number examples. method uses gradient descent optimizer introduce additional parameters making parameter-efﬁcient meta-learning methods. adapting task model’s parameters become maml updated parameter vector computed using gradient descent updates α∇θlti. simplicity notation consider gradient task i.e. update rest section using multiple gradient updates straightforward extension. model parameters trained optimizing performance respect across tasks sampled corresponding following problem note meta-optimization performed parameters whereas objective computed using updated parameters effect maml aims optimize model parameters small number gradient steps task produce maximally effective behavior task. meta-optimization across tasks uses stochastic gradient descent time e.g. image whereas output action taken time e.g. torques applied robot’s joints. denote demonstration trajectory ...ot mean squared error loss function policy parameters follows primarily consider one-shot case single demonstration used gradient update. however also multiple demonstrations resolve ambiguity. meta-learning assume dataset demonstrations least demonstrations task. data used meta-training; meta-test time assumes demonstration task. meta-training meta-optimization step entails following batch tasks sampled demonstrations sampled task. using demonstrations computed task using gradient descent equation then second demonstration task used compute gradient meta-objective using equation loss equation finally updated according gradient meta-objective. effect pair demonstrations serves training-validation pair. algorithm summarized algorithm result meta-training policy adapted tasks using single demonstration. thus meta-test time task sampled demonstration task provided model updated acquire policy task. meta-test time task might involve goals manipulating previously unseen objects. two-head architecture meta-learning loss fast adaptation standard maml setup outlined previously policy consistent across prepostgradient update stages. however make modiﬁcation parameters ﬁnal layers network shared forming heads shown figure parameters pre-update head used ﬁnal post-update policy parameters postupdate head updated using demonstration. sets parameters meta-learned effective performance adaptation. interestingly head architecture amounts using different inner objective meta-optimization keeping outer objective. this denote post-synamptic activations last hidden layer weight matrix bias ﬁnal layer. inner loss function given weights bias last layer effectively form parameters metalearned loss function. meta-learned loss function compute adapted parameter task gradient descent. then meta-objective becomes provides algorithm ﬂexibility adapts policy parameters expert demonstration found increase performance experiments however interesting implication using learned loss omit actions -shot adaptation discuss next. learning imitate without expert actions conventionally demonstration trajectory consists pairs observations actions discussed section however many scenarios practical simply provide video task performed e.g. human another robot. step towards goal consider paper remove need robot trajectory actions test time. though clear assume access expert actions meta-training. without access expert actions test time unclear loss function -shot adaptation thus meta-learn loss function discussed previous section. simply modify loss equation removing expert actions corresponds learned quadratic loss function ﬁnal layer activations parameters though practice loss function could complex. loss function learn learn observations demonstration using meta-optimization objective equation shown experiments sections model architectures meta-imitation learning convolutional neural network represent policy similar prior vision-based imitation meta-learning methods policy observation includes camera image robot’s conﬁguration e.g. joint angles end-effector pose. section overview policy architecture leave details discussed section policy consists several strided convolutions followed relu nonlinearities. ﬁnal convolutional layer transformed spatial feature points using spatial soft-argmax concatenated robot’s conﬁguration. result passed fully-connected layers relu nonlinearities. data within demonstration trajectory highly correlated across time batch normalization effective. instead used layer normalization layer although meta-imitation learning work well standard policy architectures described above optimal architecture meta-learning necessarily correspond optimal architecture standard supervised imitation learning. particular modiﬁcation found improves meta-learning performance concatenate vector parameters hidden layer post-synaptic activations leads refer bias transformation. parameter vector treated parameters policy meta-learning test-time adaptation. formally denote parameter vector post-synaptic activations pre-synaptic activations next layer standard neural network architecture sets bias weight matrix error gradient respect standard bias thus gradient update standard bias directly coupled update weight matrix parameters earlier layers network. bias transformation describe next provides control updated bias eliminating decoupling. bias transformation weight matrix bias. first note including simply corresponds reparameterization bias since neither depend input. error gradient respect gradient step updated transformed bias −αww summary bias transformation increases representational power gradient without affecting representation power network itself. experiments found simple addition network made gradientbased meta-learning signiﬁcantly stable effective. include diagram policy architecture bias transformation figure figure example tasks policy’s perspective. pair images shows start ﬁnal scenes demonstration. bottom shows corresponding scenes learned policy roll-out. left given demonstration reaching target particular color policy must learn reach color setting. center robot pushes target object goal seeing demonstration pushing object toward goal different scene. right provide demonstration placing object target robot must place object target setting. goals experimental evaluation answer following questions method learn learn policy maps image pixels actions using single demonstration task? meta-imitation learning method compare prior one-shot imitation learning methods varying dataset sizes? learn learn without expert actions? well approach scale real-world robotic tasks real images? evaluate method one-shot imitation three experimental domains. setting compare proposed method subset following methods random policy policy outputs random actions standard normal distribution. contextual policy feedforward policy takes input ﬁnal image demonstracontextual policy lstm policies proposed approach trained using datasets supervision. policies including proposed approach metatrained behavioral cloning objective supervision expert actions using adam optimizer default hyperparameters ﬁrst experimental domain family planar reaching tasks illustrated figure goal particular task reach target particular color amid distractors different colors. simulated domain allows rigorously evaluate method compare prior approaches baselines. consider vision non-vision variants task directly compare prior methods applicable vision-based policies. appendix details experimental setup choices hyperparameters. evaluate method range meta-training dataset sizes show one-shot imitation success rate figure using vision meta-imitation learning able handle pixel inputs lstm contextual policies struggle learn tasks using modestly-sized meta-learning datasets. non-vision case involves fewer parameters lstm policies fare much better particularly using attention still perform worse mil. prior work demonstrated approaches using demonstrations therefore mediocre performance methods much smaller datasets surprising. also provide comparison without bias transformation discussed section results demonstrate bias transformation perform consistently across dataset sizes. figure one-shot success rate test tasks function meta-learning dataset size simulated domains. meta-imitation learning approach perform well across range dataset sizes effectively learn tasks prior approaches feed goal image demonstration input. random policy achieves reaching success pushing success. videos policies supplementary video. video video method +state lstm contextual lstm contextual table one-shot -shot simulating pushing success rate varying demonstration information provided test-time. successfully learn demonstration without actions without robot state actions lstm contextual policies. goal second experiments evaluate approach challenging domain involving -dof torque control environment substantially physical visual diversity across tasks. experiment consists family simulated table-top pushing tasks illustrated figure goal push particular object random starting position target amid distractor. designed pushing environment starting openai pusherenv using mujoco physics engine modiﬁed environment include objects vision policy input across tasks wide range object shapes sizes textures frictions masses. selected mesh shapes thingiverse.com meshes meta-training evaluation. meshes include models chess pieces models animals like teddy bears pufferﬁsh miscellaneous shapes. randomly sampled textures images used held-out textures meta-testing. selection objects textures shown figure experimental details hyperparameters ablations appendix performance one-shot pushing held-out objects shown figure indicates effectively learned learn push objects one-shot learning success using largest dataset size. furthermore achieves average higher success lstmbased approach across dataset sizes. contextual policy struggles likely full demonstration trajectory information informative inferring friction mass target object. table provide additional evaluations using largest dataset size. ﬁrst evaluates approach handles input demonstrations less information e.g. without actions and/or robot state. this trained method able handle demonstrations discussed section lstm approach difﬁculty learning without expert actions. also sees drop performance less dramatic. second evaluation shows approaches improve performance demonstrations available rather despite policies trained -shot learning. case averaged predicted action input demonstrations contextual lstm approaches averaged gradient mil. goal ﬁnal experiment evaluate well real robot learn learn interact unknown objects single visual demonstration. handling unseen objects challenge learning-based non-learning-based manipulation methods necessity robots capable performing diverse tasks unstructured real-world environments. practice robot learning approaches focused much narrow notions generalization subset training objects figure training test objects used simulated pushing real-world placing experiments. note show subset training objects used pushing placing experiments subset textures object scales used training testing robot pushing. varied target object location block stacking order goal mind designed robotic placing experiment using -dof robot camera goal place held item target container plate bowl ignoring distractors. collected roughly demonstrations meta-training using diverse range objects evaluated one-shot learning using held-out unseen objects policy provided single visual demonstration placing held item onto target varied positions target distractors illustrated figure demonstrations collected using human teleoperation motion controller virtual reality headset demo included camera video sequence end-effector poses sequence actions end-effector linear angular velocities. appendix explanation data collection evaluation hyperparameters. results table show policy learn localize previously-unseen target object successfully place held item onto target success using single visual demonstration objects. found lstm contextual policies unable localize correct target object likely modestlysized meta-training dataset instead placed onto arbitrary object achieving success. using two-head approach described also experimented providing video demonstration omitting robot end-effector trajectory controls. also learn handle setting although less success suggesting need data and/or research. include videos placing policies supplementary video. table one-shot success rate placing held item correct container real robot using held-out test objects. meta-training used dataset objects. using video receives video part demonstration trajectory actions. proposed method one-shot visual imitation learning learn perform tasks using visual inputs single demonstration. approach extends gradient-based meta-learning imitation learning setting experimental evaluation demonstrates substantially outperforms prior one-shot imitation learning method based recurrent neural networks. gradient-based meta-learning makes approach efﬁcient terms number demonstrations needed meta-training efﬁciency makes possible also evaluate method using pixel inputs real robotic system. meta-imitation learning substantially improve efﬁciency robotic learning methods without sacriﬁcing ﬂexibility generality end-to-end training especially valuable learning skills complex sensory inputs images. experimental evaluation uses tasks limited diversity expect capabilities method increase substantially provided increasingly diverse demonstrations meta-training. since meta-learning algorithms incorporate demonstration data available tasks provide natural avenue utilizing large datasets robotic learning context making possible robots learn skills acquire demonstrations actually become faster effective learning skills process. work supported national science foundation iis- iis- iis- graduate research fellowship berkeley deepdrive pecase award well young investigator program award. authors would like thank duan providing reference implementation anonymous reviewers providing feedback. schaal ijspeert billard. computational approaches motor learning imitation. philosophical transactions royal society london biological sciences bojarski testa dworakowski firner flepp goyal jackel monfort muller zhang zhang zhao zieba. learning self-driving cars. arxiv preprint arxiv. giusti guzzi cires¸an f.-l. rodr´ıguez fontana faessler forster schmidhuber caro machine learning approach visual perception forest trails mobile robots. ieee robotics automation letters laskey staszak y.-s. hsieh mahler pokorny dragan goldberg. shiv reducing supervisor burden dagger using support vectors efﬁcient learning demonstrations high dimensional state spaces. international conference robotics automation barrett taylor stone. transfer learning reinforcement learning physical robot. ninth international conference autonomous agents multiagent systemsadaptive learning agents workshop gupta devin abbeel levine. learning invariant feature spaces transfer skills reinforcement learning. international conference learning representations section provide additional experimental details experiments including information regarding data collection evaluation training hyperparameters. experimental setup vision no-vision cases experiment input policy includes joint angles end-effector position. vision variant image also provided input. non-vision version positions objects policy index target object within state vector known must inferred demonstration. policy output corresponds torques applied joints arm. policy roll-out considered success comes within meters goal within last timesteps size arena meters. obtain expert policies task ilqg trajectory optimization generate solutions task collect several demonstrations task resulting policy injected gaussian noise. meta-test time evaluate policy tasks different trials task task corresponds held-out color. note demonstration provided meta-test time usually involves different target distractor positions corresponding test trial. thus one-shot learned policy must learn localize target using demonstration generalize positions meta-training must learn handle different colors. hyperparameters vision-based policies convolutional neural network policy convolution layers ﬁlters followed fully-connected layers hidden dimension domain only simply ﬂattened ﬁnal convolutional rather transforming spatial feature points. recurrent policies additionally lstm units takes input features ﬁnal layer. non-vision policies architecture without convolutional layers replacing output convolutional layers state input. methods trained using meta batch-size tasks. policy trained meta-imitation learning uses meta-gradient update step size bias transformation dimension also helpful clip meta-gradient interval applying normal single-head architecture shown figure simulated pushing experimental setup policy input consists image robot joint angles joint velocities end-effector pose. push considered success center target object lands target circle least timesteps within -timestep episode. reported pushing success rates computed tasks trials task acquired separate demonstration policy task using trust-region policy optimization algorithm. expert policy inputs included target distractor object poses rather vision input. encourage expert policies take similar strategies ﬁrst trained single policy single task initialized parameters policies ﬁrst policy. initializing policy parameters increased variance converged policy ensure appropriate exploration. hyperparameters methods neural network policy strided convolution layers ﬁlters followed spatial softmax fully-connected layers hidden dimension optimization method meta-batch size tasks. uses inner gradient descent step step size inner gradient clipping within range bias transformation dimension lstm policy uses hidden units. domain signiﬁcantly challenging simulating reaching domain found important two-head architecture described section include ablation two-head architecture table demonstrating beneﬁt choice. table ablation test -head -head architecture simulated pushing shown figure using dataset demonstrations meta-learning. using heads leads signiﬁcantly better performance domain. real-world placing experimental setup videos demo composed sequence images robot camera. pre-process demonstrations downsampling images factor cropping size since videos collected variable length subsample videos ﬁxed time horizon collect demonstration data task randomly select holding object three placing containers training objects place three objects front robot random positions. collect demonstrations validation rest training set. policy evaluation evaluate policy tasks trials task placing target holding object test task. addition manually code open gripper action trajectory causes robot drop holding object. deﬁne success whether held object landed target container gripper opened. hyperparameters neural network policy strided convolution layers nonstrided convolutions layers ﬁlters followed spatial softmax fully-connected layers hidden dimension initialize ﬁrst convolution layer vgg- keep ﬁxed meta-training. auxiliary loss besides imitation objective regresses learned features ﬁrst time step positions target container. additionally also feed predicted position target fully-connected network. uses meta-batch size tasks inner gradient descent steps step size inner gradient clipping within range bias transformation dimension also single-head architecture simulated reaching. lstm policy uses hidden units.", "year": 2017}