{"title": "Real-Time Scheduling via Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Cyber-physical systems, such as mobile robots, must respond adaptively to dynamic operating conditions. Effective operation of these systems requires that sensing and actuation tasks are performed in a timely manner. Additionally, execution of mission specific tasks such as imaging a room must be balanced against the need to perform more general tasks such as obstacle avoidance. This problem has been addressed by maintaining relative utilization of shared resources among tasks near a user-specified target level. Producing optimal scheduling strategies requires complete prior knowledge of task behavior, which is unlikely to be available in practice. Instead, suitable scheduling strategies must be learned online through interaction with the system. We consider the sample complexity of reinforcement learning in this domain, and demonstrate that while the problem state space is countably infinite, we may leverage the problem's structure to guarantee efficient learning.", "text": "mobile cyber-physical robots must respond adaptively dynamic operating conditions. eﬀective operation systems requires sensing actuation tasks performed timely manner. additionally execution mission speciﬁc tasks imaging room must balanced need perform general tasks obstacle avoidance. problem addressed maintaining relative utilization shared resources among tasks near user-speciﬁed target level. producing optimal scheduling strategies requires complete prior knowledge task behavior unlikely available practice. instead suitable scheduling strategies must learned online interaction system. consider sample complexity reinforcement learning domain demonstrate problem state space countably inﬁnite leverage problem’s structure guarantee eﬃcient learning. cyber-physical systems mobile robots setting enforcing utilization target shared resources useful mechanism striking balance general mission-speciﬁc goals ensuring timely execution tasks. however classical scheduling approaches inapplicable tasks domains consider. first tasks eﬃciently preemptable example actuation tasks involve moving physical resource robotic pan-tilt unit. restoring actuator state preemption would essentially restarting task. therefore instance task second duration task holds resource stochastic. true actuation tasks often involve variable mechanical processes. classical real-time scheduling approaches model tasks deterministically treating task’s worst-case execution time execution budget. inappropriate domain task’s wcet many orders magnitude larger typical duration. account variability assume task’s duration obeys underlying unknown stationary distribution. beoptimally conditions requires account uncertainty order anticipate common events exploiting early resource availability hedging delays. previous work proposed methods solving scheduling problems concerns provided accurate task models available. straightforward approach employing methods certainty equivalence constructing solving approximate model observations system. however less eﬀective interleaving modeling solution execution since interleaving learning allows controller adapt conditions observed execution diﬀer conditions observed distinct modeling phase. interleaving modeling execution raises exploration/exploitation dilemma controller must balance optimal behavior respect available information long-term beneﬁt choosing apparently suboptimal exploratory actions improve information. dilemma particularly relevant real-time systems domain sustained suboptimal behavior translates directly poor quality service. following glaubius problem modeled markov decision process consists states actions transition system cost function discrete decision epoch controller observes current state selects action transitions state distributed according incurs cost value policy expected long-term γ-discounted cost following discount factor satisﬁes recurrence thus computing optimal control reduced computing optimal value function. several dynamic programming linear programming approaches developed solve problems ﬁnite task scheduling problem modeled utilization states state n-vector component advance. provide bounds computational complexity learning near-optimal policy using balanced wandering. result novel extends established methods learning ﬁnite markov decision processes domain countably inﬁnite state space unbounded costs. also provide empirical comparison several exploration methods observe structure task scheduling problem enforces eﬀective exploration. glaubius task scheduling model consists tasks require mutually exclusive single common resource. task consists inﬁnite sequence jobs available time becomes available immediately upon completion tij. jobs cannot preempted whenever granted resource occupies resource stochastic duration completion. simplifying assumptions made regarding distribution durations holds duration always obeys distribution regardless preceded means system history necessary predict behavior particular job. holds consecutive jobs task obey distribution. thus every task duration distribution duration every drawn. actuator example previous section immediately satisfy assumptions since job’s duration depends state actuator starts executing. enforced actuator-sharing however requiring leaves actuator static reference position relinquishing control. addition assumptions stated above duration distribution must bounded support positive integers every task integert= simplicity denotes maximum among wcet individual tasks ignored. rays parallel utilization aggregated. resulting problem still inﬁnitely many states optimal policy estimated accurately using ﬁnite state approximation applying model minimization approach require prior knowledge task parameters often unavailable practice. paper reinforcement learning integrate model policy estimation. important question much experience necessary trust learned policies. address question deriving bound sample complexity obtaining near-optimal policy. best knowledge ﬁrst guarantee problems inﬁnite state spaces unbounded costs. principle uniﬁes many successful methods eﬃcient exploration optimism face uncertainty presented choice actions similar estimated value methods using principle tend select action tried less frequently. optimism take form optimistic initialization i.e. bootstrapping initial approximations value function large values interval estimation techniques instead bias action selection towards exploration maintaining conﬁdence intervals model parameters value estimates interval estimation techniques developed solving single-state bandit problems extended general setting treating state distinct bandit problem. heuristic exploration strategies often employed relative simplicity. \u0001-greedy exploration boltzmann action selection methods randomization strategies bias action selection toward exploitation. perhaps commonly used strategy \u0001-greedy exploration simply chooses action uniformly random probability epoch otherwise selects apparent best action. decaying appropriately strategy asymptotically approaches optimal policy interested quantifying sample complexity learning good policies terms number observations necessary compute nearoptimal policy high probability i.e. probably figure utilization state model twotask problem instance. stochastically transitions right deterministically transitions upward. dashed indicates utilization target. zero vector except component equal i.e. executing task alters dimension system state. cost state l-distance target utilization within hyperplane states equal elapsed time figure illustrates utilization state model problem tasks target utilization target utilization deﬁnes target utilization components rational regularly passes many utilization states. figure example utilization passes integer multiples every state zero cost states displacement target utilization equal cost. consider sample complexity estimating near-optimal policy high conﬁdence bounding number value exploratory actions taken analysis proceeds three parts. first derive bounds value estimation error function model accuracy. next determine number observations needed guarantee model accuracy. finally results determine many observations suﬃce arrive near-optimal policy high certainty. focus estimating state-action value function denote optimal state value function denote state-action value function estimated transition dynamics establish main result constraining sample complexity learning scheduling domain ﬁrst provide following simulation lemma proven appendix. lemma constant tasks result serves identical role simulation lemma kearns singh relating model estimation error value estimation error. bound replaces quadratic dependence number states result dependence wcet consistent observations indicating sample complexity obtaining good approximation depend polynomially number parameters transition model general mdps scheduling domain. theorem provides bound number observations needed arrive accurate estimate value function. sake simplicity assume balanced wandering here result easily used guide oﬄine modeling well employed online learning. theorem balanced exploration approximately correct learning kakade considered question learning mdps detail. several reinforcement learning algorithms developed including r-max mbie algorithms limited ﬁnite state case assume bounded rewards. metric learner mdps continuous compact state spaces. consider diﬃcultly learning good scheduling policies section. approach question analytically empirically. section derive bound balanced wandering approach exploration scheduling domain. result novel extends results derived ﬁnite-state bounded cost setting domain countably inﬁnite state space unbounded costs. results rely speciﬁc lipschitz-like condition restricts growth rate value function cost function ﬁnite support duration distributions i.e. ﬁnite worst-case execution times tasks. section present results simulations comparing alternative exploration strategies. estimate task duration distributions using empirical probability measure. suppose collection observations task quanta decision epoch number observations involving task number observations quanta proof. according lemma model accuracy suﬃcient guarantee thus demonstrating bound equation matter guaranteeing high certainty near speciﬁcally require theorem provides bound number observations needed learn ε-approximation however principally interested discovering number observations need trust learned policies. corollary establishes sample complexity using balanced complexity learn good scheduling policies. corollary assuming action tried equal number times classical result singh demonstrates that general policy greedy respect value function approximation within optimal. corollary follows noting require substituting constraint theorem establishes corollary. number actions. unlike bounds general mdps dependence number states; instead complexity learning determined worst-case execution time result similar bounds relocatable action models state space partitioned relatively small number classes. transition models generalized among states class sample complexity learning depends number classes rather number states. scheduling special case relocatable action model class states. relocatable action models used address inﬁnite state spaces existing sample complexity results address unbounded reward case. able handle unbounded costs taking advantage slow growth rate value function relative discount factor. speciﬁcally distance consecutive states bounded costs grow polynomially distance resource share target since costs exponentially discounted value particular state ﬁnite. observations enable bound lemma suggesting sample complexity bounds possible general inﬁnite state unbounded cost models long number classes ﬁnite individual state values bounded. course results useful good policies must represented compactly possible scheduling domain considered generally case. bound previous section gives sense ﬁnite sample performance learning good policy; however requires several simplifying assumptions balanced wandering bound tight. practice alternative exploration strategies yield better performance bound would indicate. compare performance several exploration strategies context task scheduling problem conducting experiments comparing \u0001-greedy balanced wandering interval-based exploration strategy. interval-based optimistic exploration conﬁdence intervals derived multi-armed bandit case even-dar successive elimination algorithm. algorithm constructs intervals experiments \u0001-greedy random selection rate decision epoch varying values strategy always exploits balanced wandering simply executes task ﬁxed number times prior exploiting. vary parameter determine impact learning rate. strategy always exploits current model knowledge. compare performance exploration strategies generated random problem instances tasks. duration distributions tasks generated ﬁrst selecting worst-case execution time uniformly random interval choosing normal distribution mean variance selected uniformly random respective intervals distribution truncated discretized interval utilization targets task chosen according integers selected uniformly random used discount factor tests. order avoid enumerating arbitrarily large numbers states reinitialized state whenever state cost greater encountered. high cost states treated absorbing states approximate model avoid degenerate policies exploit reset. report number mistakes number times exploration strategy chooses suboptimal action value results experiments shown figure figure report conﬁdence intervals mean number mistakes exploration strategy makes averaged across problem instances described above. note plots diﬀerent scales variation mistake rates among exploration strategies. figure compares performance intervalbased optimistic action selection exploit policy greedily follows optimal policy approximate model decision epoch. interval-based exploration settings considered exhibited statistically similar performance. interestingly exploitive strategy yields better performance explorative strategies despite lack explicit exploration mechanism. observation holds true ε-greedy exploration balanced wandering well. figure illustrates performance \u0001-greedy exploration. notice mistake rate decreases along likelihood taking exploratory actions approaches zero. explicit exploration improve performance domain. supported results balanced wandering. theory behind balanced wandering making initial mistakes early long uniformly accurate models. figure shows case scheduling domain transition model estimation error bounded maximum worstcase execution time among tasks. introduce lemmas prior demonstrating result. ﬁrst provides bound expected successor state value function lipschitz-like speed limit growth. subsequent lemmas establish costs values exhibit property. lemma suppose distributions function satisﬁes results suggest exploitive strategy best available exploration method task scheduling problem domain. plausible explanation environment enforces rational exploration task never dispatched system enter progressively costly states task becomes underused. thus eventually estimated beneﬁt running task substantial enough exploitive strategy must interesting note explorative policies considered quite mistake rates despite tight threshold used distinguish suboptimal actions. paper considered problem learning near-optimal schedules system model fully known advance. presented analytical results bound number suboptimal actions taken prior arriving near-optimal policy high certainty. interestingly transition system’s portability results bounds similar estimating underlying model single state. naturally leads comparison multiarmed bandit model single state several available actions. action causes emission reward according corresponding unknown stationary random process. however bandit model appear apply directly duration distributions stationary processes invariant states payoﬀ associated action state-dependent. focused model learning rather deriving bounds regret loss value incurred suboptimal behavior learning regret bounds translate readily guarantees transient real-time performance eﬀects learning guarantees regarding cost translate guarantees task timeliness. presented empirical results suggest learner always exploits current information outperforms agents explicitly encourage exploration domain. occurs policy consistently ignores action progressively farther utilization target resulting arbitrarily large costs. thus domain appears enforce appropriate level exploration perhaps", "year": 2012}