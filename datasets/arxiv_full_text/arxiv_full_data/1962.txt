{"title": "X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets", "tag": ["stat.ML", "cs.AI", "cs.CV"], "abstract": "In this paper we propose cross-modal convolutional neural networks (X-CNNs), a novel biologically inspired type of CNN architectures, treating gradient descent-specialised CNNs as individual units of processing in a larger-scale network topology, while allowing for unconstrained information flow and/or weight sharing between analogous hidden layers of the network---thus generalising the already well-established concept of neural network ensembles (where information typically may flow only between the output layers of the individual networks). The constituent networks are individually designed to learn the output function on their own subset of the input data, after which cross-connections between them are introduced after each pooling operation to periodically allow for information exchange between them. This injection of knowledge into a model (by prior partition of the input data through domain knowledge or unsupervised methods) is expected to yield greatest returns in sparse data environments, which are typically less suitable for training CNNs. For evaluation purposes, we have compared a standard four-layer CNN as well as a sophisticated FitNet4 architecture against their cross-modal variants on the CIFAR-10 and CIFAR-100 datasets with differing percentages of the training data being removed, and find that at lower levels of data availability, the X-CNNs significantly outperform their baselines (typically providing a 2--6% benefit, depending on the dataset size and whether data augmentation is used), while still maintaining an edge on all of the full dataset tests.", "text": "abstract—in paper propose cross-modal convolutional neural networks novel biologically inspired type architectures treating gradient descent-specialised cnns individual units processing larger-scale network topology allowing unconstrained information and/or weight sharing analogous hidden layers network— thus generalising already well-established concept neural network ensembles constituent networks individually designed learn output function subset input data cross-connections introduced pooling operation periodically allow information exchange them. injection knowledge model expected yield greatest returns sparse data environments typically less suitable training cnns. evaluation purposes compared standard four-layer well sophisticated fitnet architecture cross-modal variants cifar- cifar- datasets differing percentages training data removed lower levels data availability x-cnns signiﬁcantly outperform baselines still maintaining edge full dataset tests. recent years number success stories machine learning seen all-time rise across wide range ﬁelds tasks examples including computer vision speech recognition reinforcement learning guiding monte carlo tree search unifying idea behind deep learning utilisation neural networks many hidden layers purposes learning complex feature representations data rather relying handcrafted feature extraction. networks become deeper however become reliant amount training examples provided maximising performance. able extract large quantities labelled information many problems interest remains signiﬁcant proportion tasks data simply isn’t available time makes extremely difﬁcult fully exploit deep architecture properly learn generalisable features data. present architectural methodology attempts extract additional predictive power convolutional neural network circumstances instead focussing width data i.e. heterogeneity information present within training example. idea constitutes appropriate partitioning information training smaller cnns partitions allowing information exchange various stages classic example approach bound useful clinical studies typically many patients patient potentially heterogeneous wealth information various test results patient history ethnic background body scans depending type study. methodology inspired multilayer networks mathematical structures encompassing several layers graphs nodes allowing unrestricted intralayer well inter-layer connections. demonstrably valuable tool modelling variety natural social systems applicability machine learning already demonstrated authors managing achieve high performance sparse breast cancer classiﬁcation dataset involving gene expression methylation data. network design process initiated appropriately partitioning input data—this done either manually unsupervised pre-training step determine fragments input data likely constructively inﬂuence another. afterwards cross-modal constructed separate superlayer dedicated partition input data attempting learn target function partition only. purpose partitioning help constituent cnns become powerful predictors requiring smaller dimensionality input data allowing access fig. diagram simple cross-modal image classiﬁcation generated baseline form tmax. three channels input image receives superlayer cross-connections inserted pooling operation full weight sharing fully connected layers. in-depth view potential cross-connection layout provided figure finally superlayers interconnected sort cross-connection best seen combined arbitrary ways output stage produce ﬁnal output. similarly stage weights superlayers shared—the simplest case explore analysis constitutes complete weight sharing fully connected layers tail networks. construction biologically inspired cross-modal systems within visual auditory systems human brain —wherein several cross-connections various sensory networks discovered quantify gains approach evaluation focusses already well-understood problem coloured image classiﬁcation established cifar-/ benchmarks abundance data available easier investigate effects restricting size training various models. partitioning input consider per-channel—each three image channels input individual superlayer superlayers identical high-level architecture illustrated fig. also allows simple approach crossconnections; namely every downsampling operation allow feature maps exchanged superlayers passed another convolutional layer ensemble’s constituent models typically exchange information output stage cross-modal framework allows arbitrary information stage processing pipeline; constituent models ensemble usually receive full copy input each superlayers within cross-modal neural network receive fraction input allowing decrease degrees-of-freedom model compared unrestricted network. fact taken step further consider ensembles cross-modal cnns compound beneﬁts already given x-cnns themselves examples networks potentially struggling choose proper class sufﬁcient conﬁdence. x-cnn model observed ordinary high level ensemble strategies found useful cnns useful x-cnns well. lastly noted approach restricted cnns easiest scrutinise trained parameters bound obey certain spatial structure. line this entire section manuscript dedicated analysing learned convolutional kernels within xcnn well visualising inputs would maximise model’s cross-connection activations. purposes evaluating proposed architecture’s performance implemented baseline models— along cross-modal variants—in keras back-end). purposes reproducibility fig. illustration single cross-connection segment within x-cnn superlayers. pooling operation exchange feature maps superlayers ﬁrst passing additional convolutional layer. also perform additional intra-superlayer convolution merging feature maps superlayer concatenation. section expose architectures hyperparameters used evaluation. cross-modal variants’ feature counts altered make overall number parameters close possible baseline making fair evaluation respect degreesof-freedom. models used represent images colour space. linear transformation impact performance baselines beneﬁt decoupling luminance chrominance allowing simpler analysis cross-connections inject domain knowledge model favouring superlayer corresponding channel terms feature counts corresponds assumption majority relevant information object contained within brightness channel colour usually represents auxiliary information. initial model choice represents simple four convolutional relu layers followed fully connected layers also relu. referring kerasnet throughout manuscript based keras cifar- example represents likely style starting model going attempt apply image classiﬁcation problem perhaps especially bearing mind training data sparse. decided implement fitnet romero second baseline representing sophisticated close state-of-the-art cifar-/. opted model prominently featured variety recent neural networks research design goal thin&deep network managing keep parameter count relatively compared many successful models fitnet consists convolutional -way maxout layers followed fully connected layers ﬁrst -way maxout layer. full architecture model—as well cross-modal variant presented table models initialised using xavier initialisation trained epochs using adam optimiser batch size applied batch normalisation output hidden layer signiﬁcantly accelerate training procedure. regularisation applied weights model. finally dropout applied input every pooling operation fully connected maxout layer. verify insights utilised well-known image classiﬁcation benchmark datasets cifar- cifar- abundance data available makes easier study behaviour considered cnns different fractions training data discarded. hypothesise that lower levels data availability validity claim investigated performing comparative evaluation kerasnet fitnet baselines x-kerasnet x-fitnet respectively. individual test evaluate accuracy four models entire test samples training routine presented entire training dataset schedule tests follows initially test increments reaching afterwards test increments either reaching accuracies models within whichever later; specially always test training dataset. images preprocessed applying single batch normalisation operation them; found yield slightly better results compared global contrast normalisation whitening finally given depending task sometimes possible signiﬁcantly enhance results sparse environment data augmentation tests twice—with without random translations horizontal reﬂections applied training images— providing insight whether data augmentation compounds effects cross-modal architecture extent. results tests without data augmentation completely line claim section sufﬁciently training data sizes x-kerasnet x-fitnet signiﬁcantly outperform respective baselines testing cifar-/ datasets. cifar- threshold baselines catch around corresponding training examples available. furthermore cifar- threshold never reached likely extreme sparsity perclass examples making problem particularly suitable x-cnn models; exception scenario fitnet data sparsity probably extreme deep model reach potential. edge baselines—outperforming full training dataset experiments sometimes signiﬁcantly. naturally invites conclusion converting x-cnn always reasonable step; yield signiﬁcant beneﬁts rarely making performance signiﬁcantly worse. verify claim performed experiments full datasets monitored testing accuracy evolves function training epoch. resulting plots summarised figure clear x-cnns least powerful baselines even full training sets available. furthermore possible detect narrow edge cross-modal models cifar- experiments signiﬁcant edge cifar- experiments. concluding remark even dataset investigation sparse attempting utilise cross-modal variant considered models reasonable action might yield noticeable returns predictive power. analysis interplay data augmentation cross-modal networks cifar- remains straightforward—the x-cnn models remaining consistently signiﬁcantly ahead baselines throughout entire spectrum training sizes. cifar- however slightly complicated; catch-up threshold gets expectedly decreased behaviour x-cnns smaller training sizes always signiﬁcantly compound beneﬁts data augmentation. speciﬁcally training fitnet model manages outperform x-fitnet possible cause phenomenon note that data availability level fitnet models signiﬁcantly inferior performance kerasnet models signiﬁcant beneﬁt usage x-cnns. takeaway lesson that cross-modal architecture need always compound nicely data augmentation occurrence event could signify baseline particularly suitable properly accommodating data augmentation training size ﬁrst place. happens attempt suitable/shallower cnn—the x-cnn variant produce desired beneﬁts. element x-cnn architecture crossconnection layers enable information individual channels. therefore interest understand visualise mode operation layers. visualisations section correspond learned weights fully training cifar- data augmentation. finally taken advantage smaller training sizes perform statistical signiﬁcance tests typically scarce deep learning literature. training sizes trained models times performed t-tests choosing signiﬁcance threshold. ﬁndings show that assumptions best-performing x-cnn ﬁrst demonstrate cross-connections inserted considered models though convolutions learn complex functions simple feature passing. first note weights convolutional layer represented table maps input channels output channels fig. weight visualisation ﬁrst-level cross-connection layer x-fitnet cnn. columns correspond input channels rows correspond output channels. green colour indicates positive-weight connection input channel output channel blue colour indicates negative-weight connection. colour intensities proportional absolute weight values. bottom left-to-right rather displaying table values decided visualise weights heatmap style; figure showcases visualisation ﬁrst cross-connection layer x-fitnet. green colours indicate input channel positive connection weight respective output channel blue colours indicate negative weights. colour intensities proportional absolute weight values. seen output channel cross-connection layer obtained nontrivial weighted combination input channels. hypothesise cross-connection layers selectively ﬁlter combine input features utilisable another processing stream. delve deeper features crossconnection layers ﬁltering combining passing applied layer-wise feature-map activation techniques proposed simonyan technique performs gradient ascent white-noise input image maximise activations speciﬁc channel feature maps layers within pre-trained model. objective function gradient ascent deﬁned input image activation considered neuron provided input regularisation factor. iterating number gradient ascent steps original white-noise image modiﬁed patterns approximate detection function speciﬁc neuron. lower-level convolutional layers well-known learn ﬁlters approximating gabor wavelet ﬁlters edge detectors corner detectors etc; conﬁrm experiments indeed case. ﬁrst crossconnection layer x-fitnet visualised selection channel activations figure visualisation indicates cross-connection layer indeed passing combined fig. artiﬁcially generated images cause strong activations speciﬁc channels ﬁrst cross-connection layer xfitnet model. three channels cross-connections. middle three channels cross-connections. bottom three channels cross-connections. lower-level features addition horizontal vertical stripes upper right image ﬁgure. observe pattern frequency channel’s crossconnection layer higher layers. observation reﬂects fact human vision system able detect higher frequency variations intensity chrominance. solid indicator x-cnn architecture faced image classiﬁcation task colour scheme actually attempting mimic human vision. ﬁnal analysis focusses x-kerasnet model transformed feature maps arbitrary depths images colour-mapping scheme. figure shows feature maps inputs outputs cross connections representative images truck airplane classes. easier comparatively analyse x-kerasnet cross-connection layers alter number feature maps therefore colourmapping scheme remained meaningful both. observe cross-connection output maps background features emphasised features de-emphasised— indicates cross-connection layers performing complex inter-superlayer feature integration simply passing feature maps superlayers. introduced cross-modal convolutional neural networks novel architecture decouples convolutional processing input partitions allowing periodical information veliˇckovi´c li`o molecular multiplex network inference using gaussian mixture hidden markov models journal complex networks available http//comnet.oxfordjournals.org/ content/early////comnet.\\cnv.abstract eckert kamdar chang beckmann greicius menon cross-modal system linking primary auditory visual cortices evidence intrinsic fmri connectivity analysis human brain mapping vol. beer plank greenlee diffusion tensor imaging shows white matter tracts human auditory visual cortex experimental brain research vol. available http//dx.doi.org/./s---y yang yang tang takahashi effects sound frequency audiovisual integration event-related potential study plos vol. available http//dx.doi.org/.%fjournal.pone. theano development team theano python framework fast computation mathematical expressions arxiv e-prints vol. abs/. available http//arxiv.org/abs/ fig. visualisation input output feature maps crossconnection layer x-kerasnet. left input images middle input feature maps cross-connection layer. right output feature maps cross-connection layer. tween processing pipelines order achieve performance improvements sparse data environments. applied methodology popular cifar-/ image classiﬁcation datasets baseline models managing signiﬁcantly outperform low-data environments remaining competitive high-data environments— outperforming full-dataset experiments. aside reinforcing claim x-cnn architecture beneﬁcial baseline model veriﬁed introduced cross-connection layers perform rather complex functions capable mimicking human vision processes—conﬁrming biological inspiration behind model justiﬁed. krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural networks advances neural information processing systems pereira burges bottou weinberger eds. curran associates inc. available http//papers.nips.cc/paper/ -imagenet-classiﬁcation-with-deep-convolutional-neural-networks. hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot mastering game deep neural networks tree search nature vol.", "year": 2016}