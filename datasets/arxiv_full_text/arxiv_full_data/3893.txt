{"title": "Fast Dempster-Shafer clustering using a neural network structure", "tag": ["cs.AI", "cs.NE", "I.2.3; I.2.6; I.5.3"], "abstract": "In this paper we study a problem within Dempster-Shafer theory where 2**n - 1 pieces of evidence are clustered by a neural structure into n clusters. The clustering is done by minimizing a metaconflict function. Previously we developed a method based on iterative optimization. However, for large scale problems we need a method with lower computational complexity. The neural structure was found to be effective and much faster than iterative optimization for larger problems. While the growth in metaconflict was faster for the neural structure compared with iterative optimization in medium sized problems, the metaconflict per cluster and evidence was moderate. The neural structure was able to find a global minimum over ten runs for problem sizes up to six clusters.", "text": "paper study problem within dempster-shafer theory pieces evidence clustered neural structure clusters. clustering done minimizing metaconflict function. previously developed method based iterative optimization. however large scale problems need method lower computational complexity. neural structure found effective much faster iterative optimization larger problems. growth metaconflict faster neural structure compared iterative optimization medium sized problems metaconflict cluster evidence moderate. neural structure able find global minimum runs problem sizes clusters. paper study neural structure clustering evidence large scale problems within dempster-shafer theory studied problem concerns situation reasoning multiple events handled independently. clustering process separate evidence subsets handled separately. earlier work developed method based iterative optimization clustering evidence medium sized problems. method developed part multiple-target tracking algorithm antisubmarine intelligence analysis system subsequent paper developed classification method incoming pieces evidence. here used prototypes order obtain faster classification. prototypes derived previous clustering process. method further increased computation speed iterative optimization small medium sized problems little larger problems. solution described paper based clustering neural structure. neural network learning weights network. instead weights directly method conflict dempster’s rule input setting weights. many ideas paper inspired solution traveling salesman problem hopfield tank used neural network effective method find good shortest path several cities. paper denœux also combines neural networks dempster-shafer theory. problem different. uses four layer feed-forward neural network classify pattern belonging classes using known prototype vectors comparison. final layer neural network performs dempster’s rule yielding classification. section describe problem hand section give overview iterative optimization solution developed describe neural structure achieve effective clustering presenting comparison neural structure iterative optimization receive several pieces evidence different separate events pieces evidence mixed want arrange according event referring thus partition pieces evidence subsets subset refers particular event. figure subsets denoted conflict pieces evidence combined dempster’s rule denoted here thirteen pieces evidence partitioned four subsets. number subsets uncertain also domain conflict conflict current hypothesis number subsets prior belief. partition simply allocation pieces evidence different events. since events anything other analyze separately. uncertain event pieces evidence referring problem. could impossible know directly different pieces evidence referring event. know subset not. problem problem organization. evidence different events want analyze unfortunately mixed facing problem separating them. solve problem conflict dempster’s rule pieces evidence within subset combined indication whether pieces evidence belong together. higher conflict belong together. create additional piece evidence subset proposition adequate partition. simple frame discernment metalevel short adequate partition. proposition take value equal conflict combination within subset pieces evidence regarding subset reason partition original evidence. confuse original evidence evidence metalevel evidence combination analysis combination take place metalevel figure establish criterion function overall conflict called metaconflict function reasoning multiple events. metaconflict derived plausibility partitioning correct conflict subset viewed piece metalevel evidence partitioning evidence minimizing metaconflict function method partitioning evidence subsets representing events. method also handle situation number events uncertain. method finding best partitioning based iterative minimization metaconflict function. step consequence transferring piece evidence subset another investigated. remembered analysis concerns situation piece evidence transferred subset another. favorable simultaneously transfer pieces evidence deemed favorable individual transfer. thus always global minimum metaconflict function equal zero since take pieces evidence includes –element cluster remaining evidence take includes –element subset forth. since evidence cluster includes –element intersection nonempty evidence cluster includes –element intersection also nonempty etc. thus conflicts zero always global minimum makes easy standard efficiency clustering process. reason choose problem minimum metaconflict zero makes good test example evaluating performance. another problem used would knowledge global minimum evaluation would difficult. reason believe choice test examples atypical respect network performance. choose architecture minimizes sum. thus make change function want minimize. take logarithm minus metaconflict function change minimizing minimizing sum. study calculations taking place neural network iteration. terminology hopfield tank input voltages weighted input signals neuron output voltages output signal neuron inhibition terms negative weights. previous input voltage previous iteration plus gain factor times weighted output voltages neurons column plus excitation bias minus previous input voltage nmn. iteration voltages calculated results previous iteration. continues convergence reached. long weights neural network symmetric convergence always guaranteed. always case since factor varies conflict pieces evidence. thus weights equal. first assure output voltages neurons decrease iteration. could possibly lead piece evidence corresponding clustered all. happens output voltages decreased least unchanged. control plus fact logical conditions data-terms column neuron makes problem easier hopfield tank’s model traveling salesman problem. logical conditions column plus data-terms previous next columns neuron. allows avoid problems convergence performance described wilson pawley figure different states neural network neurons. left right convergence clustering pieces evidence five clusters first eleventh iteration. snap-shot iteration five columns represent cluster rows represent piece evidence. linear dimension square proportional output voltage neuron represent degree pieces evidence belong cluster. final state output voltage four output voltages piece evidence represented clustered cluster output voltage secondly check highest output voltages row. highest output voltage greater equal output voltages second highest output voltage regardless value highest output voltage highest output voltage done merely speed convergence. section investigate clustering performance computation time clustering processes neural structure iterative optimization. make comparison probproblem sizes clustering pieces evidence subsets. reported evidence support different subsets frame thus know metaconflict function global minimum metaconflict equal zero. table notice iterative optimization exponential computation time number items evidence. neural structure much lower complexity although higher computation time small problems. problems five subsets pieces evidence iterative optimization fastest subsets pieces evidence neural structure vastly superior table listed best median mean metaconflict different runs different random initial partitions evidence different random initial input voltages iterative optimization neural structure respectively. find best different runs methods manages find global optimum problem sizes three subsets. however also notice median mean minimum metaconflict much higher neural structure iterative optimization median conflict cluster tabulated table figure grows much slower total metaconflict. thus large part growth metaconflict depends increased number clusters whose conflicts becomes additional terms metaconflict function. study best clustering seven cluster problem. find conflicts respectively seven different clusters. median conflict cluster. cluster contains pieces evidence. contains –element. remaining support respectively. least elements also present piece evidence. pieces evidence conflict thus pairs evidence cluster pairs conflict. small price obtain effective clustering. chosen random selected items could expected pairs conflict demonstrated neural structure effective clustering evidence large scale problems. trials pieces evidence clustered clusters neural structure faster iterative optimization problems clustering pieces evidence clusters larger. best runs found global optimum methods problem sizes clusters median metaconflict higher neural structure. however since good best found median conflict cluster evidence moderate deemed acceptable.", "year": 2003}