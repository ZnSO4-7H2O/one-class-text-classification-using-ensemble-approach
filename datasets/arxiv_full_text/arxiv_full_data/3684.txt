{"title": "Probabilistic Numerics and Uncertainty in Computations", "tag": ["math.NA", "cs.AI", "cs.LG", "stat.CO", "stat.ML"], "abstract": "We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return uncertainties in their calculations. Such uncertainties, arising from the loss of precision induced by numerical calculation with limited time or hardware, are important for much contemporary science and industry. Within applications such as climate science and astrophysics, the need to make decisions on the basis of computations with large and complex data has led to a renewed focus on the management of numerical uncertainty. We describe how several seminal classic numerical methods can be interpreted naturally as probabilistic inference. We then show that the probabilistic view suggests new algorithms that can flexibly be adapted to suit application specifics, while delivering improved empirical performance. We provide concrete illustrations of the benefits of probabilistic numeric algorithms on real scientific problems from astrometry and astronomical imaging, while highlighting open problems with these new algorithms. Finally, we describe how probabilistic numerical methods provide a coherent framework for identifying the uncertainty in calculations performed with a combination of numerical algorithms (e.g. both numerical optimisers and differential equation solvers), potentially allowing the diagnosis (and control) of error sources in computations.", "text": "deliver call arms probabilistic numerical methods algorithms numerical tasks including linear algebra integration optimization solving differential equations return uncertainties calculations. uncertainties arising loss precision induced numerical calculation limited time hardware important much contemporary science industry. within applications climate science astrophysics need make decisions basis computations large complex data renewed focus management numerical uncertainty. describe several seminal classic numerical methods interpreted naturally probabilistic inference. show probabilistic view suggests algorithms ﬂexibly adapted suit application speciﬁcs delivering improved empirical performance. provide concrete illustrations beneﬁts probabilistic numeric algorithms real scientiﬁc problems astrometry astronomical imaging highlighting open problems algorithms. finally describe probabilistic numerical methods provide coherent framework identifying uncertainty calculations performed combination numerical algorithms potentially allowing diagnosis error sources computations. authors. published royal society terms creative commons attribution license http//creativecommons.org/licenses/ by/./ permits unrestricted provided original author source credited. introduction probability theory quantitative framework scientiﬁc inference codiﬁes observations combine modelling assumptions give evidence hypothesis values unknown quantities. continuing debate prior assumptions chosen validated role probability language uncertainty rarely questioned. long subject inference physical variable. quantity question mathematical statement solution computational task? make sense assign probability measure probability statistics remove noise signal seems misguided apply deterministic mathematical problem. noise stochasticity difﬁcult deﬁne precisely. probability theory rest notion randomness extends quantifying epistemic uncertainty arising solely missing information. connections deterministic computations probabilities long history. erdös showed number distinct prime factors integer follows normal distribution. statement precise useful analysis factorization algorithms even though difﬁcult sample integers. meaningful without appealing concept epistemic uncertainty. probabilistic deterministic methods inference physical quantities shared dualities early legendre introduced method leastsquares deterministic best data without probabilistic interpretation. gauss’ probabilistic formulation exact method added generative stochastic model data might assumed arisen. legendre’s least-squares useful method without generative interpretation gaussian formulation adds important notion uncertainty would later become crucial areas like study dynamical systems. several authors shown language probabilistic inference applied numerical problems using notion uncertainty result intractable incomplete computation giving rise methods call probabilistic numerics. methods uncertainty regularly arises solely lack information inherent solution intractable problem quadrature method example access ﬁnitely many function values integrand; exact answer would principle require inﬁnitely many numbers. algorithms problems like integration optimization proceed iteratively iteration providing information improving running estimate correct answer. probabilistic numerics provides methods that place estimates update probability measures space possible solutions. diaconis noted appears poincaré proposed approach already century. recent explosive growth automated inference increasing importance numerics science given idea urgency. article connects recent results promising applications central questions probabilistic numerics. collate results showing number basic popular numerical methods identiﬁed families probabilistic inference procedures. probability measures arising interpretation established methods offer improved many resources discussing epistemic uncertainty understandinguncertainty.org fact connection least-squares estimation gaussian inference re-discovered repeatedly simply least-squares estimation re-discovered repeatedly legendre names like ridge regression kriging tikhonov’s method fundamental connection normal distribution exponential square norm. exponential monotonic function minimizing -regularized loss equivalent maximizing product gaussian prior likelihood. sense paper adding numerical mathematicians list another group re-discoverers gaussian inference. ironically list includes gauss himself gaussian elimination introduced paper gaussian distribution interpreted conjugate-direction method thus gaussian regression also probabilistic numerics community site found http//www.probabilistic-numerics.org. performance enticing functionality conceptual clarity; demonstrate examples drawn astrometry computational photography. article closes pointing propagation uncertainty computational pipelines guiding goal probabilistic numerics. helpful separate issues discussed clearly areas overlap statistics numerical mathematics focus well-posed deterministic problems identifying degrees uncertainty arising computation itself. contrast notion uncertainty quantiﬁcation identiﬁes degrees freedom ill-posed problems epistemic uncertainty arises set-up computation rather computation itself. also differs several concepts stochastic numerical methods random numbers either quantify uncertainty repeated computations reduce computational cost randomly chosen projections also important note that matter course existing frameworks already analyse estimate error created numerical algorithm. theoretical analysis computational errors generally yields convergence rates—bounds unknown constant—made certain structural assumptions. probabilistic numerical methods derived classic ones described below naturally inherit analytical properties. runtime error also estimated speciﬁc problem instance. run-time error estimation frequently performed monitoring dynamics algorithm’s main estimate similarly optimization problems magnitude gradient often used monitor algorithm’s progress. error estimates informal solution estimates computed numerical method itself. meant used locally mostly criteria termination method adaptation internal parameters. typically interpreted property posterior probability measure thus communicated algorithms thus embedded larger framework error estimation. also usually inform design numerical algorithm itself; instead diagnostic tool added post-hoc. below argue estimation errors given formal framework probability theory uniquely suited task. describing numerical computations inference latent quantity yields joint consistent framework construction solution error estimates. inference perspective provide natural intuition suggest extensions improvements. probabilistic framework provides lingua franca numerical computations allows communication uncertainty methods chain computation. probabilistic numerical methods classical ones numerical algorithms estimate quantities directly computable using results readily available computations. even existing numerical methods thus seen inference rules reasoning latent quantities observables data. face value connection inference computation vague. several recent results shown made rigorous established deterministic rules various numerical problems understood maximum a-posteriori estimates speciﬁc priors likelihoods recurring picture one-to-many relationship classic numerical method speciﬁc task family probabilistic priors give maximum posterior estimate differing measures uncertainty. choosing member family amounts ﬁtting uncertainty task call uncertainty calibration. result process numerical method returns point estimate surrounded probability measure uncertainty point estimate inherits proven theoretical properties classic method uncertainty offers functionality. term probabilistic numeric approach quadrature bayesian quadrature. diaconis ﬁrst point clear connection gaussian process regression model deterministic quadrature rule observation subsequently generalised wahba o’hagan also noted rasmussen ghahramani details found works; construct intuitive example highlighting practical challenges assigning uncertainty result computation. concreteness consider exp\u0001− sin− evidently compact symbolic form computed virtually nanoseconds. wholly deterministic object. nevertheless real simple analytic value sense natively evaluated low-level code. quadrature rules offer black estimates rules optimized heavily could almost called low-level results come strict error bounds ﬂoating-point operations; instead assumptions necessary bound error. perhaps simplest quadrature rule trapezoid rule amounts linear interpolation probabilistic description requires joint probability distribution since lies inﬁnite-dimensional banach hilbert space lebesgue measure deﬁned. gaussian process measures well-deﬁned spaces offer powerful framework quadrature particular choose model p=gp gaussian process vanishing mean linear spline covariance function restriction ﬁnitely many evaluations jointly gaussian distributed zero mean covariance cov\u0001f samples drawn process shown grey left figure sample paths represent hypothesis class associated gaussian process prior continuous differentiable mean square expectation gaussian processes closed linear projections distribution identiﬁed corresponding univariate gaussian distribution observations incorporated. measure conditioned collected function p\u0001f\u0000 y\u0001=n\u0004f symmetric positive deﬁnite matrix elements kij= ﬁnal expression brackets posterior covariance approximately integrated using different gaussian process priors giving posterior distributions mean estimates. gray lines functions sampled prior. thick coloured line posterior mean thin lines posterior samples delineation marginal standard deviations. shading represents posterior probability density. bottom number evaluation points increases posterior mean converges true integral value; note rapid convergence exponentiated quadratic prior. posterior covariance provides error estimate whose scale deﬁned posterior mean alone meaningful error estimate matched well function’s actual properties. left plot shows systematic difference convergence real error convergence estimated error linear spline whereas convergence estimated error exponentiated quadratic prior better calibrated real error. grey evaluations. right columns experiment repeated function drawn spline kernel prior. function trapezoid rule optimal statistical estimator integral gaussian kernel strongly over-conﬁdent. centred evaluation points constrained likelihood pass values nodes thus posterior mean linear spline interpolant evaluations posterior mean equation exactly equal trapezoid-rule estimate bayesian quadrature linear spline prior provides probabilistic interpretation trapezoidal rule; supplements estimate full probability distribution characterising uncertainty. corresponding conditional gaussian process whose mean piecewise linear function left figure regularly interpreted estimate imprecision mean estimate. fact error estimate analogous deterministic linear convergence bound continuous functions. family error bounds associated posterior mean line corresponding different value unknown constant bound different values parameters covariance. seem though probabilistic interpretation added nothing since view identiﬁes quadrature rules varying assumptions parameter choices gaussian process regression embeds seemingly separate rules hierarchical space models models good error modelling selected hierarchical probabilistic inference. done without collecting additional data-points generally probabilistic numeric viewpoint provides principled manage parameters numerical procedures. markov chain monte carlo procedures might require hand-tuning parameters step sizes annealing schedules bayesian quadrature allows machinery statistical inference procedure brought bear upon parameters. practical example beneﬁts approximate bayesian inference parameters bayesian quadrature procedure given function used example much smoother typical functions gaussian process prior distribution associated trapezoid rule second column figure shows analogous experiments strong smoothness assumption giving quickly converging estimate. case error bars provided standard deviation converge qualitatively comparable way. course faster convergence quadrature rule based exponentiated quadraticcovariance prior universal property. effect much stronger prior assumptions. true integrand rougher expected prior quadrature estimate arising prior quite wrong. right columns figure show analogous experiments integrand true sample gaussian process spline covariance case spline prior optimal statistical estimator construction error estimate perfectly calibrated exponentiated quadratic-kernel gives over-conﬁdent inefﬁcient estimates identifying optimal regression model larger class based collected function values requires computational work regressor start. also gives better calibrated uncertainty. contemporary general-purpose quadrature implementations remain lightweight recursively re-using previous computations. experiments show possible design bayesian quadrature rules well-calibrated posterior error estimates remains question small computational overhead probabilistic computation methods made. even formulating quadrature probabilistic regression precisely captures trade-off prior assumptions inherent computation this well-known result integrand differentiable rather continuous trapezoid rule quadratic convergence rate initial behaviour lines ﬁgure function scale relates assumptions rate asymptotic quadratic convergence approached. computational effort required computation achieve certain precision. computational rules arising strongly constrained hypothesis class perform much better less restrictive rules prior assumptions valid. numerical setting—in contrast many empirical situations statistics—it often possible precisely check whether particular prior assumption valid machine performing computation access least principle formal complete description task form source code describing task using source code example test runtime whether many times integrand continuously differentiable using information future allow design improved quadrature methods. knows general purpose quadrature methods effectively gaussian process prior function values natural whether prior actually incorporates salient information available one’s speciﬁc problem. including information prior leads customized tailored numerical methods perform better. probabilistic inference also furnishes framework required tackle numerics tasks using decision theory. quadrature decision problem solved node selection determination optimal positions points evaluate integrand. deﬁnition appropriate loss function posterior variance integral nodes optimally selected minimising expected loss function. seems clear approach improve upon simple uniform grids many traditional quadrature methods enable active learning surprising evaluations inﬂuence selection future nodes. decision-theoretic approach stands contrast adopted randomised monte carlo approaches quadrature. approaches generate nodes pseudo-random number generator injecting additional epistemic uncertainty procedure designed reduce uncertainty integral. worth noting pseudo-random generators burdens procedure additional computational overhead pseudo-random numbers cheap free. principal feature monte carlo approaches conservative nature monte carlo policy always eventually take additional node arbitrarily close existing node. disadvantage strategy waste improved upon traditional quadrature bayesian quadrature methods. problem worsened common discarding evaluations known ‘burn-in’ ‘thinning’. advantage monte carlo course robustness even highly non-smooth integrands. however bayesian quadrature realise value evaluations exploiting known structure integrand. illustrate integration problem drawn astrometry measurement motion stars. order validate astrometric analysis recover number planets present synthetic data generated mimic produced astrometric facility gaia satellite here quadrature’s task compute interest compare evidence models including differing numbers exoplanets. following example provide focal problem upon compare quadrature methods compute evidence model planets data generated two-planet model. corresponding integral analytically intractable multi-modal integrand employs active selection nodes along prior knowledge smoothness non-negativity integrand converges faster monte carlo approaches simple monte carlo annealed importance sampling note bayesian monte carlo bayesian quadrature algorithm uses samples explaining similar performance. note also performance improvement offered wsabi suggesting crucial role played active selection nodes. probabilistic model well-calibrated suspected properties problem’s integrand. firstly like exponentiated quadratic wsabi’s covariance function suitable smooth integrands expected problem. secondly beyond achievable classic strictly positive. wsabi also makes opportunity afforded probabilistic numeric approach actively select nodes minimise uncertainty integral. ﬁnal contribution permits nodes selected informative gridded randomly selected evaluations. compare algorithm different monte carlo approaches problem annealed importance sampling simple monte carlo additionally compared bayesian monte carlo bayesian quadrature algorithm using simpler exponentiated quadratic model whose nodes taken samples selected smc. ground truth obtained exhaustive sampling results figure show probabilistic quadrature method achieves improved precision drastically faster monte carlo estimates. important point plot’s abscissa wall-clock time algorithmic steps. probabilistic algorithms need expensive. large exact inversion widely known approach method conjugate gradients produces convergent sequence improving estimates iteration involves matrix-vector multiplication small number linear operations optimization known task rn×n symmetric positive deﬁnite. assume access projections arbitrary produce update observation asi. cg’s good performance shorthands projection algorithm requires joint probability measure involved variables model exactly reproduces sequence{xi}i=...m≤n prior choose symmetric kronecker product projections presumed noise-free likelihood dirac distribution limit gaussian vanishing width also seen performing strict conditioning prior observed function values action rule iteration move xi+= posterior mean optimal step computed exactly linear computation. rn×n positive deﬁnite assigns ﬁnite measure every symmetric rn×n value among set{w∈ rn×n\u0000 symm. positive deﬁnite resulting algorithm exactly reproduces iteration sequence{xi}i=...n method implemented distribution p=nγ\u0016) details found manageable form. particular diagonal matrix rm×m skinny matrices rn×m function steps observed projections case quadrature family gaussian priors varying width members family give posterior mean estimate. posterior mean estimate identical classic numerical method member family gives different posterior covariance—a different uncertainty estimate. interesting question degree uncertainty parameter designed give meaningful error estimate. answers found interestingly equivalenceclass prior covariances match mean estimate degrees freedom number observations collected algorithm typical runtime. fitting posterior uncertainty thus requires strong regularization. method advocated constructs regularized estimator exclusively scalar numbers already collected method thus keeping computational overhead small. quadrature valuable applications probabilistic formulation strictly require well-calibrated width posterior. applications make primary posterior mean require algebraic structure prior arbitrary scaling constant incorporate available helpful information. instead highlight another probabilities point estimates propagation knowledge linear problem another related problem. approach described following known numerical linear algebra community recycling krylov sequences however framework classic numerical analysis required challenging bespoke derivation result follows naturally probabilistic viewpoint extension parametric regressor ﬁlter time-varying process. probabilistic formulation computation uses universal unique language inference enable solution similar problems across breadth numerics using similar techniques. contrast compartmentalised state current numerics demands distinct expert knowledge individual numeric problem order make progress towards many set-ups features sequence problems axt= others changes slightly step step figure describes blind deconvolution ground truth image spatially varying blur kernel i.e. white gaussian noise. matrix encodes convolution operation atmospherical disturbances create blur continuously varies time. model frame noisy result convolution possible ensure positive deﬁnite). blind deconvolution algorithm iterates estimating estimating iteration thus requires solution linear problems larger linear problem naïve approach running separate instances wastes information linear problems share matrix iteration next matrix changes less less iterations approach convergence. instead information propagated related computations using probabilistic interpretation starting computation sequence prior mean posterior mean preceding problem. low-rank structure cost. prevent continued rise computational costs linear problems solved restricted ﬁxed rank approximation inner loop also cost. plots lower half figure show increase computational efﬁciency baseline solving linear problems independently sequence converges equally fast lower plot shows optimization progress problems information propagated problem next. ﬁrst problem amounts standard subsequent iterations make increasingly better available information. figure also shows dominant converge relatively generic basis point-spread functions. although strictly correct scheme intuitively understood inferring pre-conditioner across sequence problems bayesian ﬁltering examples highlight areas quadrature linear algebra. analogous results identifying existing numerical methods maximum a-posteriori estimators established areas too. experiment here help complete picture numerical methods inference across problem boundaries non-linear optimization quasi-newton methods like bfgs rule deeply connected conjugate gradients bfgs algorithm interpreted speciﬁc kind autoregressive generalization gaussian model conjugate gradients among things allows explicit modelling noise evaluated gradients pressing issue large-scale machine learning equations speciﬁcally solution initial value problems form dx~dt= real-valued curve parametrised known start initial value explicit runge-kutta methods basic well-studied tool problems extrapolation rules. increasing nodes t<⋅⋅⋅< repeatedly construct estimates true solution used collect observation f\u0001ˆx figure solving sequences linear problems utilizing probabilistic interpretation conjugate gradients also known recycling krylov sequences. sequence observed astronomical images modelled convolution stationary true image time-varying point-spread function individual deconvolution task requires linear solver chosen method conjugate gradients. bottom plots problem solved independently instance solver progresses similarly posterior mean implied probabilistic interpretation solver communicated problem next solvers progress increasingly faster middle time vectors spanning important point central role linear extrapolation linear computations generally play here numerical settings discussed above. oversimpliﬁcation note numerical methods often amount efﬁciently projecting intractable problem tractable linear computation. since gaussian family closed linear operations perhaps surprise gaussian distributions play central role probabilistic re-interpretations existing numerical methods. case initial value problems gaussian process extrapolation previously suggested skilling tool solution. starting scratch skilling arrived method shares linear structure strong theoretical underpinning runge-kutta methods particular skilling’s method share high convergence order runge-kutta methods. probabilistic formulation allows novel theoretical analysis kinds applications marginalization posterior uncertainty subsequent computational steps ﬁnite prior uncertainty initial value considerably vaguer much earlier observation made side numerical mathematics nordsieck noted class methods proposed subsequently captured wider nomenclature solvers bore resemblance linear electrical ﬁlters. these turn closely connected gaussian process regression notion markov processes. recently schober showed connections gaussian regression solution ivp’s hitherto conceptual made tight. family gaussmarkov priors that used extrapolation rules give posterior gaussian processes whose mean function exactly matches members runge-kutta family. hence areas numerics family methods returns trusted point estimates established method giving posterior uncertainty estimate allowing functionality. recent results identifying probabilistic formulations classic numerical methods highlight general structure. consider problem approximating intractable variable algorithm ability choose ‘inputs’ x={xi}i=... computations result numbers y={yi}i=.... blueprint deﬁnition probabilistic numerical methods requires generative model variables involved—that joint probability measure class solutions assigns typically non-uniform measure class. likelihood explains collected tractable numbers relate basic role describing numerical task. often classic numerical problems likelihood deterministic conditioning rule point measure. previous decisions sometimes rules shown associated minimization empirical loss function thus given decision-theoretic motivation. example case regular grids quadrature rules table probabilistic description several basic numerical problems quadrature linear optimization non-linear optimization solution ordinary differential equation initial value problems classic methods cast maximum a-posteriori estimation gaussian priors. case likelihood function strict conditioning observations assumed noise-free. numerical methods active require decision rule. often greedy evaluation posterior mean estimate. exception integration area estimated solution numerical task required construct next evaluation. aforementioned results show classic base-case algorithms several fundamental numerical problems cast maximum a-posteriori inference speciﬁc cases description; typically gaussian priors often simple action rules like uniform gridding greedy extrapolation table gives short summary. numerical methods undergone centuries development analysis. result mature algorithms ingrained scientiﬁc tool-set. contrast probabilistic viewpoint suggested emerging area. many questions remain unanswered many aspects practical importance missing formal analysis early stage. efﬁcient stable implementations still development. convincing use-cases various scientiﬁc disciplines beginning emerge. hope reader take issues motivation contribute rather hold-up. large scale computation simulation data permeate quantitative sciences clearly need formal theory uncertainty computation. opinion match probabilistic inference existing numerical methods lays foundation analysis probabilistic numerical methods. primary complementary goals first implicit prior assumptions questioned. could done aggressive hope ﬁnding either algorithms faster convergence smaller problems satisfying stronger assumptions conversely conservative re-deﬁnition prior assumptions might improve robustness increased computational cost. particularly important aspect regard action rule wherever function previously collected ‘data’ bias occur. collected data also inﬂuence result future actions severe problem exploration-exploitation trade-off arise. checking biases potentially correcting them increase computational cost. applications require high robustness effort off. secondly modelling assumptions particular likelihood extended increase reach existing methods settings. ﬁrst point interest explicit modelling uncertainty noise evaluations themselves. generalization would challenging construct classical standpoint often straightforward probabilistic interpretation found. simple replacing point-mass likelihood functions table gaussian distributions. prominent case aspect optimization noisy functions arises example training large-scale machine learning architectures subsets large inﬁnite dataset. ideas like propagation knowledge problems figure difﬁcult motivate study classic formulation suggest quite naturally probabilistic formulation. also practical considerations shape research effort. gaussian distributions play important role least inferred quantity continuous valued. incidental large degree point numerical method turn intractable computation sequence linear computations. gaussian exponential family closed linear projections thus ideally suited task. efﬁcient adaptation model hyper-parameters crucial well-calibrated posterior measure. models ﬁxed parameters often simply reproduce existing analytic bounds; parameter adaptation uncertainty actively ﬁtted. perhaps challenging elsewhere statistics numerical methods inner-loop algorithms used solve complex higher-level computations. important computationally lightweight parameter estimation methods perhaps cost accepting limitations model ﬂexibility. although fundamental insight numerical methods solve inference problems study probabilistic numerical methods still young. recent work made progress exposing wealth enticing applications process. conclude text highlighting promising distant application motivating ongoing research. fueled ubiquitous collection communication data several academic industrial ﬁelds interested systems observations adapt interact with data source autonomous way. figure shows conceptual sketch autonomous machine aiming solve given task using observations build probabilistic model used predict future states xt+δt function actions chosen machine. goal i.e. integration. fitting involves optimization. prediction xt+δt entail solving differential choose actions that time maximize measure utility encodes task. requires sequence numerical steps inference requires marginalization expectations equations. three areas linear base cases combination sequence black-box numerical methods automated set-ups gives rise challenges. method receives point estimate precursor performs local computation hands result errors accumulate unexpected ways along chain modelling accumulation provides value unnecessary numerical method convergence inputs already known rough estimates. speciﬁcally numerical methods allowing probabilistic inputs outputs turn sketch figure factor graph allow propagation uncertainty estimates along chain computation message passing would identify sources computational error allowing active management computational budget across chain; dedication ﬁnite computer resources steps dominate overall error; truncation computations early reach sufﬁcient precision. uncertainty propagation computations studied widely available algorithms focus effects set-up uncertainties outcome computation rather computation itself. functionality explicitly requires calibrated probabilistic uncertainty step computation runtime. classic abstract convergence analyses used kind estimation. figure sketch autonomous system collecting data build parametrised model environment. predictions future states model used choose action strategy. intermediate operations solved numerical methods computational errors inherent uncertainty propagated across pipeline monitor target computational effort. conclusion numerical tasks interpreted inference problems giving rise probabilistic numerical methods. established algorithms many tasks cast explicitly light. establishes connections seemingly disparate problems yields functionality improve performance structured problems. allow interpretation posterior statement uncertainty care must taken ensure well-calibrated priors models. even uncertainty interpretation rigorously established probabilistic formulation already allows encoding prior information problem structure including propagation collected information among problem instances leading improved performance. many open questions remain exciting ﬁeld. long probabilistic formulations allow propagation uncertainty pipelines computation thus active control computational effort hierarchical modular computations. acknowledgements. authors would like express gratitude anonymous referees several helpful comments. also grateful catherine powell university manchester pointing connection recycled krylov sequence methods. funding statement. funded emmy noether programme german research community funded engineering physical sciences research council ep/j/ established career research fellowship royal society wolfson research merit award epsrc programme grant—enabling quantiﬁcation uncertainty large scale inverse problems— ep/k/. davis rabinowitz methods numerical integration. courier dover. diaconis bayesian numerical analysis. statistical decision theory related topics gunter osborne garnett hennig roberts sampling inference probabilistic models fast bayesian quadrature. advances neural information processing systems halko martinsson p.-g. tropp finding structure randomness probabilistic algorithms constructing approximate matrix decompositions. siam review harmeling hirsch schölkopf online blind deconvolution shental siegel wolf bickson dolev gaussian belief propagation solver systems linear equations. ieee international symposium information theory ieee. sozzetti giacobbe lattanzi micela morbidelli tinetti astrometric detection giant planets around nearby dwarfs gaia potential. monthly notices royal astronomical society", "year": 2015}