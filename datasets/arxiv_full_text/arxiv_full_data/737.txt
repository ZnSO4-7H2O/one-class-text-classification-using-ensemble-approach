{"title": "Cognitive Discriminative Mappings for Rapid Learning", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "Humans can learn concepts or recognize items from just a handful of examples, while machines require many more samples to perform the same task. In this paper, we build a computational model to investigate the possibility of this kind of rapid learning. The proposed method aims to improve the learning task of input from sensory memory by leveraging the information retrieved from long-term memory. We present a simple and intuitive technique called cognitive discriminative mappings (CDM) to explore the cognitive problem. First, CDM separates and clusters the data instances retrieved from long-term memory into distinct classes with a discrimination method in working memory when a sensory input triggers the algorithm. CDM then maps each sensory data instance to be as close as possible to the median point of the data group with the same class. The experimental results demonstrate that the CDM approach is effective for learning the discriminative features of supervised classifications with few training sensory input instances.", "text": "humans learn concepts recognize items handful examples machines require many samples perform task. paper build computational model investigate possibility kind rapid learning. proposed method aims improve learning task input sensory memory leveraging information retrieved long-term memory. present simple intuitive technique called cognitive discriminative mappings explore cognitive problem. first separates clusters data instances retrieved long-term memory distinct classes discrimination method working memory sensory input triggers algorithm. maps sensory data instance close possible median point data group class. experimental results demonstrate approach effective learning discriminative features supervised classiﬁcations training sensory input instances. scientists interest understanding relations levels describe brain does. adolphs listed unsolved problems neuroscience important questions \"how sensory transduction work?\" \"how learning memory work?\" closely connected without sensory transduction converts sensory stimulus form another brain cannot integrate process sensory input information information stored memory. people learn concept recognize item handful examples stateof-the-art machine learning algorithms typically require tens hundreds examples perform similar accuracy scientists long suspected type \"one-shot learning\" rapid learning involves different mechanism brain gradual learning believe underlying mechanism contains complex information processing procedure. humans main senses sight hearing taste smell touch human brain combine data sensory memory prior experiences retrieved long-term memory single phenomenal experience. feature integration theory tackles question humans perceive individual features part object proposing two-stage process preattentive processing focused attention processing basic idea objects analyzed features attention necessary combine features create perceptions object. apply theory model describe sensory input data processed addition people retrieve prior experiences encoded ltm. believe procedure reasons humans learn concepts quickly. therefore simulate procedure model. paper model rapid learning prediction task feature transformation augmentation procedures working memory. rapid learning problem aims achieving high prediction performance training learning system limited data drawn leveraging relatively large amount data retrieved ltm. section formally deﬁne problem introduce framework approach. present approach including learning discriminative features augmentation techniques section proceed evaluating approach benchmark data sets discuss results section finally section conclude paper offer suggestions future work. problem deﬁnition assuming labeled data points {}nl class data instance class denote data data another data interest. denote data labeled data instances {}ns instance class class class assumed equal data sets different feature spaces. cases number dimensions input data sets also different. goal build classiﬁer making small data {}ns relatively large data {}nl ltm. classiﬁer applied incoming data performs equally even better classiﬁer trained data note data instance without knowing class training phase. according deﬁnition given domains domain domain feature spaces marginal probability distributions {xi}nl implies either paper focus situation give deﬁnitions problem data different feature spaces desirable common invariant feature space data directly compared. inspired min-max principle class-based constraints apply transformations data order satisfy class constraints transformed points. order learn transformation functions data classiﬁcation ﬁrst deﬁne variables follows here distance function deﬁned common space sums distance transformed points class indicates distance projected instances different classes. then posit exist bounds inequations necessary generation pairwise disjoint discriminative clusters upper lower bounds chosen small large. given data data class constraints advance learn transformation functions data satisfy inequations able data help improve prediction performance. paper present linear version. represent linear transformation functions namely qyj. present computational mappings model make data ﬁrst sensory data instances taken sensory receptors kept sensory data instances stored long enough transferred working memory. working memory space transformed data populated clusters according classes. fig. illustrates main idea proposed model. figure illustration cognitive discriminative mappings different colors represent different classes. broken black lines represent class boundaries. circles represent grouped instances ovals represent projected instances class boundaries discriminatively separate instances different classes classiﬁcation errors expected. estimate linear transformations ﬁrst deciding transformation. time many related samples sensory cases consequently mapping function project data rnl×m rnl×d latent space working memory. therefore rd×m. latent space assumed d-dimensional riemannian manifold. present three different approaches retrieval phase. ﬁrst approach geometric medians clusters corresponding classes ﬁxed predeﬁned latent space. data mapped locations transformation matrix form discriminative clusters simultaneously. second approach apply linear discriminant analysis characterize separate classes instances. transformation matrix provided project original instances onto latent space feature generated. third approach linear transformation derived minimizing cost function similar graph embedding method follows sparse symmetric matrix represents class similarity relationship given transformation data mapping data instance class mapped point neighborhood cluster corresponding class paper provide approximation solution mapping. idea data instance class label mapped point close possible median point linear mapping matrix rd×n setting avoid overﬁtting regularization term example η||q|| weight ||·||f frobenius norm. mapping functions derived purpose obtaining low-dimensional representation data separates populations much possible. however classes sufﬁciently separated. consequently apply discrimination method determine mapping function render classes separated possible. applying ﬁnal discrimination method transfer data derive features. daumé provides feature augmentation method integrate information instance utilize feature augmentation method augment features original features represent instance deﬁne feature mapping functions data respectively. denotes zero column vectors dimensions including zeros feature representations ensures dimensions instances become same. moreover entire data information features therefore reasonable zero values projection approach learn mapping project data retrieved dimensional space working memory. projected data space grouped several clusters corresponding classes. section conduct several experiments benchmark data sets. assume data data upper bound number retained dimensions dimension space ﬁrst projection learning ﬁnal projection selection weight regularization term cross-validation applicable small number training instances domain.therefore tune parameters predeﬁned range report optimal parameter value. paper report results apply nearest neighbor classiﬁer support vector machine radial basis function kernel train ﬁnal classiﬁers data sets. training instances derived baseline approach train classiﬁer original labelled instances predict test instances. randomly sample training instances times report average accuracy rounds experiments. paper conduct experiments different settings data sets report results. ﬁrst data contains images categories originating following three domains amazon dslr webcam surf features extracted images. amazon webcam images used data retrieved ltm. randomly select twenty eight training images category amazon webcam data sets respectively. randomly select training images category dslr data data remaining images used testing. table shows summary data set. second data subset reuters rcv/rcv collections contains newswire articles written english french german italian spanish. classes articles ccat ecat gcat take spanish articles data articles written four languages individual data sets retrieved ltm. class randomly sample hundred training instances data sets training instances data also perform principal components analysis energy preserved tf-idf features. remaining instances data used test instances. table shows summary data table shows distribution classes data set. table shows performance baseline approach approach different settings object recognition dataset. results show approach using classiﬁer without augmentation outstanding performance object recognition dataset. case classiﬁer although learned features good enough training prediction model augmenting learned features original features improves performance. visit http//vision.cs.uml.edu/adaptation.html details. these categories backpack bike bike helmet bookcase bottle calculator computer desk chair desk lamp cabinet headphones keyboard laptop letter tray mobile phone monitor mouse notebook phone printer projector puncher ring binder ruler scissors speaker stapler tape trash table means standard deviations classiﬁcation accuracies baseline approaches object recognition dataset using labeled training samples class domain dslr. approach shows results different settings. table shows performance baseline approach approach different settings text categorization dataset. classiﬁers without augmentation outperform baseline approach. table means standard deviations classiﬁcation accuracies baseline approaches text categorization dataset using labeled training samples class domain spanish. approach shows results different settings. figure shows accuracy baseline proposed approach respect number training samples class data baseline approaches classiﬁer. shown fig. fig. accuracies baseline approach increase larger figure classiﬁcation accuracies baseline approach approach respect different numbers training samples class object recognition data text categorization data set. case different hatching patterns correspond different domains. proposed computational model called cognitive rapid learning problem. following excellent properties elegant intuitive efﬁcient; complete theoretical architecture. experimental results show least hypothesis classiﬁer satisﬁes hypothesis rapid learning. one-shot learning rapid learning still work progress. experience think cognitive scientists required attention role information retrieved rapid learning. experimental results found feature augmentation always guarantee performance improvement. worth investigating issue future. want create common metric space proposed global metric learning working memory; unfortunately results limitation. attempts mapping matrices minimize pairwise distances data points class maximize pairwise distances data points different classes. such implicitly assumes classes form single compact connected set. data instances highly multi-modal class distributions cost function penalized. therefore developing discrimination approach focuses local neighborhoods problem worth studying future research. massih-reza amini nicolas usunier cyril goutte. learning multiple partially observed views application multilingual text categorization. proceedings annual conference advances neural information processing systems pages steven shih-fu chang. semi-supervised distance metric learning collaborative image retrieval. proceedings ieee conference computer vision pattern recognition kate saenko brian kulis mario fritz trevor darrell. adapting visual category models domains. proceedings european conference computer vision pages shuicheng dong benyu zhang hong-jiang zhang qiang yang stephen lin. graph embedding extensions general framework dimensionality reduction. ieee transactions pattern analysis machine intelligence pages", "year": 2016}