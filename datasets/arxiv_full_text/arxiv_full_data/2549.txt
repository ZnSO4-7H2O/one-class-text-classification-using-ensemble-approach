{"title": "Automatic Differentiation Variational Inference", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.CO"], "abstract": "Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (ADVI). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models-no conjugacy assumptions are required. We study ADVI across ten different models and apply it to a dataset with millions of observations. ADVI is integrated into Stan, a probabilistic programming system; it is available for immediate use.", "text": "probabilistic modeling iterative. scientist posits simple model data reﬁnes according analysis repeats. however ﬁtting complex models large data bottleneck process. deriving algorithms models mathematically computationally challenging makes difﬁcult efﬁciently cycle steps. develop automatic differentiation variational inference using method scientist provides probabilistic model dataset nothing else. advi automatically derives efﬁcient variational inference algorithm freeing scientist reﬁne explore many models. advi supports broad class models—no conjugacy assumptions required. study advi across different models apply dataset millions observations. advi integrated stan probabilistic programming system; available immediate use. develop automatic method derives variational inference algorithms complex probabilistic models. implement method stan probabilistic programming system lets user specify model intuitive programming language compiles model inference executable. method enables fast inference large datasets expansive class probabilistic models. taxi rides taken course year million trajectories. explore patterns data propose mixture model unknown number components. non-conjugate model seek large dataset. previously would manually derive inference algorithm scales large data. method write stan program compile model minutes analyze results ease. context research ﬁeld probabilistic modeling emerged powerful language customized data analysis. probabilistic modeling lets express assumptions data formal mathematical derive algorithms assumptions compute observed dataset. impact myriad applications statistics machine learning including natural language processing speech recognition computer vision population genetics computational neuroscience. probabilistic modeling leads natural research cycle. scientist ﬁrst uses domain knowledge posit simple model includes latent variables; then uses inference algorithm infer variables data; next analyzes results identiﬁes model works falls short; last reﬁnes model repeats process. cycle steps expressive interpretable useful models broad goals machine learning make process easy. looping around cycle however easy. data study often large complex; accordingly want propose rich probabilistic models scale using models requires complex algorithms difﬁcult derive implement scale. bottleneck computation precludes scientist taking full advantage probabilistic modeling cycle. problem motivates important ideas probabilistic programming automated inference. probabilistic programming allows user write probability model computer program compile program efﬁcient inference executable. automated inference backbone system—it inputs probability model expressed program outputs efﬁcient algorithm computing previous approaches automatic inference mainly relied markov chain monte carlo algorithms. results successful automated mcmc slow many real-world applications. approach problem variational inference faster alternative mcmc used many large-scale problems though promising method developing variational inference algorithm still requires tedious model-speciﬁc derivations implementation; seen widespread probabilistic programming. automate process deriving scalable variational inference algorithms. build recent ideas so-called black-box variational inference leverage strengths probabilistic programming systems namely ability transform space latent variables automate derivatives joint distribution. result called automatic differentiation variational inference provides automated solution variational inference inputs probabilistic model dataset; outputs posterior inferences model’s latent variables. implemented deployed advi part stan probabilistic programming system advi stan resolves computational bottleneck probabilistic modeling cycle. scientist easily propose probabilistic model analyze large dataset revise model without worrying computation. advi enables cycle providing automated scalable variational inference expansive class models. sections present probabilistic modeling examples including progressive analysis million taxi trajectories. formally probabilistic model deﬁnes joint distribution observations technical summary. latent variables inference problem compute posterior conditional distribution latent variables given observations posterior reveal patterns kullback-leibler divergence exact posterior. traditionally using variational inference algorithm requires painstaking work developing implementing custom optimization routine specifying variational family appropriate model computing corresponding objective function taking derivatives running gradient-based coordinate-ascent optimization. advi solves problem automatically. user speciﬁes model expressed program advi automatically generates corresponding variational algorithm. idea ﬁrst automatically transform inference problem common space solve variational optimization. solving problem common space solves variational inference models large class. detail advi follows steps. advi transforms model unconstrained real-valued latent variables. speciﬁcally transforms mapping built joint distribution. removes original constraints latent variables advi deﬁnes corresponding advi reparameterizes gradient terms standard gaussian. this uses another transformation time within variational family. second transformation enables advi efﬁciently compute monte carlo approximations—it needs sample standard gaussian developed advi stan system gives important types automatic computation around probabilistic models. first stan provides library transformations—ways convert variety constrained latent variables unconstrained without changing underlying joint distribution. stan’s library transformations helps step above. second stan implements organization paper. section develops recipe makes advi. expose details steps present concrete algorithm. section studies properties advi. explore accuracy stochastic nature sensitivity transformations. section applies advi array probability models. compare speed mcmc sampling techniques present case study using dataset millions observations. section concludes paper discussion. automatic differentiation variational inference offers recipe automating computations involved variational inference. strategy follows transform latent variables model common space choose variational approximation common space generic computational techniques solve variational problem. likelihood relates observations latent random variables bayesian model posits prior density latent variables. combining likelihood prior gives joint density goal inference compute posterior density many posterior densities tractable compute; normalizing constants lack analytic solutions. thus often seek approximate posterior. advi approximates posterior differentiable probability models. members class models continuous latent variables discrete; latent rate continuous positive. place weibull prior deﬁned positive real numbers. resulting joint density describes nonconjugate differentiable probability model; posterior distribution class prior. however class differentiable models. partial derivative valid within support weibull distribution supp) model would many machine learning models differentiable. example linear logistic regression matrix factorization continuous discrete observations linear dynamical systems gaussian processes. ﬁrst blush restriction continuous random variables seem leave common machine learning models mixture models hidden markov models topic models. however marginalizing discrete variables likelihoods models renders differentiable. ﬁrst term expectation joint density approximation second entropy variational density. elbo equal negative divergence constant maximizing elbo minimizes divergence optimizing divergence implies constraint support approximation within support posterior constraint made explicit optimization explicitly include constraint speciﬁed form variational approximation; must ensure stays within support posterior. support posterior however also unknown. assume support recipe automating traditional solving equation difﬁcult. begin choosing variational family that deﬁnition satisﬁes support matching constraint. compute expectations elbo either analytically approximation. decide strategy maximize elbo. instance might coordinate ascent iteratively updating components might follow gradients elbo respect staying within finally implement test debug software performs above. step requires expert thought analysis service single algorithm single model. contrast approach allows scientist deﬁne differentiable probability model automate process developing corresponding algorithm. recipe automating three ingredients. first automatically transform support latent variables real coordinate space lets choose variety variational distributions without worrying support matching constraint second compute elbo model using monte carlo integration requires able sample variational distribution third employ stochastic gradient ascent maximize elbo automatic differentiation compute gradients without user input tools develop generic method automatically solves variational optimization problem large class models. begin transforming support latent variables live real coordinate space transformed density choose variational approximation independent model. joint density original latent variable space jacobian inverse transformations continuous probability densities require jacobian; accounts transformation warps unit volumes ensures transformed density integrates consider running weibull-poisson example section latent variable lives logarithm transforms real line jacobian adjustment derivative inverse logarithm jt−| exp. transformed density describe introduction implement algorithm stan stan maintains library transformations corresponding jacobians. stan automatically transforms joint density differentiable probability model real-valued latent variables. transformation latent variables support real coordinate space choice variational approximations space. here consider gaussian distributions; implicitly induce non-gaussian variational distributions original latent variable space. logarithm standard deviations applied element-wise. support real coordinate space always positive. mean-ﬁeld gaussian becomes logarithm standard deviation factor. variational parameters unconstrained full-rank gaussian. another option posit full-rank gaussian variational approximation vector concatenates mean vector covariance matrix ensure always remains positive semideﬁnite re-parameterize covariance matrix using cholesky factorization non-unique deﬁnition cholesky factorization diagonal elements need positively constrained therefore lives unconstrained space lower-triangular matrices real-valued entries. full-rank gaussian becomes unconstrained k+k/. full-rank gaussian generalizes mean-ﬁeld gaussian approximation. off-diagonal terms covariance matrix capture posterior correlations across latent random variables. leads accurate posterior approximation mean-ﬁeld gaussian; however comes computational cost. various low-rank approximations covariance matrix reduce cost limit ability model complex posterior correlations choice gaussian. choosing gaussian distribution call mind laplace approximation technique second-order taylor expansion around maximum-a-posteriori estimate gives gaussian approximation posterior. however using gaussian variational approximation equivalent laplace approximation approach distinct another posterior approximation original latent variable space non-gaussian. implicit variational density. transformation equation maps support latent variables real coordinate space. thus inverse maps back support latent variables. implicitly deﬁnes variational approximation original latent variable space sensitivity many ways transform support variable real coordinate space. form transformation directly affects shape variational approximation original latent variable space. study sensitivity choice transformation section story far. began differentiable probability model transformed latent variables live real coordinate space. deﬁned variational approximations transformed space. consider variational optimization problem. inverse transformation appears joint model along determinant jacobian adjustment. elbo function variational parameters entropy depend variational approximation. parameter vector lives appropriately dimensioned real coordinate space. unconstrained optimization problem solve using gradient ascent. traditionally would require manual computation gradients. instead develop stochastic gradient ascent algorithm uses automatic differentiation compute gradients integration approximate expectations. cannot directly automatic differentiation elbo. elbo involves unknown expectation. however automatically differentiate functions inside expectation. apply automatic differentiation want push gradient operation inside expectation. employ ﬁnal transformation elliptical standardization elliptical standardization. consider transformation absorbs variational parameters converts gaussian variational approximation standard gaussian. mean-ﬁeld case expectation terms standard gaussian density. jacobian elliptical standardization evaluates gaussian distribution member location-scale family standardizing gaussian gives another gaussian distribution. compute gradients inside expectation automatic differentiation. thing left expectation. integration provides simple approximation draw samples standard gaussian evaluate empirical mean gradients within expectation practice single sample sufﬁces. gives noisy unbiased gradients elbo differentiable probability model. gradients stochastic optimization routine automate variational inference. stochastic gradient ascent. equipped noisy unbiased gradients elbo advi implements stochastic gradient ascent algorithm guaranteed converge local maximum elbo certain conditions step-size sequence. stochastic gradient ascent falls class stochastic approximations robbins monro established pair conditions ensure convergence prominently step-size sequence must decay sufﬁciently quickly. many sequences satisfy criteria speciﬁc forms impact success stochastic gradient ascent practice. describe adaptive step-size sequence advi below. adaptive step-size sequence. adaptive step-size sequences retain memory past gradients adapt high-dimensional curvature elbo optimization space sequences enjoy theoretical bounds convergence rates. however practice slow converge. empirically justiﬁed rmsprop sequence retains ﬁnite memory initialization ﬁrst factor controls scale step-size sequence. mainly affects beginning optimization. adaptively tune searching using subset data selecting value leads fastest convergence middle term i−/+ε decays function iteration small value guarantees step-size sequence satisﬁes robbins monro conditions. processed equation weighting factor deﬁnes compromise gradient information quantity converges non-zero constant. without previous decaying term would lead possibly large oscillations around local optimum elbo. additional perturbation prevents division zero down-weights early iterations. practice step-size sensitive value complexity data subsampling. advi complexity iteration number latent variables. classical hand-derives coordinate ascent algorithm complexity ﬁrst theme probabilistic programming. class systems focuses probabilistic models user speciﬁes joint probability distribution. examples bugs jags stan another class systems allows user directly specify general probabilistic programs. examples church figaro venture anglican classes primarily rely various forms mcmc techniques inference; typically cannot scale large data. variational approximating family. kingma welling rezende describe reparameterization variational problem simpliﬁes optimization. titsias lázaro-gredilla leverage gradient model class real-valued models. rezende mohamed tran improve accuracy black-box variational approximations. build extend ideas automate variational inference; highlight technical connections study properties advi section notable work crosses themes. bishop present automated variational algorithm graphical models conjugate exponential relationships parent-child pairs. winn bishop minka extend graphical models non-conjugate relationships either using custom approximations expensive sampling approach. advi automatically supports comprehensive class nonconjugate models; section wingate weber study general setting variational approximation probabilistic program. automatic differentiation variational inference extends classical variational inference techniques directions. section simulated data study three aspects advi accuracy mean-ﬁeld full-rank approximations variance advi gradient estimator sensitivity transformation figure comparison mean-ﬁeld full-rank advi two-dimensional gaussian model. ﬁgure shows accuracy full-rank approximation. ellipses correspond two-sigma level sets gaussian. table quantiﬁes underestimation marginal variances mean-ﬁeld approximation. draw datapoints model variants advi mean-ﬁeld full-rank convergence. figure compares advi methods exact posterior. procedures correctly identify mean analytic posterior. however shape mean-ﬁeld approximation incorrect. mean-ﬁeld approximation ignores off-diagonal terms gaussian covariance. advi minimizes divergence approximation exact posterior; leads systemic underestimation marginal variances logistic regression. study model need approximate inference. consider logistic regression generalized linear model binary response covariates likelihood simulated random covariates prior distribution drew datapoints likelihood. estimated posterior coefﬁcients advi stan’s default mcmc technique no-u-turn sampler figure shows marginal posterior densities obtained approximation. mcmc advi perform similarly estimates posterior mean. mean-ﬁeld approximation expected underestimates marginal posterior variances coefﬁcients. full-rank approximation again better matches posterior. stochastic volatility time-series model. finally study model data exchangeable. consider autoregressive process model latent volatility economic asset changes time goal estimate sequence volatilities. expect posterior estimates correlated especially volatilities trend away mean value. figure comparison marginal posterior densities logistic regression model. plot shows kernel density estimates posterior coefﬁcient using samples. mean-ﬁeld advi underestimates variances coefﬁcients. simulate dataset time-steps generative model above. figure plots posterior mean volatility function time. mean-ﬁeld advi struggles describe mean posterior particularly volatility drifts away contrast full-rank advi matches estimates obtained sampling. fails capture locally correlated structure full-rank sampling covariance matrices covariance matrices exhibit blurry spread ﬁnite sample size. figure comparison posterior mean estimates volatility mean-ﬁeld advi underestimates especially moves away mean full-rank advi matches accuracy sampling. regions local correlation strongest correspond regions mean-ﬁeld underestimates volatility. help identify regions overlay sampling mean volatility estimate figure matrix. full-rank advi sampling results exhibit correlation volatility trends away mean value. recommendations. choose full-rank mean-ﬁeld advi? scientists interested posterior variances covariances full-rank approximation. full-rank advi captures posterior correlations turn producing accurate marginal variance estimates. large data however full-rank advi prohibitively slow. figure comparison empirical posterior covariance matrices. mean-ﬁeld advi covariance matrix fails capture local correlation structure seen full-rank advi sampling results. covariance matrices exhibit blurry spread ﬁnite sample size. advi offers fast algorithm approximating posterior mean. practice accurate posterior mean estimates dominate predictive accuracy; underestimating marginal variances matters less. advi uses monte carlo integration approximate gradients elbo uses gradients stochastic optimization algorithm speed advi hinges variance gradient estimates. stochastic optimization algorithm suffers high-variance gradients must repeatedly recover poor parameter estimates. advi compute monte carlo approximations gradient elbo. black variational inference takes different approach bbvi gradient estimator uses gradient variational approximation avoids using gradient model. example following bbvi estimator advi gradient estimator equation lead unbiased estimates exact gradient. bbvi general—it require gradient model thus applies settings—its gradients suffer high variance. figure empirically compares variance estimators models. figure shows variance gradient estimators simple univariate model posterior gamma. estimate variance using thousand re-calculations gradient across increasing multivariate example also show bbvi gradient variance reduction scheme using control variates described ranganath cases advi gradients statistically efﬁcient. figure comparison gradient estimator variances. advi gradient estimator exhibits lower variance bbvi estimator. moreover require control variate variance reduction available univariate situations. advi uses transformation unconstrained space constrained space. study choice transformation affects non-gaussian posterior approximation original latent variable space. consider posterior density gamma family support figure shows three conﬁgurations gamma ranging gamma places mass close gamma centered consider transformations figure show advi approximation transformations. table reports corresponding divergences. graphical numerical results prefer quick analysis corroborates this. logarithm ﬂattens large values. however almost linear large values since gamma gaussian densities light-tailed preferable transformation. cumulative density function posterior inverse cumulative density function standard gaussian. maps posterior uniform distribution maps uniform distribution standard gaussian. optimal choice transformation enables gaussian variational approximation exact. sadly estimating optimal transformation requires estimating observation motivates pairing transformations gaussian variational approximations; need complex variational families. advi takes approach using library model compiler. option. example knowles posits factorized gamma density positively constrained latent variables. theory equivalent mean-ﬁeld gaussian density paired transformation pgamma cumulative density function gamma. challis barber study fourier transform techniques location-scale variational approximations beyond gaussian. another option learn transformation optimization. discuss recent approaches direction section apply automatic differentiation variational inference array nonconjugate probability models. simulated real data study linear regression automatic relevance determination hierarchical logistic regression several variants non-negative matrix factorization mixture models probabilistic principal component analysis. compare mean-ﬁeld advi mcmc sampling algorithms hamiltonian monte carlo nuts adaptive extension linear regression ard. linear regression model hierarchical prior structure leads sparse estimates coefﬁcients. simulate dataset regressors half regressors predictive power. data points training withhold evaluation. logistic regression spatial hierarchical prior. hierarchical logistic regression model political science. prior captures dependencies states regions polling dataset united states presidential election model nonconjugate would require form approximation derive classical algorithm. results. figure plots average predictive accuracy function time. simple models methods reach predictive accuracy. study advi settings number samples used estimate gradients. single sample iteration sufﬁcient; also fastest. continue exploring nonconjugate non-negative matrix factorization models constrained gamma poisson model dirichlet exponential poisson model. here show easy explore models using advi. models frey results. figure shows average predictive accuracy well factors recovered models. advi provides order magnitude speed improvement nuts. nuts struggles dirichlet exponential model. cases produce useful samples within budget hour; omit gamma poisson model appears pick signiﬁcant frames dataset. dirichlet exponential factors sparse indicate components face move eyebrows cheeks mouth. nonconjugate gaussian mixture model applied color image histograms. place dirichlet prior mixture proportions gaussian prior component means lognormal prior standard deviations. explore imageclef dataset images withhold images evaluation. figure randomly select images train model mixture components. advi quickly ﬁnds good solution. nuts struggles adequate solution fails altogether likely label switching affect hmc-based algorithms mixture models figure shows advi results full dataset. increase number mixture components advi additional stochastic subsampling minibatches data minibatch size larger advi reaches high predictive accuracy. smaller minibatch sizes lead suboptimal solutions effect also observed hoffman advi converges hours; nuts cannot handle large datasets. figure held-out predictive accuracy results imageclef image histogram dataset. advi outperforms nuts advi scales large datasets subsampling minibatches size dataset iteration might scientist advi practice? easy develop revise models? answer questions apply advi modern exploratory data analysis task analyzing trafﬁc patterns. section demonstrate advi enables scientist quickly develop revise complex hierarchical models. city porto centralized taxi system cars. serving customers taxi reports spatial location second intervals; sequence coordinates describes trajectory duration trip. dataset trajectories publicly available contains million taxi rides taken year gain insight dataset wish cluster trajectories. ﬁrst task process data. trajectory different length shorter trips contain fewer coordinates longer ones. average trip approximately minutes long corresponds coordinates. want cluster independent length interpolate trajectories coordinate pairs. converts trajectory point trajectories structure; example major roads highways appear frequently. motivates approach ﬁrst identify lower-dimensional representation data capture aggregate features cluster trajectories representation. easier clustering original data space. begin simple dimension reduction probabilistic principal component analysis bayesian generalization classical principal component analysis easy write stan. however like classical counterpart ppca identify many principal components subspace. address this propose extension ppca automatic relevance determination ppca identiﬁes latent dimensions effective explaining variation data. strategy similar section assume latent dimensions impose hierarchical prior encourages sparsity. consequently model uses subset latent dimensions describe data. randomly subsample thousand trajectories advi infer subspace. figure plots progression elbo. advi converges approximately hour ﬁnds eleven-dimensional subspace. omit sampling results nuts struggle model; neither produce useful samples within hour. equipped eleven-dimensional subspace turn analyzing full dataset million taxi trajectories. ﬁrst project trajectories subspace. section components cluster trajectories. advi takes less half hour converge. figure shows visualization ﬁfty thousand randomly sampled trajectories. color represents trajectories associate particular gaussian mixture. clustering geographical taxi trajectories close bundled together. clusters identify frequently taken taxi trajectories. figure visualization ﬁfty thousand randomly sampled taxi trajectories. colors represent thirty gaussian mixtures trajectories associated each. processed data interpolated trajectory equal length. discards duration information. roads particularly prone trafﬁc? roads lead longer trips? supervised probabilistic principal component analysis model this. idea regress durations trip onto subspace also explains variation response variable case duration. sup-ppca simple extension ppca extend using prior before. figure shows clusters identify particularly busy roads bridges porto cross duoro river. figure shows group short trajectories bridges near city center. figure show group longer trajectories newer bridges connect highways circumscribe city. analyzing taxi trajectories illustrates exploratory data analysis iterative effort want rapidly evaluate models modify based learn. advi provides automatic fast inference enables effective exploration massive datasets. presented automatic differentiation variational inference variational inference tool works large class probabilistic models. main idea transform latent variables common space. solving variational inference problem common space solves models class. studied advi using different probability models; showcases easy advi practice. also developed deployed advi part stan probabilistic programming system; makes advi available everyone. begin accuracy. showed section advi sensitive transformations constrained parameter space real coordinate space. dinh rezende mohamed cascade simple transformations improve accuracy. tran place gaussian process learn optimal transformation prove expressiveness universal approximator. class hierarchical variational models extend complex distributions discrete latent variable models. continue optimization. advi uses ﬁrst-order automatic differentiation implement stochastic gradient ascent. higher-order gradients enable faster convergence; however computing higher-order gradients comes computational cost optimization using line search could also improve convergence speed robustness well natural gradient approaches nonconjugate models follow practical heuristics. things affect advi convergence initialization step-size scaling. initialize advi real coordinate space standard gaussian. better heuristic could adapt model dataset based moment matching. adaptively tune scale step-size sequence using ﬁnite search. better heuristic could avoid additional computation. probabilistic programming. designed deployed advi stan mind. thus focused class differentiable probability models. extend advi discrete latent variables? approach would adapt advi black gradient estimator variables requires care gradients exhibit higher variance gradients respect differentiable latent variables. support discrete latent variables modiﬁed versions advi could extended general probabilistic programming systems church figaro venture anglican acknowledgments. thank bruno jacobs reviewers helpful comments. work supported iis- iis- iis- ses- n--- darpa fa--- n--c- sloan ndseg facebook adobe amazon siebel scholar john templeton foundations. consider scalar random variable probability density function supp) support consider another random variable deﬁned supp) support one-to-one differentiable function probability density funclet sketch proof. consider cumulative density function transformation increasing directly apply inverse transformation decreasing apply inverse minus probability density function derivative cumulative density function. things combined give absolute value derivative above. extension multivariate variables requires multivariate version absolute value derivative inverse transformation. absolute determinant jacobian intuitively jacobian describes transformation warps unit volumes across spaces. matters transformations random variables since probability density functions must always integrate one. transformation linear drop jacobian adjustment; evaluates one. similarly afﬁne transformations like elliptical standardizations also jacobians evaluate one; preserve unit volumes. recall variational approximation real coordinate space begin elbo original latent variable space. transform latent variable space real coordinate space. first consider gradient respect parameter. exchange order gradient integration dominated convergence theorem rest chain rule differentiation. hierarchical logistic regression models structured datasets intuitive way. study model voting preferences united states presidential election. chapter motivates model explains dataset. also describe model below. stan code potential downfall model uniquely identiﬁable swapping rows columns give inner product. contend constrain either vector ordered vector inference. constrain vector model fashion. stan code figure gamma hyper-parameters experiments. another model discrete data dirichlet exponential model. dirichlet enforces uniqueness exponential promotes sparsity. non-conjugate model appear studied literature. gaussian mixture model celebrated probability model group dataset natural images based color histograms. build high-dimensional gaussian prior mixture means lognormal prior mixture standard deviations dirichlet prior mixture components. scale image histograms zero mean unit variance. setting small value encourages model fewer components explain data. larger values encourage model components. experiments. advi code figure stochastic data subsampling version code figure probabilistic principal component analysis bayesian extension classical principal component analysis generative process straightforward. consider dataset d-dimensional. dimension subspace seek. first deﬁne latent variables m-dimensional. draw standard normal supervised probabilistic principal component analysis augments ppca regressing vector observed random variables onto principal component subspace. idea principal components describe variation dataset also predict complete model", "year": 2016}