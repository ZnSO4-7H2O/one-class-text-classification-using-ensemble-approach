{"title": "Compositional Planning Using Optimal Option Models", "tag": ["cs.AI", "cs.LG"], "abstract": "In this paper we introduce a framework for option model composition. Option models are temporal abstractions that, like macro-operators in classical planning, jump directly from a start state to an end state. Prior work has focused on constructing option models from primitive actions, by intra-option model learning; or on using option models to construct a value function, by inter-option planning. We present a unified view of intra- and inter-option model learning, based on a major generalisation of the Bellman equation. Our fundamental operation is the recursive composition of option models into other option models. This key idea enables compositional planning over many levels of abstraction. We illustrate our framework using a dynamic programming algorithm that simultaneously constructs optimal option models for multiple subgoals, and also searches over those option models to provide rapid progress towards other subgoals.", "text": "paper introduce framework option model composition. option models temporal abstractions that like macrooperators classical planning jump directly start state state. prior work focused constructing option models primitive actions intra-option model learning; using option models construct value function inter-option planning. present uniﬁed view intrainter-option model learning based major generalisation bellman equation. fundamental operation recursive composition option models option models. idea enables compositional planning many levels abstraction. illustrate framework using dynamic programming algorithm simultaneously constructs optimal option models multiple subgoals also searches option models provide rapid progress towards subgoals. classical planning algorithms make extensive temporal abstraction construct high-level chunks useful knowledge typically provided primitive planning operators inputs. composed together macrooperators open-loop sequences planning operators. macro-operators jump directly initial state outcome state would result following sequence without execute intermediate operators. macro-operators composed together abstract operators allowing planning take place much abstract level. macro-operators thought building blocks knowledge combined together abstract knowledge. powered knowledge path goal often found small number high-level planning operations even path composed thousands primitive actions. markov decision processes outcome action stochastic. open-loop sequence capture contingencies arise result intermediate action. instead closedloop policy maps states actions respond particular situation arises. closed-loop policy followed number steps stops according termination condition also depends state known option option model describes distribution outcome states would result following option option models stochastic analogue macro-operators jump directly initial state outcome without execute intermediate actions. option models also composed together abstract option models option models thus provide basic building blocks compositional knowledge general mdps. however prior work planning options restricted shallow hierarchies. option models either constructed primitive actions approach known intra-option model learning; used compute value function approach known inter-option planning although steps sometimes combined typically combined stages ﬁrst constructing option models without using them; using option models without changing them. stages planning operators ﬁxed. paper focus explicitly compositional planning multi-level composition option models. option model constructed used constructed lowerlevel option models maximise progress towards given subgoal. also used compose higher-level option models. soon option model created used construct option models. result planning operators improves dynamically providing longer purposeful jumps planning proceeds. approach based major generalisation bellman equation along four dimensions. first provide recursive relationship state probabilities well rewards. second compose options rather primitive actions. third generalise overall goal maximising total reward given subgoal. fourth optimise termination conditions well policies. several dimensions partially explored prior work. first sutton developed bellman expectation equation state probabilities work restricted markov reward processes without actions ﬁxed policies without control present bellman optimality equation state probabilities markov decision processes including actions control. second precup provided bellman equations composing option models policies options. framework constructs policies termination conditions compose option models option models crucial step compositional planning. third sutton deﬁned optimal options respect given subgoal termination condition suggested existence corresponding bellman optimality equation. deﬁne bellman optimality equation also extend case neither either policy termination condition speciﬁed. prior work considered state probabilities associated bellman optimality equations. without knowledge state probabilities possible jump directly outcome optimal option. approach compositional planning built directly knowledge build abstract macro-operators jump state directly distant state. bellman optimality equation gives rise important planning methods value iteration. similarly generalised bellman equation derive compositional planning algorithm simultaneously recursively constructs optimal option model multiple subgoals including overall goal special case. prove algorithm converges optimal option models subgoals including optimal policy. options framework agnostic source options commit particular algorithm construction. however several approaches hierarchical reinforcement learning proposed based samples unknown mdp. architectures including dietterich’s maxq parr russell’s hams construct solution subproblem solution subproblems. however architectures directly applicable compositional planning known rather sampled. focusing planning known models develop sound theoretical framework compositional planning based generalised bellman equation. work viewed bridge generality options compositional construction algorithms used architectures maxq. illustrate approach well-known benchmark problems hierarchical path planning tower hanoi. problems extensively studied using classical planning approaches. problems planning directly primitive operators requires computation time exponential problem size whereas algorithms based compositions macro-operators solve problems polynomial time. unfortunately classical planning approaches generalise stochastic planning problems. contrast compositional planning algorithm solve deterministic stochastic variants problems polynomial number iterations. background deﬁned states actions action transition matrices action reward vectors action discount factor component action transition matrix discounted probability next state given action selected state component action reward vector expected reward given action selected state discount factor viewed chance exiting absorbing terminal state probability discounted probability interpreted probability reaching state without exiting. policy probability selecting action given state value function expected total reward state following policy optimal value function optimal action value function maximum achievable value action value rasps models model sets respectively. sequences compositions best understood reading left right e.g. model composition starts state applies model applies model achieved policy optimal policy policy achieves optimal value function. optimal value function obeys recursive relationship bellman optimality equation optimal value funcmax macro-action combines policy termination condition giving probability option stop state assume options initiated states. primitive actions options represented policy deterministically selects action termination condition stops probability denote policies termination conditions option model comprises option transition matrix option reward vector component expected total reward given option executed state random variable duration option component probability terminating state given option executed state discounted total duration option interpreted probability option terminating without exiting. informally model stochastic mapping state state combined reward accumulated along way. applying model state results distribution outcome states expected reward. compose models together apply second model outcome distribution expected reward arrive state distribution reward. formalise ideas following sutton deﬁne rasp explore recursive relationships compositions models. didactic purposes begin compositions primitive actions policy models develop model equation analogous bellman equation. extend approach compositions option models policy models; compositions action models option models; ﬁnally compositions option models option models. provide proofs unique ﬁxed points supplementary material. call equation action-policy model expectation equation. rewrites bellman expectation equation homogeneous coordinates. equation ﬁxed point i.e. composing action expectation model policy model results policy model also consider model models value models reward vector contains values transition matrix zero inﬁnite discounting. value models used express several familiar value functions. state value function represented composition corresponding value model action value function represented sav; inter-option value function represented sov. true value model requires mechanism incorporate option termination model compositions. represent termination condition termination model expectation model selects idenprobability composing action model termination model selects particular consider composition expectation model termination model gives action-option model expectation equation ﬁxed point consider optimality option models. deﬁne optimality respect subgoal value model represents value termination option e.g. whether given subgoal achieved. argmax optimal option model argmax model respect subgoal value model options i.e. maximises policies termination conditions. consider option models maximise policies termination conditions subsequent section. represent optimal termination argmax model model composed model depending whether samg gives reward state optimise deterministic termination conditions optimal deterministic termination condition must exist deﬁne option-option model optimality equation optimal option model argmax assume given base options corresponding option models consider option expectation model averages base option models according hierarchical policy similarly equations contains expected rasp state option executed given base option models necessarily include primitive actions general possible construct policy models. instead consider hierarchical policy model supp⊆ω model set; analogous hierarchically optimal value function i.e. best achieved hierarchical constraints imposed choice base options. hierarchically optimal policy model unique ﬁxed point hierarchical optimality global optimality condition. contrast recursive optimality weaker local optimality condition assumes suboptions ﬁxed. many hierarchical reinforcement learning algorithms achieve recursive optimality hierarchical optimality. present general case option models composed option models. combines intra-option model learning interoption model learning step towards goal compositional planning. option-policy model composition assume given base options corresponding option models compose together. action-option model composition consider termination conditions well policies. combining ideas together gives option-option model expectation equation fact dimensions optimality option models optimality policy optimality termination condition previous sections dealt jointly optimal option models maximise policies termination conditions. consider option models optimise dimensions. optimal β-option model argmax argmax model options termination condition i.e. maximises policies given termination condition similarly optimal π-option model argmax argmax model options policy i.e. maximises termination conditions given policy deﬁne action-option model optimality equations optimal β-option models termination condition given; optimal π-option models policy given table summarise various model equations ﬁxed points. supplementary material prove ﬁxed point satisﬁes corresponding equations furthermore subgoal value ﬁxed point unique. bellman optimality equation forms basis wide variety planning algorithms similarly model optimality equations used derive wide variety planning algorithms. particular option-option model equations used derive algorithms compositional planning mdps. focus dynamic programming algorithm uses optionoption model optimality equation iterative update. algorithm call optionoption model iteration viewed generalisation value iteration option models multiple subgoals. assume given base option subgoal. option model initialised true value model every subgoal option model updated option-option model optimality equation maximisation performed base option oomi imposes explicit hierarchy model composed option model. updating option model subgoal value model current option models considered. particular option model considered; allows option models repeatedly squared single model eﬃciently applied many times required. result even oomi cantly fewer iterations value iteration. prove supplementary material oomi converges hierarchically optimal option model subgoal value model oomi includes true value model value models corresponding option model converge hierarchically optimal policy model optionoption model iteration similarly extended βoptions termination condition given; π-options policy given using equations respectively iterative updates. finally option-option expectation model equation used basis iterative update analogous policy iteration interleaves option evaluation option improvement. illustrate framework compositional planning using hierarchical mdps tower hanoi problem nine rooms problem. -disc tower hanoi problem discount factor grid. -level nine rooms gridworld contains grid instances level problems; neighbouring instances connected width doorway; single goal state corner. discount factor rewards goal state elsewhere. also stochastic variants action causes intended move doorways level hierarchy. subgoal value function deﬁned doorway predicate value state level-l doorway otherwise. problem initiation sets used restrict states considered relevant doorways within neighbourhood subgoal. subgoal values designed achieved cost choosing large constant proportionality. primitive actions base compare three solution methods. action-policy model iteration one-level planning algorithm plans primitive action models. iteratively applies action-policy model optimality equation equivalent value iteration. action-option-policy model iteration two-level planning algorithm ﬁxed planning operators. ﬁrst performs intra-option learning constructing option models primitive action models iteratively applying action-option model optimality equation ﬁxes option models performs inter-option plantable iterations backups state required solve given problems using action-policy model iteration action-option-policy model iteration option-option model iteration ning. finally constructs value function option models iteratively applying option-policy model optimality equation optionoption model iteration compositional planning algorithm described section algorithm measured total number iterations required; also mean number backups state. results shown table larger instances problems compositional approach required signiﬁcantly fewer iterations backups either planning two-level planning options ﬁrst created used. tower hanoi apmi aopmi required number iterations grew exponentially number discs whereas oomi required additional iteration disc deterministic case additional iterations disc stochastic case. nine rooms total iterations apmi aopmi grows exponentially level polynomially oomi. several important dimensions enabling analogous variety compositional planning algorithms. illustrated approach using optionoption model iteration. ﬁrst planning algorithm dynamically create planning operators. operators composed together give increasingly deep purposeful jumps state space. like value iteration option-option model iteration applies full-width backups complete sweeps state space. principle model equations could also solved sample backups sample trajectories leading compositional algorithms hierarchical reinforcement learning. paper focused planning table lookup models; however similar maxq hams skills substantial eﬃciency improvements generated option model provided state abstraction.", "year": 2012}