{"title": "Multiple Object Recognition with Visual Attention", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.", "text": "present attention-based model recognizing multiple objects images. proposed model deep recurrent neural network trained reinforcement learning attend relevant regions input image. show model learns localize recognize multiple objects despite given class labels training. evaluate model challenging task transcribing house number sequences google street view images show accurate state-of-the-art convolutional networks uses fewer parameters less computation. convolutional neural networks recently successful variety recognition classiﬁcation tasks main drawbacks convolutional networks poor scalability increasing input image size efﬁcient implementations models multiple gpus even spanning multiple machines become necessary. applications convnets multi-object sequence recognition images avoided working images instead focused using convnets recognizing characters short sequence segments image patches containing reasonably tightly cropped instances applying recognizer large images containing uncropped instances requires integrating separately trained sequence detector bottomproposal generator. non-maximum suppression often performed obtain ﬁnal detections. combining separate components trained using different objective functions shown worse end-to-end training single system domains integrating object localization recognition single globally-trainable architecture difﬁcult. work take inspiration humans perform visual sequence recognition tasks reading continually moving fovea next relevant object character recognizing individual object adding recognized object internal representation sequence. proposed system deep recurrent neural network step processes multi-resolution crop input image called glimpse. network uses information glimpse update internal representation input outputs next glimpse location possibly next object sequence. process continues model decides objects process. show proposed system trained end-to-end approximately maximizing variational lower bound label sequence log-likelihood. training procedure used train model localize recognize multiple objects purely label sequences. evaluate model task transcribing multi-digit house numbers publicly available google street view imagery. attention-based model outperforms state-of-the-art convnets tightly cropped inputs using fewer parameters much less computation. also show model outperforms convnets much larger margin realistic setting larger less tightly cropped input sequences. recognizing multiple objects images important goals computer vision. perhaps common approach image-based classiﬁcation character sequences involves combining sliding window detector character classiﬁer detector classiﬁer typically trained separately using different loss functions. seminal work convnets lecun introduced graph transformer network architecture recognizing sequence digits reading checks also showed whole system could trained end-to-end. system however still relied number ad-hoc components extracting candidate locations. recently convnets operating cropped sequences characters achieved state-of-theart performance house number recognition natural scene text recognition goodfellow trained separate convnets classiﬁer character position house number weights except output layer shared among classiﬁers. jaderberg showed synthetically generated images text used train convnets classiﬁers achieve state-of-the-art text recognition performance realworld images cropped text. work builds long line previous attempts attention-based visual processing particular extends recurrent attention model proposed mnih shown learn successful gaze strategies cluttered digit classiﬁcation tasks visual control problem shown scale real-world image tasks multiple objects. approach learning maximizing variational lower bound equivalent reinforcement learning procedure used related work maes showed reinforcement learning used tackle general structured prediction problems. simplicity ﬁrst describe model applied classifying single object later show extended multiple objects. processing image attentionbased model sequential process steps step consists saccade followed glimpse. step model receives location along glimpse observation taken location model uses observation update internal state outputs location process next time-step. usually number pixels glimpse much smaller number pixels original image making computational cost processing single glimpse independent size image. graphical representation model shown figure model broken number sub-components mapping input vector output. term network describe non-linear sub-components since typically multi-layered neural networks. glimpse network glimpse network non-linear function receives current input image patch glimpse location tuple input outputs vector glimpse network extract useful features location visual input. gimage denote output vector function gimage takes image patch parameterized weights wimage. gimage typically consists three convolutional hidden layers without pooling layers followed fully connected layer. separately location tuple mapped gloc using fully connected hidden layer where gimage gloc dimension. combine high bandwidth image information bandwidth location tuple multiplying vectors element-wise ﬁnal glimpse feature vector type multiplicative interaction what where initially proposed larochelle hinton recurrent network recurrent network aggregates information extracted individual glimpses combines information coherent manner preserves spatial information. glimpse feature vector glimpse network supplied input recurrent network time step. recurrent network consists recurrent layers non-linear function rrecur. deﬁned outputs recurrent layers long-short-term memory units non-linearity rrecur ability learn long-range dependencies stable learning dynamics. emission network emission network takes current state recurrent network input makes prediction extract next image patch glimpse network. acts controller directs attention based current internal states recurrent network. consists fully connected hidden layer maps feature vector recurrent layer coordinate tuple ˆln+. context network context network provides initial state recurrent network output used emission network predict location ﬁrst glimpse. context network takes down-sampled low-resolution version whole input image icoarse outputs ﬁxed length vector contextual information provides sensible hints potentially interesting regions given image. context network employs three convolutional layers coarse image icoarse feature vector used initial state recurrent layer recurrent network. however bottom layer initialized vector zeros reasons explain later. classiﬁcation network classiﬁcation network outputs prediction class label based ﬁnal feature vector lower recurrent layer. classiﬁcation network fully connected hidden layer softmax output layer class ideally deep recurrent attention model learn look locations relevant classifying objects interest. existence contextual information however provides short solution much easier model learn contextual information combining information different glimpses. prevent undesirable behavior connecting context network classiﬁcation network different recurrent layers deep model. result contextual information cannot used directly classiﬁcation network affects sequence glimpse locations produced model. given class labels image formulate learning supervised classiﬁcation problem cross entropy objective function. attention model predicts class label conditioned intermediate latent location variables glimpse extracts corresponding patches. thus maximize likelihood class label marginalizing glimpse locations glimpse glimpse sequence difﬁcult evaluate exponentially many glimpse locations training. summation equation approximated using monte carlo samples. equation gives practical algorithm train deep attention model. namely sample glimpse location prediction model glimpse. samples used standard backpropagation obtain estimator gradient model parameters. notice likelihood unbounded range introduce substantial high variance gradient estimator. especially sampled location object image likelihood induce undesired large gradient update backpropagated rest model. reduce variance estimator replacing discrete indicator function using baseline technique used mnih shown recurrent network state vector used estimate state-based baseline glimpse signiﬁcantly improve learning efﬁciency. baseline effectively centers random variable learned regressing towards expected value given indicator function baseline following gradient update where hyper-parameter balances scale gradient components. fact using indicator function learning rule equation equivalent reinforce learning rule employed mnih training attention model. viewed reinforcement learning update second term equation unbiased estimate gradient respect expected reward model glimpse inference feedforward location prediction used deterministic prediction location coordinates extract next input image patch model. model behaves normal feedforward network. alternatively marginalized objective function equation suggests procedure estimate expected class prediction using samples location sequences {˜lm allows attention model evaluated multiple times image classiﬁcation predictions averaged. practice found averaging probabilities gave best performance. paper encode real valued glimpse location tuple using cartesian coordinate centered middle input image. ratio converting unit width coordinate system number pixels hyper-parameter. ratio presents exploration versus exploitation trade off. proposed model performance sensitive setting. found setting value around input image width tends work well. proposed attention model easily extended solve classiﬁcation tasks involving multiple objects. train deep recurrent attention model sequential recognition task multiple object labels given image need cast ordered sequence y··· ys}. deep recurrent attention model learns predict object time explores image sequential manner. utilize simple ﬁxed number glimpses target sequence. addition class label end-of-sequence symbol included deal variable numbers objects image. stop recurrent attention model terminal symbol predicted. concretely objective function sequential prediction learning rule derived equation free energy gradient accumulated across targets. assign ﬁxed number glimpses target. assuming targets image model would trained glimpses. beneﬁt using recurrent model multiple object recognition compact simple form ﬂexible enough deal images containing variable numbers objects. learning model images many objects challenging setup. reduce difﬁculty modifying indicator function proportional number targets model predicted correctly. addition restrict gradient objective function contains glimpses ﬁrst mislabeled target ignores targets ﬁrst mistake. curriculum-like adaption learning crucial obtain high performance attention model sequential prediction. show effectiveness deep recurrent attention model ﬁrst investigate number multi-object classiﬁcation tasks involving variant mnist. apply proposed attention model real-world object recognition task using multi-digit svhn dataset netzer compare state-of-the-art deep convnets. description models training protocols used found appendix. figure left) examples learned policy digit pair classiﬁcation task. ﬁrst column shows input image next columns show selected glimpse locations. right) examples learned policy digit addition task. ﬁrst column shows input image next columns show selected glimpse locations. suggested mnih classiﬁcation performance improved glimpse network different scales. namely given glimpse location extract patches down-sampled coarser image patch. dram model context network signiﬁcantly outperforms models. challenging task designed another dataset mnist digits empty background task predict digits image classiﬁcation problem targets. model digit digits sampled uniformly classes label distribution heavily imbalanced summation probability mass concentrated around also many digit combinations mapped target example class label provides weaker association visual feature supervision signal task digit combination task. used model combination task. deep recurrent attention model able discover glimpse policy solve task achieving error rate. comparison convnets take longer learn perform worse given weak supervision. inference samples shown ﬁgure surprising learned glimpses policy predicting next glimpse different addition task comparing predicting combination task. model learned addition toggles glimpses digits. publicly available multi-digit street view house number dataset netzer consists images digits taken pictures house fronts. following goodfellow formed validation images randomly sampling images training extra used selecting learning rate sampling variance stochastic glimpse policy. models trained using remaining training images. follow preprocessing technique goodfellow generate tightly cropped images multi-digits center similar data augmentation used create jittered images training. also convert images grayscale observe color information affect ﬁnal classiﬁcation performance. trained model classify digits image sequentially objective function deﬁned equation label sequence ordering chosen left right natural ordering house number. attention model given glimpses digit making prediction. recurrent model keeps running predicts terminal label longest digit length dataset reached. svhn dataset digits appear image. means recurrent model glimpses image plus glimpses terminal label. learning attention model took around days gpu. model performance shown table found still performance state-of-the-art deep convnet single dram reads left right even monte carlo averaging. dram often predicts additional digits place terminal class. addition distribution leading digit real-life follows benford’s law. therefore train second recurrent attention model read house numbers right left backward model. forward backward model share weights glimpse networks different weights recurrent emission networks. predictions forward backward models combined estimate ﬁnal sequence prediction. following observation attention models often overestimate sequence length ﬁrst number sequence prediction backwards model shorter length sequence length prediction forward backward model. simple heuristic works well practice obtain state-of-the-art performance street view house number dataset forward-backward recurrent attention model. videos showing sample runs forward backward models svhn test data found http//www.psi.toronto.edu/˜jimmy/dram/forward.avi http//www.psi.toronto.edu/˜jimmy/dram/backward.avi respectively. visualizations show attention model learns follow slope multi-digit house numbers down. comparison also implemented deep convnet similar architecture used goodfellow network convolutional layers ﬁlters followed fully connected layers relu units. dropout applied layers dropout rate prevent over-ﬁtting. moreover generate less cropped multi-digit svhn dataset enlarging bounding image relative size digits stays images. deep attention model trained directly applied dataset modiﬁcation. performance improved focusing model digits are. model crop bounding around glimpse location sequence feed bounding attention model generate ﬁnal prediction. allows dram focus obtain similar prediction accuracy enlarged images cropped image without ever trained large images. also compared deep convnet trained images tuned attention model. deep attention model signiﬁcantly outperforms deep convnet little training time. dram model takes hours ﬁne-tune enlarged svhn data compared week deep layer convnet. experiments proposed deep recurrent attention model outperforms state-ofthe-art deep convnets standard svhn sequence recognition task. moreover increase image area around house numbers lower signal-to-noise ratio advantage attention model becomes signiﬁcant. table compare computational cost proposed deep recurrent attention model deep convnets terms number ﬂoat-pointing operations multi-digit svhn models along number parameters model. recurrent attention models process selected subset input scales better convnet looks entire image. estimated cost dram calculated using maximum sequence length dataset however expected computational cost much lower practice since house numbers around digits long. addition since attention based model process whole image naturally work images different size computational cost independent input dimensionality. also found attention-based model less prone over-ﬁtting convnets likely stochasticity glimpse policy training. though still beneﬁcial regularize attention model dropout noise hidden layers training found gives marginal performance boost multi-digit svhn task. hand deep layer convnet able achieve error rate dropout applied last fully connected hidden layer. finally note dram easily deal variable length label sequences. moreover model trained dataset ﬁxed sequence length easily transferred tuned similar dataset longer target sequences. especially useful lack data task longer sequences. described novel computer vision model uses attention mechanism decide focus computation showed trained end-to-end sequentially classify multiple objects image. model outperformed state-of-the-art convnets multi-digit house number recognition task using fewer parameters less computation best convnets thereby showing attention mechanisms improve accuracy efﬁciency convnets real-world task. since proposed deep recurrent attention model ﬂexible powerful efﬁcient believe promising approach tackling challenging computer vision tasks. would like thank geoffrey hinton nando freitas chris summerﬁeld many helpful comments discussions. would also like thank developers distbelief dean jeffrey corrado greg monga rajat chen devin matthieu mark senior andrew tucker paul yang quoc large scale distributed deep networks. advances neural information processing systems dean jeffrey corrado greg monga rajat chen devin matthieu mark senior andrew tucker paul yang quoc large scale distributed deep networks. advances neural information processing systems goodfellow bulatov yaroslav ibarz julian arnoud sacha shet vinay. multi-digit number recognition street view imagery using deep convolutional neural networks. arxiv preprint arxiv. netzer yuval wang coates adam bissacco alessandro andrew reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning volume used relu activation function hidden layers rest results reported otherwise noted. found relu units signiﬁcantly speed training. optimized model parameters using stochastic gradient descent nesterov momentum technique. mini-batch size used estimate gradient direction. momentum coefﬁcient throughout training. learning rate scheduling applied training improve convergence learning process. starts ﬁrst epoch exponentially reduced factor epoch. unit width cartesian coordinates glimpse location sampling standard deviation lstm units hidden units fully connected layer model. intentionally used simple fully connected single hidden layer network hidden units gimage glimpse network. unlike mnist experiment number digits image varies digits variations natural backgrounds lighting variation highly variable resolution. much larger deep recurrent attention model task. crucial powerful glimpse network obtain good performance. described section glimpse network consists three convolutional layers ﬁlter kernels ﬁrst layer later two. number ﬁlters layers }.there lstm units layer recurrent network. also fully connected hidden layers relu hidden units module listed section cartesian coordinate unit width pixels glimpse location sampled ﬁxed variance", "year": 2014}