{"title": "Partial Reinitialisation for Optimisers", "tag": ["stat.ML", "cs.LG", "cs.NE", "math.OC"], "abstract": "Heuristic optimisers which search for an optimal configuration of variables relative to an objective function often get stuck in local optima where the algorithm is unable to find further improvement. The standard approach to circumvent this problem involves periodically restarting the algorithm from random initial configurations when no further improvement can be found. We propose a method of partial reinitialization, whereby, in an attempt to find a better solution, only sub-sets of variables are re-initialised rather than the whole configuration. Much of the information gained from previous runs is hence retained. This leads to significant improvements in the quality of the solution found in a given time for a variety of optimisation problems in machine learning.", "text": "heuristic optimisers search optimal conﬁguration variables relative objective function often stuck local optima algorithm unable improvement. standard approach circumvent problem involves periodically restarting algorithm random initial conﬁgurations further improvement found. propose method partial reinitialization whereby attempt better solution sub-sets variables re-initialised rather whole conﬁguration. much information gained previous runs hence retained. leads signiﬁcant improvements quality solution found given time variety optimisation problems machine learning. multivariate optimisation central importance industry used range problems ﬁnding solutions satisfy constraints training classiﬁers machine learning. despite wide range applications nearly involve ﬁnding optimal assignment variables respect cost function. exact global optimum however rarely required general signiﬁcantly costly good local optimum. heuristic optimisers therefore ubiquitous many industrial application problems heuristic optimiser generally starts random conﬁguration variables optimises conﬁguration until local optimum found. convex optimisation problem optimum also global optimum effort required. however problems considered np-hard general exponentially many local optima hence probability particular local optimum also global exponentially suppressed. nevertheless problems often contain amount structure makes solution useful. structure used obtain conﬁgurations given cost much faster random guessing. such subsequent query cost function makes preceding queries make informed guess optimal conﬁguration. better current one. starting lets increase better local optimum reachable typical sub-set picked heuristic. current local optimum good number better local optima negligible increasing would reduce chance ﬁnding better local optimum. hence except beginning optimisation process optimiser higher chance ﬁnding better local optimum re-initialising sub-sets rather variables sub-sets variables reinitialised optimiser called re-initialisation conﬁguration becomes k-optimal high probability chance ﬁnding better local optimum decreases. prevent optimiser getting stuck k-optimum sub-sets variables re-initialised. turn k-local optima sub-set variables re-initialised repeating process iteratively time increasing size sub-set conﬁguration becomes n-optimal global optimum high probability. process hence reﬁne local optimizer global optimizer. call approach partial re-initialisation following algorithm levels hierarchy algorithm started level. conﬁguration denoted checkpoints checkpoints optimal conﬁgurations found thus algorithm reverts improved conﬁguration found partial reinitialization. level reinitialisations variables performed coml ml). number variables also heuristic picks variables sub-set important. simplest approach pick variables random. however figure illustration partial reinitialisation algorithm. level sub-sets variables reinitialised bottom level k-optimal optimiser called level starts optimal conﬁguration parent reinitialises sub-set variables calling optimiser next level checkpoints optimal conﬁguration found kept level left right strategy frequently used escape local optimum restart optimisation process random initial conﬁguration. repeating process multiple times local optimum lowest cost returned high probability better certainty worse initial conﬁguration. although restarts allow optimiser local optima different restarts also completely decoupled other. information learned structure problem restart passed next relearned scratch. hence running optimiser effectively coarse grained random guessing guess improved towards local optimum. lets assume heuristic picks sub-sets variables. also deﬁne k-optimality optimiser conﬁguration returns reinitialising variables typical sub-set smaller found heuristic conﬁguration local optimum. optimiser would return conﬁguration again. however reinitialising sub-sets variables allow optimiser local optima worse variables chosen according problem-speciﬁc heuristic probability re-initialising sub-set given size leads optimal conﬁguration maximised. approach pick sub-sets optimality variables within depends values variables much possible. could increase chance getting local optimum reducing number constraints sub-set rest system. outcome heuristic optimiser directly depend initial conﬁguration also random seed could used optimise variables within sub-set variables problem kept ﬁxed. approach employed ﬁnding ground states ising spin glasses simulated annealing showed signiﬁcant speedup relative conventional global restarts. optimisation problem space continuous variables concept partial reinitialisation extended partially reinitialising variable addition sub-sets variables. rather setting variable random value within pre-deﬁned domain perturbed example adding noise standard deviation mean standard deviation distribition hence either fully reinitialise subsets variables small perturbations variables combine partially perturb sub-sets variables could improve performance. simplicity benchmarks below. although chosen empirically optimise performance optimiser randomly chosen optimization problems also theoretical basis behind choice. chosen ensure probability local optima variables input) maximised. call conﬁguration probabilistically k–optimal. require probability reinitialization variables improve optimum less assert probability given reinitialisation improves objective function sufﬁces take thus values specify value needed conclude local optimum probabilistically koptimal within margin error. furthermore consider constant logarithmic number samples needed conclude high probability value k-optimal. small hand unstructured search number requisite reinitialisations exponential number variables. thus value taken makes implicit assumption structure complexity problem. machine learning problems study advantage partial reinitialisation compared standard full reinitialisation ﬁnding optimum model parameters. problems picked ones experience. size problem chosen large enough ﬁnding optimal conﬁguration non-trivial using respective standard algorithms. worth noting machine learning local optimum sometimes sufﬁcient even desired global optimum overtraining however show below partial reinitialisation improves quality ﬁnal solution also speed local optium given quality obtained. also although general expensive overtraining reduced using classicifation accuracy random sub-sets training data cost function. simplicity level hierarchy full reinitialisation calling heuristic optimiser. full reinitialisation multiple reinitialisations sub-sets variables perfomed. maintain generality choose sub-sets random benchmark problems. parameters benchmarks size subset number partial reinitialisations done within full reinitialisation manually optimised true optima respective performance metrics. performance measure benchmark optimal cost obtained given elapsed time number iterations averaged multiple runs different random initial states. elapsed time measured intel xeon processors. learning temporal patterns signal central importance wide range ﬁelds including speech recognition ﬁnance bioinformatics. classic method model systems hidden markov models based assumption signal follows markov process. future state system depends solely present state without memory past. figure learning random -bit sequences using baumwelch algorithm. median log-likelihood observe sequence within model elapsed training time runs different random seeds shown. full reinitialisations blue partial reinitialisations red. figure benchmark median within-cluster squares elapsed time runs k-means algorithm shown. blue curve full reinitialisations curve partial reinitialisations. discrete hmms consider here system possible states hidden observer. starting discrete probability distribution states time evolves system transition states according probability matrix hidden state emmit possible visible states. model hence composed three parts initial probability distribution length hidden states; transition matrix hidden states; emission matrix hidden state possible visible states. training given input sequence matrices optimised maximise likelihood sequence observed. standard algorithm training hmms baumwelch algorithm based forward-backward procedure computes posterior marginal distributions using dynamic programming approach. model commonly initialised random values optimised maximise expectation input sequence convergece local optimum. improve accuracy multiple restarts usually performed. sequence restarts investigate partial reinitialisation improve convergence rate towards global optimum. benchmark choose learning random -bit string. although artiﬁcial problem simple free parameters. time ﬁnding model accurately represents sequence non-trivial starting random transition emission matrices. number hidden states least large length bit-string model trained bit-string observed certainty within model. visible states number hidden states bits input. generality impose restrictions transition matrix. full reinitialisation starts random emission transition matrices. partial reinitialisation elements matrices corresponding sub-set hidden states reinitialised. found reinitialisations variables within global restart optimal leads much rapid increase accuracy training time full reinitialisations dividing objects clusters according similarity metric central importance data analysis employed ubiquitously machine learning. given points ﬁnite-dimensional space idea assign points clusters maximise similarities within cluster minimise similarities clusters. similarity deﬁned many ways depending particular needs application. euclidean distance. widely used algorithm ﬁnding clusters k-means algorithm kmeans searches assignment points clusters minimise within-cluster square distances center. seeks minimise following starting random initialisation centers iteration proceeds stages. first points assigned nearest cluster center. second part center picked euclidean center cluster. repeated convergence local optimum. cost never increased convergence guaranteed. shown exists problems k-means converges exponentially many iterations number points smoothed running time however polynomial typically converges quickly. similar baum-welch algorithm multiple global reinitialisations centers often performed improve quality clusters. benchmark problem standard clustering data clusters points sampled gaussian distributions. points around uniformly placed centres. full reinitialisation starts intial assignment centers random points using forgy’s method partial reinitialisation single cluster center reinitialised. found partial reinitialisations global restart optimal. except beginning given quality clusters obtained signiﬁcantly quicker partial rather full reinitialisation reason full reinitialisation efﬁcient ﬁnding quality clusters because regime cost high larger strides reducing cost made random guessing optimising global restart partial reinitialsiations. however cost threshold becomes harder reach partial reinitialisation quickly becomes efﬁcient. often robust approach called k-medoids clustering data pick best cluster center points cluster rather euclidean center. standard algorithm ﬁnding clusters partitioning around medoids similar k-means algorithm iterates assigning points closest center ﬁnding best center cluster convergence. figure benchmark problem clustering images faces olivetti face database clusters. afﬁnity propagation compared kmedoids full partial reinitialisation. afﬁnity propagation benchmark coresponding timings done using interface benchmark problem clustering images faces olivetti database dataset used previously benchmark novel method called afﬁnity propagation ﬁnding k-medoids clusters pam. repeat benchmark also compare partial reinitialisations. chose clusters roughly regime afﬁnity propagation largest advantage k-medoids within gloal restart found reinitialisations single randomly chosen cluster center optimal. pre-computed similarity matrix available entries pixel-wise squared distances images pre-processing. afﬁnity propagation indeed quickly ﬁnds much better clusters full reinitialisations. however time partial reinitialisations ﬁnds even better clusters keep improving even time full reinitialisation ﬁnds better cluters beginning similar reasons k-means. boltzmann machines class highly generalizable models related feed-forward neural networks proven useful modeling data sets many areas including speech vision goal boltzmann machine training replicate probfigure training restricted bolzmann machine absolute values objective functions relative difference mean approximate global optima found maximizing experimental runs. boltzmann machine takes following form deﬁne layers units call visible layer hidden layer. visible units comprise input output boltzmann machine hidden units latent variables marginalized order generate correlations present data. call vector visible units vector hidden units units typically taken binary joint probability conﬁguration visible hidden units energy. matrix weights models interaction pairs hidden visible units. vectors biases units. model viewed ising model complete bipartite graph thermal equilibrium. model commonly known restricted boltzmann machine rbms stacked form layered boltzmann machines sometimes called deep boltzmann machines. simplicity focus training rbms since training deep boltzmann machines using popular methods contrastive divergence training involves optimising weights biases layered independently. regularization term introduced prevent overﬁtting. exact computation training objective function hard means computation expected intractable large rbms reasonable complexity theoretic assumptions. although cannot efﬁciently computed derivatives efﬁciently estimated using method known contrastive divergence. algorithm described detail uses markov chain algorithm estimates expectation values hidden visible units needed compute derivatives oml. speciﬁcally ·data denotes expectation value gibbs distribution visible units clamped training data ·model denotes unconstrained expectation value. derivative respect biases similar. locally optimal conﬁgurations weights biases calculated stochastic gradient ascent using approximate gradients. benchmark examine small synthetic examples boltzmann machines training objective function calculated exactly. although differs task based estimates performance bolzmann machine focus contrastive divergence training algorithm formally seeks approximate gradients objective function. value training objective function natural metric comparison purposes classiﬁcation accuracy appropriate data set. bitwise negations. order make data challenging learn bernoulli noise training examples. hundred training examples used instance learning rate epochs reinitialization. take model consisting visible units hidden units. finally rather reinitialising ﬁxed number weights iteration update weight ﬁxed probability. formally differs previous approaches performs similarly reinitialising constant fraction. upon initialisation variable drawn zero-mean gaussian distribution. figure partial reinitialization accelerates process estimating global optima contrastive divergence training. optimal probability reinitializing weight bias roughly tends lead substantial reductions number reinitializations needed attain particular objective function. particular variables reset stage takes resets average obtain training objective function strategy resets every variable attains resets. furthermore evidence suggests possible polynomial advantage strategy traditional random resetting strategy. strong regularization expected lead much simpler landscape optimisation since optimisation problem becomes convex. difference partial total reinitisation strategies becomes order although partial reinitialising variables continues superior method small differences could also arise solely stochastic nature contrastive divergence training fact takes fewer epoches partial reinitialization approach local optima. evident benchmarks partial reinitialization substantially reduce complexity ﬁnding approximate global optimum boltzmann machines. consequently expect approach also work optimising boltzmann training large task– based problems mnist digit classiﬁcation. simintroduced general purpose approach termed partial re-initialisation signiﬁcantly improve performance optimiser. numerically explored comparisons state algorithms range optimisation problems machine learning although used basic version algorithm single additional level hierarchy picking sub-sets variables random advantage standard full reinitialisation substantial. expect hierarchy multiple levels sub-sets picked according advanced problem-speciﬁc heuristics lead even improvements. would like thank supernova inspiration imriska katzgraber kosenkov soluyanov svore wecker fruitful discussions. paper based upon work supported part odni iarpa lincoln laboratory force contract fa-c. views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied odni iarpa u.s. government. u.s. government authorized reproduce distribute reprints governmental purpose notwithstanding copyright annotation thereon. arthur david manthey bodo r¨oglin heiko. kpromeans polynomial smoothed complexity. ceedings annual ieee symposium foundations computer science focs washington ieee computer sovattani andrea. k-means requires exponentially many iterations even plane. discrete computational geometry issn ./s---. http//dx. doi.org/./s---. zhang lintao madigan conor moskewicz matthew malik sharad. efﬁcient conﬂict driven learning boolean satisﬁability solver. proceedings ieee/acm international conference computer-aided design ieee press burges christopher svore krysta marie bennett paul pastusiak andrzej qiang. learning rank using ensemble lambda-gradient models. yahoo learning rank challenge donmez pinar svore krysta burges christopher local optimality lambdarank. proceedings international sigir conference research development information retrieval theodoridis sergios koutroumbas konstantinos. schemes chapter clustering algorithms koutroumbased function optimization. patbas sergios theodoridiskonstantinos tern recognition", "year": 2015}