{"title": "cGANs with Projection Discriminator", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (ImageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.", "text": "propose novel projection based incorporate conditional information discriminator gans respects role conditional information underlining probabilistic model. approach contrast frameworks conditional gans used application today conditional information concatenating conditional vector feature vectors. modiﬁcation able signiﬁcantly improve quality class conditional image generation ilsvrc -class image dataset current state-of-the-art result achieved single pair discriminator generator. also able extend application super-resolution succeeded producing highly discriminative super-resolution images. structure also enabled high quality category transformation based parametric functional transformation conditional batch normalization layers generator. code chainer generated images pretrained models available https//github.com/pfnet-research/sngan_projection. generative adversarial networks framework construct generative model mimic target distribution recent years given birth arrays state-of-the-art algorithms generative models image domain distinctive feature gans discriminator evaluates divergence current generative distribution target distribution algorithm gans trains generator model iteratively training discriminator generator turn discriminator acting increasingly meticulous critic current generator. conditional gans type gans conditional information discriminator generator drawing attention promising tool class conditional image generation generation images text image image translation unlike standard gans discriminator cgans discriminates generator distribution target distribution pairs generated samples intended conditional variable authors’ knowledge frameworks discriminators cgans time writing feeds pair conditional information discriminator naively concatenating input feature vector middle layer would like however take account structure assumed conditional probabilistic models underlined structure discriminator function measures information theoretic distance generative distribution target distribution. construction assumption form distribution would regularization choice discriminator. paper propose speciﬁc form discriminator form motivated probabilistic model distribution conditional variable given discrete uni-modal continuous distributions. model assumption fact common many real world applications including class-conditional image generation super-resolution. explain next section adhering assumption give rise structure discriminator requires take inner product embedded condition vector feature vector modiﬁcation able signiﬁcantly improve quality class conditional image generation -class ilsvrc dataset single pair discriminator generator also applied model cgans super-resolution task able produce high quality super-resolution images discriminative terms accuracy label classiﬁer cgans based concatenation well bilinear bicubic method. denote input vector conditional information also denote cgan discriminator function parameters activation function users’ choice. using designate true distributions generator model respectively standard adversarial loss representing sigmoid function. construction nature ‘critic’ signiﬁcantly affects performance conventional feeding concatenate vector feature vector either input layer hidden layer would like propose alternative approach observing form optimal solution loss function decomposed likelihood ratios model likelihood ratio parametric functions respectively. make standing assumption simple distributions like gaussian discrete linear feature space then show parametrization following form becomes natural embedding matrix vector output function scalar function appears learned parameters trained optimize adversarial loss. point refer model discriminator projection short. next section would like elaborate arrive form. section begin speciﬁc often recurring models show that certain regularity assumption write optimal solution discriminator objective function form ﬁrst consider case categorical variable. assume categorical variable taking value often common class conditional image generation task. popular model following linear model partition function input ﬁnal layer network model. assume target distribution also parametrized form choice likelihood ratio would take following form; using denote one-hot vector label using denote matrix consisting vectors notably formulation introduces label information inner product opposed concatenation. form indeed form proposed also arrive form unimodal continuous distributions well. d-dimensional continuous variable assume conditional given gaussian distributions represents terms independent assume ignore quadratic term. express form arrive form again. indeed however regularization affects training generator little unclear formulation. repeatedly explained discriminator measures divergence generator distribution target distribution assumption relatively simple highly possible gaining stability training process imposing regularity condition divergence measure. meanwhile however actual implicitly derived computation possibly take numerous forms ones considered here. must admit room important theoretical work done order assess relationship choice function space discriminator training process generator. described above form true frequently occurring situations. contrast incorporation conditional information concatenation rather arbitrary possibly include pool candidate functions sets functions difﬁcult logical basis. indeed situation calls multimodal might smart model suggest here. otherwise however expect model perform better; general preferable discriminator respects presumed form probabilistic model. still another incorporate conditional information training procedure directly manipulate loss function. algorithm ac-gans discriminator shares part structure classiﬁer incorporates label information objective function augmenting original discriminator objective likelihood score classiﬁer generated training dataset plug play generative models another approach generative model uses auxiliary classiﬁer function. method endeavors make samples using mcmc sampler based langevin equation drift terms consisting gradient autoencoder prior pretrained auxiliary classiﬁer method generate high quality image. however ways using auxiliary classiﬁer unwittingly encourage generator produce images particularly easy auxiliary classiﬁer classify deviate ﬁnal true fact odena reports problem tendency exacerbate increasing number labels. able reproduce phenomena experiments; implemented algorithm dataset class categories ﬁnal trained model able generate image classes. nguyen al.’s ppgns also likely suffer problem using order magnitude greater coefﬁcient term corresponding terms langevin equation. order evaluate effectiveness newly proposed architecture discriminator conducted sets experiments class conditional image generation super-resolution ilsvrc dataset tasks used resnet based discriminator generator used gulrajani applied spectral normalization weights discriminator regularize lipschitz constant. objective function used following hinge version standard adversarial loss last activation function identity function. standard gaussian distribution generator network. experiments used adam optimizer hyper-parameters updated discriminator times update generator. concat designate models projection designate proposed model imagenet dataset used experiment class conditional image generation consisted image classes approximately pictures each. compressed images pixels. unlike ac-gans used single pair resnet-based generator discriminator. also used conditional batch normalization generator. architecture generator network used experiment please figure detail. proposed projection model discriminator equipped ‘projection layer’ takes inner product embedded one-hot vector intermediate output structure concat model discriminator compared against used identical bulk architecture projection model discriminator except removed projection layer structure concatenated spatially replicated embedded conditional vector output third resblock. also experimented ac-gans current state model. ac-gans placed softmax layer classiﬁer structure shared concat projection. method updated generator times applied linear decay learning rate iterations rate would end. comparative experiments trained model iterations ample training concat stabilize. ac-gans collapsed prematurely completion iterations reported result peak performance experiments throughout used training iterations comparing performances. separate note method continued improve even therefore also reported inception score extended training method exclusively. table exact ﬁgures. used inception score evaluation visual appearance generated images. general difﬁcult evaluate ‘good’ generative model indeed however either subjective objective deﬁnite measures ‘goodness’ exists essential ‘diversity’ sheer visual quality images. possible candidate quantitative measure diversity visual appearance computed generated images dataset images within class designated values intra fids. precisely measures -wasserstein distance distributions given covariance ﬁnal feature vectors produced inception model true samples generated samples class generated examples collapsed modes trace becomes small trace term becomes large. order compute used samples training data belonging class concern used generated samples computation cpy. empirically observed experiments intra certain extent serving purpose well measuring diversity visual quality. highlight effectiveness inner-product based approach introducing conditional information model compared method state acgans well conventional incorporation conditional information concatenation preliminary experiments image geneation task cifar- cifar- conﬁrmed hidden concatenation better input concatenation terms inception scores. details please table appendix section. hidden layer training curves figure projection outperforms inception score concat throughout training. table compares intra class fids inception score images generated method. result shown ac-gans model prime terms inception score training collapsed end. images generated projection lower intra scores adversaries indicating wasserstein distance generative distribution projection target distribution smaller. record model performed better models cifar cifar well figure shows classes projection yielded results better intra fids concat reverse. ﬁgures listed descending order ratio intra score methods. note concat outperforms projection wins slight margin whereas projection outperforms concat large margin opposite case. quick glance cases concat outperforms projection suggests fact measuring visual quality sets looks similar human eyes terms appearance. figure shows arbitrarily selected results yielded ac-gans variety clearly observe mode-collapse batch. indeed tendency reported inventors acgans generate easily recognizable images cost losing diversity hence cost constructing generative distribution signiﬁcantly different target distribution whole. also assess score projection different perspective. construction trace term intra measures degree diversity within class. thus result intra scores also indicates projection better reproducing diversity original. gans concat discriminator also suffered mode-collapse classes images generated projection able detect notable mode-collapse. figure shows samples generated projection model classes cgan achieved lowest intra scores figure reverse. images listed figure relatively high quality still observe degree mode-collapse. note images classes high featuring complex objects like human; expect diversity within class wide. however note complicated neural network available experiments presented paper prioritized completion training within reasonable time frame. possible that increasing complexity model able improve category morphing architecture also able successfully perform category morphism. classes create interpolated generator simply mixing parameters conditional batch normalization layers conditional generator corresponding classes. figure shows output interpolated generator interestingly combination also yielding meaningful images signiﬁcantly different. fine-tuning pretrained model ilsvrc classiﬁcation task. mentioned section authors plug play generative model able improve visual appearance model augmenting cost function label classiﬁer. also followed footstep augmented original generator loss additional auxiliary classiﬁer loss. warned earlier regarding type approach however type modiﬁcation tends improve visual performance images easy pretrained model classify. fact appendix able improve visual appearance images augmentation cost diversity. also evaluated effectiveness application super-resolution task. formally super-resolution task infer high resolution image dimension rrh×rh× resolution image dimension rrl×rl×; task much case presumed model construction likely unimodal even multimodal. super-resolution task used following formulation discriminator function figure pixel images generated projection method classes bottom scores scores. string value panel respectively name corresponding class score. second panel corresponds original dataset. task. experiments constructed concat model removing module projection model containing inner product layer accompanying convolution layer altogether simply concatenated output resblock preceding inner product module original. resolutions image datasets chose created resolution images applying bilinear downsampling high resolution images. updated generators times methods applied linear decay learning rate iterations ﬁnal learning rate k-th iteration. figure shows result super-resolution. bicubic super-resolution blurry concat result suffering excessively sharp rough edges. hand edges images generated projection method much clearer smoother image much faithful original high resolution images. order qualitatively compare performances models checked ms-ssim classiﬁcation accuracy inception model generated images using validation ilsvrc dataset. table projection model able achieve high inception accuracy high ms-ssim compared bicubic concat. note performance superresolution concat model even falls behind bilinear bicubic super-resolutions terms inception accuracy. also used projection model generate multiple batches images different random values generator computed average logits inception model batches used so-computed average logits make prediction labels. ensemble seeds able improve inception accuracy even further. result indicates gans learning super-resolution distribution opposed deterministic function. also success ensemble also suggests room improve accuracy classiﬁcation task resolution images. speciﬁcation form discriminator imposes regularity condition choice generator distribution target distribution. research proposed model discriminator cgans motivated commonly occurring family probabilistic models. simple modiﬁcation able signiﬁcantly improve performance trained generator conditional image generation task super-resolution task. result presented paper strongly suggestive importance choice form discriminator design distributional metric. plan extend approach applications cgans semantic segmentation tasks image image translation tasks. would like thank members preferred networks inc. especially richard calland sosuke kobayashi crissman loomis helpful comments. would also like thank shoichiro yamaguchi graduate student kyoto university helpful comments. martin heusel hubert ramsauer thomas unterthiner bernhard nessler g¨unter klambauer sepp hochreiter. gans trained time-scale update rule converge nash equilibrium. arxiv preprint arxiv. diederik kingma jimmy adam method stochastic optimization. iclr christian ledig lucas theis ferenc husz´ar jose caballero andrew cunningham alejandro acosta andrew aitken alykhan tejani johannes totz zehan wang wenzhe shi. photo-realistic single image superresolution using generative adversarial network. cvpr hyun jong chul geometric gan. arxiv preprint arxiv. mehdi mirza simon osindero. conditional generative adversarial nets. arxiv preprint arxiv. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision ./s---y. seiya tokui kenta oono shohei hido justin clayton. chainer next-generation open source framework deep learning. proceedings workshop machine learning systems twentyninth annual conference neural information processing systems antonio torralba fergus william freeman. million tiny images large data nonparametric object scene recognition. ieee transactions pattern analysis machine intelligence zhang hongsheng shaoting zhang xiaolei huang xiaogang wang dimitris metaxas. stackgan text photo-realistic image synthesis stacked generative adversarial networks. iccv preliminary experiment compared performance conditional image generation cifar- cifar- discriminator generator reused architecture used miyato task cifar-. adversarial objective functions used trained machine learners optimizer hyper parameters used section projection model added projection layer discriminator imagenet experiment projection model achieved better performance methods cifar- cifar-. concatenation hidden layer performed output second resblock discriminator. tested hidden concat comparative method main experiments imagenet concatenation hidden layer performed better concatenation input layer number classes large explore hyper-parameters affect performance proposed architecture conducted hyper-parameter search cifar- adam hyper-parameters proposed architecture baselines. namely varied parameters keeping constant reported inception scores methods including several versions concat architectures compare. tested concat module introduced input layer hidden layer output layer. figure projection architecture excelled architectures choice parameters achieved inception score meanwhile concat architectures able achieve most. best concat model term inception score cifar- hidden concat turns choice parameters picked imagenet experiment. experiment followed footsteps plug play generative model augmented original generator loss additional auxiliary classiﬁer loss. particular used losses given ˆppre ﬁxed model pretrained ilsvrc classiﬁcation task. actual experiment trained generator original adversarial loss ﬁrst updates used augmented loss last updates. learning rate hyper parameter adopted values experiments described above. pretrained classiﬁer used resnet model used figure compares results generated vanilla objective function results generated augmented objective function. table able signiﬁcantly outperform ppgns terms inception score. however note images generated images easy classify. method auxiliary classiﬁer loss seems effective improving visual appearance training faithful generative model. figure architecture resblocks used experiments. generator generator’s resblock conditional batch normalization layer used place standard batch normalization layer. resblock generator super resolution tasks implements upsampling random vector model concatenating vector embedded resolution image vector prior ﬁrst convolution layer within block. procedure downsampling upsampling followed implementation gulrajani discriminator performed downsampling second conv resblock. generator performed upsampling ﬁrst conv resblock. resblock performing downsampling replaced identity mapping conv layer followed downsampling balance dimension. essentially resblock performing upsampling except applied upsampling conv.", "year": 2018}