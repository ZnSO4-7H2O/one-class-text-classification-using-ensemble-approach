{"title": "Exponentially Increasing the Capacity-to-Computation Ratio for  Conditional Computation in Deep Learning", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Many state-of-the-art results obtained with deep networks are achieved with the largest models that could be trained, and if more computation power was available, we might be able to exploit much larger datasets in order to improve generalization ability. Whereas in learning algorithms such as decision trees the ratio of capacity (e.g., the number of parameters) to computation is very favorable (up to exponentially more parameters than computation), the ratio is essentially 1 for deep neural networks. Conditional computation has been proposed as a way to increase the capacity of a deep neural network without increasing the amount of computation required, by activating some parameters and computation \"on-demand\", on a per-example basis. In this note, we propose a novel parametrization of weight matrices in neural networks which has the potential to increase up to exponentially the ratio of the number of parameters to computation. The proposed approach is based on turning on some parameters (weight matrices) when specific bit patterns of hidden unit activations are obtained. In order to better control for the overfitting that might result, we propose a parametrization that is tree-structured, where each node of the tree corresponds to a prefix of a sequence of sign bits, or gating units, associated with hidden units.", "text": "many state-of-the-art results obtained deep networks achieved largest models could trained computation power available might able exploit much larger datasets order improve generalization ability. whereas learning algorithms decision trees ratio capacity computation favorable ratio essentially deep neural networks. conditional computation proposed increase capacity deep neural network without increasing amount computation required activating parameters computation on-demand per-example basis. note propose novel parametrization weight matrices neural networks potential increase exponentially ratio number parameters computation. proposed approach based turning parameters speciﬁc patterns hidden unit activations obtained. order better control overﬁtting might result propose parametrization tree-structured node tree corresponds preﬁx sequence sign bits gating units associated hidden units. deep learning learning hierarchically-organized representations higher levels corresponding abstract concepts automatically learned data either supervised unsupervised semi-supervised reinforcement learning bengio recent review. number breakthroughs application deep learning e.g. speech computer vision involve deep neural networks much capacity possible given constraints training test time made experiments reasonably feasible. recently reported bigger models could yield better generalization number datasets provided appropriate regularization dropout used. experiments however generally limited training time amount training data could exploited. important factor recent breakthroughs availability gpus allowed training deep nets least times faster often however whereas task recognizing handwritten digits trafﬁc signs faces solved point achieving roughly human-level performance true tasks computing power could harnessed training could train correspondingly larger models correspondingly larger datasets covering categories modalities concepts. important considering current neural network models still small size compared biological brains even reaching size animals frogs several orders magnitude less mammals humans. sense expect much larger models needed build computers truly master visual world world ideas expressed language i.e. make sense world around level comparable child. moore’s practically saturated considers computing power single computing core. continued growth computing power comes parallelization. unfortunately despite impressive progress recent years exploiting large computer clusters efﬁciently parallelize training procedures deep neural networks remains challenge. furthermore additionally faster training applications want faster inference test. thus question need besides distributed training ways build deep neural networks much higher capacity without waiting decade hardware evolve required level? bengio bengio proposed notion conditional computation deep learning answer positively question. idea activate small fraction parameters model particular examples correspondingly reduce amount computation performed. currently ratio number parameters amount computation essentially deep nets i.e. every parameter touched example. contrast machine learning models decision trees much favorable ratio computations decision tree actively select parameters pool unfortunately decision trees suffer poor statistical properties prevent them like many non-parametric techniques relying smoothness prior generalizing non-trivial regions input space training examples. cucker grigoriev bengio mathematical analysis case decision trees bengio longer analysis covering wider class learning algorithms gaussian kernel svms graph-based non-parametric statistical models. hand theoretical empirical indications suggesting deep distributed representations beneﬁt advantageous statistical properties data generated multiple factors organized hierarchically characteristics factor learnable without requiring conﬁgurations factors. conditional computation deep learning well paper aimed combining statistical efﬁciency deep learning computational efﬁciency terms ratio capacity computation algorithms decision trees. objective mind propose novel parametrize deep neural networks allows exponential increase ratio number parameters computation. words allow exponentially many parameters respect amount computation. achieve observing exploit patterns associated hidden units order selectively activate different weight vectors weight matrices. since number patterns grow exponentially number bits considered gives required rate growth controllable maximum size patterns. consider single layer consisting p-dimensional input vector q-dimensional output vector conventional approach layer parametrized weight matrix rp×q similarly conventional approach single layer consists however weight matrix anymore independent input variable parametrized using basic idea bits derived weight matrices deﬁned used deﬁne actual weight matrix mapping predeﬁned scalar threshold. also possible make stochastic decision sampled bernoulli distribution mean rp×k. using binary indicators obtain column weight matrix function using bits obtain subset elements maps binary k-dimensional vector vector output weights unit example simply look-up table indexed simply ﬁrst bits consecutive bits indexed ⌊j/k⌋ ⌊j/k⌋ view generalization three-way connections found models -way interactions example sutskever select different recurrent weight matrix recurrent neural network current state next state depending input parametrization proposed enables association weight vectors unit triggered particular values selected bits number parameters therefore required computation depends cost table look-up followed actual computation matrix multiplication i.e. next section describe particular strategy implementing aims improve generalization. potential issue proposed scheme model easily overﬁt training samples fraction samples used activate/update possible weight vectors. beside obvious regularization choosing small propose additional device inspired impressive success smoothed interpolated n-grams back-off models statistical language modeling basic idea maintain weight vectors indexed sequences different lengths. vectors associated shorter sequences updated examples therefore requiring much regularization. weight vectors indexed longer sequences examples used make small corrections needed data. regularization simply norms weight vectors. regularization either weight decay automatically penalize less often activated since weight vector activated receive gradient counterbalance regularizer’s pull towards understood intuitively imagining binary tree depth node weight matrix. procedure traverses tree root leaves using sequence sums j-th columns nodes’ weight matrices weight vector computation involves additions unit instead small constant total. noticeable time reasonable overhead multiply-adds required actual matrix multiplication. number parameters much larger proposed scheme efﬁcient implement weight decay regularization selected weight vectors update regularized. however case must keep track interval since weight vector last updated current update step last time weight vector updated. next time weight vector chosen treat weight vector specially compensate lost steps regularization. issue raised earlier bengio question training signal gating decisions i.e. credit assignment gating decisions. correct update parameters associated gating units order improve gating decisions? interesting hypothesis sufﬁcient back-prop usual network ignoring effect gating units choice weight vectors although gating decisions adapted toward minimizing training loss case weight vectors regardlessly updated according objective training. words long gating units perform reasonable partitioning input space might good enough adapt exponentially many parameters stored table index input vector assumed output rectiﬁer i.e. non-negative. hence unit active tends turn weight contributions controls outside power normalizes length controlling sequence. greatest challenges expand scope applicability performance deep neural networks ability increase capacity without increasing required computations much. approach proposed paper potential achieve exponential increases ratio controllable way. future work clearly required validate proposal experimentally large enough datasets increased capacity would actually valuable speech language datasets order billion examples.", "year": 2014}