{"title": "Using Discretization for Extending the Set of Predictive Features", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "To date, attribute discretization is typically performed by replacing the original set of continuous features with a transposed set of discrete ones. This paper provides support for a new idea that discretized features should often be used in addition to existing features and as such, datasets should be extended, and not replaced, by discretization. We also claim that discretization algorithms should be developed with the explicit purpose of enriching a non-discretized dataset with discretized values. We present such an algorithm, D-MIAT, a supervised algorithm that discretizes data based on Minority Interesting Attribute Thresholds. D-MIAT only generates new features when strong indications exist for one of the target values needing to be learned and thus is intended to be used in addition to the original data. We present extensive empirical results demonstrating the success of using D-MIAT on $ 28 $ benchmark datasets. We also demonstrate that $ 10 $ other discretization algorithms can also be used to generate features that yield improved performance when used in combination with the original non-discretized data. Our results show that the best predictive performance is attained using a combination of the original dataset with added features from a \"standard\" supervised discretization algorithm and D-MIAT.", "text": "abstract date attribute discretization typically performed replacing original continuous features transposed discrete ones. paper provides support idea discretized features often used addition existing features such datasets extended replaced discretization. also claim discretization algorithms developed explicit purpose enriching non-discretized dataset discretized values. present algorithm d-miat supervised algorithm discretizes data based minority interesting attribute thresholds. d-miat generates features strong indications exist target values needing learned thus intended used addition original data. present extensive empirical results demonstrating success using d-miat benchmark datasets. also demonstrate discretization algorithms also used generate features yield improved performance used combination original non-discretized data. results show best predictive performance attained using combination original dataset added features standard supervised discretization algorithm d-miat. discretization data preprocessing technique transforms continuous attributes discrete ones. accomplished dividing numeric attribute discrete intervals minimal value maximal value resulting values within constitute discretization scheme attribute dm−} points. traditionally discretization used place original values preprocessing data used instead basic types discretization exist supervised unsupervised. unsupervised discretization divides ﬁxed number intervals within typically equal– width equal–frequency heuristics supervised discretization considers target class creating popular supervised discretization algorithm based information entropy maximization whereby points created minimize entropy within interesting by-product supervised discretization methods points found according selection criteria created variable effectively eliminating dataset null set. supervised discretization also function type feature selection large number supervised discretization algorithms developed addition including ameva caim cacc modied-chi ur-caim refer reader recent survey detailed comparison algorithms. discretization originally performed necessary preprocessing step certain supervised machine learning algorithms require discrete feature spaces several addition advantages since noted. first discretization times improve performance classiﬁcation algorithms require discretization. consequently multiple studies used instead achieve improved prediction performance recently phenomenon particularly noted within medical bioinformatic datasets hypothesized source improvement attribute selection component evident within supervised discretization algorithms second advantage discretization contributes interpretability machine learning results people better understand connection different ranges values impact learned target paper’s ﬁrst claim discretization necessarily used replace dataset’s original values instead generate features augment existing dataset. such discretized feature used addition original feature using discretization fashion best knowledge completely novel idea. claim using datasets often improve performance classiﬁcation algorithm given dataset. claim somewhat surprising seem intuitive. widely established reducing number features feature selection extension using discretization feature selection helps improve prediction performance core claim curse dimensionality overcome reducing number candidate predictive attributes. counter claim posit number attributes problematic lack added value within attributes. times information gained discretized attributes signiﬁcant point addition original attributes improves prediction accuracy. paper also contains second claim discretization algorithms explicitly developed purpose augmenting original data. support point present d-miat algorithm discretizes numeric data based minority interesting attribute thresholds. believe d-miat unique best knowledge ﬁrst discretization algorithm explicitly extends features discretization. d-miat generates features minority attribute’s values strongly point target classes. claim times important create discretized features indications. however attribute selection approaches date typically treat values within given attribute equally thus focus general importance values within given attribute combinations full different attributes’ values hence approaches would typically focus points based strong indications within small subset values. again potential success d-miat counterintuitive generates features addition original dataset something often believed reduce performance support claims studied prediction accuracy within datasets previous discretization study considered performance different classiﬁcation algorithms naive bayes k-nn adaboost random forest logistic regression different types datasets. first considered accuracy original dataset without discretization. second created dataset original data combined features generated d-miat studied combination successful. third compared accuracy baseline datasets discretized datasets algorithms previously considered using training testing data paper. somewhat surprisingly found prediction accuracy average decreased discretized data considered. based understanding discretized features improve performance created fourth dataset appended original data features generated canonical discretization algorithms creating combined datasets based again noted combination discretized features original data improved predictive performance. fifth studied combinations features different discretization algorithms created. speciﬁcally created dataset combined discretized values d-miat discretization algorithms best prediction performance. combination three types features original data standard discretization algorithms d-miat performed best. contrary discretization algorithms d-miat explicitly developed augment original data discretized features. generate features strong indications exist target classes even within relatively small subsets values within working hypothesis generated features improve prediction accuracy encapsulating features classiﬁcation algorithms could miss original data using standard discretization methods. assumption motivated recent ﬁndings times values important subsets attributes serve either triggers indicators biological processes. recent genomic transciptomic research shown people natural predisposition immunity towards certain diseases similarly posit even small subsets values pointing target classes signiﬁcant even within non-medical datasets. success d-miat ﬁnding subsets thus shift focus studying attribute values done date discretization algorithms ﬁnding important subsets values within range given attribute. make general idea clearer consider following example. assume attribute numeric value many cigarettes person smokes given day. dataset contains total population smoked cigarettes day. heavier smokers develop cancer cancer rates within remaining elevated. traditional discretization analyze values within attribute equally thus ignore relatively small evidently important subset dataset. methods discretize based overall effectiveness discretization criteria entropy reduction attribute interesting subset necessarily large enough signiﬁcant entropy reduction within attribute. accordingly discretize attribute effectively removing dataset. similarly even discretization algorithms selection measures based information gain typically ignore signiﬁcance subset minority attribute values size something d-miat explicitly designed speciﬁcally deﬁne d-miat algorithm follows assume numeric attributes exist dataset denoted given attribute continuous values sorted denoted valb. target values within target variable discretized created subset minimum size supp exists strong indication conf target values within d-miat’s assumption attempt create cuts within minimum maximum value valb cases size subset needs contain least supp records. considered types strong indications support type based entropy based lift. similar considered conﬁdence thresholds based −pilogpi relative size within value used threshold quantify support percentage values within given attribute points given target example smallest values point ﬁrst target value entropy within subset thus constitutes strong indication. second type conﬁdence based lift typically deﬁned conditional probability given divided general probability lift content support measure relative strength subset target value given general probability. example consider subset lowest values within respect relative strength subset deﬁned probability p)|c/ p).assuming general probability occurs times records occurrences within lowest values additional times within values lift assuming d-miat’s value conf parameter less subset would considered signiﬁcant d-miat create discretized subset. worth noting parameters within d-miat supp conf motivated association rule learning cases attempt even relatively small subsets values however minimum amount support supp. similarly cases refer conﬁdence threshold deﬁned absolute relative number instances within subset corresponding target value. best knowledge d-miat ﬁrst discretization algorithm using support conﬁdence signiﬁcant subsets attribute values. based deﬁnitions algorithm presents d-miat. line begins creating null discretized features extracted full attributes lines loop attributes within dataset sort continuous values within attribute consider target variable consider discretization ranges beginning smallest value ending largest value algorithm uses binary search potential points based selection criteria conf. trivially step could bound equal smallest value within bound aj’s largest value. typically subset beyond points. regardless lines check number records within interval larger predeﬁned support threshold supp. discretized variable created. implementation discretized variable values original values within zero value zero lines cuts generated features within thus algorithm potentially create cuts every value typically create much fewer stringent requirements typically assigned supp conf association rule algorithms. line returns dataset combining original attributes discretized features d-miat. motivation d-miat’s attempt potentially create cuts extremes search support within subset association rule theory based medical research. motivation d-miat comes observation probability distribution features relating medical classiﬁcation problem need unimodal. fact discussed previous smoker example features likely multi–modal modes representation sampled dataset within modes prevalence class maybe signiﬁcantly different remainder samples. furthermore expect discretized cuts extreme values would highest amount interpretability main general motivations behind discretization table presents small portion datasets glass used experiments. dataset goal identify type glass target values present attributes within dataset limited space size dataset subset values attributes shown. d-miat using minimal support parameter dataset lift value minimal conﬁdence note values sorted non-zero values highlighted ﬁrst column table algorithm found records training records belonged class class occurs approximately probability within training approximately probability d-miat computed lift|a >.)= p/p) much greater thus d-miat created higher values based line algorithm resulting shown column table note discretized values within range including target values class created based lower values size resulting greater required support. conversely created based value bound line algorithm here d-miat found strong indications target values– target number found instances training values less corresponded target probability signiﬁcantly higher general probability target within dataset thus lift greater comparison present columns discretized cuts algorithm three attributes. algorithm based entropy minimization heuristic discretization continuous value intervals. attribute minimized overall entropy thus created intervals– either less greater value. attribute intervals created intervals created example highlights similarities differences dmiat discretization algorithms. first d-miat binary discretization method– d-miat creates intervals supp conf met. contrast classic discretization algorithms maximize score attribute values thus algorithms often choose different numbers cuts thresholds cuts different values impact interpretability results. d-miat focuses subset focus one’s analysis range values either given threshold given threshold table examples. please note ﬁrst d-miat focuses one’s attention higher range values strong indication exists target value second focuses one’s attention smaller range values strong indication exists target value often algorithms agree nothing interest. note discretized cuts formed either d-miat algorithms’ conditions creating cuts present attribute. explain below experiments generated discretization cuts generated training data applied testing data. used datasets within recent discretization study datasets collected keel machine learning repositories represent variety complexity number classes number attributes number instances imbalance ratio detailed information datasets found online. downloaded discretized versions datasets discretization algorithms considered. included unsupervised algorithms equal-width equal-frequency supervised algorithms information entropy maximization class-attribute interdependence maximization ameva modied-chi hypercube divisionbased discretization class-attribute contingency coefcient interval distance-based method discretization ur-caim datasets contain different folds whereby discretized intervals determined training portion ﬁrst intervals applied remaining data used testing. thus datasets contained independently constructed training testing components effectively create -fold cross validation total training testing pairs. general thesis work adding features inherently detrimental long features added value. conversely claim traditional discretization continuous values replaced discretized ones detrimental important information lost removing original continuous values. expected datasets enriched d-miat provide accurate results without generally removing original features favor exclusively using discretized ones would less effective using combination features extended set. checked several research questions study issues order check issues proceeded create different sets data. ﬁrst data composed base datasets without modiﬁcation training testing components used data previous study second data original data addition discretized cuts d-miat. third data consisted discretized versions datasets above-mentioned algorithms also generated based previous study fourth data appended original datasets discretized datasets last created ﬁfth data appending original data features created d-miat several best performing discretization algorithms. facilitate replicating experiments future made matlab version d-miat available http//homedir.jct.ac.il/∼rosenfa/d-miat.zip. d-miat personal computer measured predictive accuracy different classiﬁcation algorithms naive bayes using kernel k-nn using value adaboost random forest logistic regression datasets. ﬁrst algorithms implemented weka parameters used previous work last algorithm added well-accepted deterministic classiﬁcation algorithm present previous study. default parameters used within weka’s simple logistic implementation algorithm. shown below found classic discretization often improve average performance across datasets. instead using discretization addition original features typically yielded better performance either using d-miat addition original data using discretized features canonical algorithms addition original data. best performance typically reached combined dataset original data enriched d-miat discretization algorithms posited research question thus present evidence effective pipeline using d-miat along discretized features extend given features. goal d-miat algorithm supplement supplant original data. decision whether additional features generated depends supp conf parameters algorithm applied every numeric attribute within dataset. thus d-miat could potentially generate features every attribute based lines algorithm experiments deﬁned supp equal training data. three conﬁdence values conf checked. ﬁrst value conf=entropy meaning yielded completely decisive indication target classes thus yielded zero entropy ﬁrst type conﬁdence mentioned above. also considered types lift conﬁdence conf= lift conf=lift. conﬁdence levels checked yielded stronger indication target classes measured lift values. potentially lift thresholds could satisﬁed. example assume yielded lift conﬁdence thresholds would consider signiﬁcant generate feature accordingly. also considered possibility cuts could added cumulatively thus overlapping cuts could added based combinations different thresholds. conversely thresholds used cuts created given attribute. illustrate point table presents number continuous attributes datasets total number attributes d-miat generated within datasets combined thresholds conf considered. d-miat generated cuts exclusively based different iterations within training possible number features d-miat generated would vary across iterations within datasets. example note ﬁrst datasets abalone arrhythmia features generated d-miat across iterations values conf supp. shows d-miat stringent condition generating cuts canonical discretization algorithms previously studied generate features datasets. also note d-miat generated average features within glass dataset. based training data d-miat generated features leading average fraction. times case here number discretized features generated d-miat less number original features. however notable example found iris dataset contains continuous attributes average d-miat generated features. case many cuts generated conf conditions algorithm checked features generated d-miat improved predictive performance. table displays average accuracy baseline data ﬁles dmiat generated cuts noted table please note d-miat supp thresholds– entropy line lift line lift line typically improved prediction performance across classiﬁcation algorithms. demonstrates values chosen within threshold extremely sensitive values chosen typically improve prediction accuracy. note performance increases highlighted table. notable exception adaboost algorithm performance increase noted also note certain algorithms naive bayes logistic regressions consistently beneﬁted algorithms not. also combination features created cuts three d-miat thresholds results found ﬁfth line table yielded best performance. cuts generated d-miat signiﬁcant typically advantageous cuts addition original data. remainder paper results d-miat using cuts found yielded highest performance. thus overall found support paper’s thesis addition features necessarily problematic unless lack added value within attributes even thresholds within discretized cuts necessarily optimal. proceeded check research questions assumed research question would also found correct using discretized features alone would typically useful previously found surprise often case within datasets considered. table displays average accuracy results experiment. baseline average performance without discretization compared canonical discretization algorithm across classiﬁcation algorithms. again colored cell represents improvement given discretization algorithm versus baseline note lack color within much table algorithms particularly naive bayes showed often large improvements versus non-discretized data majority classiﬁcation algorithms typically show improvement across discretization algorithms datasets. point illustrated last column table shows discretization algorithms decrease accuracy average found exclusively using discretized features. thus found classiﬁcation algorithms datasets considered using discretization alone less effective using original features. found signiﬁcantly empirical support paper’s third research question using discretization combination original features effective using either original generated discretization alone. results experiment found table compare results original dataset combination original data features discretization algorithms. naive bayes algorithms beneﬁt combined dataset most. considering learning algorithms improvements noted combining discretization algorithms. note strong improvement results table versus table within adaboost logistical regression algorithms. contrast results table average performance across algorithms shows improvement. also note almost without exception combined features outperformed corresponding discretized set. example average accuracy ameva algorithm jumps ameva combined original data. even improvements noted found performance typically negatively affected addition features would assume curse dimensionality. despite general support found research question using discretization augment original data better using discretized data alone still note algorithms particularly k-nn random forest learning algorithms often beneﬁt form discretization. explore results differences generally impact discretization algorithms section given ﬁnding adding features d-miat canonical discretization algorithms helps improve performance hypothesized combination features would effective achieve best performance research question evaluate issue combined d-miat features best performing discretizatable comparing accuracy results different classiﬁcation algorithms within original baseline data discretized data formed ameva cacc caim ur-caim algorithms. table combining d-miat discretization features yields highest performance considering datasets d-miat added features average across datasets. tion algorithms datasets– caim urcaim. considered performance combination within datasets d-miat generated features average within datasets. results experiment found table seen results datasets combination discretization algorithms almost always outperforms original data improvements predictive accuracy noted three combinations naive bayes k-nn adaboost logistic regression algorithms. exception seems random forest algorithm large performance improvements noted. comparison also present improvements across datasets bottom portion table include datasets features generated d-miat. expected combination d-miat somewhat less signiﬁcant demonstrating beneﬁt adding features d-miat. overall average across algorithms considered found combination successful prediction accuracies typically improving thus found thesis typically correct using d-miat conjunction existing discretization algorithms recommended enrich features considered. somewhat surprised using discretization alone successful previously found classiﬁcation algorithms random forests believe differences datasets analyzed likely responsible gaps. such believe important open question predict discretization successful given machine learning algorithm dataset used input. contrast found d-miat yielded stable results typically improved performance something discretization algorithms especially learning algorithms. exploring make discretization algorithms similarly stable. note pipeline described paper using d-miat discretized features addition original data effective algorithms without discretization namely naive bayes logistic regression classiﬁcation algorithms. conversely approach less effective methods discretization component particularly random forests adaboost algorithms. previously noted localized discretization component thus gain less adding globally discretized features split intervals across decision tree nodes. similarly decision stumps used weak classiﬁers adaboost perform essentially global discretization thus made less likely beneﬁt globally discretized features added d-miat something evident table noted table random forest algorithm beneﬁted least pipeline described paper– possibly discretization component. hope study algorithms inherent discretization component still beneﬁt additional discretization. additionally seems k-nn algorithm despite discretization component beneﬁts less proposed pipeline algorithms. seems plausible because algorithm known particularly sensitive curse dimensionality thus speciﬁc algorithm beneﬁt proposed approach classiﬁcation algorithms less sensitive plan study complex issue future research. believe several additional directions also pursued future work. first study consider learning neural networks deep learning algorithms previously considered relatively small size several datasets within study made infeasible obtain accurate deep learning models using approach. currently considering additional datasets particularly larger amounts training data allow better understand deep learning augmented discretized features. similar vein believe interconnections likely exist generated discretized features. multivariate feature selection deep learning could potentially used help stress interconnections remove features redundant. second propose using metacognition process learning learning allow learn discretized features added given dataset. also studying could optimal value values conf supp thresholds within d-miat. paper demonstrates multiple d-miat thresholds used combination threshold typically improve performance claim thresholds used study represent optimal value values datasets. potential solution would develop metacognition mechanism learning thresholds given dataset. similarly possible form metacognition could added machine learning algorithms generally suggested within neural networks help achieve goal. paper present paradigm shift discretization used. date discretization typically used pre-processing step removes original attribute values replacing discretized intervals. instead suggest using features generated discretization extend dataset using discretized non-discretized versions data. demonstrated discretization often used generate features used addition dataset’s non-discretized features. d-miat algorithm present paper based assumption. d-miat discretize values particularly strong indications based high conﬁdence relatively support target class assumes classiﬁcation algorithms also using original data. also show canonical discretization algorithms used similar fashion fact combination original data d-miat discretized features algorithms yields best performance. hopeful ideas presented paper advance discretization application datasets algorithms.", "year": 2018}