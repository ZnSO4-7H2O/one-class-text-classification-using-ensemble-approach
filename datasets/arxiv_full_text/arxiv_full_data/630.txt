{"title": "Converting Cascade-Correlation Neural Nets into Probabilistic Generative  Models", "tag": ["q-bio.NC", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Humans are not only adept in recognizing what class an input instance belongs to (i.e., classification task), but perhaps more remarkably, they can imagine (i.e., generate) plausible instances of a desired class with ease, when prompted. Inspired by this, we propose a framework which allows transforming Cascade-Correlation Neural Networks (CCNNs) into probabilistic generative models, thereby enabling CCNNs to generate samples from a category of interest. CCNNs are a well-known class of deterministic, discriminative NNs, which autonomously construct their topology, and have been successful in giving accounts for a variety of psychological phenomena. Our proposed framework is based on a Markov Chain Monte Carlo (MCMC) method, called the Metropolis-adjusted Langevin algorithm, which capitalizes on the gradient information of the target distribution to direct its explorations towards regions of high probability, thereby achieving good mixing properties. Through extensive simulations, we demonstrate the efficacy of our proposed framework.", "text": "humans adept recognizing class input instance belongs perhaps remarkably imagine plausible instances desired class ease prompted. inspired this propose framework allows transforming cascade-correlation neural networks probabilistic generative models thereby enabling ccnns generate samples category interest. ccnns well-known class deterministic discriminative autonomously construct topology successful giving accounts variety psychological phenomena. proposed framework based markov chain monte carlo method called metropolis-adjusted langevin algorithm capitalizes gradient information target distribution direct explorations towards regions high probability thereby achieving good mixing properties. extensive simulations demonstrate efﬁcacy proposed framework. green-striped elephant probably seen thing—no surprise. surprise ability imagine almost trouble. humans adept recognizing class input instance belongs remarkably imagine plausible instances desired class ease prompted. fact humans generate instances desired class elephant never encountered before like green-striped elephant. sense humans’ generative capacity goes beyond merely retrieving memory. computational terms notion generating examples desired class formalized terms sampling underlying probability distribution extensively studied machine learning rubric probabilistic generative models. cascade-correlation neural networks well-known class discriminative models successful simulating variety phenomena developmental literature e.g. infant learning word-stress patterns artiﬁcial languages syllable boundaries visual concepts also successful capturing important developmental regularities variety tasks e.g. balance-scale task conservation seriation moreover ccnns exhibit several similarities known brain functions distributed representation self-organization network topology layered hierarchical topologies cascaded direct pathways s-shaped activation function activation modulation integration neural inputs longterm potentiation growth newer network synaptogenesis neurogenesis pruning weight freezing nonetheless virtue deterministic discriminative ccnns lacked capacity probabilistically generate examples category interest. ability used e.g. diagnose network knows various points training particularly dealing highdimensional input spaces. work propose framework allows transforming ccnns probabilistic generative models thereby enabling ccnns generate samples category. proposed framework based markov chain monte carlo method called metropolisadjusted langevin algorithm employs gradient target distribution guide explorations towards regions high probability thereby signiﬁcantly reducing undesirable random walk often observed beginning mcmc mcmc methods family algorithms sampling desired probability distribution successful simulating important aspects wide range cognitive phenomena e.g. temporal dynamics multistable perception developmental changes cognition category learning causal reasoning children giving accounts many cognitive biases furthermore work theoretical neuroscience shed light possible mechanisms according mcmc methods could realized generic cortical circuits particular moreno-bote showed attractor neural network implementing could account multistable perception drifting gratings savin deneve showed network leaky integrate-and-ﬁre neurons could implement biologically-realistic manner. follows propose framework transforms ccnns probabilistic generative models thereby enabling generate samples category interest. proposed framework based algorithm given sec. denote input-output mapping learned ccnn training phase denote weights ccnn training. upon termination training presented input ccnn outputs note that case ccnn possesses multiple output units mapping vector rather scalar. convert ccnn probabilistic generative model propose algorithm target distribution follows ||·|| denotes l-norm damping factor normalizing constant vector whose element corresponding desired class rest elements −.s. intuition behind articulated follows input instance belonging desired class output network expected close l-norm sense; light adjusting likelihood input ccnns special class deterministic artiﬁcial neural networks construct topology autonomous fashion—an appealing property simulating developmental phenomena cases networks need constructed. ccnn training starts two-layer network hidden units proceeds recruiting hidden units time needed. hidden unit trained maximally correlated residual error network built recruited hidden layer giving rise deep network many hidden layers number recruited hidden units. ccnns sum-of-squared error objective function typically symmetric sigmoidal activation functions range hidden output units. variants proposed ccnns e.g. sibling-descendant cascade-correlation knowledge-based cascade-correlation although work speciﬁcally focus standard ccnns proposed framework handle sdcc kbcc well. special type mcmc method employs gradient target distribution guide explorations towards regions high probability thereby reducing burn-in period. speciﬁcally combines concepts langevin dynamics metropolis-hastings algorithm denote random variables small bold-faced letters random vectors capital bold-faced letters corresponding realizations non-bold-faced letter. algorithm outlined algorithm wherein denotes target probability distribution positive real-valued parameter specifying time-step used euler-maruyama approximation underlying langevin dynamics denotes number samples generated algorithm denotes proposal distribution denotes multivariate normal distribution mean vector covariance matrix ﬁnally denotes identity matrix. sequence samples generated algorithm guaranteed converge distribution worth noting work theoretical neuroscience shown outlined algorithm could implemented neurally-plausible manner also suggest linear gaussian asymmetric sigmoidal activation functions alternatives. proposed framework straightforwardly adapted handle activation functions. reader familiar probabilistic graphical models expression looks similar expression joint probability distribution markov random ﬁelds probabilistic energy-based models e.g. restricted boltzman machines deep boltzman machines. however crucial distinction normalizing constant computation intractable general renders learning models computationally intractable. appropriate interpret gibbs distribution non-probabilistic energy-based model whose energy deﬁned square prediction error section discusses topic gibbs distribution nonprobabilistic energy-based models context discriminitive learning computationally modeled raises issue highlighted regarding intractability computing normalizing constant general. sharp contrast framework proposed purpose generating examples desired class evidenced deﬁned terms also crucially intractability computing raises issue proposed framework intriguing property algorithm according normalizing constant need computed all. line algorithm mal’s proposal distribution requires computation ∇log essentially involves computation ;w∗) merely treated ﬁxed parameters). multi-layer structure ccnn ensures ;w∗) efﬁciently computed using backpropagation. alternatively settings ccnns recruit small number input units small) ;w∗) obtained introducing negligible perturbation component input signal dividing resulting change network’s outputs introduced perturbation repeating process components input signal worth noting although idea computing gradients introducing small perturbations would lead computationally inefﬁcient approach learning ccnns leads computationally efﬁcient approach generation number input units typically much fewer number weights ccnns also crucial note normalizing constant plays role computation ∇log ˜π). learning accomplished input output units. permits visualization input-output space lies note proposed framework handle arbitrary number input output units; restriction solely ease visualization. continuous-xor problem subsection show proposed framework allows ccnn trained continuous-xor classiﬁcation task generate examples category interest. output unit symmetric sigmoidal activation function range training consists samples unit-square paired corresponding labels. speciﬁcally training comprised ordered-pairs starting going equal steps size paired corresponding labels fig. training ccnn hidden layers obtained whose inputf depicted fig. figure ccnn trained continuous-xor classiﬁcation task. top-left training patterns. patterns gray quadrants negative examples label patterns white quadrants positive examples label dotted lines depict boundaries. topright input-output mapping learned ccnn along colorbar. bottom top-down view curve depicted top-right along colorbar. inherent randomness ccnn construction training could lead networks different structures. however since work solely concerned generating examples using ccnns rather well ccnns could learn given discriminitive task arbitrarily pick learned network. note proposed framework handle ccnns arbitrary structures; light choice network without loss generality. figure generating example positive category various choices parameter damping factor contour-plot learned mapping along corresponding colorbar shown sub-ﬁgure. generated samples depicted dots. denotes total number samples generated denotes corresponding acceptance rate. leads slow exploration input space. leads adequate exploration input space however penalizing undesirable input regions severely enough. desirable performance achieved fig. shows efﬁcacy proposed framework enabling ccnns generate samples category interest various choices parameter damping factor generated samples depicted dots. results shown fig. category interest category positive examples i.e. category input patterns which upon presented network would classiﬁed positive network. controls amount jump consecutive proposals made following behavior expected small consecutive proposals close another leading slow exploration input domain. increases bigger jumps made parameter controls severely deviations desired class label penalized. larger parameter severely deviations penalized less likely becomes make moves toward regions input space. acceptance rate deﬁned number accepted moves divided total number suggested moves also presented results shown fig. fig. shows proposed framework demonstrates desirable performance virtually generated samples fall within desired input regions desired regions adequately explored nicely demonstrates mal—by directing suggestions toward direction gradient therefore making moves toward regions high likelihood—could alleviate need discarding number samples generated beginning mcmc assumed unrepresentative equilibrium state a.k.a. burn-in period. fig. shows performance framework enabling learned ccnn generate category negative examples next show proposed framework allows ccnn trained famously difﬁcult two-spirals classiﬁcation task generate examples category interest. output unit symmetric sigmoidal activation function range training consists samples square paired corresponding labels training pattern shown fig. details. training ccnn hidden layers obtained whose input-output mapping depicted fig. figure ccnn trained two-spirals classiﬁcation task. top-left training patterns. positive patterns shown hollow circles negative patterns black circles. positive spiral depicted dashed line negative spiral dotted line. top-right input-output mapping learned ccnn along colorbar. bottom top-down view curve depicted topright along colorbar. fig. fig. show efﬁcacy proposed framework enabling ccnns generate samples positive negative categories respectively. although similar patterns behavior observed sec. increasing/decreasing observed well lack space results omitted. note results shown fig. depicts ﬁrst samples generated without excluding burn-in period. light results shown fig. fig. demonstrate efﬁcacy alleviating need discarding number samples generated beginning mcmc run. interestingly proposed framework also allows ccnns generate samples subject forms constraints. example fig. demonstrates proposed framework enables ccnn trained continuous-xor classiﬁcation task generate examples posifigure generating example positive negative categories contour-plot along correspondlearned mapping colorbar shown sub-ﬁgure. denotes total number samples generated denotes corresponding acceptance rate. generating example positive category generated samples depicted dots. bottom generating example negative category generated samples depicted blue dots. tive category following constraint generated samples must curve .sin generate samples positive category satisfying said constraint adopts proposed target distribution given treats independent dependent variable. although discussed proposed framework context ccnns straightforwardly extended handle kinds artiﬁcial neural networks e.g. multi-layer perceptron convolutional neural networks. furthermore proposed framework together recent work theoretical neuroscience showing possible neurally-plausible implementations suggests intriguing modular hypothesis according generation could result separate modules interacting case ccnn neural self-organized generative model generative model possessing self-constructive property ccnns. selforganized generative models could provide wealth developmental hypotheses imaginative capacities children change development models quantitative predictions compare against. work step towards models.", "year": 2017}