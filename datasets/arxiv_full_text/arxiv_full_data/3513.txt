{"title": "One pixel attack for fooling deep neural networks", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution. It requires less adversarial information and can fool more types of networks. The results show that 70.97% of the natural images can be perturbed to at least one target class by modifying just one pixel with 97.47% confidence on average. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks.", "text": "figure one-pixel attacks created proposed algorithm successfully fooled three types dnns trained cifar- dataset convolutional network network network vgg. original class labels black color target class labels corresponding conﬁdence blue. ample). additionally investigating adversarial images created extremely limited scenarios might give insights geometrical characteristics overall berecent research revealed output deep neural networks easily altered adding relatively small perturbations input vector. paper analyze attack extremely limited scenario pixel modiﬁed. propose novel method generating one-pixel adversarial perturbations based differential evolution. requires less adversarial information fool types networks. results show natural images cifar test dataset imagenet validation images perturbed least target class modifying pixel conﬁdence average. thus proposed attack explores different take adversarial machine learning extreme limited scenario showing current dnns also vulnerable dimension attacks. domain image recognition dnn-based approach overcome traditional image processing techniques achieving even human-competitive results however several studies revealed artiﬁcial perturbations natural images easily make misclassify accordingly proposed effective algorithms generating samples called adversarial images common idea creating adversarial images adding tiny amount well-tuned additive perturbation expected imperceptible human eyes correctly classiﬁed natural image. modiﬁcation cause classiﬁer label modiﬁed image completely different class. unfortunately previous attacks consider extremely limited scenarios adversarial attacks namely modiﬁcations might excessive perceptible human eyes side proposed fewpixel perturbations regarded cutting input space using low-dimensional slices different exploring features high dimensional input space. measure perceptiveness attack effective hiding adversarial modiﬁcation practice. best knowledge none previous works guarantee perturbation made completely imperceptible. direct mitigating problem limit amount modiﬁcations possible. speciﬁcally instead theoretically proposing additional constraints considering complex cost functions conducting perturbation propose empirical solution limiting number pixels modiﬁed. words number pixels units instead length perturbation vector measure perturbation strength consider worst case one-pixel modiﬁcation well scenarios comparison. security problem become critical topic szegedy ﬁrst revealed sensitivity well-tuned artiﬁcial perturbation crafted several gradient-based algorithms using backpropagation obtaining gradient information speciﬁcally i.j.goodfellow proposed fast gradient sign algorithm calculating effective perturbation based hypothesis linearity high-dimensions inputs main reason broad class networks sensitive small perturbation s.m. moosavidezfooli proposed greedy perturbation searching method assuming linearity decision boundaries addition papernot utilize jacobian matrix build adversarial saliency indicates effectiveness conducting ﬁxed length perturbation direction axis another kind adversarial image also proposed nguyen figure one-pixel attacks imagenet dataset modiﬁed pixels highlighted circles. original class labels black color target class labels corresponding conﬁdence blue. havior dnn’s model high dimensional space example characteristics adversarial samples close decision boundaries help describing boundaries’ shape. paper perturbing pixel differential evolution propose black-box attack scenario information available probability labels proposal mainly following advantages compared previous works effectiveness cifar- dataset able launch non-targeted attacks modifying pixel three common deep neural network structures success rates. additionally natural image perturbed classes. imagenet dataset non-targeted attacking bvlc alexnet model also changing pixel shows validation images attacked. semi-black-box attack requires black-box feedback inner information target dnns gradients network structures. method also simpler since abstract problem searching perturbation explicit target functions directly focus increasing probability label values target classes. input space gathered near boundaries addition fawzi revealed clues conducting curvature analysis. conclusion region along directions around natural images directions space curved images sensitive perturbation. interestingly universal perturbations shown possible achieve high effectiveness compared random pertubation. indicates diversity boundaries might boundaries’ shapes near different data points similar generating adversarial images formalized optimization problem constraints. assume input image represented vector scalar element represents pixel. target image classiﬁer receives n-dimensional inputs original natural image correctly classiﬁed class probability belonging class therefore vector additive adversarial perturbation according target class limitation maximum modiﬁcation note always measured length vector goal adversaries case targeted attacks optimized solution following question one-pixel modiﬁcation seen perturbing data point along direction parallel axis dimensions. similarly -pixel modiﬁcation moves data points within -dimensional cubes. overall few-pixel attack conducts perturbations lowdimensional slices input space. fact one-pixel perturbation allows modiﬁcation image towards chosen figure illustration adversarial images generated using jacobian saliency-map approach perturbation conducted total pixels obvious human eyes. since adversarial pixel perturbation become common generating adversarial images abnormal noise might recognized expertise. several black-box attacks require internal knowledge target systems gradients also proposed particular best knowledge work ever mentioned using one-pixel modiﬁcation change class labels carried narodytska however differently work utilized starting point derive semi black-box attack needs modify pixels without considering scenario one-pixel attack. addition neither measured systematically effectiveness attack obtained quantitative results evaluation. analysis one-pixel attack’s geometrical features well discussion implications also lacking. many efforts understand visualizing activation network nodes geometrical characteristics boundary gained less attraction difﬁculty understanding high-dimensional space. however robustness evaluation respect adversarial perturbation might shed light complex problem example natural random images found vulnerable adversarial perturbation. assuming images evenly distributed suggests data points higher probability finding global optima meta-heuristic relatively less subject local minima gradient descent greedy search algorithms moreover problem considered article strict constraint making relatively harder. require less information target system require optimization problem differentiable required classical optimization methods gradient descent quasi-newton methods. critical case generating adversarial images since networks differentiable instance calculating gradient requires much information target system hardly realistic many cases. many variations/improvements self-adaptive multi-objective among others. current work improved taking variations/improvements account. method settings encode perturbation array optimized differential evolution. candidate solution contains ﬁxed number perturbations perturbation tuple holding elements coordinates value perturbation. perturbation modiﬁes pixel. initial number candidate solutions iteration another candidate solutions produced using usual formula element candidate solution random numbers scale parameter current index generation. generated candidate solution compete corresponding father according index population winner survive next iteration. maximum number iteration early-stop criteria triggered probability label target class exceeds case targeted attacks cifar- figure illustration using two-pixel perturbation attack -dimensional input space green point denotes natural image. case one-pixel perturbation search space three perpendicular lines denoted black stripes. two-pixel pertubations search space three blue two-dimensional planes. summary two-pixel attacks search perturbation respectively dimensional slices original three dimensional input space. thus usual adversarial samples constructed perturbating pixels overall constraint strength accumulated modiﬁcation few-pixel attack considered paper opposite speciﬁcally focus pixels limit strength modiﬁcation. differential evolution population based optimization algorithm solving complex multi-modal optimization problems belongs general class evolutionary algorithms moreover mechanisms population selection phase keep diversity practice expected efﬁciently higher quality solutions gradient-based solutions even kinds speciﬁc iteration another candidate solutions generated according current population children compared corresponding fathers surviving ﬁtted fathers. comparing father child goal keeping diversity improving ﬁtness values simultaneously achieved. gradient information optimizing therefore require objective function differentiable previously known. thus utilized wider range optimization problems compared gradient based methods cifar- images imagenet images generating coordinate gaussian distributions values. ﬁtness function simply probabilistic label target class case cifar- label true class case imagenet. success rate case non-targeted attacks deﬁned percentage adversarial images successfully classiﬁed target system arbitrary target class. case targeted attack deﬁned probability perturbing natural image speciﬁc target class. adversarial probability labels accumulates values probability label target class successful perturbation divided total number successful perturbations. measure indicates average conﬁdence given target system mis-classifying adversarial samples. number target classes counts number natural images successfully perturb certain number network network network target image classiﬁers cifar- dataset structures networks described table network setting kept similar possible original modiﬁcations order highest classiﬁcation accuracy. scenarios targeted non-targeted attacks considered. attacks three types neural networks natural image samples randomly selected cifar- test dataset conduct attack. addition experiment conducted convolution network generating adversarial samples three pixel-modiﬁcation. objective compare one-pixel attack three pixel attacks. natural image nine target attacks launched trying perturb target classes. note actually launch targeted attacks effectiveness non-targeted attack evaluated based targeted attack results. image perturbed least target class total classes non-targeted attack image succeeds. overall leads total adversarial images created. evaluate effectiveness attacks established measures literature used well kinds measures introduced imagenet applied non-targeted attack paramater settings used cifar- dataset although imagenet search space times larger cifar-. note actually launch non-targeted attack imagenet using ﬁtness function aims decrease probability label true class. different cifar- whose effectiveness non-targeted attack calculated based targeted attack results carried using ﬁtness function increasing probability target classes. given time constraints conduct experiment without proportionally increasing number evaluations i.e. keep number evaluations. tests bvlc alexnet using samples ilsvrc validation selected randomly attack. imagenet conduct pixel attack want verify tiny modiﬁcation fool images larger size computationally tractable conduct attacks. success rates adversarial probability labels one-pixel perturbations three cifar- networks bvlc network shown table three ﬁvepixel perturbations cifar- shown table number target classes shown figure number original-target class pairs shown heat-maps figure addition number originaltarget class pairs total number times class attack either originated targeted shown figure since non-targeted attacks launched imagenet number target classes number original-target class pairs metrics included imagenet results. table results conducting one-pixel attack four different types networks convolutional network network network alexnet. originalacc accuracy natural test datasets. targeted/non-targeted indicate accuracy conducting targeted/non-targeted attacks. conﬁdence average probability target classes. cifar- success rates one-pixel attacks three types networks show generalized effectiveness proposed attack different network structures. average image perturbed target classes network. addition increasing number pixels modiﬁed three number target classes reached increases signiﬁcantly. dividing adversarial probability labels success rates conﬁdence values obtained respectively three ﬁve-pixel attacks. imagenet results show pixel attack generalizes well large size images fool corresponding neural networks. particular chance arbitrary imagenet validation image perturbed target class conﬁdence. note imagenet results done settings cifar- resolution images imagenet test times larger cifar- notice successful attack probability label target class highest. therefore conﬁdence relatively tell remaining classes even lower almost uniform soft label distribution. thus one-pixel attack break conﬁdence alexnet nearly uniform soft label distribution. conﬁdence caused fact utilized non-targeted evaluation focuses decreasing probability true class. ﬁtness functions give different results. table compassion non-targeted attack effectiveness proposed method previous works. suggests pixel enough create adversarial samples natural images. shows using dimensional perturbation vectors enough corresponding adversarial images natural images. fact increasing number pixels considerable number images simultaneously perturbed eight target classes. rare cases image target classes one-pixel modiﬁcation illustrated figure speciﬁc original-target class pairs much vulnerable others example images much easily perturbed hardly reach automobile indicates vulnerable target classes shared different data points belong class. moreover case one-pixel attack classes robust others since data points relatively hard perturb classes. among data points points perturbed classes. indicates labels points rarely change going across input space directions perpendicular axes. therefore corresponding original classes kept robust along directions. however seen robustness rather easily broken merely increasing dimensions perturbation three success rates number target classes reached increase conducting higher-dimensional perturbations. additionally also seen heat-map matrix approximately symmetric indicating class similar number adversarial samples crafted classes well classes target classes using three ﬁve-pixel perturbation. vertical axis shows percentage images pertubated horizontal axis indicates number target classes. regarding results shown figure one-pixel modiﬁcation fair amount natural images perturbed three four target classes. increasing number pixels modiﬁed perturbation target classes becomes highly probable. case non-targeted one-pixel attack network slightly higher robustness proposed attack. suggests three types networks vulnerable type attack. boundary shape close natural images boundary. words boundary shape wide enough possible natural images away boundary hard craft adversarial images contrary boundary shape mostly long thin natural images close border easy craft adversarial images hard craft adversarial images them. practice classes easy craft adversarial images exploited malicious users make whole system vulnerable. case here however exceptions shared networks revealing whatever causing phenomenon shared. therefore current systems given attacks vulnerability seems hard exploited. evaluate time complexity number evaluations common metric optimization. case number evaluations equal population size multiplied number generations. also calculate average distortion single pixel attacked taking average modiﬁcation three color chanfigure heat-maps number times successful attack present corresponding original-target class pair three ﬁve-pixel attack cases. blue indices indicate respectively original target classes. number indicates respectively following classes airplane automobile bird deer frog horse ship truck. said that exceptions example class attacking class attacking allconv networks pixel among others. ship class attacking networks example relatively easy craft adversarial samples relatively hard craft adversarial samples them. unbalance intriguing since indicates ship class similar classes like truck airplane vice-versa. might figure natural image class perturbed nine classes. attack conducted allconv network using proposed pixel attack. table bottom shows class labels output target approximately conﬁdence. curious result emphasize difference limitations current methods compared human recognition. previous results shown many data points might located near decision boundaries analysis data points moved small steps input space quantitatively analyzing frequency change class labels. paper showed also possible move data points along dimension points class labels change. results also suggest assumption made goodfellow small addictive perturbation values many dimensions accumulate cause huge change output might necessary explaining natural images sensitive small perturbation. since changed pixel successfully perturb considerable number images. according experimental results vulnerability exploited proposed pixel attack generalized different network structures well different image sizes. addition results showed table cost conducting one-pixel attack four different types networks. avgevaluation average number evaluations produce adversarial images. avgdistortion required average distortion one-channel single pixel produce adversarial images. mimics attacker therefore uses number iterations relatively small initial candidate solutions. therefore perturbation success rates improve either iterations bigger initial candidate solutions. additionally proposed algorithm widely vulnerable samples collected might useful generating better artiﬁcial adversarial samples order augment training data set. aids development robust models left future work.", "year": 2017}