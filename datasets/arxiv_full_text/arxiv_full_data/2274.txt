{"title": "Geometrical Insights for Implicit Generative Modeling", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Learning algorithms for implicit generative models can optimize a variety of criteria that measure how the data distribution differs from the implicit model distribution, including the Wasserstein distance, the Energy distance, and the Maximum Mean Discrepancy criterion. A careful look at the geometries induced by these distances on the space of probability measures reveals interesting differences. In particular, we can establish surprising approximate global convergence guarantees for the $1$-Wasserstein distance,even when the parametric generator has a nonconvex parametrization.", "text": "learning algorithms implicit generative models optimize variety criteria measure data distribution differs implicit model distribution including wasserstein distance energy distance maximum mean discrepancy criterion. careful look geometries induced distances space probability measures reveals interesting differences. particular establish surprising approximate global convergence guarantees -wasserstein distance even parametric generator nonconvex parametrization. instead representing model distribution parametric density function implicit generative models directly describe draw samples model distribution ﬁrst drawing sample ﬁxed random generator mapping data space parametrized generator function reparametrization trick variational auto-encoders generative adversarial networks recent instances approach. many authors motivate implicit modeling computational advantage results ability using efﬁcient back-propagation algorithm update generator parameters. contrast work targets another fundamental advantage implicit modeling. although unsupervised learning often formalized estimating data distribution practical goal learning process rarely consists recovering actual probabilities. instead probability models often structured manner interpretable physical causal model data. often achieved deﬁning interpretable density well chosen latent variables letting appearance model take slack. approach well illustrated inverse graphics approach computer vision implicit modeling makes much simpler implicit modeling easily represent simple phenomena involving small observed inferred variables. corresponding model distribution cannot represented density function supported low-dimensional manifold. nothing prevents implicit model generating samples. density functions respect common measure criterion particularly convenient enjoys favorable statistical properties optimization written expectation respect data distribution readily amenable computationally attractive stochastic optimization procedures first expression ill-deﬁned model distribution cannot represented density. second likelihood single example zero dataset likelihood also zero nothing maximize. typical remedy noise term model distribution. virtually generative models described classical machine learning literature include noise component whose purpose model anything useful merely make work. instead using ad-hoc noise terms coerce optimizing different similarity criterion data distribution model distribution could well explicitly optimize different criterion. therefore crucial understand selection particular criterion inﬂuence learning process ﬁnal result. section reviews known results establishing many interesting distribution comparison criteria expressed adversarial form amenable tractable optimization algorithms. section reviews statistical properties interesting families distribution distances namely family wasserstein distances family containing energy distances maximum mean discrepancies. although wasserstein distances worse statistical properties experimental evidence shows deliver better performances meaningful applicative setups. section reviews essential concepts geodesic geometry metric spaces. section shows different probability distances induce different geodesic geometries space probability measures. section leverages geodesic structures deﬁne various ﬂavors convexity parametric families generative models used prove simple gradient descent algorithm either reach approach global minimum regardless traditional nonconvexity parametrization model family. particular uses implicit generative models minimizing wasserstein distance gradient descent algorithm offers much better guarantees minimizing energy distance. adversarial training framework popularized generative adversarial networks used minimize great variety probability comparison criteria. although criteria also optimized using simpler algorithms adversarial training provides common template compare criteria themselves. section presents adversarial training framework reviews main categories probability comparison criteria supports namely integral probability metrics f-divergences wasserstein distances energy distances maximum mean discrepancy distances setup although intuitively useful consider sample space convex subset also useful spell precisely properties essential development. following assume polish metric space complete separable space whose topology deﬁned distance function borel σ-algebra generated open sets notation probability measures deﬁned notation satisfying exy∼µp] condition equivalent ex∼µp] arbitrary origin ﬁnite symmetric satisﬁes triangular inequality. satisfy properties metric distance. word pseudodistance nonnegative criterion fails satisfy separation property word divergence criteria symmetric fail satisfy triangular inequality generally assume contribution distance deﬁned ﬁnite. however allow probability comparison criteria inﬁnite. distributions belong domain particular criterion deﬁned take otherwise. particularly interested model distributions supported low-dimensional manifold large ambient sample space since distributions typically density function cannot represent model family using parametric density function. following example variational auto-encoders generative adversarial networks represent model distributions deﬁning produce samples. random variable known distribution deﬁned suitable probability space measurable function called generator parametrized implicit modeling approach useful ways. first unlike densities represent distributions conﬁned low-dimensional manifold. second ability easily generate samples frequently useful knowing numerical value density function general computationally difﬁcult generate samples given arbitrary high-dimensional density learning algorithms implicit models must therefore formulated terms sampling oracles. ﬁrst oracle returns training examples samples data distribution second oracle returns generated examples samples model distribution gθµz. particularly easy comparison criterion expressed terms expectations respect distributions although failing satisfy separation property serious practical consequences recall pseudodistance always becomes full ﬂedged distance quotient space denotes equivalence relation theory applies long never distinguishes points separated zero distance. deﬁnes pairs real-valued critic functions deﬁned considered maximization. discussed following subsections different choices lead broad variety criteria. formulation mild generalization integral probability metrics functions constrained equal result means compute gradient without taking account changes important assumption differentiability cost without assumption assert −ez∼µz belongs local subgradient dividing last inequality taking limit recalling dominated convergence theorem assumption allow take limit inside expectation operator rearranging result gives thanks result compute unbiased stochastic estimate gradient grad ﬁrst solving maximization problem using back-propagation algorithm compute average gradient minibatch sampled although algorithmic idea made work relatively reliably serious conceptual practical issues remain remark order obtain unbiased gradient estimate need solve maximization problem true distributions rather particular subset examples. hand standard machine learning toolbox avoid overﬁtting maximization problem. hand toolbox essentially works restricting family ways change meaning comparison criteria remark practice solving maximization problem iteration stochastic gradient algorithm computationally costly. instead practical algorithms interleave kinds stochastic iterations gradient ascent steps gradient descent steps much smaller effective stepsize. algorithms belong general class stochastic algorithms time scales convergence properties form delicate topic clearly beyond purpose contribution. stochastic gradient descent often relies unbiased gradient estimates given estimating wasserstein distance gradients small minibatches gives severely biased estimates fact obvious minibatches size one. theorem therefore provides imperfect useful alternative. therefore special case critic functions constrained identical constrained contain opposite every critic function. whereas expression guarantee ﬁnite distance always pseudodistance. proposition integral probability metric pseudodistance. proof establish triangular inequality write respectively densities relative measure continuous convex function deﬁned expression trivially satisﬁes always nonnegative pick subderivative inequality also shows separation property satisﬁed inequality strict proposition usually table provides examples f-divergences provides function corresponding conjugate function appears variational formulation. particular argued analysis clariﬁes probability comparison criteria associated early variants statement holds /p)|>m}= restricting exclude subsets taking limit work general. practice result veriﬁed elementary calculus usual choices shown table figure distribution supported segment according distance f-divergences sequence distributions converge however sequence converges according either wasserstein distances energy distance despite elegance framework comparison criteria attractive distributions supported low-dimensional manifolds overlap. following simple example shows problem example uniform distribution real segment consider distributions deﬁned disjoint support neither total variation distance f-divergence depend exact value therefore according topologies induced criteria sequence distributions converge fundamental problem neither total variation distance f-divergences depend distance deﬁned sample space minimization criterion appears effective adjusting probability values matching distribution supports. example deﬁned example since easy optimal transport plan wassertein distance converges zero tends zero. therefore according topology induced wasserstein distance sequence distributions converges thanks kantorovich duality theory wasserstein distance easily expressed variational form summarize essential results useful work direct reader full exposition. theorem polish metric spaces {+∞} nonnegative continuous cost function. probablity measures marginals minimizes e∼π] deﬁnition polish metric spaces r+∪{+∞} nonnegative continuous cost function. pair functions c-conjugate theorem polish metric spaces {+∞} nonnegative continuous cost function. probability distributions deﬁned marginal distributions pairs respectively -integrable functions satisfying property conclude presentation wassertein distance mentioning deﬁnition immediately implies several distance properties zero distributions equal strictly positive different symmetric property gives triangular inequality case general case triangular inequality also established using minkowsky inequality since one-to-one mapping distributions characteristic functions relation establishes isomorphism space probability distributions equipped distance space characteristic functions equipped since squared expressed simple combination expectations easy design stochastic minimization algorithm relies oracles producing samples distribution makes energy distance computationally attractive criterion training implicit models discussed section right-hand side expression well deﬁned obviously symmetric trivially zero distributions equal. ﬁrst part following theorem gives necessary sufﬁcient conditions ensure right-hand side nonnegative therefore square shall later triangular inequality comes free condition second part theorem gives necessary sufﬁcient condition satisfying separation property strictly positive function strongly negative deﬁnite kernel negative deﬁnite kernel that probability measure µ-integrable real-valued function ex∼µ remark deﬁnition strongly negative kernel best explained considering meaning would change considering probability measures ﬁnite support xn}. amounts requiring equality zero. however weaker property sufﬁcient ensure separation property holds. remark relation therefore means euclidean distance strongly negative deﬁnite kernel. fact shown observations implying conversely assume using weak large numbers ﬁnite support distributions proceeding observation contradicts also ﬁnite support. contraposition suppose µ{h=}< observation gives conversely suppose observation gives since must zero requiring negative deﬁnite kernel quite strong assumption. instance classical result schoenberg establishes squared distance negative deﬁnite kernel whole metric space induced distance isometric subset hilbert space therefore euclidean geometry theorem metric space isometric subset hilbert space negative deﬁnite kernel. positive deﬁnite kernels machine learning literature extensively studied context so-called kernel trick particular well known theory reproducing kernel hilbert spaces establishes unique hilbert space called rkhs contains functions remark context theorem relation simply analytic expression rkhs norm associated triangular kernel euclidean distance. corollary negative deﬁnite kernel pseudodistance satisﬁes properties distance except maybe separation property last expression also called maximum mean discrepancy associated positive deﬁnite kernel conversely positive deﬁnite kernel reader easily prove symmetric function therefore formulations essentially equivalent note however negative deﬁnite kernel deﬁned satisfy triangular inequality remark equivalence immediately recognized many important concepts rediscovered subtle technical variations. instance notion characteristic kernel depends subtly chosen domain want injective. corollary gives simple necessary sufﬁcient condition domain choosing different domain leads complications ﬁrst sight requiring functions -lipschitz contained rkhs unit ball seem slightly different ways enforce smoothness constraint. nevertheless closer comparison reveals important differences. although ed/mmd metrize weak convergence topology quantitatively different therefore hard compare practical situations. following upper bound provides clariﬁcation. proposition equipped distance also negative deﬁnite kernel. -wasserstein distance energy distance deﬁned contrast following results show signiﬁcantly smaller -wasserstein distance also show happens particularly important situation approximates distribution ﬁnite sample. theorem probability distributions correspondindependent q-distributed random variables empirical probability distribution. deﬁned kernel satisfying then therefore effect replacing empirical approximation disappears quickly like grows. result surprising notices v-statistic however gives precise equality particularly direct proof. proof using following equalities deﬁnition gives ﬁrst result. comparable results -wasserstein distance describe convergence speed quickly becomes considerably slower dimension sample space theorem equipped usual euclidean distance. following example inspired illustrates slow rate consequences. example uniform distribution supported unit sphere equipped euclidean distance. points sampled independently distribution corresponding empirical distribution. additional point sampled well known mini remains arbitrarily close theorem example therefore show much smaller also reveal statistical properties -wasserstein distance discouraging. since argument example naturally extends pwasserstein distance problem seems shared wasserstein distances. curious reader pick expression derive asymptotic impressive achievement associated implicit modeling approach certainly generation photo-realistic random images resemble images provided training data apparent contradiction statistical results previous section couple notable exceptions discussed later section visual quality images generated using models trained directly minimizing usually lags behind obtained original generative adversarial network formulation discussing exceptions worth recalling visual quality generated images peculiar benchmark generative models. incomplete criterion ensure model generates images cover space covered training data. interesting criterion common statistical metrics estimates negative log-likelihood generally unable indicate models generate better-looking images ﬁnicky criterion because despite efforts quantify visual quality well-deﬁned scores evaluation image quality fundamentally remains beauty contest. figure nevertheless shows clear difference. ae+gmmn approach improves pure approach training implicit model directly generate images targets compact representation computed pretrained auto-encoder network. changes highdimensional image generation problem comparatively low-dimensional code generation problem good notion distance. independent evidence low-dimensional implicit models work relatively well ed/mmd cramérgan approach minimizes energy distance computed representations produced adversarially trained -lipschitz continuous transformation layer resulting optimization problem figure comparing images generated implicit model trained different criteria. square shows sample training examples represening bedroom pictures. bottom left square shows images generated model trained using algorithm bottom right square shows images generated model trained using wgan-gp approach cost relies critic functions form belongs rkhs unit ball -lipschitz continuous. hybrid critic functions still smoothness properties comparable lipschitz-continuous critics -wasserstein distance. however since critic functions usually form rkhs ball resulting criterion longer belongs ed/mmd family. positive side hybrid approaches lead efﬁcient training algorithms described section precise parametric structure transformation layer also provides means match wgan models achieve selecting precise parametric structure critic. order understand subtle effects remains useful clarify similarities differences pure ed/mmd training pure training. section gives concise review elementary metric geometry concepts useful rest analysis. readers safely skip section already familiar metric geometry textbooks ﬁnite. intuitively thanks triangular inequality dividing curve segments summing sizes yields quantity greater smaller curvilinear length curve. construction constant speed curves together continuity additivity property implies function nondecreasing continuous thanks intermediate value theorem curve rectiﬁable therefore construct curve visits points order curve figure consider equipped distance. left points gray area center minimal geodesics connecting live gray area. right curves live gray area minimal geodesics. indeed satisﬁes properties distance. also easy check distance induced coincides reason distance satisﬁes called intrinsic distance. polish metric space equipped intrinsic distance called intrinsic polish space. metric space equipped intrinsic distance called length space. distance intrinsic length minimal geodesic satisﬁes relation curve exists points distance called strictly intrinsic. polish space equipped strictly intrinsic distance called strictly intrinsic polish space. conversely rectiﬁable curve length minimal geodesic curve joining shorter. curve points strictly intrinsic distance. makes clear every minimal geodesic length space made points triangular inequality equality. however shown figure sufﬁcient ensure curve minimal geodesic. consider intermediate points theorem curve joining points curve minimal geodesic length minimal geodesics probability space assume strictly intrinsic polish space also assume distance never inﬁnite. therefore pair points connected least minimal geodesic. space probability distributions equipped probability distances discussed section often becomes length space inherits geometrical properties since process depends critically probability distance compares different distributions understanding geodesic structure reveals fundamental differences probability distances. approach fact quite different celebrated work amari information geometry seek understand geometry space probability measures equipped different distances. information geometry characterizes riemannian geometry parametric family probability measures kullbackleibler distance. difference obviously related contrast relying good distances versus relying good model families discussed section since particularly interested relatively simple models physical causal interpretation cannot truly represent actual data distribution cannot restrict geometrical insights happens within model family. form curve space distributions theorem equipped distance belongs family mixture curve joining distributions constant speed minimal geodesic making strictly intrinsic distance. proof proof relies corollary last equality relies fact corollary mixture curve constant speed minimal geodesic. since true distance strictly intrinsic. remark although theorem makes length space alone make strictly intrinsic polish space. also needs establish completeness separability properties polish space. fortunately properties true ground space polish. since -wasserstein distance energy distance belong family equipped either distance strictly intrinsic polish space. probability measures connected least minimal geodesic mixture geodesic. shall later -wasserstein distance admits many minimal geodesics. however case ed/mmd distances mixture geodesics minimal geodesics. instance probability measures equipped total variation distance separable dense subset needs element disjoint balls p∈pr wasserstein distance energy distance properties derived theorem recaling complete separable isometric polish. displacement geodesics displacement geodesics euclidean case ﬁrst assume euclidean space equipped p-wasserstein distance distributions optimal transport plan displacement curve joining formed distributions intuitively whenever optimal transport plan speciﬁes grain probability mass must transported follow shortest path connecting euclidean space straight line drop grain performing fraction journey. proposition euclidean space equipped p-wasserstein distance displacement curve joining distributions constant speed minimal geodesic making strictly intrinsic distance. proof optimal transport plan deﬁne tentative transport plan lemma polish metric spaces. probability measures px×x px×x marginal distribution exists px×x×x proof notes lemma ﬁrst sight simply signiﬁcant technical difﬁculties arise needs topological properties polish space proof proposition constant speed minimal geodesic. point must satisfy equality thanks properties minkowski’s inequality happen exists that π-almost surely x−y=λx−z y−z=x−z. constant constant speed minimal geodesic. therefore π-almost surely. therefore describes displacement curve deﬁned note however displacement geodesics minimal geodesics -wasserstein distance since know mixture geodesics also minimal geodesics fact many geodesics. intuitively whenever optimal transport plan transports grain probability drop grain fraction journey randomly decide whether transport grain planned also smear grain probability along shortest path connecting using different different parts space. displacement geodesics general case rest section reformulates results general situation strictly intrinsic polish space. rather following random curve approach described chose elementary approach also want characterize many geodesics deﬁnition equivalent subtly weaker main difﬁculties longer single shortest path connecting points able push-forward formulation function returns point located position along constant speed minimal geodesic joining satisfy necessary measurability requirements. deﬁnition strictly intrinsic polish metric space equipped p-wasserstein distance curve called displacement geodesic ≤t≤t≤ distribution proposition deﬁnition indeed implies constant speed minimal geodesic length furthermore pairwise marginals optimal transport plans marginals. proof third inequality minkowski’s inequality. since ends chain inequalities equal inequalities must equalities implying optimal transport plan likewise pairwise marginals proposition establish displacement geodesic always exists. know cannot established without making additional assumption local compacity intrinsic polish space since often easy directly deﬁne displacement geodesic shown omit lengthy general proof. theorem strictly intrinsic polish metric space distributions equipped p-wasserstein constant speed minimal geodesics length joining displacement geodesics. since chain inequalities value ends inequalities must equalities. ﬁrst means optimal transport plan second means π-almost surely. third minkowski’s inequality inequality scalars λ+λ+λ= that πalmost surely since must satisfy corollary scalars minimal geodesics -wasserstein distance characterize many minimal geodesics -wasserstein distance using comparable strategy. theorem strictly intrinsic polish space equipped distance curve joining minimal geodesic length distribution interesting compare condition theorem instead telling successive triangular inequalities probability space must equality result tells holds almost-surely sample space particular means must aligned along geodesic case mixture geodesic coincide case displacement geodesic unsupervised learning geodesic structures seen previous section geometry space probability distributions changes considerably choice probability distance. critical aspects possible geometries understood characterization shortest paths distributions purpose section investigate consequences geometrical following discussion differences unsupervised learning problems. represents data distribution known training examples represent family parametric models considered learning algorithm. provides means extend familiar euclidean notion convexity length spaces. section investigates geometry implicit modeling learning problems lens generalized notion convexity. convexity à-la-carte assume strictly intrinsic polish space equipped distance family smooth constant speed curves although curves need minimal geodesics focus section limited three families curves deﬁned section deﬁnition strictly intrinsic polish space. closed subset called convex respect family curves contains curve connecting whose graph contained deﬁnition strictly intrinsic polish space. real-valued function deﬁned called convex respect family constant speed curves when every curve function convex. convex displacement convex theorem strictly intrinsic polish space equipped distance closed subset cost function convex respect family constant speed curves. then minf result essentially means possible optimize cost function descent algorithm. result means minima global minima result means neighborhood suboptimal distribution contains distribution sufﬁciently smaller cost ensure descent continue. proof since belong contains curve joining know since convex function write therefore since holds connected. irrelevant long family cost function convex respect well-chosen curves level sets cost function connected nonincreasing path connecting starting point global optimum therefore important understand deﬁnition makes easy hard ensure model family training criterion convex respect particularly interested case implicit models distributions expressed pushing samples known source distribution parametrized generator function push-forward operation deﬁnes deterministic coupling distributions function maps every source sample single point contrast stochastic coupling distribution pz×x would allowed distribute source sample several locations according conditional distribution using smooth generator functions example distributions associated parameters continuous path disjoint supports separated distance greater parameter space gθtµz mixture contrast keeping source sample constant small change parameter causes small displacement generated sample space therefore expect implicit model family particular afﬁnity displacement geodesics. difﬁcult fully assess consequences quasi-impossibility achieve mixture convexity implicit models. instance although energy distance mixture convex function cannot expect family implicit models mixture convex. example uniform distribution parameter constrained square generator function easy model family displacement convex mixture convex. figure shows level sets criteria target distribution criteria global minima however energy distance spurious local minima relatively high value cost function. constructing example nontrivial. whether situations arise commonly higher dimension known. however empirically observe optimization criterion high-dimensional image data often stops unsatisfactory results convexity distances target distribution learning algorithm. could true data distribution empirical training distribution. learning algorithm minimizes cost function cost function therefore distance. since distance function always convex euclidean space whether distance strictly intrinsic polish space geodesically convex. always case. figure gives simple counterexample equipped distance. figure geodesic convexity often differs euclidean convexity important ways. many different minimal geodesics connecting points equipped distance cross-shaped subset shown left plot geodesically convex. center plot shows intersection geodesically convex sets necessarily convex even connected. right plot shows points located inside unit ball connected minimal geodesic stay unit ball. means distance convex restriction minimal geodesic convex. proposition equipped distance belongs family mixture convex. proof mixture curve. theorem tells mixtures minimal geodesics. target distribution write therefore distance mixture convex family generative models theorem tells simple descent algorithm global minimum discussed example hard achieve mixture convexity family implicit models. could achieved nonparametric techniques. however hold displacement convexity. instance wasserstein distance displacement convex even sample space distance geodesically convex even sample space euclidean. example equipped euclidean distance. uniform distribution unit circle uniform distribution line segment length centered origin distance independent decreases increases consider displacement geodesic <θ<θ<π/. since although negative result prevents invoking theorem minimization wasserstein distance observe convexity violation example rather small. convexity violation examples fact rather difﬁcult construct. following section shows still obtain interesting guarantees bounding size convexity violation. proof proof starts construction distribution illustrated figure thanks proposition construct distribution whose marginals respectively whose pairwise marginals optimal transport plans that π-almost surely construct distribution gluing optimal transport plans finally constructed letting equal probability equal probability last three marginals equal unfortunately found elegant leverage idea global description cost landscape. proposition merely bounds expected diameter distribution nevertheless bound describe level sets theorem strictly intrinsic polish space equipped geodesically convex distance equipped -wasserstein distance displacement convex expected diameter therefore displacement geodesic contained joins without leaving ilarly construct second displacement geodesic joins therefore continuous path connecting result means optimizing wasserstein distance descent algorithm stop ﬁnding generative model whose distance target distribution within global minimum. beyond point algorithm could meet local minima stop progressing. rather coarse bound constant believe possible give much better suboptimality guarantee particular cases. note result depend parametrization therefore applies level sets potentially nonconvex neural network parametrizations. previous results connexity level sets tied speciﬁc parametric form. fact give result abstract setup rather surprising. hope developments clarify much approach help efforts. finally comparing result example also reveals fascinating possibility simple descent algorithm might fact unable dirac distribution center sphere global minimum. therefore effective statistical performance learning process subtantially better theorem suggests. research necessary check whether phenomenon occurs practice. work illustrates geometrical study probability distances provides useful —but still incomplete— insights practical performance implicit modeling approaches using different distances. addition using technique differs substantially previous works also obtain surprising global optimization results remain valid parametrization nonconvex. would like thank joan bruna marco cuturi arthur gretton yann ollivier arthur szlam stimulating discussions also pointing numerous related works.", "year": 2017}