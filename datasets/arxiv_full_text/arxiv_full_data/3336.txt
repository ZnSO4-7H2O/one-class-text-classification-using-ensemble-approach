{"title": "Variational methods for Conditional Multimodal Deep Learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "In this paper, we address the problem of conditional modality learning, whereby one is interested in generating one modality given the other. While it is straightforward to learn a joint distribution over multiple modalities using a deep multimodal architecture, we observe that such models aren't very effective at conditional generation. Hence, we address the problem by learning conditional distributions between the modalities. We use variational methods for maximizing the corresponding conditional log-likelihood. The resultant deep model, which we refer to as conditional multimodal autoencoder (CMMA), forces the latent representation obtained from a single modality alone to be `close' to the joint representation obtained from multiple modalities. We use the proposed model to generate faces from attributes. We show that the faces generated from attributes using the proposed model, are qualitatively and quantitatively more representative of the attributes from which they were generated, than those obtained by other deep generative models. We also propose a secondary task, whereby the existing faces are modified by modifying the corresponding attributes. We observe that the modifications in face introduced by the proposed model are representative of the corresponding modifications in attributes.", "text": "abstract. paper address problem conditional modality learning whereby interested generating modality given other. straightforward learn joint distribution multiple modalities using deep multimodal architecture observe models aren’t effective conditional generation. hence address problem learning conditional distributions modalities. variational methods maximizing corresponding conditional log-likelihood. resultant deep model refer conditional multimodal autoencoder forces latent representation obtained single modality alone ‘close’ joint representation obtained multiple modalities. proposed model generate faces attributes. show faces generated attributes using proposed model qualitatively quantitatively representative attributes generated obtained deep generative models. also propose secondary task whereby existing faces modiﬁed modifying corresponding attributes. observe modiﬁcations face introduced proposed model representative corresponding modiﬁcations attributes. problem learning several modalities simultaneously garnered attention several deep learning researchers past years primarily wide availability data numerous real-world applications multimodal data used. instance speech accompanied text resultant data used training speech-to-text text-to-speech engines. even within medium several modalities exist simultaneously instance plan elevation object multiple translations text. task learning several modalities simultaneously complicated fact correlations within modality often much stronger correlations across modalities. hence many multi-modal learning approaches capture cross-modal correlations abstract latent feature level rather visible feature level. assumption latent features comparatively less alternative approach capture joint distribution modelling conditional distribution across modalities done whereby authors make simplifying assumption joint log-likelihood maximized conditional log-likelihood modality given modality maximized. assumption untrue general idea learning conditional distributions capture joint distribution several advantages. particular conditional distributions often less complex model since conditioning modality reduces possibilities modality. moreover underlying task generate modality given other learning conditional distributions directly addresses task. hence address problem multimodal learning capturing conditional distributions. particular variational approximation joint loglikelihood training. paper restrict directed graphical models whereby latent representation sampled modality modality sampled latent representation. hence model referred conditional multimodal autoencoder formal description problem follows. given i.i.d sequence datapoints y)}. ﬁxed datapoint modality wish generate modality wish condition assume generated ﬁrst sampling real-valued latent representation distribution sampling distribution graphical representation model given figure furthermore assume conditional distribution latent representation given distribution given parametric. choices evaluation conditional log-likelihood intractable. hence resort minimization variational lower bound conditional log-likelihood. achieved approximating posterior distribution given tractable distribution explained detail following section. posterior distribution approximated distribution whose graphical representation shown figure particular approximation posterior distribution latent variables given approximation posterior distribution given individual datapoint conditional refers kl-divergence distributions always non-negative. note choice decomposition posterior forces distribution ‘close’ true posterior thereby encouraging model learn features alone representative well. term equation referred variational lower bound conditional log-likelihood datapoint denoted rewritten last equation observe variational lower bound written terms. ﬁrst term negative reconstruction error reconstructed encoding second term ensures encoding ’close’ corresponding encoding closeness deﬁned terms kl-divergence corresponding distributions. adding bound obtain lower bound joint loglikelihood. shown learning distribution samples sufﬁcient train transition operator markov chain whose stationary distribution distribution wish model. using idea replace note terms quite different gradients respect parameters terms expected ’close’. order simplify computation variational lower bound assume conditioned latent representation normally distributed mean diagional covariance matrix whose diagonal entries given efσ. moreover conditioned normally distributed mean diagonal covariance matrix whose diagonal entries given egσ. rest paper assume multi-layer perceptrons. furthermore approximate posterior distribution given normal distribution mean diagonal covariance matrix whose diagonal entries given multi-layer perceptrons. order make dependence distributions explicit represent reference parametric forms likelihood prior posterior distributions representations demonstrating explicit dependence given table assumptions simplify calculation kl-divergence denote component function size latent representation ignoring constant terms kl-divergence term variational lower bound written negative reconstruction error term variational lower bound obtained generating samples posterior distribution given averaging negative reconstruction error. ﬁxed term written order train model using ﬁrst-order methods need compute derivative variational lower bound respect parameters model. parameters respectively. note kl-divergence term depends hσ}. derivatives respect computed chain rule. derivative negative reconstruction error respect minibatch training learn parameters model whereby gradient model respect model parameters computed every minibatch corresponding parameters updated. gradient kl-divergence computed exactly gradient negative reconstruction error requires sample standard normal random vectors compute gradient sampled vector take mean. practise minibatch size large enough sufﬁcient sample standard normal random vector training example compute gradient negative reconstruction error respect parameters vector. also observed case variational autoencoder pictorial representation implemented model given figure firstly neural network generate mean log-variance distribution moreover neural network generate mean log-variance distribution kl-divergence computed using gradient backpropagated update parameters furthermore mean log-variance used sample forwarded neural network compute mean log-variance distribution finally negative reconstruction error computed using equation speciﬁc gradient backpropagated update parameters past years several deep generative models proposed. include deep boltzmann machines generative adversarial networks variational autoencoders generative stochastic networks dbms learn markov random ﬁeld multiple latent layers effective modelling mnist norb data. however training dbms involves mean-ﬁeld approximation step every instance training data hence computationally expensive. moreover tractable extensions deep boltzmann machines handling spatial equivariance. models mentioned above trained using backpropagation stochastic variant hence incorporate recent advances training deep neural networks faster libraries better optimization methods. particular learns distribution data forcing generator generate samples ‘indistinguishable’ training data. achieved learning discriminator whose task distinguish generated samples samples training data. generator trained fool discriminator. though approach intuitive requires careful selection hyperparameters. moreover given data sample latent variables generated since posterior never learnt model. posterior distribution latent variables conditioned data approximated normal distribution whose mean variance output neural network allows approximate estimation variational log-likelihood optimized using stochastic backpropagation directed probabilistic models edge latent layer data. conditional extensions models incorporating attributes/labels also proposed graphical representation conditional conditional shown figure observed models assume latent layer independent attributes/labels. stark contrast model cmma assumes latent layer sampled conditioned attributes. note lower bound proposed model cmma contains kldivergence term explicitly force latent representation ’close’ latent representation term lower bound cvae. proves disadvantage cvae reﬂected experiments section. consider task learning conditional distribution faces given attributes. task cropped labelled faces wild dataset consists faces people people image. images size contain channels prior distribution latent representations several randomly selected individuals. width circles correspond standard deviation. note high overlap priors despite attributes different individuals. contours prior distribution individual contours posterior distribution individual high uncertainty prior causes posterior distribution several individuals within prior distribution individual faces faces attributes associated them obtained partially using amazon mechanical turk partially using attribute classiﬁers attributes include ‘male’ ‘asian’ eye-wear’ ‘eyeglasses’ ‘moustache’ ‘mouth open’ ‘big nose’ ‘pointy nose’ ‘smiling’ ‘frowning’ ‘big lips’ etc. data also contains attributes hair necklace earrings etc. though attributes visible cropped images. ﬁrst faces corresponding attributes training model next faces validation keep remaining faces corresponding attributes testing. dataset challenging since people dataset image. moreover possible combination attributes occurs dataset. forces model learn mapping attributes faces shared across possible combinations attributes. contrast face dataset used consists several subsets faces attribute changes others remain unchanged. hence tune mapping attributes faces attribute time. this however isn’t possible lfw. order emphasize factor show prior posterior distribution -dimensional latent representatation randomly selected individuals modalities figure note despite conditioning attributes prior distributions high uncertainty prior distribution several attribute combinations overlap considerably particularly lower dimensions. dimensions increase overlap decreases however. hand assumes common prior individuals. hence think conditioning cmma tilting prior direction conditioning modality. moreover posterior always much lower variance prior. fact access decreases uncertainty huge amount reduced variance also artifact variational methods general. particular dimensional latent representations observed average standard deviation cmma posterior distribution latent representations iterations reduce further. cmma used paper encodes attributes neural network hidden units soft thresholding unit form parallel output layers comprising units. mlps convolution deconvolution neural networks respectively. corresponding architectures given figure compare quantitative qualitative performance cmma conditional generative adversarial networks conditional variational autoencoders tried ensure architecture models used comparison close possible architecture cmma used experiments. hence generator discriminator cgan encoder decoder cvae closely mimic mlps cmma described previous section. coded models torch trained iterations tesla gpu. model training time approximately day. adagrad optimization algorithm used proposed model cmma found relatively stable selection initial learning rate variance randomly initialized weights various layers. cgan selected learning rate generator discriminator variance weights verifying conditional log-likelihood validation set. results best hyperparameters reported. found cgan model quite unstable selection hyperparameters. fig. faces generated attributes using various models ﬁxed model rows correspond ‘female asian’ ’female not-asian’ ’male asian’ ’male not-asian’ order. remaining attributes varied time generate columns. particular model columns faces correspond change mouth open iii) spectacles bushy eyebrows nose pointy nose vii) thick lips. note that model cmma change attributes mouth open spectacles etc. clearly reﬂected corresponding face. models change evident corresponding faces. ﬁrst experiments compare conditional log-likelihood faces given attributes test models cmma cgan cvae. direct evaluation conditional log-likelihood infeasible size latent layer used experiments mcmc estimates conditional log-likelihood unreliable. proposed model cmma variational lower bound log-likelihood test data computed difference negative reconstruction error kl-divergence also done cvae model using since obtain variational lower bound models also parzen-window based log-likelihood estimation method comparing models. particular ﬁxed test instance condition attributes generate samples models. gaussian parzen window generated samples log-probability face test instance computed obtained gaussian parzen window. σ-parameter parzen window estimator obtained cross-validation validation set. corresponding log-likelihood estimates models given table quantitative results convey sense superiority proposed model models used comparison convincing look actual samples generated models. hence compare three models cgan cvae cmma task generating faces attributes. also compare models cvae cmma modifying existing face changing attributes. cgan used modifying faces uni-directional nature model possible sample latent layer image generative adversarial network. generated feeding noise attributes generator. similarly cvae noise attributes corresponds sample images. order generate images attributes cmma prune cmma model connect stead shown figure fig. modifying faces training data modifying corresponding attributes using cmma cvae respectively rows ﬁgures correspond change nose iii) spectacles moustache lips. except spectacles change attributes reﬂected faces modiﬁed cvae. set/reset ’male’ ’asian’ attributes generate four possible combinations. faces generated varying attributes time. order remove bias selection images variance parameter noise level cmma cvae cgan. corresponding faces model cmma models cgan listed figure columns images model correspond attributes change mouth open iii) spectacles bushy eyebrows nose pointy nose vii) thick lips. evident ﬁrst image figure cmma incorporate change attribute ‘open mouth’ ‘spectacles’ corresponding face rows. however seem case models. hypothesize model explicitly minimizes kl-divergence latent representation attributes joint representation face attributes. attr orig original attributes face attr attributes pass selected face attr hσ}. pass attr orig attr compute difference. difference output hσ}. pass resultant decoder gσ}. note that cgan experiments since given face possible sample latent layer cgan. hence present results corresponding model cmma cvae. corresponding transformed faces given figure observed attributes model cmma successfully able transform images removing moustaches adding spectacles making nose bigger pointy etc. modifying faces missing attributes next select faces evaluate performance model modifying faces. order modify faces needs sample attributes conditioned faces. algorithm modifying faces mentioned previous section applied. corresponding results given figure paper proposed model conditional modality generation forces latent representation modality ‘close’ joint representation multiple modalities. explored applicability model generating modifying images using attributes. quantitative qualitative results suggest model suitable task cgan cvae model proposed general used tasks whereby modalities need conditioned whereas others need generated instance translation text transliteration speech. wish explore applicability model problems future.", "year": 2016}