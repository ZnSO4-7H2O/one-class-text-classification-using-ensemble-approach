{"title": "Progressive Reinforcement Learning with Distillation for Multi-Skilled  Motion Control", "tag": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "abstract": "Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems, including agents that can move with skill and agility through their environment. An open problem in this setting is that of developing good strategies for integrating or merging policies for multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. We extend policy distillation methods to the continuous action setting and leverage this technique to combine expert policies, as evaluated in the domain of simulated bipedal locomotion across different classes of terrain. We also introduce an input injection method for augmenting an existing policy network to exploit new input features. Lastly, our method uses transfer learning to assist in the efficient acquisition of new skills. The combination of these methods allows a policy to be incrementally augmented with new skills. We compare our progressive learning and integration via distillation (PLAID) method against three alternative baselines.", "text": "glen berseth* cheng xie* paul cernek michiel panne gbersethcs.ubc.cacheng.k.xiegmail.compcernekcs.ubc.ca vancs.ubc.ca university british colubia deep reinforcement learning demonstrated increasing capabilities continuous control problems including agents move skill agility environment. open problem setting developing good strategies integrating merging policies multiple skills individual skill specialist speciﬁc skill associated state distribution. extend policy distillation methods continuous action setting leverage technique combine expert policies evaluated domain simulated bipedal locomotion across different classes terrain. also introduce input injection method augmenting existing policy network exploit input features. lastly method uses transfer learning assist efﬁcient acquisition skills. combination methods allows policy incrementally augmented skills. compare progressive learning integration distillation method three alternative baselines. gain experience humans develop rich repertoires motion skills useful different contexts environments. recent advances reinforcement learning provide opportunity understand motion repertoires best learned recalled augmented. inspired studies development recall movement patterns useful different locomotion contexts develop evaluate approach learning multi-skilled movement repertoires. follows refer proposed method plaid progressive learning integration distillation. long lived applications complex control tasks learning system need acquire integrate additional skills. accordingly problem deﬁned sequential acquisition integration skills. given existing controller capable one-or-more skills wish efﬁciently learn skill movement pattern informed existing control policy reintegrate single controller capable full motion repertoire. process repeated necessary. view plaid continual learning method consider context tasks known advance wish learn task efﬁcient manner. however also proves surprisingly effective multitask solution given three speciﬁc benchmarks compare against. process acquiring skill also allow control policy augmented additional inputs without adversely impacting performance. process refer input injection. understanding time course sensorimotor learning human motor control open research problem exists concurrently recent advances deep reinforcement learning. issues generalization context-dependent recall transfer savings fast learning forgetting scalability play human motor control models learning curricula proposed reinforcement learning. development hierarchical models skills offers particular solution supports scalability avoids problems related forgetting eschew approach work instead investigate progressive approach integration control policy deﬁned single deep network. distillation refers problem combining policies experts order create single controller perform tasks experts. cast supervised regression problem objective learn model matches output distributions expert policies however given task expert given less clear learn task successfully integrating skill pre-existing repertoire control policy agent. wellknown technique machine learning signiﬁcantly improve sample efﬁciency across similar tasks transfer learning seeks reuse knowledge learned solving previous task efﬁciently learn task. however transferring knowledge previous tasks tasks straightforward; negative transfer wherein previously-trained model take longer learn task ﬁne-tuning would randomlyinitialized model additionally learning skill control policy forget perform skills. core contribution paper method progressive learning integration distillation repeatedly expand integrate motion control repertoire. main building blocks consist policy transfer multi-task policy distillation method evaluated context continuous motor control problem robust locomotion distinct classes terrain. evaluate method three alternative baselines. also introduce input injection convenient mechanism adding inputs control policies support skills preserving existing capabilities. transfer learning distillation broad interest machine learning outline relevant work area deep reinforcement learning continuous control environments. distillation recent works explored problem combining multiple expert policies reinforcement learning setting. popular approach uses supervised learning combine policy regression action distribution. approach yields model compression well viable method multi-task policy transfer discrete action domains including arcade learning environment adopt techniques extend case complex continuous action space tasks make building block. transfer learning transfer learning exploits structure learned previous task learning task. focus transfer learning environments consisting continuous control tasks. concept appending additional network structure keeping previous structure reduce catastrophic forgetting worked well atari games methods reproduce data tasks reduce possibility forgetting perform previously learned skills recent work seeks mitigate issue using selective learning rates speciﬁc network parameters different approach combining policies hierarchical structure setting previously-learned policies available options execute policy trained task. however approach assumes tasks least partial composition previous tasks reintegration newly learned tasks. recent promising approach apply meta-learning achieve control policies quickly adapt behaviour according current rewards work demonstrated parameterized task domains. powerplay method provides general framework training increasingly general problem solver based iteratively inventing task using play invention; solving task; lastly demonstrating ability solve previous tasks. last stages broadly similar plaid approach although best knowledge experiments motor control tasks comparable complexity ones tackle. work develop speciﬁc progressive learning-and-distillation methodology motor skills provide detailed evaluation compared three plausible baselines. speciﬁcally interested understanding issues arise interplay transfer related tasks forgetting occur. hierarchical uses modularity achieve transfer learning robotic tasks allows substitution network modules different robot types similar tasks methods hierarchical reinforcement learning method simplifying complex motor control problem deﬁning decomposition overall task smaller tasks methods examine knowledge transfer examine reintegration policies related tasks associated problems catastrophic forgetting. recent work examines learned motions shaped prior mocap clips integrated hierarchical controller. leveraging framework reinforcement learning frame problem markov decision processes time step world state wherein agent able perform actions sampled policy resulting state according transition probabilities performing action state produces reward expected cumulative reward earned following policy written time horizon discount factor deﬁning planning horizon length. agent’s goal learn optimal policy maximizing policy parameters goal reformulated identify optimal parameters optimize policy stochastic policy gradient methods well-established family techniques reinforcement learning gradient expected reward respect policy parameters given value function gives expected discounted cumulative reward following policy starting state beneﬁt insensitive advantage function scale. furthermore limiting policy updates direction actions positive advantage found increase stability learning true value function unknown approximation parameters learned formulated regression problem given expert agents solved/mastered different tasks want combine skills different experts single multi-skilled agent. process referred distillation. distillation necessarily produce optimal given experts instead tries produce expert best matches action distributions produced experts. method functions independent reward functions used train expert. distillation also scales well respect number tasks experts combined. given expert solved/mastered task want reuse expert knowledge order learn task efﬁciently. problem falls area transfer learning considering state distribution expert skilled solving advantageous start learning target task target distribution dωi+ using assistance expert. agent learning solve target task domain dωi+ referred student. expert used assist student learning target task referred teacher. success methods dependent overlap dωi+ state distributions. although focus problem presented tasks sequentially exist methods learning multi-skilled character. considered overall integration methods learning multiple skills ﬁrst controller learns multiple tasks time number skills learned time. shown learning many tasks together faster learning task separately curriculum using method shown figure single simulation tasks learned together. also possible randomly initialize controllers train parallel combine resulting policies figure found learning many skills scratch challenging able fair results task. also task learned parallel model would occur outside original parallel learning leading sequential method. tl-only method uses learning tasks sequence figure possibly ending distillation step combine learned policies decrease forgetting. details appendix last version learns task sequentially using previous skilled policy resulting policy capable solving tasks figure method works well combining learned skills learning skills. figure different curriculum learning process. denotes distillation step combines policies. gray denotes iteration learning policy. larger boxes lterrain−type denotes learning step skill learned. section detail proposed learning framework continual policy transfer distillation acquisition step interested learning task ωi+. transfer beneﬁcial task structure somewhat similar previous tasks adopt strategy using existing policy network ﬁne-tuning task. since concerned retaining previous skills step update policy without concern forgetting. agent learns develop skills addition every skill increase probability transferring knowledge assist learning next skill. integration step interested combining past skills newly acquired skill πi+. traditional approaches used policy regression data generated collecting trajectories expert policy task. training student trajectories always result robust behaviour. poor behaviour caused student experiencing different distribution trajectories expert evaluation. compensate distribution difference portions trajectories generated student. allows expert suggest behaviour pull state distribution student closer expert’s. common problem learning model reproduce given distribution trajectories method similar dagger algorithm useful distilling policies appendix details. algorithm actor-critic method also perform regression critic ﬁtting step. results presented work cover range tasks share similar action space state space. focus demonstrate continual learning related tasks. addition conceptual framework allows extensions would permit differing state spaces described later section experiment tasks consists different terrains humanoid walker learns traverse. humanoid walker trained navigate multiple types terrain including incline steps slopes gaps combination terrains mixed agents trained. goal tasks maintain consistent forward velocity traversing various terrains also matching motion capture clip natural human walking gait ground similar pd-biped receives input character terrain state representation consisting terrains heights equally-spaced points front character. action space -dimensional corresponding joints. reasonable torque limits applied helps produce natural motions makes control problem difﬁcult. detailed description experimental setup included section tasks presented agent sequentially goal progressively learn traverse terrain types. evaluate approach three baselines. first compare learning curriculum learning tasks plaid learning tasks parallel. demonstrate knowledge previous tasks effectively transferred distillation steps. second compare multitasker demonstrate iterated distillation effective retention learned skills. multitasker also used baseline comparing learning speed. last method performs tasks concludes distillation step evaluated illustrate result different distillation schedules. results plaid controller displayed accompanying video first pd-biped trained produce walking motion ground figure plaid compared three baselines training incline. tl-only method learns fast given signiﬁcant information perform similar skills. parallel method given prior information leading less skilled policy. ﬁrst multitasker incline task initialized terrain injected controller trained walk ground. subsequent multitasker initialized ﬁnal multitasker model preceding task. controller learn multiple tasks together complicate learning process simulation task split across training overall task challenging. contrast using plaid also initialized policy trained integrate skills together skill learned. figure multitasker learning task similar speed plaid. however adding tasks multitasker beginning struggle figure starts forget figure number tasks must learn time. plaid learns tasks faster able integrate skill required solve task robustly. tl-only also able learn tasks efﬁciently. appealing property using distillation plaid combined policy model need resemble individual expert controllers. example different experts lacking state features trained without local terrain combined single policy state features terrain. terrain features assist agent task domain operates. introduce idea input injection purpose. augment policy additional input features allowing retain original functional behaviour similar achieved adding additional inputs neural network initializing connecting layer weights biases setting weights biases layer connecting features original network gradient still propagate lower layers initialized random without changing functional behaviour. performed distilling incline experts. details found appendix training multiple tasks time help agent learn skills quicker scale respect number tasks. training multitasker even three tasks method displays good results however learning fourth tasks method struggles shown figure part reason struggle figure learning comparison environments. plots show mean simulations initialized different random seeds. learning plaid split steps going ﬁrst followed distillation part table values relative percentage changes average reward value forgetting value corresponds completely forgetting perform task. value corresponds agent learning better perform task training tasks. here ﬁnal policy training gaps compared original polices produced training task noted column heading. tl-only baseline forgets plaid. multitasker forgets less plaid lower average reward tasks. tasks added multitasker make trade-offs tasks maximizes. tasks added trade-off becomes increasingly complex resulting multitasker favouring easier tasks. using plaid combine skills many policies appears scale better respect number skills integrated. likely distillation semi-supervised method stable un-supervised solution. seen figure especially plaid combines skills faster higher value policies practice. plaid also presents zero-shot training tasks never trained figure generalization shown agent navigates across mixed environment. also reﬂected table shows ﬁnal average reward comparing methods distillation. tl-only able achieve high performance much lost learning tasks. ﬁnal distillation step helps mitigate issue work well plaid. possible performing large ﬁnal distillation step lead over-ﬁtting. indications distillation hindering training initial iterations. initializing network used distillation recently learning policy large change initial state distribution previous seen distribution could causing larger gradients appear disrupting structure learned step shown figure also might exist smooth transition policy space newly learned policy previous policy distribution. multitasker plaid multitasker able produce policy higher overall average reward practise constraints keep method combining skills gracefully. reward functions different tasks multitasker favour task higher rewards tasks receive higher advantage. also non-trivial task normalize reward functions task order combine them. multitasker also favour tasks easier tasks general. shown plaid scales better respect number tasks multitasker. expect plaid would outperform multitasker tasks difﬁcult reward functions dissimilar. evaluation compare number iterations plaid uses number multitasker uses task necessarily fair. multitasker gains beneﬁts training tasks together. idea reduce number simulation samples needed learn tasks multitasker would fall behind. distillation also efﬁcient respect number simulation steps needed. data could collected simulator groups learned many batches data needed common behavioural cloning. expect another reason distillation beneﬁts learning multiple tasks integration process assists pulling policies local minima prone assisted learning process. transferring task state distribution changed reward function completely different. makes unlikely value function accurate task. addition value functions general easier faster learn policies implying value function reuse less important transfer. also helpfulness depends task difﬁculty reward function well. tasks overlap state space area overlap could easily reachable. case give signiﬁcant beneﬁt overall problem easy. greatest beneﬁt gained state space overlaps tasks difﬁcult reach difﬁcult reach area highest rewards achieved. integrated skills locomotion tasks self-selecting based context i.e. knowledge upcoming terrain. augmentation distillation strategies better situations either reward functions different one-hot vector used select currently active expert. transfer learning results could ﬁtting initial expert particular task learning. making challenging policy learn task resulting negative transfer. learning many tasks previous tasks receive large enough potion distillation training process preserve experts skill well enough. best chose data trained next best preserve behaviour experts general problem multi-task learning. distillation treats tasks equally independent reward. result value tasks receiving potentially distribution desired high value tasks receiving enough. needed one-hot vector indicate task agent performing. want agent able recognize task given realize tasks could similar differentiate walking jogging ground. would interesting develop method prioritize tasks distillation step. could assist agent forgetting issues help relearning tasks. currently mean squared error pull distributions student policies line expert polices distillation better distance metrics would likely helpful. previous methods used divergence discrete action space domain state-action value function encodes policy e.g. deep q-network work focus producing best policy mixture experts instead match distributions number experts. difference subtle practice challengine balance many experts respect reward functions. could also beneﬁcial penalty performing distillation i.e. something similar work order keep policy changing rapidly training. proposed evaluated method progressive learning integration motion skills. method exploits transfer learning speed learning skills along input injection needed well continuous-action distillation using dagger-style learning. compares favorably baselines consisting learning skills together learning skills individually integration. believe remains much learned best training integration methods movement skill repertoires also reﬂected human motor learning literature. samy bengio oriol vinyals navdeep jaitly noam shazeer. scheduled sampling sequence prediction recurrent neural networks. cortes lawrence sugiyama garnett advances neural information processing systems curran associates inc. http//papers.nips.cc/paper/ -scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf. coline devin abhishek gupta trevor darrell pieter abbeel sergey levine. learning modular neural network policies multi-task multi-robot transfer. robotics automation ieee international conference ieee nicolas heess gregory wayne yuval tassa timothy lillicrap martin riedmiller david silver. learning transfer modulated locomotor controllers. corr abs/. http//arxiv.org/abs/.. james kirkpatrick razvan pascanu neil rabinowitz joel veness guillaume desjardins andrei rusu kieran milan john quan tiago ramalho agnieszka grabska-barwinska demis hassabis claudia clopath dharshan kumaran raia hadsell. overcoming catastrophic forgetting neural networks. proceedings national academy sciences ./pnas.. http//www.pnas.org/content///.abstract. tejas kulkarni karthik narasimhan ardavan saeedi josh tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. advances neural information processing systems alex lamb anirudh goyal alias parth goyal ying zhang saizheng zhang algorithm sugiyama luxburg information processing systems http//papers.nips.cc/paper/ josh merel yuval tassa sriram srinivasan lemmon ziyu wang greg wayne nicolas heess. learning human behaviors motion capture adversarial imitation. arxiv preprint arxiv. peng glen berseth kangkang michiel panne. deeploco dynamic locomotion skills using hierarchical deep reinforcement learning. transactions graphics rajendran lakshminarayanan khapra prasanna ravindran. attend adapt transfer attentive deep architecture adaptive transfer multiple sources domain. arxiv preprint arxiv. october st´ephane ross geoffrey gordon andrew bagnell. no-regret reductions imitation learning structured prediction. corr abs/. http//arxiv.org/abs/. andrei rusu sergio gomez colmenarejo caglar gulcehre guillaume desjardins james kirkpatrick razvan pascanu volodymyr mnih koray kavukcuoglu raia hadsell. policy distillation. arxiv preprint arxiv. andrei rusu neil rabinowitz guillaume desjardins hubert soyer james kirkpatrick koray kavukcuoglu razvan pascanu raia hadsell. progressive neural networks. arxiv http//arxiv.org/abs/.. j¨urgen schmidhuber. powerplay training increasingly general problem solver continually searching simplest still unsolvable problem. corr abs/. http //arxiv.org/abs/.. john schulman philipp moritz sergey levine michael jordan pieter abbeel. highdimensional continuous control using generalized advantage estimation. international conference learning representations richard sutton david mcallester satinder singh yishay mansour. policy gradient methods reinforcement learning function approximation. advances neural information processing systems whye victor bapst wojciech marian czarnecki john quan james kirkpatrick raia hadsell nicolas heess razvan pascanu. distral robust multitask reinforcement learning. arxiv preprint arxiv. chen tessler shahar givony zahavy daniel mankowitz shie mannor. deep hierarchical approach lifelong learning minecraft. arxiv http //arxiv.org/abs/.. used different network models experiments paper. ﬁrst model blind model terrain features. blind policy neural network hidden layers relu activations. output layer policy network linear activations. network used value function design except output ﬁnal layer. design used incline tasks. augment blind network design adding features terrain create agent sight. network terrain features single convolution layer ﬁlters width constitutional layer followed dense layer units. dense layer concatenated twice along original hidden layers blind version policy. policy network models gaussian distribution outputting state dependant mean. state independent standard deviation normalized respect action space multiplied also version epsilon greedy exploration probability exploration action generated. experiments linearly anneal iterations leave point training simulation takes approximately hours across threads. network training stochastic gradient decent momentum. distillation step gradually anneal probability selecting expert action iterations. evaluation model particular task average reward achieved agent seconds simulation time. average running agent number randomly generated simulation runs. distillation steps initialize policy recently trained policy. policy seen tasks thus overﬁt recent tasks. version dagger algorithm distillation process anneal selecting actions expert polices selecting actions student policy probability selecting action expert annealed near zero training updates. still exploration noise policies generating actions take simulation. also annealed along probability selecting expert policy. actions used training always come expert policy. although actions applied simulation student training update actions replaced ones proper expert. expert used generate actions tasks expert used generate action task πi+. keep around policies time. order additional input features policy network construct network. network portion design previous network plus additional parameters. first initialize network random parameters. copy values previous network portion network design matches old. weight layers connect portion network allow network preserve previous distribution modeled. parameters network also help generate gradients train valued network parameters. feature injection assist learning method differentiating different states. example could challenging discover difference incline tasks using character features. therefore terrain features allow controller better differentiate different tasks. also evaluate baseline tasks. baseline performed number tasks distillation used combined many learned skills. method considered version plaid tasks learned groups number tasks collection policies/skills distilled together. figure learning curves tl-only baseline given. tl-only method learns tasks well. show incline tasks methods starting steps tasks. table amount forgetting compared methods. compare amount forgetting tl-only plaid show relative loss average reward original policy trained tasks steps slopes ﬁnal polices method gaps. tl-only method shows larger drop figure input features include character state shown lines root character character’s links terrain features shown blue arrows along ground. diagram method used inject additional state features terrain. policy performance corresponding large amount forgeting compared plaid particular complex tasks steps slopes. interestingly ﬁnal distllation step tl-only appears reduce performance policy. believe related ﬁnal distillation step challenging performing simpler distillation task. note compare steps process ﬁrst tasks plaid tl-only same. comparison average rewards ﬁnal policies given table agent used simulation models dimensions masses average adult. size character state parameters include relative position velocity links agent action space consists parameters indicate target joint positions agent. target joint positions turned joint torques proportional derivative controllers joint. reward function agent consists primary terms. ﬁrst velocity term rewards agent going velocity second term difference pose agent current pose kinematic character controlled motion capture clip. difference agent clip consists rotational difference corresponding joint difference angular velocity. angular velocity clip approximated ﬁnite differences current pose clip it’s last pose. last term penalty torques generated agent help reduce spastic motions. also impose torque limits joints reduce unrealistic behaviour limits hips knees ankles shoulders elbows neck n/m. terrain types terrain types randomly generated episode except terrain. incline terrain slanted slant terrain randomly sampled degrees. steps terrain consists segments widths randomly sampled followed sharp steps randomly generated heights slopes terrain randomly generated updating slope previous point ground value sampled degrees generate portion ground every gaps terrain generate gaps width separated segments widths sampled mixed terrain combination terrains portion randomly chosen terrain types. certain cases multitasker learn task faster plaid. figure present multitasker compare plaid. case multitasker splits training time across multiple tasks compare methods respect time spent learning single task. good baseline compare method ways fair. real measure efﬁcient learning method number simulation samples needed learn would fall behind multitasker needs train across tasks gain beneﬁts improving single task without forgetting tasks.", "year": 2018}