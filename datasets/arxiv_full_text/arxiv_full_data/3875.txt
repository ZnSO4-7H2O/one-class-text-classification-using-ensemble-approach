{"title": "Finding a sparse vector in a subspace: Linear sparsity using alternating  directions", "tag": ["cs.IT", "cs.CV", "cs.LG", "math.IT", "math.OC", "stat.ML"], "abstract": "Is it possible to find the sparsest vector (direction) in a generic subspace $\\mathcal{S} \\subseteq \\mathbb{R}^p$ with $\\mathrm{dim}(\\mathcal{S})= n < p$? This problem can be considered a homogeneous variant of the sparse recovery problem, and finds connections to sparse dictionary learning, sparse PCA, and many other problems in signal processing and machine learning. In this paper, we focus on a **planted sparse model** for the subspace: the target sparse vector is embedded in an otherwise random subspace. Simple convex heuristics for this planted recovery problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds $O(1/\\sqrt{n})$. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is $\\Omega(1)$. To the best of our knowledge, this is the first practical algorithm to achieve linear scaling under the planted sparse model. Empirically, our proposed algorithm also succeeds in more challenging data models, e.g., sparse dictionary learning.", "text": "possible sparsest vector generic subspace problem considered homogeneous variant sparse recovery problem ﬁnds connections sparse dictionary learning sparse many problems signal processing machine learning. paper focus planted sparse model subspace target sparse vector embedded otherwise random subspace. simple convex heuristics planted recovery problem provably break fraction nonzero entries target sparse vector substantially exceeds contrast exhibit relatively simple nonconvex approach based alternating directions provably succeeds even fraction nonzero entries best knowledge ﬁrst practical algorithm achieve linear scaling planted sparse model. empirically proposed algorithm also succeeds challenging data models e.g. sparse dictionary learning. suppose linear subspace embedded contains sparse vector given arbitrary basis efﬁciently recover equivalently provided matrix null efﬁciently nonzero sparse vector language sparse recovery solve contrast standard sparse recovery problem convex relaxations perform nearly optimally broad classes designs computational properties problem nearly well understood. known several decades basic formulation np-hard arbitrary subspace paper assume speciﬁc random planted sparse model subspace target sparse vector embedded otherwise random subspace. show speciﬁc random model problem tractable efﬁcient algorithm based nonconvex optimization. general version problem arbitrary subspace takes several forms numerical computation computer science underlies several important problems modern signal processing machine learning. provide sample applications. sparse null space matrix sparsiﬁcation sparse null space problem ﬁnding sparsest matrix whose columns span null space given matrix problem arises context solving linear equality problems constrained optimization null space methods quadratic programming solving work partially supported grants n--- funding moore sloan foundations. wright electrical engineering department columbia university york paper extension previous conference version underdetermined linear equations matrix sparsiﬁcation problem similar ﬂavor task ﬁnding sparsest matrix equivalent given full rank matrix elementary column operations. sparsity helps simplify many fundamental matrix operations problem applications areas machine learning discovering cycle bases graphs discusses connections problems also problems complexity theory. sparse dictionary learning dictionary learning given data matrix seeks approximation representation dictionary certain desired structure collects representation coefﬁcients maximal sparsity. compact representation naturally allows signal compression also facilitates efﬁcient signal acquisition classiﬁcation required complete linear algebra have problem reduces ﬁnding sparsest vectors known subspace i.e. insights problem theoretical developments complete dictionary learning sparse principal component analysis geometric terms sparse early developments discussion recent results) concerns stable estimation linear subspace spanned sparse basis data-poor regime i.e. available data numerous enough allow decouple subspace estimation sparsiﬁcation tasks. formally given data matrix rp×n collects column-wise data points rp×r sparse basis noise matrix asked estimate factorization ﬁnds applications gene expression ﬁnancial data analysis pattern recognition subspace known problem reduces instances already nontrivial. full geometric sparse treated ﬁnding sparse vectors subspace subject perturbation. addition variants generalizations problem also studied applications regarding control optimization nonrigid structure motion spectral estimation prony’s problem outlier rejection blind source separation graphical model learning sparse coding manifolds also references therein. despite potential applications problem recently efﬁcient computational surrogates nontrivial recovery guarantees discovered cases practical interest. context sparse dictionary learning spielman introduced convex relaxation replaces nonconvex problem sequence linear programs proved generated span random sparse vectors high probability relaxation recovers vectors provided probability entry nonzero planted sparse model formed direct single sparse vector generic subspace hand demanet proved also correctly recovers provided fraction nonzeros scales might imagine improving results tightening analyses. unfortunately results essentially sharp substantially exceeds models relaxation provably breaks down. moreover natural semideﬁnite programming relaxation threshold simply intrinsic price must efﬁcient algorithm even random models. evidence towards conjecture might borrowed superﬁcial similarity sparse sparse substantial achieved currently available efﬁcient algorithms information theoretic optimum also case recovering sparse vector subspace? simply best efﬁcient guaranteed algorithms? perhaps surprising possible polynomial time algorithm. unfortunately runtime approach high-degree polynomial machine learning problems often either feature dimension sample size algorithm mostly theoretical interest only. however raises interesting algorithmic question practical algorithm provably recovers sparse vector initial submission paper hopkins proposed different simple algorithm based spectral method. algorithm guarantees recovery planted sparse vector also linear sparsity whenever comes better time complexity. algorithm based alternating directions special twists. first introduce special data driven initialization seems important achieving second theoretical results require second linear programming based rounding phase similar core algorithm simple iterations linear complexity size data hence scalable moderate-to-large scale problems. besides enjoying guarantee reach previous practical algorithms algorithm performs well simulations empirically succeeding also performs well empirically challenging data models complete dictionary learning model subspace interest contains random target sparse vectors. encouraging breaking sparsity barrier practical algorithm optimal guarantee important problem theoretical dictionary learning regard recent work presents efﬁcient algorithm based riemannian optimization guarantees recovery linear sparsity. however result based different ideas different nonconvex formulation optimization algorithm analysis methodology. rest paper organized follows. section provide nonconvex formulation show capability recovering sparse vector. section introduces alternating direction algorithm. section here estimation based degree- hierarchy used obtain initial approximate recovery. despite improved guarantees planted sparse model method still produces appealing results real imagery data present main results sketch proof ideas. experimental evaluation method provided section conclude paper drawing connections related work discussing potential improvements section full proofs deferred appendix sections. matrix denote i-th column j-th respectively column vector form. moreover denote i-th component vector compact notation positive integer indexed versions denote absolute numerical constants. scope constants always local namely within particular lemma proposition proof apparently constant different contexts carry different values. probability events sometimes event holds with high probability probability failure dominated study problem recovering sparse vector element known subspace dimension provided arbitrary orthonormal basis rp×n starting point nonconvex formulation objective constraint nonconvex hence easy optimize over. relax replacing norm norm. constraint since applications care solution scaling natural force live unit sphere giving formulation still nonconvex general nonconvex problems known np-hard even local minimizer nevertheless geometry sphere benign enough well-structured inputs actually possible give algorithms global optimizer. formulation contrasted effectively optimize norm subject constraint polyhedral ∞-constrained problem immediately yields sequence linear programs. convenient computation analysis. however suffers contrast though sphere aforementioned breakdown behavior around complicated geometric constraint allow much larger number nonzeros indeed consider global optimizer reformulation orthonormal basis sufﬁcient condition guarantees exact recovery planted sparse model subspace follows theorem recovery planted sparse model). exists constant subspace follows planted sparse model hence could global optimizer would able recover whose number nonzero entries quite large even linear dimension hand obvious possible nonconvex. next section describe simple heuristic algorithm approximately solving relaxed version problem surprisingly prove class random problem instances algorithm plus auxiliary rounding technique actually recovers global optimizer target sparse vector proof requires detailed probabilistic analysis sketched section iv-b. here penalty parameter. difﬁcult problem equivalent minimizing huber m-estimator relaxation makes possible apply alternating direction method problem. method starts initial point alternates optimizing respect optimizing w.r.t. additional tricks needed. initialization. problem nonconvex arbitrary random initialization produce global minimizer. fact good initializations critical proposed algorithm succeed linear sparsity regime. purpose suggest using every normalized initializations solving sequence nonconvex programs algorithm. intuition initialization works recall planted sparse model span magnitude size hence large somewhat bigger entries another biased towards ﬁrst standard basis vector probabilistic model assumptions well conditioned using gram-schmidt process orthonormal basis upper triangular well-conditioned since i-th biased direction well-conditioned i-th also biased direction words canonical orthobasis subspace i-th biased direction global optimizer. heuristic arguments made rigorous appendix appendix initialization invariant rotation orthobasis. hence even handed arbitrary orthobasis i-th still biased direction global optimizer. rounding linear programming denote output algorithm illustrated fig. prove particular initialization appropriate choice algorithm uniformly moves towards optimal large portion sphere solution falls within certain small radius globally optimal solution exactly recover equivalently recover exact sparse vector solve linear program since feasible essentially tangent space sphere whenever close enough expect optimizer exactly recovers hence scale. prove indeed true appropriate conditions. previous section succeeds. theorem iv.. suppose obeys planted sparse model columns form arbitrary orthonormal basis subspace denote rows apply algorithm positive constants. remark iv.. result theorem suboptimal sample complexity compared global optimality result theorem barak al.’s result successful believe still ﬁrst practical efﬁcient method guaranteed achieve rate. lower bound theorem mostly convenience proof; fact rounding stage algorithm already succeeds w.h.p. orthogonalization i.e. gpx⊥ large nearly orthogonal hence close thus proofs whenever convenient make arguments ﬁrst propagate quantitative results onto perturbation arguments. noted y··· transpose rows note independent random vectors. prove result theorem need following results. first given speciﬁed show initialization biased towards global optimum proposition suppose least initialization vectors suggested section want show w.h.p. iterate sequence converges small neighborhood algorithm plus rounding successfully retrieves sparse vector thus hope general concentrated ﬁrst coordinate sn−. partition vector rn−; correspondingly inner product strictly larger inner product region bounded grossly w.h.p.. proposition constant holds probability least algorithm algorithm initialization satisfying least iterations provided constant following holds probability least provided suppose input basis deﬁned algorithm produces output rounding procedure returns desired solution positive constants. w.l.o.g. ﬁrst consider deﬁned orthogonalization natural/canonical form show w.h.p. algorithmic pipeline described section exactly recovers optimal solution scale following argument taken together claims imply least initializers algorithm produce output accurate enough rounding exactly return x/x. hand optimality theorem implies unique vectors smallest norm among ﬁnally produce minimal norm selector successfully locate vector. target solution following technical pieces perfectly parallel argument unit vectors subspace. since w.h.p. among unit vectors initializers general case input arbitrary orthonormal basis orthogonal matrix discussion appendix implies w.h.p. least provides initial point thatq strictly positive indicating steady progress towards point thatq next iterate move discussion appendix implies satisﬁesq remark iv.. planted sparse model practice algorithm proposed initialization converges global optimizer correctly recovers fact simple calculation shows desired point successful recovery indeed critical point near pole fig. unfortunately using current analytical framework succeed proving convergence theory. proposition imply iterations however sequence stay small neighborhood target. hence proposed stop steps round output using provable recover target implied proposition iv.. rounding procedure purpose completing theory seems necessary practice. suspect alternative analytical strategies geometrical analysis discuss section likely around artifact. section show performance proposed algorithm synthetic real datasets. synthetic dataset show phase transition algorithm planted sparse dictionary learning models; real dataset demonstrate seeking sparse vectors help discover interesting patterns face images. planted sparse model pair generate dimensional subspace direct k-sparse vector uniformly random support nonzero entries equal i.i.d. gaussian matrix distributed basis subspace constructed denotes gram-schmidt orthonormalization operator rn×n arbitrary orthogonal matrix. regularization parameter normalized rows initializations proposed algorithm alternating steps iterations. determine recovery successful whenever least trials determine empirical recovery performance algorithm ﬁrst relationship plot phase transition next sparsity level plot phase transition pair repeat simulation times. fig. shows phase transition plots. also experiment complete dictionary learning model speciﬁcally observation assumed square invertible matrix sparse matrix. since invertible space pair generate vector k-sparse every nonzero entry following i.i.d. gaussian sparse model described above. difference determine recovery successful long sparse recovered programs. fig. shows phase transition plots. fig. fig. suggest algorithm could work linear sparsity regime models provided moreover models factor seems necessary working linear sparsity regime suggested fig. fig. clear nonlinear transition boundaries success failure regions. models sample requirement near optimal planted sparse model obviously necessary; complete dictionary learning model proved required exact recovery. planted sparse model result much fig. phase transition planted sparse model using algorithm ﬁxed relationship ﬁxed relationship white indicates success black indicates failure. fig. phase transition dictionary learning model using algorithm ﬁxed relationship ﬁxed relationship white indicates success black indicates failure. well known computer vision collection images convex object subject illumination changes well approximated low-dimensional subspaces raw-pixel space play face subspaces here. first extract face images person different illumination conditions. apply robust principal component analysis data dimensional subspace dimension i.e. basis apply algorithm sparsest elements subspace randomly selecting rows initializations judge sparsity sense vectors found project subspace onto orthogonal complement sparse vectors already found continue seeking process projected subspace. fig. shows ﬁrst four sparse vectors data. correspond well different extreme illumination conditions. also implemented spectral method proposed comparison protocol. result presented fig. ratios signiﬁcantly higher ratios promote sparsity) signiﬁcantly lower. criteria spectral method rounding consistently produces vectors higher sparsity levels evaluation protocol. moreover resulting images harder interpret physically. second manually select different persons’ faces normal lighting condition. again dimension subspace repeat experiment stated above. fig. shows four sparse vectors data. interestingly sparse vectors roughly correspond differences face images concentrated around facial parts different people tend differ other e.g. brows forehead hair nose etc. comparison vectors returned spectral method relatively denser sparsity patterns images less structured physically. algorithm seems useful sparse vectors potential applications peculiarity discovery ﬁrst setting locating differences second setting. nevertheless main goal experiment invite readers think similar pattern discovery problems might cast problem seeking sparse vectors subspace. experiment also demonstrates concrete practicality algorithm handling data sets realistic size producing meaningful results even beyond planted sparse model adopted analysis. planted sparse model substantial performance terms relationship optimality theorem empirical simulations guarantees obtained efﬁcient algorithm careful tighter analysis based decoupling chaining geometrical analysis described probably help bridge theoretical empirical results. matching theoretical limit depicted theorem seems require novel algorithmic ideas. random models assume subspace extended random models particularly dictionary learning bases sparse work part recent surge research efforts deriving provable practical nonconvex algorithms central problems modern signal processing machine learning. problems include low-rank matrix recovery/completion tensor recovery/decomposition phase retrieval dictionary learning approach like others start carefully chosen problem-speciﬁc initialization perform local analysis subsequent iterates guarantee convergence good solution. comparison subsequent work complete dictionary learning generalized phase retrieval taken geometrical approach characterizing function landscape designing efﬁcient algorithm accordingly. geometric approach allowed provable recovery efﬁcient algorithms arbitrary initialization. article summarizes geometric approach applicability several problems interest. hybrid initialization geometric approach discussed likely powerful computational framework. action current planted sparse vector problem fig. provide asymptotic function landscape huber loss sphere clear initialization biased towards either north south pole situated region gradients always nonzero points favorable directions many reasonable optimization algorithms take gradient information make steady progress towards target. probably ease algorithm development analysis help yield tight performance guarantees. provide efﬁcient algorithm ﬁnding sparse vector subspace strong guarantee. algorithm practical handling large datasets—in experiment face dataset successfully extracted meaningful features human face images. however potential seeking sparse/structured element subspace seems largely unexplored despite cases mentioned start. hope work could inspire application ideas. thanks family private foundation generous support. thank ieor department columbia university helpful discussion input regarding work. thank anonymous reviewers constructive comments helped improve manuscript. work partially supported grants n--- funding moore sloan foundations. fig. function landscape sphere note near spherical caps around north south poles critical points gradients always nonzero; projected function landscape obtained reparameterization projecting upper hemisphere onto equatorial plane. mathematically function convenience. large nearly orthonormal columns expect closely approximates section make intuition rigorous. prove several results needed proof theorem translating results results appendix e-d. realization supp bernstein’s inequality lemma event proof proposition without loss generality work canonical orthonormal basis deﬁned recall orthogonalization planted sparse basis deﬁned deﬁne processes separate correspondingly. task lower bound ﬁnite samples deﬁned since deterministically constrain deﬁned arbitrary here always take sufﬁciently small challenge lies lower bounding upper bounding depend orthonormal basis unnormalized basis much easier work proof follow observation derived appendix e-a. analysis based scalar vector bernstein’s inequalities moment conditions. finally appendix uniformize bound applying classical discretization argument. donoho large underdetermined systems linear equations minimal -norm solution also sparsest solution communications pure applied mathematics vol. simple prior-free method non-rigid structure-from-motion factorization computer vision pattern recognition ieee conference ieee beylkin monz´on approximation functions exponential sums applied computational manolis rene dual principal component pursuit arxiv preprint arxiv. zibulevsky pearlmutter blind source separation sparse decomposition signal dictionary anandkumar janzamin kakade when overcomplete topic models identiﬁable? uniqueness tensor tucker decompositions structured sparsity advances neural information processing systems foucart rauhut mathematical introduction compressive sensing. springer figiel lindenstrauss milman dimension almost spherical sections convex bodies", "year": 2014}