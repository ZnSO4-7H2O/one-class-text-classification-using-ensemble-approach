{"title": "Cognitive Mapping and Planning for Visual Navigation", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "abstract": "We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as \"go to a chair\".", "text": "figure overall network architecture learned navigation network consists mapping planning modules. mapper writes latent spatial memory corresponds egocentric environment planner uses memory alongside goal output navigational actions. supervised explicitly rather emerges naturally learning process. approaches ﬁrst build using lidar depth structure motion plan paths map. maps built purely geometrically nothing known explicitly observed even obvious patterns. becomes problem goal directed navigation. humans often guess example chair hallway probably lead another hallway classical robot agent best uninformed exploration. separation mapping planning also makes overall system unnecessarily fragile. example mapper might fail texture-less regions corridor leading failure whole system precise geometry even necessary robot keep traveling straight. inspired reasoning recently increasing interest end-to-end learning-based approaches directly pixels actions without going explicit model state estimation steps. methods thus enjoy power able learn behaviors experience. however necessary carefully design architectures capture structure task hand. instance reactive memory-less vanilla feed forward architectures introduce neural architecture navigation novel environments. proposed architecture learns ﬁrst-person views plans sequence actions towards goals environment. cognitive mapper planner based ideas uniﬁed joint architecture mapping planning mapping driven needs planner spatial memory ability plan given incomplete observations world. constructs topbelief world applies differentiable neural planner produce next action time step. accumulated belief world enables agent track visited regions environment. experiments demonstrate outperforms reactive strategies standard memory-based architectures performs well novel environments. furthermore show also achieve semantically speciﬁed goals chair. humans navigate novel environments draw previous experience similar conditions. reason free-space obstacles topology environment guided common sense rules heuristics navigation. example room another must ﬁrst exit initial room; room building getting hallway likely succeed entering conference room; kitchen likely situated open areas building middle cubicles. goal paper design learning framework acquiring expertise demonstrate problem robot navigation novel environments. solving visual navigation problems contrast experiments tolman shown even rats build sophisticated representations space form ‘cognitive maps’ navigate giving ability reason shortcuts something reactive agent unable motivates cognitive mapping planning approach visual navigation consists spatial memory capture layout world planner plan paths given partial information. mapper planner together uniﬁed architecture trained leverage regularities world. mapper fuses information input views observed agent time produce metric egocentric multi-scale belief world top-down view. planner uses multi-scale egocentric belief world plan paths speciﬁed goal outputs optimal action take. process repeated time step convey agent goal. time step agent updates belief world previous time step using egomotion transform belief previous time step current coordinate frame incorporating information current view world update belief. allows agent progressively improve model world moves around. signiﬁcant contrast prior work approach trained end-to-end take good actions world. instead analytically computing update belief frame learning problem train convolutional neural network predict update based observed ﬁrst person view. make belief transformation update operations differentiable thereby allowing end-to-end training. allows method adapt statistical patterns real indoor scenes without need explicit supervision mapping stage. planner uses metric belief world obtained mapping operation described plan paths goal. value iteration planning algorithm crucially trainable differentiable hierarchical version value iteration. three advantages trainable naturally deals partially observed environments explicitly learning explore differentiable enables train mapper navigation hierarchical allows plan paths distant goal locations time complexity logarithmic number steps goal. approach reminiscent classical work navigation also involves building maps planning paths maps reach desired target locations. however approach differs classical work following signiﬁcant except architectural choice maintaining metric belief everything else learned data. leads desirable properties model learn statistical regularities indoor environments task-driven manner jointly training mapper planner makes planner robust errors mapper model used online manner novel environments without requiring pre-constructed map. navigation fundamental problems mobile robotics. standard approach decompose problem separate stages mapping environment planning path constructed decomposing navigation manner allows stage developed independently prevents exploiting speciﬁc needs other. comprehensive survey classical approaches mapping planning found mapping well studied computer vision robotics form structure motion simultaneous localization mapping variety sensing modalities range sensors cameras rgb-d cameras. approaches take purely geometric approach. learning based approaches study problem isolation thus learning generic taskindependent maps. path planning inferred maps also well studied pioneering works canny kavraki lavalle kuffner works studied joint problem mapping planning. relaxes need pre-mapping incrementally updating navigating still treat navigation purely geometric problem konolige aydemir proposed approaches leveraged semantics informed navigation. kuipers introduce cognitive mapping model using hierarchical abstractions maps. semantics also associated environments generally alternative separating discrete mapping planning phases reinforcement learning methods directly learn policies robotic tasks major challenge using task need process complex sensory input camera images. recent works deep reinforcement learning learn policies end-to-end manner going pixels actions. follow-up works propose improvements algorithms study incorporate memory neural network based models. build work tamar study explicit planning incorporated agents consider case ﬁrst-person visual navigation provide framework memory mapping. study generalization behavior algorithms novel environments figure architecture mapper mapper module processes ﬁrst person images robot integrates observations latent memory corresponds egocentric top-view environment. mapping operation supervised explicitly mapper free write memory whatever information useful planner. addition ﬁlling obstacles mapper also stores conﬁdence values allows make probabilistic predictions unobserved parts exploiting learned patterns. context navigation learning used obtain policies works focus problem learning controllers effectively maneuvering around obstacles directly sensor data. others focus planning problem associated navigation full state information designing strategies faster learning episodic control incorporate memory algorithms ease generalization environments. research focuses navigation synthetic mazes little structure them. given environments randomly generated policy learns random exploration strategy statistical regularities layout exploit. instead test layouts obtained real buildings show architecture consistently outperforms feed forward lstm models used prior work. research directly relevant work contemporary work similar also study ﬁrst-person view navigation using macroactions realistic environments instead synthetic mazes. propose feed forward model trained environment ﬁnetuned another environment. memory-less agent cannot plan explore environment expressive model naturally does. also don’t consider zero-shot generalization previously unseen environments focus smaller worlds memorization landmarks feasible. contrast explicitly handle generalization never seen interiors show model generalizes successfully ﬂoor plans seen training. relationship contemporary work. since conducting research numerous works study visual navigation come out. notable among work sadeghi levine shows simulated mobility policies transfer real world. mirowski study sources auxiliary supervision faster training bhatti incorporate slam based maps improving performance playing doom. brahmbhatt hays study navigation cities using feed-forward models. zhang duan show speed learning related tasks. show results synthetic mazes show results real images. works study visual navigation none leverage mapping planning modules propose end-to-end architectures jointly mapping planning focus work. able focus high-level mapping planning problem remove confounding factors arising low-level control conducting experiments simulated real world indoor environments. studying problem simulation makes easier exhaustive evaluation experiments scanned real world environments allows retains richness complexity real scenes. also study static version problem though extensions dynamic environments would interesting explore future work. ﬁxed pitch. robot equipped low-level controllers provide relatively high-level macro-actions axθ. macro-actions stay place rotate left rotate right move forward denoted respectively. assume environment grid world robot uses macro-actions move nodes graph. robot also access precise egomotion. amounts assuming perfect visual odometry learned defer joint learning problem future work. want learn policies robot navigating novel environments previously encountered. study navigation tasks geometric task robot required target location speciﬁed robot’s coordinate frame semantic task robot required object interest tasks performed novel environments neither exact environment topology available robot. navigation problem deﬁned follows. given time step assume robot global position time step robot receives input image environment target location speciﬁed coordinate frame robot. navigation problem learn policy every time steps uses inputs output action convey robot target quickly possible. experimental testbed. conduct experiments stanford large-scale indoor spaces dataset introduced armeni dataset consists scans collected largescale indoor areas originate different buildings educational ofﬁce use. dataset collected using matterport scanner scans buildings used training agents tested scans building. pre-processed meshes compute space traversable robot. also precompute directed graph consisting locations robot visit nodes connectivity structure based actions available robot efﬁciently generate training problems. details section describe mapping portion learned network integrate ﬁrst-person camera images topd representation environment learning leverage statistical structure world. note that unlike analytic mapping systems model amounts latent representation. since directly learned planning module need encode purely free space representations instead function general spatial memory. model learns store inside whatever information useful generating successful plans. however make description section concrete assume mapper predicts free space. mapper architecture illustrated figure every time step maintain cumulative estimate free space coordinate frame robot. represented multi-channel feature metrically represents space top-down view world. estimated current image cumulative estimate previous time step egomotion last step using following update rule here function transforms free space prediction previous time step according egomotion last step function takes input current image outputs estimate free space based view environment current location function accumulates free space prediction current view accumulated prediction previous time steps. next describe functions realized. function realized using bi-linear sampling. given ego-motion compute backward ﬁeld backward maps pixel current free space image location previous free space image come from. backward analytically computed ego-motion section function uses bi-linear sampling apply ﬁeld free space estimate previous frame. bi-linear sampling allows back-propagate gradients make possible train model end. function realized convolutional neural network. choice represent free space always coordinate frame robot becomes relatively easy function learn given network output free space current coordinate rather arbitrary world coordinate frame determined cumulative egomotion robot far. intuitively network semantic cues alongside learned priors size shapes common objects generate free space estimates even object partiality visible. qualitative results section show example proposed mapper able make predictions spaces haven’t observed. figure architecture hierarchical planner hierarchical planner takes egocentric multi-scale belief world output mapper uses value iteration expressed convolutions channel-wise max-pooling output policy. planner trainable differentiable backpropagates gradients mapper. planner operates multiple scales problem leads efﬁciency planning. lutional encoder uses residual connections produces representation scene image space. representation transformed egocentric top-down view fully connected layers. representation up-sampled using up-convolutional layers obtain update belief world current frame. addition producing estimate free space current view model also produces conﬁdence estimate also warped warping function accumulated time estimate allows simplify update function thought playing role update gate gated recurrent unit. update function takes tuples produces follows ft−ct− mapper performance isolation. demonstrate proposed mapper architecture works test isolation task free space prediction. section shows qualitative quantitative results. planner based value iteration networks proposed tamar observed particular type planning algorithm called value iteration implemented neural network alternating convolutions channel-wise pooling operations allowing planner differentiated respect inputs. value iteration thought generalization dijkstra’s algorithm value state iteratively recalculated iteration taking values neighbors plus reward transition neighboring states. plays nicely grid world navigation problems operations implemented small kernels followed max-pooling channels. tamar also showed reformulation value iteration also used learn planner providing supervision optimal action state. thus planning done trainable differentiable manner deep convolutional network problem mapper produces top-view world shares grid world structure described above value iteration networks trainable differentiable planner. hierarchical planning. value iteration networks presented impractical longhorizon planning problem. planning step size coupled action step size thus leading high computational complexity time hard learning problem gradients back many steps. alleviate problem extend hierarchical version presented hierarchical planner plans multiple spatial scales. start times spatially downsampled environment conduct value iterations downsampled environment. output value iteration process center cropped upsampled used value iterations ﬁner scale. process repeated ﬁnally reach resolution original problem. procedure allows planning partially observed environments. value iteration networks evaluated environment fully observed i.e. entire known planning. however navigation problem partially observed. planner hand speciﬁed learned data learn policies naturally take partially observed maps account. note mapper produces belief world also uncertainty planner knows parts haven’t observed. ﬁnal architecture cognitive mapping planning puts together mapper planner described above. time step mapper updates multi-scale belief world based current observation. updated belief input planner outputs action take. described previously parts network differentiable allow end-to-end training additional direct supervision used train mapping module rather producing maps match ground truth free space mapper produces maps allow planner choose effective actions. training procedure. optimize network fully supervised training using dagger generate training trajectories sampling arbitrary start goal locations graph gxθ. generate supervision training computing shortest paths graph. online version dagger episode sample next state based action agent’s current policy expert policy. scheduled sampling anneal probability sampling expert policy using inverse sigmoid decay. note focus work studying different architectures navigation. proposed architecture also trained alternate paradigms learning policies reinforcement learning. chose dagger training models found signiﬁcantly sample efﬁcient stable domain allowing focus architecture design. models trained asynchronously parallel workers parameter servers using tensorflow used adam optimize loss function trained iterations learning rate dropped factor every iterations different runs). weight decay regularize network batch-norm resnet- pre-trained imagenet represent images. transfer supervision images depth images using cross modal distillation rgb-d image pairs rendered meshes training obtain pre-trained resnet- model represent depth images. compare proposed architecture alternate architectures reactive agent lstm based agent. since goal paper study various architectures navigation train architectures using dagger described earlier. geometric task. ﬁrst present results task goal speciﬁed geometrically terms position goal robot’s coordinate frame table figure problems task generated ﬁrst sampling start node graph sampling node within steps starting node preferably another room hallway sampling process used training testing. sample problems testing remain ﬁxed across different algorithms compare. measure performance using distance goal running learned policy episode length report multiple error metrics mean distance goal percentile distance goal success rate table reports metrics episode figure plots across time steps. report numbers test set. test consists ﬂoor altogether different building contained training set. figure report mean distance goal percentile distance goal success rate function number steps frame reactive agent lstm based agent proposed based agent using images input using depth images input note outperforms baselines cases generally using depth images input leads better performance using images input. also show variance performance re-trainings different random initializations agents using depth images input note variation performance reasonably small models consistently outperforms baseline. figure representative success failure cases visualize trajectories typical success failure cases cmp. dark gray regions show occupied space light gray regions show free space. agent starts blue required reach green star agent’s trajectory shown dotted line. visualize trajectories view note agent receives ﬁrst person view input. left plots show success cases geometric task. agent able traverse large distances across multiple rooms target location around obstacles quickly resolve needs head next room current room. last plots show cases agent successfully backtracks. center plots show failure cases geometric task problems navigating around tight spaces missing openings would lead shorter paths thrashing around space without making progress. right plots visualize trajectories chair’ semantic task. ﬁrst ﬁgure shows success case right ﬁgure shows typical failure case agent walks right chair region. nearest neighbor trajectory transfer quantify similarity training testing environments transfer optimal trajectories train test using visual nearest neighbors transfer done follows. time step pick location training results similar view seen agent current time step. compute optimal action conveys robot relative offset training environment location execute action current time step. procedure repeated time step. transfer leads poor results. mean median distance goal steps respectively highlighting differences train test environments. image goal location lstm refers experimental setting ignore image simply relative goal location input lstm predict action agent take. relative goal location embedded dimensional space fully connected layers relu non-linearities input lstm. expected rather poorly. described resnet- extract features. features passed fully connected layers combined representation relative goal location used predict ﬁnal action. experimented additive multiplicative combination strategies performed similarly. note reactive baseline able perform well training environments obtaining mean distance goal steps perform poorly test able within steps goal average. suggests reactive agent able effectively memorize environments trained fails generalize novel environments surprising given form memory allow plan. also experimented using drop fully connected layers model found hurt performance train test sets. reactive policy multiple frames also consider case reactive policy receives previous frames addition current view. given robot’s step-size fairly large consider late fusion architecture fuse information extracted resnet-. note architecture similar used primary differences goal speciﬁed terms relative offset training uses dagger instead testing done novel environments. adaptations necessary make interpretable comparison task. using additional frames input leads large improvement performance specially using depth images. lstm based agent finally also compare agent uses lstm based memory. introduce lstm units multiplicatively combined image relative goal location representation. architecture also gives lstm access egomotion agent thus model access information method uses. also experimented lstm based models weren’t able reliably train early experiments pursue further. lstm based model able consistently outperform reactive baseline. compare baselines proposed method. able outperform baselines across metrics depth image case. achieves lower %ile distance goal improves success rate also report variance performance retrainings different random initializations network competitive methods depth image case. figure shows performance solid line shows median metric value surrounding shaded region represents minimum maximum metric value re-trainings. variation performance reasonably small models leads signiﬁcant improvements. ablations. also studied ablated versions proposed method. summarize takeaways learned mapper leads better navigation performance analytic mapper planning crucial single-scale planning works slightly better multi-scale planning cost increased planning cost. details section additional comparisons lstm cmp. also conducted additional experiments compare performance lstm baseline model competitive scenario methods depth images. summarize conclusions provide details section studied performance changes target much away larger performance lstm test scenarios. also compared performance lstm problems different difﬁculty observed generally better across values hardness images particularly better cases high hardness. also evaluate well models generalize trained single scene transferring across datasets. smaller drop performance compared lstm. details section figure visualizes discusses representative success failure cases video examples available project website. semantic task. present experiments target speciﬁed semantically. consider three tasks chair’ door’ table’. agent receives one-hot vector indicating object category must considered successful reach instance indicated object category. object annotations sdis dataset label nodes graph object categories. note done generate supervision optimal actions during training evaluate performance agents test time. supervision used agent must learn appearance models chairs jointly policy reach them. initialize agent within time steps least instance indicated category train towards nearest instance. table reports average %ile distance nearest category instance success rate after executing ﬁxed number steps across tasks three categories. compare method best performing reactive lstm based baseline models geometric navigation task. challenging task specially agent start location desired object visible must learn explore environment desired object. able achieve higher success rate baselines. figure shows sample trajectories task cmp. category performance analysis presented section performance currently hindered limited ability network recognizing objects incorporating stronger appearance models boost performance. visualizations. also analyzed mapper representation value maps learned networks. found mapper representation capture free space value maps indicative agent’s behavior. details section acknowledgments thank shubham tulsiani david fouhey christian h¨ane useful discussion feedback manuscript well marek fiser sergio guadarrama help google infrastructure tensorﬂow. this lstm impoverished longer receives egomotion agent input experiment lstm model received egomotion input weren’t able train initial experiments.", "year": 2017}