{"title": "An Overview of Multi-Task Learning in Deep Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.", "text": "multi-task learning successes many applications machine learning natural language processing speech recognition computer vision drug discovery. article aims give general overview particularly deep neural networks. introduces common methods deep learning gives overview literature discusses recent advances. particular seeks help practitioners apply shedding light works providing guidelines choosing appropriate auxiliary tasks. machine learning typically care optimizing particular metric whether score certain benchmark business kpi. order this generally train single model ensemble models perform desired task. ﬁne-tune tweak models performance longer increases. generally achieve acceptable performance laser-focused single task ignore information might help even better metric care about. speciﬁcally information comes training signals related tasks. sharing representations related tasks enable model generalize better original task. approach called multi-task learning multi-task learning used successfully across applications machine learning natural language processing speech recognition computer vision drug discovery comes many guises joint learning learning learn learning auxiliary tasks names used refer generally soon optimizing loss function effectively multi-task learning scenarios helps think trying explicitly terms draw insights even optimizing loss typical case chances auxiliary task help improve upon main task. summarizes goal succinctly improves generalization leveraging domain-speciﬁc information contained training signals related tasks\". course article give general overview current state multi-task learning particular comes deep neural networks. ﬁrst motivate different perspectives section introduce frequently employed methods deep learning section subsequently section describe ∗this paper originally appeared blog post http//sebastianruder.com/multi-task/index. mechanisms together illustrate works practice. looking advanced neural network-based methods provide context section discussing literature mtl. introduce powerful recently proposed methods deep neural networks section finally talk commonly used types auxiliary tasks discuss makes good auxiliary task section motivate multi-task learning different ways biologically multi-task learning inspired human learning. learning tasks often apply knowledge acquired learning related tasks. instance baby ﬁrst learns recognize faces apply knowledge recognize objects. pedagogical perspective often learn tasks ﬁrst provide necessary skills master complex techniques. true learning proper falling martial arts e.g. judo much learning program. taking example culture also consider karate movie sensei miyagi teaches karate seemingly unrelated tasks sanding ﬂoor waxing car. hindsight these however turn equip invaluable skills relevant learning karate. finally motivate multi-task learning machine learning point view view multi-task learning form inductive transfer. inductive transfer help improve model introducing inductive bias causes model prefer hypotheses others. instance common form inductive bias regularization leads preference sparse solutions. case inductive bias provided auxiliary tasks cause model prefer hypotheses explain task. shortly generally leads solutions generalize better. focused theoretical motivations mtl. make ideas concrete look commonly used ways perform multi-task learning deep neural networks. context deep learning multi-task learning typically done either hard soft parameter sharing hidden layers. hard parameter sharing commonly used approach neural networks goes back generally applied sharing hidden layers tasks keeping several task-speciﬁc output layers seen figure hard parameter sharing greatly reduces risk overﬁtting. fact showed risk overﬁtting shared parameters order number tasks smaller overﬁtting task-speciﬁc parameters i.e. output layers. makes sense intuitively tasks learning simultaneously model representation captures tasks less chance overﬁtting original task. soft parameter sharing hand task model parameters. distance parameters model regularized order encourage parameters similar evidenced figure instance distance regularization trace norm. even though inductive bias obtained multi-task learning seems intuitively plausible order understand better need look mechanisms underlie ﬁrst proposed examples assume related tasks rely common hidden layer representation effectively increases sample size using training model. tasks least somewhat noisy training model task learn good representation task ideally ignores data-dependent noise generalizes well. different tasks different noise patterns model learns tasks simultaneously able learn general representation. learning task bears risk overﬁtting task learning jointly enables model obtain better representation averaging noise patterns. task noisy data limited high-dimensional difﬁcult model differentiate relevant irrelevant features. help model focus attention features actually matter tasks provide additional evidence relevance irrelevance features. features easy learn task difﬁcult learn another task might either interacts features complex features impeding model’s ability learn allow model eavesdrop i.e. learn task easiest hints i.e. directly training model predict important features. biases model prefer representations tasks also prefer. also help model generalize tasks future hypothesis space performs well sufﬁciently large number training tasks also perform well learning novel tasks long environment finally acts regularizer introducing inductive bias. such reduces risk overﬁtting well rademacher complexity model i.e. ability random noise. order better understand deep neural networks look existing literature linear models kernel methods bayesian algorithms. particular discuss main ideas pervasive throughout history multi-task learning enforcing sparsity across tasks norm regularization; modelling relationships tasks. note many approaches literature deal homogenous setting assume tasks associated single output e.g. multi-class mnist dataset typically cast binary classiﬁcation tasks. recent approaches deal realistic heterogeneous setting task corresponds unique outputs. notation order better connect following approaches ﬁrst introduce notation. tasks. task model parameters dimensionality write parameters column vector stack column vectors column column form matrix rd×t i-th contains parameter corresponding i-th feature model every task j-th column contains parameters corresponding j-th model. many existing methods make sparsity assumption regard parameters models. assume models share small features. terms task parameter matrix means rows corresponds features used across tasks. order enforce this generalize norm setting. recall norm constraint parameters forces parameters exactly also known lasso single-task setting norm computed based parameter vector respective task compute task parameter matrix order this ﬁrst compute norm across containing parameter corresponding i-th feature across tasks yields vector .adq] compute norm vector forces entries i.e. rows depending constraint would like place different general refer mixed-norm constraints norms. also known block-sparse regularization lead entire rows regularization mixed norm. latter also known group lasso ﬁrst proposed also show problem optimizing non-convex group lasso made convex penalizing trace norm forces low-rank thereby constrains column parameter vectors live low-dimensional subspace. furthermore establish upper bounds using group lasso multi-task learning. much block-sparse regularization intuitively plausible dependent extent features shared across tasks. show features overlap much regularization might actually worse element-wise regularization. reason improve upon block-sparse models proposing method combines block-sparse element-wise sparse regularization. decompose task parameter matrix matrices enforced block-sparse using regularization made element-wise sparse using lasso. recently propose distributed version group-sparse regularization. group-sparsity constraint forces model consider features features largely used across tasks. previous approaches thus assume tasks used multi-task learning closely related. however task might closely related available tasks. cases sharing information unrelated task might actually hurt performance phenomenon known negative transfer. rather sparsity would thus like leverage prior knowledge indicating tasks related others not. scenario constraint enforces clustering tasks might appropriate. suggest impose clustering constraint penalizing norms task column vectors well variance following constraint a·t)/t mean parameter vector. penalty enforces clustering task parameter vectors towards mean controlled apply constraint kernel methods equally applicable linear models. similar constraint svms also proposed constraint inspired bayesian methods seeks make models close mean model. svms loss thus trades large margin close mean model. make assumptions underlying cluster regularization explicit formalizing cluster constraint assumption number clusters known advance. decompose penalty three separate norms measure between-cluster variance measures close clusters tc¯ac number tasks c-th cluster measure within-cluster variance gauges compact cluster ωwithin constraint assumes clusters known advance introduce convex relaxation penalty allows learn clusters time. another scenario clusters inherent structure. extend group lasso deal tasks occur tree structure apply tasks graph structures. previous approaches modelling relationship tasks employ norm regularization approaches without regularization ﬁrst ones presented task clustering algorithm using k-nearest neighbour learn common structure multiple related tasks application semi-supervised learning. much work learning task relationships multi-task learning uses bayesian methods propose bayesian neural network multi-task learning placing prior model parameters encourage similar parameters across tasks. extend gaussian processes inferring parameters shared covariance matrix. computationally expensive adopt sparse approximation scheme greedily selects informative examples. also assuming models sampled common prior. place gaussian prior distribution task-speciﬁc layer. order encourage similarity different tasks propose make mean task-dependent introduce clustering tasks using mixture distribution. importantly require task characteristics deﬁne clusters number mixtures speciﬁed advance. building this draw distribution dirichlet process enable model learn similarity tasks well number clusters. share model among tasks cluster. propose hierarchical bayesian model learns latent task hierarchy gp-based regularization extend previous gp-based approach computationally feasible larger settings. approaches focus online multi-task learning setting adapt existing methods approach online setting. also propose extension regularized perceptron encodes task relatedness matrix. different forms regularization bias task relatedness matrix e.g. closeness task characteristic vectors dimension spanned subspace. importantly similar earlier approaches require task characteristics make matrix provided advance. extend previous approach learning task relationship matrix. assume tasks form disjoint groups tasks within group low-dimensional subspace. within group tasks share feature representation whose parameters learned jointly together group assignment matrix using alternating minimization scheme. however total disjointness groups might ideal tasks might still share features helpful prediction. turn allow tasks different groups overlap assuming exist small number latent basis tasks. model parameter vector every actual task linear combination these rk×d matrix containing parameter vectors latent tasks vector containing coefﬁcients linear combination. addition constrain linear combination sparse latent tasks; overlap sparsity patterns tasks controls amount sharing these. finally learn small pool shared hypotheses task single hypothesis. many recent deep learning approaches used multi-task learning either explicitly implicitly part model employ approaches introduced earlier hard soft parameter sharing. contrast papers looked developing better mechanisms deep neural networks. computer vision approaches often share convolutional layers learning taskspeciﬁc fully-connected layers. improve upon models proposing deep relationship networks. addition structure shared task-speciﬁc layers seen figure place matrix priors fully connected layers allow model learn relationship tasks similar bayesian models looked before. approach however still relies pre-deﬁned structure sharing adequate well-studied computer vision problems prove error-prone novel tasks. starting extreme propose bottom-up approach starts thin network dynamically widens greedily training using criterion promotes grouping similar tasks. widening procedure dynamically creates branches seen figure however greedy method might able discover model globally optimal assigning branch exactly task allow model learn complex interactions tasks. start separate model architectures soft parameter sharing. refer cross-stitch units allow model determine task-speciﬁc networks leverage knowledge task learning linear combination output previous layers. architecture seen figure place cross-stitch units pooling fully-connected layers. contrast natural language processing recent work focused ﬁnding better task hierarchies multi-task learning show low-level tasks i.e. tasks typically used preprocessing part-of-speech tagging named entity recognition supervised lower layers used auxiliary task. instead learning structure sharing take orthogonal approach considering uncertainty task. adjust task’s relative weight cost function deriving multi-task loss function based maximizing gaussian likelihood task-dependant uncertainty. architecture per-pixel depth regression semantic instance segmentation seen figure recent work seeks generalize existing approaches deep learning generalize previously discussed matrix factorisation approaches using tensor factorisation split model parameters shared task-speciﬁc parameters every layer. finally propose sluice networks model generalizes deep learningbased approaches hard parameter sharing cross-stitch networks block-sparse regularization approaches well recent approaches create task hierarchy. model seen figure allows learn layers subspaces shared well layers network learned best representations input sequences. surveyed recent approaches brieﬂy summarize draw conclusion share deep models. approaches history focused scenario tasks drawn distribution scenario beneﬁcial sharing always hold. order develop robust models thus able deal unrelated loosely related tasks. early work deep learning pre-speciﬁed layers share task pairing strategy scale heavily biases architectures. hard parameter sharing technique originally proposed still norm years later. useful many scenarios hard parameter sharing quickly breaks tasks closely related require reasoning different levels. recent approaches thus looked towards learning share generally outperform hard parameter sharing. addition giving models capacity learn task hierarchy helpful particularly cases require different granularities. mentioned initially soon optimizing loss function. rather constraining model compress knowledge tasks parameter space thus helpful draw advances discussed enable model learn tasks interact other. natural situations interested obtaining predictions multiple tasks once. scenarios common instance ﬁnance economics forecasting might want predict value many possibly related indicators bioinformatics might want predict symptoms multiple diseases simultaneously. scenarios drug discovery tens hundreds active compounds predicted accuracy increases continuously number tasks using related task auxiliary task classical choice. idea related task present prominent examples. uses tasks predict different characteristics road auxiliary tasks predicting steering direction self-driving car; head pose estimation facial attribute inference auxiliary tasks facial landmark detection; jointly learn query classiﬁcation search; jointly predicts class coordinates object image; ﬁnally jointly predict phoneme duration frequency proﬁle text-to-speech. often labeled data related task unavailable. circumstances however access task opposite want achieve. data leveraged using adversarial loss seek minimize maximize training error using gradient reversal layer. setup found recent success domain adaptation adversarial task case predicting domain input; reversing gradient adversarial task adversarial task loss maximized beneﬁcial main task forces model learn representations cannot distinguish domains. mentioned before used learn features might easy learn using original task. effective achieve hints i.e. predicting features auxiliary task. recent examples strategy context natural language processing predict whether input sentence contains positive negative sentiment word auxiliary tasks sentiment analysis predict whether name present sentence auxiliary task name error detection. similarly auxiliary task used focus attention parts image network might normally ignore. instance learning steer single-task model might typically ignore lane markings make small part image always present. predicting lane markings auxiliary task however forces model learn represent them; knowledge also used main task. analogously facial recognition might learn predict location facial landmarks auxiliary tasks since often distinctive. many tasks training objective quantized i.e. continuous scale might plausible labels available discrete set. case many scenarios require human assessment data gathering predicting risk disease sentiment analysis using less quantized auxiliary tasks might help cases might learned easily objective smoother. scenarios impractical features inputs unhelpful predicting desired objective. however might still able guide learning task. cases features used outputs rather inputs. present several problems applicable. many situations features become available predictions supposed made. instance self-driving cars accurate measurements obstacles lane markings made passing them. also gives example pneumonia prediction results additional medical trials available. examples additional data cannot used features available input runtime. however used auxiliary task impart additional knowledge model training. goal auxiliary task enable model learn representations shared helpful main task. auxiliary tasks discussed implicitly closely related main task learning likely allows model learn beneﬁcial representations. explicit modelling possible instance employing task known enable model learn transferable representations. language modelling objective employed fulﬁls role. similar vein autoencoder objective also used auxiliary task. section discussed different auxiliary tasks used leverage even care task. still know though auxiliary task useful practice. finding auxiliary task largely based assumption auxiliary task related main task helpful predicting main task. however still good notion tasks considered similar related. deﬁnes tasks similar features make decision. argues theoretically related tasks share common optimal hypothesis class i.e. inductive bias. propose tasks f-related data tasks generated ﬁxed probability distribution using transformations allows reason tasks different sensors collect data classiﬁcation problem e.g. object recognition data cameras different angles lighting conditions applicable tasks deal problem. ﬁnally argue tasks similar classiﬁcation boundaries i.e. parameter vectors close. spite early theoretical advances understanding task relatedness made much recent progress towards goal. task similarity binary resides spectrum. allowing models learn share task might allow temporarily circumvent lack theory make better even loosely related tasks. however also need develop principled notion task similarity regard order know tasks prefer. recent work found auxiliary tasks compact uniform label distributions preferable sequence tagging problems conﬁrmed experiments addition gains found likely main tasks quickly plateau non-plateauing auxiliary tasks experiments however limited scope recent ﬁndings provide ﬁrst clues towards deeper understanding multi-task learning neural networks. overview reviewed history literature multi-task learning well recent work deep learning. frequently used -year hard parameter sharing paradigm still pervasive neural-network based mtl. recent advances learning share however promising. time understanding tasks similarity relationship hierarchy beneﬁt still limited need study thoroughly gain better understanding generalization capabilities regard deep neural networks.", "year": 2017}