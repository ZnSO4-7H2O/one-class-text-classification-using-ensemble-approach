{"title": "Stochastic Variance Reduction Methods for Policy Evaluation", "tag": ["cs.LG", "cs.AI", "cs.SY", "math.OC", "stat.ML"], "abstract": "Policy evaluation is a crucial step in many reinforcement-learning procedures, which estimates a value function that predicts states' long-term value under a given policy. In this paper, we focus on policy evaluation with linear function approximation over a fixed dataset. We first transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle point problem, and then present a primal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the problem. These algorithms scale linearly in both sample size and feature dimension. Moreover, they achieve linear convergence even when the saddle-point problem has only strong concavity in the dual variables but no strong convexity in the primal variables. Numerical experiments on benchmark problems demonstrate the effectiveness of our methods.", "text": "policy evaluation concerned estimating value function predicts long-term values states given policy. crucial step many reinforcement-learning algorithms. paper focus policy evaluation linear function approximation ﬁxed dataset. ﬁrst transform empirical policy evaluation problem convex-concave saddle-point problem present primal-dual batch gradient method well stochastic variance reduction methods solving problem. algorithms scale linearly sample size feature dimension. moreover achieve linear convergence even saddle-point problem strong concavity dual variables strong convexity primal variables. numerical experiments benchmark problems demonstrate effectiveness methods. introduction reinforcement learning paradigm sequential decision making agent interacts environment repeatedly observing current state taking action according certain policy receiving reward signal transitioning next state. policy speciﬁes action take given current state. policy evaluation estimates value function predicts expected cumulative reward agent would receive following ﬁxed policy starting certain state. addition quantifying long-term values states interest value functions also provide machine learning department carnegie mellon university pittsburgh pennsylvania usa. microsoft research redmond washington usa.. correspondence simon <ssducs.cmu.edu> jianshu chen <jianshucmicrosoft.com> lihong <lihonglimicrosoft.com> xiao <lin.xiaomicrosoft.com> dengyong zhou <denzhomicrosoft.com>. important information agent optimize policy. example policy-iteration algorithms iterate policy-evaluation steps policy-improvement steps until optimal policy found therefore estimating value function efﬁciently accurately essential substantial work policy evaluation temporal-difference methods perhaps popular. methods bellman equation bootstrap estimation process. different cost functions formulated exploit idea leading different policy evaluation algorithms; dann comprehensive survey. paper study policy evaluation minimizing mean squared projected bellman error linear approximation value function. focus batch setting ﬁxed ﬁnite dataset given. ﬁxed-data setting important itself also important component methods experience replay ﬁnite-data regime makes possible solve policy evaluation efﬁciently recently developed fast optimization methods based stochastic variance reduction svrg saga minimizing strongly convex functions ﬁnite-sum structure methods enjoy computational cost iteration classical stochastic gradient method also achieve fast linear convergence rates however cannot applied directly minimize mspbe whose objective ﬁnite-sum structure. paper overcome obstacle transforming empirical mspbe problem equivalent convex-concave saddle-point problem possesses desired ﬁnite-sum structure. saddle-point problem consider model parameters primal variables coupled dual variables bilinear term. moreover without -regularization model parameters objective strongly concave dual variables primal variables. propose primal-dual batch gradient method well stochastic variance-reduction methods based svrg saga respectively. surprisingly show coupling matrix full rank algorithms achieve linear convergence primal expected reward vector policy deﬁned elementwise eπr; transition matrix induced policy applying deﬁned entrywise mean squared projected bellman error approach scale state space size large inﬁnite linear approximation formally feature model parameter estimated. here want minimizes mean squared projected bellman error mspbe mspbe properly deﬁned described follows. suppose policy settles stationary distribution generates inﬁnite transition sequence current state action reward next state. deﬁnitions respect stationary distribution. many solutions converge minimizer mspbe limit empirical mspbe practice quantities often unknown access ﬁnite dataset transitions replacing unknown statistics ﬁnite-sample estimates obtain empirical mspbe em-mspbe. speciﬁcally dual spaces despite lack strong convexity objective primal variables. results also extend off-policy learning eligibility traces note balamurugan bach extended svrg saga solve convex-concave saddlepoint problems linear-convergence guarantees. main difference results linear convergence balamurugan bach relies assumption objective strongly convex primal variables strongly concave dual. results show somewhat surprisingly necessary primal-dual coupling bilinear coupling matrix full rank. fact aware similar previous results even primal-dual batch gradient method show paper. even strongly convex regularization primal variables introduced mspbe objective algorithms balamurugan bach cannot applied efﬁciently. algorithms require proximal mappings strongly convex concave regularization functions computed efﬁciently. saddle-point formulation strong concavity dual variables comes quadratic function deﬁned feature covariance matrix cannot inverted efﬁciently makes proximal mapping costly compute. instead algorithms gradients hence much efﬁcient. preliminaries consider markov decision process described states actions transition probability state state taking action reward received taking action state discount factor. goal agent actionselection policy long-term reward policy maximized. ease exposition assume ﬁnite none results relies assumption. step many algorithms estimate value function given policy deﬁned γtr|s denote vector constructed stacking values other. unique ﬁxed point bellman operator therefore minimizing em-mspbe equivalent solving saddle-point problem convex primal variable concave dual variable moreover ﬁnite-sum structure similar valcarcel macua independently showed algorithm indeed stochastic gradient method solving saddle-point problem although obtained saddle-point formulation different derivations. recently used conjugate function approach obtain saddle-point formulations general class problems derived primal-dual stochastic gradient algorithms solving them. however algorithms sublinear convergence rates leaves much room improve applied problems ﬁnite datasets. recently lian developed svrg methods general ﬁnite-sum composition optimization achieve linear convergence rate. different methods stochastic gradients biased worse dependency condition numbers fast linear convergence algorithms presented sections requires following assumption deﬁned respectively. thus true statistics non-singular positive deﬁnite enough training samples assumptions usually satisﬁed. widely used previous works gradient-based algorithms complete calculation. method known leastsquares temporal difference lstd expensive large. also skip forming matrices explicitly compute using recusive rank-one updates since rank-one update costs total cost sequel develop efﬁcient algorithms minimize em-mspbe using stochastic variance reduction methods samples update without saddle-point formulation em-mspbe algorithms based stochastic variance reduction techniques developed minimizing ﬁnite convex functions speciﬁcally svrg saga deal problems form ture given thus extending variance reduction techniques em-mspbe minimization straightforward. nevertheless show minimizing emmspbe equivalent solving convex-concave saddlepoint problem actually possesses desired ﬁnitesum structure. proceed resort machinery conjugate functions function conjugate function direct consequence assumption unique minimizer em-mspbe even withstrongly convex regularization however lagrangian strongly concave strongly convex case show non-singularity coupling primal-dual batch gradient method diving stochastic variance reduction algorithms ﬁrst present algorithm primal-dual batch gradient algorithm solving saddlepoint problem step vector obtained stacking primal negative dual gradients notation needed order characterize convergence rate algorithm symmetric positive deﬁnite matrix λmax λmin denote maximum minimum eigenvalues respectively deﬁne condition number λmax/λmin. also deﬁne assumption following theorem proved appendix theorem suppose assumption holds solution step sizes chosen number iterations algorithm achieve upper bounded assigned speciﬁc values step sizes clarity. general similar step sizes keeping ratio roughly constant appendices details. practice parameter search small subset data reasonable step sizes. interesting open problem automatically select adjust step sizes. note linear rate determined parts strongly convex regularization parameter reason even saddle-point problem strong concavity dual variables algorithm still enjoys linear convergence rate. moreover even inefﬁcient solve problem using primal-dual algorithms based proximal mappings strongly convex concave terms reason that strong concavity lagrangian respect dual lies quadratic func stochastic variance reduction methods replace algorithm stochastic gradient recover algorithm sutton applied ﬁxed dataset possibly multiple passes. periteration cost slow sublinear convergence rate. section provide stochastic variance reduction methods show achieve fast linear convergence. algorithm adapted stochastic variance reduction gradient method uses layers loops maintains sets parameters outer loop algorithm computes full gradient using takes computing component gradients iteration algorithm randomly picks index computes stochastic gradient then updates using variance reduced stochastic gradient previously computed stochastic gradient using tm-th sample afterwards updates batch gradient estimate algorithm proceeds different vectors computed using different values general need store vectors facilitate individual updates cost additional storage. however exploiting rank-one structure need store three scalars form using computation. overall iteration saga costs operations. order study convergence properties svrg saga policy evaluation introduce smoothness parameter based stochastic gradients σw/σθ ratio primal dual step-sizes deﬁne pair weighted euclidean norms deﬁnition similar smoothness constant used balamurugan bach except used step-size ratio rather strong convexity concavity parameters lagrangian deﬁne substituting deﬁnition here contains stochastic gradients computed using random sample index term used reduce variance keeping unbiased estimate since computed iteration outer loop cost iterations inner loop cost operations total computational cost outer loop present overall complexity analysis algorithm section second stochastic variance reduction method policy evaluation adapted saga algorithm uses single loop maintains single parameters algorithm starts ﬁrst gence svrg saga balamurugan bach requires lagrangian strongly convex strongly concave moreover policy evaluation problem strong concavity respect dual variable comes efﬁcient proximal mapping required proximal versions svrg saga balamurugan bach algorithms require computing stochastic gradients function easy ﬁnite structure. balamurugan bach also proposed accelerated variants svrg saga using catalyst framework extensions done similarly three algorithms presented paper omit details space limit. comparison different algorithms section compares computation complexities several representative policy-evaluation algorithms minimize em-mspbe summarized table upper part table lists algorithms whose complexity linear feature dimension including algorithms presented previous section. also apply ﬁnite dataset samples drawn uniformly random replacement. costs iteration sublinear convergence rate regarding practice people choose generalization reasons leading ond) overall complexity condition number related algorithm. however veriﬁed experiments bounds table show svrg/saga-based algorithms much faster effective condition numbers vanish becomes large. similar complexity gtd. table list different implementations pdbg. pdbg- computes gradients averaging stochastic gradients entire dataset iteration costs operations; discussions gradient iteration operations. large pdbg- would advantage pdbg-. lower part table also includes lstd complexity rankone updates used. svrg saga efﬁcient algorithms either large. particular lower complexity lstd condition easy satisfy large. hand svrg saga algorithms efﬁcient pdbg- large convergence slow number iterations required reach desired accuracy grows worse. algorithm uses similar idea svrg reduce variance updates. algorithm shown similar linear convergence rate online setting data stream generated markov process ﬁnite states exponential mixing. method solves ﬁxed-point solution stochastic approximation. result non-convergent off-policy learning algorithms remain stable cases want estimate value function policy data generated different behavior policy called off-policy learning off-policy case samples generated distribution induced behavior policy target policy mismatch often causes stochastic-approximation-based methods diverge gradient-based algorithms remain convergent convergence rate. consider framework outlined section state-action pair deﬁne importance ratio π/πb. emmspbe off-policy learning expression except modiﬁed weight factor listed table also related discussion.) algorithms remain off-policy case modiﬁed correspondingly. eligibility traces useful technique trade bias variance learning used pre-compute table running algorithms. note em-mspbe eligibility traces form deﬁned differently according last table m-th step learning process algorithm randomly samples ﬁxed dataset computes corresponding stochastic gradients index uniformly distributed independent different values algorithms immediately work case enjoying similar linear convergence rate computation complexity linear need additional operations pre-compute recursively additional storage however change order total complexity svrg/saga. experiments section compare following algorithms benchmark problems pdbg samples drawn randomly replacement flstd-sa algorithm dataset; prashanth svrg saga note solution em-mspbe minimizer differ include step size tuning chosen chosen report results algorithm correspond best-tuned step sizes; svrg choose ﬁrst task consider randomly generated states actions transition probabilities deﬁned data-generating policy start distribution generated similar way. state represented -dimensional feature vector features sampled uniform distribution last feature constant one. chose fig. shows performance various algorithms first notice stochastic variance methods converge much faster others. fact proposed methods achieve linear convergence. second increase performances pdbg svrg saga improve signiﬁcantly better conditioning predicted theoretical results. next test algorithms mountain collect dataset ﬁrst sarsa cmac features obtain good policy. then policy collect trajectories comprise dataset. figs. show proposed stochastic variance reduction methods dominate ﬁrst-order methods. moreover better conditioning pdbg svrg saga achieve faster convergence rate. finally increase sample size svrg saga converge faster. simulation veriﬁes conclusions paper reformulated em-mspbe minimization problem policy evaluation empirical saddlepoint problem developed analyzed batch gradient method ﬁrst-order stochastic variance reduction methods solve problem. important result obtained even reformulated saddle-point problem lacks strong convexity primal variables strong concavity dual variables proposed algorithms still able achieve linear convergence rate. aware similar results primal-dual batch gradient methods stochastic variance reduction methods. furthermore showed feature dimension number samples large developed stochastic variance reduction methods efﬁcient gradient-based methods convergent off-policy settings. work leads several interesting directions research. first believe important extend stochastic variance reduction methods nonlinear approximation paradigms especially deep neural networks. moreover remains important open problem apply stochastic variance reduction techniques policy optimization. references balamurugan bach francis. stochastic variance reduction methods saddle-point problems. advances neural information processing systems bhatnagar shalabh precup doina silver david sutton richard maei hamid szepesv´ari csaba. convergent temporal-difference learning arbitrary smooth function approximation. advances neural information processing systems defazio aaron bach francis lacoste-julien simon. saga fast incremental gradient method support adnon-strongly convex composite objectives. vances neural information processing systems geramifard alborz bowling michael zinkevich martin sutton richard ilstd eligibility traces convergence analysis. advances neural information processing systems johnson zhang tong. accelerating stochastic gradient descent using predictive variance reduction. advances neural information processing systems kearns michael singh satinder bias-variance proerror bounds temporal difference updates. ceedings thirteenth annual conference computational learning theory korda nathaniel prashanth l.a. function approximation concentration bounds centered variant exponential convergence. proceedings thirty-second international conference machine learning lange sascha gabel thomas riedmiller martin. batch reinforcement learning. wiering marco otterlo martijn reinforcement learning state springer verlag lazaric alessandro ghavamzadeh mohammad munos r´emi. finite-sample analysis lstd. proceedings twenty-seventh international conference machine learning lian xiangru wang mengdi finite-sum composition optimization variance reduced gradient proceedings artiﬁcial intelligence descent. statistics conference ghavamzadeh mohammad mahadevan sridhar petrik marek. finite-sample analysis proc. proximal gradient algorithms. conf. uncertainty artiﬁcial intelligence amsterdam netherlands nedi´c bertsekas dimitri least squares policy evaluation algorithms linear function approximation. discrete event dynamics systems theory applications prashanth korda nathaniel munos r´emi. fast lstd using stochastic approximation finite time analysis application trafﬁc control. joint european precup doina sutton richard dasgupta sanjoy. off-policy temporal-difference learning funtion approceedings eighteenth conproximation. ference machine learning shen shu-qian huang ting-zhu cheng guanghui. condition nonsymmetric saddle point matrix diagonalizable real positive eigenvalues. journal computational applied mathematics sutton richard maei hamid szepesv´ari csaba. convergent temporal-difference algorithm off-policy learning linear function approximation. advances neural information processing systems sutton richard maei hamid reza precup doina bhatnagar shalabh silver david szepesv´ari csaba wiewiora eric. fast gradient-descent methods temporal-difference learning linear function approximation. proceedings annual international conference machine learning valcarcel macua sergio chen jianshu zazo santiago sayed distributed policy evaluation multiple behavior strategies. automatic control ieee transactions critical analyzing convergence pdbg saga svrg algorithms policy evaluation. σw/σθ ratio dual primal step sizes algorithms. convenience following notation diagonal matrix whose diagonal entries eigenvalues consists eigenvectors columns. goal bound condition number matrix analysis inspired liesen parlett core following fundamental result linear algebra. theorem suppose diagonalizable. symmetric positive definite matrix symmetric exist complete eigenvectors orthonormal respect inner product induced exactly matrix deﬁned analyzed section choose sufﬁciently large diagonalizable eigenvalues real positive. case matrix eigenvectors eigenvalue decomposition qλq− potential function algorithm outer loop inner loop. index outer iteration inner iteration. fixing outer loop index look inner loop algorithm similar full gradient method ﬁrst simplify dynamics svrg. linear convergence extra factor remark potential function intrinsic geometric interpretation. view column vectors basis vector space orthogonal. goal show coordinate system distance optimal solution shrinks every iteration. proceed bound growth unlike analysis batch gradient methods nonorthogonality eigenvectors lead additional dependency iteration complexity condition number give bound multiplying sides eqn. taking squared -norm taking expectation obtain step used facts independent cross terms zero step used independence variance random variable less second moment step used deﬁnition bound last term inequality simple notation q−gtj λmaxδt results imply number outer iterations needed log. outer iteration svrg algorithm need operations compute full gradient operator min) inner iterations costing operations. therefore overall computational cost", "year": 2017}