{"title": "Spectral Networks and Locally Connected Networks on Graphs", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.", "text": "convolutional neural networks extremely efﬁcient architectures image audio recognition tasks thanks ability exploit local translational invariance signal classes domain. paper consider possible generalizations cnns signals deﬁned general domains without action translation group. particular propose constructions based upon hierarchical clustering domain another based spectrum graph laplacian. show experiments lowdimensional graphs possible learn convolutional layers number parameters independent input size resulting efﬁcient deep architectures. convolutional neural networks extremely succesful machine learning problems coordinates underlying data representation grid structure data studied coordinates translational equivariance/invariance respect grid. speech images video prominent examples fall category. input coordinates grid dimensions fully connected layer outputs requires parameters typical operating regimes amounts complexity parameters. using arbitrary ﬁlters instead generic fully connected layers reduces complexity parameters feature using metric structure building locally connected using together gives parameters number feature maps support ﬁlters result learning complexity independent finally using multiscale dyadic clustering allows succesive layer factor less coordinates ﬁlter. many contexts however faced data deﬁned coordinates lack some geometrical properties. instance data deﬁned meshes surface tension temperature measurements network meteorological stations data coming social networks collaborative ﬁltering examples structured inputs cannot apply standard convolutional networks. another relevant example intermediate representation arising deep neural networks. although spatial convolutional structure exploited several layers typical architectures assume geometry feature dimension resulting tensors convolutional along spatial coordinates. graphs offer natural framework generalize low-dimensional grid structure extension notion convolution. work discuss constructions deep neural networks graphs regular grids. propose different constructions. ﬁrst show extend properties general graphs deﬁne locally connected pooling layers require parameters instead term spatial construction. construction call spectral construction draws properties convolutions fourier domain. convolutions linear operators diagonalised fourier basis extend convolutions general graphs ﬁnding corresponding fourier basis. equivalence given graph laplacian operator provides harmonic analysis graphs spectral construction needs paramters feature also enables construction number parameters independent input dimension constructions allow efﬁcient forward propagation applied datasets large number coordinates. immediate generalisation general graphs consider multiscale hierarchical local receptive ﬁelds suggested purpose grid replaced weighted graph discrete size symmetric nonnegative matrix. notion locality generalized easily context graph. indeed weights graph determine notion locality. example straightforward deﬁne neighborhoods threshold take neighborhoods restrict attention sparse ﬁlters receptive ﬁelds given neighborhoods locally connected networks thus reducing number parameters ﬁlter layer average neighborhood size. cnns reduce size grid pooling subsampling layers. layers possible natural multiscale clustering grid input feature maps cluster output single feature cluster. grid dyadic clustering behaves nicely respect metric laplacian large literature forming multiscale clusterings graphs example finding spatial construction starts multiscale clustering graph similarly consider scales. deﬁne partition clusters; collection neighborhoods around element hand deﬁne k-th layer network. assume without loss generality input signal real signal deﬁned denote number ﬁlters created layer layer network transform fk−-dimensional signal indexed fk-dimensional signal indexed thus trading-off spatial resolution newly created feature coordinates. formally input layer output deﬁned fkij sparse matrix nonzero entries locations given outputs result pooling operation cluster construcion illustrated figure current code build following construction figure spatial construction described illustration purposes pooling operation assimilated ﬁltering stage. layer transformation loses spatial resolution increases number ﬁlters. amongst many strategies perform found covering hierarchicial agglomerative clustering. larger account problem refer reader average support neighborhoods verify number parameters learn layer practice ·|ωk| α·|ωk−| oversampling factor typically spatial construction might appear na¨ıve advantage requires relatively weak regularity assumptions graph. graphs intrinsic dimension localized neighborhoods even nice global embedding exists. however construction easy induce weight sharing across different locations graph. possible option consider global embedding graph dimensional space rare practice high-dimensional data. harmonic analysis weighted graphs combinatorial laplacian graph laplacian d−/w generalizations laplacian grid; frequency smoothness relative interrelated operators simplicity combinatorial laplacian. m-dimensional vector natural deﬁnition smoothness functional ||∇x|| eigenvector eigenvalues allow smoothness vector read coefﬁcients equivalently fourier coefﬁcients signal deﬁned grid. thus case grid eigenvectors laplacian fourier vectors diagonal operators spectrum laplacian modulate smoothness operands. moreover using diagonal operators reduces number parameters ﬁlter section weighted graph index denoted eigenvectors graph laplacian ordered eigenvalue. given weighted graph generalize convolutional operating spectrum weights given eigenvectors graph laplacian. fkij diagonal matrix before real valued nonlinearity. often ﬁrst eigenvectors laplacian useful practice carry smooth geometry graph. cutoff frequency depends upon intrinsic regularity graph also sample size. case replace obtained keeping ﬁrst columns graph underlying group invariance construction discover best example standard cnn; however many cases graph group structure group structure commute laplacian cannot think ﬁlter passing template across recording correlation template location. homogenous allows make sense shall example section assuming eigenvectors laplacian kept equation shows layer requires paramters train. shall section global local regularity graph combined produce layers parameters i.e. number learnable parameters depend upon size input. construction suffer fact graphs meaningful eigenvectors spectrum. even individual high frequency eigenvectors meaningful cohort high frequency eigenvectors contain meaningful information. however construction able access information nearly diagonal highest frequencies. finally obvious either forwardprop backprop efﬁciently applying nonlinearity space side make expensive multiplications obvious standard nonlinearities spectral side. however diagonal operators laplacian simply scale principal components seem trivial well known principal components images ﬁxed size correspond discrete cosine transform basis organized frequency. explained noticing images translation invariant hence covariance operator correlated away pixels. results principal components covariance essentially ordered high frequencies consistent standard group structure fourier basis. upshot that applied natural images construction using covariance similarity kernel recovers standard convolutional network without prior knowledge. indeed linear operators fijv previous argument diagonal fourier basis hence translation invariant hence classic convolutions. moreover section explains spatial subsampling also obtained dropping last part spectrum laplacian leading max-pooling ultimately deep convolutonal networks. standard grid need parameter fourier function ﬁlters compactly supported space ﬁlter requires parameter eigenvector acts. even ﬁlters compactly supported space construction still would less parameters ﬁlter spatial response would different location. possibility getting around generalize duality grid. euclidian grid decay function spatial domain translated smoothness fourier domain viceversa. results funtion spatially localized smooth frequency response case eigenvectors laplacian thought arranged grid isomorphic original spatial grid. suggests that order learn layer features shared across locations also well localized original domain learn spectral multipliers smooth. smoothness prescribed learning subsampled frequency multipliers using interpolation kernel obtain rest cubic splines. however notion smoothness requires geometry domain spectral coordinates obtained deﬁning dual particularly simple navie choice consists choosing -dimensional arrangement obtained ordering eigenvectors according eigenvalues. setting diagonal ﬁlter fkij parametrized ﬁxed cubic spline kernel αkij spline coefﬁcients. seeks ﬁlters constant spatial support follows choose sampling step spectral domain results constant number coefﬁcients αkij ﬁlter. although results section seem indicate arrangement given spectrum laplacian efﬁcient creating spatially localized ﬁlters fundamental question deﬁne dual graph capturing geometry spectral coordinates. possible algorithmic stategy consider input distribution consisting spatially localized signals construct could measured instance e|))t large literature building wavelets graphs example wavelet basis grid language neural networks linear autoencoder certain provable regularity properties forward propagation classical wavelet transform strongly resembles forward propagation neural network except ﬁlter layer output layer kept rather output ﬁnal layer. classically ﬁlter learned constructed facilitate regularity proofs. graph case goal same; except smoothness grid replaced smoothness graph. classical case works tried construct wavelets explicitly based graph corresponding autencoder correct sparsity properties. work recent work ﬁlters constrained construction regularity properties wavelets also trained appropriate task separate smoothness graph. whereas still builds linear autoencoder keeps basic wavelet transform setup work focuses nonlinear constructions; particular tries build analogues cnn’s. another line work rellevant present work discovering grid topologies data. authors empirically conﬁrm statements section showing recover grid structure second order statistics. authors estimate similarities features construct locally connected networks. could improve constructions extent unify them multiscale clustering graph plays nicely laplacian. mentioned before case grid standard dyadic cubes property subsampling fourier functions grid coarser grid ﬁnding fourier functions coarser grid. property would eliminate annoying necessity mapping spectral construction ﬁnest grid layer nonlinearity; would allow interpret local ﬁlters deeper layers spatial construction frequency. kind clustering underpinning multigrid method solving discretized pde’s several papers extending multigrid method particular multiscale clustering associated multigrid method settings general regular grids example situations paper algebraic multigrid method general. work simplicity naive multiscale clustering space side construction guaranteed respect original graph’s laplacian explicit spatial clustering spectral construction. previous constructions tested variations mnist data set. ﬁrst subsample normal grid coordinates. coordinates still structure possible standard convolutions. make dataset placing points unit sphere project random mnist images onto points described section ﬁrst apply constructions sections subsampled mnist dataset. figure shows examples resulting input signals figures show hierarchical clustering constructed graph eigenfunctions graph laplacian respectively. performance various graph architectures reported table serve baseline compute standard nearest neighbor classiﬁer performs slightly worse full mnist dataset two-layer fully connected neural network reduces error geometrical structure data exploited graph architectures. local receptive fields adapted graph structure outperform fully connected network. particular layers ﬁltering max-pooling deﬁne network efﬁciently aggregates information ﬁnal classiﬁer. spectral construction performs slightly worse dataset. considered frequency cutoff however frequency smoothing architecture described section contains smallest number parameters outperforms regular spectral construction. results interpreted follows. mnist digits characterized localized oriented strokes require measurements good spatial localization. locally receptive ﬁelds constructed explicitly satisfy constraint whereas spectral construction measurements enforced become spatially localized. adding smoothness constraint spectrum ﬁlters improves classiﬁcation results since ﬁlters enforced better spatial localization. fact illustrated figure verify locally receptive ﬁelds encode different templates across different spatial neighborhoods since global strucutre tying together. hand spectral constructions capacity generate local measurements generalize across graph. spectral multipliers constrained resulting ﬁlters tend spatially delocalized shown panels corresponds fundamental limitation fourier analysis encode local phenomena. however observe panels simple smoothing across spectrum graph laplacian restores form spatial localization creates ﬁlters generalize across different spatial positions expected convolution operators. test section graph constructions another low-dimensional graph. case lift mnist digits sphere. dataset constructed follows. ﬁrst sample random points {sj}j≤ unit sphere consider orthogonal basis random covariance operator gaussian matrix variance signal original mnist dataset sample covariance operator former distribution consider basis basis deﬁnes point view in-plane rotation table classiﬁcation results mnist subsampled random locations different architectures. stands fully connected layer outputs lrfn denotes locally connected construction section outputs max-pooling layer outputs stands spectral layer section figure subsampled mnist learnt ﬁlters using spatial spectral construction. different receptive ﬁelds encoding feature different clusters. example ﬁlter obtained spectral construction. filters obtained smooth spectral construction. project onto using bicubic interpolation. figure shows examples resulting projected digits. since digits equivalent modulo rotations remove dataset. figure shows eigenvectors graph laplacian. ﬁrst consider mild rotations effect rotations however negligible. indeed table shows nearest neighbor classifer performs considerably worse previous example. neural network architectures considered signiﬁcatively improve basic classiﬁer. furthermore observe convolutional constructions match fully connected constructions less parameters figure displays ﬁlters learnt using different constructions. again verify smooth spectral construction consistently improves performance learns spatially localized ﬁlters even using naive organization eigenvectors detect similar features across different locations graph finally consider uniform rotation case basis random basis case intra-class variability much severe seen inspecting performance nearest neighbor classiﬁer. previously described neural network architectures signiﬁcantly improve classiﬁer although performance notably worse mild rotation scenario. case efﬁcient representation needs fully roto-translation invariant. since non-commutative group likely deeper architectures perform better models considered here. figure filters learnt mnist-sphere dataset using spatial spectral construction. different receptive ﬁelds encoding feature different clusters. example ﬁlter obtained spectral construction. filters obtained smooth spectral construction. using graph-based analogues convolutional architectures greatly reduce number parameters neural network without worsening test error simultaneously giving faster forward propagation. methods scaled data large number coordinates notion locality. much done here. suspect careful training deeper networks consistently improve fully connected networks manifold like graphs like sampled sphere. furthermore intend apply techniques less artiﬁcal problems example netﬂix like recommendation problems biclustering data coordinates. finally fact smoothness naive ordering eigenvectors leads improved results localized ﬁlters suggests possible make dual constructions parameters ﬁlter much generality grid.", "year": 2013}