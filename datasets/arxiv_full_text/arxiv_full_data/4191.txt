{"title": "Deep Reinforcement Learning-based Image Captioning with Embedding Reward", "tag": ["cs.CV", "cs.AI"], "abstract": "Image captioning is a challenging problem owing to the complexity in understanding the image content and diverse ways of describing it in natural language. Recent advances in deep neural networks have substantially improved the performance of this task. Most state-of-the-art approaches follow an encoder-decoder framework, which generates captions using a sequential recurrent prediction model. However, in this paper, we introduce a novel decision-making framework for image captioning. We utilize a \"policy network\" and a \"value network\" to collaboratively generate captions. The policy network serves as a local guidance by providing the confidence of predicting the next word according to the current state. Additionally, the value network serves as a global and lookahead guidance by evaluating all possible extensions of the current state. In essence, it adjusts the goal of predicting the correct words towards the goal of generating captions similar to the ground truth captions. We train both networks using an actor-critic reinforcement learning model, with a novel reward defined by visual-semantic embedding. Extensive experiments and analyses on the Microsoft COCO dataset show that the proposed framework outperforms state-of-the-art approaches across different evaluation metrics.", "text": "figure illustration proposed decision-making framework. lookahead inference procedure utilize policy network value network collaboratively predict word time step. policy network provides action prediction locally predicts next word according current state. value network provides reward prediction globally evaluates possible extensions current state. paper introduce novel decision-making framework image captioning. instead learning sequential recurrent model greedily look next correct word utilize policy network value network jointly determine next best word time step. policy network provides conﬁdence predicting next word according current state serves local guidance. value network evaluates reward value possible extensions current state serves global lookahead guidance. value network adjusts goal predicting correct words towards goal generating captions similar ground truth captions. framework able include good words probability drawn using policy network alone. figure shows example illustrate proposed framework. word holding among choices policy network current image captioning challenging problem owing complexity understanding image content diverse ways describing natural language. recent advances deep neural networks substantially improved performance task. state-of-the-art approaches follow encoder-decoder framework generates captions using sequential recurrent prediction model. however paper introduce novel decision-making framework image captioning. utilize policy network value network collaboratively generate captions. policy network serves local guidance providing conﬁdence predicting next word according current state. additionally value network serves global lookahead guidance evaluating possible extensions current state. essence adjusts goal predicting correct words towards goal generating captions similar ground truth captions. train networks using actor-critic reinforcement learning model novel reward deﬁned visual-semantic embedding. extensive experiments analyses microsoft coco dataset show proposed framework outperforms state-ofthe-art approaches across different evaluation metrics. image captioning task automatically describing content image natural language attracted increasingly interests computer vision. interesting aims endowing machines core human intelligence understand huge amount visual information express natural language. recent state-of-the-art approaches follow encoder-decoder framework generate captions images. generally employ convolutional neural networks encode visual information utilize recurrent neural networks decode inforstep. value network goes forward step state supposing holding generated evaluates good state goal generating good caption end. networks complement able choose word holding. learn policy value networks deep reinforcement learning embedding reward. begin pretraining policy network using standard supervised learning cross entropy loss pretraining value network mean squared loss. then improve policy value networks deep reinforcement learning. reinforcement learning widely used gaming control theory etc. problems control gaming concrete targets optimize nature whereas deﬁning appropriate optimization goal nontrivial image captioning. paper propose train using actor-critic model reward driven visualsemantic embedding visual-semantic embedding provides measure similarity images sentences measure correctness generated captions serve reasonable global target optimize image captioning reinforcement learning. conduct detailed analyses framework understand merits properties. extensive experiments microsoft coco dataset show proposed method outperforms state-of-the-art approaches consistently across different evaluation metrics including bleu meteor rouge cider contributions paper summarized follows present novel decision-making framework image captioning utilizing policy network value network. method achieves state-of-the-art performance coco dataset. best knowledge ﬁrst work applies decisionmaking framework image captioning. learn policy value networks introduce actor-critic reinforcement learning algorithm driven visual-semantic embedding. experiments suggest supervision embedding generalizes well across different evaluation metrics. many image captioning approaches proposed literature. early approaches tackled problem using bottom-up paradigm ﬁrst generated descriptive words image object recognition attribute prediction combined language models. recently inspired successful neural networks machine translation encoder-decoder framework brought image captioning. researchers adopted framework translating image sentence analogous task machine translation. approaches following framework generally encoded image single feature vector convolutional neural networks vector recurrent neural networks generate captions. various modeling strategies developed. karpathy fei-fei fang presented methods enhance models detecting objects images. mimic visual system humans spatial attention semantic attention proposed direct model attend meaningful details. dense captioning proposed handle localization captioning tasks simultaneously. ranzato proposed sequence-level training algorithm. inference state-of-the-art methods employ common decoder mechanism using greedy search beam search. words sequentially drawn according local conﬁdence. since always predict words local conﬁdence mechanism miss good words early steps lead captions. contrast addition local guidance method also utilizes global lookahead guidance compensate errors. decision-making decision-making core problem computer gaming control theory navigation path planning etc. problems exist agents interact environment execute series actions fulﬁll pre-deﬁned goals. reinforcement learning known machine learning technique concerning software agent ought take actions environment maximize notion cumulative reward well suited task decisionmaking. recently professional-level computer program designed silver using deep neural networks monte carlo tree search. human-level gaming control achieved deep q-learning. visual navigation system proposed recently based actor-critic reinforcement learning model. decision-making framework applied image captioning. previous work text generation used reinforce train model directly optimizing user-speciﬁed evaluation metric. metricdriven approach hard generalize metrics. perform well across different metrics needs re-trained isolation. paper propose training method using actor-critic reinforcement learning driven visual-semantic embedding performs well across different evaluation metrics without re-training. approach shows signiﬁcant performance improvement moreover decision-making framework generate captions follows existing encoder-decoder framework. figure illustration policy network comprised rnn. cnnp output initial input rnnp. policy network computes probability executing action certain state section ﬁrst deﬁne formulation deep reinforcement learning-based image captioning propose novel reward function deﬁned visual-semantic embedding. introduce training procedure well inference mechanism. formulate image captioning decision-making process. decision-making agent interacts environment executes series actions optimize goal. image captioning goal given image generate sentence correctly describes image content word sentence length. model including policy network value network viewed agent; environment given image words predicted wt}; action predict next word wt+. figure illustration value network comprised mlp. given state contains image input partially generated sentence value network evaluates value. action provided. generated word back rnnp next time step network input drives rnnp state transition ht+. speciﬁcally main working governed following equations decision-making process consists series actions. action state observed. problem state time step consists image words predicted wt}. action space dictionary words drawn from i.e. policy network policy network provides probability agent take actions state current state action wt+. paper convolutional neural network recurrent neural network construct policy network denoted cnnp rnnp. similar basic image captioning model used encoderdecoder framework. shown figure ﬁrstly cnnp encode visual information image visual information initial input node introduce value network ﬁrst deﬁne value function policy deﬁned prediction total reward observed state assuming decisionmaking process following policy i.e. approximate value function using value network serves evaluation state wt}. shown figure value network comprised multilayer perceptron denoted cnnv rnnv mlpv. value network takes image sentence inputs. cnnv utilized encode visual information rnnv designed encode semantic information partially generated sentence wt}. components trained simultaneously regress scalar reward investigate value network architecture section maximizing exactly non-trivial since involves expectation high-dimensional interaction sequences involve unknown environment dynamics turn. viewing problem partially observable markov decision process however allows bring techniques literature bear shown sample approximation gradient value network serves moving baseline. subtraction evaluation value network leads much lower variance estimate policy gradient. quantity used scale gradient seen estimate advantage action state approach viewed actor-critic architecture policy actor critic. however reinforcement learning image captioning hard train large action space comparing decision-making problems. action space image captioning order equals vocabulary size visual navigation indicates four directions handle problem follow apply curriculum learning train actor-critic model. order gradually teach model produce stable sentences provide training samples gradually difﬁculty iteratively ﬁrst words cross entropy loss actor-critic model train remaining words reinforcement learning used train whole sentence. please refer details. contributions proposed decisionmaking framework existing framework lies inference mechanism. decision-making problems inference guided local guidance global guidance e.g. alphago utilized mcts combine guidances. image captioning propose novel lookahead inference mechanism combines local guidance policy network global guidance value network. learned value network provides lookahead evaluation decision complement policy network collaboratively generate captions. decision-making framework important deﬁne concrete reasonable optimization goal i.e. reward reinforcement learning. propose utilize visual-semantic embedding similarities reward. visual-semantic embedding successfully applied image classiﬁcation retrieval etc. embedding model comprised linear mapping layer denoted cnne rnne learning mapping images sentences semantic embedding space provides measure similarity images sentences. given sentence embedding feature represented using last hidden state rnne i.e. denote feature vector image extracted cnne mapping function image features embedding space. train embedding model using image-sentence pairs image captioning. cnne weight learn rnne weights well using bidirectional ranking loss deﬁned follows following learn steps. ﬁrst step train policy network using standard supervised learning cross entropy loss loss function deﬁned −log train value network minimizing mean squared loss ||vθ ﬁnal reward generated sentence denotes randomly selected state generating process. generated sentence successive states strongly correlated differing word regression target shared entire captioning process. thus randomly sample single state distinct sentence prevent overﬁtting. stores top-b highly scoring candidates time step. beam width. denote sequences held time wbt} sequence generated words then wbt}. time step considers possible single word extensions beams given selects top-b scoring extensions beam sequences operator argtopb denotes obtaining top-b operation implemented sorting members denotes scoring function generated sequence. existing image captioning log-probability generated sequence. however scoring function miss good captions assumes log-probability every word good caption must among choices. necessarily true. analogously alphago every move probability. beneﬁcial sometimes allow actions probability selected long ﬁnal reward optimized. propose lookahead inference combines policy network value network consider executes action taking options wt+. current policy lookahead reward evaluation consideration i.e. st+) wbt+}) st+) score extending current sequence word wbt+ denotes conﬁdence policy network predict wbt+ extension denotes evaluation value network state supposing wbt+ generated. hyperparameter combining policy value network analyze experimentally section section perform extensive experiments evaluate proposed framework. reported results computed using microsoft coco caption evaluation tool including metrics bleu meteor rouge-l cider commonly used together fair thorough performance measure. firstly discuss dataset implementation details. compare proposed method state-of-the-art approaches image captioning. finally conduct detailed analyses method. dataset evaluate method widely used coco dataset image captioning task. fair comparison adopt commonly used splits proposed images training images validation images testing. image given least captions different workers. follow preprocess captions network architecture shown figure policy network value network contain rnn. adopt architectures them train independently. vgg- architecture lstm architecture. input node dimension hidden state dimension lstm i.e. many architectures literature e.g. resnet etc. reported better performance ones use. latest architecture fair comparison existing methods. value network three-layer regresses scalar reward value -dim -dim hidden layers between. figure state represented concatenating visual semantic features. visual feature -dim embedded feature mapped -dim cnnv output. semantic feature -dim hidden state rnnv last time step. thus dimension visual-semantic embedding visual-semantic embedding measure similarity images sentences mapping space. followed vgg- cnne rnne. image feature equation extracted last -dim layer vgg-. input node dimension hidden state dimension linear mapping layer. margin equation training details training adam algorithm model updating. worth noting that using pretrained vgg- model images captions provided dataset train networks embedding without external data. curriculum learning testing caption formed drawing words sequentially special token reached using proposed lookahead inference mechanism. ensemble models. comparing state-of-the-art methods table provide summary results method existing methods. obtain state-of-theart performance coco evaluation metrics. note semantic utilized rich extra data social media train visual attribute predictor table performance method coco dataset comparing state-of-the-art methods. beam size competing methods show results latest version paper. numbers bold face best known results indicates unknown scores. indicates external data used training methods. table performance variants method coco dataset beam size supervised learning baseline. slembed embedding. sl-rawvn plus pretrained value network. hid-vn value network directly utilizes policy hidden state. hid-im-vn value network utilizes policy hidden state policy image feature. full-model full model. utilized external data prove unique transfer capacity. makes results incomparable methods extra training data. surprisingly even without external training data method outperforms comparing methods approach shows signiﬁcant improvements metrics except bleu- method ranks second. bleu related single word accuracy performance bleu- method different preprocessing word vocabularies. mixer metric-driven trained method. model trained bleu- using hard generalize metrics. embedding-driven decision-making approach performs well metrics. especially considering policy network shown figure based mechanism similar basic image captioning model similar google signiﬁcant improvement validates effectiveness proposed decision-making framework utilizes policy value networks. moreover proposed framework modular w.r.t. network design. powerful mechanisms spatial attention semantic attention directly integrated policy network improve performance. since proposed embedding-driven decision-making framework different existing methods want perform insightful analyses answer following questions powerful embedding? performance gain framework embedding alone? important lookahead inference? important reinforcement learning framework? value network designed figure sensitive method hyperparameter beam size? answer questions conduct detailed analyses following three sections. much component contributes? section answer questions above. discussed section train policy value networks steps pretraining reinforcement learning. name initial policy network pretrained supervised learning name initial value network pretrained mean squared loss model served baseline value network lookahead inference. evaluate impact embedding incorporate embedding follows last step beam search beam candidate captions generated rank candidates according embedding similarities test image log-probabilities ﬁnally output highest embedding score. baseline named validate contribution lookahead inference reinforcement learning construct baseline rawvn proposed lookahead inference named finally full model named figure qualitative results method supervised learning baseline. ﬁrst three columns method generates better captions show failure cases last column. stands ground truth caption. shown table answer questions above using embedding alone sl-embed performs slightly better baseline. however sl-embed full-model big. therefore conclude using embedding alone powerful. proposed embedding-driven decision-making framework merit method. show qualitative captioning results method baseline figure stands ground truth caption. ﬁrst three columns compare method baseline. method better recognizing objects easily missed e.g. snowboard umbrellas ﬁrst column images. besides method reduce chance generating incorrect word accumulating errors e.g. generate word eating sitting image lower second column. moreover thanks global guidance method better generating correct captions global level e.g. recognize airplane painting images third column. finally show failure cases method last column cases fail understand important visual contents take small portions images. policy network architecture. adding detailed visual modeling techniques detection attention alleviate problem future. paper propose novel framework involves value network whose architecture worth looking into. figure cnnv rnnv extract visual semantic information image sentence inputs. since hidden state policy network time step representation state well natural question directly utilize policy hidden state?. answer question construct variants value network ﬁrst named comprised mlpv policy hidden state rnnp; second variant comprised mlpv concatenation policy hidden state rnnp visual input policy rnnp. results shown table variants utilize policy hidden state work well comparing full-model. problem policy hidden state compresses also loses lots information. thus reasonable better train independent value network ittable show evaluation impact method. equation hyperparameter combining policy value networks lookahead inference means value network guide lookahead inference; means policy network identical beam search. shown table best performance goes goes overperformance drops monotonically. validates importance networks; emphasize much either network lookahead inference. besides performs much worse policy network provides local guidance important sequential prediction. thus lookahead inference weak global guidance i.e. value network approach. table provide evaluation different beam sizes’ impact baseline full model. discovered previous work image captioning performance becomes worse beam size gets larger. validate discovery existing encoder-decoder framework. shown upper half table test baseline different beam sizes note based beam search follows encoder-decoder framework existing approaches. impact beam size relatively big. it’s mainly increase beam size word candidates likely drawn beam since conﬁdence provided sequential word generator consider local information. hand shown lower part table method less sensitive beam sizes. performance variations different beam sizes fairly small. argue proposed lookahead inference considers policy value networks. local global guidances framework robust stable policy mistakes. conclusion work present novel decision-making framework image captioning achieves state-of-the-art performance standard benchmark. different previous encoder-decoder framework method utilizes policy network value network generate captions. policy network serves local guidance value network serves global lookahead guidance. learn networks actor-critic reinforcement learning approach novel visual-semantic embedding rewards. conduct detailed analyses framework understand merits properties. future works include improving network architectures investigating reward design considering embedding measures. donahue hendricks guadarrama rohrbach venugopalan saenko darrell. long-term recurrent convolucvpr tional networks visual recognition description. fang gupta iandola srivastava deng dollar mitchell platt zitnick zweig. captions visual concepts back. cvpr farhadi hejrati sadeghi young rashtchian hockenmaier forsyth. every picture tells story generating sentences images. eccv hendricks venugopalan rohrbach mooney saenko darrell. deep compositional captioning describing novel object categories without paired training data. cvpr mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik antonoglou king kumaran wierstra legg hassabis. human-level control deep reinforcement learning. nature silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis. mastering game deep neural networks tree search. nature", "year": 2017}