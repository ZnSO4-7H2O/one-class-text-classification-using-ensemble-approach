{"title": "A Unified Framework for Representation-based Subspace Clustering of  Out-of-sample and Large-scale Data", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Under the framework of spectral clustering, the key of subspace clustering is building a similarity graph which describes the neighborhood relations among data points. Some recent works build the graph using sparse, low-rank, and $\\ell_2$-norm-based representation, and have achieved state-of-the-art performance. However, these methods have suffered from the following two limitations. First, the time complexities of these methods are at least proportional to the cube of the data size, which make those methods inefficient for solving large-scale problems. Second, they cannot cope with out-of-sample data that are not used to construct the similarity graph. To cluster each out-of-sample datum, the methods have to recalculate the similarity graph and the cluster membership of the whole data set. In this paper, we propose a unified framework which makes representation-based subspace clustering algorithms feasible to cluster both out-of-sample and large-scale data. Under our framework, the large-scale problem is tackled by converting it as out-of-sample problem in the manner of \"sampling, clustering, coding, and classifying\". Furthermore, we give an estimation for the error bounds by treating each subspace as a point in a hyperspace. Extensive experimental results on various benchmark data sets show that our methods outperform several recently-proposed scalable methods in clustering large-scale data set.", "text": "abstract—under framework spectral clustering subspace clustering building similarity graph describes neighborhood relations among data points. recent works build graph using sparse low-rank ℓnorm-based representation achieved state-of-the-art performance. however methods suffered following limitations. first time complexities methods least proportional cube data size make methods inefﬁcient solving large-scale problems. second cannot cope out-of-sample data used construct similarity graph. cluster out-of-sample datum methods recalculate similarity graph cluster membership whole data set. paper propose uniﬁed framework makes representation-based subspace clustering algorithms feasible cluster out-of-sample large-scale data. framework large-scale problem tackled converting out-ofsample problem manner sampling clustering coding classifying. furthermore give estimation error bounds treating subspace point hyperspace. extensive experimental results various benchmark data sets show methods outperform several recently-proposed scalable methods clustering large-scale data set. index terms—scalable subspace clustering out-of-sample problem sparse subspace clustering low-rank representation least square regression error bound analysis. cluster maximizing inter-cluster dissimilarity intra-cluster similarity. past decades number clustering approaches proposed example partitioning-based clustering kernelbased clustering subspace clustering subspace clustering aims ﬁnding low-dimensional subspace group data points. mainly contains tasks i.e. projecting data another space calculating cluster membership data projection space popular subspace clustering methods include limit statistical methods spectral clustering spectral clustering ﬁnds cluster membership data points using spectrum afﬁnity matrix. afﬁnity matrix corresponds similarity graph vertex denotes data point edge weights representing similarities connected points. thus heart spectral clustering similarity graph construction problem. widely-used approaches build similarity graph i.e. pairwise distance reconstruction coefﬁcients speciﬁcally computes similarity based distance data points. however cannot reﬂect global structure data value depends connected data points. contrast denotes data point linear combination points uses representation coefﬁcients similarity measurement. several recent works shown superior subspace clustering example sparse representation rank representation latent rank representation ℓ-normbased representation although representation-based subspace clustering extensively studied solve large-scale out-ofsample clustering problems less explored. taking sparse subspace clustering example iteratively computes sparse codes data points performs eigen-decomposition graph laplacian matrix. computational complexity even though fastest ℓ-solver used denotes dimensionality data set. thus medium-sized data bring large-scale problem ssc. moreover cannot handle out-of-sample data used construct similarity graph. cluster previously unseen datum recompute similarity graph cluster membership whole data set. fact representation-based subspace clustering methods suffered similar limitations dealing large-scale out-of-sample data. address issues propose uniﬁed framework representation-based subspace clustering algorithms. framework treats large-scale problem outof-sample problem manner sampling clustering coding classifying speciﬁcally split large fig. architecture proposed framework scalable representation-based subspace clustering. framework summarized sampling clustering coding classifying solid dotted lines used show processes clustering in-sample data out-of-sample data respectively. out-of-sample problem steps needed. scale data in-sample data outof-sample data then obtain cluster membership assign out-of-sample datum nearest subspace spanned framework three scalable methods presented i.e. scalable sparse subspace clustering scalable rank representation scalable least square regression proposed methods remarkably improve computational efﬁciency original approaches preserving good clustering performance. paper substantial extension conference paper improved following aspects perform error analysis framework treating subspace well-deﬁned hyperspace. presented lower upper error bounds helpful understanding working mechanism nearest subspace classiﬁer best knowledge ﬁrst work perform errors analysis src. additionally propose scalable methods i.e. slrr slsr make rank representation least square regression feasible cluster large scale data out-of-sample data. perform extensive experiments compare methods scalable clustering methods data sets. conduct comprehensive analysis approaches including performance different outof-sample grouping strategies inﬂuence different parameters. rest paper organized follows provide section brief review representation-based clustering algorithms scalable spectral clustering methods. section propose framework three scalable representation-based clustering algorithms present theoretical results error bound analysis framework. demonstrate performance proposed methods compare recently-proposed scalable clustering approaches nine data sets section lastly give conclusions work section number data points dimensionality given data number clusters number in-sample data number iterations algorithm rank given data matrix prediction given data data points belonging subspace in-sample data out-of-sample data representation given data afﬁnity matrix based laplacian matrix eigenvector matrix represent matrices. denote transpose pseudo-inverse matrix respectively. denotes identity matrix. table summarizes notations used throughout paper. recently elhamifar vidal proposed well-founded recovery theory independent subspaces disjoint subspaces. calculates similarity among data points solving following optimization problem rn×n sparse representation data rm×n corresponds sparse outlying entries denotes reconstruction errors limited representational capability parameters balance three terms objective function. convex solved number ℓ-solvers getting builds similarity graph |c|t performs spectral clustering graph. addition takes calculate eigenvectors laplacian matrix considering sparse matrix time complexity step could reduced lanczos eigensolver used. however still daunting task even moderate denotes nuclear norm could chosen ℓ-norm ℓ-norm frobenius norm depending prior knowledge error structure. generally ℓ-norm adopted deal sample-speciﬁc corruption outlier ℓ-norm used characterize random corruption frobenius norm used handle gaussian noise. adopts augmented lagrange multipliers method solve takes perform singular value decomposition dense matrix iteration. addition take perform clustering denotes number iterations k-means method. therefore overall time complexity number iterations alm. extensions solve convex optimization problem computational complexities high. recently least square regression shown ℓ-norm-based representation achieve competitive result faster speed. aims solving besides large scale clustering problem suffered out-of-sample problem i.e. cannot cope data used construct similarity graph. previously unseen datum perform algorithm whole data again. makes impossible cluster incremental data. summarized algorithm methods proposed reduce time cost minimizing lowest-rank matrix however methods mainly focused speeding encoding process without consideration clustering process. recently works focused solving largescale clustering problem traditional spectral clustering. natural reduce time cost eigendecomposition laplacian matrix. example adopted nystr¨om method approximation eigenvectors whole similarity matrix. solved generalized eigenvalue problem distributed computing platform. another reducing data size replacing original data small number samples. presented fast spectral clustering algorithm selecting representative points input cluster assignment based chosen samples. proposed landmark-based spectral clustering algorithm. algorithm chooses representative points landmarks constructs laplacian matrix element rp×n pairwise distance input data landmarks. selects landmarks performing selective sampling technique running spectral clustering chosen samples based pairwise distance. proposed spectral embedded clustering groups out-of-sample data linear projection space. main difference among works method handle out-of-sample data. different sampling-based method belabbas wolfe proposed quantization based method theoretical justiﬁcation select in-sample data deterministic way. extending quantization based method self-organizing maps tasdemir recently proposed novel method utilizing quantization property soms neural handle large scale data set. extensive experimental studies show method achieved impressive performance compared sampling-based methods range data sets. although numerous works conducted speeding pairwise distance based clustering methods researches done enhance scalability representation based approaches. section present framework makes representation based subspace clustering methods feasible handle large scale data out-of-sample data. method treats large-scale problem out-of-sample problem taking strategy sampling clustering coding classifying. ﬁrst steps choose small number data points in-sample data calculate cluster membership them. third fourth steps low-dimensional subspace group out-of-sample data assign data subspace minimal residual. note that solve out-of-sample problem last steps needed. assumption twofold. first implies data point could encoded linear combination basis second requires independent identically distributed out-of-sample data could represented assumption general data mining machine learning works based. practice sparsity assumption easily satisﬁed high-dimensional data facial images. satisfy assumption i.i.d. need representative points rm×p rm×n out-of-sample data locate subspaces spanned sampling techniques column selection method used. however sampling methods inefﬁcient cannot applied large scale setting. paper adopt uniform random sampling approach time cost addition computational efﬁciency uniform random sampling method perform comparably complex sampling techniques shown sampling getting cluster membership in-sample data handle out-of-sample data based knowledge learnt simplest approach assigning nearest terms euclidean distance. however approach implicitly requires prior knowledge. example data must locate euclidean space otherwise would correctly clustered. although achieved successes pattern recognition recent works showed non-sparse representation achieve comparable results less time cost. therefore perform linear coding scheme instead sparse solving positive real number. second term used avoid over-ﬁtting. zhang named method collaborative representation-based classiﬁcation empirically showed collaborative representation rather sparse plays important role face recognition. getting coefﬁcient solving assigned subspace produces minimal regularized residuals classes. note that also known linear regression based classiﬁcation framework sssc slrr slsr proposed make feasible cluster large scale out-of-sample data. algorithm summarizes approaches fig. gives example show effectiveness framework. example nodexl software obtain visualization similarity graphs fig. section perform error analysis framework. lemma shows clustering partitions solely based in-sample data rm×p converge partitions based whole data rm×n sampled data enough. based lemma show error bound framework depend grouping errors out-of-sample data rm×. moreover lemma preliminary step result. lemma assumption ﬁrst eigenvalues multiplicity holds ﬁrst eigenvalues sufﬁciently large denote laplacian matrix based respectively. case ﬁrst eigenvalues converge ﬁrst eigenvalues corresponding eigenvectors converge almost surely. clustering partitions constructed normalized spectral clustering ﬁrst eigenvectors ﬁnite samples converge almost surely limit partition whole data space. fig. example based sssc slrr showing effectiveness framework. given data satisfying sparsity assumption rank data equals two; in-sample data identifying using unique random sampling method. i.i.d.; similarity graph achieved sssc; similarity graph achieved slrr; out-of-sample data locating union subspaces spanned projection coefﬁcients out-of-sample data point coefﬁcients nonzero. grouped subspace terms method matches ground truth. assumption example shows framework solve large-scale out-of-sample problems representation-based subspace clustering without loss clustering quality. algorithm scalable sparse subspace clustering scalable rank representation scalable least square regression input given data rm×n desired number clusters rigid regression parameter randomly select data points in-sample data remaining samples used out-of-sample data lemma additive clustering error induced framework comes process grouping out-of-sample data thus problem becomes ﬁnding error boundary nearest subspace classiﬁer representation-based classiﬁers extensively studied however theoretical analysis receives little attention. presents theoretical explanation view maximizing performance margin. however error boundary still unknown. paper mainly investigate performance theoretical perspective. best challenging perform error analysis classiﬁers active sets different data points different. words difﬁcult invariant support vectors represent subspace. therefore classic margin analysis theory cannot directly used classiﬁers. solve problem propose treating subspace point hyperspace. following deﬁnition. based deﬁnition classiﬁer could regarded nearest neighbor classiﬁer hyperspace avoid support vectors category. note that treats subspace data point grassmann space distance deﬁned principle angle subspaces. clearly adopted distance metric major difference grassmann space deﬁned hyperspace. indeed grassmann space regarded special case hyperspace discussed section. fig. subspaces spanned in-sample data. denote out-of-sample data point principal angle residuals associating hyperspace regarded data points. decision boundary nearest subspace classiﬁer hyperspace. analysis conclude that error bound depends structure subspaces spanned in-sample data assumption indeed structure subspaces also unique factor affect clustering quality shown thus argue framework solves large-scale out-of-sample problems representationbased subspace clustering methods without introducing error factors. largely different traditional methods whose performance depends sampling rate. fig. real-world example validate estimated error bounds. classiﬁcation error error bound subsets extended yale database subset consists samples ﬁrst category another category. different error bounds derived equations considering well conditioned sets measures similarity subspace using inner product. generally i-th principal angle then holds i-th singular value according deﬁnition frobenius norm i.e. measures distance principal angles rank assumption error analysis method validate following conditions satisﬁed when data sampled subspaces i.e. extend method recursively transforming multiple clusters problem binary even though task need massive effort; in-sample data correctly clustered. otherwise needs identify error bound whole framework grouping out-ofsample data. difﬁculty task identify inﬂuence perturbation sampling. possible solve problem perturbation theory studied quantum mechanics. however beyond main scope paper. subsets extended yale database subset consists samples ﬁrst category others. samples training remaining samples testing. moreover principle components analysis preprocess step extract features training testing data. fig. shows results that successfully estimate error bounds subsets case respectively. failure cases attributed following reasons first classiﬁcation error calculated based training data testing data whereas error bounds estimated based training data. training data cannot represent distribution whole data space estimated bounds incorrect. second analysis based assumption perfectly satisﬁed real-world data since real-world data often complex. figs. show larger reduce classiﬁcation error rate increasing failure rate error analysis method. reason used avoid overﬁtting adding value diagonal entries actually affects structure observed data. suppose samples selected data points dimensionality sssc needs cluster membership in-sample data homotopy optimizer used solve ℓminimization problem lanczos eigensolver used compute eigenvectors rp×p number clusters number iterations homotopy optimizer k-means algorithm respectively. group out-of-sample data points sssc needs compute pseudo-inverse matrix calculate linear representation op). putting everything together computational complexity sssc since clearly cost sssc largely less similar computational complexities slrr slsr. table reports computational complexities methods original algorithms. perform experiments nine real-world data sets including facial images handwritten digital data news corpus etc. data sets consist three small-sized data sets three medium-sized data sets three large scale data sets. presented statistics data sets table brief description follows. general facial images assumed located low-dimensional manifold. experiments investigate four popular facial data sets i.e. extended yale database labeled faces wild-a multi-pie includes face images people implementation used subset contains clean faces randomly selected male subjects female subjects. contains images captured uncontrolled environment variations pose illumination expression misalignment occlusion. subset aligned includes subjects less samples subject. mpie contains facial images individuals captured four sessions simultaneous variations pose expression illumination. frontal images sessions. computational efﬁciency downsize images exyaleb images mpie images moreover perform downsized data retain energy. image divide conquer strategy adopted details image partitioned blocks; discriminationenhanced feature block extracted; that blocks’ features concatenated form ﬁnal feature vector. reuters- documental corpus. experiments ﬁrst principle components extracted features. also three data sets i.e. pendigits covtype pokerhand pokerhand unbalanced data maximal class contains samples compared samples minimal class. examine performance tested algorithms using original data subset data points three largest subjects. experiments consist parts section iv-c investigates performance methods varying parameters; section iv-d reports results evaluated algorithms different sampling rates; section iv-e compares methods corresponding original algorithms three facial data sets. moreover also investigate performance nearest subspace classiﬁers section iv-f reports clustering quality tested methods three medium-sized data sets including facial images handwritten digital data documental corpus; section iv-g shows results three large scale data sets. spectral clustering kernel-based clustering methods popular cope linearly inseparable data. studies established equivalence them. experiments compare proposed methods four scalable spectral clustering algorithms nystr¨om approximation based spectral clustering scalable kernel-based clustering approach moreover report results k-means clustering algorithm baseline. besides investigate performance variants nystr¨ombased methods denoted nystr¨om nystr¨om-orth afﬁnity matrix nystr¨om-orth orthogonal whereas nystr¨om not. obtains results performing k-means embedding space. algorithms implemented matlab. used data sets codes algorithms downloaded www.machineilab.org/users/pengxi/. evaluated algorithms take approaches insample data. speciﬁcally sssc slrr slsr nystr¨om nystr¨om orth identify in-sample data performing uniform random sampling method whereas kasp adopt k-means clustering method. avoid disparity data partitions pre-partition data parts in-sample data out-of-sample data. that different algorithms data partitions. measure clustering quality using accuracy normalized mutual information produced clusters ground truth categories. accuracy indicates perfect matching true subspace distribution whereas indicates totally mismatch. consistent previous works tune parameters evaluated methods achieve highest accuracy. sssc adopted homotopy optimizer solve ℓ-minimization problem. optimizer user-speciﬁed parameters sparsity parameter error tolerance parameter tuned parameters range slrr slsr value chosen shown fig. referring parameter setting parameter kasp nystr¨om interval interval parameter ranges interval three user-speciﬁed parameters i.e. size neighborhood balanced parameters moreover value range used kasp lsc. sssc uses control sparsity representation measure reconstruction errors. slrr uses balance different parts objective function slsr utilizes avoid overﬁtting. choice parameters depends data distribution. fig. shows results sssc slrr slsr different parameter values. sssc assigned small positive value achieves good performance. parameters assigned value performance sssc degraded. slrr ranges accuracy almost unchanged. slsr performs worse increasing veriﬁes claim small preferable clean data set. study inﬂuences in-sample data size perform experiments exyaleb setting denotes sample size subject increases interval fig. reports result following observations except scalable clustering methods outperform k-means method accuracy nmi. sssc slrr slsr superior investigated approaches considerable performance margin. example slrr achieves gain accuracy gain best baseline algorithm cases algorithms except nystr¨om nystr¨om orth perform better increasing possible reason result nystr¨om nystr¨om orth speed clustering process reducing size afﬁnity matrix rather data size. accuracy slrr decreased increased result seems inconsistent common sense data tend bring better performance. result attributed characteristic slrr i.e. slrr based rank representation incorporates relations among different subspaces. increasing would result intersections among different subspaces weaken discrimination model. obtain optimal model selection methods -estimator could used. carry experiments three facial data sets i.e. exyaleb lfw. moreover investigate performance methods classiﬁers used group out-of-sample data. experiments sssc ssc. fig. inﬂuence parameters. half images chosen exyaleb in-sample data rest used out-of-sample data. x-coordinate denotes values parameters y-coordinate corresponds clustering quality clustering quality. example accuracy data higher sssc whereas time cost three times sssc. increase data size fail results whereas sssc slrr slsr results acceptable time cost. compared scalable methods sssc slrr slsr elegant balance clustering quality time costs. although sssc slrr slsr fastest achieve best results. slrr performs better sssc tests. possible reason rank representation could capture structure among different categories whereas sparse representation cannot pointed moreover regularized residual based classiﬁer perform slightly better non-regularized residual based classiﬁcation method investigated performance exyaleb. highest accuracy tests comparing experiment. potential reason performance difference adopted spectral rotation cluster membership whereas k-means clustering method. note that best result reported methods outperform scalable methods. example slrr achieves gain accuracy mpie best competing algorithm gains achieved sssc slsr respectively. running time weakness sssc slrr slsr even though efﬁcient original approaches. found time consumed handle in-sample data. example sssc takes seconds cluster in-sample data seconds handle out-of-sample data case rcv. since in-sample data clustering ofﬂine process assume algorithms competitive large scale setting shown section iv-g. fig. clustering quality competing algorithms extended yale database x-coordinate denotes in-sample data size y-coordinate denotes clustering quality note denotes in-sample data size. number parenthesis tuned parameters. k-means method cannot handle out-of-sample data. thus results four methods achieved directly performing whole data set. sssc slrr slsr assign out-of-sample data nearest subspace minimal residual whereas sssc slrr slsr results using eq.. bold number indicate best performance. table reports performance algorithms three large scale data sets. data samples selected in-sample data remaining samples used out-of-sample data. assign sssc covtype sssc slrr slsr outperform approaches tests. example accuracy sssc least higher tested methods covtype. pokerhand- pokerhand- gains respectively. highest accuracy covtype achieved nystr¨om orth kasp respectively. experiments accuracy four algorithms respectively. possible reason attribute subtle engineering details e.g. in-sample out-of-sample data partitions. increase data size methods demonstrate good balance running time clustering quality. moreover used memory methods depends in-sample data size makes methods competitive large scale setting. summary conclude three methods outperform competing algorithms tests. particular sssc advantageous large scale data sets slrr outperforms high-dimensional data clustering problems slsr achieve comparable clustering performance sssc slrr higher computational efﬁciency latter. paper proposed general framework solve large-scale out-of-sample clustering problems representation-based subspace clustering. framework presented three scalable methods i.e. sssc slrr slsr largely reduce computational complexity original methods preserving good performance. proved performance method depends latent structure data independent sampling rate. moreover proposed novel method analyze error bounds nearest subspace classiﬁer terms binary case applied work extended improved following aspects. first proposed framework based assumption out-of-sample data represented in-sample data. hence method fail handle out-of-sample datum comes subspaces emerge in-sample data. worth explore overcome problem future. second proposed error analysis method considers binary case practical challenging explore error analysis method w.r.t. authors would like thank anonymous reviewers valuable comments suggestions improve quality paper. work supported national nature science foundation china grant tron vidal motion segmentation robust subspace separation presence outlying incomplete corrupted trajectories proc. ieee conf. comput. vis. pattern recognit. anchorage jun. huang zeng human gait recognition using patch distribution feature locality-constrained group sparse representation ieee trans. image process. vol. jan. i.-h. tsang l.-t. chia laplacian sparse coding hypergraph laplacian sparse coding applications ieee trans. pattern anal. mach. intell. vol. jan. favaro vidal ravichandran closed form solution robust subspace estimation clustering proc. ieee conf. comput. vis. pattern recognit. colorado springs jun. c.-y. z.-q. zhao d.-s. huang robust efﬁcient subspace segmentation least squares regression proc. eur. conf. comput. vis. florence italy oct. yang ganesh sastry fast l-minimization algorithms application robust face recognition review eecs department university california berkeley tech. rep. ucb/eecs- feb. tasdemir vector quantization based approximate spectral clustering large datasets pattern recogn. vol. halko martinsson tropp finding structure randomness probabilistic algorithms constructing approximate matrix decompositions siam review vol. zhang yang feng sparse representation collaborative representation helps face recognition? proc. ieee int. conf. comput. vis. barcelona spain nov. smith milic-frayling shneiderman mendes rodrigues leskovec dunne nodexl free open network overview discovery exploration add-in excel social media research foundation wang yang nasrabadi huang max-margin perspective sparse representation-based classiﬁcation proc. ieee conf. comput. vis. sydney australia dec. chitta havens jain approximate kernel k-means solution large scale kernel clustering proc. sigkdd int. conf. knowl. dis. data min. diego aug. georghiades belhumeur kriegman from many illumination cone models face recognition variable lighting pose ieee trans. pattern anal. mach. intell. vol. huang ramesh berg learned-miller labeled faces wild database studying face recognition unconstrained environments university massachusetts amherst tech. rep. oct. yang zhang zhang wang relaxed collaborative representation pattern classiﬁcation proc. ieee conf. comput. vis. pattern recognit. providence jun. alimoglu alpaydin combining multiple representations classiﬁers pen-based handwritten digit recognition proc. int. conf. doc. anal. recognit. germany aug. blackard dean comparative accuracies artiﬁcial neural networks discriminant analysis predicting forest cover types cartographic variables comput. electron. agric. vol. zhang received b.s. masters degrees mathematics ph.d. degree computer science university electronic science technology china chengdu china respectively. post-doctoral research fellow department computer science engineering chinese university hong kong shatin hong kong currently professor sichuan university chengdu. current research interests include theory applications neural networks based neocortex computing data analysis methods inﬁnity deep neural networks. peng research scientist institute infocomm. research agency science technology research singapore. received beng degree electronic engineering meng degree computer science chongqing university posts telecommunications ph.d. degree sichuan university china respectively. current research interests include computer vision image processing pattern recognition. peng recipient china national graduate scholarship csc-ibm scholarship outstanding chinese students excellent student paper ieee chengdu section served member international conferences ijcnn reviewer international journals ieee tnnls tkde tifs tgrs tcyb. huajin tang received b.eng. degree zhejiang university hangzhou china m.eng. degree shanghai jiao tong university shanghai china ph.d. degree electrical computer engineering national university singapore singapore system engineer stmicroelectronics singapore post-doctoral fellow queensland brain institute university queensland brisbane australia currently research scientist leading cognitive computing group institute infocomm research agency science technology research singapore. authored monograph international journal papers. current research interests include neural computation neuromorphic cognitive systems neurocognitive robots machine learning. zhang received ph.d. degree mathematics institute mathematics chinese academy science beijing china currently professor college computer science sichuan university chengdu china. co-author three books convergence analysis recurrent neural networks neural networks computational models applications subspace learning neural networks chair associate editor ieee ieee chengdu section associate editor ieee transactions cybernetics current research interests include neural networks data. founding director machine intelligence laboratory. also founder ieee computational intelligence society chengdu chapter. shijie xiao received b.e. degree harbin institute technology harbin china currently pursuing ph.d. degree school computer engineering nanyang technological university singapore.", "year": 2013}