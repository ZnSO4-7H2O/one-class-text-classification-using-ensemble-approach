{"title": "Meta-Learning for Semi-Supervised Few-Shot Classification", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more challenging situation where examples from other distractor classes are also provided. To address this paradigm, we propose novel extensions of Prototypical Networks (Snell et al., 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. We evaluate these methods on versions of the Omniglot and miniImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet, consisting of a large set of classes, with a hierarchical structure. Our experiments confirm that our Prototypical Networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would.", "text": "mengye ren† eleni triantaﬁllou∗† sachin ravi∗§ jake snell† kevin swersky¶ joshua tenenbaum hugo larochelle¶‡ richard zemel†‡ †university toronto §princeton university ¶google brain ‡cifar vector institute {mreneleni}cs.toronto.edu sachinrcs.princeton.edu jsnellcs.toronto.edu kswerskygoogle.com jbtmit.edu hugolarochellegoogle.com zemelcs.toronto.edu few-shot classiﬁcation interested learning algorithms train classiﬁer handful labeled examples. recent progress few-shot classiﬁcation featured meta-learning parameterized model learning algorithm deﬁned trained episodes representing different classiﬁcation problems small labeled training corresponding test set. work advance few-shot classiﬁcation paradigm towards scenario unlabeled examples also available within episode. consider situations unlabeled examples assumed belong classes labeled examples episode well challenging situation examples distractor classes also provided. address paradigm propose novel extensions prototypical networks augmented ability unlabeled examples producing prototypes. models trained end-to-end episodes learn leverage unlabeled examples successfully. evaluate methods versions omniglot miniimagenet benchmarks adapted framework augmented unlabeled examples. also propose split imagenet consisting large classes hierarchical structure. experiments conﬁrm prototypical networks learn improve predictions unlabeled examples much like semi-supervised algorithm would. availability large quantities labeled data enabled deep learning methods achieve impressive breakthroughs several tasks related artiﬁcial intelligence speech recognition object recognition machine translation. however current deep learning approaches struggle tackling problems labeled data scarce. speciﬁcally current methods excel tackling single problem lots labeled data methods simultaneously solve large variety problems labels lacking. humans hand readily able rapidly learn classes types fruit visit tropical country. signiﬁcant human machine learning provides fertile ground deep learning developments. reason recently increasing body work few-shot learning considers design learning algorithms speciﬁcally allow better generalization problems small labeled training sets. focus case few-shot classiﬁcation given classiﬁcation problem assumed contain handful labeled examples class. approach few-shot learning follows form meta-learning performs transfer learning pool various classiﬁcation problems generated large quantities available labeled data classiﬁcation problems classes unseen training time. meta-learning take form learning shared metric common initialization few-shot classiﬁers generic inference network various meta-learning formulations signiﬁcant progress recently few-shot classiﬁcation. however progress limited setup few-shot learning episode differs humans learn concepts many dimensions. paper generalize setup ways. first consider scenario classes learned presence additional unlabeled data. many successful applications semisupervised learning regular setting single classiﬁcation task classes training test time same work addressed challenge performing transfer classes never seen training time consider here. second consider situation classes learned viewed isolation. instead many unlabeled examples different classes; presence distractor classes introduces additional realistic level difﬁculty fewshot problem. figure consider setup learn classiﬁer distinguish previously unseen classes goldﬁsh shark given labeled examples classes also larger pool unlabeled examples belong classes interest. work move step closer natural learning framework incorporating learning episodes unlabeled data classes learn representations well distractor classes work ﬁrst study challenging semi-supervised form few-shot learning. first deﬁne problem propose benchmarks evaluation adapted omniglot miniimagenet benchmarks used ordinary few-shot learning. perform extensive empirical investigation settings mentioned above without distractor classes. second propose study three novel extensions prototypical networks state-ofthe-art approach few-shot learning semi-supervised setting. finally demonstrate experiments semi-supervised variants successfully learn leverage unlabeled examples outperform purely supervised prototypical networks. recent progress few-shot learning made possible following episodic paradigm. consider situation large labeled dataset classes ctrain. however training examples ctrain ultimate goal produce classiﬁers disjoint classes ctest labeled examples available. idea behind episodic paradigm simulate types few-shot problems encountered test taking advantage large quantities available labeled data classes ctrain. speciﬁcally models trained k-shot n-way episodes constructed ﬁrst sampling small subset classes ctrain generating training containing examples classes test different examples classes. input vector dimension class training episodes done feeding support label confusing. sake clarity refer content episodes support query sets process iterating training episodes simply training. prototypical network few-shot learning model virtue simple obtaining state-of-the-art performance. high-level uses support extract prototype vector class classiﬁes inputs query based distance prototype class. precisely prototypical networks learn embedding function parameterized neural network maps examples space examples class close different classes far. parameters prototypical networks embedding function. compute prototype class per-class average embedded examples performed loss function used update prototypical networks given training episode simply average negative log-probability correct class assignments query examples generalization performance measured test episodes contain images classes ctest instead ctrain. test episode predictor produced prototypical network provided support classify query input likely class argmaxc deﬁne semi-supervised setting considered work few-shot learning. training denoted tuple labeled unlabeled examples labeled portion usual support few-shot learning literature containing list tuples inputs targets. addition classic few-shot learning introduce unlabeled containing inputs ˜xm}. purely supervised setting models trained perform well predicting labels examples episode’s query figure shows visualization training test episodes. figure example semi-supervised few-shot learning setup. training involves iterating training episodes consisting support unlabeled query goal labeled items unlabeled items within episode generalize good performance corresponding query set. unlabeled items either pertinent classes considering distractor items belong class relevant current episode however note model actually ground truth information whether unlabeled example distractor not; plus/minus signs shown illustrative purposes. test time given episodes consisting novel classes seen training evaluate meta-learning method. original formulation prototypical networks specify leverage unlabeled follows propose various extensions start basic deﬁnition prototypes provide procedure producing reﬁned prototypes using unlabeled examples reﬁned prototypes obtained models trained loss function ordinary prototypical networks equation replacing ˜pc. query example classiﬁed classes based proximity embedded position corresponding reﬁned prototypes average negative logprobability correct classiﬁcation used training. figure left prototypes initialized based mean location examples corresponding class ordinary prototypical networks. support unlabeled query examples solid dashed white colored borders respectively. right reﬁned prototypes obtained incorporating unlabeled examples classiﬁes query examples correctly. ﬁrst consider simple leveraging unlabeled examples reﬁning prototypes taking inspiration semi-supervised clustering. viewing prototype cluster center reﬁnement process could attempt adjust cluster locations better examples support unlabeled sets. view cluster assignments labeled examples support considered known ﬁxed example’s label. reﬁnement process must instead estimate cluster assignments unlabeled examples adjust cluster locations accordingly. natural choice would borrow inference performed soft k-means. prefer version k-means hard assignments since hard assignments would make inference non-differentiable. start regular prototypical network’s prototypes cluster locations. then unlabeled examples partial assignment cluster based euclidean distance cluster locations. finally reﬁned prototypes obtained incorporating unlabeled examples. predictions query input’s class modeled equation using reﬁned prototypes ˜pc. could perform several iterations reﬁnement usual k-means. however experimented various number iterations found results improve beyond single reﬁnement step. soft k-means approach described implicitly assumes unlabeled example belongs either classes episode. however would much general make assumption model robust existence examples classes refer distractor classes. example situation would arise wanted distinguish pictures unicycles scooters decided unlabeled downloading images web. would realistic assume images unicycles scooters. even focused search similar classes bicycle. since soft k-means distributes soft assignments across classes distractor items could harmful interfere reﬁnement process prototypes would adjusted also partially account distractors. simple address additional cluster whose purpose capture distractors thus preventing polluting clusters classes interest take simplifying assumption distractor cluster prototype centered origin. also consider introducing length-scales represent variations within-cluster distances speciﬁcally distractor cluster modeling distractor unlabeled examples single cluster likely simplistic. indeed inconsistent assumption cluster corresponds class since distractor examples well cover single natural object category. continuing unicycles bicycles example search unlabeled images could accidentally include bicycles related objects tricycles cars. also reﬂected experiments constructed episode generating process would sample distractor examples multiple classes. address problem propose improved variant instead capturing distractors high-variance catch-all cluster model distractors examples within area legitimate class prototypes. done incorporating soft-masking mechanism allows threshold information amount intra-cluster variation determine aggressively unlabeled examples. then soft masks contribution example prototype computed comparing threshold normalized distances follows sigmoid function. training reﬁnement process model equation learn include ignore entirely certain unlabeled examples. soft masks makes process entirely differentiable. finally much like regular soft k-means could recursively repeat reﬁnement multiple steps found single step perform well enough. best performing methods few-shot learning episodic training framework prescribed meta-learning. approach within work falls metric learning methods. previous work metric-learning few-shot-classiﬁcation includes deep siamese networks matching networks prototypical networks model extend semi-supervised setting work. general idea learn embedding function embeds examples belonging class close together keeping embeddings separate classes apart. distances embeddings items support query used notion similarity classiﬁcation. lastly closely related work regard extending few-shot learning setting bachman employ matching networks active learning framework model choice unlabeled item support certain number time steps classifying query set. unlike setting meta-learning agent acquire ground-truth labels unlabeled distractor examples. meta-learning approaches few-shot learning include learning support update learner model generalize query set. recent work involved learning either weight initialization and/or update step used learner neural network another approach train generic neural architecture memory-augmented recurrent network temporal convolutional network sequentially process support perform accurate predictions labels query examples. methods also competitive few-shot learning chose extend prototypical networks work simplicity efﬁciency. literature semi-supervised learning quite vast relevant category work related self-training here classiﬁer ﬁrst trained initial training set. classiﬁer used classify unlabeled items conﬁdently predicted unlabeled items added training prediction classiﬁer assumed label. similar soft k-means extension prototypical networks. indeed since soft assignments match regular prototypical network’s classiﬁer output inputs reﬁnement thought re-feeding prototypical network support augmented self-labels unlabeled set. algorithm also related transductive learning base classiﬁer gets reﬁned seeing unlabeled examples. practice could method transductive setting unlabeled query set; however avoid model memorizing labels unlabeled meta-learning procedure split separate unlabeled different query set. addition original k-means method related work setup involving clustering algorithms considers applying k-means presence outliers goal correctly discover ignore outliers wrongly shift cluster locations form partition true data. objective also important setup ignoring outliers wrongly shift prototypes negatively inﬂuence classiﬁcation performance. contribution semi-supervised learning clustering literature beyond classical setting training evaluating within single dataset consider setting must learn transfer training classes ctrain test classes ctest. evaluate performance model three datasets benchmark few-shot classiﬁcation datasets novel large-scale dataset hope useful future few-shot learning work. omniglot dataset handwritten characters alphabets. character drawn human subjects. follow few-shot setting proposed vinyals images resized pixels rotations multiples applied yielding classes total. split training classes validation classes testing classes. miniimagenet modiﬁed version ilsvrc- dataset images classes randomly chosen part dataset. rely class split used ravi larochelle splits classes training validation test. images size pixels. tieredimagenet proposed dataset few-shot classiﬁcation. like miniimagenet subset ilsvrc-. however tieredimagenet represents larger subset ilsvrc- analogous omniglot characters grouped alphabets tieredimagenet groups classes broader categories corresponding higher-level nodes imagenet hierarchy. categories total category containing classes. split training validation testing categories ensures training classes sufﬁciently distinct testing classes unlike miniimagenet alternatives randimagenet proposed vinyals example pipe organ training class electric guitar test class ravi larochelle split miniimagenet even though musical instruments. scenario would occur tieredimagenet since musical instrument high-level category split training test classes. represents realistic few-shot learning scenario since general cannot assume test classes similar seen training. additionally tiered structure tieredimagenet useful few-shot learning approaches take advantage hierarchical relationships classes. leave interesting extensions future work. dataset ﬁrst create additional split separate images class disjoint labeled unlabeled sets. omniglot tieredimagenet sample images class form labeled split. remaining used unlabeled portion episodes. miniimagenet data labeled split remaining unlabeled since noticed small achieve reasonable performance avoid overﬁtting. report average classiﬁcation scores random splits labeled unlabeled portions training uncertainty computed standard error would like emphasize labeled/unlabeled split using strictly less label information previously-published work datasets. this expect results match published numbers instead interpreted upperbound performance semi-supervised models deﬁned work. episode construction performed follows. given dataset create training episode ﬁrst sampling classes uniformly random training classes ctrain. sample images labeled split classes form support images unlabeled split classes form unlabeled set. optionally including distractors additionally sample classes training classes images unlabeled split distractors. distractor images added unlabeled along unlabeled images classes interest query portion episode comprised ﬁxed number images labeled split chosen classes. test episodes created analogously classes sampled ctest. experiments reported used i.e. classes labeled classes distractor classes. used training testing cases thus measuring ability models generalize larger unlabeled size. details dataset splits including speciﬁc classes assigned train/validation/test sets found appendices dataset compare three semi-supervised models baselines. ﬁrst baseline referred supervised tables ordinary prototypical network trained purely supervised labeled split dataset. second baseline referred semi-supervised inference uses embedding function learned supervised prototypical network performs semi-supervised reﬁnement prototypes test time using step soft k-means reﬁnement. contrasted semi-supervised models perform reﬁnement training time test time therefore learning different embedding function. evaluate model settings unlabeled examples belong classes interest challenging includes distractors. details model hyperparameters found appendix online repository. different values additional results comparing protonet model various baselines datasets analysis performance masked soft k-means model found appendix across three benchmarks least proposed models outperforms baselines demonstrating effectiveness semi-supervised meta-learning procedure. nondistractor settings three proposed models outperform baselines almost experiments without clear winner three models across datasets shot numbers. scenario training testing includes distractors masked soft k-means shows robust performance across three datasets attaining best results case one. fact model reaches performance close upper bound based results without distractors. figure observe clear improvements test accuracy number items unlabeled class grows models trained thus showing ability extrapolate generalization. conﬁrms that meta-training models learn acquire better representation improved semi-supervised reﬁnement. work propose novel semi-supervised few-shot learning paradigm unlabeled added episode. also extend setup realistic situations unlabeled novel classes distinct labeled classes. address problem current fewshot classiﬁcation datasets small labeled unlabeled split also lack hierarchical levels labels introduce dataset tieredimagenet. propose several novel extensions prototypical networks show consistent improvements semi-supervised settings compared baselines. future work working incorporating fast weights framework examples different embedding representations given contents episode. acknowledgement supported grants nserc samsung intelligence advanced research projects activity department interior/interior business center contract number dpc. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa doi/ibc u.s. government. jimmy geoffrey hinton volodymyr mnih joel leibo catalin ionescu. using fast advances neural information processing systems weights attend recent past. annual conference neural information processing systems december barcelona spain sanjay chawla aristides gionis. k-means– uniﬁed approach clustering outlier detection. proceedings siam international conference data mining siam deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee ville hautamäki svetlana cherednichenko ismo kärkkäinen tomi kinnunen pasi fränti. improving k-means outlier removal. scandinavian conference image analysis springer brenden lake ruslan salakhutdinov jason gross joshua tenenbaum. shot learning simple visual concepts. proceedings annual meeting cognitive science society cogsci boston massachusetts july olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. international journal computer vision oriol vinyals charles blundell lillicrap koray kavukcuoglu daan wierstra. matching networks shot learning. advances neural information processing systems used following split details experiments omniglot dataset. train/test split created validation split selecting hyper-parameters. models trained train split only. train alphabets alphabet_of_the_magi angelic anglo-saxon_futhorc arcadian asomtavruli_ atemayar_qelisayer atlantean aurek-besh avesta balinese blackfoot_ cyrillic futurama ge_ez glagolitic grantha greek gujarati gurmukhi inuktitut_ japanese_ japanese_ korean latin malay_ n_ko ojibwe_ sanskrit syriac_ tagalog tiﬁnagh validation alphabets armenian bengali early_aramaic hebrew mkhedruli_ test alphabets gurmukhi kannada keble malayalam manipuri mongolian old_church_slavonic_ oriya sylheti syriac_ tengwar tibetan ulog high-level category tieredimagenet contains ilsvrc- classes imagenet hierarchy classes multiple parent nodes. therefore classes belonging category removed dataset ensure separation training test categories. test categories chosen reﬂect various levels separation training test classes. test categories fairly similar training categories whereas others quite different. list categories shown statistics dataset found table visualization categories according imagenet hierarchy shown figure full list classes category also made public however sake brevity include here. table few-shot learning baseline results using labeled/unlabeled splits. baselines either takes inputs directly pixel space extract features. denotes using randomly initialized denotes using pretrained supervised classiﬁcation training classes. provide baseline results few-shot classiﬁcation using -nearest neighbor logistic regression either pixel inputs features. compared baselines regular protonet performs signiﬁcantly better three few-shot classiﬁcation datasets. figure shows test accuracy values different number unlabeled items test time. figure shows mask output value distribution masked soft k-means model omniglot. mask values bi-modal distribution corresponding distractor non-distractor items. omniglot adopted best hyperparameter settings found ordinary prototypical networks snell settings learning rate half every updates starting update trained total updates. miniimagenet tieredimagenet trained starting learning rate also decayed. started decay updates every updates thereafter half. trained total updates. used adam optimization models. used masked soft k-means model single hidden layer hidden units tanh non-linearity datasets. tune hyparameters better performance attained rigorous hyperparameter search. fromilsvrc-.bestviewedzoomed-inonelectronicversion. figurehierarchyoftieredimagenetcategories.trainingcategoriesarehighlightedinredandtestcategoriesinblue.eachcategoryindicatesthenumberofassociatedclasses", "year": 2018}