{"title": "Fast Training of Convolutional Networks through FFTs", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.", "text": "convolutional networks widely employed architectures computer vision machine learning. order leverage ability learn complex functions large amounts data required training. training large convolutional network produce state-of-the-art results take weeks even using modern gpus. producing labels using trained network also costly dealing web-scale datasets. work present simple algorithm accelerates training inference signiﬁcant factor yield improvements order magnitude compared existing state-of-the-art implementations. done computing convolutions pointwise products fourier domain reusing transformed feature many times. algorithm implemented architecture addresses number related challenges. computer vision machine learning solve increasingly challenging tasks models greater complexity required. turn requires orders magnitude data take advantage powerful models avoiding overﬁtting. early benchmark datasets machine learning contained thousands tens thousands samples current datasets order millions brings challenges train networks feasible amount time. even using parallel computing environments training network imagenet take weeks addition although inference labels using trained network comparatively fast real-world applications producing labels images internet represent signiﬁcant cost terms time resources. therefore important need develop fast algorithms training inference. work present simple algorithm accelerates training inference using convolutional networks. idea based performing convolutions products fourier domain reusing transformed feature maps many times. signiﬁcant operations training convolutional networks viewed convolutions pairs matrices represent input output feature maps gradients loss respect feature maps weight kernels. typically convolutions performed pairings sets matrices. computing fourier transforms matrices once efﬁciently perform convolutions pairwise products. although long known convolutions computed products fourier domain recently number feature maps used convolutional networks small make method like effective. previous work explored possibility using ffts accelerate inference ﬁrst layer trained network fourier transforms ﬁlters could precomputed ofﬂine. however used training possibly number feature maps used time small make overhead computing ffts every iteration worthwhile. number feature maps large case modern convolutional networks using ffts accelerates training inference signiﬁcant factor lead speedup order magnitude. backpropagation algorithm standard method compute gradient training convolutional network. training layer performs three tasks describe. first notation given layer input feature maps indexed image dimensions output feature maps indexed also images whose dimension depends convolutional kernel stride. layer’s trainable parameters consist weights small kernel dimensions forward pass output feature computed input feature maps convolved corresponding trainable weight kernel step necessary computing gradients previous layer. finally gradients loss respect weight computed convolving input feature gradients respect outputs well-known convolution theorem states circular convolutions spatial domain equivalent pointwise products fourier domain. letting denote fourier transform inverse compute convolutions functions follows method requires operations. complexity fft-based method requires operations requires pointwise product frequency domain requires represents hidden constant notation. algorithm based observation operations matrices indexed convolved matrices indexed therefore compute matrix once pairwise convolutions performed products frequency domain. even though using fft-based method less efﬁcient given convolution effectively reuse ffts many times compensates overhead. following analysis makes idea precise. assume input feature maps output feature maps images consisting pixels kernels pixels. also assume performing updates minibatches size represents hidden constant complexity. example using direct approach take total s·f·f operations. approach requires operations transform input feature maps kernels fourier domain total additions multiplications fourier domain operations transform output feature maps back spatial domain. analysis yields similar complexity estimates operations represents size output feature map. note high complexity direct method convolution comes product terms whereas method products four terms. figure shows theoretical number operations direct convolution method various input sizes. although conceptually straighforward number challenges relating implementation needed addressed. first current implementations cufft designed parallelize individual transforms. useful computing limited number transforms large inputs suitable task since performing many ffts since fft-based method actually computing circular convolution output cropped discard coefﬁcients kernel completely contained within input image. yields output size direct method require additional computation. relatively small inputs. therefore developed custom cuda implementation cooleytukey algorithm enabled parallelize feature maps minibatches within transform. note ffts lend naturally parallelization since decomposed sets ffts done parallel. second additional memory required store feature maps fourier domain. note keeping fourier representations memory layers forward pass could avoid recomputing several ffts backward pass. however might become prohibitively expensive terms memory large networks. therefore reuse memory different convolutions network necessary amount memory determined largest convolution layer. analysis previous section experiments remainder paper assume using memory-efﬁcient approach. convolution layer taking input size input features output features minibatch size need store total frequency representations size another means save memory symmetry properties ffts real inputs store half data i.e. complex numbers. assuming ﬂoat representations necessary memory bytes test analysis series experiments comparing method cudaconv implementation custom implementation using torch machine learning environment implementations compute convolutions using direct method spatial domain. experiments performed geforce titan gpu. began performing unit tests comparing results convolutions computed method computed torch implementation three operations. found differences results operations order operation order differences likely rounding errors ﬂoating-point operations within acceptable range. compared method performed terms speed varying kernel sizes input sizes minibatch sizes. results shown figure experiments chose input feature maps output feature maps represents typical conﬁguration deep network’s second layer. functions updateoutput updategradinput accgradparameters correspond operations respectively. times measured seconds. method signiﬁcantly outperforms nearly cases. improvement especially pronounced accgradparameters operation computationally expensive. likely fact convolution computing large kernel ffts better suited case. also note method performs regardless kernel size since kernel size input image applying fft. enables much larger kernels intend explore future work. next experiments parameter conﬁgurations typical used different layers large convolutional network. time taken different methods given milliseconds. -tuple indicating width kernel width input image number input feature maps number output feature maps. kernels input images square size respectively. conﬁgurations minibatches size ﬁrst conﬁguration represents ﬁrst layer report times updategradinput operation. conﬁguration best-performing method highlighted bold. fft-based method performs faster total conﬁgurations sometimes substantial degree. improvement signiﬁcant forward pass makes method especially well suited inference large datasets using trained network. finally tested times taken perform training iteration network obtained composing layers inserting max-pooling rectiﬁed linear units them adding fully connected layer prediction outputs. account possible changes performance implementation details padding accessing memory following table shows results milliseconds presented simple fast algorithm training inference using convolutional networks. outperforms known state-of-the-art implementations terms speed veriﬁed numerical experiments. future plan explore possibility learning kernels directly fourier domain. another interesting direction would investigate non-linearities fourier domain rather spatial domain since would remove need inverse transforms accelerate training inference further. worth mentioning current implementation algorithm input images power must padded next highest power. example using input images size suboptimal terms speed since must padded limitation intrinsic intend extend implementation accept sizes future. hand fact method’s speed invariant kernel size enables larger kernels different layers network. future work intend thoroughly explore effect input image kernel sizes performance.", "year": 2013}