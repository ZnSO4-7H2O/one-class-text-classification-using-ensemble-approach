{"title": "Adaptive Simulation-based Training of AI Decision-makers using Bayesian  Optimization", "tag": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "abstract": "This work studies how an AI-controlled dog-fighting agent with tunable decision-making parameters can learn to optimize performance against an intelligent adversary, as measured by a stochastic objective function evaluated on simulated combat engagements. Gaussian process Bayesian optimization (GPBO) techniques are developed to automatically learn global Gaussian Process (GP) surrogate models, which provide statistical performance predictions in both explored and unexplored areas of the parameter space. This allows a learning engine to sample full-combat simulations at parameter values that are most likely to optimize performance and also provide highly informative data points for improving future predictions. However, standard GPBO methods do not provide a reliable surrogate model for the highly volatile objective functions found in aerial combat, and thus do not reliably identify global maxima. These issues are addressed by novel Repeat Sampling (RS) and Hybrid Repeat/Multi-point Sampling (HRMS) techniques. Simulation studies show that HRMS improves the accuracy of GP surrogate models, allowing AI decision-makers to more accurately predict performance and efficiently tune parameters.", "text": "graduate researcher computer science aiaa student member assistant professor aerospace engineering sciences aiaa member director orbit logic incorporated aiaa professional member software engineer orbit logic incorporated division technical advisor force research laboratory warﬁghter readiness research division consequently specialized class optimization needed handle inherently noisy data; identify best local minima respect reduce number evaluations needed minimum bayesian optimization goal optimization minimize objective function search solution space element minimizer typically solution space bounded global optimization lower updated subsequent observations evidence consisting sample evaluations diﬀerent values mathematically leads application bayes’ rule detail). quantity also known observation likelihood posterior i.e. updated posterior belief formed. long prior likelihood consistent true nature large numbers ensures posterior converges high probability true limit inﬁnite observations covering statistically approximates easier evaluate optimization used determine next point sample evaluation occur order update beliefs thus simultaneously improve ﬁnding sampled diﬀerent locations samples eventually converge expected minimizer since contains statistical information level uncertainty posterior belief) bayesian optimization eﬀectively leverages probabilistic surrogate model bayesian optimization focus work here. especially useful continuous optimization problems relatively dimensionality henceforth acronym gpbo refers bayesian optimization using surrogate model optimum reﬂects promising location next function evaluation occur. formally acquisition function deﬁned equation general surrogate model replaced gaussian process reﬂect model used gpbo. columns fig. show successive iterations gpbo algorithm; bottom rows show respectively. case seed points randomly selected surrogate points shown left plot along associated since integral general analytically intractable approximation laplace method approxischemes like laplace method used. mated multivariate normal estimate poorly noise outliers thus converge poor local optimum space and/or produce incorrect surrogate overﬁts data. resulting model accurately reﬂect stochastic objective function known kernel function true hyperparameter values performing gpbo another surrogate model kernel function learns estimate hyperparameters noisy data sampled since statistically consistent thus impact using function evaluation data. fifteen random data points used construct approximate ‘initial seed’ surrogate model using kernel function estimated number sample points added using either", "year": 2017}