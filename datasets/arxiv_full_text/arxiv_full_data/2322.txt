{"title": "Rapid Feature Learning with Stacked Linear Denoisers", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We investigate unsupervised pre-training of deep architectures as feature generators for \"shallow\" classifiers. Stacked Denoising Autoencoders (SdA), when used as feature pre-processing tools for SVM classification, can lead to significant improvements in accuracy - however, at the price of a substantial increase in computational cost. In this paper we create a simple algorithm which mimics the layer by layer training of SdAs. However, in contrast to SdAs, our algorithm requires no training through gradient descent as the parameters can be computed in closed-form. It can be implemented in less than 20 lines of MATLABTMand reduces the computation time from several hours to mere seconds. We show that our feature transformation reliably improves the results of SVM classification significantly on all our data sets - often outperforming SdAs and even deep neural networks in three out of four deep learning benchmarks.", "text": "investigate unsupervised pre-training deep architectures feature generators shallow classiﬁers. stacked denoising autoencoders used feature pre-processing tools classiﬁcation lead signiﬁcant improvements accuracy however price substantial increase computational cost. paper create simple algorithm mimics layer layer training sdas. however contrast sdas algorithm requires training gradient descent parameters computed closed-form. implemented less lines matlabtmand reduces computation time several hours mere seconds. show feature transformation reliably improves results classiﬁcation signiﬁcantly data sets often outperforming sdas even deep neural networks three four deep learning benchmarks. recently great deal attention deep-learning architectures architectures consistently achieved state-of-the-art results many challenging learning tasks including object recognition natural language processing dimensionality reduction typical paradigm deep-learning ﬁrst perform unsupervised pre-training neural network initialize weights back-propagation supervised training. showed second phase supervised back-propagation replaced shallow classiﬁers support vector machines concretely outputs hidden units pre-trained deep neural networks used input features classiﬁers. besides achieving superior performance recognition tasks substituting classiﬁers offer appealing computational properties. particular parameters adjusted convex optimization free local optima often plague back-propagation based techniques. further classiﬁers tend ready used out-of-the-box often parallelized effectively. replace pre-training phase equally attractive learning model? note pre-training phrase merely used unsupervised feature generation major bottleneck applying deep learning architectures. needs adjust several parameters network architectures optimization etc. compounded factors pre-training phase takes large portion overall training time even multi-core processors graphical processing units propose method pre-training thus feature generation investigate effectiveness paper. show one-layer linear denoising autoencoders used basic building blocks pre-training phases. autoencoders ordinary linear regression models reconstructing data corrupted features. closed-form solutions thus easy implement. particular parameters identiﬁed matrix inversion nonlinear optimization needed. outputs linear denoising autoencoders ways input features support vector machines; inputs successively stacked linear autoencoders. proposed method refer stacked linear denoiser similar spirit stacked denoising autoencoders however important differences. first hidden layers used extract representations inputs makes training nonconvex optimization. contrast slide hidden layers convex. second requires setting several crucial meta-parameters including noise level learning rates number training epochs network architecture speciﬁcs typically cross-validation. comparison slide free meta-parameters controlling amount noise added data well number autoencoders would like stack. finally leveraging analytic tractability linear regression train parameters autoencoders optimally denoise possible corrupted training inputs arguably inﬁnitely many. practically feasible whose parameters adjusted subset corrupted data. paper make several contributions. slide implemented easily learning algorithm runs fast. fact -lines implementation three orders magnitudes faster highly optimized parallel implementation state-ofthe-art gpu. even large data sets tens thousands samples computation takes seconds achieving thousand-fold speed-up pre-training phase similar approaches. finally addition vastly reduced computation time demonstrate several deep-learning benchmark data sets classiﬁcation slide features tends even accurate features pre-trained deep neural networks back-propagation. following introduce notation algorithms used rest paper. training data consist input vectors corresponding discrete class labels drawn unknown joint distribution deep architecture. deep learning algorithms learn hierarchies hidden layers output lower level layer becomes input higher level. deep learning algorithms differ traditional neural networks ways tend hidden layers supervised training back-propagation preceded unsupervised pre-training step weights initialized generative manner. additional layers believed provide powerful learning models. pre-training makes training efﬁcient regularizes optimization back-propagation starts near good local minima support vector machines popular reliable out-of-the-box supervised classiﬁcation algorithms. svms linear classiﬁers involve quadratic minimization problem convex plagued local optima. maximum margin separation promotes reliably good generalization kernel-trick allows svms generate highly non-linear decisions boundaries computational overhead. kernel-trick maps input vectors implicitly higher dimensional feature space using kernel function among various functions radial basis function -kernel commonly used kernels. used euclidean distances traditional autoencoder described maps input data visible features hidden representations hidden representations mapped back reconstructions original input denoising autoencoder incorporates slight modiﬁcation traditional autoencoders. input picks ﬁxed percentage features uniformly random sets zero keeping others untouched. maps corrupted input hidden representations mapped back reconstruct original uncorrupted input minimizing stacked denoising autoencoder stacks several together feeding hidden representations input training performed greedily layer layer layer trained layers ...i ﬁxed noise added hidden nodes layer. intuitively forcing removed features reconstructed remaining data learns convolute features tend correlated. increases robustness noise local transformations e.g. small translation rotation. similar approach successfully used many years convolutional neural networks leverage fact natural images local pixels highly correlated hard-coded network structure. general learn convolution patterns applied data sets feature correlation unknown however sdas suffer inherent down-sides long training time sensitivity several hyper-parameters network architecture learning rates etc. carefully tuning parameters validation datasets often time consuming. pre-training feature generator. many researchers noticed pre-training phase deep learning networks seen kind nonlinear feature mappings. example showed hidden representations computed either partial layers stacked denoising autoencoders make excellent features classiﬁcation support vector machines. introduced recursively deﬁned kernels mimic pre-training deep feature extractors support vector machines. work follows line thinking shows simpler computationally tractable models also used similar purposes. ˜xij represents corrupted version original input input corrupted setting feature randomly zero probability simplify notation assume constant feature added input appropriate bias incorporated within mapping constant feature never corrupted. notational simplicity deﬁne m-times repetitions design matrices rd×nm. corrupted equivalent i.e. deﬁne outer-product matrices virtual denoising. larger different corruptions average over. ideally would like effectively using inﬁnitely many copies noisy data compute denoising transformation weak large numbers matrices deﬁned converge expected values becomes large. interested limit case derive closed-form expectations express corresponding mapping remainder section compute expectations focus whose expectation deﬁned off-diagonal entries uncorrupted features survived corruption happens probability diagonal entries holds probability deﬁne vector represents probability feature surviving corruption. constant feature never corrupted deﬁne scatter matrix original uncorrupted input express expectation matrix analogous reasoning obtain expectation closed-form sαβqβ. refer linear transformation linear denoiser algorithm shows -lines matlabtmimplementation. success sdas attributed fact learn deep internal representation. lide consists linear transformation therefore cannot compete terms feature expressiveness. inspired layer-wise stacking dbns stack several lide layers together feeding representations denoising layer input layer. training performed greedily layer layer layer trained layers ...l ﬁxed means learn denoising matrix wk+. given input denote output lide transformation. notational simplicity denote able move beyond linear transformation need apply non-linear squashingfunction layers. several choices might possible including sigmoid tanh. however experiments simply threshold function δa>t apply element-wise vectors. obtain layer’s representation previous layer transformation i.e. threshold input denoising transformation. analogously transformation learned minimizing denoising reconstruction error previous lide output layers slide computed regard features classiﬁcation. simplicity kernel throughout paper. kernel described accesses individual inputs pairwise distances. however representation average distance data points vary signiﬁcantly. concatenating hidden layers using single kernel width across input features would over-emphasize impact layers setup learn individual method described following subsection. kernel parameters learning. features different layers might varying utility ﬁnal discrimination task. suspect exact values might noticeable inﬂuence classiﬁcation accuracy. computation time cross-validation grows exponentially number parameters investigate beneﬁts learning best values automatically data. propose straight-forward effective algorithm learn multiple kernel parameters support vector machines gradient descent. chapelle’s publicly available code trivial modiﬁcation learn would like emphasize whole time required training optimizing individual kernel widths took order minutes even larger data sets. code implemented binary settings apply technique multi-class dataset also kernel parameter learning rbfs input setting cross validation yielded better results. important note kernel parameter learning equally applies slide. evaluate algorithm several data sets deep-learning benchmark collection including mnist handwriting digits recognition dataset rectangles-images rectangles convex data set. datasets. mnist data consists training testing images handwritten digits image size pixels. learning task predict digit identity image. rectangles dataset training testing images also size image contains rectangle whose border pixel value pixels learning task determine whether rectangle larger width length. rectangles-images dataset motivated learning task except background rectangle created noisy image patches. also training testing images. last dataset evaluation convex dataset. consists training testing images size images made binary pixels task determine whether white pixels given image form single convex region. weight matrices. figure visualizes reconstruction weights four example pixels rectangles rectangles-images convex mnist data various layers slide. image shows color reﬂects weight value. ﬁgure shows clear trend towards fuzzier reconstruction depth increases. rectangles data sets pixel reconstructed neighboring pixels tendency towards vertical horizontal offset thus incorporating inherent structure rectangles data. clarity trend particularly impressive rectangles-images data rectangles consist noisy image patches. convex mnist data sets pixel reconstructed small circular patch surrounding pixels. case mnist small non-zero weights also exist distant corner locations originating fact pixels non-zero images training data set. experiments settings. data sets training testing splits pre-deﬁned. create additional random split training purpose parameter tuning. http//olivier.chapelle.cc/ams/ each iteration take gradient-step respect kernel widths instead global http//tinyurl.com/fgmzv http//yann.lecun.com/exdb/mnist/ figure reconstruction weights different layers rectangles rectangles-images convex mnist data set. reconstructed pixel correlated therefore marked dark red. cruciform lines illustrate de-noising weights adapted shape rectangles. convex mnist data show spherical neighborhood structure obvious neighborhood pattern inherent data. best combination noise level de-noising layers cross-validation validation set. features across data sets naturally within interval threshold parameter experiments. described previous section used kernel parameter learning svms kernel classiﬁcation refer comparison evaluate three methods svms kernel input cross-validation select kernel width regularization trade-off mentioned previous section. further deep neural networks pre-trained ﬁne-tuned back-propagation. network architecture noise level learning rates recommended authors personal communication. also hidden representations input exact fashion slide-svm. finally linear transformation also evaluated whitening random projection whitening reduce feature dimension keeping percent variance selected cross-validation. parameter sensitivity. figure displays classiﬁcation error rectangles data function noise level four colored lines correspond different depths correspond original input processed slide. general slide appears somewhat sensitive exact choice choose cross validation experiments. clear trend deep layers improve single layered transformation experiments results. classiﬁcation results shown table general trends observed. first using slide feature pre-processing yields considerable improvements results original features data sets. fact transforming features slide makes outperform even deep neural nets learning tasks. finally slide even outperforms much complex features data sets obtains equivalent results mnist. especially convex rectimages data sets slide clearly outperforms algorithms. results also signiﬁcantly better original features best convex dataset worse slide others. whitening tops rectangle dataset yield signiﬁcant improvements datasets. results encouraging slide trivial implement orders magnitudes faster sda. running time. table compares running times feature generation slide. timings performed desktop dual inteltmsix core xeon .ghz processors. train highly optimized theano open-source package carefully parallelized experiments utilizes state-of-the-art additional cores. explicit parallelization used slide. results show three orders magnitude speed-up across data sets reducing pre-training time several hours seconds. related work categorized three lightly correlated dimensions layer-wised unsupervised pre-training learning partially corrupted data linking svms neural networks. area layer-wised unsupervised pre-training provide various successful approaches. propose pre-train neural network unsupervised training criterion initialization back-propagation features algorithms contrast method require training gradient descent orders magnitudes faster. carefully constructed random projections create features approximate kernelization linear svms. area learning partially corrupted data proposes using stacked denoising auto-encoder reconstruct corrupted data demonstrates learnt representations robuster. method heavily inspired work viewed convex closed-form transformation mimics feature generator. also investigate linking neural networks svms deep kernels. work construct recursively composed positive semi-deﬁnite kernel functions viewed mimicking layer layer training neural networks. different work method based feature de-noising still requires computationally expensive ﬁne-tuning distance metric learning introduced slide novel algorithm based stacked linear denoising autoencoders extremely fast layer-wise deep pre-training feature generation. derived simple closed-form solution implemented lines matlabtm. demonstrated slide classiﬁcation match classiﬁcation results features three four data even beyond error rates deep neural networks. notably running time slide reliably order seconds even data sets tens thousands data points. future directions plan investigate classiﬁers different learning settings. slide entirely unsupervised lends naturally towards semi-supervised transfer learning tasks. slide straightforward implement takes seconds compute improves results classiﬁcation surprising consistency great hopes general pre-processing algorithm across many areas machine learning.", "year": 2011}