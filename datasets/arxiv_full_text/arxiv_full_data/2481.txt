{"title": "A Sparse and Adaptive Prior for Time-Dependent Model Parameters", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We consider the scenario where the parameters of a probabilistic model are expected to vary over time. We construct a novel prior distribution that promotes sparsity and adapts the strength of correlation between parameters at successive timesteps, based on the data. We derive approximate variational inference procedures for learning and prediction with this prior. We test the approach on two tasks: forecasting financial quantities from relevant text, and modeling language contingent on time-varying financial measurements.", "text": "consider scenario parameters probabilistic model expected vary time. construct novel prior distribution promotes sparsity adapts strength correlation parameters successive timesteps based data. derive approximate variational inference procedures learning prediction prior. test approach tasks forecasting ﬁnancial quantities relevant text modeling language contingent time-varying ﬁnancial measurements. learning streams data make predictions future handle timestamp associated instance? ignoring timestamps assuming data i.i.d. scalable risks distracting model irrelevant ancient history. hand using recent portion data risks overﬁtting current trends missing important timeinsensitive effects. paper seek general approach learning model parameters overall sparse adapt variation different effects change time. approach prior parameters exponential family assume parameter values shift timestep correlation adjacent timesteps captured using multivariate normal distribution whose precision matrix restricted tridiagonal structure. marginalize variance parameters normal distribution using jeffreys prior resulting model allows smooth variation time encouraging overall sparsity parameters. many related bayesian approaches time-varying model parameters well work time-varying signal estimation model distinctive generative story correlations parameters successive timesteps encoded precision matrix. additionally unlike fully bayesian approaches infer full posterior distributions obtain posterior mode estimates coefﬁcients computational advantages prediction time demonstrate usefulness model tasks. ﬁrst text regression problem economic variable forecast ﬁnancial reports second forecasts text constructing language model conditions highly timedependent economic variables. focus paper prior distribution throughout denote taskspeciﬁc log-likelihood assume generalized linear model feature vector function maps inputs linked distribution using e.g. logit identity. refer elements features coefﬁcients. assume discrete timesteps. time-series prior draws inspiration probabilistic interpretation sparsity-inducing lasso group lasso non-overlapping group lasso features divided groups coefﬁcients within group drawn according seek prior lets coefﬁcient vary smoothly time. high-level intuition prior create copies timestep feature sequence form group denoted group lasso view coefﬁcients group explicitly correlated; independent given variance parameter. given sequential structure replace covariance matrix capture autocorrelation. speciﬁcally assume vector drawn multivariate normal distribution mean zero precision matrix following tridiagonal form scalar multiplier whose role control sparsity coefﬁcients dictates degree correlation coefﬁcients adjacent timesteps importantly allowed different group design choice precision matrix driven scalability concerns. instead using e.g. random draw wishart distribution specify precision matrix tridiagonal structure. induces dependencies coefﬁcients adjacent timesteps allows prior scale ﬁne-grained timesteps efﬁciently. denote number training instances number base features number timesteps. single pass variational algorithm runtime space requirement instead drawn wishart distribution. make difference applications large numbers features additionally choose off-diagonal entries uniform need base feature. design choice restricts expressive power prior still permits ﬂexibility adapting trends different coefﬁcients see. prior encourages sparsity group level essentially performing feature selection feature coefﬁcients driven zero across timesteps others allowed vary time expectation smooth changes. exact inference model intractable. mean-ﬁeld variational inference derive lower bound log-likelihood function. apply standard optimization technique jointly optimize variational parameters coefﬁcients supplementary materials details. ﬁnance volatility refers measure variation quantity time; stock returns measured using standard deviation ﬁxed period volatility used measure ﬁnancial risk. consider linear regression model predicting volatility stock features interpret linear regression model probabilistically drawing normal distribution mean normal. therefore experiment table dataset. baselines ridge lasso indicates regularizer used indicates amount training data used. ridge-ts non-adaptive time-series ridge model yogatama supplementary materials details baselines. overall differences model competing models statistically signiﬁcant used collection securities exchange commission-mandated annual reports publicly traded companies u.s. reports period years corpus. reports known form feature downcased tokenized texts selected st–st frequent words binary features. feature kept across experiments models. widely known ﬁnancial community past history volatility stock returns good indicator future volatility. therefore also included volatility stocks twelve months prior report feature. response variable volatility stock returns period twelve months report published. year used development data hyperparameter tuning initialized feature coefﬁcients coefﬁcients training lasso regression last year training data compare baselines vary training data regularize table provides summary experimental results. report results mean squared error test model consistently outperformed ridge variants including time-series penalty note ridge-ts obtained model ﬁxing features model also outperformed lasso variants without time-series penalty average three test sets apiece. major challenges working time-series data choose right window size data still relevant current predictions. model automates process bayesian treatment strength feature coefﬁcient’s autocorrelation. results indicate model able learn trust longer history training data trust shorter history training data demonstrating adaptiveness prior. presented time-series prior parameters probabilistic models; produces sparse models adapts strength temporal effects coefﬁcient separately based data without explosion number hyperparameters. showed inference prior using variational approximations. evaluated prior task forecasting volatility stock returns ﬁnancial reports demonstrated outperforms competing models. also evaluated prior task modeling collection texts time i.e. predicting probability words given observed real-world variables. showed prior achieved state-of-the-art results well. authors thank anonymous reviewers helpful feedback earlier drafts paper. research supported part google research award second third authors. research supported part intelligence advanced research projects activity department interior national business center contract number dpc. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa doi/nbc u.s. government.", "year": 2013}