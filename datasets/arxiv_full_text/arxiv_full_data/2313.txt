{"title": "A Multi-Objective Deep Reinforcement Learning Framework", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper presents a new multi-objective deep reinforcement learning (MODRL) framework based on deep Q-networks. We propose linear and non-linear methods to develop the MODRL framework that includes both single-policy and multi-policy strategies. The experimental results on a deep sea treasure environment indicate that the proposed approach is able to converge to the optimal Pareto solutions. The proposed framework is generic, which allows implementation of different deep reinforcement learning algorithms in various complex environments. Details of the framework implementation can be referred to http://www.deakin.edu.au/~thanhthi/drl.htm.", "text": "abstract paper presents multi-objective deep reinforcement learning framework based deep q-networks. propose linear non-linear methods develop modrl framework includes single-policy multi-policy strategies. experimental results deep treasure environment indicate proposed approach able converge optimal pareto solutions. proposed framework generic allows implementation different deep reinforcement learning algorithms various complex environments. details framework implementation referred http//www.deakin.edu.au/~thanhthi/drl.htm. introduction multi-objective reinforcement learning studies relatively simple gridworld tasks extending current algorithms sophisticated function approximation important order allow applications complex problem domains. inefficient impractical environment’s state space large. example tabular q-learning cannot solve deep treasure problem environment comprises columns requires least memory deal with. deep reinforcement learning approaches possible solutions overcome problem memory required store neural network experience replay. addition problems non-linear scalarization exist morl using tabular q-values simple function approximation methods like tile-coding actually less problematic context deep methods. small amount prior work investigating deep methods morl henceforth multi-objective deep reinforcement learning problems. therefore standard benchmarks emerged. mossalam extended deep q-network handle single-policy linear morl. address multi-policy task finding convex coverage embedding algorithm within outer loop method identifies weight vectors training establish ccs. used small gridworld tasks different fashions test problems modrl. first provided underlying discrete continuous state information directly information low-dimensional capacity essentially overkill tasks. second approach better evaluation modrl methods visualization environment generate image input dnn. show efficiencies achieved retaining parts outer loop changes weights. overall method addressing multi-policy linear morl problem sequential rather parallel learning policies. tajmajer also extended used non-linear action selection approach based subsumption architecture. prioritized ordering objectives specified higher priority objectives ‘supress’ q-values associated lower-priority objectives. suppression values state-dependent whole system essentially performs dynamic state-dependent linear weighting q-values whenever action selected. work addresses singlepolicy non-linear morl problem manner tied specific form nonlinear action selection. vamplew developed morl framework based rl_glue called morl_glue implemented benchmark environments several tabular tile-coding morl algorithms. however framework currently provides support passing state information form vectors integer continuous values allow passing images. addition implementation environments generate image-based representations state. importantly framework support deep learning algorithm implementation e.g. variants. paper proposes benchmark python framework supports single-policy multi-policy approaches solving modrl problems. implementation deep networks based tensorflow deep learning library google proposed framework flexible supports vector rewards multiple objectives. importantly framework accept state representations able take image state input convolutional layers. practice agents camera capture environment’s state feed graphics learning algorithm. contrast tabular qlearning cannot accept images encoded data large. therefore integration deep methods morl problems critical. next section describes single-policy multi-policy approaches implemented proposed framework. note framework generic modification approaches implemented efficiently. modrl development single-policy linear approach straightforward extension modrl. involves learning single-policy based linear scalarization objectives using fixed weights. equivalent learning optimal policy single-objective markov decision process objectives pre-scalarized single reward. framework based agent receives vector rewards time step scalar value. addition agent provided fixed weight vector indicating relative desirability dqn. given weight vector discounted rate denote current state next problem domains linear methods inadequate accurately easily express desired trade-off objectives non-linear methods thresholded lexicographic ordering preferable framework problem domains linear methods unable find optimal policies propose multi-policy choice weights linear scalarization intended represent desirable trade-off different objectives. many problems user’s preferences objectives change time. single-policy approach described subsection requires agent re-learn policy whenever weights change introduce unwarranted delays responding changes particularly agent operating real-time context. framework implemented multi-threads allow multiple agents learn parallel multiple policies optimal policy advance possible weights might encounter. immediately adapt behaviour informed change weights. experiments results several benchmark morl problems deep treasure mopuddleworld mo-mountain-car resource gathering study testify proposed modrl framework using problem demonstration purpose. takes advantages predefined pareto solutions becomes normative environment verify methods. state output scalar graphical representation therefore allows general evaluation methods using scalar image inputs. three grid environments illustrated fig. consisting rows columns respectively. agent designed control submarine searches treasure sea. objectives need optimized maximum treasure values minimize searching time. submarine starts episode left state ends finds treasure location predefined maximum number action reached. four actions including move down left right available agent. agent receives reward characterized -element vector representing treasure value time penalty. treasure value unless agent reaches treasure location. move returns time penalty. pareto fronts including nondominated policies corresponding three environments demonstrated fig. parameters initial epsilon final epsilon steps epoch learning rate gamma target network update root mean square optimizer width environment epsilon annealing steps experience replay size warmup steps training epochs configuration details network includes hidden layers activated relu function. output units corresponding number possible actions. case graphical states hidden layers replaced convolutional layers. network settings used experiments presented table demonstration results following subsections present results experiments -column environment including single policy multi-policy dqn. demonstration results experiments using single policy -column -column environments presented appendix single policy linear fig. shows convergence dqn-based modrl framework applied -column environment. solution found steps using linear scalarization weights -column environment. -axis represents values objectives learning process whilst -axis show number steps agent gone through. -column environment possible pareto rewards linear approach direct algorithm find solution using weights solution using weights impossible find second solution weights therefore non-linear approach combines linear scalarization used. first threshold truncate rewards first objective. example applying threshold first objective rewards means rewards greater truncated accordingly resulted rewards then linear scalarization weights applied guide agent solution fig. learning process converges solution -column environment threshold equal average threshold applied first objective linear scalarization weights employed cut-off. fig. history difference actual pareto front approximation front learned agent. around steps agent successfully found solutions therefore difference converged zero. multi-policy framework developed multiple agents trained parallel multiple threads. means agent responsible finding individual optimal policy. therefore efficient select suitable policy required goal changes real-world applications. fig. convergence process three agents running find three solutions parallel -column environment; solution resolved threading agent represents solution; corresponds represents fig. history difference actual pareto front front approximated three parallel agents -column environment. difference converges zero around steps approximately twice faster case agent continuously finds three solutions presented fig. conclusions work paper modrl framework proposed implementation using python demonstrated. integration algorithms traditional morl methods important traditional methods like tabular q-learning able deal high-dimensional complex environments. proposed modrl framework facilitates single-policy multi-policy strategies solving morl problems efficiently. importantly framework generic able accommodate different algorithms e.g. dual double unreal various environments e.g. gridworlds atari mujoco entails future researches expand proposed modrl framework. another work focus developing multi-agent environments integrated current framework solve various problems multi-agent-based systems. environment requires agent find optimal policies actual pareto front including following figures illustrate convergence learning process optimal solutions. first solution i.e. found linear scalarization remaining solutions found nonlinear method. seven pareto solutions need found including following figures demonstrate convergence proposed modrl framework mentioned solutions. last figure shows history difference actual pareto front front approximated agent learning process. abadi barham chen chen davis dean kudlur tensorflow system large-scale machine learning. proceedings usenix conference operating systems design implementation usenix association. gábor kalmár szepesvári multi-criteria reinforcement learning. proceedings fifteenth international conference machine learning morgan kaufmann publishers inc.. issabekov vamplew empirical comparison common multiobjective reinforcement learning algorithms. australasian joint conference artificial intelligence springer berlin heidelberg.", "year": 2018}