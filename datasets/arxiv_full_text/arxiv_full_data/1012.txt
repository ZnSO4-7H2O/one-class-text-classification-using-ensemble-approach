{"title": "Variational Autoencoders for Feature Detection of Magnetic Resonance  Imaging Data", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Independent component analysis (ICA), as an approach to the blind source-separation (BSS) problem, has become the de-facto standard in many medical imaging settings. Despite successes and a large ongoing research effort, the limitation of ICA to square linear transformations have not been overcome, so that general INFOMAX is still far from being realized. As an alternative, we present feature analysis in medical imaging as a problem solved by Helmholtz machines, which include dimensionality reduction and reconstruction of the raw data under the same objective, and which recently have overcome major difficulties in inference and learning with deep and nonlinear configurations. We demonstrate one approach to training Helmholtz machines, variational auto-encoders (VAE), as a viable approach toward feature extraction with magnetic resonance imaging (MRI) data.", "text": "abstract. independent component analysis approach blind source-separation problem become de-facto standard many medical imaging settings. despite successes large ongoing research eﬀort limitation square linear transformations overcome general infomax still realized. alternative present feature analysis medical imaging problem solved helmholtz machines include dimensionality reduction reconstruction data objective recently overcome major diﬃculties inference learning deep nonlinear conﬁgurations. demonstrate approach training helmholtz machines variational auto-encoders viable approach toward feature extraction magnetic resonance imaging data. feature selection central theme analyzing many variants magnetic resonance imaging data. supervised approaches highly capable performing regression classiﬁcation rely features best specialized maps input data output labels. lack crucial component inference produce generalizations input data. meanwhile inference process ability generalizeable features structure data core scientiﬁc discovery; research structures necessary general goal understanding brain. inferring latent structure generally goal unsupervised learning wide success analyzing data. combined supervised learners structures diagnostic value. independent component analysis representative approach found success means inferring latent structure brain imaging data represented linear mixture maximally-independent sources linear mixture models successful neuroimaging applications success relative nonlinear models simple tractable inference strong belief linearity correct depiction latent structure. linear mixtures non-gaussian sources necessary ensure uniqueness gaussian sources cannot guarantee independence beyond correlation luckily converse true non-gaussian sources linear mixtures assure maximum independence generative learning framework maximum likelihood estimation requiring prior distribution non-gaussian enabling inference learning linear methods breaks relationship data sources nonlinear necessitating advanced methods. although nonlinear versions well alternative nonlinear methods exist comes shortcomings none successful enough supplant linear ica. alternatively nonlinear independent component estimation method drawing family nonlinear transformations parameterized feed forward networks computing jacobian inverse tractable. nice estimate sources nonlinear mixings better simulations also limited square transformations requires principle component analysis practical medical imaging setting addition constrained square transformations many nonlinear variants cannot incorporate multimodal data natural way. linear mixing assumption harder justify modes drawn fundamentally diﬀerent distributions electroencephalography variables gender clinical diagnoses. issues ongoing challenges realizing full potential deep independence network general infomax approach feature extraction medical imaging. possible lack progress strong requirement data output deterministic sources. alternative propose learning features directed graphical model setting using recent advances variational inference demonstrate eﬀectiveness approach data. linear mixture models fall general category volumepreserving bijective maps learn deterministic parameterized transformation along prior distribution sources density data density sources jacobian. constraints ﬁrst linear transformation square unmixing matrix second prior distribution sources non-gaussian. probabilistic relaxes square requirement learning still reliant linear transformation well noise operator known covariance. linear transformation computing jacobian hence learning tractable cannot said general nonlinear transformations. nonlinear independent component estimation gets around problem parameterizing feed-forward network aﬃne transformation layer lower upper triangular still limited square transformations. directed graphical model bayesian network generative model composed prior conditional distributions make acyclic graph. directed graphical models used various medical imaging settings limited relatively simple often linear conﬁgurations. special case bayesian network directed belief network hierarchical model represents joint layers latent variables within layer conditionally independent prior distribution layer. directed graphical models commonly trained using maximumlikelihood estimation method maximizes log-likelihood data adjusting parameters conditional prior distributions. present latent variables need marginalized stage process; training become diﬃcult marginalizing joint distribution latent variables often computationally infeasible. potentially learning notable advances variational inference made helmholtz machines model approximate posterior conditional distributions deep neural networks model diﬃculty oﬀset inference training approximate posterior modeled recognition network. example suppose conditional distribution modeled output makes parameters multivariate gaussian distribution multilayer ffns parameters visible variables corresponding data latent variables finally assume prior distribution latent variables spherical multivariate logistic distribution. lower bound equation becomes commonly known variational autoencoder type reparameterization available number continuous distributions gaussian poisson gumbel available helmholtz machines discrete latent variables though good methods exist prior factorized lower bound corresponds learning generative model maximally-independent latent variables feature desirable many research settings. approach should principle work directed graph continuous latent variables given appropriate approximate posterior prior. however approximation typically requires large number samples accurate mnist dataset). addition provides single point marginal continuous function reality interested changes eﬀect generation image. therefore following fast approximation determine projection latent variable reserved parameters prior distribution encode ﬁrst second order statistics respectively. instance logistic distribution would center unit would scale factor. approximation capture full generative eﬀect latent variables suﬃcient demonstration. medical imaging study used dataset combined schizophrenia studies plis whole brain mris obtained signa scanner using identical parameters software resulting dataset segmented grey matter regions voxels sample. quality control correlation coeﬃcient volume calculated volumes mean coeﬃcient standard deviations mean across volumes categorized noisy removed. resulting dataset subjects healthy controls. generative model used logistic prior units layer generation feed-forward network deterministic intermediate layer softplus units parameterize gaussian multivariate factorized logistic parameterized -layer recognition hyperbolic tangent deterministic units. learn maximizing variational lower bound trained model batch size using rmsprop algorithm epochs. latent variable projections section positive negative prior distribution symmetric respect choice positive scale factors reversed sign projections mean voxels standard deviations negative. latent variable component used logistic regression schizophrenia using approximate posterior means component tested signiﬁcance using resulting values logistic regression one-sample t-test. fig. projections logistic latent variables variational autoencoder additional deterministic nonlinearity generative recognition networks. projection thresholded standard deviations latent variables showed showed high signiﬁcance onesampled t-test values logistic regression schizophrenia. visual inspection latent variables revealed diverse features mostly identiﬁable regions interest little noisy features. signiﬁcantly overlap features typical preprocessing data beneﬁcial depending research setting. latent variables showed high signiﬁcance schizophrenia shown figure complete supplementary material. means approximate fig. correlation matrix logistic centers approximate posterior subject components high signiﬁcance schizophrenia. columns rows ordered show healthy controls ﬁrst followed patients higher inter-group correlation. fig. correlation matrix components across subjects. rows columns ordered according grouping determined community multi-level analysis shows interintra-group structure components. posterior used input classiﬁcation task using simple logistic regression -fold class-balanced cross validation. resulting classiﬁcation rate signiﬁcantly chance. conclusion that despite lacking information labels objective much lower dimensionality latent variables similar amount information necessary perform diagnosis. also apparent correlation matrix figure using components showed high signiﬁcance schizophrenia. finally components grouped calculating correlation approximate posterior centers across subjects. figure shows several groupings well intergroup relationships. conclusions demonstrated variational autoencoders means training nonlinear directed graphical models extracting maximally indepdent features data. results show relevant structure preservation information relevant schizophrenia diagnosis. work opens door studies using helmholtz machines medical imaging research including multimodal multilayer analysis.", "year": 2016}