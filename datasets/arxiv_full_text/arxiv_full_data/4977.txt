{"title": "Teacher-Student Curriculum Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e. where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with LSTM and navigation in Minecraft. Using our automatically generated curriculum enabled to solve a Minecraft maze that could not be solved at all when training directly on solving the maze, and the learning was an order of magnitude faster than uniform sampling of subtasks.", "text": "propose teacher-student curriculum learning framework automatic curriculum learning student tries learn complex task teacher automatically chooses subtasks given student train describe family teacher algorithms rely intuition student practice tasks makes fastest progress i.e. slope learning curve highest. addition teacher algorithms address problem forgetting also choosing tasks student’s performance getting worse. demonstrate tscl matches surpasses results carefully hand-crafted curricula tasks addition decimal numbers lstm navigation minecraft. using automatically generated curriculum enabled solve minecraft maze could solved training directly solving maze learning order magnitude faster uniform sampling subtasks. deep reinforcement learning algorithms used solve difﬁcult tasks video games locomotion robotics tasks sparse rewards like robot fetch beer remain challenging solve direct application algorithms. reason number samples needed solve task random exploration increases exponentially number steps reward approach overcome problem curriculum learning tasks ordered increasing difﬁculty training proceeds harder tasks easier ones mastered. curriculum learning helps mastering simpler task policy harder task discoverable random exploration. curriculum learning researcher must able order subtasks difﬁculty. decide mastery threshold. based achieving certain score requires prior knowledge acceptable performance task. alternatively based plateau performance hard detect given noise learning curve. determines tasks student train training step order maximize student’s progression curriculum. student machine learning model. teacher learning student it’s giving tasks part single training session. describe several teacher algorithms based notion learning progress main idea student practice tasks making fastest progress i.e. learning curve slope highest. counter forgetting student also practice tasks performance getting worse i.e. learning curve slope negative. main contributions paper figure illustrates teacher-student interaction. timestep teacher chooses tasks student practice student trains tasks returns back score. teacher’s goal student succeed ﬁnal task training steps possible. usually task parameterized categorical value representing subtasks imagine also multi-dimensional continuous task parameterization. score episode total reward reinforcement learning validation accuracy supervised learning. formalize teacher’s goal helping student learn ﬁnal task solving partially observable markov decision process present pomdp formulations simple best suited reinforcement learning; batch best suited supervised learning. state represents entire state student simplest case scores could accuracies tasks training set. case minibatch training model evolves training therefore additional evaluation pass needed anyway produce consistent results. therefore separate validation contains uniform tasks evaluation pass. setup could also used reinforcement learning performing training batches episodes. scoring sample reinforcement learning usually much computationally expensive supervised learning makes sense simple pomdp formulation make decision next task training step. last training step task trained obvious choice optimization criteria would performance ﬁnal task initially student might success ﬁnal task provide meaningful feedback signal teacher. therefore choose maximize performances tasks. assumption curriculum learning ﬁnal task includes elements previous tasks therefore good performance intermediate tasks usually leads good performance ﬁnal task. pomdps typically solved using reinforcement learning algorithms. require many training episodes train student teacher episode. therefore resort simpler heuristics. basic intuition student practice tasks making progress also practicing tasks risk forgotten. figure idealistic curriculum learning. left scores different tasks improve time next task starts improving previous task mastered. right probability sampling task depends slope learning curve. student masters task learning curve ﬂattens teacher samples task less often. point student also starts making progress task teacher samples task picture idealistic since practice unlearning often occurs i.e. probability mass allocated task performance task might worse. counter student also practice learned tasks especially unlearning occurs. reason sample tasks according absolute value slope learning curve instead. change scores negative must mean unlearning occurred task practiced more. description alone prescribe algorithm. need propose method estimating learning progress noisy task scores balance exploration exploitation. take inspiration algorithms non-stationary multi-armed bandit problem adapt tscl. brevity give intuition simple formulation algorithms here formal descriptions found appendices online algorithm inspired basic non-stationary bandit algorithm uses exponentially weighted moving average track expected return different tasks learning rate. next task chosen \u0001-greedy exploration sample random task probability argmax otherwise. alternatively next task chosen using boltzmann distribution estimate learning progress reliably practice task several times. naive algorithm trains task times observes resulting scores estimates slope learning curve using linear regression. regression coefﬁcient used reward non-stationary bandit algorithm. details algorithm appendix repeating task ﬁxed number times expensive clearly progress made. window algorithm keeps fifo buffer last scores timesteps scores recorded. linear regression performed estimate slope learning curve task timesteps input variables. regression coefﬁcient used reward non-stationary bandit algorithm. details algorithm appendix previous algorithms require tuning hyperparameters balance exploration. exploration hyperparameters take inspiration thompson sampling. sampling algorithm keeps buffer last rewards task. choose next task recent reward sampled task’s k-last-rewards buffer. whichever task yielded highest sampled reward chosen. makes exploration natural part algorithm tasks recently high rewards sampled often. details algorithm appendix addition decimal numbers lstm well known task requires curriculum learn reasonable time implemented sequence-to-sequence model input network decimal-coded numbers separated ’plus’ sign output network numbers also decimal coding. curriculum based number digits input numbers easier learn addition short numbers move longer numbers. number addition supervised learning problem therefore trained efﬁciently including several curriculum tasks mini-batch. therefore adopt batch training scheme outlined score accuracy task calculated validation set. results shown means standard deviations runs different random seeds. full experiment details found appendix started similar setup curriculum task determines maximum number digits added numbers. results shown figure algorithms outperformed uniform sampling best manual curriculum -digit addition example task distribution training session given ﬁgure figure progression task distribution time -digit addition algorithm progresses simpler tasks complicated. harder tasks take longer learn algorithm keeps training easier tasks counter unlearning. also experimented curriculum ordering tasks obvious. used decimal addition task case length number chosen separately making task-space -dimensional. training batch modelled probability distribution length numbers also tried making distribution independent work well. equivalent experiment created manual curriculum inspired best curriculum. particular increase difﬁculty increasing maximum length numbers reduces problem curriculum. figure shows results -digit addition. figure illustrates different approaches taken manual automated curriculum. figure accuracy progress -digit addition. tscl. bottom best manual curriculum. algorithm takes distinctively different approach training shorter numbers ﬁrst. -digit videos found https//youtu.be/y_qicqspwk https//youtu.be/fbkx-esjgw. whether \u0001-greedy boltzmann exploration works better depends algorithm. uniform sampling surprisingly efﬁcient especially case. task solved faster manual curriculum hard beat minecraft popular video game players explore craft tools build arbitrary structures making potentially rich environment research. used malmo platform openai wrapper interact minecraft reinforcement learning experiments. particular used classroomdecorator malmo generate random mazes agent solve. mazes contain sequences rooms separated following obstacles implemented window algorithm minecraft task algorithms rely score change straightforward calculate parallel training scheme. baseline uniform sampling training last task manually tuned curriculum. full experimental details found appendix refer figure room layout. starting position agent location target randomized episode. manual curriculum trained ﬁrst task steps second third fourth task steps ﬁfth task steps. figure shows learning curves minecraft -step curriculum. mean curve standard deviation based runs different random seeds. figure minecraft -step curriculum results y-axis shows mean episode reward timesteps current task. left training performance notice manual curriculum task switches steps. automatic curriculum training score clear interpretation. right evaluation training last task. training last task agent make progress all. training uniform tasks progress slow. manual curriculum allowed agent learn last task acceptable level. tscl comparable manual curriculum performance. video trained agent found here https//youtu.be/cadad_adic. learned policy robust number rooms given obstacles type. code available https//github.com/tambetm/tscl. work sparked general interest curriculum learning. recent results include learning execute short programs ﬁnding shortest paths graphs learning play ﬁrst-person shooter works rely manually designed curricula attempt produce automatically. idea using learning progress reward could traced back successfully applied context developmental robotics learn object manipulation also actual classroom settings teach primary school students using learning progress reward linked concept intrinsic motivation several algorithms adversarial bandits analyzed many algorithms formal worst-case guarantees experiments perform well. problem come assumptions. curriculum learning assume rewards change smoothly time. recently proposed method generate incremental goals therefore curricula automatically. setup consists agents alice alice generating trajectories trying either repeat reverse them. similar work uses generative adversarial network generate goal states agent. compared tscl able generate subtasks mainly aids exploration guaranteed help learning ﬁnal task. apply similar setup multi-task learning. work practice tasks underperforming compared preset baseline opposed approach using learning progress. estimate transfer subtasks target task create curriculum based that. similar work done concurrently problem statement strikingly similar approaches differ. apply automatic curriculum learning supervised sequence learning tasks consider also reinforcement learning tasks. exp.s algorithm adversarial bandits propose alternative algorithms inspired non-stationary bandits. consider learning progress metrics based complexity gain focus prediction gain moreover work uses uniform sampling tasks baseline whereas compares best known manual curriculum given tasks. summary arrive similar conclusions ours. decimal addition also explored sometimes improving results original work goal improve addition results evaluate different curriculum approaches therefore direct comparison. minecraft relatively recent addition reinforcement learning environments. work evaluates memory-based architectures minecraft. cognition-inspired tasks visual grid-world. tasks differ need explicit memory movement continuous grid-world. another work uses tasks similar take different approach learn deep skill module subtask freeze weights modules train hierarchical deep reinforcement learning network pick either single actions subtask policies. contrast approach uses simple policy network relies tscl learn subtasks. exploration bonuses solve problem sparse rewards apply student algorithms considering different teacher approaches. reason leave comparison exploration bonuses future work. presented framework automatic curriculum learning used supervised reinforcement learning tasks. proposed family algorithms within framework based concept learning progress. many algorithms performed equally well crucial rely absolute value slope learning curve choosing tasks. guarantees re-training tasks network starting forget. lstm decimal addition experiments sampling algorithm outperformed best manually designed curriculum well uniform sampling. challenging -task minecraft navigation problem window algorithm matched performance carefully designed manual curriculum signiﬁcantly outperformed uniform sampling. problems curriculum learning necessary tscl avoid tedium ordering difﬁculty subtasks hand-designing curriculum. work considered discrete task parameterizations. future would interesting apply idea continuous task parameterizations. another promising idea explore usage automatic curriculum learning contexts subtasks pre-deﬁned. example subtasks sampled generative model taken different initial states environment. thank microsoft excellent malmö environment minecraft josh tobin pieter abbeel suggestions comments vicky cheung jonas schneider mann chaidarun marc bellemare sriram srinivasan georg ostrovski schaul david saxton remi munos. unifying count-based exploration intrinsic motivation. advances neural information processing systems pages yoshua bengio jérôme louradour ronan collobert jason weston. curriculum learning. proceedings annual international conference machine learning icml ./.. alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabskabarwi´nska sergio gómez colmenarejo. hybrid computing using neural network dynamic external memory. nature rein houthooft chen duan john schulman filip turck pieter abbeel. vime variational information maximizing exploration. advances neural information processing systems pages matthew johnson katja hofmann hutton david bignell. malmo platform artiﬁcial intelligence experimentation. international joint conference artiﬁcial intelligence page timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous control deep reinforcement learning. arxiv preprint arxiv. piotr mirowski razvan pascanu fabio viola hubert soyer andy ballard andrea banino misha denil ross goroshin laurent sifre koray kavukcuoglu learning navigate complex environments. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves. human-level control deep reinforcement learning. nature pierre-yves oudeyer frdric kaplan verena hafner. intrinsic motivation systems autonomous mental development. ieee transactions evolutionary computation richard sutton andrew barto. reinforcement learning introduction. ieee transactions neural networks publication ieee neural networks council chen tessler shahar givony zahavy daniel mankowitz shie mannor. deep hierarchical choose task based using \u0001-greedy boltzmann policy train student using task observe score store score timestep linear regression predict coef. update expected return create prob. dist. train student using prob. dist. observe scores store score apply linear regression extract coefﬁcients vector update expected return sample reward task based argmaxa |˜ra| create one-hot prob. dist. uniform dist. train student using prob. dist. observe scores calculate score changes store reward task reimplementation decimal addition based keras encoder decoder lstms units. contrast original implementation hidden state passed encoder decoder instead last output encoder provided inputs decoder. curriculum training step consists training samples. validation consists samples also batch size. adam optimizer used training default learning rate input output padded ﬁxed size. experiments used number steps validation accuracy reached comparison metric. exploration coefﬁcient ﬁxed temperature ﬁxed learning rate window size experiments. minecraft task consisted navigating randomly generated mazes. maze ends target block agent gets points touching move costs dying lava getting timeout yields points. timeout seconds ﬁrst task seconds subsequent tasks. learning used proximal policy optimization algorithm implemented using keras optimized real-time environments. policy network used four convolutional layers lstm layer. input network color image outputs gaussian actions move forward/backward turn left/right. addition policy network state value output used baseline. figure shows network architecture. training used setup parallel minecraft instances. agent code separated runners interact environment trainer performs batch training similar babaeizadeh runners regularly update snapshot current policy weights perform prediction never training. ﬁxed number steps fifo buffers send collected states actions rewards trainer. trainer collects experiences runners assembles batches performs training. fifo buffers shield runners trainer occasional hiccups. also means trainer completely on-policy problem handled importance sampling ppo. training also used frame skipping i.e. processed every frame. sped learning considerably resulting policy also worked without frame skip. also used auxiliary loss predicting depth suggested surprisingly resulted minor improvements. automatic curriculum learning implemented window algorithm minecraft task algorithms rely score change straightforward calculate parallel training scheme. window size deﬁned timesteps ﬁxed experiments exploration rate idea ﬁrst task curriculum make agent associate target reward. practice task proved simple agent could achieve almost reward backwards circles room. reason added penalty moving backwards policy loss function. ﬁxed problem cases occasionally still discard unsuccessful runs. results reﬂect successful runs. also preliminary success combining continuous actions binary actions \"jump\" \"use\" controls shown ﬁgure allowed agent learn cope also rooms involve doors switches jumping obstacles https//youtu.be/eokiplav.", "year": 2017}