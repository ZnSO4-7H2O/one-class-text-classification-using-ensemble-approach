{"title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word  Embeddings", "tag": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "abstract": "The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to \"debias\" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.", "text": "blind application machine learning runs risk amplifying biases present data. danger facing word embedding popular framework represent text data vectors used many machine learning natural language processing tasks. show even word embeddings trained google news articles exhibit female/male gender stereotypes disturbing extent. raises concerns widespread describe often tends amplify biases. geometrically gender bias ﬁrst shown captured direction word embedding. second gender neutral words shown linearly separable gender deﬁnition words word embedding. using properties provide methodology modifying embedding remove gender stereotypes association words receptionist female maintaining desired associations words queen female. deﬁne metrics quantify direct indirect gender biases embeddings develop algorithms debias embedding. using crowd-worker evaluation well standard benchmarks empirically demonstrate algorithms signiﬁcantly reduce gender bias embeddings preserving useful properties ability cluster related concepts solve analogy tasks. resulting embeddings used applications without amplifying gender bias. hundreds thousands papers written word embeddings applications search parsing curriculum vitae however none papers recognized blatantly sexist embeddings hence risk introducing biases various types real-world systems. word embedding represent word d-dimensional word vector word embeddings trained word co-occurrence text corpora serve dictionary sorts computer programs would like word meaning. first words similar semantic meanings tend vectors close together. second vector diﬀerences words embeddings shown represent relationships words example given analogy puzzle king woman simple arithmetic embedding vectors ﬁnds x=queen best answer because similarly x=japan returned parisfrance tokyox. surprising simple vector arithmetic simultaneously capture variety relationships. also excited practitioners tool could useful across applications involving natural language. indeed studied used variety downstream applications sentiment analysis question retrieval figure analogy examples. examples automatically generated analogies pair she-he using procedure described text. example ﬁrst analogy interpreted shesewing hecarpentry original wvnews embedding. automatically generated analogy evaluated crowd-workers whether reﬂects gender stereotype. illustrative gender stereotypic analogies automatically generated wvnews rated least crowd-workers. bottom illustrative generated gender-appropriate analogies. figure example indirect bias. extreme occupations softball-football axis indirectly captures gender bias. occupation degree association represents gender bias shown described section words system solved reasonable analogies oﬀensively answer computer programmer woman x=homemaker. similarly outputs father doctor mother nurse. primary embedding studied paper popular publicly-available wordvec embedding trained corpus google news texts consisting million english words terms dimensions refer wvnews. might hoped google news embedding would exhibit little gender bias many authors professional journalists. also analyze publicly available embeddings trained algorithms similar biases. paper quantitatively demonstrate word-embeddings contain biases geometry reﬂect gender stereotypes present broader society. wide-spread usage basic features word embeddings reﬂect stereotypes also amplify them. poses signiﬁcant risk challenge machine learning applications. illustrate bias ampliﬁcation consider bias present task retrieving relevant pages given query. search recent project shown that carefully combined existing approaches word vectors potential improve page relevance results example suppose search query computer science student computer science ph.d. student carnegie mellon university. directory oﬀers nearly identical pages students pages diﬀer names students. word embedding’s semantic knowledge improve relevance identifying examples terms graduate research assistant student related. however word embeddings also rank terms related computer science closer male names female names consequence that pages diﬀer names mary john word embedding would inﬂuence search engine rank john’s page higher mary. hypothetical example usage word embedding makes even harder women recognized computer scientists would contribute widening existing gender computer science. focus gender bias speciﬁcally female-male bias approach applied types bias. uncovering gender stereotypes text seem like trivial matter counting pairs words occur together. however counts often misleading instance term male nurse several times frequent female nurse hence extracting associations text otherwise simple ﬁrst-order approaches would predict word nurse male quarterback. generally gordon durme show reporting bias including fact common assumptions often left unsaid male suggesting word embeddings capable circumventing reporting bias cases. happens word embeddings trained using second-order methods require large amounts data extract associations relationships words. analogies generated embeddings spell bias implicit data trained. hence word embeddings serve means extract implicit gender associations large text corpus similar implicit association tests detect automatic gender associations possessed people often align self reports. consider distinction gender speciﬁc words associated gender deﬁnition remaining gender neutral words. standard examples gender speciﬁc words include brother sister businessman businesswoman. fact share deﬁnitive feature relating males. gender speciﬁc words learn gender subspace embedding debiasing algorithm removes bias gender neutral words respecting deﬁnitions gender speciﬁc words. gender pair direct bias. also consider notion indirect bias manifests associations gender neutral words clearly arising gender. instance fact word receptionist much closer softball football arise female associations receptionist softball. note many pairs male-biased words legitimate associations nothing gender. instance words mathematician geometry strong male bias similarity justiﬁed factors gender. often associations combinations gender factors diﬃcult disentangle. nonetheless geometry word embedding determine degree associations based gender. aligning biases stereotypes. stereotypes biases widely held among group people. show biases word embedding fact closely aligned social conception gender stereotype evaluated u.s.-based crowd workers amazon’s mechanical turk. crowd agreed −−−→ doctor closer −−→man −−−−→woman) well biases reﬂected location vectors exhibit common gender stereotypes. debiasing. goal reduce gender biases word embedding preserving useful properties embedding. surprisingly embedding capture bias also contains suﬃcient information reduce bias illustrated leverage fact exists dimensional subspace embedding empirically captures much gender bias. goals debiasing maintain meaningful non-gender-related associations gender neutral words including associations within stereotypical categories words fashion-related words words associated football. paper outline. discussing related literature give preliminaries necessary understanding paper section next propose methods identify gender bias embedding show wvnews exhibits bias aligned common gender stereotypes section deﬁne several simple geometric properties associated bias particular discuss identify gender subspace. using geometric properties introduce debiasing algorithms demonstrate performance finally conclude additional discussions related literature types biases embedding future works. terminology indirect bias follows pedreshi distinguish direct versus indirect discrimination rules fair classiﬁers. direct discrimination involves directly using sensitive features gender race whereas indirect discrimination involves using correlates inherently based sensitive features that intentionally unintentionally lead disproportionate treatment nonetheless. gender bias stereotype english important quantify understand bias languages biases reinforce psychological status diﬀerent groups gender bias language studied number decades variety contexts highlight ﬁndings here. biases diﬀer across people though commonalities detected. implicit association tests uncovered gender-word biases people self-report even aware common biases link female terms liberal arts family male terms science careers bias seen word morphology i.e. fact words actor default associated dominant class female versions words e.g. actress marked. also imbalance number words various associations. instance words referring males many words sexualize females males glick fiske introduce notion benevolent sexism women perceived positive traits helpful intimacy-seeking. despite seemingly positive nature benevolent sexism harmful insulting discriminatory. terms words female gender associations word even subjectively positive word attractive cause discrimination women reduces association words professional. stereotypes mentioned biases widely held within group. gender bias kind concerning stereotypes often easier study consistent nature. stereotypes commonalities across cultures though variation cultures complimentary stereotypes common females males gender associated strengths perceived oﬀset weaknesses compliment strengths gender compensatory stereotypes used people justify status quo. consistent biases studied within online contexts speciﬁcally related contexts study online news search wikipedia wikipedia wager found that suggested prior work gender bias language articles women often emphasize gender husbands husbands’ jobs topics discussed consistently less often articles men. regarding individual words certain words predictive gender e.g. husband appears signiﬁcantly often articles women baseball occurs often articles men. bias within algorithms number online systems shown exhibit various biases racial discrimination gender bias presented users recent study found algorithms used predict repeat oﬀenders exhibit indirect racial biases diﬀerent demographic geographic groups also diﬀerent dialects word-choices social media implication eﬀect language used minority group might able processed natural language tools trained standard data-sets. biases curation machine learning data-sets explored independent work schmidt identiﬁed bias present word embeddings proposed debiasing entirely removing multiple gender dimensions gender pair. goal approach similar simpler ours entirely remove gender embedding. also intense research agenda focused improving quality word embeddings diﬀerent angles diﬃculty evaluating embedding quality parallels diﬃculty deﬁning bias embedding. within machine learning body notable work focused fair binary classiﬁcation particular. deﬁnition fairness based legal traditions presented barocas selbst approaches modify classiﬁcation algorithms deﬁne achieve various notions fairness described number works e.g. recent survey feldman distinguish classiﬁcation algorithms achieve fairness modifying underlying data achieve fairness modifying classiﬁcation algorithm. approach similar former. however unclear apply previous approaches without clear prior work algorithmic fairness largely supervised learning. fair classiﬁcation deﬁned based fact algorithms classifying individuals using features distinguished sensitive feature. word embeddings clear individuals priori deﬁned classiﬁcation problem. however similar issues arise direct indirect bias ﬁrst brieﬂy deﬁne embedding terminology. embedding consists unit vector word assume gender neutral words ﬂight attendant shoes which deﬁnition speciﬁc gender. denote size |s|. also assume given gender pairs she-he mother-father whose deﬁnitions diﬀer mainly gender. section discusses found within embedding itself take given. common similarity words measured inner product finally abuse terminology refer embedding word word interchangeably. example −→cat −−→cow. arbitrary vectors normalized similarity vectors written cosine angle vectors. since words normalized cosw embedding. unless otherwise stated embedding refer paper aforementioned wvnews embedding -dimensional wordvec embedding proven immensely useful since high quality publicly available easy incorporate application. particular downloaded pre-trained embedding google news corpus normalized word unit length common. starting frequent words selected lower-case words phrases consisting fewer lower-case characters ﬁltering words remained. focus wvnews show later gender stereotypes also present embedding data-sets. crowd experiments. human experiments performed amazon mechanical turk crowdsourcing platform. selected u.s.-based workers maintain homogeneity reproducibility extent possible crowdsourcing. types experiments performed ones solicited words crowd ones solicited ratings words analogies generated embedding types experiments analogous experiments performed rating results information retrieval evaluate precision recall. speak majority crowd judgments mean annotations made independent workers. since gender associations vary culture person ratings stereotypes rather bias. addition possessing greater consistency biases people feel comfortable rating stereotypes culture discussing gender biases. appendix contains questionnaires given crowd-workers perform tasks. ﬁrst task understand biases present word-embedding extent geometric biases agree human notion gender stereotypes. simple methods approach problem evaluate whether embedding figure comparing bias diﬀerent embeddings–the wvnews glove web-crawl embedding. embedding occupation words projected onto she-he direction. corresponds occupation word; gender bias occupations highly consistent across embeddings stereotypes occupation words evaluate whether embedding produces analogies judged reﬂect stereotypes humans. exploratory analysis section motivate rigorous metrics used next sections. occupational stereotypes. figure lists occupations closest wvnews embeddings. asked crowdworkers evaluate whether occupation considered femalestereotypic male-stereotypic neutral. occupation word evaluated crowd-workers whether reﬂects gender stereotype. hence word integer rating scale stereotypicality. projection occupation words onto she-he axis strongly correlated stereotypicality estimates words suggesting geometric biases embedding vectors aligned crowd judgment gender stereotypes. used occupation words easily interpretable humans often capture common gender stereotypes. word sets could used task. also note could used words e.g. woman gender-pair task. chose frequent fewer alternative word senses projected occupations onto she-he direction wvnews embedding well diﬀerent embedding generated glove algorithm web-crawl corpus results highly consistent suggesting gender stereotypes prevalent across diﬀerent embeddings artifact particular training corpus methodology wordvec. analogies exhibiting stereotypes. analogies useful evaluate quality word embedding also stereotypes. ﬁrst brieﬂy describe embedding generate analogies discuss analogies quantify gender stereotype embedding. detailed discussion algorithm prior analogy solvers given appendix standard analogy tasks given three words example king look word solve king modify analogy task given words e.g. want generate pair words good analogy. modiﬁcation allows systematically generate pairs words embedding believes analogous threshold similarity. intuition scoring metric want good analogy pair close parallel seed direction words apart order semantically coherent. parameter sets threshold semantic similarity. experiments take choice often works well practice. since embeddings normalized threshold corresponds angle indicating words closer origin. practice means words forming analogy signiﬁcantly closer together random embedding vectors. given embedding seed words output analogous pairs largest positive scores. reduce redundancy output multiple analogies sharing word since analogies stereotypes biases heavily inﬂuenced culture employed u.s. based crowdworkers evaluate analogies output analogy generating algorithm described above. analogy asked workers yes/no questions whether pairing makes sense analogy whether reﬂects gender stereotype. every analogy judged workers used number workers rated pair stereotyped quantify degree bias analogy. overall analogies rated gender-appropriate crowd-workers analogies rated exhibiting gender stereotype crowd-workers examples analogies generated wvnews rated stereotypical shown figure examples analogies make sense rated gender-appropriate shown bottom figure full list analogies crowd ratings appendix indirect gender bias. direct bias analyzed manifests relative similarities genderspeciﬁc words gender neutral words. gender bias could also aﬀect relative geometry gender neutral words themselves. test indirect gender bias take pairs words gender-neutral example softball football. project occupation words onto football direction looked extremes words listed figure instance fact words bookkeeper receptionist much closer softball football result indirectly female associations bookkeeper receptionist softball. it’s important point many pairs male-biased words legitimate associations nothing gender. example footballer football strong male biases similarity justiﬁed factors gender. section deﬁne metric rigorously quantify indirect eﬀects gender bias. section study bias present embedding geometrically identifying gender direction quantifying bias independent extent aligned crowd bias. develop metrics direct indirect bias rigorously quantify observations previous section. language messy therefore individual word pairs always behave expected. instance word several diﬀerent usages used exclamation refer people either gender verb e.g. station. robustly estimate bias shall aggregate −−−−→woman −−→man across multiple paired comparisons. combining several directions identify gender direction largely captures gender embedding. direction helps quantify direct indirect biases words associations. figure possible word pairs deﬁne gender ordered word frequency along agreement sets words solicited crowd deﬁnitional stereotypical gender associations. words comprised frequent female male crowd suggestions accuracy shown corresponding gender classiﬁer based word closer target word e.g. she-he classiﬁer predicts word female closer roughly accuracy gender pairs predict gender stereotypes deﬁnitionally gendered words solicited crowd. english many languages numerous gender pair terms consider diﬀerence embeddings. looking data might imagine roughly vector diﬀerences following caricature however gender pair diﬀerences parallel practice multiple reasons. first diﬀerent biases associated diﬀerent gender pairs. second polysemy mentioned case occurs grandfather grandfather regulation. finally randomness word counts experimentally veriﬁed pairs vectors corresponding words agree crowd concept gender. amazon mechanical turk asked crowdworkers generate lists words list corresponding words think gendered deﬁnition separate list corresponding words believe captures gender stereotypes generated frequently suggested male female words list used accurately classiﬁes classiﬁcation task. candidate pair example crowd suggested female deﬁnition word word vector closer table reports classiﬁcation accuracy deﬁnition stereotype words gender pair. accuracies high indicating pairs capture intuitive notion gender. identify gender subspace took gender pair diﬀerence vectors computed principal components figure shows single direction explains majority variance vectors. ﬁrst eigenvalue signiﬁcantly larger rest. note that randomness ﬁnite sample noisy vectors expects decrease eigenvalues. however also illustrated decrease observes random sampling much gradual uniform. therefore hypothesize denoted unit vector captures gender subspace. general gender subspace could higher dimensional analysis algorithms work general subspaces. direct bias measure direct bias ﬁrst identify words gender-neutral application question. generate gender-neutral words described section given gender neutral words denoted gender direction learned above deﬁne direct gender bias figure left percentage variance explained vector diﬀerences component explains signiﬁcantly variance other. right comparison corresponding percentages random unit vectors parameter determines strict want measuring bias. |cosw g)|c overlap otherwise strict measurement bias might desirable settings college admissions example introduction would unacceptable embedding introduce slight preference candidate another gender. gradual bias would setting presentation chosen favors simplicity would natural extend deﬁnitions weight words frequency. example wvnews take occupations directbias conﬁrms many occupation words substantial component along gender direction. unfortunately deﬁnitions still capture indirect bias. this imagine completely removing embedding words gender pairs would still indirect gender association word gender neutral receptionist closer softball football discussed introduction subtle obtain ground truth extent similarities gender. gender subspace identiﬁed allows quantify contribution similarities pair words. decompose given word vector contribution gender note word vectors normalized unit length. deﬁne gender component similarity word vectors inner product vectors project gender subspace renormalize vectors unit length. metric quantiﬁes much inner product changes operation removing gender subspace. noise data every vector non-zero component well-deﬁned. note reasonable since similarity word depend gender contribution. figure selected words projected along axes projection onto diﬀerence embeddings words direction learned embedding captures gender neutrality gender neutral words line gender speciﬁc words line. hard debiasing algorithm removes gender pair associations gender neutral words. ﬁgure words horizontal line would collapsed vertical line. −−−−→ softball −−−−−→ football) shown table. words receptionist waitress homemaker closer softball football words softball substantial −−−−→ suggests apparent similarity embeddings words softball largely explained gender biases embedding. similarly businessman maestro closer football also attributed largely indirect gender bias respectively. debiasing algorithms deﬁned terms sets words rather pairs generality consider biases racial religious biases. also assume words neutralize come list embedding described section ﬁrst step called identify gender subspace identify direction embedding captures bias. second step deﬁne options neutralize equalize soften. neutralize ensures gender neutral words zero gender subspace. equalize perfectly equalizes sets words outside subspace thereby enforces property neutral word equidistant words equality set. instance {grandmother grandfather} {guy gal} equality sets equalization babysit would equidistant grandmother grandfather also equidistant presumably closer grandparents guy. suitable applications want pair display bias respect neutral words. disadvantage equalize removes certain distinctions valuable certain applications. instance wish language model assign higher probability phrase grandfather regulation) grandmother regulation since grandfather meaning grandmother equalizing removes distinction. soften algorithm reduces diﬀerences sets maintaining much similarity original embedding possible parameter controls trade-oﬀ. deﬁne algorithms convenient introduce notation. subspace deﬁned orthogonal unit vectors case subspace simply direction. denote projection vector onto equalize equates words outside simple average adjusts vectors unit length. perhaps easiest understand thinking separately components latter simply equated average. within centered scaled unit length. motivate center beyond fact common machine learning consider bias direction gender direction gender pair {male female}. discussed happens words positive gender direction though female greater projection. speculate case e.g. perhaps frequency text male nurse male escort assaulted male. however female greater gender component centering symmetrically balanced across origin. instead simply scaled vector’s component bias direciton without centering male female would exactly embedding would lose analogies fathermale motherfemale. proof. step ensures step ensures vece lies entirely hence inner product w·e. lastly follows fact unit vectors pairbias follows trivially deﬁnition pairbias. step soft bias correction. overloading notation rd×|vocab| denote matrix embedding vectors denote matrix embedding vectors corresponding gender neutral words. learned corpus inputs algorithm. desired debiasing transformation rd×d linear transformation seeks preserve pairwise inner products word vectors minimizing projection gender neutral words onto gender subspace. formalized following optimization problem gender subspace learned step tuning parameter balances objective preserving original embedding inner products goal reducing gender bias. large would remove projection onto vectors corresponds exactly step experiment optimization problem semi-deﬁnite program solved eﬃciently. output embedding normalized unit length practical purposes since many fewer gender speciﬁc words eﬃcient enumerate gender speciﬁc words take gender neutral words compliment using dictionary deﬁnitions derive subset words words wvnews. recall embedding subset words full million words embedding described section base list given appendix note choice words subjective ideally customized application hand. generalize list entire million words google news embedding using linear classiﬁer resulting gender-speciﬁc words. speciﬁcally trained linear support vector machine default regularization parameter classiﬁer remaining words taking words labeled gender speciﬁc classiﬁer among words entire embedding words wvnews. using -fold cross-validation evaluate accuracy process -score based stratiﬁed -fold cross-validation. binary accuracy well imbalanced nature classes. another test accurately embedding agrees base words evaluate class-balanced error re-weighting examples positive negative examples equal weights i.e. weighting class inverse proportionally number samples class. again stratiﬁed -fold cross validation evaluate error. within fold regularization parameter also chosen -fold cross validation. average accuracy linear classiﬁers across folds conﬁdence. figure illustrates results classiﬁer separating gender-speciﬁc words gender-neutral words. make ﬁgure legible show subset words. x-axis correspond projection words onto direction y-axis corresponds distance decision boundary trained svm. table columns show performance original wvnews embedding debiased wvnews standard evaluation metrics measuring coherence analogy-solving abilities msr-analogy higher better. results show performance degrade debiasing. note subset vocabulary experiments. therefore performances lower previously published results. direct bias. first used analogy generation task before hard-debiased soft-debiased embeddings automatically generated pairs words analogous she-he asked crowd-workers evaluate whether pairs reﬂect gender stereotypes. figure shows results. initial wvnews embedding analogies judged showing gender stereotypes majority workers. applying hard debiasing algorithm embedding judged stereotypical. example consider analogy puzzle doctor original embedding returns nurse hard-debiased embedding ﬁnds physician. moreover hard-debiasing algorithm preserved gender appropriate analogies ovarian cancer prostate cancer. demonstrates hard-debiasing eﬀectively reduced gender stereotypes word embedding. figure also shows number appropriate analogies remains similar original embedding executing hard-debiasing. demonstrates quality embeddings preserved. details results appendix soft-debiasing less eﬀective removing gender bias. conﬁrms quality embeddings debiasing tested debiased embedding several standard benchmarks measure whether related words similar embeddings well well embedding performs analogy tasks. table shows results original embeddings transformation negatively impact performance. indirect bias. also investigated strict debiasing algorithm aﬀects indirect gender bias. ground truth indirect eﬀects gender bias challenging quantify performance algorithm regard. however promising qualitative improvements shown figure softball football example. applying strict debias algorithm repeated experiment show extreme words football direction. extreme words closer softball inﬁelder major leaguer addition pitcher relevant exhibit gender bias. gender stereotypic associations receptionist waitress homemaker moved list. similarly words clearly show male bias e.g. businessman also longer football direction pitcher list. note extreme words footballer. similarities pitcher softball footballer football comes actual functions words hence little gender contribution. words essentially unchanged debiasing algorithm. word embeddings help understanding bias language. single direction largely captures gender helps capture associations gender neutral words gender well indirect inequality.the projection gender neutral words direction enables quantify degree femalemale-bias. gender associations. instance nurse moved equally male female direction addition gender-speciﬁc words additional biases beyond instance grandmother grandfather closer wisdom reﬂect gender diﬀerence. hand fact babysit much closer grandmother grandfather gender bias speciﬁc grandmother. equating grandmother grandfather outside gender since we’ve removed babysit grandmother grandfather equally close babysit debiasing. retaining gender component gender-speciﬁc words maintain analogies shegrandmother hegrandfather. empirical evaluations show hard-debiasing algorithm signiﬁcantly reduces direct indirect gender bias preserving utility embedding. also developed soft-embedding algorithm balances reducing bias preserving original distances could appropriate speciﬁc settings. perspective bias word embeddings merely reﬂects bias society therefore attempt debias society rather word embeddings. however reducing bias today’s computer systems increasingly reliant word embeddings small debiased word embeddings hopefully contribute reducing gender bias society. least machine learning used inadvertently amplify biases seen naturally happen. speciﬁc applications might argue gender biases embedding could capture useful statistics that special cases original biased embeddings could used. however given potential risk machine learning algorithms amplify gender stereotypes discriminations recommend side neutrality debiased embeddings provided much possible. paper focus quantifying reducing gender bias word embeddings. corpus documents often contain undesirable stereotypes also reﬂected embedding vectors. wvnews also exhibits strong racial stereotype. example projecting occupation words onto direction whites extreme occupations closer whites parliamentarian advocate deputy chancellor legislator lawyer. contrast extreme occupations minorites butler footballer socialite crooner. subtle issue understand direct indirect bias racial ethnic cultural stereotypes. important direction future work would quantify remove biases. focus english word embeddings also interesting direction consider approach ﬁndings would apply languages especially languages grammatical gender deﬁnitions nouns carry gender marker.", "year": 2016}