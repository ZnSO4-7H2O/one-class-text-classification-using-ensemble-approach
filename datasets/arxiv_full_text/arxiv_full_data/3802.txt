{"title": "Generating Sequences With Recurrent Neural Networks", "tag": ["cs.NE", "cs.CL"], "abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.", "text": "paper shows long short-term memory recurrent neural networks used generate complex sequences long-range structure simply predicting data point time. approach demonstrated text online handwriting extended handwriting synthesis allowing network condition predictions text sequence. resulting system able generate highly realistic cursive handwriting wide variety styles. recurrent neural networks rich class dynamic models used generate sequences domains diverse music text motion capture data rnns trained sequence generation processing real data sequences step time predicting comes next. assuming predictions probabilistic novel sequences generated trained network iteratively sampling network’s output distribution feeding sample input next step. words making network treat inventions real much like person dreaming. although network deterministic stochasticity injected picking samples induces distribution sequences. distribution conditional since internal state network hence predictive distribution depends previous inputs. rnns ‘fuzzy’ sense exact templates training data make predictions rather—like neural networks— internal representation perform high-dimensional interpolation training examples. distinguishes n-gram models compression algorithms prediction partial matching whose predictive distributions determined counting exact matches recent history training set. result—which immediately apparent samples paper—is rnns synthesise reconstitute training data complex rarely generate thing twice. furthermore fuzzy predictions suffer curse dimensionality therefore much better modelling real-valued multivariate data exact matches. principle large enough suﬃcient generate sequences arbitrary complexity. practice however standard rnns unable store information past inputs long well diminishing ability model long-range structure ‘amnesia’ makes prone instability generating sequences. problem network’s predictions based last inputs inputs predicted network little opportunity recover past mistakes. longer memory stabilising eﬀect even network cannot make sense recent history look back past formulate predictions. problem instability especially acute real-valued data easy predictions stray manifold training data lies. remedy proposed conditional models inject noise predictions feeding back model thereby increasing model’s robustness surprising inputs. however believe better memory profound eﬀective solution. long short-term memory architecture designed better storing accessing information standard rnns. lstm recently given state-of-the-art results variety sequence processing tasks including speech handwriting recognition main goal paper demonstrate lstm memory generate complex realistic sequences containing long-range structure. section deﬁnes ‘deep’ composed stacked lstm layers explains trained next-step prediction hence sequence generation. section applies prediction network text penn treebank hutter prize wikipedia datasets. network’s performance competitive state-of-the-art language models works almost well predicting character time predicting word time. highlight section generated sample wikipedia text showcases network’s ability model long-range dependencies. section demonstrates prediction network applied real-valued data mixture density output layer provides experimental results online handwriting database. also presents generated handwriting samples proving network’s ability learn letters short words direct traces model global features handwriting style. section introduces extension prediction network allows condition outputs short annotation sequence whose alignment predictions unknown. makes suitable handwriting synthesis human user inputs text algorithm generates handwritten version synthesis network trained database used generate cursive handwriting samples cannot distinguished real data figure deep recurrent neural network prediction architecture. circles represent network layers solid lines represent weighted connections dashed lines represent predictions. naked eye. method biasing samples towards higher probability described along technique ‘priming’ samples real data thereby mimicking particular writer’s style. finally concluding remarks directions future work given section fig. illustrates basic recurrent neural network prediction architecture used paper. input vector sequence passed weighted connections stack recurrently connected hidden layers compute ﬁrst hidden vector sequences output vector sequence output vector used parameterise predictive distribution possible next inputs xt+. ﬁrst element every input sequence always null vector whose entries zero; network therefore emits prediction ﬁrst real input prior information. network ‘deep’ space time sense every piece information passing either vertically horizontally computation graph acted multiple successive weight matrices nonlinearities. reducing number processing steps bottom network thereby mitigating ‘vanishing gradient’ problem special case architecture reduces ordinary single layer next step prediction rnn. partial derivatives loss respect network weights eﬃciently calculated backpropagation time applied computation graph shown fig. network trained gradient descent. architecture uses purpose-built memory cells store information better ﬁnding exploiting long range dependencies data. fig. illustrates single lstm memory cell. version lstm used logistic sigmoid function respectively input gate forget gate output gate cell cell input activation vectors size hidden vector weight matrix subscripts obvious meaning example hidden-input gate matrix input-output gate matrix etc. weight matrices cell gate vectors diagonal element gate vector receives input element cell vector. bias terms omitted clarity. original lstm algorithm used custom designed approximate gradient calculation allowed weights updated every timestep however full gradient instead calculated backpropagation time method used paper. diﬃculty training lstm full gradient derivatives sometimes become excessively large leading numerical problems. prevent this experiments paper clipped derivative loss respect network inputs lstm layers within predeﬁned range. text data discrete typically presented neural networks using ‘onehot’ input vectors. text classes total class time length vector whose entries zero except thing remains decided classes use. cases text prediction performed word level. therefore number words dictionary. problematic realistic tasks number words often exceeds well requiring many parameters model many classes demands huge amount training data adequately cover possible contexts words. case softmax models diﬃculty high computational cost evaluating exponentials training rank approximations stochastic derivatives furthermore word-level models applicable text data containing non-word strings multi-digit numbers addresses. character-level language modelling neural networks recently considered found give slightly worse performance equivalent word-level models. nonetheless predicting character time interesting perspective sequence generation allows network invent novel words strings. general experiments paper predict ﬁnest granularity found data maximise generative ﬂexibility network. ﬁrst text prediction experiments focused penn treebank portion wall street journal corpus preliminary study whose main purpose gauge predictive power network rather generate interesting sequences. although relatively small text corpus penn treebank data widely used language modelling benchmark. training contains words validation contains words test contains words. vocabulary limited words words mapped special ‘unknown word’ token. end-ofsentence token included input sequences counted sequence loss. start-of-sentence marker ignored role already fulﬁlled null vectors begin sequences experiments compared performance word character-level lstm predictors penn corpus. cases network architecture single hidden layer lstm units. character-level network input output layers size giving approximately weights total word-level network inputs outputs around weights. comparison therefore somewhat unfair word-level network many parameters. however dataset small networks easily able overﬁt training data clear whether character-level network would beneﬁted weights. networks trained stochastic gradient descent using learn rate neural networks usually evaluated test data ﬁxed weights. prediction problems however inputs targets legitimate allow network adapt weights evaluated mikolov refers dynamic evaluation. dynamic evaluation allows fairer comparison compression algorithms division training test sets data predicted once. since networks overﬁt training data also experiment types regularisation weight noise std. deviation applied network weights start training sequence adaptive weight noise variance noise learned along weights using minimum description length loss function. weight noise used network initialised ﬁnal weights unregularised network. similarly adaptive weight noise used weights initialised network trained weight noise. found retraining iteratively increased regularisation considerably faster training random weights regularisation. adaptive weight noise found prohibitively slow word-level network regularised ﬁxed-variance weight noise only. advantage adaptive weight early stopping needed average value whole test set; perplexity usual performance measure language modelling. table shows word-level performed better characterlevel network appeared close regularisation used. overall results compare favourably collected tomas mikolov’s thesis example records perplexity -gram keyserney smoothing word level feedforward neural network state-of-the-art compression algorithm dynamically evaluated word-level rnn. however combining multiple rnns -gram cache model ensemble able achieve perplexity interestingly beneﬁt dynamic evaluation pronounced mikolov’s thesis suggests lstm better rapidly adapting data ordinary rnns. marcus hutter bowery matt mahoney organised following challenge commonly known hutter prize compress ﬁrst million bytes complete english wikipedia data small possible. include compressed data also code implementing compression algorithm. size therefore considered measure minimum description length data using part coding scheme. contains huge range dictionary words also many character sequences would included text corpora traditionally used language modelling. example foreign words indented tags used deﬁne meta-data website addresses markup used indicate page formatting headings bullet points etc. extract hutter prize dataset shown figs. ﬁrst bytes data evenly split sequences bytes used train network remaining used validation. data contains total one-byte unicode symbols. total number characters much higher since many characters deﬁned multi-symbol sequences. keeping principle modelling smallest meaningful units data network predicted single byte time therefore size input output layers. wikipedia contains long-range regularities topic article span many thousand words. make possible network capture these internal state reset every sequences. furthermore order sequences shuﬄed training usually neural networks. network therefore able access information characters past making predictions. error terms backpropagated start byte sequence meaning gradient calculation approximate. form truncated backpropagation considered language modelling found speed training without aﬀecting network’s ability learn long-range dependencies. much larger network used data penn data seven hidden layers lstm cells giving approximately weights. network trained stochastic gradient descent using learn rate momentum took four training epochs converge. lstm derivates penn data tested network validation data without dynamic evaluation seen table performance much better dynamic evaluation. probably long range coherence wikipedia data; example certain words much frequent articles others able adapt evaluation advantageous. seem surprising dynamic results validation substantially better training set. however easily explained factors ﬁrstly network underﬁt training data secondly portions data much diﬃcult others variant paq- compression algorithm achieves data mainstream compressors generally character level applied text-only version data achieved held-out data improved combined maximum entropy model four page sample generated prediction network shown figs. sample shows network learned structure data wide range diﬀerent scales. obviously learned large vocabulary dictionary words along subword model enables invent feasible-looking words names example lochroom river mughal ralvaldens submandration swalloped. also learned basic punctuation commas full stops paragraph breaks occurring roughly right rhythm text blocks. able correctly open close quotation marks parentheses clear indicator language model’s memory closure cannot predicted intervening text hence cannot modelled shortrange context sample shows network able balance parentheses quotes also formatting marks equals signs used denote headings even nested tags indentation. network generates non-latin characters cyrillic chinese arabic seems learned rudimentary model languages english also generates convincing looking internet addresses network generates distinct large-scale regions headers bullet-point lists article text. comparison figs. suggests regions fairly accurate reﬂection constitution real data signiﬁcant region span hundreds even thousands timesteps. fact network able remain coherent large intervals testament long-range memory. text generated language models sample make sense beyond level short phrases. realism could perhaps improved larger network and/or data. however seems futile expect meaningful language machine never exposed sensory lastly network’s adaptation recent sequences training clearly observed extract. last complete article training intercontinental ballistic missiles. inﬂuence article network’s language model seen profusion missile-related terms. recent topics include ‘individual anarchism’ italian writer italo calvino international organization standardization make felt network’s vocabulary. test whether prediction network could also used generate convincing real-valued sequences applied online handwriting data online handwriting attractive choice sequence generation dimensionality ease visualisation. data used paper taken online handwriting database iam-ondb consists handwritten lines collected diﬀerent writers using ‘smart whiteboard’. writers asked write forms lancaster-oslo-bergen text corpus position tracked using infra-red device corner board. samples training data shown fig. original input data consists co-ordinates points sequence lifted whiteboard. recording errors data corrected interpolating missing readings removing steps whose length exceeded certain threshold. beyond that preprocessing used network trained predict co-ordinates endof-stroke markers point time. contrasts approaches handwriting recognition synthesis rely sophisticated preprocessing feature-extraction techniques. eschewed techniques tend reduce variation data wanted network model. predicting traces point time gives network maximum ﬂexibility invent novel handwriting also requires memory average letter occupying timesteps average line occupying around predicting delayed strokes especially demanding. iam-ondb divided training validation sets test containing respectively handwritten lines taken forms. experiments line treated separate sequence order maximise amount training data used training test larger validation sets training smaller validation early-stopping. lack independent test means recorded results somewhat overﬁt validation set; however validation results secondary importance since benchmark results exist main goal generate convincing-looking handwriting. principal challenge applying prediction network online handwriting data determining predictive distribution suitable real-valued inputs. following section describes done. figure training samples online handwriting database. notice wide range writing styles variation line angle character sizes writing recording errors scribbled letters ﬁrst line repeated word ﬁnal line. idea mixture density networks outputs neural network parameterise mixture distribution. subset outputs used deﬁne mixture weights remaining outputs used parameterise individual mixture components. mixture weight outputs normalised softmax function ensure form valid discrete distribution outputs passed suitable functions keep values within meaningful range mixture density network trained maximising probability density targets induced distributions. note densities normalised therefore straightforward diﬀerentiate pick unbiased sample from contrast restricted boltzmann machines undirected models. mixture density outputs also used recurrent neural networks case output distribution conditioned current input history previous inputs. intuitively number components number choices network next output given inputs far. handwriting experiments paper basic architecture update equations remain unchanged section input vector consists real-valued pair deﬁnes oﬀset previous input along binary value vector ends stroke value otherwise. mixture bivariate gaussians used predict bernoulli distribution used output vector therefore consists stroke probability along means standard deviations correlations mixture weights mixture components. note mean standard deviation dimensional vectors whereas component weight correlation end-of-stroke probability scalar. vectors obtained network outputs figure mixture density outputs handwriting prediction. heatmap shows sequence probability distributions predicted locations word ‘under’ written. densities successive predictions added together giving high values distributions overlap. types prediction visible density small blobs spell letters predictions strokes written three large blobs predictions ends strokes ﬁrst point next stroke. end-of-stroke predictions much higher variance position recorded whiteboard hence large distance stroke start next. bottom heatmap shows mixture component weights sequence. stroke ends also visible here active components switching three places components switching evidently end-of-stroke predictions diﬀerent mixture components in-stroke predictions. point data sequences consisted three numbers oﬀset previous point binary end-of-stroke feature. network input layer therefore size co-ordinate oﬀsets normalised mean std. dev. training set. mixture components used model oﬀsets giving total mixture parameters timestep parameter used model end-of-stroke probability giving output layer size network architectures compared hidden layers three hidden layers consisting lstm cells single hidden layer lstm cells. networks around weights. three layer network retrained adaptive weight noise std. devs. initialised training ﬁxed variance weight noise proved ineﬀective probably prevented mixture density layer using precisely speciﬁed weights. table shows three layer network average per-sequence loss nats lower layer net. however sum-squared-error slightly lower single layer network. adaptive weight noise reduced loss another nats relative unregularised three layer network signiﬁcantly change sum-squared error. adaptive weight noise network appeared generate best samples. fig. shows handwriting samples generated prediction network. network clearly learned model strokes letters even short words also appears learned basic character level language models since words invents look somewhat plausible english. given average character occupies timesteps demonstrates network’s ability generate coherent long-range structures. handwriting synthesis generation handwriting given text. clearly prediction networks described unable this since constrain letters network writes. section describes augmentation allows prediction network generate data sequences conditioned high-level annotation sequence resulting sequences suﬃciently convincing often cannot distinguished real handwriting. furthermore realism achieved without sacriﬁcing diversity writing style demonstrated previous section. main challenge conditioning predictions text sequences diﬀerent lengths alignment unknown data generated. number co-ordinates used write character varies greatly according style size speed etc. neural network model able make sequential predictions based sequences diﬀerent length unknown alignment transducer however preliminary experiments handwriting synthesis transducers encouraging. possible explanation transducer uses separate rnns process sequences combines outputs make decisions usually desirable make information available single network. work proposes alternative model ‘soft window’ convolved text string extra input prediction network. parameters window output network fig. illustrates network architecture used handwriting synthesis. prediction network hidden layers stacked other feeding layer above skip connections inputs hidden layers hidden layers outputs. diﬀerence added input character sequence mediated window layer. window weight timestep intuitively parameters control location window parameters control width window parameters control importance window within mixture. size soft window vectors size character vectors note window mixture normalised hence determine probability distribution; however window weight loosely interpreted network’s belief writing character time fig. shows alignment implied window weights training sequence. note location parameters deﬁned oﬀsets previous locations size oﬀset constrained greater zero. intuitively means network learns slide window step rather absolute location. using oﬀsets essential getting network align text trace. figure synthesis network architecture circles represent layers solid lines represent connections dashed lines represent predictions. topology similar prediction network fig. except extra input character sequence presented hidden layers window layer figure window weights handwriting synthesis sequence point shows value indexes trace along horizontal axis indexes text character along vertical axis. bright line alignment chosen network characters writing. notice line spreads boundaries characters; means network receives information next previous letters makes transitions helps guide predictions. note function well loss derivatives respect outputs remain unchanged eqs. given loss derivative respect size window vector obtained backpropagating output derivatives computation graph fig. derivatives respect window parameters follows synthesis network applied input data handwriting prediction network previous section. character-level transcriptions iam-ondb used deﬁne character sequences full transcriptions contain distinct characters however used subset figure mixture density outputs handwriting synthesis. heatmap shows predictive distributions locations bottom heatmap shows mixture component weights. comparison fig. indicates synthesis network makes precise predictions prediction-only network especially ends strokes synthesis network advantage knowing letter comes next. network architecture similar possible best prediction network three hidden layers lstm cells each bivariate gaussian mixture components output layer size input layer. character sequence encoded one-hot vectors hence window vectors size mixture gaussian functions used window parameters requiring size parameter vector. total number weights increased approximately network trained rmsprop using parameters previous section. network retrained adaptive weight noise initial standard deviation output lstm gradients table shows adaptive weight noise gave considerable improvement log-loss signiﬁcant change sum-squared error. regularised network appears generate slightly realistic sequences although diﬀerence hard discern eye. networks performed considerably better best prediction network. particular sumsquared-error reduced likely large part improved predictions ends strokes error largest. deﬁned sequence sampling ends. examples unbiased synthesis samples shown fig. subsequent ﬁgures generated using synthesis network retrained adaptive weight noise. notice stylistic traits character size slant cursiveness etc. vary this oversight; however interesting result text contains non-letter network must select digits punctuation mark generate. sometimes character inferred context otherwise chosen random. widely samples remain more-or-less consistent within them. suggests network identiﬁes traits early sequence remembers end. looking enough samples given text appears possible virtually combination stylistic traits suggests network models independently text. ‘blind taste tests’ carried author presentations suggest least unbiased samples cannot distinguished real handwriting human eye. nonetheless network make mistakes would expect human writer make often involving missing confused garbled letters; suggests network sometimes trouble determining alignment characters trace. number mistakes increases markedly less common words phrases included character sequence. presumably network learns implicit character-level language model training gets confused rare unknown transitions occur. problem unbiased samples tend diﬃcult read intuitively would expect network give higher probability good handwriting tends smoother predictable handwriting. true read. principled search high probability samples could lead diﬃcult inference problem probability every output depends previous outputs. however simple heuristic sampler biased towards probable predictions step independently generally gives good results. deﬁne probability bias real number greater equal zero. network always outputs mode probable component mixture fig. shows eﬀect progressively increasing bias fig. shows samples generated bias texts fig. another reason constrain sampling would generate handwriting style particular writer easiest would retrain writer only. even without retraining possible mimic particular style ‘priming’ network real sequence generating extension real sequence still network’s memory. achieved real synthesis character string setting character sequence clamping data inputs ﬁrst timesteps sampling usual sequence ends. examples primed samples shown figs. fact priming works proves network able remember stylistic features identiﬁed earlier sequence. technique appears work better sequences training data network never seen. primed sampling reduced variance sampling also combined. shown figs. tends produce samples ‘cleaned version priming style overall stylistic traits slant cursiveness retained strokes appearing smoother regular. possible application would artiﬁcial enhancement poor handwriting. paper demonstrated ability long short-term memory recurrent neural networks generate discrete real-valued sequences complex long-range structure using next-step prediction. also introduced novel convolutional mechanism allows recurrent network condition predictions auxiliary annotation sequence used approach synthesise diverse realistic samples online handwriting. furthermore shown samples biased towards greater legibility modelled style particular writer. several directions future work suggest themselves. application network speech synthesis likely challenging handwriting synthesis greater dimensionality data points. another gain better insight internal representation data manipulate sample distribution directly. would also interesting develop mechanism automatically extract high-level annotations sequence data. case handwriting could allow figure samples biased towards higher probability. probability biases shown left. bias increases diversity decreases samples tend towards kind ‘average handwriting’ extremely regular easy read note even variance disappears letter written diﬀerent points sequence predictions still inﬂuenced previous outputs. look closely last three lines quite exactly same. figure samples primed real sequences. priming sequences shown block. none lines sampled text exist training set. samples selected legibility. figure samples primed real sequences biased towards higher probability. priming sequences blocks. probability bias none lines sampled text exist training set. thanks yichuan tang ilya sutskever navdeep jaitly geoﬀrey hinton colleagues university toronto numerous useful comments suggestions. work supported global scholarship canadian institute advanced research.", "year": 2013}