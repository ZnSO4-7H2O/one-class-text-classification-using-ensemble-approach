{"title": "Learning Mixtures of DAG Models", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We describe computationally efficient methods for learning mixtures in which each component is a directed acyclic graphical model (mixtures of DAGs or MDAGs). We argue that simple search-and-score algorithms are infeasible for a variety of problems, and introduce a feasible approach in which parameter and structure search is interleaved and expected data is treated as real data. Our approach can be viewed as a combination of (1) the Cheeseman--Stutz asymptotic approximation for model posterior probability and (2) the Expectation--Maximization algorithm. We evaluate our procedure for selecting among MDAGs on synthetic and real examples.", "text": "describe computationally eﬃcient methods learning mixtures component directed acyclic graphical model argue simple search-and-score algorithms infeasible variety problems introduce feasible approach parameter structure search interleaved expected data treated real data. approach viewed combination cheeseman–stutz asymptotic approximation model posterior probability expectation–maximization algorithm. evaluate procedure selecting among mdags synthetic real examples. almost decade statisticians computer scientists used directed-acyclic graph models learning data paper consider mixtures models methods choosing among models class. mdag models generalize models accurately model domains containing multiple distinct populations. general hope mdag models lead better predictions accurate insights causal relationships. paper concentrate prediction. take decidedly bayesian perspective problem learning mdag models. principle learning straightforward compute posterior probability model class given data criterion average models select models. computational perspective however learning extremely diﬃcult. problem number possible model structures grows super-exponentially number random variables domain. second problem available methods computing posterior probability mdag model including monte-carlo paper introduce heuristic method mdag model selection addresses diﬃculties. method guaranteed mdag model highest probability experiments present suggest often identiﬁes good one. approach handles missing data component models contain hidden latent variables. approach used learn models incomplete data well. section describe multi-dag mdag models. first however introduce notation. denote random variable uppercase letter value corresponding random variable letter lower case discrete denote number values sometimes refer value state. denote random variables bold-face capitalized letter letters corresponding boldface lower-case letter letters denote assignment value random variable given set. conﬁguration shorthand) denote probability probability density given also denote probability distribution given whether refers probability probability density probability distribution clear context. suppose problem domain consists random variables model graphical factorization joint probability distribution model consists components structure local distribution families. structure directed acyclic graph represents conditional-independence assertions denote structure parameters multi-dag model addition denote structure parameters dag-model component multi-dag model. also denote hypothesis true joint distribution represented mdag model precisely conditional independence assertions implied then joint distribution encoded multi-dag model given follows assume distinguished random variable multinomial distribution. addition exception discussed section limit structure component models parametric families local distributions follows. discrete random variable require every random variable also discrete local distribution families multinomial distributions conﬁguration pai. continuous random variable require local distribution family linear-regressions xi’s continuous parents gaussian error regression conﬁguration xi’s discrete parents. lauritzen refers restrictions conditional-gaussian distribution model. important subclass models gaussian model subclass local distribution family every random variable given parents linear regression gaussian noise. well known gaussian model uniquely determines multivariate-gaussian distribution random variables. model structure model determines shape multivariate-gaussian distribution. thus mdag model class includes mixtures multivariate-gaussian conﬁguration parents structure consistent local distribution families associated model equation discussion assume local distribution families parametric. using denote collective parameters local distributions rewrite equation exception discussed section parametric family corresponding variable determined whether discrete continuous model structure. consequently suppress parametric family notation refer model simply structure denote assertion hypothesis true joint distribution represented model precisely conditional independence assertions implied useful include structure hypothesis explicitly factorization joint distribution compare model structures. particular write structure model encodes limited form conditional independence call context-nonspeciﬁc conditional independence. particular structure implies sets random variables independent given conﬁguration random variables also independent given every conﬁguration general form conditional independence sets random variables independent given conﬁguration dependent given another conﬁguration multi-dag model called bayesian multinet geiger heckerman generalization model encode context-speciﬁc conditional independence. particular multi-dag model distinguished random variable component models encodes joint distribution given state distribution thus multi-dag model encodes joint distribution encode context-speciﬁc conditional independence among random variables structure component model diﬀerent. data restricted variable dxc=c data restricted variables cases term marginal likelihood trivial model single discrete node terms marginal likelihoods component models multi-dag. hence closed form. important issue regarding model selection search models high posterior probabilities. consider problem ﬁnding model highest marginal likelihood models node parents. chickering shown problem np-hard. follows immediately problem ﬁnding multi-dag model highest marginal likelihood multidags node component parents np-hard. consequently researchers heuristic search algorithms including greedy search greedy search restarts best-ﬁrst search monte-carlo methods. consolation various model-selection criteria including marginal likelihood factorable. criterion crit multi-dag structure factorable written follows data restricted parents component dxipac data restricted random variables cases functions. criterion factorable search eﬃcient reasons. component models non-interacting subcriteria search good structure component separately. search good structure component need reevaluate criterion whole component. example greedy search good structure iteratively transform graph choosing transformation improves model criterion most transformation possible. typical transformations include removal reversal addition given factorable criterion need reevaluate it’s parents changed. following sections consider bayesian approach learning multi-dag models mdag models. assume data exchangeable reason data random sample true joint distribution. addition assume true joint distribution encoded multi-dag model uncertain structure parameters. deﬁne discrete random variable whose states correspond possible true model hypotheses encode uncertainty structure using probability distribution addition model deﬁne continuous vector-valued random variable whose conﬁgurations correspond possible true parameters. encode uncertainty using probability density function given random sample true distribution compute posterior distributions using bayes’ rule. model posterior probability various forms model comparison including model averaging work limit selection model high posterior probability. follows concentrate model selection using posterior model probability. simplify discussion assume possible model structures equally likely priori case selection criterion marginal likelihood consider model encodes conditionalgaussian distribution denote random variables corresponding parameters local distribution family buntine heckerman geiger shown that parameters mutually independent given parameter priors conjugate data complete marginal likelihood closed form computed eﬃciently. observation extends multi-dag models. denote random variables corresponding local distribution family component also denote random variables corresponding mixture weights. θ|c| θn|c| mutually independent given parameter priors conjugate data complete marginal likelihood learning multi-dag models given complete data marginal likelihood closed form. contrast learning mdags assumption data complete hold distinguished random variable hidden. data incomplete tractable closed form marginal likelihood available. nonetheless approximate marginal likelihood using either montecarlo large-sample methods thus straightforward class algorithm choosing mdag model search among structures using approximation marginal likelihood. shall refer class simple searchand-score algorithms. shall simple search-and-score algorithms mdag model selection computationally infeasible practice. nonetheless consider approximation marginal likelihood help motivate tractable class algorithms consider next section. approximation examine large-sample approximation ﬁrst proposed cheeseman stutz approximation heuristic chickering heckerman give argument perform well practice. furthermore provide empirical study using multinomial mixtures shows approximation quite good. experiments least accurate sometimes accurate standard approximation obtained using laplace’s method important idea behind cheeseman–stutz approximation treat data completed algorithm real data. idea underlies step algorithm. shall next section idea also applied structure search. simple selecting mdag models ineﬃcient reasons. computing approximations marginal likelihood slow another approximations factor. consequently every time transformation applied structure search entire structure need basic idea behind approach interleave parameter search structure search. schematic approach shown figure first choose initial model parameter values. then perform several iterations algorithm fairly good values parameters structure. next parameter values current model compute expected suﬃcient statistics complete mdag call statistics current model parameters data expected complete model suﬃcient statistics denote quantity ecmss. detailed discussion computation quantity given appendix. next treat expected suﬃcient statistics suﬃcient statistics complete data perform structure search. because pretend data complete model scores closed form factorable making structure search eﬃcient. structure search reestimate parameters structure parameters given expected suﬃcient statistics. finally ecmss computation structure search parameter reestimation steps iterated convergence criterion satisﬁed. remainder section discuss variations approach. addition examine criterion used model search initialization structure parameters approach determining number mixture components number states hidden variables component models. compute suﬃcient statistics complete model want possible dependencies data reﬂected statistics. compute suﬃcient statistics incomplete model models visited duralthough argument chickering heckerman suggests equation accurate approximation marginal likelihood equation less accurate criterion practical reasons. include likelihood ratio correction term equation criterion would factor. ﬁrst term equation would still need compute conﬁguration every structure evaluate. contrast using equation compute conﬁguration once. despite shortcuts experiments described section suggest criterion equation guides structure search good models. approach requires initial structure initial parameterization chosen. first consider structural initialization. initialize structure component model placing every hidden variable every observable variable exception nodes corresponding continuous random variables point nodes corresponding discrete random variables. simpler choice initial graph every component consists empty graph—that graph containing arcs. however initialization restricted priors conjecture approach would unable discover connections hidden observable variables. next consider parameter initialization. mixture components contain hidden continuous variables initialize parameters component structure follows. first remove hidden nodes adjacent arcs creating model next determine conﬁguration given data since data complete respect compute closed form. then create conjugate distribution whose conﬁguration maximum value agrees conﬁguration computed whose equivalent sample sizes speciﬁed user. next non-hidden node conﬁguration xi’s hidden discrete parents initialize parameters local distribution family drawing conjugate distribution described. hidden discrete node conﬁguration xi’s parents initialize multinomial parameters associated local distribution family ﬁxed distribution mixture components contain hidden continuous variables simpler approach initializing parameters random methods initializing parameters distinguished random variable include setting parameters equal setting parameters prior means drawing parameters dirichlet distribution. mentioned approach several variations. source variation heuristic algorithm used search ecmss) computed. options simple search-and-score algorithms include greedy search greedy search restarts best-ﬁrst search monte-carlo methods. preliminary studies found greedy search eﬀective; analysis real data section technique. another source variation schedule used alternate parameter structure search. respect parameter search convergence step ﬁxed number steps number steps depends many times performed search phase. respect structure search perform model-structure transformations ﬁxed number steps number steps depends many times performed search phase local maximum. finally iterate steps consisting computation ecmss structure search either mdag structure change across consecutive search phases approximate marginal likelihood resulting mdag structure increase. under second schedule algorithm guaranteed terminate marginal likelihood cannot increase indeﬁnitely. ﬁrst schedule know proof algorithm terminate. experiments greedy structure search however found schedule halts. convenient describe schedules using regular grammar denote step step computation ecmss structure search respectively. example ∗ecs∗m)∗ denote case where within outer iteration convergence compute expected complete model suﬃcient statistics structure search convergence perform step. another schedule examine ecs∗m)∗. schedule steps computing expected complete model suﬃcient statistics. when structure search leaves model structure unchanged force another iteration outer loop convergence rather steps. model structure changes forced iteration continue iterate steps. experiments indicate that although necessary convergence structure search single step structure searches selects models lower prediction accuracy. found schedule ecs∗m)∗ works well variety problems. finally algorithm described compare neither models contain diﬀerent random variables models random variable diﬀerent number states. nonetheless perform additional search number states discrete hidden variable applying algorithm figure initial models diﬀerent numbers states hidden variables. discard discrete hidden variable model setting number states one. best mdag initialization identiﬁed select overall best structure using criterion. because relatively small number alternatives considered computationally expensive approximation marginal likelihood cheeseman-stutz approximation monte-carlo method. section evaluate predictive performance mdag models real data. addition evaluate assumptions underlying method learning models. domain consider observable random variables continuous. consequently focus attention mixtures gaussian models. accommodate outliers contained data analyze mixture models consider noise component addition gaussian components. noise component modeled multivariate uniform distribution viewed empty model distribution function random variables uniform. compare predictive performance mixtures models mixtures multivariate-gaussian distributions covariance matrices diagonal mixtures multivariate-gaussian distributions covariance matrices full mdiag/n mfull/n model classes correspond mdag models ﬁxed empty structures ﬁxed complete structures respectively gaussian components. suﬃx indicates existence uniform noise component. perform outer search identify number components within mixture model described section particular ﬁrst learn two-component model increase number gaussian mixture components model score clearly decreasing. choose best number components using cheeseman–stutz criterion. then measure predictive ability chosen model dtest test cases |dtest| number test cases. approximate likelihood evaluated parameter conﬁguration. learning mdag/n models ecs∗m)∗ search schedule; learning mdiag/n mfull/n models algorithm convergence. experiments deem converged ratio change likelihood proceeding step change likelihood initialization falls below initialize structure parameters search procedures described section equivalent sample sizes equal example consider addresses digital encoding handwritten digits domain random variables corresponding gray-scale values scaled smoothed -pixel -pixel images handwritten digits obtained cedar u.s. postal service database applications joint prediction include image compression digit classiﬁcation. sample sizes digits range digit samples training remaining samples testing. relatively diﬀuse normal-wishart parameter prior gaussian components mdiag/n mfull/n models. notation degroot prior values rough assessment average gray-scale value pixels identity matrix. choose number observed variables compute parameter values closed form. parameter priors gaussian components mdag/n models normal-wishart priors speciﬁed using hyperparameters described methods described heckerman geiger uniform prior number components mixture learning mdag/n models uniform prior structure component models. know values variables constrained range parameters uniform distribution noise model accordingly. finally hyperparameters dirichlet prior mixture weights noise component gaussian components. figure cheeseman–stutz criterion intermediate model obtained structure search learning three-component model digit abrupt increases around steps occur structure search transitions component. nent. several samples look blurred appear multiple sevens superimposed. generally samples mdag/n component look like sevens distinct style closely resemble mean. turn attention evaluation assumptions underlying learning method. discussed criterion used guide structure search heuristic approximation true model posterior. investigate quality approximation evaluate model posterior using cheeseman-stutz approximation intermediate models visited structure search. heuristic criterion good cheeseman–stutz criterion increase structure search progresses. perform evaluation learning three-component mdag model digit using ecs∗m)∗ schedule. model transitions cheeseman–stutz approximation decreased. overall however shown figure cheeseman–stutz score progresses upward apparent convergence. obtain similar results data sets. results suggest heuristic criterion useful guide structure search. using statistics experiment able estimate time would take learn mdag model using simple search-and-score approach described section time learn three-component mdag model digit using cheeseman–stutz approximation model comparison approximately years computer thus substantiating previous claim intractability simple searchand-score approaches. finally natural question whether cheeseman– stutz approximation marginal likelihood accurate model selection. answer important mdag models select evaluate chosen using approximation. evidence reasonableness approximation provided fact that vary number components mdag models cheeseman-stutz predicbest model class displayed table figure indicates mdag/n models average improve predictive accuracy mfull/n models mdiag/n models. note gains predictive accuracy mfull/n models obtained reducing average number parameters third. using computer time taken learn mdag/n mfull/n mdiag/n models single digit—including time needed optimal number components—is average hours respectively. times could improved using clever search optimal number mixture components. better understand diﬀerences distributions mixture models represent examine individual gaussian components learned mdag/n mfull/n mdiag/n models digit ﬁrst figure shows means components models. mean values variables component displayed grid shade grey indicates value mean. displays indicate components type model capturing distinctive types sevens. however reveal dependency structure component models. help visualize dependencies drew four samples component type model. grid sample shaded indicate sampled values. whereas samples mdiag/n components look like sevens mottled. surprising variables conditionally independent given component. samples mfull/n components mottled indicate multiple types sevens represented compomentioned introduction many computer scientists statisticians using statisticalinference techniques learn structure models observational pearl verma spirtes argued that simple assumptions structures learned used infer cause-and-eﬀect relationships. interesting possibility results generalized structure learned mdag models infer causal relationships mixed populations section present preliminary investigation well approach learn mdag structure. perform analysis follows. first construct gold-standard mdag model model generate data sets varying size. then data approach learn mdag model finally compare structure learned model goldstandard model measure minimum number manipulations needed transform learned component structure corresponding gold-standard structure. gold-standard model mdag model continuous random variables. model three mixture components. structure ﬁrst third components identical structure shown figure structure second component shown figure dags parameterized spatial overlap. particular unconditional means comp comp zero; means comp equal ﬁve; linear coeﬃcients conditional variances table shows results learning models data sets using ecs∗m)∗ schedule. columns table contain number components learned mdag mixture weights three largest components minimum number manipulations needed transform learned component structure corresponding gold-standard structure three components largest mixture weights. manipulations lead model diﬀerent structures family distributions included count. learned mdag structures close gold-standard model. addition although apparent table structure every learned component additional arcs comparison gold-standard model sample sizes larger finally interesting note that essentially structure recovered sample size models hidden variables generalize many well-known statistical models including linear factor analysis latent factor models probabilistic principle component analysis mdag models generalize variety mixtures models including naive-bayes models used clustering mixtures factor analytic models mixtures probabilistic principle component analytic models also work related learning methods. idea interleaving parameter structure search learn graphical models discussed meil˘a jordan morris singh friedman meil˘a consider problem learning mixtures models discrete random variables component spanning tree. similar approach treat expected data real data produce completed data structure search. unlike work replace heuristic model search polynomial algorithm ﬁnding best spanning-tree components given completed data. also unlike work likelihood selection criterion thus take account complexity model. singh concentrates learning single model discrete random variables incomplete data. consider continuous variables mixtures models. contrast approach singh uses monte-carlo method produce completed data sets structure search. friedman describes general algorithms learning models given incomplete data provides theoretical justiﬁcation methods. similar approach approach meil˘a friedman treats expected data real data produce completed data sets. contrast approach friedman obtains expected suﬃcient statistics model using current model. statistics calculated performing probabilistic inference current model although statistics obtained cache previous inferences. approach need perform inference every case missing values compute expected complete model suﬃcient statistics. statistics computed model scores arbitrary structures computed without additional inference. described mixtures models class models general models presented novel heuristic method choosing good models class. although evaluations examples needed preliminary evaluations suggest model selection within expanded model class lead substantially improved predictions. result fortunate evaluations also show simple search-and-score algorithms models evaluated time using montecarlo large-sample approximations model posterior probability intractable real problems. important observation evaluations selection criterion introduce—the marginal likelihood completemodel suﬃcient statistics—is good guide model search. interesting question why? hope work stimulate theoretical work answer question perhaps uncover better approximations guiding model search. friedman initial insight. possibly related challenge theoretical study apparent accuracy cheeseman– stutz approximation marginal likelihood. discussed experiments multinomial mixtures chickering heckerman found approximation least accurate sometimes accurate standard laplace approximation. evaluations also provided evidence cheeseman–stutz approximation accurate criterion model selection. evaluations considered situations component models contain hidden variables. order learn models class methods structure search needed. situations number possible models signiﬁcantly larger number possible dags ﬁxed variables. without constraining possible models hidden variables—for instance restricting number hidden variables—the number possible models inﬁnite. positive note spirtes shown constraint-based methods suitable assumptions sometimes indicate existence hidden common cause between variables. thus possible constraint-based methods suggest initial plausible models containing hidden variables subjected bayesian analysis. section recover structure mdag model fair degree accuracy. observation raises intriguing possibility infer causal relationships population consisting subgroups governed diﬀerent causal relationships. important issue needs addressed ﬁrst however structural identiﬁability. example mdag models superﬁcially diﬀerent structures otherwise statistically equivalent. although criteria structural identiﬁability among single-component models well known criteria well understood mdag models. diciccio kass raftery wasserman computing bayes factors combining simulation asymptotic approximations. technical report department statistics carnegie mellon university friedman learning belief networks presence missing values hidden variables. proceedings fourteenth international conference machine learning. morgan kaufmann mateo friedman bayesian structural algorithm. proceedings fourteenth conference uncertainty artiﬁcial intelligence learning. morgan kaufmann mateo appear. meil˘a jordan morris estimating dependency structure hidden variable. technical report massachusetts institute technology artiﬁcial intelligence laboratory. singh learning bayesian networks incomplete data. proceedings aaai- fourteenth national conference artiﬁcial intelligence providence pages aaai press menlo park appendix examine complete model suﬃcient statistics closely. shall limit discussion multi-dag models component models conditional-gaussian distributions. extension noise component straightforward. consider multi-dag model random variables denote continuous variables denote conﬁguration denote number variables denote discrete variables denote number possible conﬁgurations addition conﬁguration observed variables case note diﬀerent variables observed different cases. finally dempster denote complete case—the conﬁguration case. consider complete model suﬃcient statistics complete case denote vector smii triples scalars vectors length square matrices size particular discrete variables take conﬁguration expectation taken respect joint distribution random variables given observations current case. expectation computed performing probabilistic inference multi-dag model. inference simple extension work lauritzen expectations simply scalar vector matrix additions triple coordinates vector. note that computation described require statistic triple every possible conﬁguration discrete variables. practice however sparse representation store triples complete observations consistent data.", "year": 2013}