{"title": "Investigating the Role of Prior Disambiguation in Deep-learning  Compositional Models of Meaning", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "This paper aims to explore the effect of prior disambiguation on neural network- based compositional models, with the hope that better semantic representations for text compounds can be produced. We disambiguate the input word vectors before they are fed into a compositional deep net. A series of evaluations shows the positive effect of prior disambiguation for such deep models.", "text": "paper aims explore effect prior disambiguation neural networkbased compositional models hope better semantic representations text compounds produced. disambiguate input word vectors compositional deep net. series evaluations shows positive effect prior disambiguation deep models. distributed representations meaning began largely word level need representations semantics larger units text phrases entire documents evident. early attempts compositionality distributed representations used ﬁxed algebraic operations vector addition component-wise multiplication obtain semantic representations larger units text constitutent representations. recently models represented relational words tensors various orders tensor contraction adopted mean composition alongside principally multi-linear methods composition observing emergence non-linear neural network-based compositional approaches derive sentence vector recursively applying neural networks pair word vectors although levels sophistication vary compositional methods distributed semantics share problem take ambiguous word vectors input token represented single vector regardless actual number senses word context appears. solve problem reddy propose disambiguate word vector composition simple additive multiplicative compositional models. idea successfully tested series multi-linear compositional distributional models kartsaklis colleagues paper move step further evaluating effectiveness prior disambiguation step neural compositional models. section discuss models evaluated paper followed description disambiguation procedure adapted models section experimental evaluation procedure presented discussed sections conclude section prior disambiguation positive effect neural models composition. deep learning algorithms capable modelling complex relationships inputs outputs paper interested capturing meaning sentence composing vectorial representations words therein. generic form composition neural network applied pair words denotes concatenation vectors assigned words model parameters non-linear activation function. compositional result vector representing meaning bigram used input compute representation larger text constituent recursive fashion. process continues vectors words sentence merged single vectorial representation serve semantic representation sentence phrase. class models known recursive neural networks variation structure intermediate composition performed autoencoder instead feed-forward network. recursive auto-encoder learns reconstruct input encoded hidden layer faithfully possible. state hidden layer used compressed representation original inputs. since optimization based reconstruction error trained unsupervised fashion. usual practice deep learning models meaning inputs ambiguous word representations possible meanings token merged single vector. paper evaluate methodology input word associated vectors representing different meanings word training corpus. input compositional network select probable meaning vector word given context. general methodology essentially recasts approach deep learning setting; depicted figure ﬁrst word sense induction step order discover latent senses target word. every occurrence target word corpus calculate context vector average neighbours −→ct −→wj distributional vector neighbour. creating context vectors target word apply hierarchical agglomerative clustering order discover sensible groupings hopefully correspond different meanings word. vectorial representation meaning cluster centroid. point target word associated ambiguous vector −→wt meaning vectors assuming arbitrary word context select probable meaning vector word creating context vector −→ct choosing meaning vector closest −→ct meaning vectors distance metric given order test effect prior disambiguation deep learning compositional models disambiguate constituent words simple sentences form subject-verb-object verb phrases verb-object composition number tasks. furthermore evaluate disambiguation strategies ﬁrst disambiguate every word sentence second disambiguation applies verbs usually ambiguous part language. evaluate quality compositional results measuring similarity sentence vectors—a good compositional model able construct sentence vectors reﬂect true semantic relationships among sentences. towards purpose three phrase similarity datasets work grefenstette sadrzadeh kartsaklis mitchell lapata consisting pairs sentences phrases annotated similarity scores human evaluators. task measure extent similarity computed composite vectors matches human judgements. ﬁrst datasets based subject-verb-object structures pair sentences constructed around ambiguous verbs subject object nouns sentences. datasets differ ambiguous verbs selected fact dataset every noun modiﬁed appropriate adjective. dataset word ambiguity play speciﬁc role aspect dataset constitutes natural evaluation test models wild. terms neural composition models implement recnn rae. furthermore simple additive multiplicative models baselines representation sentence derived summing word vectors taking component-wise multiplication them. dataset model evaluation conducted ways. first measure spearman’s correlation computed cosine similarities composite sentence vectors corresponding human scores. second apply relaxed evaluation based binary classiﬁcation task. speciﬁcally human score corresponds pair sentences order decide label pair training results procedure input logistics regression classiﬁer. report -fold cross validation accuracy measure matching rate. results dataset experiment listed tables results quite promising since suggest disambiguation extra step prior composition bring least marginal beneﬁts deep learning compositional models. comparing numbers three datasets effect disambiguation clearest dataset. evaluations carried disambiguation positive effect subsequent composition. encouraging since words dataset chosen ambiguous purpose. words results imply generic sentence prior disambiguation useful pre-processing step might improve ﬁnal outcome never decrease performance. effect disambiguation seems also quite clear dataset whereas result dataset although positive less deﬁnite. speculate reason behind difference dataset constructed example dataset contains verbs alternatives meanings correspond distinct homonymous cases e.g. verb ‘ﬁle’ alternative meanings ‘register’ ‘smooth’. hand dataset contains many polysemous cases exist slight variations senses verbs verb ‘write’ alternative meanings ‘spell’ ‘publish’. terms comparison deep learning models algebraic baselines results clear. despite well-known beneﬁts deep learning methods natural language processing work suggests time simple component-wise compositional operators might constitute hard-to-beat baseline certain tasks although deep learning models general returned superior results second evaluation task could beat additive approach spearman’s correlation measure. fact similar ﬁndings reported previously study blacoe lapata kartsaklis however trying interpret effectiveness approaches need consider generic scenario sentences phrases restricted ﬁxed length structure. obviously advantage deep learning models would signiﬁcant dealing longer text segments. main contribution paper suggests explicitly dealing issue disambiguation effective improve performance deep learning compositional models meaning. simple approach adding prior disambiguation step word vectors beneﬁts small. reasonable future direction then would incorporate explicit disambiguation step within architecture compositional model deals ambiguity training process itself. current work indicates approach much aligned concept deep learning could result drastic improvements performance compositional model.", "year": 2014}