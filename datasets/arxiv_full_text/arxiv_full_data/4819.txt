{"title": "Angrier Birds: Bayesian reinforcement learning", "tag": ["cs.AI", "cs.LG"], "abstract": "We train a reinforcement learner to play a simplified version of the game Angry Birds. The learner is provided with a game state in a manner similar to the output that could be produced by computer vision algorithms. We improve on the efficiency of regular {\\epsilon}-greedy Q-Learning with linear function approximation through more systematic exploration in Randomized Least Squares Value Iteration (RLSVI), an algorithm that samples its policy from a posterior distribution on optimal policies. With larger state-action spaces, efficient exploration becomes increasingly important, as evidenced by the faster learning in RLSVI.", "text": "abstract train reinforcement learner play simpliﬁed version game angry birds. learner provided game state manner similar output could produced computer vision algorithms. improve efﬁciency regular ε-greedy q-learning linear function approximation systematic exploration randomized least squares value iteration algorithm samples policy posterior distribution optimal policies. larger state-action spaces efﬁcient exploration becomes increasingly important evidenced faster learning rlsvi. keywords reinforcement learning q-learning rlsvi exploration model algorithms q-learning randomized least squares value iteration feature extractors position values position indicator nested position counter nested positions shifted counters nested positions counters obstacles results discussion comparison feature extractors rlsvi regular q-learning learning success angry birds wildly successful internet franchise centered around original mobile game players shoot birds slingshot targets points. angry birds largely deterministic game complex physics engine governs ﬂight collision impact appear almost realistically. such optimal gameplay could achieved optimizing highly complex function describes trajectory planning instead train several reinforcement learning algorithms play game compare ﬁnal performance learning speeds human-play oracle. simulate gameplay adapted open-source project recreate angry birds python using pygame framework. adapting base code needs designed exposed game markov decision process largely following conventions class framework. python port used simpliﬁed version original angry birds game includes levels type bird type target type beam. level ends whenever either targets birds screen. targets destroyed agent advances next level otherwise agent loses goes back ﬁrst level. level’s score calculated according three parameters number birds left shattered beams columns destroyed targets. agent loses incurs arbitrary penalty. somewhat reduced state-action space allowed relatively straightforward proof-of-concept believe similar algorithms models would also work complex versions game. game state exposed composed following information. state representation fully describes relevant game situation player’s perspective therefore sufﬁces interface learner game. note parts game state could obtained computer vision algorithms principle direct interaction game mechanics would necessary. figure angry birds gameplay stationary slingshot left used targets right. beam structures destroyed allowing complex consequences interactions chosen actions. simpliﬁed game mechanics making game fully turn-based learner always waits objects motionless taking next action. assumption allowed work absolute positions only. possible actions pair describing extension aiming slingshot thereby bird’s launching momentum direction. someaccelerate learning allowed shooting forwards. within constraint allowed possible extensions angles compare discretizations action space varying levels granularity. base discretization used different actions. reinforcement learning sub-ﬁeld artiﬁcial intelligence describes ways learning algorithm unknown unspeciﬁed decision processes guided merely rewards punishments desirable unwanted outcomes respectively. such reinforcement learning closely mirrors human intuition learning conditioning allows algorithms master systems would otherwise hard impossible specify. erratic exploration possible actions. implemented variation updates belief distribution optimal policies samples policies distribution implies systematic exploration since policies seem unattractive high conﬁdence re-tried. q-learning exposed q-learning model-free algorithm approximates state-action value function qopt order derive optimal policy unknown decision process. order learn optimal behavior unknown game q-learner needs explore system choosing previously unseen actions exploit knowledge thus gathered choosing action promises best outcomes given previous experience. balancing efﬁcient exploration exploitation main challenge reinforcement learning popular method literature ε-greedy methods probability algorithm would ignore previous experience pick action random. method tends work well practice whenever actions needed increase payoff complicated. however complex sets actions need taken order higher reward ε-greedy tends take exponential time rewards feature extractor calculates features given state-action pair. allows exploitation step update weights vector performed fashion similar stochastic gradient descent randomized least squares value iteration osband propose randomized least squares value iteration algorithm differs implementation q-learning points given hyperparameters quality linear approximation bayesian least squares employed estimate distribution around optimal policy derived learner samples policy distribution replaces ε-greedy exploration. given memory tuples bayesian least squares derive expected value optimal policy covariance optimal policy note given assumption linear approximation weights vector fully speciﬁes q-function thereby policy. taking action predicts highest reward. instead taking entirely random actions probability algorithm thus always samples random policies converges optimal means data accumulated variances shrink. therefore less time expected wasted policies highly unlikely successful. keep algorithm erratic policy changes observations made initialize uninformed prior. furthermore inspection showed almost diagonal. save computation therefore decided sample using variances individual weights steady algorithm unfortunately game simulation computationally expensive relatively small number actions simulated forced learn continuous context. feature extractors designing feature extractors followed premises ﬁrst given value function approximation linear clear interaction features would linear; particular separating components locations would work. second wanted give away little domain knowledge game possible task algorithm understand play angry birds without previously concept targets obstacles. premises effectively impose constraint strategies reliable algorithm detect -for examplewhether obstacle front behind target. regardless learner developed impressive performance seen later. reason levels simulator relatively simple. iterated following ideas form meaningful features. relatively complex state-action space feature space fairly quickly spanning several thousand features. turned problem rlsvi memory computational requirements grew intractable. rewriting algorithm sparse matrices offered help ultimately found reasonable performance best-functioning features. position values ﬁrst attempt used rounded target positions feature values every given state would feature values features repeated every action would non-zero action taken. allows different weights different positionaction combinations ultimately implies learning best position-action combinations. including interaction term hoped capture relative distance target slingshot would allowed fast generalization target positions. unfortunately hindsight surprisingly approach failed quickly produced practically learning. position indicator next approach indicator variable grid positions created separate feature every possible position included action taken. created impractically large feature space worked relatively well. however clearly over-ﬁt successful series actions found algorithm would reliably complete level again. target moved pixels however algorithm would retrained completely. nested position counter order solve over-ﬁtting problem developed nested mechanism would generalize unobserved states. achieve this deﬁned nested grids game screen exempliﬁed figure three grids progressively smaller square grid count number targets contained solved generalization issue maintaining nice properties previous feature extractor. larger grids helped generalize ﬁner ones provided remember level already observed. figure nppo grid structure progressively smaller grids create squares within squares extracting speciﬁc location information allowing generalization. nested positions shifted counters issue especially larger squares targets close square boundary captured adequately. improve this created copy grid shifted diagonally half square size. every target exactly squares allowing judge whether target left right within square. npps therefore feature sets three overlapping square grids each. surprise npps performed worse simpler npp. assume much bigger feature space fact could learn algorithms full convergence computational limits. nested positions counters obstacles finally tried address issue obstacles described want give away gameplay-related information representing ﬁxed relationship targets obstacles. therefore added counters obstacles case npps surprised adding information obstacles detrimental learning success indeed adding obstacle information stopped learning altogether. npps suspect nppo work well given time converge bloated feature space. somewhat crude feature extractor provided best results rslvi indeed outperform regular ε-greedy q-learner. rlsvi particularly impressive ability clear levels almost speed human player. could simulate gameplay convergence result computational limitations game engine used intended simulations even speed signiﬁcantly graphics disabled. noteworthy starts high baseline score improve much there fact learner quickly masters level generalize following levels. consequence tends stuck achieve relatively constant game scores ﬁrst levels. npps shows similar slope supporting explanation sheer number features slows learning process down. however npps clearly outperformed within simulation time frame. somewhat surprised complete failure nppo improve time. rlsvi regular q-learning exploring state-action space according posterior beliefs indeed produce better results ε-greedy q-learner. trained algorithms feature extractor compared moving average game score attempt. rlsvi achieved higher scores overall simulated time frame learned quickly. comparison algorithms given ﬁgure noted implementation rlsvi required signiﬁcantly memory computation regular q-learner since required keeping complete history observed features matrix operations large data blocks. figure moving average different feature extractors. dysfunctional position values feature extractor excluded. display moving average game score next attempts number attempts made successively used attempt explore recorded following attempt without exploration order distort results exploration behavior. y-axis units points. figure rlsvi faster achieves greater rewards q-learning. display moving average game score next attempts number attempts made successively used attempt explore recorded following attempt without exploration order distort results exploration behavior. y-axis units points. provided game engine used attempt. regular q-learner rlsvi however outperformed human player terms maximum points achieved single attempt terms highest level reached. table summarizes maximum score attained highest level reached different players. comparing number attempts required pass given level provides interesting insights especially later levels rlsvi’s exploration proves efﬁcient passes levels almost number attempts required human player. features regular q-learning required times many attempts seen table rlsvi’s bayesian approach state-action space exploration seems like promising rather intuitive navigating unknown systems. previous work shown bayesian belief updating sampling exploration highly efﬁcient ﬁndings conﬁrm. rlsvi beat baseline q-learning algorithm somewhat embarrassingly human oracle. main constraint work simulation speed limiting algorithm convergence could achieve. would liked explore possible features especially computation. vein would interesting train deep neural network function allow learning complex interaction effects particular targets obstacles. promising results combining bayesian exploration deep reinforcement learning shown volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv preprint arxiv. bution assumptions within rlsvi; assuming least-squares noise normally distributed therefore sampling normal distribution around expected optimal policies particularly prone stuck local minima. using bootstrapping methods lieu closed-form bayesian updates prove powerful improvement rlsvi algorithm explored here.", "year": 2016}