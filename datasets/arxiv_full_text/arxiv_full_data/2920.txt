{"title": "Continual Learning with Deep Generative Replay", "tag": ["cs.AI", "cs.CV", "cs.LG"], "abstract": "Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (\"generator\") and a task solving model (\"solver\"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.", "text": "attempts train comprehensive artiﬁcial intelligence capable solving multiple tasks impeded chronic problem called catastrophic forgetting. although simply replaying previous data alleviates problem requires large memory even worse often infeasible real world applications access past data limited. inspired generative nature hippocampus short-term memory system primate brain propose deep generative replay novel framework cooperative dual model architecture consisting deep generative model task solving model models training data previous tasks easily sampled interleaved task. test methods several sequential learning settings involving image classiﬁcation tasks. distinctive ability humans large primates continually learn skills accumulate knowledge throughout lifetime even small vertebrates rodents established connections neurons seem last year besides primates incorporate information expand cognitive abilities without seriously perturbing past memories. ﬂexible memory system results good balance synaptic plasticity stability continual learning deep neural networks however suffers phenomenon called catastrophic forgetting model’s performance previously learned tasks abruptly degrades trained task. artiﬁcial neural networks inputs coincide outputs implicit parametric representation. therefore training towards objective cause almost complete forgetting former knowledge. problem obstacle continual learning deep neural network sequential training multiple tasks. previous attempts alleviate catastrophic forgetting often relied episodic memory system stores past data particular recorded examples regularly replayed real samples drawn task network parameters jointly optimized. network trained manner performs well separate networks trained solely task major drawback memory-based approach requires large working memory store replay past inputs. moreover data storage replay viable real-world situations. notably humans large primates learn knowledge even limited experiences still retain past memories. several biological mechanisms contribute multiple levels apparent distinction primate brains artiﬁcial neural networks existence separate interacting memory systems complementary learning systems theory illustrates signiﬁcance dual memory systems involving hippocampus neocortex. hippocampal system rapidly encodes recent experiences memory trace lasts short period reactivated sleep conscious unconscious recall memory consolidated neocortex activation synchronized multiple replays encoded experience mechanism inspired experience replay training reinforcement learning agents. recent evidence suggests hippocampus simple experience replay buffer. reactivation memory traces yields rather ﬂexible outcomes. altering reactivation causes defect consolidated memory co-stimulating certain memory traces hippocampus creates false memory never experienced properties suggest hippocampus better paralleled generative model replay buffer. speciﬁcally deep generative models deep boltzmann machines variational autoencoder generate high-dimensional samples closely match observed inputs. propose alternative approach sequentially train deep neural networks without referring past data. deep generative replay framework model retains previously acquired knowledge concurrent replay generated pseudo-data. particular train deep generative model generative adversarial networks framework mimic past data. generated data paired corresponding response past task solver represent tasks. called scholar model generator-solver pair produce fake data desired target pairs much needed presented task produced pairs interleaved data update generator solver networks. thus scholar model learn task without forgetting knowledge teach models generated input-target pairs even network conﬁguration different. deep generative replay supported scholar network retains knowledge without revisiting actual past data framework employed various practical situation involving privacy issues. recent advances training generative adversarial networks suggest trained models reconstruct real data distribution wide range domains. although tested models image classiﬁcation tasks model applied task long trained generator reliably reproduces input space. term catastrophic forgetting catastrophic interference ﬁrst introduced mccloskey cohen claimed catastrophic interference fundamental limitation neural networks downside high generalization ability. cause catastrophic forgetting studied analytically known neural networks parameterize internal features inputs training networks samples causes alteration already established representations. several works illustrate empirical consequences sequential learning settings provide primitive solutions replaying previous data. branch works assumes particular situation access previous data limited current task. works focus optimizing network parameters minimizing alterations already consolidated weights. suggested regularization methods dropout regularization help reduce interference learning furthermore elastic weight consolidation proposed demonstrates protecting certain weights based importance previous tasks tempers performance loss. attempts sequentially train deep neural network capable solving multiple tasks reduce catastrophic interference augmenting networks task-speciﬁc parameters. general layers close inputs shared capture universal features independent output layers produce taskspeciﬁc outputs. although separate output layers free interference alteration earlier layers still causes performance loss older tasks. lowering learning rates parameters also known reduce forgetting recently proposed method called learning without forgetting addresses problem sequential learning image classiﬁcation tasks minimizing alteration shared network parameters. framework network’s response task input prior ﬁne-tuning indirectly represents knowledge tasks maintained throughout learning process. handful works devoted designing complementary networks architecture alleviate catastrophic forgetting. training data previous tasks accessible pseudoinputs pseudo-targets produced memory network task network. called pseudorehearsal technique method claimed maintain input-output patterns without accessing real data tasks elementary coupling binary patterns simply feeding random noises corresponding responses sufﬁces recent work proposes architecture resembles structure hippocampus facilitate continual learning complex data small binary pixel images however none demonstrates scalability high-dimensional inputs similar appear real world difﬁculty generating meaningful high-dimensional pseudoinputs without supervision. generative replay framework differs aforementioned pseudorehearsal techniques fake inputs generated learned past input distribution. generative replay several advantages approaches network jointly optimized using ensemble generated past data real current data. performance therefore equivalent joint training accumulated real data long generator recovers input distribution. idea generative replay also appears mocanu trained restricted boltzmann machine recover past input distribution. generative model refers model generates observable samples. speciﬁcally consider deep generative models based deep neural networks maximize likelihood generated samples given real distribution deep generative models variational autoencoders gans able mimic complex samples like images. gans framework deﬁnes zero-sum game generator discriminator discriminator learns distinguish generated samples real samples comparing data distributions generator learns mimic real distribution closely possible. objective networks thereby deﬁned ﬁrst deﬁne several terminologies. continual learning framework deﬁne sequence tasks solved task sequence tasks. deﬁnition task optimize model towards objective data distribution training examples drawn. next call model scholar capable learning task teaching knowledge networks. note term scholar differs standard notion teacher-student framework ensemble models networks either teach learn only. deﬁnition scholar tuple generator generative model produces real-like samples solver task solving model parameterized solver perform tasks task sequence full objective thereby given minimize unbiased losses among tasks task sequence entire data distribution loss function. trained task model samples drawn consider sequential training scholar model. however training single scholar model referring recent copy network equivalent training sequence scholar models n-th scholar learns current task knowledge previous scholar hn−. therefore describe full training procedure figure training scholar model another scholar involves independent procedures training generator solver. first generator receives current task input replayed inputs previous tasks. real replayed samples mixed ratio depends desired importance task compared older tasks. generator learns reconstruct cumulative input space solver trained couple inputs targets drawn real replayed data. here replayed target past solver’s response replayed input. formally loss function i-th solver given dpast cumulative distribution past data. second loss term ignored functions replayed data refer ﬁrst solver. build scholar model solver suitable architecture solving task sequence generator trained generative adversarial networks framework. however framework employ deep generative model generator. figure sequential training scholar models. training sequence scholar models equivalent continuous training single scholar referring recent copy. generator trained mimic mixed data distribution real samples replayed inputs previous generator. solver learns real input-target pairs replayed input-target pairs replayed response obtained feeding generated inputs previous solver. prior main experiments show trained scholar model alone sufﬁces train empty network. tested model classifying mnist handwritten digit database sequence scholar models trained scratch generative replay previous scholar. accuracy classifying full test data shown table observed scholar model transfers knowledge without losing information. table test accuracy sequentially learned solver measured full test data mnist database. ﬁrst solver learned real data subsequent solvers learned previous scholar networks. section show applicability generative replay framework various sequential learning settings. generative replay based trained scholar network superior continual learning approaches quality generative model constraint task performance. words training networks generative replay equivalent joint training entire data generative model optimal. draw best possible result used wgan-gp technique training generator. base experiment test generative replay enables sequential learning compromising performance neither tasks task. section sequentially train networks independent tasks examine extent forgetting. section train networks different related domains. demonstrate generative replay enables continual learning design scholar network also compatible known structures. section show scholar network gather knowledge different tasks perform meta-task training network disjoint subsets training data. compare performance solver trained variants replay methods. model generative replay denoted ﬁgure specify upper bound assuming situation generator perfect. therefore replayed actual past data paired predicted targets solver network. denote case exact replay. also consider opposite case generated samples resemble real distribution all. case denoted noise. baseline naively trained solver network denoted none. notation throughout section. common experimental formulation used continual learning literature simple image classiﬁcation problem inputs images mnist handwritten digit database pixel values inputs shufﬂed random permutation sequence unique task. solver classify permuted inputs original classes. since most pixels switched tasks tasks technically independent other good measure memory retention strength network. figure results mnist pixel permutation tasks. test performances task sequential training. performances previous tasks dropped without replaying real meaningful fake data. average test accuracy learnt tasks. higher accuracy achieved replayed inputs better resembled real data. observed generative replay maintains past knowledge recalling former task data. figure solver generative replay maintained former task performances throughout sequential training multiple tasks contrast naively trained solver average accuracy measured cumulative tasks illustrated figure solver generative replay achieved almost full performance trained tasks sequential training solver alone incurred catastrophic forgetting replaying random gaussian noises paired recorded responses help tempering performance loss training independent tasks network inefﬁcient information shared. thus demonstrate merit model reasonable settings model beneﬁts solving multiple tasks. model operating multiple domains several advantages model works single domain. first knowledge domain help better faster understanding domains domains completely independent. second generalization multiple domains result universal knowledge applicable unseen domains. phenomenon also observed infants learning categorize objects encountering similar diverse objects young children infer properties shared within category make guess category object belong tested model incorporate knowledge domain generative replay. particular sequentially trained model classifying mnist street view house number dataset vice versa. experimental details provided supplementary materials. figure accuracy classifying samples different domains. models trained mnist svhn dataset vice versa. previous data recalled generative replay knowledge ﬁrst domain retained real inputs predicted responses replayed sequential training solver alone incurs forgetting former domain thereby resulting average performance figure samples trained generator mnist svhn experiment training svhn dataset iterations. samples diverted ones mimic either svhn mnist input images. figure illustrates performance original task task solver trained alone lost performance task data replayed since mnist svhn input data share similar spatial structure performance former task drop zero decline critical. contrast solver generative replay maintained performance ﬁrst task accomplishing second one. results worse replaying past real inputs paired predicted responses solver cases model trained without replay data achieved slightly better performance task network solely optimized solve second task. generative replay compatible continual learning models well. instance learning without forgetting replays current task inputs revoke past knowledge augmented generative models produce samples similar former task inputs. requires context information task performed task-speciﬁc output layers tested performance separately task. note scholar model generative replay need task context. figure compare performance algorithm variant lwf-gr task-speciﬁc generated inputs maintain older network responses. used training regime proposed original literature namely warming network head amount time tuning whole network. solver trained original algorithm loses performance ﬁrst task ﬁne-tuning begins alteration shared figure performance augmented generative replay classifying samples domain. networks trained svhn mnist database. test accuracy svhn classiﬁcation task dropped shared parameters ﬁne-tuned generative replay greatly tempered loss networks achieved high accuracy mnist classiﬁcation illustrate generative replay recollect past knowledge even inputs targets highly biased tasks propose experiment network sequentially trained disjoint data. particular assume situation agent access examples classes time. agent eventually correctly classify examples classes sequentially trained mutually exclusive subsets classes. tested networks mnist handwritten digit database. note training artiﬁcial neural networks independently classes difﬁcult standard settings network responses change match target distribution. hence replaying inputs outputs represent former input target distributions necessary train balanced network. thus compare variants described earlier section perspective whether input target distributions cumulative real data recovered. models input target distributions represent cumulative distribution. noise model maintains cumulative target distributions input distribution mirrors current distribution. none model current distribution both. figure models sequentially trained tasks task deﬁned classify mnist images belong labels. case networks given examples ﬁrst task second manner. networks achieved test performance close upper bound. figure divided mnist dataset disjoint subsets contains samples classes. networks sequentially trained subsets observed naively trained classiﬁer completely forgot previous classes learned subset data recovering past output distribution without meaningful input distribution help retaining knowledge evidenced model noise generator input assume past data completely discarded trained generator mimic current inputs generated samples previous generator. generator thus reproduces cumulative input distribution encountered examples far. shown figure generated samples trained generator include examples equally encountered classes. introduce deep generative replay framework allows sequential learning multiple tasks generating rehearsing fake data mimics former training examples. trained scholar model comprising generator solver serves knowledge base task. although described cascade knowledge transfer sequence scholar models little change formulation proposes solution topically relevant problems. instance previous scholar model past copy network learn multiple tasks without explicitly partitioning training procedure. comparable approaches regularization methods careful training shared parameters shown catastrophic forgetting could alleviated protecting former knowledge network. however regularization approaches constrain network additional loss terms protecting weights potentially suffer tradeoff performances tasks. guarantee good performances tasks train huge network much larger normally needed. also network maintain structure throughout tasks constraint given speciﬁc parameter ewc. drawbacks framework also twofold performance highly depends relevance tasks training time task linearly increases number former tasks. deep generative replay mechanism beneﬁts fact maintains former knowledge solely input-target pairs produced saved networks allows ease balancing former task performances ﬂexible knowledge transfer. importantly network jointly optimized towards task objectives hence guaranteed achieve full performance former input spaces recovered generator. defect generative replay framework efﬁcacy algorithm heavily depends quality generator. indeed observed performance loss training model svhn dataset within setting employed section detailed analysis provided supplementary materials. acknowledge completely exclusive contribute memory retention different levels. nevertheless method poses constraints training procedure network conﬁgurations straightforward mixture frameworks. believe good three frameworks would give better solution chronic problem continual learning. future works generative replay extend reinforcement learning domain form continuously evolving network maintains knowledge past copy self. also expect improvements training deep generative models would directly performance generative replay framework complex domains. bornstein arterberry. development object categorization young children hierarchical inclusiveness perceptual attribute group versus individual analyses. developmental psychology fagot cook. evidence large long-term memory capacities baboons pigeons implications learning evolution cognition. proceedings national academy sciences girshick donahue darrell malik. rich feature hierarchies accurate object detection semantic segmentation. proceedings ieee conference computer vision pattern recognition pages goodfellow pouget-abadie mirza warde-farley ozair courville bengio. generative adversarial nets. advances neural information processing systems goodfellow mirza xiao courville bengio. empirical investigation catastrophic forgetting gradient-based neural networks. arxiv preprint arxiv. kirkpatrick pascanu rabinowitz veness desjardins rusu milan quan ramalho grabska-barwinska overcoming catastrophic forgetting neural networks. proceedings national academy sciences netzer wang coates bissacco reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning volume page robins. catastrophic forgetting neural networks role rehearsal mechanisms. artiﬁcial neural networks expert systems proceedings. first zealand international two-stream conference pages ieee", "year": 2017}