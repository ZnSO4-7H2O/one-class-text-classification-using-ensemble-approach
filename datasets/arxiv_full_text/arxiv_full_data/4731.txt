{"title": "A Survey of Inductive Biases for Factorial Representation-Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. A factorial representation is compact and faithful, makes the causal factors explicit, and facilitates human interpretation of data. Factorial representations support a variety of applications, including the generation of novel examples, indexing and search, novelty detection, and transfer learning.  This article surveys various constraints that encourage a learning algorithm to discover factorial representations. I dichotomize the constraints in terms of unsupervised and supervised inductive bias. Unsupervised inductive biases exploit assumptions about the environment, such as the statistical distribution of factor coefficients, assumptions about the perturbations a factor should be invariant to (e.g. a representation of an object can be invariant to rotation, translation or scaling), and assumptions about how factors are combined to synthesize an observation. Supervised inductive biases are constraints on the representations based on additional information connected to observations. Supervisory labels come in variety of types, which vary in how strongly they constrain the representation, how many factors are labeled, how many observations are labeled, and whether or not we know the associations between the constraints and the factors they are related to.  This survey brings together a wide variety of models that all touch on the problem of learning factorial representations and lays out a framework for comparing these models based on the strengths of the underlying supervised and unsupervised inductive biases.", "text": "resurgence interest neural networks representation learning re-emerged central focus artiﬁcial intelligence. representation learning refers discovery useful encodings data make domain-relevant information explicit. factorial representations identify underlying independent causal factors variation data. factorial representation compact faithful makes causal factors explicit facilitates human interpretation data. factorial representations support variety applications including generation novel examples indexing search novelty detection transfer learning. article surveys various constraints encourage learning algorithm discover factorial representations. dichotomize constraints terms unsupervised supervised inductive bias. unsupervised inductive biases exploit assumptions environment statistical distribution factor coeﬃcients assumptions perturbations factor invariant assumptions factors combined synthesize observation. supervised inductive biases constraints representations based additional information connected observations. supervisory labels come variety types vary strongly constrain representation many factors labeled many observations labeled whether know associations constraints factors related survey brings together wide variety models touch problem learning factorial representations lays framework comparing models based strengths underlying supervised unsupervised inductive biases. compact faithful explicit interpretable statistical independence useful properties factorial representations factorial representations non-factorial datasets environments comparison gaussian sparsity bias sparse environment invariance bias cooperative vector quantizer inductive biases invariance combination bias bilinear models trilinear models functional parts non-negative matrix factorization models functional-parts bias hierarchical layers recursive models hierarchical layers bias constraint types direct equality distance inequality analogy evaluation representations examples models semi-supervised inverse graphics network siamese network multiple-maps t-sne karaletsos deep visual analogy-making transforming auto-encoders disentangling boltzmann machines desiderata supervised models factor-agnostic factor-speciﬁc encoder proportion dataset labeled proportion factors labeled generator hierarchy constraints advent modern deep neural nets representation learning re-emerged primary focus machine learning. representation learning refers discovery useful representations data. data samples drawn environment example photographs faces audio recordings music. data representation means transforming examples space. typically particular task mind data representation allows easily recover attributes data relevant task hand. suppose task create searchable index dataset. best representation task might summarize face photographs attributes ethnicity hair color color. might similarly represent music tempo genre artist signature. good representation makes explicit relevant information task. ethnicity hair color represented pixels image tempo audio recording explicit representation makes acting information much easier. diﬀerent tasks environments place diﬀerent demands representations characteristics commonly associated good representations useful across wide variety tasks. good representations compact faithful information represented input. also explicitly represent attributes required task hand. finally interpretable humans. faithful representation preserves information observation little distortion possible. perfectly faithful representation course achieved transformation input good representations also need encode information compactly. equivalent representation eﬃcient compression input. faithfulness representation evaluated terms task it’s intended used with. diﬀerent tasks diﬀerent demands much input need represented. classiﬁcation task might need know object category. image compression program represent non-noise features image. representation medical test results probably fully invertible meaning perfectly reconstruct input representation. representation explicitly encode attributes necessary whatever task hand without interference attributes sources noise. attributes removed low-level observation; example facial expression observed complex conjunction muscle movements around mouth eyes jaw. attributes often combined complex ways attributes. consider glasses figure interact identity pose lighting conditions onto cast shadow face. explicit representations attributes invariant kinds variations identity change image slightly corrupted noise light source moved even though many pixels face change. explicit representations also invariant small local changes noise input. representation attribute makes information explicit structure reﬂects information represented categorical discrete continuous attribute needs modeled variable appropriate type. additionally attributes singlemulti-dimensional. example color represented single dimension three-dimensional vector choice variable type depends environment task. many tasks rely data representations interpretable humans. representations often better easily interpreted humans regardless whether humans directly not. example task predicting face gender much easier presence facial hair makeup directly represented code. representations interpretable need unique. representation exchanged another equally valid diﬃcult attach meaning also identiﬁable meaning tied named concept attribute. described three desirable properties representations compactness/faithfulness expressivity intepretability. describe type representation called factorial representation help achieve goals. factorial representation attributes statistically independent. factor independent attributes. generative story landscapes location number sheep largely independent location number trees. another common example blind source separation room full people speaking assume probability person speaking independent others speaking simultaneously. independence assumption places restrictions environment factors truly independent task depend independent causes. right environment task factorial representations provide numerous beneﬁts provide useful bias learning provide compact/faithful explicit interpretable representations. horace barlow argued biological computational utility factorial representations variables statistically independent joint probability distribution factorial. consider example colored shapes figure observation encoded combination shape color. occurrence probability shape color one-fourth occurrence probability combination one-sixteenth. two-factor representation factorial joint distribution factorized product distributions components consequently component factors statistically independent another redundancy. factorial representation faithful factors modeled minimizes redundancy them. therefore representations also maximally compact. factorial representations also interpretable representation explicitly captures true independent generative factors factors unique identiﬁable. shape color example illustrates nicely describe images compact faithful explicit interpretable shape color. independent factors need invariant respect another change shape alter impression image’s color. properties make factorial representations useful certain types tasks found original dataset. fact manipulate representation generate examples full cartesian product factor combinations could generate women’s styles glasses figure independence assumption factorial representation easily violated. violated method used sample environment environment itself. consider case missing data dataset generally datasets sampling bias certain combinations factors systematically missing sampled combinations. imbalance factors course also caused noise. bias often environment non-factorial. datasets natural dependencies example facial hair gender naturally dependent. factorial representation teases apart facial hair gender over-generate assume facial hair gender independent woman beard likely beard according representation. tasks representing dependencies critical factorial representation inappropriate. example person’s identity might naturally correlated style glasses worn person. task generate example typical images people want consider person’s glasses style part identity. tasks might want ignore dependency. task high-security face veriﬁcation want sure program falsely accept impostor wears glasses user. setting imposing bias factorial representation helpful. last section noted imposing bias factorial representation result improved model explicitness well better interpretability. paper explores diﬀerent inductive biases used learn factorial representations variety environments tasks. section highlight several diﬀerent approaches biasing representations factorial. biases makes diﬀerent assumption generative environment. first look biases assume diﬀerent distributions coeﬃcients factor. next look biases allow factor invariant changes output related factor. another class bias look ways factors combine generate observed data. finally look apply supervision bias improve factorial representations. since supervision mostly orthogonal arbitrarily combined types bias given dimension model chart figure unsupervised biases leverage assumptions generative environment learn factorial representations supervised biases speciﬁc type prior domain knowledge. here focus inductive biases highly generic meaning could apply almost observation domain images audio medical test results. biases considered allow learn representations explicit interpretable biases allow gain better understanding underlying causal structure regardless observation domain. therefore avoid describing inductive biases speciﬁc apply range domains. consistency introduce unsupervised inductive bias way. first describe generative assumptions model. then describe inductive bias consistent environment corresponding learned representation structure. finally show examples inductive bias action mention models similar inductive bias. possible also show examples incorporate multiple inductive biases simultaneously. section explore inductive biases discovering factorial representations motivated statistical distribution factors environment. highlight diﬀerences distribution biases mostly focus methods also linear bias assumption factors linearly related linearly combined observation. keeping bias constant makes easier highlight diﬀerences distributional bias. applicable also highlight non-linear models incorporate distributional biases. describe diﬀerent assumptions distribution coeﬃcients factors lead type inductive bias. environments factor coeﬃcients follow gaussian distribution. describe inductive bias ﬁnds factorial representations environment gaussian-distributed factors. show naturalistic environments often non-gaussian sparse. likewise describe bias corresponds sparse environments. sparse environments show sparsity bias produces representations better aligned true causal structure environment representations gaussian-distributed factors. generative assumptions. factorial representation ideally non-redundant statistically independent factors. since factors jointly gaussian-distributed independent uncorrelated independent factors gaussian environment reduce minimize correlation pairs factors. principle components analysis model learns representations zero correlation latent factors. also restricts dimensions uncorrelated columns orthogonal. variables jointly gaussian independent uncorrelated. therefore decorrelation ﬁnds independent components case gaussian-distributed causes. constrained consider components linearly related input. produces list components ordered amount variance input explain. considering components trade compactness faithfulness representation. representation structure. pca’s representation simple observation mapped vector scalar factor coeﬃcients unfortunately representations learned unique representations unique rotations latent space means representation learned often sample many possible representations equally valid according pca. presents obvious problems interpret results since solution exchanged other equally valid solutions. family related models share similar characteristics probabilistic adds isotropic gaussian observation noise equation factor analysis extends adding diagonal instead isotropic noise. linear autoencoders essentially variant trained stochastic gradient descent. approaches reducing removing correlations representations. example added cross-covariance penalty loss function autoencoder. extra penalty term seeks directly minimize pairwise correlations hidden units. author used direct inhibitory feedback connections representation factors eﬀect decreasing correlations factors. variational auto-encoder adds bias gaussian-distributed factors autoencoder imposing gaussian prior representation covariance matrix diagonal. corresponds assuming factors gaussian uncorrelated/independent. autoencoders nonlinear models thus share linear bias appears left model chart figure generative assumptions. factors many environments non-gaussian. section show methods ﬁnding factorial representations non-gaussian settings also solve uniqueness problem pca. first describe theoretical motivation decorrelation insuﬃcient non-gaussian settings leads generic method ﬁnding factorial representations called independent components analysis ica. then describe subset non-gaussian distributions called sparse distributions appropriate naturalistic environments. distributions lead speciﬁc type called sparse ica. describe non-ica methods discovering non-gaussian independent factors. independent factors gaussian-distributed essential impose stronger bias decorrelation. correlation random variables depends ﬁrst moment joint however independence generalized measure non-correlation variables independent positive integer values jointly gaussian-distributed variables higher-order correlation structure non-correlation suﬃcient condition independence. however factors variation many naturalistic environments non-gaussian. finding true independent causes environments requires stronger inductive bias. inductive bias. approach ﬁnding truly independent factors non-gaussian factors also faithful input data. intuitively learned component actually true independent non-gaussian factors look gaussian according central limit theorem. true non-gaussian source factors exhibit maximal non-gaussianity. maximization non-gaussianity connection information theory well equivalent maximizing mutual information observed data learned representation forms basis group models called independent components analysis ica. many variants including original neural networks approach information-theoretic approaches statistical approaches models like generally assume linear relationship observation representation research developing non-linear ica-like methods generative assumptions. many factors found naturalistic environments sparsely distributed. example consider photographs objects. many possible objects photographs display small number them given object occur relatively small number photographs. sparsity interpreted ways limited number factors present given observation factor present relatively observations. course components naturalistic images sparse example mean luminance images tends follow roughly uniform distribution sub-gaussian. however sparsity well-matched features naturalistic images relevant tasks object classiﬁcation. inductive bias. environments sparse factors oﬀer nice opportunity employ stronger independence bias. sparse distributions type non-gaussian distribution called super-gaussian distribution. distributions higher kurtosis gaussian. examples super-gaussian distributions include laplacian cauchy distributions. sparsity also used ﬁeld sparse coding unique solutions over-complete problems. transform adding laplace prior centered/peaked zero represents shape parameter equivalent parameter governing loss tradeoﬀ. purpose optimization often minimize negative logarithm equation leads formula formulation equation called reconstruction reconstruction loss equation also replaced restriction representation fully invertible helps prevent degenerate solutions. course methods sparse factors methods explicitly seek sub-gaussian factors non-gaussian characteristic. likewise methods employ sparsity reduced ica; example sparse autoencoders allow non-linear relationships observation representation. next section describe non-ica methods non-gaussian factors. representation structure. representation exactly like observation mapped vector factor coeﬃcients. methods ﬁnding non-gaussian independent factors explicitly maximize non-gaussianity factors. like sparse algorithms make assumption environment; example assume factors binary make assumption generative process environment. binary factorial representations learned. paper reconstruction error minimized alongside additional penalty term predictability factor given factors. penalty term made tractable assumption representations binary coeﬃcients. penalty works non-gaussian settings looks dependencies beyond correlations. cooperative vector quantizer composed several vector quantizer units make weighted contribution output. vector quantizer viewed implementing sparsity unit within active time. combination outputs several vector quantizers models environment several groups sparse factors contribute observation. author found factorial representations discovered non-sparse settings designing mapping function discourage over-cooperation factors closely reﬂect generative structure environment. austerweil griﬃths investigate humans might select features category learning. indian buﬀet process prior corresponds assumption humans learn independent features features covary grouped together. model predicts human categorization behavior experiments. relationship seen model organization chart figure fairly high inductive bias linear mapping assumption. diﬀerence inductive bias models obvious comparing equations extra term objective. problem pca/decorrelation bias well-matched environments independent components sparse. illustration problem found recreated figure left. synthetic dataset consists -dimensional vectors numbers number either factor controls position black rectangle left separate independent factor controls position black rectangle right. factors synthetic environment sparse rectangle side active time. also dataset fully factorial showing combinations positions each. trained model dataset. right figure mean vector components learned rows matrix. visualization pixels scaled grey represents neutral weight black represents negative coeﬃcient white represents positive coeﬃcient. basis learned compact faithful four factors explain variation dataset. however representation interpretable generative process synthesized dataset apparent representation. words bias decorrelation insuﬃcient explaining true generative process environment. bottom right figure decomposition dataset. components shown rows matrix parlance called mixing matrix. four components reﬂect causal structure environment better component describes activity side image. representation compact faithful representation since original examples reconstructed four components. figure synthetic dataset like nine -dimensional test data samples positions black rectangles left right sides controlled independent processes. mean vector four components learned model. mean vector four components learned model. another example improves interpretability shown figure using naturalistic face dataset introduction. again trained model dataset. left components learned right components learned ica. dataset several independent sources variation including identity lighting conditions glasses style. subjectively component seems highlight mixture person identity glasses style lighting conditions. components tend focus cause glasses emphasized identity lighting conditions de-emphasized. also notable unlike produce ordered list components terms much variance explain. note components isolate causal factors perfectly still limited inductive bias linear mapping observation representation cannot disentangle glasses rest face. environment image patch interpreted generated number localized factors operate independently diﬀerent scales crest wave generated independently overall lighting scene. center basis right basis components scaled ﬁgures. images components code global features diﬀerent shown bias factorial representations help learn better representations given factorial environment. sparse environments observed naturalistic images decorrelation insuﬃcient bias learn factorial representations. stronger bias sparsity shown improve representations. solutions also unique make interpretable non-unique solutions. next describe bias assumes factor representations insensitive variations expression unrelated factor. factors vary expressed. objects appear various locations image rotated scaled. linear methods like unable express type variation tend learn redundant representations factors describe ways factor realized. environments would prefer representation factor invariant random ﬂuctuations factor expressed. generative assumptions. many factors expressed input way. nature scene animals might appear diﬀerent parts image. would nice able represent presence animal coherent factor rather code animal location. likewise texture pattern image might show various phases rotations. introductory example shapes colors shape factor invariant color vice versa. term view refer possible realizations factor e.g. location rotation angle translation. views factor compete cooperate generate observation. example competition locations animal appear nature scene locations mutually exclusive. however views could exist extremes locations observations would interpolate views. ﬁrst show inductive bias consistent environment factors compete views within factor cooperate. show inductive bias consistent environment factors cooperate views within factor compete. inductive bias. method learning invariant factors views cooperate pool similar views together. case view corresponds linear component dimensionality input like ica. magnitude coeﬃcients pool coeﬃcient factor. sparsity applied pooled layer eﬀect making pools compete other. subspace formed pool represents views factor. independent subspace analysis extension implements pooling method. generative model ﬁrst choose small number factors present generate observation linearly combining elements within pools representing active factors. model layers ﬁrst linear layer second pools linear components together form ﬁnal representation. ﬁrst layer coeﬃcients like linearly related input sparsity applied pooled layer pools individual components within pool encouraged sparse. side-eﬀect encouraging components pool dependent other increase cooperation. intuitively components pool independent equation gaussian according central limit theorem. sparsity objective eliminates possibility encourages components pool dependent. complex structure layers. factor representation structure. second layer accompanied vector coeﬃcients corresponding subspace representation therefore composed scalar coeﬃcient well coordinate corresponding subspace. examples. visualization pools shown figure authors applied dataset nature images used figure visualization learned components inside matrix shown figure case number components pool four pools ordered sparsest ones according top. analysis indicates pools less sensitive phase features learned meaning case factors images every four consecutive components corresponds pooled component. positive weights shown light regions negative weights shown dark weights close zero gray. phase-invariance. solution components highly localized space operate diﬀerent scales pool’s components scale localized within region image seem oriented way. course many types inductive bias invariant features especially image processing known classes variation. models often hard-wired invariance types variations. biases helpful learning respective domains used many models later sections. however domain-speciﬁc adaptations concerned speciﬁcs environment representation. therefore purpose largely orthogonal purpose work describe learn factorial representations environment. inductive bias weaker introduces non-linearity mapping function reduces number components sparsity applied reduced setting number groups equal number ﬁrst-layer components. therefore appears left model chart figure components within factor compete. methods cooperative pools. factor models cooperative pools used variety contexts. pooling used overlapping spatial grid create topographic representation kohonen’s adaptive-self organizing maps topographic maps ﬁrst level latent representation grid. pooling happens overlapping sub-regions spatial grid. encourages sub-region similar ﬁlters grid cell spatial coherency. topographic model incorporating sparsity found inductive bias. environments views compete factors cooperate. factor represented vector quantizer corresponds pool views compete explain observation. array cooperate generate output. composed pool linear components like isa. architecture called cooperative vector quantizer unlike components pool mutually exclusive active time. additionally competition imposed factors. assumed present time. within pool pools themselves. representation structure. representation structure identical factor represented pool components linearly related input. examples. trained dataset overlapping shapes reproduced figure dataset black-and-white image generated combining three white shapes cross diagonal line empty square number diﬀerent locations. examples three shapes although shapes completely overlapping. therefore example three cooperating factors factor realized component. trained three pools/vqs. components pools shown right figure ﬁrst pool clearly represents diagonal line various locations image. second represents empty square last represents cross. example model learned represent shape invariant location. models encourage representations invariant minor perturbations input. contractive regularization penalty used encourage factors insensitive local directions variation observation space denoising autoencoders also encourage representations invariant noise added observation. highlight three diﬀerent modeling biases determine factors interact generating observations. simplest strongest inductive bias factor combination linear bias. linear model observations generated weighted summation factor components. coeﬃcients either negative positive factor cancel contributions factors. examples linear bias. first discuss bias allows multiplicative interactions occur small factors. environments factors mutually exclusive explaining portions observation called functional parts; describe biases work well environments. finally environments composed hierarchy layers factors generate observations increasing levels detail decreasing levels abstraction. environments small number factors known always present generative assumptions. interactions responsible generating observations. factor number views view interacts views factors. example face dataset introductory section expressed interaction person identity glasses style interactions factors account large portion variation images. person’s identity modulates glasses face glasses style modulates shadow cast person’s face. inductive bias. environments motivation behind general class multilinear models multiplicative interactions factors account variability observations. first describe simplest case multilinear model called bilinear model factors. describe multilinear models combined ica-like sparsity penalty. finally show example three interacting factors. bilinear model factors interact generate observation. like factor bilinear model represented pool coeﬃcients. coeﬃcient factor interacts multiplicatively coeﬃcients factor. combination coeﬃcients associated component vector dimensionality input. weight component determined product coeﬃcients associated observation generated summation weighted components. factors named content style factors. example consider environment upright characters italicized characters. appearance character completely determined interaction factors identity character style. factor bilinear model represented pooled structure like isa. suppose modeling factors representations components pool components pool weight matrix three-dimensional component weighted interaction bilinear models minimize reconstruction loss. interpret model model representation matrix factored interactions vectors zat. model essentially conjunctive given coeﬃcient non-zero non-zero. bilinear model encourages similarity rows columns component matrix help understand eﬀect consider diagram figure coeﬃcient acts gate components encourages active inputs thus correlated similar other. bilinear model encourages similarity rows columns figure components factor assumed present. representation structure. multilinear representations pools represent factors like cvq. however unlike components associated pool encouraged consistent pool pool. figure illustration grouping structure bilinear representation factors composed sub-factors. circles represent components weight component determined product coeﬃcients groups part examples bilinear models. simple example bilinear model found also includes sparsity constraint sparsity means that factor pool sample small number components present observation. sparse bilinear loss function looks like settings govern tradeoﬀ reconstruction loss sparsity penalties authors compared sparse bilinear model. dataset used training generated randomly selecting image patch locations drawn naturalistic images. image patch location image patches generated shifting patch location pixels horizontally. authors construct sparse bilinear model image patch feature represented pool name translation modeled another pool named model trained using custom training procedure encouraged represent image patch features represent translations. training procedure also used supervised bias utilizing knowledge relationship image feature translations. supervised bias covered thoroughly later section. authors also trained model dataset. right figure labeled example bilinear basis sample components corresponding factors. rows correspond factor columns correspond factor components consistent represent image patch feature diﬀerent translations. components column consistent represent image patch feature given translation. left figure labeled example linear basis components model shown similar components examples chosen bilinear model. learns represent similar components would redundantly represent diﬀerent translations image patch feature rather grouping together. figure shows response sample image patch labeled canonical patch. note sparse coeﬃcients signiﬁcantly nonzero. according model canonical patch mixture translated version canonical patch also shown labeled translated patch. vector patch diﬀerent canonical patch instead activating activates representing translation. figure visualization example linear model left corresponding bilinear model right negative component pixels represented dark regions positive pixels light regions. bilinear models. bilinear models assume sparse environment learn represent translations. example tensor analyzers bilinear models assume gaussian-distributed factors applied model lighting conditions identity face dataset. factors variation dataset identity emotion. authors also model third factor call group. third factor allows model represent local consistency without requiring global consistency. identity expression consistent within group necessarily consistent outside group. factors known spike-and-slab variables variable binary spike subvariable indicating whether variable continuous slab sub-variable indicating weight factor. coeﬃcient component nonzero three spike variables value factor acts gate factors. block rows represent identity columns represent emotion. additionally blocks provide local consistency identity ﬁrst left-hand block correspond identity ﬁrst right-hand block. similar multilinear model used supervised setting separate face identity emotion pose factors. multilinear models good choice environments known factors known exist known strong interactions. terms inductive bias multilinear models weaker isa. allow multiplicative interactions coeﬃcients whereas allows linear combinations. reﬂected model chart figure sparse bilinear model reduced setting weights factor pool reduced reducing model factor. generative assumptions. environments factor responsible generating part given observation. example face broken number independent parts eyes eyebrows mouth nose hair etc. overlap parts interactions factors. models suited environments often called parts-based functional parts models. linear environment factor components combined summation construct output. functional parts environment factor components concatenated construct output. inductive bias. model capable learning parts-based representations non-negative matrix factorization like reconstructs observation multiplying weight matrix representation vector. however weight matrix representation vector constrained non-negative corresponds following loss function case positive coeﬃcients positive component vectors allowed. generate zero output observed dimensions summing positive component negative component cooperation allowed zero output cancel out. components zero. eliminates type cooperation factors encourages represent independent portions input. corresponds generative model functional-part factor cannot remove anything another functional part; contribute positively observation. bias leads highly localized components. representation structure. structure representations learned factor’s coeﬃcient scalar value. examples. figure shows example parts-based representations faces representations model shown. also shown weights reconstruction example face marked original. reconstructions equally good models representation clearly parts-based factors correspond eyes noses mouth. before factors highly global. inductive bias fairly high. still essentially linear exception non-negativity constraint. obviously higher bias since subset unclear whether bias higher ica. therefore resides coordinate model chart figure means discovering parts-based representations. factorial parts-based model learned restricting element observation correspond explanatory factor vector quantizer. viewed another application notion sparsity instead sparse factor coeﬃcients figure visualization face representation learned non-negative matrix factorization compared pca. positive component pixels shown dark regions zero pixels shown white negative pixels shown red. note representation negative components. functional parts bias easily lead non-factorial representations used improper environments. face example face parts course independent. morphology face highly inﬂuenced gender ethnicity course correlations symmetry. however tasks want ignore types correlations treat parts independent anyway. generative assumptions. environments multiple levels abstraction. example nature scene might composed foreground background. first choose locations foreground background. foreground composed group trees rocks sampled independently. choose shape rock independent shape rocks. layer represents choices conditionally independent given higher level layer. inductive bias. recursive multi-layered version sparse algorithm applied iteratively desired number layers reached. ﬁrst representation layer r-ica output ica. ﬁrst layer prepared next layer transforming outputs function purpose reformat highly-non-gaussian coeﬃcients gaussian. gaussianized ﬁrst layer form second layer procedure repeated desired number layers reached. details gaussian transformation procedure found involves discarding sign representation transforming uniform distribution applying prior transforming result back gaussian using inverse gaussian distribution. signs discarded carry redundancy signal removing helps reveal nonlinear dependencies. representation structure. structure r-ica multi-layered extended arbitrary number layers. examples. example r-ica shown figure algorithm trained dataset naturalistic images. sample gabor-like components shown along responses second layer arranged position gabor-like ﬁlter frequency orientation. second-layer factors strongly position-oriented whereas others seem oriented towards patterns frequency orientation. dark regions representing negative pixels light regions positive. weights ﬁrst layer selection second-layer factors. square represent weights ﬁrst-layer factors second-layer factor. square represents weight components ﬁrst layer warmer colors represent positive weight cooler colors negative. dots arranged based location center ﬁlter. factor coeﬃcients second layer ica. dots panel arranged frequency orientation ﬁtted gabor ﬁlter polar coordinates. similar approach learning higher-order statistical regularities using hierarchical framework explored also made hierarchy form hierarchical layers invariant factors. r-ica representation less restrictive ica; enables discovery abstract factors. also multi-layered method fact construct isa-like representations r-ica model. since r-ica superset includes lower inductive bias isa. however r-ica model multiplicative interactions cannot whether lower higher inductive bias multilinear model therefore appears location chart figure layered deep mixture experts model found empirically represent location object ﬁrst layer experts identity object second layer. models trained sequentially; ﬁrst layer estimated output used input next layer. neural network variant hierarchical model entire model trained simultaneously. described three diﬀerent unsupervised inductive biases factorial representation learning distributional bias invariance bias combination bias. describe another type inductive bias supervisory signal combined arbitrarily last three. supervisory data another source bias additional information connected observations constrain representation factor. identity classiﬁcation task face dataset face identity would useful factor represent explicitly confounded non-identity factor. supervised signal make representation learning completely unnecessary representation already available. however supervisory signal might always available might weak. need model works data labeled. supervisory signal specify many factors representation others still need learned. supervisory signal might available portion training data. supervisory signals also come weaker forms provide constraints representation specify directly. supervised factors help make representations interpretable extent factors described supervisory data interpretable. supervision also useful learning factorial representations case non-factorial dataset environment. various supervisory constraints listed table sorted strongly constrain solution strongest constraints top. speciﬁcally consider constrained space possible learned type constraint detail types tasks representations constraint well-matched also discuss strength constraint reasoning position table present examples literature illustrate incorporation constraints models. direct constraints commonly used attributes data easily assign value binary attribute categorical label scalar quantity. example class image handwritten digit categorical variable closed zero nine. direct constraints strongest constraints considered here explicitly state setting factor. basic non-direct constraint equality simply groups examples together equal regard particular factor. equality constraints simpler generate direct constraints example humans face images person diﬀerent identities rather asking label identity image dataset. equality constraints useful learning open-class representations categorical variable. example face database might identity label associated image. task learn representation useful face veriﬁcation subjects training database useful simply learn categorical representation indicating people represented image. instead we’d prefer representation could accommodate identities. instead directly constraining representation constrain representation satisfy equalities database. another common constraint type distance equivalently similarity constraint. distance constraints commonly used three ways quantify scalar diﬀerence objective factor values specify objective similarity objects specify human’s subjective similarity rating objects. objective similarities come diﬀerences high dimensional feature vectors. notoriously diﬃcult visualize representation respects distances useful. example images tend high dimensional diﬃcult visualize relationships many images simultaneously. another common source distance information object co-occurrences example rate word similarity based often occur together sentence. subjective similarities come humans. example might want create visualization dataset birds designed help understand human judgments bird similarity. visualization could cartesian space point represents bird distance birds reﬂective human judgments similarity. visualization could help understand features humans attend making similarity judgments. case direct constraint would speciﬁc coordinate value bird. however human judgment image similarity known complex based simultaneous comparison many features. therefore labeling task diﬃcult humans. setting direct equality constraint appropriate. could collect human judgments similarity pairs bird pictures judgments constrain representation. motivation solutions like multidimensional scaling creates cartesian space distance pair reﬂective similarity ratings humans. pair objects distance constrains solution without specifying exactly coordinates pictures. many models dealing distances designed learn space satisﬁes constraints. however one-space assumption valid case multiple factors might contribute comparison. example consider case intransitive similarities. example bird similar respect color. bird also similar blue bird respect animal type. however similar blue bird. correctly factorize space distance constraints assigned color animal type easy satisfy constraints. human judgments distances known unreliable diﬀerent users employ diﬀerent scales judge objects sequential eﬀects cause drift judgments based order presentation stimuli. solution issues collect inequality constraint instead distance. inequality constraints commonly used collect human judgments similarity avoiding issues direct similarity judgments. acquire inequality constraint present human labeler anchor object alternatives similar anchor object. user chooses resulting inequality constraint representation encourage given point inequality degrees freedom. given point free anywhere space constrained inside open ball around positive radius r+). since choice maps open ball constraining rather many sizes sets together. since inequalities weaker constraint distances reﬂected position table another class constraints called analogies. humans adept analogical reasoning able extrapolate answers hypothetical questions what face viewed diﬀerent angle? what piece music played higher tempo?. analogy constraints convenient situations direct constraints possible access pairs observations expect relationship same. example might images people completing high school completing college. interpret pairs analogy constraint learn factor representation. interpreted given representation free anywhere space given still free anywhere else space however given determine exact location constrain space further. size constrained since analogies weaker constraint inequalities lowest level supervision table direct constraints useful learning representations factors unambiguous category quantity. equality constraints almost strong easier obtain useful learning open-class representations categorical variable. objective measures similarity best modeled distances human judgments similarity best modeled inequalities. analogies weakest constraint applicable situations relationships among observations available. best representation task might correspond type constraint available. example might direct constraints face identity form categorical labels. however might want represent faces categorical variable. instead might choose representation distance related similarity faces understand faces similar. case direct constraints transformed equality constraints. present models showing examples across entire spectrum constraint types. model shown present evidence factorial representations. presenting examples need discuss evaluate factorial representations papers. supervised context opportunity directly measure properties factorial representations. three broad categories evidence demonstrate factorial representations distillation make information easily accessible evidenced results improved classiﬁcation performance example reconstructions traverse factor manifold showing variation factor. nuisance variation even non-supervised factors. therefore expect better factor distillation even non-supervised factors evidenced improved classiﬁcation performance example reconstructions. next describe example models constraints used learn factorial representations. model summarize constraints method used incorporating constraint show examples demonstrate evidence factorial representations available. also describe modes combine several diﬀerent types constraints. direct constraints commonly applied directly encouraging representation mirror labels. example variational auto-encoder modiﬁed also include categorical label data point form semi-supervised standard observations generated ﬁrst choosing representation zero-mean unit gaussian distribution diagonal covariance corresponding bias independent gaussian factors. observation generated non-linear function representation parameters ssvae account partially-labeled datasets marginalizing datapoints label available. representation composed factors class label remainder representation. optimization follows procedure similar vae. authors trained ssvae standard mnist dataset composed small black-and-white images hand-written digits. authors show evidence factor distillation ssvae accurate classifying digit identity given image. authors also demonstrate cross-over eﬀects representing digit identity using encouraged represent sources variation. figure shows example eﬀect. model trained using two-dimensional vector. image block samples model clamped represent digit within block dimensions varied smoothly. nearby samples correspond similar styles handwriting left region corresponds non-slanted styles right region represents slanted styles. similar models. many semi-supervised models direct constraints fashion. example authors class labels part representation ladder network form deep denoising autoencoder. report state-of-the-art performance semi-supervised mnist cifar- tasks. examples using direct constraints alongside higher encoder inductive biases. example supervised versions direct constraints straightforward constraints appropriate types factors easily quantiﬁed binary categorical numerical variables. next describe weaker kind constraint exact value factor known. deep convolutional inverse graphics network uses equality constraints syntheticallygenerated face images. autoencoder trained explicitly represent number factors including azimuth angle face elevation angle azimuth angle light source. faces synthetically generated authors unique ability constraints factors variation images. dc-ign representation split represent three factors explicitly represent remaining factors using single pool neurons. ﬁrst factor neuron represents face azimuth second face elevation third represents light source azimuth. remaining pool neurons represents nuisance factors including face identity morphology expression. authors developed custom auto-encoder training procedure varies factor time holding factors variation constant. representations non-varying factors encouraged learning procedure remain constant. training pool representing nuisance factors treated factor. figure shows sample reconstructions network shows evidence factor distillation disentangling. left sample input images reconstructions. remainder grid formed manipulating representation elevation changing smoothly range values leaving factors unchanged. reconstructions face elevation changing face azimuth light source identity remain constant. authors demonstrate variation factors well. authors explore cross-over eﬀects. figure varying elevation random test samples. scatter plot showing inferred elevation factor representation ground truth setting used render face image. noted equality constraints useful learning open-set representations categorical variables. procedure also called learned similarity metric. method shown siamese network learns low-dimensional embedding mnist digits similarity corresponds likelihood digit class. used siamese network build embedding representation face identity. instead clamping procedure siamese networks trained loss minimizes distance same-category examples maximizes distance out-of-category examples. given examples whose factor values factor corresponding representations loss function authors performed experiments using speech dataset consisting diﬀerent speakers speaking words. embedding trained represent speaker identity anther trained represent target phonemes. instead euclidean distance diﬀerences equation used cosine similarity. networks trained subset librispeech dataset consisted hours read speech speakers. test model authors performed task estimate classiﬁcation accuracy. error rates reported percentage correct answers tests. tests examples triphones test phonetic discrimination examples speaker examples matched phonetic content pronounced diﬀerent speakers. allows testing directly measure disentanglement well distillation. figure shows table results table stands siamese stands triamese network trained using loss function took positive negative sample account simultaneously. number model name indicates many stacked frames included input. mfsc baseline model simply concatenates mfcc frames together uses embedding. task refers whether speaker phonetic embeddings trained separately together performance reported embeddings tasks lower error rates indicating better performance. strong evidence distillation trained embeddings learn perform better predicting target factor. also evidence disentanglement trained embeddings perform worse non-target factor. also indirect evidence cross-over eﬀects since double models tend perform better single models especially speaker identiﬁcation. model accounts distances multiple factors multiple-maps t-sne extension t-distributed stochastic neighbor embedding t-sne distances pairs observations converted probabilities; form joint distribution pairs objects proportional similarity objects dataset used composed human-labeled word associations. humans given prompt word asked name associated words. words used prompts total words labeled associations. word associations counted word co-occurrence. co-occurrences made symmetric normalized probabilities. case factors roughly correspond topics lead intransitive similarities. example word might co-occur frequently tuxuedo topic formal wear. word might also co-occur frequently rope topic related ropes knots. however rope likely co-occur tuxuedo. factorial representation separate topics allow model represent intransitive similarities. goal learn word-representation distances reﬂective probabilities facilitate goal learning procedure deﬁnes probability distribution distances minimizes klmodel extended include multiple factors. factor deﬁnes space words mapped. word factor model learns importance weight importance weights zero factors word. given combination words distance calculation multiplied product factor-speciﬁc importance weights. eﬀect penalizing model respecting distances words high importance weight factor. updated factors optimization ﬁnds best settings well original co-occurrence probabilities respected. model automatically assigns words factors depend prior knowledge factors associations factors similarity ratings makes constraints model factor-agnostic. number factors must known advance. figure shows example maps model. model trained maps/factors two-dimensional. dataset contains many topics typically multiple topics factor. every contains every word words importance weight shown. corresponds sports fashion whereas bottom corresponds topics including criminal justice kitchen equipment ropes/knots. placed near tuxuedo near rope bottom map. maps topic coherency evidence distillation. every every topic evidence disentangled representation. case t-sne fact constraints factor-agnostic severely weakens constraint space multiplying size space number possible factors constraint could assigned times number factors/maps. triplet inequalities used learn factorial representation variational autoencoder oracle answers questions form similar terms factor answer question triplet interpreted factor-speciﬁc inequality simf simf factors inequality speciﬁed training signal. number factors triplets associated factor speciﬁed advance. choice important determines distances attended learning. t-distribution chosen instead gaussian allows points slightly similar high dimensional space become apart dimensional representation solution crowding problem high-dimensional spaces. model learns dimensions contribute explaining inequality. factor-speciﬁc mask selectively attends dimensions representation relevant factor hides irrelevant dimensions. factor-speciﬁc mask variable -dimensional constrained interval distance function takes mask account comparing dimensions representation examples optimization involves maximizing factor-speciﬁc triplets alongside reconstruction error representation regularization terms vae. masks learned optimization. model learns switch appropriate subspace represents factor explain triplets drawn respect factor. authors test model yale faces dataset composed images individuals varying lighting conditions. azimuth elevation light sources varied images. triplet composed three randomly selected images. three factors variation identity light source azimuth light source elevation used create triplet inequalities three randomly chosen images. example light source azimuth triplet would provide information images similar azimuths. authors demonstrate factor distillation masked model performs better identity classiﬁcation azimuth prediction elevation prediction compared model trained constraints. figure shows visualization three factor-speciﬁc masks learned face dataset. masks learn factorize space subset dimensions contribute decisions comparing identity light source position. dimension-reduced visualization dimensions masked subspaces shown right figure subspaces group similar examples together would expected. similar models. similar approach taken demonstrated model’s ability learn factorial representations datasets stylized fonts images shoes. factor-agnostic triplets used figure visualization three factor-speciﬁc masks learned yale faces dataset t-sne visualization identity azimuth elevation subspaces derived masked dimensions right. dimensions weight greater used input visualization. colors represent actual values factor interest. deep visual analogy-making diﬀerent types constraints used learn factorial representation autoencoder architecture. equality constraints used enforce factorization representation diﬀerent attributes analogy constraints used shape subspace representing factor. constraints incorporated autoencoder architecture constraint-speciﬁc loss function. corresponds vector-addition interpretation analogy. however interpretation make sense operations rotations. rotations circular suﬃcient rotation cause point back started. address issue authors include additional interpretations analogy. non-overlapping sets factors called additionally factors included choose three example observations share factors share factors. corresponds factor-speciﬁc equalities binary mask dimension representation serves select representation dimensions corresponding dimensions corresponding mask learned; speciﬁed advance. case expect able reconstruct using dimensions corresponding dimensions corresponding corresponds following loss function demonstrate model authors train dataset images video game characters called sprites rotated renderings. example sequence reconstructions analogies shown figure sequence pose changes character transferred character bottom. character identity pose. three models built ﬁrst trained analogy constraints second trained separate identity pose using equality constraints third trained classify identity direct constraints predict pose equality constraints. according constraint hierarchy table model strongest constraints third model followed second ﬁrst. prediction results shown figure paper follow pattern. authors demonstrate factor distillation improved reconstruction performance test analogies. figure example predictions autoencoder trained using analogies animation transfer results sprites dataset. analogy prediction results models trained three diﬀerent combinations constraints. similar models. visual analogies also used learn representations neural word representations known obey analogical reasoning models explicitly trained using analogy constraints. approach combines equality distance constraints teach model transform observation setting factor another observation diﬀerent known setting factor. discrepancy factor values input output input model. approach also uses equality constraints requires factors remain equal varying target factor. example transforming auto-encoder based series auto-encoder capsules. capsule presented image input trained reconstruct translated version input image. amount horizontal vertical translation input network training. illustration three capsules reproduced left figure units represent recognition units units green represent generation units. variables represent translation input image. inputs network represent translation output image. variable represents probability object represented capsule present image multiplied output generation units produce output capsule. authors demonstrate factor distillation translated mnist digits stereo image pairs rotated cars. case learns representation translation rotation strongly correlates true value. distillation demonstrated scatterplot outputs capsule figure right. similar models. representation faces learned separates view identity trained transform images similar using direct constraints pose equality constraints identity. similar approach taken generates sequence transformed views input image using recurrent network. figure illustration three transforming auto-encoder capsules scatterplot showing vertical axis output capsule input image horizontal axis output capsule image translated pixels direction. figure diagram disbm model architecture example reconstructions factors mixed observations. left block images represents transfer expression right block represents transfer pose. left hand image represents identity. identity combined expression pose middle image form image right. disentangling boltzmann machine adds supervisory constraints restricted boltzmann machine multilinear bias. paper authors explore various combinations constraints explore eﬀect constraining factors representation. figure shows diagram model architecture. hidden representation divided sets units model factors. diagram visible units hidden units associated factor associated another. direct constraints applied adding extra layer whose predict class label factor. encourages units represent information related class. version model direct constraints labeled disbm figure equality constraints applied diﬀerent ways. first authors clamping procedure units corresponding factor encouraged constant pair. example represents face identity clamp units images individual. similar spirit clamping procedure used second method incorporating equality constraints involves adding penalty term encourage representations similar individual. approach loss used siamese network. authors tested models face datasets toroto face dataset multi-pie. datasets individuals diﬀering identities. datasets units correspond identity. units correspond expression multi-pie correspond pose. right hand side figure example pose expression transfer examples. identity units pose expression units combined diﬀerent images form image demonstrates disentangling factors. authors performed experiments diﬀerent levels supervision investigated disentangling eﬀect supervising factor disentangling other. report performance prediction target factor value well prediction value non-target factor. representation factorial performance target factor improve performance non-target factor degrade. tables figure show predictive performance factor subspace multi-pie dataset dataset bottom. table authors explore diﬀerent models. example multi-pie naive refers mutlilinear model without supervision. labels model pose variable supervised direct constraints. clamp uses clamping procedure learn identity. labels clamp combination manifold uses penalty term learn factors. naming convention applies pose replaced expression. model pool units authors report test accuracy prediction task. test accuracy pose emotion prediction percentage correct accuracy veriﬁcation reported rate zero one. multi-pie table evidence distillation supervised models generally higher performance naive model. also constraining factor improves performance pose units labels predict pose better naive model. similar eﬀect clamp also also cross-over eﬀect pose units clamp perform better pose estimation naive model. eﬀect similar observed ssvae adding supervised constraints factor tends remove related variability factors. pattern results generally holds tfd. supervised models performing better naive decreasing performance non-target tasks. also cross-over eﬀects units labels perform better veriﬁcation units naive. three four cases best test accuracy seems come manifold training. discussed various constraints incorporated example models help discover factorial representations. however models vary along four dimensions well. order completely characterize models summarize dimensions variation since also relevant constructing factorial representations supervisory signals. table shows model laid dimensions. constraints factor-speciﬁc meaning label directly tied known factor part representation. constraints also factor-agnostic constraint satisﬁed factor particular factor indicated label. example people attend diﬀerent features rating bird image similarity person attend primarily head color another person might attend tail color. case label collected human simply similarity judgment without indication features person attending case factors compared hidden model infer factor similarity constraint associated with. model fully described factor-agnostic t-sne. however models factor-agnostic constraints model kind encoder means mapping observation representation. models t-sne simply ﬁxed lookup table. avoids additional inductive bias also disallows looking representation un-labeled observation. model make direct features low-level data observations uses distance constraints. therefore without distance labels observation representation observation. models discussed kind functional encoder. various encoders models diﬀerent inductive biases ssvae dc-ign karaletsos models based variational auto-encoder turn uses bias gaussiandistributed factors. disbm model based multilinear architecture combination biases described earlier. siamese network based neural architectures lower unsupervised inductive bias encoder. stronger constraints also labels larger portion factors representation. naturalistic environments cannot hope label factors variation example consider factors face glasses dataset. easy come labels face identity glasses style. diﬃcult label position light source face’s pose likely factors variation beyond might trouble even identifying. represented factors labeled stronger constraints tasks image generation require mapping representation back observation. models suited tasks include functional generator function maps representation back observation. models except siamese network t-sne functional generator component. functional generator puts heavy burden representation; needs able reconstruct input representation. case subset factors labeled means factors unsupervised. case ssvae disbm observed cross-over eﬀects constraining factor help learning unsupervised factors. functional generator also makes producing fully-labeled dataset diﬃcult. means need know full factors contributed generating observations. explains models generator fully labeled dataset synthetically-created datasets. many diﬀerent ways supervision vary rank strength supervision across diﬀerent models? paper type constraint primary indicator supervision strength. concerns related proportion examples labeled dataset proportion factors labeled fairly dataset-speciﬁc interested diﬀerences constraints particular test environments papers. whether constraint factor-agnostic important. however noted earlier concern move constraint type overall ranking make given constraint type slightly weaker. model also include supervisory information using multiple constraint types. constraint types mixed ranking model chart determined weakest type constraint used. wide variety biases help discover factorial representations bias comes associated generative assumptions. demonstrated sparsity bias help learn factorial representations generative environment sparse. likewise invariance bias helps bring coherency factors expressed variety ways. diﬀerent combination biases also shown lead factorial representations environment matched generative assumptions. supervisory bias used shape representations match prior knowledge generative environment. diﬀerent types supervised constraints associated diﬀerent types prior knowledge vary strongly constrain representation. biases described combined together various ways. example sparse distribution bias combined invariance bias model multilinear bias sparse bilinear model hierarchical layers r-ica. likewise supervised constraints combined encoder distributional invariance combination biases. example dc-ign combines bias gaussian-distributed independent factors supervised equality constraints. disbm combines multilinear factor combination bias equality direct constraints. general models supervised bias make unsupervised bias enough supervised constraints super-strong unsupervised bias longer necessary. since numerous examples diﬀerent combinations biases natural question compare other? bias leads representations factors distilled disentangled another? space diﬀerent levels supervised constraints correspond degrees factorization? however indication stronger constraints lead cross-over eﬀects indicates better factorization. model demonstrates evidence factorization representation models show evidence distillation disentangling. evidence cross-over beneﬁt found ssvae disbm lesser extent siamese models. positions model chart figure indicate models fairly strong supervised constraints disbm strong combination bias. possible papers could shown cross-over eﬀects report them seems plausible higher levels bias cause stronger factorization representation. another hint stronger constraints lead better factorization datasets used training papers. papers used partially labeled datasets ssvae disbm also used strongest biases. model least evidence factorization t-sne despite fairly strong distance constraints. attributed dataset high number factors factor-agnostic constraints large space inputs none papers considered eﬀect data sampling bias non-factorial environments. supervisory signal always necessary learn factorial representations unsupervised inductive biases suﬃcient? interesting future project could investigate eﬀect level factorization dataset model’s ability discover factorial representation. example suppose could vary amount correlation identity presence glasses face dataset variable number supervised constraint examples learn factorial representation. many supervised examples required learn case correlation case correlation case identity glasses completely correlated? relationship change constraint type? relationship change choice unsupervised inductive bias? several unsupervised biases could combined autoencoder reduce factor combination bias help invariance. example could combine bias multilinear combination autoencoder architecture. multilinear models pretty wasteful terms model size need learn component vector every combination latent factors. autoencoder could eﬃcient representing combinations lower bias terms combinations combined. sparse autoencoders combine sparsity bias autoencoder. could similarly extend autoencoder pooled/block sparsity multi-dimensional sparse factors. could even experiment merging diﬀerent combination biases e.g. non-negativity bias multilinear model create functional parts model parts interact multiplicatively. could similarly create combinations supervised biases. example could apply learning transform model direct constraints learn model represents presence glasses transforming image person glasses image person without. relatively papers factor-agnostic constraints despite potential usefulness. diﬃcult identify dimensions humans might compare examples. none papers factor-agnostic constraints learn representation functional encoder. would therefore interesting extend model like karaletsos handle factor-agnostic triplets.", "year": 2016}