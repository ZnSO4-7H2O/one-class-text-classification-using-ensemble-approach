{"title": "Cognitive Science in the era of Artificial Intelligence: A roadmap for  reverse-engineering the infant language-learner", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "During their first years of life, infants learn the language(s) of their environment at an amazing speed despite large cross cultural variations in amount and complexity of the available language input. Understanding this simple fact still escapes current cognitive and linguistic theories. Recently, spectacular progress in the engineering science, notably, machine learning and wearable technology, offer the promise of revolutionizing the study of cognitive development. Machine learning offers powerful learning algorithms that can achieve human-like performance on many linguistic tasks. Wearable sensors can capture vast amounts of data, which enable the reconstruction of the sensory experience of infants in their natural environment. The project of 'reverse engineering' language development, i.e., of building an effective system that mimics infant's achievements appears therefore to be within reach. Here, we analyze the conditions under which such a project can contribute to our scientific understanding of early language development. We argue that instead of defining a sub-problem or simplifying the data, computational models should address the full complexity of the learning situation, and take as input the raw sensory signals available to infants. This implies that (1) accessible but privacy-preserving repositories of home data be setup and widely shared, and (2) models be evaluated at different linguistic levels through a benchmark of psycholinguist tests that can be passed by machines and humans alike, (3) linguistically and psychologically plausible learning architectures be scaled up to real data using probabilistic/optimization principles from machine learning. We discuss the feasibility of this approach and present preliminary results.", "text": "spectacular progress information processing sciences promises revolutionize study cognitive development. here analyse conditions ’reverse engineering’ language development i.e. building eﬀective system mimics infant’s achievements contribute scientiﬁc understanding early language development. argue that computational side important move problems full complexity learning situation take input faithful reconstructions sensory signals available infants possible. data side accessible privacy-preserving repositories home data setup. psycholinguistic side speciﬁc tests constructed benchmark humans machines diﬀerent linguistic levels. discuss feasibility approach present overview current results. artiﬁcial intelligence speech psycholinguistics computational modeling corpus analysis early language acquisition infant development language bootstrapping machine learning. recent years artiﬁcial intelligence hitting headlines impressive achievements matching even beating humans complex cognitive tasks promising revolution manufacturing processes human society large. successes show statistical learning techniques powerful computers large amounts data possible mimic important components human cognition. shockingly achievements reached throwing classical theories linguistics psychology training relatively unstructured neural network systems large amounts data. tell underlying psychological and/or neural processes used humans solve tasks? provide scientiﬁc insights human learning processing? here argue developmental psychology particular study language acquisition area where indeed machine learning advances transformational provided involved ﬁelds make signiﬁcant adjustments practices order adopt call reverse engineering approach. speciﬁcally reverse engineering approach study infant language acquisition consists constructing scalable computational systems realistic input data mimic language acquisition observed infants. three italicised terms discussed length subsequent sections paper. intuitive understanding terms suﬃce. idea using machine learning techniques means study child’s language learning actually although relatively studies concentrated early phases language learning however whereas previous approaches limited proofs principle miniature languages modern techniques scaled much end-to-end language processing systems working real inputs deployed commercially. paper examines whether unprecedented change scale could address lingering scientiﬁc questions ﬁeld language development. structure paper follows section present deep scientiﬁc puzzles large scale modeling approaches could principle address solving bootstrapping problem accounting developmental trajectories. section review past theoretical modeling work showing puzzles received adequate answer. section argue answer reverse engineering three requirements addressed modeling computationally scalable done realistic data model performance compared humans. section recent progress reviewed light three requirements. section assess feasibility reverse engineering approach road followed reach objectives first linguistic system uniquely complex mastering language implies mastering combinatorial sound system open ended morphologically structured lexicon compositional syntax semantics animal communication system uses complex multilayered organization basis claimed humans evolved innately speciﬁed computational architecture process language second overt manifestations system extremely variable across languages cultures. language expressed oral manual modality. oral modality languages vowels consonants inventories vary words mostly composed single syllable long strings stems aﬃxes semantic roles identiﬁed ﬁxed positions within constituents identiﬁed functional morphemes etc. evidently infants acquire relevant variant learning genetic transmission. third human language capacity viewed ﬁnite computational system ability generate inﬁnity utterances. turns learnability problem infants basis ﬁnite evidence induce inﬁnity corresponding language. discussed since aristotle induction problems generally valid solution. therefore language simultaneously human-speciﬁc biological trait highly variable cultural production apparently intractable learning problem. despite complexities infants spontaneously learn native language matter years immersion linguistic environment. know simple fact puzzling appears. speciﬁcally outline deep scientiﬁc puzzles reverse engineering approach could principle help solve solving bootstrapping problem accounting developmental trajectories. ﬁrst puzzle relates ultimate outcome language learning so-called stable state deﬁned stabilized language competence adult. second puzzle relates know intermediate steps acquisition process variations function language input. stable state operational knowledge enables adults process virtual inﬁnity utterances native language. articulated description stable state oﬀered theoretical linguistics; viewed grammar comprising several components phonetics phonology morphology syntax semantics pragmatics. bootstrapping problem arises fact different components appear interdependent learning point view. instance phoneme inventory language deﬁned pairs words diﬀer minimally sounds would suggest learn phonemes infants need ﬁrst learn words. however processing viewpoint words recognized phonological constituents suggesting infants learn phonemes words. similar paradoxical co-dependency issues noted between linguistic levels words order learn component language competence many others need learned ﬁrst creating apparent circularities. puzzles independent facets phenomenon. practice proposals solving bootstrapping problem oﬀer insights observed trajectories. vice-versa data developmental trajectories provide manageable subgoals diﬃcult task solving bootstrapping problem. bootstrapping problem compounded fact infants taught formal linguistics language courses learn native language. cases animal communication infants spontaneously acquire language community merely being immersed community experimental observational studies revealed infants start acquiring elements language even talk therefore parents give much feedback progress language learning. suggests language learning occurs largely without supervisory feedback. reverse engineering approach potential solving puzzle providing computational system demonstrably bootstrap language similar supervisory poor inputs. first given multi-layered structure language could expect stage-like developmental tableau acquisition would proceed discrete succession learning phases organized logically hierarchically observed instance infants start diﬀerentiating native foreign consonants vowels months continue tune phonetic categories well ﬁrst year life however start learning sequential structure phonemes done acquiring phoneme inventory even that start acquiring meaning small common words words instead stage-like developmental tableau evidence shows acquisition takes places levels less simultaneously gradual largely overlapping fashion. second observational studies revealed considerable variations amount language input infants across cultures across socio-economic strata exceed order magnitude variations impact language achievement measured vocabulary size syntactic complexity least markers language achievement diﬀerences outcome much less extreme variations input. canonical babbling instance order magnitude would mean children start babble months others years observed range months less ratio. similarly reduced range variations found onset word production onset word combinations. suggests surprising level resilience language learning i.e. minimal amount input suﬃcient trigger certain landmarks. reverse engineering approach potential accounting otherwise perplexing developmental tableau provide quantitative predictions across linguistic levels cultural individual variations input impossible limited space justice rich diverse sets viewpoints proposed account language development. instead next sections present exhaustive selection four research strands draw source inspiration mixture psycholinguistics formal linguistics computer science share explanatory goals reverse engineering approach. argument even though strands provide important insights acquisition process still fall short accounting puzzles presented section speciﬁcally adressing bootstrapping problem frameworks build systematic correlations linguistic levels e.g. syntactic semantic categories prosodic boundaries syntactic ones bergelson swingley mandel jusczyk aslin mehler jusczyk hirsh-pasek jusczyk kuhl eilers jusczyk werker tees mazuka stark hypothesis infants equipped innate language acquisition device constrains hypothesis space learner enabling acquisition presence scarse ambiguous input conceptual frameworks focus aspects developmental trajectories oﬀering overarching architectures scenarios integrate many empirical results. among others competition model bates macwhinney macwhinney wrapsa jusczyk emergentist coalition model hollich primir werker curtin usage-based theory tomasello frameworks propose collection mechanisms linked linguistic input and/or social environment infant account developmental trajectories. conceptual framework useful summarizing organizing vast amount empirical results oﬀer penetrating insights speciﬁc enough address scientiﬁc puzzles. tend refer mechanisms using verbal descriptions boxes arrows diagrams. type presentation intuitive also vague. description correspond many diﬀerent computational mechanisms would yield diﬀerent predictions. frameworks therefore diﬃcult distinguish another empirically descriptive ones impossible disprove. addition formal cannot demonstrate models eﬀectively solve language bootstrapping problem. provide quantitative predictions observed resilience developmental trajectories variations function language input individual linguistic cultural level. psycholinguists sometimes supplement conceptual frameworks propositions speciﬁc learning mechanisms tested using artiﬁcial language paradigm. example mechanism based tracking statistical modes phonetic space proposed underpin phonetic category learning infancy. tested infants presentation simpliﬁed language statistical distribution acoustic tokens controlled also modeled computationally using unsupervised clustering algorithms tested using simpliﬁed corpora synthetic data similar double-pronged approach conducted mechanisms word segmentation based transition probability word meaning learning based cross sitpuzzle addressed syntax verb semantics word segmentation part speech syntax syntax phonetic categories word segmentation phonetic speaker phonological semantic categories syntax semantics syntax proposed mechanism inductive biases based syntax/semantic correlations inductive biases syntax/semantic correlations inductive prosodic/lexicon/syntax correlations perceptual intake; universal grammar; inference engine auditory processing; syllable segmentation; attentional weighting; pattern extraction; exemplar theory examplar theory statistical clustering associative learning attentional dynamic ﬁlters competitive learning construction grammar; intention reading; analogy; competitive learning; distributional analysis although studies artiﬁcial languages useful discover candidate learning algorithms could incorporated global architecture algorithms proposed tested artiﬁcial languages; therefore guarantee would actually work faced realistic corpora large noisy even though much current theoretical linguistics devoted study language competence stable state interesting work also conducted area formal models grammar induction. models propose algorithms provably powerful enough learn fragment grammar given certain assumptions input. instance tesar smolensky proposed algorithm provided pairs surface underlying word forms learn phonological grammar similar learnability assumptions results obtained stress systems learnability results syntax clark lappin models establish important learnability results particular demonstrate certain hypotheses particular class grammar learnable. demonstrate however hypotheses infants. particular grammar induction studies assume infants error-free adult-like symbolic representation linguistic entities perception certainly error-free clear infants adult-like symbols acquired them. words even though models advanced psycholinguistic models formally addressing eﬀectiveness proposed learning algorithms clear solving bootstrapping problem faced infants. addition typically lack connection empirical data developmental trajectories. idea using computational models shed light language acquisition ﬁeld cognitive science itself complete review would beyond scope paper. mention landmarks ﬁeld refer developmental separating three learning subproblems syntax lexicon speech. strings words alone additionally uses conceptual representation utterance meaning. ﬁrst strand illustrated kelley views grammar induction problem representing input corpus grammar compact fashion using priori constraints shape complexity grammars measure ﬁtness grammar data ﬁrst systems used artiﬁcial input part-of-speech tags provided side-information. since then manual tagging replaced automatic tagging using variety approaches artiﬁcial datasets replaced naturalistic ones second strand traced back siklossy makes radically diﬀerent hypothesis language learning essentially translation problem children provided parallel corpus speech unknown language conceptual representation corresponding meaning. language acquisition system anderson good illustration approach. learns context-free parsers provided pairs representations meaning sentences since then algorithms proposed learn directly meaning words context-free grammars replaced powerful ones sentence meaning replaced sets candidate meanings noise note types models take textual input therefore make assumption infants able represent input terms error-free segmented string words. computational models word discovery tackle problem segmenting continuous stream phonemes word-like units. idea distributional properties distinguish within word word phoneme sequences second idea simultaneously build lexicon segment sentences words ideas frequently combined addition segmentation models augmented jointly learning lexicon morphological decomposition tackling phonological variation noisy channel model note studies assume speech represented error-free string adult-like phonemes assumption cannot apply early language learners. finally computational model started address language learning speech. either concerned discovery phoneme-sized units discovery words both. several ideas proposed discover phonemes speech signal regarding words pentland proposed model learn segment continuous speech words visual categories ﬁrst models work real speech corpus although model used output supervised phoneme recognizer. acorns project used speech input discover candidate words learn word-meaning associations although speech collected laboratory real life situations. developmental represents clearest attempt addressing full bootstrapping problem. although clear progression simple models examples towards integrative algorithms realistic datasets still large models learn speech limited discovery phonemes word forms models learn syntax semantics work textual input. closed clear bootstrapping problem faced infants solved. research unfortunately scattered disjoint segments literature little sharing algorithms evaluation methods corpora making diﬃcult compare merits diﬀerent ideas register progress. finally even though studies mention infants source inspiration models seldom attempt account developmental trajectories. psycholinguistic conceptual frameworks capture important insights language development speciﬁed enough demonstrably solve bootstrapping problem make quantitative predictions. artiﬁcial language experiments yield interesting learning mechanisms aimed explaining experimental data necessarily scale larger noisy data. limitations call need develop eﬀective computational models work scale. linguistic models developmental attempt eﬀectively address bootstrapping problem make unrealistic assumptions respect input data result models address different bootstrapping problem faced infants. would call need realistic data input models. linguistic models developmental models take gold standard description stable state adults. objective explain ultimate attainment enable connect learning trajectory data. would call direct human-machine comparison ages. obiously four reviewed research traditions limalso address part language development puzzles examining reverse engineering approach could combine best traditions examine next scrutiny requirements meet order fully address puzzles. here argue scientiﬁc import models development beyond conceptual box-andarrow frameworks turned eﬀective scalable computational systems beyond data realistic input evaluated human/machine comparisons. scalable computational systems provide proof principle bootstrappping problem solved generate quantitative predictions. even compelling reason strive them verbal resoning models tend badly misjudge combination contradictory tendencies play practice resulting sometimes spectacularly incorrect predictions. illustrate three examples. ’easy’ problems proving diﬃcult. infant learn phonemes? popular hypothesis states track statistical modes speech sounds construct phonetic categories turn verbal description scalable algorithm? vallabha mcmurray among others proposed done unsupervised clustering algorithms. turns algorithms validated data manually obtained measurments. problem many clustering algorithms sensitive data size variability dimensionality tested continuous audio representations large variable high dimension diﬀerent result ensue. instance varadarajan khudanpur dupoux shown clustering algorithm based hidden markov models gaussian mixtures converge phonetic segments rather much shorter highly context-sensitive acoustic clusters surprising given phonemes realized discrete acoustic events complicated overlapping gestures. instance stop consonant surfaces burst closure formant transitions next segment. shows contrary distributional learning hypothesis ﬁnding phonetic units problem clustering also includes continuous speech segmentation contextual modeling. problems independent therefore addressed jointly learning algorithms. despite optimistic conclusions vallabha mcmurray unsupervised discovery phonetic categories still unsolved problem speech technology ’impossible’ approaches turning feasible. second example relates popular hypothesis acquiring meaning words essentially problem associating word form referents outside world view would seem impossible learn word meaning language input only. however research natural language processing shown fact possible derive approximate representation word meanings using coocurrence patterns within verbal material itself. distributional techniques construct vector representation word meanings correlate surprisingly well human semantic similarity judgments fourtassi dupoux found possible derive vectors even without properly segmented lexicon even without adult-like phonetic categories. turns approximate meaning representation derived provide top-down feedback helping clustering phonetic information phonemes. thus computational systems suggest priori implausible potentially eﬀective mechanisms. empirical validity mechanisms infants remains tested. statistically signiﬁcant eﬀects ending unimportant. third example relates so-called ’hyperspeech hypothesis’. proposed parents adapt pattern speech infants order facilitate perception kuhl observed parents tend increase separation point vowels child directed speech possibly making easier learn. ludusan seidl dupoux cristia word discovery algorithm borrowed developmental speech failed diﬀerence word learning between child adult directed speech; anything former slightly diﬃcult. paradoxical result explained fact child directed speech parents increase phonetic variability even increase separation point vowels eﬀects cancel even result small degradation category discriminability lesson completely explicit model quantitative eﬀect linguistic phonetic variables learning assessed. turn controversial three requirements idea address language learning full complexity running computational models inputs close infants’ sensory signals possible. seem exageration. simpliﬁcation hallmark scientiﬁc method usually proceeds breaking complicated problems smaller manageable ones. here claim exception made language learnability. why? nutshell learning process whose outcome exquisitely sensitive details input signal. makes even slightly incorrect assumptions input learning process ends studying diﬀerent learning problem altogether. illustrate three cases simpliﬁcations learnability game changer. conclude since learnabilityrelevant properties infant’s input currently unknown data selection matters. entire sensory stimulations available child called input. subset input used learn target language called intake. diﬀerence input intake deﬁnes data selection problem which claim important part learning problem itself. unfortunately many computational models language acquisition short-circuit selection problem human experts prepare pre-selected pre-cleaned data. illustrate three data selection problems. ﬁrst problem relates deﬁning counts linguistic versus non-linguistic information. language-universal answer question. instance gestures typically paraextra-linguistic communities using oral communication main vehicle language sign language learned children deaf mixed hearing/deaf communities within auditory modality vocal sounds like clicks considered nonlinguistic many languages others used phonologically similarly phonatory characteristics vowels like breathiness creakiness second problem even linguistic nonlinguistic signals deﬁned language actual unmixing signals diﬃcult. instance infants hear superposition many audio sources contain linguistic signals. auditory source separation computationally diﬃcult problem human adults inﬂuenced top-down word recognition pre-verbal infants sources top-down information learned. third problem even non-linguistic signals separated linguistic ones non-linguistic signals? instances considered noise discarded. cases however useful language learning. instance non-linguistic contextually relevant information form visually perceived objects scenes help lexical learning bootstrap syntactic learning social signals also taken crucial language learning again proper channeling non-linguistic cues part learning problem. brief data selection critical component learning problem. performed modeler inside information target language culture model whose task precisely discover assuming data selection solved ambiguity variability prevalent properties language level structure phonetics semantics pragmatics. many modeling approaches simplify complexity replacing real input synthetic idealized data. although useful practice debug algorithms prove mathematical results generalizing simpliﬁed real input risky business. already discussed clustering algorithms discover phonetic categories synthetic simpliﬁed phonetic data yield much totally diﬀerent results speech signals. level word segmentation algorithms recover word boundaries phoneme transcriptions utterly fail speech signals problem pervasive. learning algorithms work incorporate models shape data learned. mismatches models data likely result learning failure. vice versa however oversimplifying input make learning problem harder reality. example syntax learning models often operate abstract transcriptions result ignore prosodic information could prove useful purpose syntactic analysis lexical acquisition notion ’presentation’ comes formal learning theory corresponds particular order parent selects language inputs child. well known examples presentation extreme consequences learned not. instance constraints order environment presents grammatical sentences even simple classes grammars unlearnable. contrast environment presents sentences according computable process even complex classes grammars become learnable. result extends probabilistic scenario input sentences sampled according statistical distribution importance presentation boils question whether parents ’pedagogical’ i.e. whether present language according curriculum facilitates learning. importantly curriculum also include phonetic aspects para-linguistic aspects constitutes noise versus useful information. particular presentation target language associated contextual information result caretaker’s communicative pedagogic intentions formally characterized. even level syntax range possible languages completely known although perhaps area current propositions approach therefore runs risk locking researchers bubble universe problems mathematically tractable unrelated faced infants real world. second strategy radical actual data reconstruct infant’s sensory experience. data-driven solution advocate reverse engineering approach forces confront squarely problem data selection removes problems associated idealization variability ambiguity mode presentation. importantly input data limited single dataset want reverse engineer infant’s ability learn mode presentation possible human problem unrestricted presentations that learner always exists adversarial environment trick learner converging wrong grammar. vice versa computable processes enumerated hence stupid learner test increasingly many grammars presentations converge. language modality. practical address would sample ﬁnite although ever evolving attested languages split development test interesting sample typologies sociolinguistic groups stratiﬁed fashion avoid overﬁtting learning model prevalent types. sensory reconstruction obviously would make sense reconstruct stimuli outside sensory range infants precision superior discimination abilities. hence input simpliﬁcations done according known properties sensory attentional capacities infants. idealizing assumptions made least explicit impact potential oversimpliﬁcation overcomplexiﬁcation learning problem discussed turn apparently least controversial requirement everybody agrees modeling enterprise sort success criterion speciﬁed. however little agreement criterion use. quote proposals within cognitive psychology macwhinney proposed nine criteria berwick nine criteria pinker criteria yang three criteria frank goldwater griﬃths tenenbaum criteria. sorted conditions eﬀective modeling input product learning learning trajectories plausibility computational mechanisms proposed. formal learning theorists success usually deﬁned terms learnability limit learner said learn target grammar limit ﬁnite amount time grammar becomes equivalent target grammar. deﬁnition diﬃcult apply specify upper bound amount time quantity input required learning specify operational procedure deciding grammars equivalent pragmatically researchers ai/machine learning area deﬁne success terms performance system measured gold standard obtained human adults. interesting procedure testing end-state learning little measuring learning trajectories. deﬁnition suﬃcient itself shifts problem selecting good success criterion problem selecting tests included cognitive benchmark. least enables arbitrary aesthetic criteria forces deﬁne operational tests compare models. leaves open number questions tests measure behavioral choices reaction times physiological responses brain responses? include metaparalinguistic tests addition given range theoretical options formulated language development disagreements essential properties language would think proposed cognitive benchmark diﬃcult come about. benchmark propose construct within reverse engineering approach speciﬁc purpose. tease apart competing views language acquisition target developmental puzzles presented section infant bootstrap onto adult language system? gradual overlapping resilient patterns development possible? objective expressed terms level marr’s hierarchy computational/informational level. abstracts away considerations processing neural implementation. means benchmark considered ’cognitively indistinguishable’ models child little similarity infants psychological brain processes long acquired languagespeciﬁc information. course could enrich benchmark adding tests address lower levels marr’s hierarchy grammars said equivalent generate utterances. case context free grammars undecidable problem. generally many learning algorithms clear grammar learned therefore success criterion cannot applied. ﬁrst conditions standard best practices psychometrics psychophysics test validity refers whether test theoretically empirically sensitive psychological construct supposed measure. counterexample famous imitation game turing tests whether machines ’think’ measuring well appear humans on-line text-based interaction. test dubious theoritical validity ’thinking’ well deﬁned cognitive construct rather underspeciﬁed folk psychology concept dubious empirical validity easy fool human observers using simplistic text manipulation rules section presents turing test replacements. test reliability refers signal noise ratio measure. estimated computing betwenhuman test-retest agreement sampling initial parameters machines. directly human infants machines. infants testing apparatus constructed i.e. controlled artiﬁcial environment whereby responses test stimuli measured using spontaneous tendencies participants machines learning algorithms constructed linguistic tests optimize particular function nothing test. therefore need supplemented particular task interfaces proposed tests order extract response would equivalent response generated humans. cases administering task compromise test’s validity. biases knowledge desired response removed instructions testing apparatus interface brief motivated importance humanmachine benchmark presented principles construct construction benchmark viewed part research program itself. seek common ground competing views language acquisition periodically revised understanding language competence progresses experimental protocols language competence established. would been recent period major stumbling block acheiving reverse engineering approach. indeed many years computers struggling language processing. customary psycholinguistic courses mock dismal performance automatic dictation translation systems. started change paper hinton colleagues speech recognition years making neural networks starting perform better dominating technology based probabilistic models years later entire speech processing pipeline replaced neural networks trained endto-end performance claimed achieve human parity dictation task following brieﬂy review systems constructed turning whether could used inform infants language acquisition studies. important characteristics systems specialized design features predecessors replace generic neural network architectures trained large annotated corpora. continuing example speech specialized audio features replaced spectrograms phonetic transcriptions prononciation lexicons eliminated systems trained directly speech orthographic transcriptions end-to-end fashion. turn basic architectures many core ideas diﬀerent proposed early days connectionism. instance figure shows architecture deep speech state-of-the-art speech recognition system composed rather classical elements popularized late changed though scale networks volume data trained enabled tremendous progress computer hardware mathematical optimization techniques limits inability perform causal reasoning display systematic behavior gives rise exciting area research applying cognitive psychology cognitive neuroscience methods machine learning systems statistical learning mechanisms claimed core language acquisition a-priori reasons optimistic however fundamental term means cognitive studies used machine learning. diﬀerence machine learning statistical techniques used convenient construct systems models human acquisition processes. interpreted cognitively machine learning procedures would correspond caricature century schooling learner initially kind tabula rasa relentlessly inputs paired desired responses annotations input provided human supervisor. drill repeated learner gets right. setup called supervised learning given input correct answer. example speech recognition system trained associate speech utterance it’s written transcription. natural language processing tasks system presented sequences words input trained associate word part-of-speech semantic role co-reference text diﬀers infants learn language important ways. first children learn ﬁrst language asked associate sensory inputs linguistic tags. long even exposed linguistic tags going school learn read write acquired amounts fully functional speech recognition language processing system. done basis sensory input alone supervisatory signals adults neither unambiguous systematic. moves problem language learning area figure network architecture deep speech input spectrogram output sequence characters. layer incorporate particular patterns connectivity. convolutional layers organized terms local patches sharing connections along time and/or frequency dimensions. recurrent layers accumulate activations time. fully connected layers particular topology. batch normalization process activations layers rescaled means variance small examples training. courville advanced introduction). result neural networks grown pace slightly faster moore’s speech processing network elman zipser parameters; years later deep speech twelve thousand times larger. speech area deep learning shaken landscape object recognition language translation speech synthesis areas neural networks displaced large margin previous state-ofthe-art approaching human performance. explosion research faciliated large distribution programming frameworks open sourcing datasets state-of-the-art systems downloaded pre-trained tested inputs. successes generating interest taking machine learning systems trained large corpora quantitative models cognitive functions. indeed despite a-priori lack neural biological plausibility performance systems show surprising convergences biological organisms. instance deep neural network trained recognize artefacts natural kind categories images turn good predictors multi-unit responses neurons inferior temporal cortex primates also surprising divergences strange second diﬀerence sheer amount data required artiﬁcial systems compared infants. instance deep speech system described trained hours transcribed speech comparison four-year-old child admittedly functional speech recognition abilities spoken total amount varying depending language community means deep speech requires around times speech times words four-year mayan child get. recent time allocation study tsimane community shows amount child directed input even lower maya factor shows human infant equipped learning algorithm enables learn language scarse data. machine learning made progress point ’cognitive services’ incorporated everyday life applications. means major road block reverse engineering approach i.e. feasibility building language processing systems deal realistic input scale lifted. instead locked simpliﬁed data problems time becomes possible address bootstrapping problem full complexity derive quantitative developmental predictions along way. still challenges ahead; current machine learning systems fail provide models infant acquisition discard simplify input much sheer quantity adding extra inputs infant could possibly needs done therefore adapt existing algorithms construct ones learn data infants figure learning scenario child’s internal state grammar updated learning function based input environment’s internal state constant adult grammar variable context produces input child. method test empirical adequacy model comparing outcome psycholinguistic experiments children adults. seem reasonable essentially puts open loop situation described figure environment delivers ﬁxed curriculum inputs learner recovers grammar generated utterances. situation output child modeled environment modify inputs according behavior inferred internal states. input-driven idealization overestimate diﬃculty task compared realistic closeloop scenario. think however useful study input-driven scenario sake gives estimate learned worse case scenario parents priorities optimizing children’s language learning. within scenario claim recent advances data make reverse engineering roadmap actionnable. discuss current avenues research challenges need met. following three requirements review turn feasibility constructing systems learn without expert labels collection large realistic dataset establishment humanmachine benchmarks illustrate selection recent work. bringing machine learning bear language development requires construct systems discover linguistic structure little expert supervision. obviously diﬃcult learning associate inputs linguistic labels. here learner discover labels given input. class machine learning problems unfortunately less well studied understood supervised learning expanding ﬁeld research machine learning. main exclusive ideas explored address challenge. ﬁrst idea build learner prior knowledge underlying nature data generalization made noisy datapoints. strong prior knowledge logically impossible learning problems become easily solvable. models acquisition syntax mentioned section favor strong priors thing learn small number syntactic binary parameters. learning problem becomes constrained single sentence suﬃcient decide parameter’s value notion inductive biases formulated elegantly using bayesian graphical models models prior knowledge speciﬁed probability distributions model’s parameters updated input purpose illustration revisit discovery phonetic categories continuous speech. mentionned previously generic clustering algorithms fail learn phonemes mismatch clustering algorithms expect data consists glass proposed bayesian graphical model phonemes deﬁned sequences three acoustic states state modeled mixture gaussians space acoustic parameters phoneme durations also controlled binary boundary variable number phonemes speciﬁed dirichlet prior expects distribution phonemes follow power general purpose clustering algorithm algorihm glass uses languageuniversal information phonemes specify model inductively biased discover kind structure data. bayesian probabilistic models also used natural language processing infer syntactic structures challenge types models optimization parameters computationally intensive becomes prohibitive large models and/or large datasets. instance glass model applied relatively small corpus read speech abend model textual input. current research devoted develop eﬃcient approximations algorithms deploy naturalistic datasets here idea diﬀerent components language interdependant help jointly learn components rather learn separately. actually turning bootstrapping problem head instead liability codependancies linguistic components become asset. course empirical issue whether joint learning language components always successful separate learning. existence synergies documented using bayesian models phonemes words inventories syllables words segmentation referential intentions word meanings existence synergies leveraged models bayesian ones including deep learning algorithmic speech engineering systems. instance returning issue phonetic learning several lines research indicate words could help discovery subword units even imperfect automatically discovered proto-lexicon help model described figure implements idea. consists word discovery system extracts similar segments speech across large corpus. discovered segments constitute proto-lexicon acoustic word forms used train neural network discriminative fashion. resulting output network representation speech sound much invariant change talker original spectral representation system started similar spirit harwath torralba glass good illustration following tell colors balls selecting ball? task impossible without prior knowledge distribution colors easy know balls color. figure outline clustering algorithm hierarchical generative architecture learning phonemes speech model provided speech described speech features infers parameters data according hierarchical model sampling space possible values parameters. harwath glass showed training neural network associate image speech input corresponding short description image network develops phone-like word-like intermediate representations speech. brief even though unsupervised/weakly supervised learning diﬃcult growing interest within machine learning study algorithms shown special sessions topic machine learning conferences organization challenges involving laboratories cognitive science speech technology communities large number datasets across languages collected organized repositories proved immensely useful research community. prominent example childes repository enabled research papers datasets however contain relatively sparse datapoints perhaps ambitious large scale dense data collection eﬀort date speechome project video audio equipment installed room apartment recording years’ worth data around infant. pioneering work illustrates several technological analysis ethical issues arise ’ecological’ data collection. figure architecture illustrating top-down synergy between learning phonemes words. auditory spectrograms computed speech signal. then protowords extracted using spoken term discovery; words used learn invariant speech representation using discriminative learning siamese deep neural network architecture regarding technological issues falling costs digital sensors data storage make feasible duplicate speechome-like projects across many languages. challenging fact usable modeling captured enable reconstruction infant’s sensory experience ﬁrst person point view. already relatively inexpensive out-of-the wearable technology direction. miniaturized recorders enable recording infant’s sound environment full time even outside home become usable microphone array advanced signal processing enable source reconstruction even noisy environment. proximity accelerometor sensors used categorize activities ’life logging’ wearable devices capture images every seconds help reconstruct context speech interactions head-mounted cameras help reconstruct infant’s ﬁeld view upcoming progress miniaturization sensors would enable reconstruction infant’s visual experience. manual annotations costly scale large dense datasets. speechome corpus hours speech transcribed wich represents fraction total hours audio recordings recent breakthroughs machine learning discussed section enable semi-automatic annotations large amounts data. ethical issues main challenge point equilibrium requirement sharability open scientiﬁc data need protecting privacy familie’s personal data. response scientiﬁc community dichotomous either make everything public completely close corpora anybody outside institution recorded data neither solutions acceptable. alternative strategies considered research community. homebank repository contains transcribed audio restricted case case access researchers databrary similarly organized system secure storage large sets video recordings developemental data progress cryptographic techniques would make possible envision preserving privacy enabling open exploitation data. instance data could locked secure servers thereby remaining accessible revokable infants’ families. researchers’ access would restricted anonymized meta-data aggregate results extracted automatic annotation algorithms. speciﬁcs type linguistic data repository would worked dense speech video home recordings become mainstream tool infant research. brief large scale data collection infant data within reach number research projects although it’s exploitation open source format requires speciﬁc developments privacypreserving storage computing infrastructures. easy administer conceptually simple administered naive participants; kinds goodness judgments matching judgments validity linguistic tests often minimal design linguistic construct manipulated every variable kept constant regarding test reliability turns many linguistic tests quite reliable results grammaticality judgment textbooks replicable using on-line experiments given simplicity tasks relatively straightforward apply machines. indeed matching judgments stimulus stimulus derived extracting machine representations triggered stimulus compute similarity score representations. goodness judgments perhaps tricky; easily done generative algorithms assign probability score reconstruction error prediction error individual stimuli. seen table tests already used quite standardly evaluation unsupervised learning systems particular evaluation phonetic semantic levels others less widespread. challenge comes applicability tests infants children. seen table considerable variations diﬀerent linguistic levels tested children generally younger child diﬃcult construct reliable tests. addressing challenge would require improving substantially signal-to-noise techniques. also possibility increase number participants communityaugmented meta-analyses collaborative testing remotely experiments course even simple psychophysical tasks humans aﬀected many factors like attention fatigue learning habituation stimuli regularities stimulus presentations etc. methods minimize never totally suceed neutralizing eﬀects. place already possible test speciﬁc predictions using existing techniques. patterns errors made computational models infant input data generate predictions. reasoning errors viewed ’bugs’ rather signatures intrinsic computational diﬃculties also faced infants. instance even good word discovery algorithms make systematic segmentation errors undersegmentations frequent pairs words over-segmentations ngon showed possible preferential listening paradigm eleven month infants probe signature mis-segmentations. deriving predictions simple model word discovery childes corpus constructed otherwise matched frequent versus unfrequent missegmentations. eleven month olds preferred listen frequent mis-segmentations distinguish real words frequency. larsen cristia dupoux found possible compare outcome diﬀerent segmentation algorithms measuring ability predict vocabulary acquisition measured brief cognitive benchmark established already possible test infants predictions computational models large scale model comparison require progress developmental experimental methods. ﬁrst years life infants learn vast array cognitive competences amazing speed; studying development major scientiﬁc challenge cognitive science requires cooperation wide variety approaches methods. here proposed existing arsenal experimental theoretical methods reverse engineering approach consists building eﬀective system mimics infant’s achievements. idea constructing eﬀective system mimics object order gain knowledge object course general applied beyond language even beyond development. tional system scale using realistic data input assessing success running tests derived linguistics humans machines we’ve showed even challenges approach help challenging verbal theories help characterize learning consequences diﬀerent kinds inputs available infant across cultures suggesting empirical tests. closing note reverse engineering approach propose endorse particular model theory view language acquisition. instance take position rationalist versus empiricist debate proposal methodological speciﬁes needs done machine learning tools used address scientiﬁc questions relevant debate. strives constructing least eﬀective model learn language. model initial architecture feed real data comparison several models possible assess minimal amount information initial architecture have order perform well. comparison would give quantitative estimate number bits required genome construct architecture therefore relative weight sources information. words roadmap start given position rationalist/empiricist debate rather position debate outcome enterprise. paper would come light without numerous inspiring discussions paul smolensky alex cristia. also beneﬁtted insightful comments paul bloom emmanuel chemla ewan dunbar michael frank giorgio magri steven pinker thomas schatz gabriel synnaeve members cognitive machine learning team laboratoire sciences cognitives psycholinguistique three anonymous cognition reviewers. work supported european research council agence nationale pour recherche ecole neurosciences paris region france section describe estimated amount variability speech input infants. mainly interested number hours number words since common metrics used automatical speech recognition natural language processing. therefore metrics available original data estimate otherwise. table lists sample four studies included survey incorporate large variations languages cultures. hart risley studied english speaking infants splitted three groups according socio-economic status familly. analysis include extreme groups shneidman goldin-meadow studied groups rural mayan speaking community english speaking urban community weisleder fernald studied group spanish speaking familly finally weijer extensively measured dutch speaking child netherlands. methodological problem four studies reported diﬀerent kinds metrics order compare them therefore estimate convert metric another requires possibly incorred assumptions conversion parameters. therefore taken large grain salt subject revision precise data comes along. table lists results indicate value conversion factor used. compute total number hours year used waking time estimate studies except directly estimated speaking time day. convert number words hours used estimate word duration compatible numbers reported vdw. convert number utterances number words used sesdependant estimate mean utterance length high finally estimate total amount speech heard infants used proportion child directed input high updated version analysis including population forager-farmers table estimates yearly input total restricted child directed speech number hours words year four studies function sociolinguistic group numbers brackets provide range numbers across families. uses wake time estimate hours day. uses word duration estimate uses s&g’s estimate %cds high ses. uses w&f’s estimate %cds ses. uses h&r’s mlu’s estimates constraint perfectly reasonable argue diﬃcult apply modeling early language acquisition following reasons first computational power human brain currently unknown. current supercomputers simulate synapse level fraction brain several orders magnitude slower real time computational models still massively underpowered compared child’s brain. second particular algorithm appear complex brain diﬀerent version performing function not. instance word segmentation algorithms require procedure called gibbs sampling which theory require inﬁnite number time steps converge. would seem discredit algorithm alltogether. turns truncated version algorithm running ﬁnite time works reasonably well. similarly algorithms require time steps rewritten algorthms require less steps memory. still biological plausibility place theoretical bounds system complexity initial state. indeed initial state constructed basis human genome plus prenatal interactions environment. allows rule instance nativist acquisition model would pre-compile state-of-the-art language understanding systems existing languages planet plus mechanism selecting probable given input. reason system would biologically realizable parameters state-of-the-art phoneme recognition system single languages already require times memory storage available fraction genome diﬀerentiate humans apes. dnn-based phone recognizer typically parameters barring ways compress information takes mbytes. human-speciﬁc genome .gbase boils mbytes. infant input. intrinsically harder collect input already done many corpora older children issue categorize linguistic linguistic output annotate completely trivial. regarding computational modeling instead focusing component agent full interactive framework model agents total four components furthermore internal states agent split linguistic states non-linguistic states represent communicative aspects interaction this turn causes split processing component linguistic cognitive subcomponents. although clearly diﬃcult endeavor many individual ingredients needed constructing system already available following research areas. first within speech technology available components build language generator well perception comprehension components adult caregiver. second within linguistics psycholinguistics neuroscience interesting theoretical models learning speech production articulation young children third within machine learning great progress made recently reinforcement learning powerful class learning algorithms assume besides sensory data environment provides sporadic positive negative feedback could adapted model eﬀect feedback loops learning components caregiver infant. fourth developmental robotics studies developed notion intrinsic motivation agent actively seek information reinforced learning rate notion could used model dynamics learning child adaptive eﬀects caregiver-child feedback loops. diﬃcult part enterprise would perhaps concern evaluation models. indeed components subcomponents would evaluated spirit before i.e. running scalable data testing using humanvalidated tasks. instance child language generator tested comparing output appropriate children’s outputs requires development appropriate metrics human known computational capacity brain. compatible approach since soon diagnostic tests language computation brain available could added cognitive benchmark deﬁned section section revisit simplifying assumptions input-driven scenario endorsed section displayed figure scenario take consideration child’s output possible feedback loops parents based output. many researchers would major fatal limitation approach. real learning situations infants also agents environment reacts outputs creating feedback loops general description learning situation therefore figure here child able generate observable actions modify internal state environment environment able generate input child function internal state. general form learning situation consists therefore coupled dynamic systems. could complex situation addressed within reverse engineering approach? would like answer cautious extent possible adhere three requirements i.e. realistic data explicit criteria success scalable modeling none requirements seem reach would like pinpoint diﬃculties source caution. regarding data interactive scenario would require accessing full output judgments. cognitive subcomponents would tested experiments studying children adults experimentally controlled interactive loops addition complex system parts individual component validation would suﬃcient entire system would evaluated. fully specifying methodological requirements reverse engineering interactive scenario would project own. clear present much complications introduced scenario necessary least understand ﬁrst steps language bootstrapping. extent cultures direct input child severely limited and/or interactive character input circumscribed would seem fair amount bootstrap take place outside interactive feedback loops. course entirely empirical issue reverse engineering approach help clarify.", "year": 2016}