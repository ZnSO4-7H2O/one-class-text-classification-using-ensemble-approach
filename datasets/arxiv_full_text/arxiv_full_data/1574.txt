{"title": "Multi-attention Recurrent Network for Human Communication Comprehension", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Human face-to-face communication is a complex multimodal signal. We use words (language modality), gestures (vision modality) and changes in tone (acoustic modality) to convey our intentions. Humans easily process and understand face-to-face communication, however, comprehending this form of communication remains a significant challenge for Artificial Intelligence (AI). AI must understand each modality and the interactions between them that shape human communication. In this paper, we present a novel neural architecture for understanding human communication called the Multi-attention Recurrent Network (MARN). The main strength of our model comes from discovering interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory of a recurrent component called the Long-short Term Hybrid Memory (LSTHM). We perform extensive comparisons on six publicly available datasets for multimodal sentiment analysis, speaker trait recognition and emotion recognition. MARN shows state-of-the-art performance on all the datasets.", "text": "modalities. example arrangement words sentence according generative grammar language activation facial muscles presentation smile cross-view dynamics refer dynamics modalities divided synchronous asynchronous categories. example synchronous cross-view dynamics simultaneous cooccurrence smile positive sentence example asynchronous cross-view dynamics delayed occurrence laughter sentence. machines understand human communication must able understand view-speciﬁc cross-view dynamics. model dual dynamics human communication propose novel deep recurrent neural model called multi-attention recurrent network marn distinguishable previous approaches explicitly accounts view-speciﬁc cross-view dynamics network architecture continuously models dynamics time. marn view-speciﬁc dynamics within modality modeled using long-short term hybrid memory assigned modality. hybrid memory allows modality’s lsthm store important cross-view dynamics related modality. cross-view dynamics discovered recurrence time-step using speciﬁc neural component called multi-attention block capable simultaneously ﬁnding multiple crossview dynamics recurrence timestep. marn resembles mechanism brains understanding communication different regions independently process understand different modalities lsthm connected together using neural links multimodal information integration mab. benchmark marn evaluating understanding different aspects human communication covering sentiment speech emotions conveyed speaker displayed speaker traits. perform extensive experiments different attributes related human communication public multimodal datasets. approach shows state-of-the-art performance modeling human communication datasets. human face-to-face communication complex multimodal signal. words gestures changes tone convey intentions. humans easily process understand faceto-face communication however comprehending form communication remains signiﬁcant challenge artiﬁcial intelligence must understand modality interactions shape human communication. paper present novel neural architecture understanding human communication called multiattention recurrent network main strength model comes discovering interactions modalities time using neural component called multi-attention block storing hybrid memory recurrent component called long-short term hybrid memory perform extensive comparisons publicly available datasets multimodal sentiment analysis speaker trait recognition emotion recognition. marn shows state-of-the-art performance datasets. humans communicate using highly complex structure multimodal signals. employ three modalities coordinated manner convey intentions language modality vision modality acoustic modality understanding multimodal communication natural humans; subconsciously cerebrum brains everyday. however giving artiﬁcial intelligence capability understand form communication humans incorporating involved modalities fundamental research challenge. giving capability understand human communication narrows computers’ understanding humans opens horizons creation many intelligent entities. coordination different modalities human communication introduces view-speciﬁc cross-view dynamics view-speciﬁc dynamics refer dynamics within modality independent non-temporal models studies focused simplifying temporal aspect cross-view dynamics order model co-occurrences information across modalities. models modality summarized representation collapsing time dimension averaging modality information time models successful understanding co-occurrences lack temporal modeling major models cannot deal multiple contradictory evidences smile frown happen together utterance. furthermore approaches cannot accurately model long sequences since representation long periods time become less informative. early fusion approaches used multimodal input feature concatenation instead modeling view-speciﬁc cross-view dynamics explicitly. words approaches rely generic models learn view-speciﬁc cross-view dynamics without speciﬁc model design. concatenation technique known early fusion often early fusion approaches remove time factor well additionally compare stronger recurrent baseline uses early fusion maintaining factor time. shortcoming models lack detailed modeling view-speciﬁc dynamics turn affects modeling cross-view dynamics well causing overﬁtting input data late fusion late fusion methods learn different models modality combine outputs using decision voting methods generally strong modeling view-speciﬁc dynamics shortcomings cross-view dynamics since inter-modality dynamics normally complex decision vote. example shortcoming model trained sentiment analysis using vision modality predicts negative sentiment late fusion models access whether negative sentiment frowning face disgusted face. multi-view learning extensions hidden markov models hidden conditional random fields proposed learning multiple different views extensions lstms also proposed multi-view setting marn different ﬁrst category since model view-speciﬁc cross-view dynamics. differs second third category since explicitly model view-speciﬁc dynamics using lsthm modality well cross-view dynamics using mab. finally marn different fourth category since explicitly models view-speciﬁc dynamics proposes advanced temporal modeling cross-view dynamics. section outline pipeline human communication comprehension multi-attention recurrent network marn components long-short term hybrid memory multi-attention block. long-short term hybrid memory extension long-short term memory reformulating memory component carry hybrid information. lsthm intrinsically designed multimodal setups modality assigned unique lsthm. lsthm hybrid memory stores view-speciﬁc dynamics assigned modality cross-view dynamics related assigned modality. component discovers cross-view dynamics across different modalities called multi-attention block ﬁrst uses information hidden states lsthms timestep regress coefﬁcients outline multiple existing cross-view dynamics among them. weights output dimensions based coefﬁcients learns neural cross-view dynamics code lsthms update hybrid memories. figure shows overview marn. marn differentiable end-to-end allows model learned efﬁciently using gradient decent approaches. next subsection ﬁrst outline longshort term hybrid memory. proceed outline multi-attention block describe components integrated marn. long-short term hybrid memory long-short term memory networks among successful models learning sequential data important component lstm memory stores representation input time. lsthm model seek build memory mechanism modality addition storing view-speciﬁc dynamics also able store cross-view dynamics important modality. allows memory function hybrid manner. pipeline. modality input lsthm form ={xm word vectors tanh tangent hyperbolic activation function.ࣷ denotes vector concatenation and⊙ denotes element-wise multiplication. similar lstm input gate forget gate output gate. proposed update hybrid memory time time distributed output modality. neural cross-view dynamics code output multi-attention block previous time-step discussed detail next subsection. neural cross-view dynamics code passed individual lsthms using audio lsthm memory. examples cross-view dynamics exist time therefore cross-view dynamics span across various modalities scattered across time forming asynchronous cross-view dynamics. multi-attention block network capture multiple different possibly asynchronous cross-view dynamics encode neural cross-view dynamics code important step multi-attention block different dimensions lsthm outputs assigned attention coefﬁcients according whether form cross-view dynamics. attention coefﬁcients high dimension contributes formation cross-view dynamics irrelevant. coefﬁcient assignment performed multiple times existence possibly multiple cross-view dynamics across outputs lsthm. multi-attention block formulated algorithm assume maximum cross-view dynamics present timestamp obtain attention coefﬁcients softmax distributions assigned concatenated lsthm memories using deep neural networka∶ rdmem\u0015 rk×dmem dmem= ∑m∈m set{hm mem}.a takes concatenation lsthm outputs =ࣷm∈m rdmem input outputs attentions{ak rdmem} rk×dmem.a softmax layer at=ࣷk ∑dmem forms probability distribu element-wise multiplied rdmem} rk×dmem.⇑k denotes broadcasting parameter ﬁrst dimension of̃ht contains information needed second dimension of̃ht contains information second k.̃ht high attentions. thereforẽht split different parts reduction usingcm rk×dm local networks{cm∶ maps attended outputs modalitỹhm attended modality outputs st=ࣷm∈m local\u0015 rdmem passed deep neural networkg∶ r∑m∈m paper benchmark marn’s understanding human communication three tasks multimodal sentiment analysis multimodal speaker traits recognition multimodal emotion recognition. perform experimentations publicly available datasets compare performance marn performance state-of-the-art approaches datasets. ensure generalization model datasets split train validation test sets include identical speakers sets i.e. speakers test different train validation sets. models re-trained train/validation/test splits. train marn different tasks ﬁnal outputs neural cross-view dynamics code inputs another deep neural network performs classiﬁcation regression code hyperparameters instruction data splits publicly available https//github.com/azadeh/marn. ict-mmmo ict-mmmo dataset consists online social review videos encompass strong diversity people express opinions annotated video level sentiment. dataset contains multimodal review videos used training validation testing. youtube youtube dataset contains videos social media site youtube span wide range product reviews opinion videos. videos used training validation testing. consists product review videos spanish. video consists multiple segments labeled display positive negative neutral sentiment. videos dataset used training validation testing. persuasion opinion multimodal dataset contains movie review videos annotated following speaker traits conﬁdence passion dominance credibility entertaining reserved trusting relaxed nervous humorous persuasive. videos split training validation testing. iemocap iemocap dataset consists videos recorded dialogues speakers session total videos across dataset. segment annotated presence emotions well valence arousal dominance. dataset recorded across sessions pairs speakers. ensure speaker independent learning dataset split level sessions training performed sessions validation testing performed session language datasets provide manual transcriptions. pre-trained word embeddings convert transcripts videos sequence word vectors. dimension word vectors vision facet used extract features including per-frame basic advanced emotions facial action units indicators facial muscle movement. acoustic covarep extract level acoustic features including melfrequency cepstral coefﬁcients pitch tracking voiced/unvoiced segmenting features glottal source parameters peak slope parameters maxima dispersion quotients. modality alignment reach time alignment different modalities choose granularity input level words. words aligned audio using exact utterance times. time step represents spoken word transcript. treat speech pause word vector values zero across dimensions. visual acoustic modalities follow granularity. expected feature values across entire word vision acoustic since extracted higher frequency port accuracy denotes number classes score. regression report mean absolute error pearson’s correlation metrics higher values denote better performance except lower values denote better performance. baseline models compare performance marn following state-of-the-art models multimodal sentiment analysis speaker trait recognition emotion recognition. baselines trained datasets complete comparison. explicitly models view-speciﬁc cross-view dynamics creating multi-dimensional tensor captures unimodal bimodal trimodal interactions across three modalities. current state cmu-mosi dataset. trained concatenated multimodal features classiﬁcation regression compare another strong non-neural baseline using similar multimodal inputs. ef-hcrf uses hcrf learn latent variables conditioned concatenated input time step. also implement following variations ef-ldhcrf class models learn hidden states using latent code observed concatenated input hidden output. mv-hcrf multi-view hcrf extension hcrf multi-view data explicitly capturing view-shared view speciﬁc sub-structures. mv-ldhcrf variation mv-hcrf model uses ldhcrf instead hcrf. ef-hsshcrf layered model uses hcrfs latent variables learn hidden spatiotemporal dynamics. mv-hsshcrf extends efhsshcrf performing multi-view hierarchical sequence summary representation. best performing early fusion table sentiment prediction results cmu-mosi test using multimodal methods. model outperforms previous baselines best scores highlighted bold. ef-lstm concatenates inputs different modalities time-step uses input single lstm. also implement stacked bidirectional stacked bidirectional lstms stronger baselines. best performing model reported efmajority performs majority voting classiﬁcation tasks predicts expected label regression tasks. baseline useful lower bound model performance. human performance calculated cmu-mosi dataset offers annotator results. accuracy human performance one-vs-rest classiﬁcation/regression. finally marn indicates proposed model. additionally modiﬁed baseline marn removes learns dense cross-view dynamics code model seen three disjoint lstms used investigate importance modeling temporal cross-view results cmu-mosi dataset summarize results cmu-mosi dataset table able achieve state-of-the-art results dataset metrics using marn. highlights model’s capability understanding sentiment aspect multimodal communication. results ict-mmmo youtube moud datasets achieve state-of-the-art performance signiﬁcant improvement comparison metrics english sentiment analysis datasets. table shows comparison marn state-of-the-art approaches ict-mmmo dataset well comparison youtube dataset. assess generalization marn speakers communicating different languages compare state-ofthe-art approaches sentiment analysis moud opinion utterance video clips spanish. ﬁnal third table shows results also achieve signiﬁcant improvement state-of-the-art approaches. results dataset experiment speaker traits recognition based observed multimodal communicative behaviors. table shows performance marn dataset achieves state-of-the-art accuracies speaker trait recognition tasks including persuasiveness credibility. results iemocap dataset results multimodal emotion recognition iemocap dataset reported table approach achieves state-of-the-art performance emotion recognition emotion classiﬁcation well continuous emotion regression except case correlation dominance results competitive state art. experiments indicate outstanding performance marn modeling various attributes related human communication. section better understand different characteristics model. cients needed. marn whether attention enough extract cross-view dynamics. whether different tasks datasets require different numbers attentions. marn model learn simple rules among modalities decision voting simple cooccurrence rules tensor fusion baseline. across datasets marn outperformed marn. indicates continuous modeling cross-view dynamics crucial understanding human communication. experiments marn attention under-performs compared models multiple attentions. could argue models attentions parameters result better performance better modeling cross-view dynamics rather parameters. however performed extensive grid search number parameters marn attention. increasing number parameters improve performance. indicates better performance marn multiple attentions higher number parameters rather better modeling cross-view dynamics. visualization attentions visually display attention sensitive different dimensions lsthm outputs figure column ﬁgure denoted shows behavior attention sample video cmu-mosi. left side attention coefﬁcients active throughout time. dimensions carry view-speciﬁc dynamics needed modality modalities. hence needed cross-view dynamics carry weight formation. attentions change behaviors across time. coefﬁcients changes drastic others. suspect less drastic change attention dimension time higher chances dimension part multiple cross-view dynamics. thus attentions activate important dimension. attentions focus cross-view dynamics involve modalities. example audio modality dark blue dimensions modalities dark blue dimensions. attentions seem residual effects. shows activations broad variables shows activation fewer sets indicating attentions could learn complementary way. figure visualization attention units throughout time. blue activated attentions non-activated attentions. learned attentions diverse evolve across time. paper modeled multimodal human communication using novel neural approach called multi-attention recurrent network approach designed model view-speciﬁc dynamics well cross-view dynamics continuously time. view-speciﬁc dynamics modeled using long-short term hybrid memory modality. various cross-view dynamics identiﬁed time-step using multi-attention block outputs multimodal neural code hybrid memory lsthm. marn achieves state-of-the-art results publicly available datasets across different attributes related understanding human communication.", "year": 2018}