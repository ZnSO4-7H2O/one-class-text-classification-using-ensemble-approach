{"title": "StarCraft II: A New Challenge for Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.", "text": "paper introduces scle reinforcement learning environment based game starcraft domain poses grand challenge reinforcement learning representing difﬁcult class problems considered prior work. multi-agent problem multiple players interacting; imperfect information partially observed map; large action space involving selection control hundreds units; large state space must observed solely input feature planes; delayed credit assignment requiring long-term strategies thousands steps. describe observation action reward speciﬁcation starcraft domain provide open source python-based interface communicating game engine. addition main game maps provide suite mini-games focusing different elements starcraft gameplay. main game maps also provide accompanying dataset game replay data human expert players. give initial baseline results neural networks trained data predict game outcomes player actions. finally present initial baseline results canonical deep reinforcement learning agents applied starcraft domain. mini-games agents learn achieve level play comparable novice player. however trained main game agents unable make signiﬁcant progress. thus scle offers challenging environment exploring deep reinforcement learning algorithms architectures. recent progress areas speech recognition computer vision natural language processing attributed resurgence deep learning provides powerful toolkit non-linear function approximation using neural networks. techniques also proven successful reinforcement learning problems yielding signiﬁcant successes atari game three-dimensional virtual environments simulated robotics domains many successes stimulated availability simulated domains appropriate level difﬁculty. benchmarks critical measuring therefore advancing deep learning reinforcement learning research therefore important ensure availability domains beyond capabilities current methods dimensions. paper introduce scle challenging domain reinforcement learning based starcraft video game. starcraft real-time strategy game combines fast paced micro-actions need high-level planning execution. previous decades starcraft pioneering enduring e-sports millions casual highly competitive professional players. defeating human players therefore becomes meaningful measurable long-term objective. reinforcement learning perspective starcraft also offers unparalleled opportunity explore many challenging frontiers. first multi-agent problem several players compete inﬂuence resources. also multi-agent lower-level player controls hundreds units need collaborate achieve common goal. second imperfect information game. partially observed local camera must actively moved order player integrate information. furthermore fog-of-war obscuring unvisited regions necessary actively explore order determine opponent’s state. third action space vast diverse. player selects actions among combinatorial space approximately possibilities using point-and-click interface. many different unit building types unique local actions. furthermore legal actions varies player progresses tree possible technologies. fourth games typically last many thousands frames actions player must make early decisions consequences seen much later game leading rich challenges temporal credit assignment exploration. paper introduces interface intended make starcraft straightforward observations actions deﬁned terms resolution grids features; rewards based score starcraft engine built-in computer opponent; several simpliﬁed minigames also provided addition full game maps. future releases extend interface full challenge starcraft observations actions expose pixels; agents ranked ﬁnal win/loss outcome multi-player games; evaluation restricted full game maps used competitive human play. addition provide large dataset based game replays recorded human players increase millions replays people play game. believe combination interface dataset provide useful benchmark test existing algorithms also interesting aspects perception memory attention sequence prediction modelling uncertainty active areas machine learning research. several environments already exist reinforcement learning original version starcraft. work differs previous environments several regards focuses newer version starcraft observations actions based human user interface rather programmatic; directly supported game developers blizzard entertainment windows linux. current best artiﬁcial starcraft bots based built-in research previous environments defeated even amateur players fact coupled starcraft’s interesting game-play properties large player base makes ideal research environment exploring deep reinforcement learning algorithms. computer games provide compelling solution issue evaluating comparing different learning planning approaches standardised tasks important source challenges research artiﬁcial intelligence games offer multiple advantages clear objective measures success; computer games typically output rich streams observational data ideal inputs deep networks; externally deﬁned difﬁcult interesting human play. ensures challenge tuned researcher make problem easier algorithms developed; games designed anywhere interface game dynamics making easy share challenge precisely researchers; cases pool avid human players exists making possible benchmark highly skilled individuals. since games simulations controlled precisely scale. well known example games driving reinforcement learning research arcade learning environment allows easy replicable experiments atari video games. standardised tasks incredible boon recent research scores games environment compared across publications algorithms allowing direct measurement comparison. prominent example rich tradition video game benchmarks including super mario pac-man doom unreal tournament well general video game-playing frameworks competitions genre games attracted large amount research including original starcraft recommend surveys ontanon robertson watson overview. many research directions focus speciﬁc aspects game speciﬁc techniques aware efforts solve full games end-to-end approach. tackling full versions games seemed daunting rich input output spaces well sparse reward structure standard starcraft thus bwapi related wrappers simpliﬁed versions games also developed research notably microrts recent previous work applied approaches wargus game reduced state action spaces learning based agents also explored micromanagement mini-games learning game outcome build orders replay data main contribution paper release scle exposes starcraft research environment. release consists three sub-components linux starcraft binary starcraft pysc starcraft allows programmatic control starcraft used start game observations take actions review replays. normal game available windows also provide limited headless build runs linux especially machine learning distributed cases. using built pysc open source environment optimised agents. pysc python environment wraps starcraft ease interaction python reinforcement learning agents starcraft pysc deﬁnes action observation speciﬁcation includes random agent handful rule-based agents examples. also includes mini-games challenges visualisation tools understand agent starcraft updates simulation times second. game mostly deterministic randomness mainly cosmetic reasons; main random elements weapon speed update order. sources randomness removed/mitigated setting random seed. describe environment used experiments paper. full game starcraft opponents spawn contains resources elements ramps bottlenecks islands. game player must accumulate resources construct production buildings amass army eliminate opponent’s buildings. game typically lasts minutes hour early actions taken game long term consequences. players imperfect information since typically portion units. want understand react opponent’s strategy must send units scout. describe later section action space also quite unique challenging. people play online human players. common games team games possible complicated games unbalanced teams teams. focus format popular form competitive starcraft consider complicated situations future. starcraft includes built-in based handcrafted rules comes levels difﬁculty unfortunately fact rule-based means strategies fairly narrow thus easily exploitable. nevertheless reasonable ﬁrst challenge purely learned approach like baselines investigate sections play better random play quickly little compute offer consistent baselines compare against. deﬁne different reward structures ternary received game blizzard score. ternary win/tie/loss score real reward care about. blizzard score score seen players victory screen game. players score game provide access running blizzard score every step game change score used reward reinforcement learning. computed current resources upgrades researched well units buildings currently alive built. means player’s cumulative reward increases mined resources decreases losing units/buildings actions affect blizzard score zero-sum since player-centric less sparse ternary reward signal correlates extent winning losing. starcraft uses game engine renders graphics whilst utilising underlying game engine simulates whole environment starcraft currently render pixels. rather generates feature layers abstract away images seen figure pysc viewer shows human interpretable view game left coloured versions feature layers right. example terrain height fog-of-war creep camera location player identity shown feature layers. video found https//youtu.be/-fkuytg-. human play maintaining core spatial graphical concepts starcraft thus main observations come sets feature layers rendered pixels layers represents something speciﬁc game example unit type points owner visibility. scalars others categorical. sets feature layers minimap coarse representation state entire world screen detailed view subsection world corresponding player’s on-screen view actions executed. features exist screen minimap others exist screen. environment documentation complete description observations provided. addition screen minimap human interface game provides various non-spatial observations. include amount minerals collected actions currently available detailed information selected units build queues units transport vehicle. observations also exposed pysc fully described environment documentation. audio channel exposed wave form important notiﬁcations exposed part observations. retail game engine screen rendered full perspective camera high resolution. leads complicated observations units getting smaller higher screen world real estate visible back front. simplify this feature layers rendered camera uses orthographic projection. means pixel feature layer corresponds precisely amount world real estate consequence units size regardless view. unfortunately also means feature layer rendering quite match human would see. agent sees little front little less back. mean actions humans make replays cannot fully represented. future releases expose rendered allowing agents play pixels. allow study effects learning pixels versus learning feature layers make closer comparisons human play. mean time played game feature layers verify agents severely handicapped. though game-play experience obviously altered found resolution sufﬁcient allow human player select individually control small units zerglings. reader encouraged using pysc play. also figure designed environment action space mimic human interface closely possible whilst maintaining conventions employed environments atari figure shows short sequence actions produced player agent. many basic manoeuvres game compound actions. example move selected unit across player must ﬁrst choose move pressing possibly choose queue action holding shift click point screen minimap execute action. instead asking agents produce key/mouse presses sequence three separate actions give atomic compound function action move screen. formally action represented composition function identiﬁer sequence arguments function identiﬁer requires instance consider selecting multiple units drawing rectangle. intended action select rect ﬁrst argument select binary. arguments integers deﬁne coordinates allowed range resolution observations. action environment form represent full action space deﬁne approximately action-function identiﬁers possible types arguments environment documentation detailed speciﬁcation description actions available pysc figure example sequence actions. starcraft actions available every game state. example move command available unit selected. human players actions available command card screen. similarly provide list available actions observations given agent step. taking action available considered error agents ﬁlter action choices legal actions taken. humans typically make actions minute roughly increasing player skill professional players often spiking apm. experiments every game frames equivalent reasonable choice intermediate players. believe early design choices make environment promising testbed developing complex agents. particular ﬁxed-size feature layer input space human-like action space natural neural network based agents. contrast recent work game accessed unit-per-unit basis actions individually speciﬁed unit. advantages interface styles pysc offers following investigate elements game isolation provide ﬁne-grained steps towards playing full game built several mini-games. focused scenarios small maps constructed purpose testing subset actions and/or game mechanics clear reward structure. unlike full game reward win/lose/tie reward figure comparison humans starcraft actions exposed pysc. designed action space close possible human actions. ﬁrst shows game screen second human actions third logical action taken pysc fourth actions exposed environment note ﬁrst columns feature ‘build supply’ action available agent situations worker selected ﬁrst. structure mini-games reward particular behaviours encourage community build modiﬁcations mini-games powerful starcraft editor. allows designing broad range smaller challenge domains. permits sharing identical setups evaluations researchers obtaining directly comparable evaluation scores. restricted action sets custom reward functions and/or time limits deﬁned directly resulting .scmap easy share. therefore encourage users method deﬁning tasks rather customising agent side. seven mini-games releasing follows findanddefeatzerglings agent starts marines must explore defeat individual zerglings. requires moving camera efﬁcient exploration. defeatroaches agent starts marines must defeat roaches. every time defeats roaches gets marines reinforcements roaches spawn. reward roach killed marine killed. marines keep alive roaches defeat. defeatzerglingsandbanelings defeatroaches except opponent zerglings banelings give reward killed. requires different strategy enemy units different abilities. collectmineralsandgas agent starts limited base rewarded total resources collected limited time. successful agent must build workers expand increase resource collection rate. buildmarines agent starts limited base rewarded building marines. must build workers collect resources build supply depots build barracks train marines. action space limited minimum action needed accomplish goal. starcraft also similar broodwar case observations list visible units along properties without visual component. fog-of-war still exists camera visible units simultaneously. simpler precise representation correspond humans perceive game. purposes comparing humans considered cheating since offers signiﬁcant additional information. using actions control units groups units individually unit identiﬁer. need select individuals groups units issuing actions. allows much precise actions human interface allows thus yields possibility super-human behaviour api. although used data train agents included release order support cases. pysc uses visualization blizzard’s examples commandcenter rule-based agents. often environment faster real time. observations rendered speed depends several factors complexity screen resolution number non-rendered frames action number threads. complex maps computation dominated simulation speed. taking actions less often allowing fewer rendered frames reduces compute diminishing returns kicks fairly quickly meaning little gain steps action. given little time spent rendering higher resolution hurt. running instances parallel threads scales quite well. simpler maps world simulation quick rendering observations dominates. case increasing frames action decreasing resolution large effect. bottleneck becomes python interpreter negating gains roughly threads single interpreter. resolution acting rate frames action single-threaded speed ladder varies game steps wall-clock second order magnitude faster real-time. exact speeds depends multiple factors including stage game number units play computer runs collectmineralshards settings permit game steps wall-clock second. section provides baseline results serve calibrate difﬁculty demonstrate established algorithms learn useful policies least mini-games also many challenges remain. mini-games additionally provide scores human players deepmind game tester starcraft grandmaster reinforcement learning agents built using deep neural network parameters deﬁnes policy time step agent receives observations selects action probability receives reward environment. goal agent γkrt+k+ discount factor. notational clarity assume policy conditioned observation without loss generality might conditioned previous states e.g. hidden memory state describe below. parameters policy learnt using asynchronous advantage actor critic described mnih shown produce state-of-the-art results diverse environments. policy gradient method performs approximate gradient ascent gradient deﬁned follows value function estimate expected return produced γkrt+k+ γnvθ gradient above hyper-parameter. last term regularises policy towards larger entropy promotes exploration hyper-parameters trade importance loss components. details refer reader original paper references therein. described section exposes actions nested list contains function identiﬁer arguments. since arguments including pixel coordinates screen minimap discrete naive parametrisation policy would require millions values specify joint distribution even spatial resolution. could instead represent policy auto-regressive manner utilising chain rule representation implemented efﬁciently arguably simpler transforms problem choosing full action sequence decisions argument straightforward baselines reported here make simpliﬁcation policies choose function identiﬁer arguments independently another note that depending function identiﬁer number required arguments different. actions require arguments whereas others figure example. line human ensure unavailable actions never chosen agents. mask function identiﬁer choice available subset sampled. implement masking actions renormalising probability distribution section presents several agent architectures purpose producing straightforward baselines. take established architectures literature adapt speciﬁcs environment particular action space. figure illustrates proposed architectures. note auto-regressive case could arbitrary permutation arguments deﬁne order chain rule applied. also ‘natural’ ordering arguments used since decisions click screen depend purpose click identiﬁer function called. input pre-processing baseline agents share pre-processing input feature layers. embed feature layers containing categorical values continuous space equivalent using one-hot encoding channel dimension followed convolution. also re-scale numerical features logarithmic transformation hit-points minerals might attain substantially high values. atari-net agent ﬁrst baseline simple adaptation architecture successfully used atari benchmark deepmind environments processes screen minimap feature layers convolutional network layers ﬁlters size stride respectively. non-spatial features vector processed linear layer tanh non-linearity. results concatenated sent linear layer relu activation. resulting vector used input linear layers output policies action function action-function argument {al}l independently. spatial actions independently model policies select coordinates. fullyconv agent convolutional networks reinforcement learning usually reduce spatial resolution input layer ultimately ﬁnish fully connected layer discards spatial structure completely. allows spatial information abstracted away actions inferred. starcraft though major challenge infer spatial actions spatial actions within space inputs might detrimental discard spatial structure input. propose fully convolutional network agent predicts spatial actions directly sequence resolution-preserving convolutional layers. network propose stride uses padding every layer thereby preserving resolution spatial information input. simplicity assume screen minimap inputs resolution. pass screen minimap observations separate -layer convolutional networks ﬁlters size respectively. state representation formed concatenation screen minimap network outputs well broadcast vector statistics along channel dimension. note likely non-optimal since screen minimap spatial extent future work could improve arrangement. compute baseline policies categorical actions state representation ﬁrst passed fully-connected layer units relu activations followed fully-connected linear layers. finally policy spatial actions obtained using convolution state representation single output channel. figure visual representation computation. fullyconv lstm agent baselines feed-forward architectures therefore memory. sufﬁcient tasks cannot expect enough full complexity starcraft. introduce baseline architecture based convolutional lstm. follow fully-convolutional agent’s pipeline described simply convolutional random agents random baselines. random policy agent picks uniformly random among valid actions highlights difﬁculty stumbling onto successful episode large action space. random search baseline based fullyconv agent works taking many independent randomly initialised policy networks evaluating episodes keeping highest mean score. complementary samples policy space rather action space. truncate trajectory backpropagation forward steps network terminal signal received. optimisation process runs asynchronous threads using shared rmsprop. method experiments using randomly sampled hyperparameters. learning rate sampled form interval. learning rate linearly annealed sampled value half initial rate agents. independent entropy penalty action function action-function argument. rate every game steps equivalent three actions second apm. experiments steps figure performance full game best hyper-parameters versus easy built-in player opponent using outcome reward; using native game score provided blizzard reward. notably baseline agents learn even single game. architectures original atari architecture used network uses convnet preserve spatial information screen minimap actions convolutional lstm layer. lines smoothed visibility. experiments full game selected abyssal reef ladder used ranked online games well professional matches. agent played easiest built-in terran versus terran match-up. maximum game length minutes declared episode terminated. results experiments shown figure unsurprisingly none agents trained sparse ternary rewards developed viable strategy full game. successful agent based fully convolutional architecture without memory managed avoid constant losses using terran ability lift move buildings attack range. makes difﬁcult easy within minute time limit. agents trained blizzard score converged trivial strategies avoid distracting workers mining minerals. agents converged simply preserving initial mining process withbuilding units structures results suggest full game starcraft indeed challenging domain especially without access sources information human replays. figure training process baseline agent architectures. displayed lines mean scores function game steps. three network architectures used figure faint lines show runs different hyper-parameters; solid line best mean. lines smoothed visibility. table aggregated results human baselines agents mini-games. agents trained steps. mean corresponds average agent performance best mean average performance best agent across different hyper-parameters corresponds maximum observed individual episode score. described section avoid complexity full game deﬁning minigames focus certain aspects game trained agents mini-game. aggregated training results shown figure ﬁnal results comparisons human baselines found table video showcasing agents also found https//youtu.be/lygsm. overall fully convolutional agents performed best across non-human baselines. somewhat surprisingly atari-net agent appeared quite strong competitor mini-games involving combat namely findanddefeatzerlings defeatroaches defeatzerlingsandbanelings. collectmineralsandgas best convolutional agent learned increase initial resource income producing worker units assigning mining. found buildmarines strategically demanding mini-game perhaps closest full game starcraft. best results game achieved fullyconv lstm random search atari-net failed learn strategy consistently produce marines episode. noted that without restrictions action space imposed would signiﬁcantly diffucult learn produce marines mini-game. agents performed sub-optimally compared grandmaster player except simplest movetobeacon mini-game requires good mechanics reaction time artiﬁcial agents expected good however games like defeatroaches findanddefeatzerglings agents fare well versus deepmind game tester. game replays crucial resource used professional amateur players alike learn strategies critical mistakes made game simply enjoy watching others play form entertainment. replays especially important starcraft hidden information fog-of-war hides opponent’s units unless within view own. thus among professional players standard practice review analyse every game play even win. supervised data replays human demonstrations successful robotics game atari also used context starcraft though train policy basic actions rather discover build orders. starcraft provides opportunity collect learn large growing human replays. whereas central standardised mechanism collecting replays starcraft large numbers anonymised starcraft games readily available blizzard’s online ladder. well games added regular basis relatively stable player pool plays games. learning replays useful bootstrap complement reinforcement learning. isolation could also serve benchmark sequence modelling memory architectures deal long term correlations. indeed understand game unfolds must integrate information across many time steps efﬁciently. furthermore partial observability replays could also used study models uncertainty variational autoencoders finally comparing performance outcome/action prediction help guide design neural architectures suitable inductive biases domain. rest section provide baselines using architectures described section using games learn value function policy games contain possible matchups starcraft figure statistics replay used supervised training policy value nets. distribution player rating function apm. distribution actions sorted probability usage human players. figure shows statistics replays used. summarize interesting stats here skill level players measured match making rating varies casual gamer high-end amateur professionals. average number actions minute mean replays ﬁltered instead ‘ranked’ league games played battlenet used less percent masters level replays players. also show distribution actions sorted frequency human players. frequent action taken time moving camera. overall action distribution heavy tail commonly used actions large number actions used infrequently train dual-headed networks predict game outcome action taken player time step. sharing body network makes necessary balance weights loss functions also allows value policy predictions inform another. make ties separate game outcome class supervised training setup since number ties dataset compared victory defeat predicting outcome game challenging task. even professional starcraft commentators often fail predict winner despite full access game state value functions accurately predict game outcomes desirable used alleviate challenge learning sparse rewards. given state well trained value function suggest neighbouring states would worth moving long seeing game outcome. setup supervised learning begins straightforward baseline architectures described section atari-net fullyconv. networks take account previous observations i.e. predict outcome single frame furthermore observation include privileged information agent produce value predictions based given time step thus opponent managed secretly produce many units effective army agent built mistakenly believe position stronger figure accuracy predicting outcome starcraft games using network operates screen minimap feature planes well scalar player stats. train curves three different network architectures. accuracy game time. beginning game network accuracy expected since outcome less clear earlier game. minute mark network able correctly predict winner time. networks proposed section produce action identiﬁer arguments independently. however accuracy predicting point screen improved conditioning base action e.g. building extra base versus moving army. thus addition atari-net fullyconv architecture arfullyconv uses auto-regressive policy introduction introduced section i.e. using function identiﬁer previously sampled arguments model policy current argument networks trained steps gradient descent possible match-ups starcraft trained mini-batches observations taken random replays uniformly across time. observations sampled step multiplier consistent setup. resolution screen minimap observation consists screen minimap spatial feature layers well player stats food number collected minerals human players screen. replays training ﬁxed test frames drawn rest replays. agent performance evaluated continuously test training progresses. figure shows average accuracy training step well accuracy trained model function game time. random baseline would correct approximately time since game well balanced across race pairs tying extremely rare. training progresses fullyconv architecture achieves accuracy also game progresses value prediction becomes accurate seen figure mirrors results prior work starcraft table policy accuracies base actions screen/minimap arguments. arfullyconv refers autoregressive version fullyconv. random baseline arfullyconv randomly initialised weights. network trained predict values separate output designed predict action issued user. sometimes refer part network policy since readily deployed play game. many schemes might employ train networks imitate human behaviour replays. simple approach connects straightforwardly work section training policy sampled observations ﬁxed step multiplier frames. take ﬁrst action issued within frames learning target policy. action taken period take target ‘no-op’ i.e. special action effect. humans play starcraft subset possible actions available given time. example building marine enabled barracks currently selected. networks need learn avoid illegal actions since information readily available. thus training ﬁlter actions would available human player. take union available actions past frames apply mask sets probability unavailable actions near zero. note that previously mentioned trained policy play possible matchups. thus principle agent play race. however consistency reinforcement learning agents studied section report in-game metrics single terran versus terran matchup. table shows different architectures perform terms accuracy predicting action identiﬁer screen minimap argument. expected fullyconv arfullyconv architectures perform much better spatial arguments. well arfullyconv architecture outperforms fullyconv presumably knows action identiﬁer argument used for. directly plug policy trained supervised learning game able produce units play better function observed replay data shown figure video https//youtu.be/weozidexfc. also outperforms agents trained section simpler mini-game buildmarines restricted action space even though supervised policy playing unrestricted full game. results suggest supervised imitation learning promising direction bootstrapping starcraft agents. future work look improve imitation initialised policies training directly reinforcement learning objective really care i.e. game outcome. paper introduces starcraft challenge deep reinforcement learning research. provide details freely available python interface play game well human replay data ranked games collected blizzard’s ofﬁcial battlenet ladder. initial release figure probability building army units training policy nets progresses training data. game setup terran terran. probability building army units game. average number army units built game. describe supervised learning results human replay data policy value networks. also also describe results straightforward baseline agents seven mini-games full game. regard mini-games primarily unit tests. agent able achieve human level performance relative ease chance succeed full game. instructive build additional mini-games take full game evaluated ﬁnal outcome interesting problem hope ﬁrst foremost encourage research lead solution. performance mini-games close expert human play expected current state-of-the-art baseline agents cannot learn easiest built-in full game. true game outcome used reward signal also shaping reward provided timestep sense provided environment presents challenge canonical externally deﬁned completely intractable off-the-shelf baseline algorithms. release simpliﬁes several aspects game played humans observations preprocessed given agent action space simpliﬁed easily used agents instead using keyboard mouse-click setup used humans played lock-step agents compute long need time-step rather real-time full game allows play built-in however consider real challenge build agents play best human players turf pixel observations strict time limits. therefore future releases relax simpliﬁcations above well enable self-play moving towards goal training agents humans consider fair opponents. would like thank many blizzard especially tommy tran tyler plass brian song dijck greg risselada grandmaster. would also like thank deepmind team especially kalchbrenner eslami jamey stevenson adam cain esteemed game testers amir sadik sarah york. also would like thank david churchill early feedback building commandcenter comments manuscript. charles beattie joel leibo denis teplyashin ward marcus wainwright heinrich k¨uttler andrew lefrancq simon green v´ıctor vald´es amir sadik deepmind lab. arxiv preprint arxiv. george dahl dong deng alex acero. context-dependent pre-trained deep neural networks large-vocabulary speech recognition. ieee transactions audio speech language processing duan chen rein houthooft john schulman pieter abbeel. benchmarking deep reinforcement learning continuous control. international conference machine learning pages todd hester matej vecerik olivier pietquin marc lanctot schaul bilal piot andrew sendonaris gabriel dulac-arnold osband john agapiou. learning demonstrations real world reinforcement learning. arxiv preprint arxiv. ulit jaidee h´ector mu˜noz-avila. classq-l q-learning algorithm adversarial realtime strategy games. eighth artiﬁcial intelligence interactive digital entertainment conference michał kempka marek wydmuch grzegorz runc jakub toczek wojciech ja´skowski. vizdoom doom-based research platform visual reinforcement learning. computational intelligence games ieee conference pages ieee alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems pages sergey levine peter pastor alex krizhevsky julian ibarz deirdre quillen. learning hand-eye coordination robotic grasping deep learning large-scale data collection. international journal robotics research page volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. icml santiago ontan´on gabriel synnaeve alberto uriarte florian richoux david churchill mike preuss. survey real-time strategy game research competition starcraft. ieee transactions computational intelligence games peng peng quan yuan ying yaodong yang zhenkun tang haitao long wang. multiagent bidirectionally-coordinated nets learning play starcraft combat games. arxiv preprint arxiv. diego perez spyridon samothrakis julian togelius schaul simon lucas adrien cou¨etoux jeyull chong-u tommy thompson. general video game playing competition. computational intelligence games ivaylo popov nicolas heess timothy lillicrap roland hafner gabriel barth-maron matej vecerik thomas lampe yuval tassa erez martin riedmiller. data-efﬁcient deep reinforcement learning dexterous manipulation. arxiv preprint arxiv. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg feifei imagenet large scale visual recognition challenge. international journal computer vision david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature gabriel synnaeve nantas nardelli alex auvolat soumith chintala timoth´ee lacroix zeming florian richoux nicolas usunier. torchcraft library machine learning research real-time strategy games. arxiv preprint arxiv. yuandong tian qucheng gong wenling shang yuxin larry zitnick. extensive lightweight ﬂexible research platform real-time strategy games. arxiv preprint arxiv. yuandong tian qucheng gong wenling shang yuxin larry zitnick. extensive lightweight ﬂexible research platform real-time strategy games. arxiv preprint arxiv. nicolas usunier gabriel synnaeve zeming soumith chintala. episodic exploration deep deterministic policies starcraft micromanagement. international conference learning representations yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv.", "year": 2017}