{"title": "Non-Negative Matrix Factorization, Convexity and Isometry", "tag": ["cs.AI", "cs.CV"], "abstract": "In this paper we explore avenues for improving the reliability of dimensionality reduction methods such as Non-Negative Matrix Factorization (NMF) as interpretive exploratory data analysis tools. We first explore the difficulties of the optimization problem underlying NMF, showing for the first time that non-trivial NMF solutions always exist and that the optimization problem is actually convex, by using the theory of Completely Positive Factorization. We subsequently explore four novel approaches to finding globally-optimal NMF solutions using various ideas from convex optimization. We then develop a new method, isometric NMF (isoNMF), which preserves non-negativity while also providing an isometric embedding, simultaneously achieving two properties which are helpful for interpretation. Though it results in a more difficult optimization problem, we show experimentally that the resulting method is scalable and even achieves more compact spectra than standard NMF.", "text": "paper explore avenues improving reliability dimensionality reduction methods non-negative matrix factorization interpretive exploratory data analysis tools. ﬁrst explore diﬃculties optimization problem underlying showing ﬁrst time non-trivial solutions always exist optimization problem actually convex using theory completely positive factorization. subsequently explore four novel approaches ﬁnding globallyoptimal solutions using various ideas convex optimization. develop method isometric preserves non-negativity also providing isometric embedding simultaneously achieving properties helpful interpretation. though results diﬃcult optimization problem show experimentally resulting method scalable even achieves compact spectra standard nmf. paper explore avenues improving reliability dimensionality reduction methods nonnegative matrix factorization interpretive exploratory data analysis tools make reliable enough making scientiﬁc conclusions astronomical data. dimensionality reduction method much recent interest common kinds data sometimes yield results meaningful returned classical method principal component analysis example data signiﬁcant interest images text astronomical spectra data values non-negative produce components interpreted objects type data added together produce observed data. words components likely sensible images documents spectra. makes potentially useful intersecond important interpretive usage dimensionality reduction methods plot data points low-dimensional space obtained multidimensional scaling methods recent nonlinear manifold learning methods focus usage typically enforcing distances points original high-dimensional space preserved low-dimensional space then apparent relationships low-d plot correspond actual relationships. plot points using components found standard methods general produce misleading results regard existing methods enforce constraint. another major reason might yield reliable interpretive results current optimization methods might actual optimum leading poor performance terms interpretive usages. objective function convex unconstrained optimizers used. thus obtaining reliably interpretable method requires understanding optimization problem deeply especially going actually create additionally diﬃcult optimization problem adding isometry constraints. paper organization. section ﬁrst study fundamental level optimization problem standard nmf. relate ﬁrst time problem theory completely positive factorization using theory show every nonnegative matrix non-trivial exact non-negative matrix factorization form w=vh basic fact shown now. using theory also show convex formulation optimization problem exists though practical solution method formulation exist. explore four novel formulations optimization problem toward achieving global optimum convex relaxation using positive semideﬁnite cone approximating semideﬁnite cone smaller ones convex multi-objective optimization generalized geometric programming. highlight diﬃculorder turn question create isometric section give background recent successful manifold learning methods maximum variance unfolding variant maximum furthest neighbor unfolding shown experimentally recover intrinsic dimension dataset reliably eﬀectively compared previous well-known methods isomap laplacian eigen-maps diﬀusion maps synthetic experiments methods manage decompose data meaningful dimensions. example dataset consisting images statue photographed diﬀerent horizontal vertical angles mfnu dimensions identiﬁed horizontal vertical camera angle. mfnu contain ideas particularly concerning formulation optimization problems upon isometric based. section show practical algorithm isometric representing data mining method capable producing interpretable components interpretable plots simultaneously. ideas eﬃcient optimization eﬃcient neighborhood computation obtain practical scalable method. section demonstrate utility isonmf experiments datasets used previous papers. show components ﬁnds comparable found standard additionally preserves distances much better also results compact spectra. given non-negative matrix goal decompose matrices ℜk×m factorization always exists factorization trivial solution determining minimum diﬃcult problem algorithm exists ﬁnding general show cast completely positive factorization problem negative matrix cholesky factorization always exists guarantee cholesky factors non-negative too. algorithm polynomial complexity decide given positive matrix simple observation show positive deﬁnite necessary suﬃcient condition. notice arbitrary diagonally dominant completely positive matrices always exists. simplest choice would chose diagonal matrices element greater equal rows/columns since diagonally dominant according always since exists although theorem also provides algorithm constructing cp-factorization cp-rank usually high. corollary theorems rank) always compact spectrum nmf. solving optimization problem nmf. although current literature widely believed non-convex problem local minima found show following subsections convex formulation exist. despite existence convex formulation also show formulation problem generalized geometric program non-convex could give better approach ﬁnding global optimum. unfortunately sign components cannot guaranteed non-negative except ﬁrst eigenvector however project nonnegative orthant good estimate nmf. call clipped csvd used benchmark relaxations follow. also used initializer algorithms relaxation positive semideﬁnite cone. minimization problem cost function norm nonlinear terms wilhlj appear. typical terms would generate large vector matlab notation column-wise unfolding matrix). true terms appearing linear following example show structure terms bold ones need express constraint determining recovered factorization easy problem. fact practical barrier function known cone interior point methods employed. finding practical description cone open problem. although problem convex algorithm known solving matrix often interpreted basis vectors factorization matrix coeﬃcients. widely known nature spectral analysis giving spectrum decays either exponentially e−λf slowly depending problem diﬀerent spectral functions. experiments chose exponential one. course decay parameter something adhoc. experimented several values couldn’t come systematic heuristic practical rule. cases reconstruction error others not. another relaxation necessary making optimization tractable reduce non-negativity constraints elements involved equality constraints. second formulation relaxed objective function tries minimize rank matrix constraints match values given matrix solving optimization problem solution found ﬁrst eigenvector quality relaxation depends ratio ﬁrst eigenvalue rest. positivity guarantee ﬁrst eigenvector elements sign according peron frobenious theorem ideally rest eigenvectors positive also included. problems method complexity. k)k−) non-negative constraints. quickly problem becomes unsolvable. practice problem posed always gives matrices rank one. testing method exhaustively random matrices either product representation solution always rank always case convex formulations presented paper. unfortunately multi-objective optimization problems even convex local minima global. interesting direction would test robustness existing multi-objective algorithms nmf. local solution non-convex problem. previous sections gave several convex formulations relaxations problem unfortunately either unsolvable give trivial rank solutions useful all. practice non-convex formulation along like distance used practice non-convex several methods recommended alternating least squares gradient decent active methods experiments used l-bfgs method scales well large matrices. total magnitude okk−) complexity previous method. also signiﬁcant improvement computational part. problem solved interior point methods require inversion symmetric positive definite matrix point. previous method would require steps method invert matrices would cost special structure actual cost maxk convex multi-objective problem. diﬀerent approach would convex solution lives search there. assume want match wilhlj section show controlling ratio norms possible solution nmf. del= vijl form concept manifold learning represent dataset lower dimensional space preserving local distances. diﬀerences methods isomap maximum variance unfolding laplacian eigenmaps diﬀusion maps treat distances points local neighborhood. example isomap preserves exactly geodesic distances diﬀusion maps preserves distances based diﬀusion kernel. maximum furthest neighbor unfolding variant maximum variance unfolding preserves local distance tries maximize distance furthest neighbors. section going present mfnu method basis building isonmf. convex maximum furthest neighbor unfolding. weinberger formulated problem isometric unfolding semideﬁnite programming algorithm kulis presented non-convex formulation problem requires less memory semideﬁnite one. also claimed non-convex formulation scalable. non-convex formulation global optimum semideﬁnite proven vasiloglou presented experiments veriﬁed scalability formulation. variant maximum furthest neighbor unfolding also presented paper. latest formulation tends robust scalable employ basis isonmf. methods cast semideﬁnite programming problem given data number points dimensionality product gram matrix deﬁned goal gram matrix rank rank words dataset represented fewer dimensions requirement isometric unfolding euclidian distances given neighborhood around every point expressed solved general oﬀ-the-shelf global optimization algorithms. also formulated special case programming generalized geometric programming. following transformation ˜wil e˜hlj objective becomes algorithm proposed employed. algorithm uses branch bound scheme impractical high dimensional optimization problems requires many iterations converge. worthwhile though compare thelocal nonconvex solver small matrix. tried order following random matrix another direction investigated paper recently developed algorithm diﬀerence convex problems applied successfully data mining applications multidimensional scaling. gradient descent possible solve minimization lagrangian rather slow. newton method also prohibitive. hessian problem sparse matrix although cost inversion might high worth investigating. experiments used limited memory bfgs method known give good rate convergence. mfnu non-convex formulation behaves much better mvu. experiments presented mfnu tends often global optimum mvu. experiments also showed method scales well points. mfnu optimization problems. goal isonmf combine optimization problems optimization problem. mfnu convex non-convex formulation nonconvex formulation solved known. structure computes distance point furthest neighbor. last condition centering constraint covariance matrix. dimensions eigenvectors general mfnu gives gram matrices compact spectrum least compact traditional linear principal component analysis method behaves equally well mvu. mfnu convex converge global optimum. unfortunately method handle datasets hundreds points complexity. following section non-convex formulation problem scales better presented. computing local neighborhoods. already discussed previous section mfnu isonmf require computation all-nearest all-furthest neighbors. all-nearest neighbor problem special case general class problems called n-body problems following sections give sort description nearest neighbor computation. actual algorithm four-way recursion. details found kd-tree. kd-tree hierarchical partitioning structure fast nearest neighbor search every node recursively partitioned nodes until points contained less ﬁxed number. leaf. nearest neighbor search based recursion query point ﬁnds closest leaf. recursion hits leaf searches locally candidate nearest neighbor. point upper bound nearest neighbor distance meaning true neighbor away candidate one. recursion backtracks eliminates nodes away candidate neighbor. kd-trees provide average nearest neighbor search time although pathological cases kd-tree performance asymptotically linear complexity like naive method. dual tree algorithm nearest neighbor computation. single tree algorithm reference points ordered kd-tree. every nearest neighbor computation requires computations. since query points total cost dual-tree algorithm orders query points tree too. query reference share tree. instead querying single point time dual-tree algorithm always queries group points live node. instead doing top-down recursion individually every point whole group once. moreover instead computing distances points nodes computes distances nodes. reason times dual-tree algorithm prune larger portions tree single tree algorithm. complexity dual-tree algorithm empirically dataset pathological algorithm quadratic complexity too. pseudo-code algorithm described results classic isonmf kneighborhood equal presented tables observe classic gives always lower reconstruction error rates away isonmf. classic fails preserve distances contrary isonmf always good preserving distances. another observation isonmf gives sparse solution classic nmf. case diﬀerence reconstruction error cbcl-face database dual tree algorithm furthest neighbor algorithm. computing furthest neighbor naive computation also quadratic complexity. trees speed computations too. turns furthest neighbor search single query point similar nearest neighbor search presented original paper kd-tree diﬀerence top-down recursion algorithm always chooses furthest node. similarly bottom recursion prune node maximum distance point node smaller current furthest distance. pseudo code presented preprocessed. mainly preprocessing distorts images spoils manifold structure. don’t preprocessing reconstruction error isonmf almost same. would also like point isonmf scales equally well classic nmf. moreover seem show sensitivity initial conditions. represents energy component normalized energy prototype image generated nmf/isonmf. although results show isonmf much compact reasonable metric. prototypes some images cbcl face database images variance normalization mean thresholding interval synthetic statue dataset isomap website images faces database paper presented deep study optimization problem showing fundamental existence theorems ﬁrst time well various advanced optimization approaches convex nonconvex global local. believe study capability open doors advances nmf-like methods well machine learning problems. also developed experimentally demonstrated method isonmf preserves non-negativity isometry simultaneously providing types interpretability. added reliability scalability stemming eﬀective optimization algorithm believe method represents potentially valuable practical tool exploratory analysis common data images text spectra.", "year": 2008}