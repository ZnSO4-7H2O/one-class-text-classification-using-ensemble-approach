{"title": "All You Need is Beyond a Good Init: Exploring Better Solution for  Training Extremely Deep Convolutional Neural Networks with Orthonormality and  Modulation", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. Moreover, we design a backward error modulation mechanism based on the quasi-isometry assumption between two consecutive parametric layers. Equipped with these two ingredients, we propose several novel optimization solutions that can be utilized for training a specific-structured (repetitively triple modules of Conv-BNReLU) extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/ identity mappings from scratch. Experiments show that our proposed solutions can achieve distinct improvements for a 44-layer and a 110-layer plain networks on both the CIFAR-10 and ImageNet datasets. Moreover, we can successfully train plain CNNs to match the performance of the residual counterparts.  Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset.", "text": "deep neural network difﬁcult train predicament becomes worse depth increases. essence problem exists magnitude backpropagated errors result gradient vanishing exploding phenomenon. show variant regularizer utilizes orthonormality among different ﬁlter banks alleviate problem. moreover design backward error modulation mechanism based quasiisometry assumption consecutive parametric layers. equipped ingredients propose several novel optimization solutions utilized training speciﬁc-structured extremely deep convolutional neural network without shortcuts/ identity mappings scratch. experiments show proposed solutions achieve distinct improvements -layer -layer plain networks cifar- imagenet datasets. moreover successfully train plain cnns match performance residual counterparts. besides propose principles designing network structure insights evoked orthonormality. combined residual structure achieve comparative performance imagenet dataset. deep convolutional neural networks improved performance across wider variety computer vision tasks especially image classiﬁcation object detection segmentation much improvement give credit gradually deeper network architectures. four years layer number networks escalates several hundreds learns abstract expressive representations large amount data e.g. simply stacking layers onto current architectures reasonable solution incurs vanishing/exploding gradients handle relatively shallower networks variety initialization normalization methodologies proposed deep residual learning utilized deal extremely deep ones. though works e.g. also announced train extremely deep network improved performance deep residual network still best practical solution dealing degradation training accuracy depth increases. however substantial residual networks exponential ensembles relatively shallow ones interpretation veit avoids vanishing/exploding gradient problem instead resolving directly. intrinsically performance gain networks determined multiplicity depth. train ultra-deep network still open research question works concern. researches still focus designing complicated structures based residual block variants anyway dose exist applicable methodology used training genuinely deep network? paper direct feasible solution answer question. think batch normalization necessary ensure propagation stability forward pass ultra-deep networks learning availability exists backward pass propagates errors top-down way. constrain network’s structure repetitive modules consisted convolution relu layers analyze jacobian output respect input consecutive modules. show cannot guarantee magnitude errors stable backward pass ampliﬁcation/attenuation effect signal acinitialization neural networks. depth increases gaussian initialization cannot sufﬁce train network scratch prevalent works proposed glorot bengio respectively. core idea works keep unit variance layer’s output. sussillo abbott propose novel random walk initialization mainly focus adjusting so-called scalar factor make ratio input/output error constant around kr¨ahenb¨uhl introduce data-dependent initialization ensure layers training equal rate. orthogonality also consideration. saxe analyse dynamics learning linear deep neural networks. convergence rate random orthogonal initialization weights equivalent unsupervised pre-training superior random gaussian initialization. lsuv initialization method proposed takes advantage orthonormality also makes unit-variance layer’s output. opinion well-behaved initialization enough resist variation learning progresses good initial condition cannot ensure preferred condition keep unchanged time especially extremely deep networks. argument forms basic idea motivates explore solutions genuinely deep networks. signal propagation normalization. normalization common ubiquitous technique machine learning community. whitening decorrelation input data brings beneﬁts deep learning machine learning algorithms helps speeding training process batch normalization generalize idea ensure layer’s output identical distributions reduce internal covariate shift. weight normalization inspired decoupling norm weight vector direction introducing independencies examples minibatch. overcome disadvantage dependent minibatch size layer normalization proposed solve normalization problem recurrent neural networks. method cannot applied assumption violates statistics hidden layers. applicable arpit introduce normalization propagation reduce internal covariate shift convolutional layers even rectiﬁed linear units. idea normalization layers’ activations promising little idealistic practice. since incoherence prior weight matrix actually true initialization phase even worsen iterations normalized magnitude layer’s activafigure diagram plain network architecture repetitive triple-layer module paper. green input data color ones denotes parametric layers yellow represents batch normalization layers blue means activation layers. actually structure similar plain designed cumulate layer-wisely results gradients exploding/vanishing. view norm-preserving keeping orthonormality ﬁlter banks within layer learning process sufﬁcient necessary condition ensure stability backward errors. condition cannot satisﬁed nonlinear networks equipped orthonormal constrain mitigate backward signal’s attenuation prove experiments. orthonormal regularizer introduced replace traditional weight decay regularization experiments show gains -layer network cifar-. however depth increases e.g. deeper layers non-orthogonal impact induced relu gradients updating accumulates breaks dynamic isometry makes learning unavailable. neutralize impact design modulation mechanism based quasi-isometry assumption consecutive parametric layers. show quasi-isometry property mathematical analysis experiments. modulation global scale factor applied magnitude errors little unscrupulously backward pass layer-wise fashion. combined orthonormality experiments show plain shown fig. trained relatively well match performance residual counterpart. contributions paper summarized follows. demonstrate necessity applying explain potential reason results degradation problem optimizing deep cnns; concise methodology equipped orthonormality modulation proposed provide insights understand learning dynamics cnns; experiments analysis exhibit interequation represents kind pseudo-normalization transformation error signals compared forward operation. mean distribution input error zero symmetric infer mean distribution output error approximately zero. cenj= bias tralizes errors last term distribution biases cancelled owing normalized coefﬁcient normal distribution. besides errors normalized mismatched variance. type transformation change error signal’s original distribution layer-wise since second order moment layer’s output errors loses isometry progressively. however phenomenon ignored consider pair consecutive layers. sense think backward propagated errors also normalized well forward pass apply conv-bn-relu triple instead conv-relu-bn. biased distribution effect accumulated depth increases distort input signals’ original distribution several reasons make training extreme deep neural network difﬁcult. next section solve problem extent. orthonormality norm-preserving resides core idea section. vector mapped linear transformation another vector call transforma= tion norm-preserving. obviously orthonormality normalization proposed alone sufﬁcient necessary holding equation since given precondition signals forward pass deﬁnitely normalized analyse magnitude variation errors backward pass. keep gradient respect input previous layer normpreserving straightforward conclude would better maintain orthonormality among columns weight matrix speciﬁc layer learning process rather initialization according equivalently signal modulation. work done ﬁeld explicitly implicitly integrated idea modulation. broad sense modulation viewed persistent process combination normalization methodology keep magnitude variety signals steady learning. understanding summarize methods uniﬁed framework e.g. batch normalization activation modulation weight normalization parameter modulation etc. since complexity dynamics learning nonlinear neural networks even proven mathematical theory cannot guarantee variety signals keeping isometrical time practice applications. depth results butterﬂy effect exponential diffusion nonlinear gives rise indeﬁniteness randomness. recently proposed methods utilize isometry fail keep steady propagation signals over-layer networks. methods stabilize magnitude signals direction substituted control signals directions. however since complexity variations signals impossible conditions held ways modulation method. alternative option simplify problem constrain magnitude signals either direction whole attention another direction. batch normalization existed solution satisﬁes requirement. normalization forward pass reduce internal covariate shift layer-wise which opinion make focus analyses opposite direction. denotes partial derivative output sample respect sample component. jacobian speciality partial derivatives related components activations also samples mini-batch. component activations transformed independently expressed blocked diagonal matrix since independence among activations analyse sub-jacobians e.g. jkk. indicator operator. still omit index since dropping brings ambiguity. concludes obviously orthonormality held operation. correlation among columns directly impacted normalized activations corresponding weights determine activations turn results complicated situation. fortunately deduce preferred equation according subadditivity matrix rank makes jacobian ideally dynamical isometry obviously property cannot ensured because gradient update makes correlation among different columns weights stronger learning proceeding; nonlinear operations relu destroy orthonormality. however think reasonable force learned parameters conformed orthogonal group possible alleviate vanishing/exploding phenomenon magnitude errors signal distortion accumulated nonlinear transformation. rationality statements hypotheses proved experiments. adapt orthonormality convolutional operations generalize orthogonal expression direct modiﬁcation. denote convolution kernels layer width height input channel number output channel number respectively. replace original weight decay regularizer orthonormal regularizer regularization coefﬁcient weight decay total number convolutional layers and/or fully connected layers identity matrix fin× fout fout represents frobenius norm. words equation constraints orthogonality among ﬁlters layer makes learned features minimum correlation other thus implicitly reduce redundancy enhance diversity among ﬁlters especially lower layers besides orthonormality constraints provide alternative solution regularization exploration weight space learning process. provides probabilities limiting parameters orthogonal space instead inside hypersphere. dynamical isometry signal propagation neural networks mentioned underlined several times amounts maintain singular values jacobian section analyze variation singular values jacobian different types layers detail. omit layer index bias term simplicity clarity. close zero. think reasons violate perfect dynamic isometry result degradation problem kind non-full rank. since value determined bounded long variables keep stable learning process achieves so-called quasi-isometry notice changes change every iteration. based observation propose scale factor adjusted dynamically instead ﬁxing like according nonlinearity mean activity layer approximately neural population variance second order moment output errors capture dynamical properties quantitatively. relu nonlinearity satisﬁed owing pseudo-normalization regard errors propagated backwardly zero mean makes second order moment statistics reasonable. insist keep orthonormality throughout training process implement constraint initialization regularization. convolution parameter fin× fout layer initialize subset fin-dimension vectors ﬁrst output channel. gram-schmidt process applied sequentially generate next orthogonal vectors channel channel. mathematically generating orthogonal vectors d-dimension space satisﬁes ill-posed hence impossible. solution avoid fan-ins fan-outs kernels violating principle fout designing structures networks; another candidate group-wise orthogonalization proposed fout divide vectors fout foutmodfin groups orthogonalizafin tion implemented within group independently. encourage hybrid utilization regularization parameters fout orthonormal regularization fout. forcing parameters retract inconsistent manifolds cause convergence problems. details referred experiments. signal modulation compute second order moment statistics output errors consecutive parametric layers iteration. scale factor deﬁned square root ratio second order moment higher layer that lower layer. however modulate layers long magnitude propagated signal tend identical input error signal probably eliminate variety encoded error signal. make trade-off modulation happens magnitudes propagated signals consecutive layers mismatch. experiments show relatively reasonable non-extreme modulation mechanism first must demonstrate core idea paper show proposed methods used train extremely deep plain cnns improve performance drastically compared prevalent stochastic gradient descent regularization rather achieving state-of-the-art performance certain dataset manner means. moreover show degradation problem training plain network reported partially solved methods. datasets protocols representative datasets cifar- imacifar-. cifar- consists real world color images classes split train test images. present experiments trained training evaluated test set. top- accuracy evaluated. imagenet classiﬁcation. large-scale dataset imagenet classiﬁcation dataset used experiments. consists classes million training images validation images. top- top- error rates evaluated. protocol cifar-. demonstrate proposed method partially solve degradation problem show deeper plain network shallower shrunk even removed fair comparison plain network directly adopt proposed architectures minor modiﬁcations plain networks residual networks. speciﬁcally network inputs images per-pixel mean subtracted standard deviation divided. ﬁrst layer convolution following stack convolution layers convolution layer accompanied layer relu layer residual case size feature maps doubles e.g. projection shortcuts instead identity ones. hyperparameters weight decay momentum learning rate identical horizontal data augmentation. protocol imagenet classiﬁcation. architectures protocol also slight variation. detailed architectures referred table hyperparameters identical cifar- protocol. crops randomly sampled images plus horizontal color augmentation mean subtracted scaling factor mini-batch size performances validation reported. figure variation correlation among weight vectors layer plain network. meaning blue green line refereed legend. aware ﬁrst phase correlation blue line lower green allow negative correlations msra+l method generates negative ones ﬁrst iterations. section design experiments show orthonormality indeed enhance magnitude propagated signals deep plain networks decorrelating learned weights among different channels. -layer plain network cifar- dataset adopted. first make statistics average correlation among different channels layers types methods msra initialization plus regularization proposed orthonormal initialization orthonormality regularization cosine distance dcos considered compute value denotes kernel layer total computation count. fig. variation correlation among weights iterations. under constraints orthonormality correlation learned weights forced consistent relatively lower level contrary msra+l cannot prevent increasing correlation among weights learning progresses. finally correlation msra+l times higher ortho init+ortho demonstrates effectiveness orthonormality constraints. next make statistics variation second order moments back-propagated errors. since empirical risk convergence learning progresses results smaller magnitude loss value hence unscaled magnitude error signals actually plot ratio second order moment output error signals input error signals convolution layer). fig. tells ﬁrst training phase evolution signal propagation insensitive second third training phases mismatched order magnitudes learning rate decay coefﬁcient regularizer however shows advantage orthonormal regularization regularization matter phase especially later phases. magnitude propagated signals enhanced order magnitude orthonormality. important note omit ratios ﬁrst iterations fig. since disproportional order magnitude. interesting phenomenon magnitude error signals vanishing e.g. ratio less except initialization phase signals ampliﬁed. think randomness plays role phenomenon also provides evidence makes introduce orthonormality beyond initialization optimizing extremely deep networks. -layer network trained well orthonormality -layer incurs seriously divergence states accumulation effect mentioned sec. evidence. proposed modulation applied train -layer network achieves distinct performance improvement one-order methods training methodology little tricky ﬁrst apply orthonormality modulation ﬁrst iterations signals regulated orthonormality converges. keeping magnitude error signals isometric easily done modulation observed strategy undermines propagation signals modulation interesting research topic totally solve degradation problem. paper value somewhat heuristic derived observation evolution ratios second-order moment output errors layer second-order moment input errors iteration training -layer network. fig. reveals probably exists potential evolution pattern training deep networks. actually shrink degradation instead eliminating training genuinely deep networks future work focus methodology modulation. prove proposed method advantage methods integrated idea adaptivity training extremely deep plain networks compare prevalent one-order methods section. compare second-order methods consideration implementation memory practicality. table shows perfigure evolution backward signal propagation -layer plain network. axis denotes layer index axis denotes ratio second-order moment current layer highest layer. present iteration respectively. iterations ratio trend converge certain stable evolution pattern shown iterations. formances. methods cannot handle relatively shallow networks well methods except cannot even converge deeper version. pointed one-order methods effective method optimizing certain types deep learning architectures. next focus making comparison general method. also compare method modulation methods e.g. fail convergence ﬁrst iteration deep architecture compare performance different regularizer identical network architecture compare performance plain networks residual networks similar architectures results shown fig. conclude proposed method distinct advantage optimizing plain networks orthonormality indeed enhance magnitude signal alleviates gradient vanishing training process. emphasize orthonormality general prevalent network architectures large-scale datasets extend experiments imagenet dataset. fig. shows decreasing performance boost resnet- almost comparative performance resnet-. compared architectures cifar- channels e.g. introduces redundanfigure miscellaneous performance comparisons plain residual networks cifar- imagenet. left performance comparisons -layer plain network cifar-. orthonormality boosts plain network match performance residual architecture. middle performance comparisons -layer resnet cifar-. orthonormality helps convergence thus achieve higher performance. right performance comparisons -layer resnet -layer resnet different regularization imagenet. table performance comparison cifar- imagenet different optimization methods. plain -layer -layer networks trained methods. common hyperparameters identical speciﬁc ones default regularizer applied methods except ours. demonstrates corresponding method cannot convergence means methods tested imagenet. figure weights visualization ﬁrst convolution layer layer residual network imagenet. left converged weights regularization. right converged weights orthonormality. cies among intra-layer’s ﬁlter banks. fig. used explain results probably difﬁcult orthonormality explore parameter space many redundancies. right sub-ﬁgure fig. shows noise-like feature maps left inspires design thinner architectures future work. recently proposed similar ideas. unify three types kernel normalization methods geometric framework called kernel submanifolds sphere oblique compact stiefel manifolds considered. differences exists three aspects intrinsic explanations performance improvement different mainly focus regularization models data augmentation learning models endowed geometric invariants; orthogonalization different orthogonalize convolutional kernels within channel among channel; second statement tells believe proposed method still cannot handle extremely deep plain networks. besides details steps implement methods ambiguous prevents understanding verifying further. intrinsically regard proposed modulation assigning parametric layer individual adaptive learning rate. kind modulation practical local methods e.g. second-order methods ﬂexible global ones e.g. sgd. besides approach strategies compensate evanescent orthonomality learning progresses believe training genuinely deep network available. propose simple direct method train extremely deep plain networks orthonormality modulation. furthermore orthonormality reveals generalization capability applied residual networks. great performance boost observed experiments. however degradation problem still totally solved condition understanding comprehensively insights signal modulation reparametrization novel constraints etc. hope work encourage attentions problem.", "year": 2017}