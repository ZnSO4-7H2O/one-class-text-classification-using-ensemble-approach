{"title": "LogDet Rank Minimization with Application to Subspace Clustering", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Low-rank matrix is desired in many machine learning and computer vision problems. Most of the recent studies use the nuclear norm as a convex surrogate of the rank operator. However, all singular values are simply added together by the nuclear norm, and thus the rank may not be well approximated in practical problems. In this paper, we propose to use a log-determinant (LogDet) function as a smooth and closer, though non-convex, approximation to rank for obtaining a low-rank representation in subspace clustering. Augmented Lagrange multipliers strategy is applied to iteratively optimize the LogDet-based non-convex objective function on potentially large-scale data. By making use of the angular information of principal directions of the resultant low-rank representation, an affinity graph matrix is constructed for spectral clustering. Experimental results on motion segmentation and face clustering data demonstrate that the proposed method often outperforms state-of-the-art subspace clustering algorithms.", "text": "system identiﬁcation. instance low-rank representation based subspace clustering matrix completion methods achieved great success recently. subspace clustering fundamental topics numerous applications e.g. image representation face clustering motion segmentation assumed high-dimensional data likely union low-dimensional subspaces rather individual subspace. example different subspaces needed describe trajectories diﬀerent moving objects video sequence. subspace clustering intrinsically diﬃcult problem since need simultaneously cluster data points multiple groups low-dimensional subspace ﬁtting group points. subspace clustering active research topic past decades. four main categories methods proposed iterative algebraic statistical spectral clustering-based methods. ﬁrst three kinds approaches sensitive initialization noise outliers; addition diﬃcult optimize spectral clustering-based methods achieved promising performance learn good aﬃnity matrix data points. instance algorithms local subspace aﬃnity locally linear manifold clustering spectral local best-ﬁt ﬂats local information around point construct aﬃnity matrix spectral curvature clustering method preserves global structures whole data deriving aﬃnity matrix. subsequently k-means normalized cuts applied aﬃnity matrix obtain clustering results. abstract low-rank matrix desired many machine learning computer vision problems. recent studies nuclear norm convex surrogate rank operator. however singular values simply added together nuclear norm thus rank well approximated practical problems. paper propose logdeterminant function smooth closer though non-convex approximation rank obtaining low-rank representation subspace clustering. augmented lagrange multipliers strategy applied iteratively optimize logdet-based non-convex objective function potentially large-scale data. making angular information principal directions resultant low-rank representation aﬃnity graph matrix constructed spectral clustering. experimental results motion segmentation face clustering data demonstrate proposed method often outperforms state-of-the-art subspace clustering algorithms. keywords matrix rank approximation subspace clustering nuclear norm log-determinant low-rank representation angular information segmentation matrix rank minimizing ubiquitous machine learning computer vision control signal processing kang peng cheng computer science department southern illinois university carbondale e-mail qchengcs.siu.edu state-of-the-art results subspace clustering. represents data point sparse linear combination points solves l-norm regularized minimization problem sparsity. shows promising results subspaces either independent disjoint basic idea learn low-rank representation data capturing global euclidean structure whole data. scheme data point represented linear combination examples data matrix itself convex nuclear norm minimization used surrogate rank function obtain desired low-rank representation. though optimization well-studied global optimum performance optimal real applications nuclear norm might good approximation rank function. compared rank function nonzero singular values equal contributions nuclear norm treats values diﬀerently simply adding together. result nuclear norm dominated large singular values signiﬁcantly deviated true value rank. several papers considered problem using nuclear norm designed methods alleviate either thresholding removing singular values; instance singular value thresholding truncated nuclear norm considerably enhance performance matrix completion. paper propose log-determinant function rank approximation study minimization subspace clustering. diﬀerent nuclear norm-based approaches minimize summation singular values approach aims minimize rank making contribution much closer singular value zero small singular value. closer robust approximation rank function nuclear norm. since logdet function non-convex apply method augmented lagrange multipliers solve associated optimization potentially large-scale applications subproblem minimizing logdet function iteration closed-form solution. demonstrate eﬀectiveness logdet minimization method apply subspace clustering. employing rather simple formulation based logdet function obtain low-rank representation subspace clustering. subsequently exploit angular information principal directions representation enhance separation ability aﬃnity matrix. summary main contributions work include accurate robust rank approximation used obtain low-rank representation able capture global structure dataset. iterative optimization algorithm designed minimizing rank approximation-based objective function. theoretical analysis shows algorithm converges stationary point. speciﬁcally proposed optimization method applied subspace clustering. angular information principal directions low-rank representation employed exploit intrinsic local geometrical structure relevant membership data points. extensive experiments demonstrate eﬀectiveness proposed logdet minimization method rank approximation. especially used subspace clustering simple formulation shows favorable performance compared state-of-theart methods although explicitly account outliers model. demonstrates robustness approach. remainder paper organized follows section provides brief review ssc. section present proposed approximation design eﬃcient optimization scheme. give convergence analysis section experimental results shown section finally conclusions drawn section tries seek lowest rank representation among many possible linear combinations bases given dictionary typically data matrix itself. problem formulated surrogate rank function obvious easily veriﬁed log) allog especially large nonzero singular values logdet function much smaller nuclear norm since log) large noted small nonzero singular values contribution logdet function signiﬁcantly reduced compared nuclear norm. small nonzero singular values often regarded noise data logdet function reduces noise eﬀect compared nuclear norm. proposed approximate rank iterative linearization used local minimum. however small constant leads biased approximation small singular values. logdet function diﬀerentiable respect singular values theorem even though non-convex minimization rather simple using optimization method. explain minimization consider speciﬁc application subspace clustering. employing logdet function simply formulate subspace clustering following unconstrained nonconvex minimization problem rn×n identity matrix. ﬁrst term minimize rank second relaxation referred selfexpressiveness representing similarity data points. logdet function convex resort technique solve re-writing follows function absolutely symmetric invariant arbitrary permutations sign changes elements based function following theorem theorem function rn×n unitarily invariant rn×n whose singular value decomposition diagv rn×n singular values min. gradient work utilize unitarily invariant function logdet achieve closer though convex rank relaxation nuclear norm. apply method logdet rank approximation associated minimization. explain method speciﬁcally consider using logdet rank surrograte subspace clustering. ﬁrst obtain low-rank representation high-dimensional data based logdet optimization. construct aﬃnity graph matrix spectral clustering using angular information principal directions low-rank representation. above holds frobenius norm unitary invariant; holds unitary invariant; true neumann’s inequality; holds inequality also obtained hoﬀmanwielandt inequality. therefore lower bound obtained minimizing note equality attained because minimizer problem hence proof completed. solved updating alternatively ﬁxing variables. speciﬁcally assume iteration obtained iteration optimization problem updated following four steps. algorithm scld algorithm input data matrix number subspaces parameters obtain algorithm compute skinny u∗σ∗t calculate construct aﬃnity graph matrix apply perform ncuts. using resultant aﬃnity matrix apply spectral clustering algorithm segmentation. paper simply perform ncuts proposed subspace clustering procedure summarized algorithm section give convergence analysis algorithm show optimization algorithm attains least stationary point problem ﬁrst rewrite objective function problem nonconvex. diﬃcult give rigorous mathematical argument convergence optimum. provide theoretical proof algorithm converges accumulation point accumulation point stationary point. empirical experiments conﬁrm convergence proposed method benchmark datasets. experimental results promising despite solution obtained proposed optimization method local optimum. construct aﬃnity matrix subspace clustering. optimal accurately describe relationship samples data severely corrupted. therefore general good idea construct directly using spirit construct aﬃnity matrix following way. represent i-th columns respectively parameter tunes sharpness aﬃnity between points helping separate clusters. increases between-cluster separability increased intra-cluster cohesiveness would nevertheless degraded. thus suitable needs balance within-cluster cohesiveness between-cluster separability. paper post-processing lrr. spans principal directions employ angle information powered correlation without loss generality assume converges z∗}. next prove accumulation point stationary point problem {yk} bounded i.e. ﬁrst-order optimality condition deﬁnition step satisﬁes βk−zk− i.e. βk−zk− βk−wk βk−+βk−+yk− βk−+yk. assumption satisﬁes conditions thus stationary point experiments synthetic data construct independent subspaces whose bases {ui} generated random rotation matrix random orthogonal matrix sample data vectors subspace ujtj matrix. data vectors randomly chosen corrupt; example data vector corrupted adding gaussian noise zero mean variance scld segment data clusters. subspace clustering error rate deﬁned misclassiﬁed points used assess performance. report clustering error rate diﬀerent corruption levels figure without corruption scld cluster data points correctly. section evaluate eﬀectiveness robustness scld benchmark datasets extended yale hopkins compare proposed method scld several state-of-theart subspace clustering algorithms lrsc local subspace aﬃnity methods parameters given term right hand side equation nonnegative term bounded. bounded implies singular values bounded bounded. since clearly bounded therefore {wk} {zk} bounded. theorem least accumulation point stationary point optimization problem assumption first experiment scenario done test algorithms ﬁrst classes eyaleb consists frontal face images. half images corrupted shadow noise. heavily corrupted data test eﬀectiveness method. shown table scld signiﬁcantly enhances performance. speciﬁcally improves clustering accuracy least compared algorithms. since difference approach rank approximation improvement logdet. second experiment scenario fair comparison followed experimental setup divide subjects four groups subjects consider choices subjects ﬁrst three groups. last group consider choices implement subspace clustering algorithm subjects. experiments stopping criterion triggered relative difference successive iterations maximum iterations. results presented table methods cited results table paper scld consistently clustering error rates stable methods whose error rates increase drastically number subjects increases respective authors. method also tune obtain best performance. generally relatively large data slightly corrupted. little inﬂuence clustering results ensure unique minimizer empirically. parameters shown table experiments conducted window memory intel core cpu. face clustering cluster face images multiple individuals hope reveal identity individuals. eyaleb database includes frontal images individuals. individual images taken lighting conditions described low-dimensional subspace images resized pixels shown figure many sparse within-sample outliers face images shadows. although uses regularization term count corruptions regularization term appear well suited eyaleb. inferior performance possibly explicitly exploit lowrank structure data. third experiment scenario section compare scld algorithms rpca preprocessing step. practice know clustering data beforehand hence apply rpca collection data points trial prior clustering. shown table scld still superior methods though apply rpca deal sparse outlying entries. compared table clustering error rates lrsc reduced cases. conclude applying rpca data points simultaneously eﬀective improve clustering performance. fact rpca seeks common low-rank subspace decrease principal angles subspaces decrease distance data points diﬀerent subjects motion segmentation segment trajectories associated diﬀerent moving objects diﬀerent groups according motions video sequence. diﬀerent motions treated diﬀerent subspaces hopkins dataset validate scld. dataset slightly corrupted shown figure consists sequences three experimental results reported table also used results table seen scld produces superior results compared methods. sequences error rate sequences overerror rate proposed algorithm report average computation time every sequence bottom table computational cost lrsc much lower methods scld comparable. sults show outperforms low-rank representation algorithms based nuclear norm. therefore logdet appears eﬀective rank approximation function well suited subspace clustering applications. although model simple explicit modeling outliers resilient various corruptions. future research consider modeling corruptions explicitly.", "year": 2015}