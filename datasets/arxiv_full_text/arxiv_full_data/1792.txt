{"title": "Structural Correspondence Learning for Cross-lingual Sentiment  Classification with One-to-many Mappings", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Structural correspondence learning (SCL) is an effective method for cross-lingual sentiment classification. This approach uses unlabeled documents along with a word translation oracle to automatically induce task specific, cross-lingual correspondences. It transfers knowledge through identifying important features, i.e., pivot features. For simplicity, however, it assumes that the word translation oracle maps each pivot feature in source language to exactly only one word in target language. This one-to-one mapping between words in different languages is too strict. Also the context is not considered at all. In this paper, we propose a cross-lingual SCL based on distributed representation of words; it can learn meaningful one-to-many mappings for pivot words using large amounts of monolingual data and a small dictionary. We conduct experiments on NLP\\&CC 2013 cross-lingual sentiment analysis dataset, employing English as source language, and Chinese as target language. Our method does not rely on the parallel corpora and the experimental results show that our approach is more competitive than the state-of-the-art methods in cross-lingual sentiment classification.", "text": "structural correspondence learning effective method cross-lingual sentiment classiﬁcation. approach uses unlabeled documents along word translation oracle automatically induce task speciﬁc cross-lingual correspondences. transfers knowledge identifying important features i.e. pivot features. simplicity however assumes word translation oracle maps pivot feature source language exactly word target language. one-to-one mapping words different languages strict. also context considered all. paper propose cross-lingual based distributed representation words; learn meaningful one-to-many mappings pivot words using large amounts monolingual data small dictionary. conduct experiments nlp&cc cross-lingual sentiment analysis dataset employing english source language chinese target language. method rely parallel corpora experimental results show approach competitive state-of-the-art methods cross-lingual sentiment classiﬁcation. sentiment classiﬁcation task predict sentiment polarity given document product review commentary essay. goal develop automated approaches classify sentiment polarity text positive neutral negative. obtain satisfactory classiﬁcation performance methods require lots labeled data costly terms time human efforts. lots resources available english including labeled corpora sentiment lexicons languages resources often insufﬁcient. thus expected make knowledge learned resource-rich languages perform sentiment classiﬁcation languages substantially reduce human efforts. problem called cross-lingual sentiment classiﬁcation address paper. clsc uses annotated sentiment corpora language training data predict sentiment polarity data another language. domain adaptation focuses solving problem transferring knowledge copyright association advancement artiﬁcial intelligence rights reserved. typically sufﬁcient training samples resource-rich language target resource-scarce language. result domain adaptation proposed address problem work domain adaptation effective method clsc based structural correspondence learning named cross-lingual structural correspondence learning proposed prettenhofer stein idea identify low-dimensional representation captures correspondence features domains modeling correlations special pivot features. correspondences cross-lingual representation created enables transfer classiﬁcation knowledge source target language. approach good clsc transfers knowledge identifying important features. however simplicity cl-scl assumes word translation oracle maps pivot word source language exactly word target language. know machine translation performs simple substitution words language words another alone usually cannot produce good translation. furthermore one-to-one translation words different languages strict. mikolov proposed method exploiting similarities among languages. used distributed representation words learned linear mapping vector spaces represent corresponding languages respectively. translated word phrase entries learning language structures based large monolingual data mappings between languages small amount bilingual data. despite simplicity results showed method surprisingly effective especially translation between languages substantially different paper introduce distributed representation words cl-scl propose novel structural correspondence learning method oneto-many mappings method aims building one-to-many mappings pivot features source language target language. evaluations nlp&cc datasets show algorithm outperforms state-of-the-art methods. sentiment classiﬁcation sentiment classiﬁcation usually formulated two-class classiﬁcation problem positive negative. training testing data normally product reviews. emerged become active research area since year general sentiment classiﬁcation investigated mainly three levels document level sentence level aspect level. paper focus document level. document level sentiment classiﬁcation aims classify opinion document expressing positive negative opinion. approaches generally based kinds resources sentiment lexicons corpora. lexiconbased approaches predict sentiment polarities creating using sentiment lexicons corpora-based approaches generally treat sentiment classiﬁcation problem machine learning task. existing approaches focus extracting various features text applying supervised learning techniques learn classiﬁers pang ﬁrst take supervised learning classify movie reviews using unigrams features classiﬁcation standard machine learning techniques subsequent research many features learning algorithms developed number researchers. like supervised machine learning applications factor sentiment classiﬁcation effective features. also unsupervised methods. turney ﬁrst apply unsupervised learning technique based mutual information document phrases predict sentiment orientation average scores phrases given document. zhai proposed autoencoder based semisupervised learning method learn representations labeled unlabeled data. however supervised approaches mainstream methods sentiment classiﬁcation. moreover existing supervised semi-supervised approaches typically require high-quality labeled data train classiﬁers good accuracy. cross-lingual sentiment classiﬁcation traditional clsc approaches employ machine translation systems bridge source language target language. proposed cotraining approach address problem. labeled english reviews unlabeled chinese reviews translated labeled chinese reviews unlabeled english reviews separately. review thus views. support vector machines applied learn classiﬁers. finally classiﬁers combined single classiﬁer. designed bi-view nonnegative matrix tri-factorization model using machine translation. learned previously unseen sentiment words large parallel dataset. studied semi-supervised learning imbalanced sentiment classiﬁcation using dynamic co-training approach. compared several approaches incorporated class noise detection transductive transfer learning reduce negative transfers process transfer learning however machine translation perfect. translated text potentially mislead classiﬁer. consequently many researchers domain adaptation solve problem. various domain adaptation techniques explored prettenhofer stein proposed representative domain adaptation approach cl-scl effective cross-lingual sentiment classiﬁcation. found pivot features shared source language target language learned correlations pivot non-pivot features generated projection matrix build bridge languages. meng proposed cross-lingual mixture model leverage unlabeled bilingual parallel data. zhang proposed semi-supervised learning approach adjusted method train initial classiﬁer predict labels target instances obtain label space large-scale labeled target language dataset. selected conﬁdent instances trained classiﬁer. method needs learn three classiﬁers time-consuming. development deep learning shared deep representations employed clsc. researchers apply deep learning techniques learn bilingual representations paired sentences parallel corpora used learn word embeddings across languages eliminating need machine learning. zhou proposed approach learning bilingual sentiment word embeddings english-chinese clsc incorporated sentiment information text bilingual embeddings. zhou proposed cross-lingual representation learning model simultaneously learned word document representations languages. however high-quality bilingual embeddings rely large-scale task-related parallel corpora also scarce resource. problem deﬁnition labeled training documents written source language source language feature space target language feature space class labels. denote feature space. simplicity without loss generality consider binary classiﬁcation problem i.e. {+−}. addition denotes source vocabulary denotes target vocabulary unlabeled documents source language target language respectively. denote documents written different language prepivot sets based cl-scl one-to-many mappings approach based cl-scl approach cross-lingual text classiﬁcation builds structural correspondence learning. order induce task-speciﬁc crosslingual word correspondences approach used unlabeled documents along word translation oracle. advantage method resource efﬁciency parallel corpora recquired. ﬁrst step cl-scl deﬁne pivot features unlabeled data languages. pivot features used learn mapping original feature spaces languages shared low-dimensional feature space. mapping found cross-lingual sentiment classiﬁcation problem reduced standard classiﬁcation problem cross-lingual space. pivot features selected induce correspondences among words languages play important role cl-scl. pivot pair words separately source language target language possess similar semantics. ws’s translation target vocabulary querying translation oracle. however simplicity cl-scl assumes word translation oracle maps pivot source language exactly word target language. one-to-one mapping words different languages strict. addition consider context either translating pivot features. step propose cross-lingual based distributed representation words; learns meaningful one-to-many translations words using large amounts monolingual data small dictionary. framework approach generate pivot features demonstrated figure mutual information respect class labels labeled training documents source language. clscl used word translation oracle words source vocabulary corresponding translations target vocabulary unlike cl-scl reasonable build one-to-many mappings words languages. thus instead using translator domain expert build one-to-many mapping learning bilingual word pairs using distributed representation words. incorporate method wordvec recently proposed cl-scl translation. wordvec’s cbow model learn representations languages. model learns word representations using neural network architecture aims predict neighbors word. process follows step build monolingual models languages using large amounts documents suppose word pairs translation obtain associated vector representations distributed representation word source language vector representation target language step learn linear projection languages. goal translation matrix approximates learn following optimization problem section evaluate effectiveness efﬁciency algorithm scl-om proposed paper. english source language chinese target language task cross-lingual sentiment classiﬁcation. dataset preprocessing evaluate proposed approach open cross-lingual sentiment analysis task nlp&cc dataset includes product reviews three product categories amazon category contains labeled english reviews training data chinese reviews test data thousands chinese product reviews without label. furthermore since training monolingual language model needs large amount text data unlabeled english reviews learn representations english words. table details. english chinese review includes summary text category; extract content summary text combine review document expressed feature vector using unigram bag-of-words model. addition select words features frequency larger summarize vocabulary size datasets table first monolingual word vectors trained using cbow model negative sampling window size generate bilingual dictionary languages frequent words monolingual source datasets translate words using online google translate. addition experiments chinese word segmentation tool jieba monolingual sentiment classiﬁer svm. methods train labeled english reviews translated chinese google translate correspondence labels. chinese classiﬁer learned translated reviews. chinese testing dataset used test. tion word xas. target language space computing xas. obtain words closest target language space using cosine similarity distance metric. simplicity denotes cosine distance wordi automatically obtained wordvec. deﬁne threshold smaller take {word word word} translation smaller larger take {word word} translation; otherwise take {word} translation example consider pivot words excellent recommend follows ψexcellent ψrecommend english word translation chinese english non-chinese readers. supposing translations excellent recommend respectively {棒太好了出色} {推荐}. finally eliminate candidate pivots document frequency smaller threshold framework proposed method first described above generate pivot features pairs words pivot word source language pivot word target language. one-tomany mapping learning bilingual word pairs using distributed representation words. details described section. second similar cl-scl build connection unlabeled documents source target languages obtain low-dimensional hypothesis space pivot linear classiﬁer trained model correlations pivot words wt}. linear classiﬁer characterized parameter vector thus dimensional parameter matrix obtained correlations across pivots identiﬁed computing singular value decomposition dimensional representation. choosing columns associated largest singular values yields substructures capture correlation deﬁne columns associated largest singular values train using labeled english reviews training data english classiﬁer learned. chinese test reviews translated english google translate. translated testing dataset used test. proposed mixed clsc model combining co-training transfer learning strategies. improved accuracy removing noise transferred samples avoid negative transfers bswe zhou proposed method learned bilingual sentiment word embedding english-chinese clsc. proposed bswe incorporated sentiment information text bilingual embedding. baseline methods described categorized classes ﬁrst four preliminary methods; last three state-of-the-art models clsc. methods basic classiﬁer unigram+bigram features train basic classiﬁers except cl-scl approach unigram features. performance results recall scl-om parameters input number pivots dimensionality cross-lingual representation minimum support pivot word similarity distance threshold dimensionality english word vectors dimensionality chinese word vectors. ﬁxed values dimensionality word vectors mikolov showed dimensionality vectors trained source language several times larger vectors trained target language best performance. thus dimensionality english word vectors chinese word vectors 无聊/boring；乏味/tedious；空洞/inanity 最差/worst；最烂/worst 差/bad；差劲/awful；不好/not good 事实/fact 精彩/wonderful；棒/excellent 极其/highly；非常/very；十分/very 旅行/travel；旅程/journey；旅途/journey 可笑/ridiculous；扯淡/nonsense 无用/useless；没用/useless 吃惊/surprise；惊讶/amazing； 不敢相信/unbelievable 太棒了/ awesome；太好了/very good； terriﬁc 节省/save terrible；不好/not good；差/bad 傻/stupid；愚蠢/fool 太差/too bad；太烂/horrible 差劲/awful；糟糕/terrible save terrible stupid horrible awful recommend 推荐/recommend 没有/no；不/not 钱/money money 好玩/fun；有趣/interesting；搞笑/funny 漂亮/beautiful；可爱/lovely；美/beautiful beautiful 生活/life；人生/life life 喜欢/like like best performance co-training better clscl requires parallel dataset training testing processes. combined co-training transfer learning strategies. method achieved highest accuracy nlp&cc clsc task improved accuracy removing noise transferred samples avoid negative transfers. zhou built denoising auto-encoders independent views enhance robustness translation errors inputs. integrated bilingual embedding learning uniﬁed process achieved accuracy. approach reaches average accuracy ﬁxed parameters. table last shows best results approach. books music categories scl-om achieves best accuracy experimental results show approach competitive state-of-theart cross-language sentiment classiﬁcation. sensitivity analysis section analyze sensitivity important parameters keeping others ﬁxed number pivots dimensionality cross-lingual representation number pivots figure shows inﬂuence number pivots performance scl-om. plots show small number pivots capture signiﬁcant amount correspondence dimensionality cross-lingual representation figure shows inﬂuence dimensionality cross-lingual representation performance sclom. evaluate scl-om parameter varies shown figure average accuracies generally move upward increases. accuracy reaches peak value three categories accuracy declines increase furthermore gain insight results visualize small part pivots learned scl-om shown table table ﬁrst third columns examples pivot features source language chinese characters column column four corresponding mappings target language obtained approach. english word translation non-chinese readers. table one-to-many mappings based distributed representation words reasonable one-to-one mapping machine translation. paper propose novel structural correspondence learning method cross-lingual sentiment classiﬁcation one-to-many mappings. method employs distributed representation words build one-to-many mappings pivot features source language target language. rely parallel corpora. method evaluated nlp&cc cross-lingual sentiment analysis dataset employing english source language chinese target language. experimental results show approach competitive state-of-the-art methods cross-lingual sentiment classiﬁcation. however approach ignores polysemy one-to-many mappings. future explore method learning sense-speciﬁc word embedding.", "year": 2016}