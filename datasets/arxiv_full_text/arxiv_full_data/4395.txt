{"title": "Loss Max-Pooling for Semantic Image Segmentation", "tag": ["cs.CV", "stat.ML"], "abstract": "We introduce a novel loss max-pooling concept for handling imbalanced training data distributions, applicable as alternative loss layer in the context of deep neural networks for semantic image segmentation. Most real-world semantic segmentation datasets exhibit long tail distributions with few object categories comprising the majority of data and consequently biasing the classifiers towards them. Our method adaptively re-weights the contributions of each pixel based on their observed losses, targeting under-performing classification results as often encountered for under-represented object classes. Our approach goes beyond conventional cost-sensitive learning attempts through adaptive considerations that allow us to indirectly address both, inter- and intra-class imbalances. We provide a theoretical justification of our approach, complementary to experimental analyses on benchmark datasets. In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach.", "text": "datasets dominant portions data assigned majority classes rest belongs minority classes forming comparably under-represented categories. consequence observed classiﬁers trained without correction mechanisms tend biased towards majority classes inference. mitigate class-imbalance problem emphasize balanced compilations datasets ﬁrst place collecting samples approximately uniformly. datasets following approach imagenet caltech cifar training validation test sets roughly balanced w.r.t. instances class. another widely used procedure conducting over-sampling minority classes under-sampling majority classes compiling actual training data. approaches known change underlying data distributions result suboptimal exploitation available data increased computational effort and/or risk over-ﬁtting repeatedly visiting samples minority classes ways avoid over-ﬁtting). however efﬁciency straightforward application tasks like image-level classiﬁcation rendered sampling commonly-agreed practice. learning changes algorithmic behavior introducing classspeciﬁc weights often derived original data statistics. methods recently investigated deep learning following ideas previously applied shallow learning methods like random forests support vector machines many works statically-deﬁned cost matrices introduce additional parameter learning steps spatial arrangement strong correlations classes adjacent pixels costsensitive learning techniques preferred resampling methods performing dense pixel-wise classiﬁcation semantic segmentation tasks. however current trends semantic segmentation datasets show strong increase complexity minority classes added. introduce novel loss max-pooling concept handling imbalanced training data distributions applicable alternative loss layer context deep neural networks semantic image segmentation. real-world semantic segmentation datasets exhibit long tail distributions object categories comprising majority data consequently biasing classiﬁers towards them. method adaptively re-weights contributions pixel based observed losses targeting under-performing classiﬁcation results often encountered under-represented object classes. approach goes beyond conventional cost-sensitive learning attempts adaptive considerations allow indirectly address both interintra-class imbalances. provide theoretical justiﬁcation approach complementary experimental analyses benchmark datasets. experiments cityscapes pascal segmentation datasets consistently improved results demonstrating efﬁcacy approach. deep learning approaches undoubtedly matured facto standards many traditional computer vision tasks like image classiﬁcation object detection semantic segmentation. semantic segmentation aims assign categorical labels pixel image therefore constitutes basis high-level image understanding. recent works contributed progress research ﬁeld building upon convolutional neural networks enriching task-speciﬁc functionalities. extending cnns directly cast dense semantic label maps including contextual information reﬁning results graphical models impressive results many realworld applications standard benchmark datasets. works focused properly handle imbalanced class distributions often encountered semantic segmentation datasets within deep neural network training far. imbalanced refer contributions. work propose principled solution handling imbalanced datasets within deep learning approaches semantic segmentation tasks. speciﬁcally introduce novel loss function upper bounds traditional losses contribution pixel weighted equally. upper bound obtained generalized max-pooling operator acting pixel-loss level. maximization taken respect pixel weighting functions thus providing adaptive re-weighting contributions pixel based loss actually exhibit. general pixels incurring higher losses training weighted pixels lower loss thus indirectly compensating potential inter-class intraclass imbalances within dataset. latter imbalance approached dynamic re-weighting classagnostic i.e. taking advantage class label statistics like previous cost-sensitive learning approaches. generalized max-pooling operator hence loss instantiated different ways depending delimit space feasible pixel weighting functions. paper focus particular family weighting functions bounded p-norm ∞-norm study properties loss function exhibits under setting. moreover provide theoretical contribution deriving explicit characterization loss function special case enables computation gradients needed optimization deep neural network. additional complementary contribution describe performance-dependent sampling approach guiding minibatch compilation training. keeping track prediction performance training show relatively simple change sampling scheme allows faster reach convergence improved results. rest section discusses related works current semantic segmentation approaches typically deal class-imbalance problem provide compact description notation used rest paper. sect. describe depart standard uniform weighting scheme proposed adaptive pixel-loss max-pooling space weighting functions considering. sect. describe eventually solve novel loss function provide algorithmic details respectively. sect. assess performance contributions challenging cityscapes pascal segmentation benchmarks conclude sect. note extended version cvpr paper related works. many semantic segmentation works follow relatively simple cost-sensitive approach inverse frequency rebalancing scheme e.g. median frequency re-weighting approaches construct best-practice heuristics e.g. restricting number pixels updated backpropagation work suggests increasing minibatch size decreasing absolute number pixel positions updated. approach coined online bootstrapping introduced pixel losses sorted highest loss positions updated. similar idea termed online hard example mining found effective object detection high-loss bounding boxes retained non-maximum-suppression step preferably updated. work tackles class imbalance enforcing inter-cluster inter-class margins obtained employing quintuplet instance sampling triple-header hinge loss. another recent work proposed cost-sensitive neural network classiﬁcation jointly optimizing class dependent costs standard neural network parameters. work addresses problem contour detection convolutional neural networks combining speciﬁc loss contour versus non-contour samples conventional log-loss. separate though related research ﬁelds focus directly optimizing target measures like area curve intersection union average class work proposing nonlinear activation function computing norm projections lower layers allowing interpret max- averageroot-mean-squared-pooling operators special cases activation function. notation. paper denote space functions mapping elements elements natual number denotes usual product n-tuples elements sets real integer numbers respectively. operations deﬁned e.g. addition multiplication exponentiation etc. inherited pointwise application function additionally notations goal semantic image segmentation provide assignment class labels pixel image. input space task denoted corresponds possible images. sake simplicity assume images number pixels denote pixels within image number pixels i.e. |i|. output space acting pixel-losses. indeed recover conventional max-pooling operator special case probability distributions similarly standard loss regarded application average-pooling operator boiled special case proper choice space weighting functions. property loss max-pooling operator exhibits depends shape here restrict focus weighting functions p-norm ∞-norm upper bounded respectively bound p-norm n−/q corresponds p-norm uniform weighting function. instead left hyper-parameters. possible values chosen range indeed lower values would prevent uniform weighting function belonging higher values would equivalent putting intuitively user control pixel selectivity degree pooling operation changing indeed optimal weights general concentrated around single pixel uniformly spread across pixels hand allows conber pixels supported optimal weighting function. fig. show examples given synthetically-generated losses pixels left report optimal weightings different values weights peaked high losses moves towards constraint prevents selecting less pixels. hand weights tend become uniform approaches plot right ﬁxes varies weights tend uniformly support larger share pixels increase yielding uniform distribution possible network parameters ry×y loss function penalizing wrong image labelings regularizer. loss function commonly decomposes pixel-speciﬁc losses follows assigns pixel loss incurred predicting class instead rest paper assume non-negative bounded loss max-pooling. loss function deﬁned weights uniformly contribution pixel within image. effect choice bias learner towards elements dominant within image detriment elements occupying smaller portions image. order alleviate issue propose adaptively reweigh contribution pixel based actual loss observe. goal shift focus image parts loss higher retaining theoretical link loss solution propose upper bound constructed relaxing pixel weighting scheme. general terms design convex compact space weighting functions subsuming uniform weighting function i.e. parametrize loss function since uniform weighting function belongs maximize follows upper bounds i.e. consequently obtain upper bound replace title work ties loss max-pooling inspired observation loss proposed application generalized max-pooling operator segmentation task denoted corresponds pixelwise labelings classes labeling function mapping pixels classes i.e. standard setting. typical objective used train model parameters given training takes following form maximization problem concave explicit-form solution deﬁned provide details cases considering parametrization place clear intuitive meaning mentioned previous section. valid parametrizations satisfy case moving primal dual formulation legitimate formulations share optimal value. indeed slater’s condition applies strictly feasible). maximization deﬁnition dual norm p-norm correp− evaluated sponds q-norm scaled accordingly maximizes however computing solution straightforward recursive nature formula involving multiple variables reduce problem ﬁnding largest root single-variable function i\\jα complement. characterization solutions dual formulation terms roots proved correct appendix used derive theorem below provides explicit formula optimal weighting function maximization optimal dual variable theorem ˆyyqj ηˆyy) case solution takes form becomes subset pixels highest losses highest loss among remaining pixels optimal weighting function α∗}\\j probability distribution indeed available determine loss compute gradients respect segmentation model’s parameters report algorithm pseudo-code computation start sorting losses yields bijective function i{...n} satisfying case trivial since know last ranked pixels form corresponds highest loss among remaining pixels pixel left case walk losses ascending order stop soon index satisfying following conditions ﬁrst condition hence ˆyyq/m/q. indeed obtain line instead condition prop. appendix consequently gradient. order train semantic segmentation model need compute partial derivative exists almost everywhere given open neighborhood change. figure example optimal weightings pixels. left varying values right varying values losses synthetically generated sorted visualization purposes. note function also partial derivative technically exist. implementation notes. values close becomes arbitrarily large might cause numerical issues algorithm simple trick improve stability consists normalizing losses division maximum loss i.e. consider ˆyy) place ˆyy. modiﬁcation requires multiplying ˆyy) adjust objective optimal primal solution remains unaffected change. complimentary sampling strategy. addition main contribution described previous sections propose complimentary idea compile minibatches training. propose mixed sampling approach taking both uniform sampling training data’s global distribution current performance model account. surrogate latter keep track per-class intersection union scores training data conduct inverse sampling suggest preferably pick underperforming classes blending performance-based sampling idea uniform sampling ensures maintain stochastic behavior training therefore helps over-ﬁt particular classes. evaluated novel loss max-pooling approach cityscapes extended pascal semantic image segmentation datasets. particular performed extensive parameter sweep cityscapes assessing performance development different settings hyper-parameters fig. reported numbers intersectionover-union measures either averaged classes provided per-class basis. ment nccl multi-gpu support. particular using resnet- fully-convolutional atrous extensions base layers adding deeplab’s atrous spatial pyramid pooling finally apply upscaling using standard softmax loss baseline methods base base+ inverse median frequency weighting proposed loss max-pooling layer lmp. approaches base+ using complimentary sampling strategy minibatch compilation described previous section plain uniform sampling base leads similar results reported also report results loss plain uniform sampling save computation time provide conclusive parameter sensitivity study approach disabled both multi-scale input networks post processing conditional random ﬁelds consider features complementary method highly relevant improving overall performance case time hardware budgets permit however primary intention demonstrate effectiveness comparable settings baselines like base+. reported numbers plots obtained ﬁnetuning ms-coco pre-trained available download. order provide statistically signiﬁcant results provide mean standard deviations obtained averaging results certain steps last training iterations. report results obtained single opposed using ensemble cnns trained using stochastic gradient descent solver polynomial decay learning rate setting both decay rate momentum data augmentation random scale perturbations range patches cropped positions given aforementioned sampling strategy horizontal ﬂipping images. recently-released dataset contains street-level images taken daytime driving scenes major central european cities germany france switzerland. images captured high resolution divided training validation test sets holding images respectively. training validation data densely annotated ground truth label categories publicly available frequent classes account annotated pixel mass. following previous works table sensitivity analysis parameter ﬁxed valid pixels crop using efﬁcient tiling test time. numbers correspond results cityscapes validation indicated training iterations boldface underlined values correspondence best second best results respectively. bottom-most shows results baseline base+ efﬁcient tiling setting. report results obtained validation set. training minibatches comprising image crops size initial learning rate total number training iterations. tab. provide sensitivity analysis hyperparameter ﬁxing non-ignore per-crop pixels. large resolution images considerable number trainings employ different tiling strategies inference. numbers tab. obtained using so-called efﬁcient tiling strategy dividing validation images non-overlapping rectangular crops full image height. setting best result obtained closely followed seen increasing values show trend towards base+ results empirically conﬁrming theoretical underpinnings sect. ﬁxing conducted additional experiments selected correspond selecting least non-ignore per-crop pixels obtaining validation data respectively. finally locked running optimized tiling strategy validation consider pixel overlap tiles allowing improved context capturing. ﬁnal class label decisions ﬁrst half overlap area exclusively taken left tile second half provided right tile respectively. resulting scores listed tab. demonstrating improved results base base+ related approaches like deeplabv deeper resnet ondemonstrate impact approach underrepresented classes provide plot showing per-class performance gain absolute number pixels given object category fig. positive values indicate improvements class labels attached indicate increasing object class pixel label volume categories left right. e.g. motorcycle underrepresented road present. plot conﬁrms naturally improves underrepresented object classes without accessing underlying class statistics. another experiment compares base order match result e.g. improve worst categories worst categories each convincing argument lmp. additionally illustrate qualitative evolution semantic segmentation training images fig. rows show segmentations obtained training conventional log-loss base+ even rows show ones obtained using loss max-pooling increasing number iterations. starts improving under-represented classes sooner standard log-loss finally also report individual perclass scores tab. both base+ corresponding setting tab. additionally assess quality novel pascal segmentation benchmark dataset comprising object classes background class. images dataset considerably smaller ones cityscapes dataset increased minibatch size using training images testing done validation containing images. total training iterations ﬁxed parameters account valid pixels crop lmp. inference images evaluated full scale i.e. special tiling mechanism needed. report mean scores tab. list results comparable state-of-the-art approaches next ours. obtain considerable relative improvement base+ well comparable baselines approach compares slightly worse deeplabv’s strongest variant however additionally uses multi-scale inputs reﬁnements respectively) come increased computational costs. additionally mentioned above consider techniques complementary contributions plan integrate future works. ﬁnally notice also dataset loss alone without complimentary sampling strategy yields consistent improvements base base+. work introduced novel approach tackle imbalances training data distributions occur under-represented classes might occur also within class proposed loss function performs generalized max-pooling pixelspeciﬁc losses. loss upper bounds traditional gives equal weight pixel contribution implicitly introduces adaptive weighting scheme bifigure evolution semantic segmentation images training. left pairs original images ground-truth segmentations images show semantic segmentations obtained standard log-loss base+ loss max-pooling training iterations. ases learner towards under-performing image parts. space weighting functions involved maximization shaped enforce desired properties. paper focused particular family weighting functions enabling control pixel selectivity extent supported pixels. derived explicit formulas outcome pooling operation family pixel weighting functions thus enabling computation gradients training deep neural networks. experimentally validated effectiveness loss function showed consistently improved results standard benchmark datasets semantic segmentation. proofs auxiliary results proof thm. max{α ηˆyy) start proving deﬁnition ηˆyy) implies proposition consequently therefore deﬁnition ηˆyy) implies ηˆyy) since hold proposition therefore thus proved obtain given theorem solving equation variable replaced equivalent equation admits unique solution proposition accordingly then proposition |ˆyy α∗|+ solution derive assume contradiction strict inequality holds terms ˆyy. w∗}. note otherwise holds yielding contradiction upper bounded follows deﬁnition additionally otherwise holds contradicting necessarily lower bounded follows deﬁnition substituting back obtain proposition solution satisﬁes max{ˆyy satisﬁes solution proof. solution trivially satisﬁed. otherwise satisﬁed proposition max{ˆyy satisﬁes solution proposition point satisfying follows proposition solution exists differentiable. however least solution exist minimization problem admits ﬁnite solution. point non-differentiable ˆyy. therefore solution proposition ˆyy. solution satisﬁes proof. solution point differentiable karush-kuhntucker necessary conditions optimality satisﬁed. speciﬁcally exists satisfying optimality satisﬁed. since convex conditions also sufﬁcient therefore solution proposition max{α ηˆyy) proof. solution proposition m−/qˆyy root proposition positive elements. follows imply accordingly always holds since take min{ˆyy ηˆyy) necessarily non-empty contains pixels loss last equality follows fact however implies deﬁnition yields contradiction follows deﬁnition hence proposition minimizer positive elements. proof. result trivially true. otherwise assume contradiction exist least positive elements indices then dual objective yields τˆyyj however value cannot attained primal formulation exist elements weight remaining element would weight exceeding m)/p implies contradicts strong duality implied satisfaction slater’s condition proposition proof. hypothesis implies hence write expression positive moreover since element deﬁnition therefore nonnegative. follows proposition proof. proposition result follows contrapositive proposition holds used fact constant follows ηˆyy) theorem trivially holds sets ηˆyy) empty. deﬁnition hence ηˆyy) consequently proposition proposition ηˆyy) derivation gradient", "year": 2017}