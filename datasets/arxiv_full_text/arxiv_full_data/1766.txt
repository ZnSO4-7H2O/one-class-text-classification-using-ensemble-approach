{"title": "Text-mining the NeuroSynth corpus using Deep Boltzmann Machines", "tag": ["cs.LG", "cs.CL", "q-bio.NC", "stat.ML"], "abstract": "Large-scale automated meta-analysis of neuroimaging data has recently established itself as an important tool in advancing our understanding of human brain function. This research has been pioneered by NeuroSynth, a database collecting both brain activation coordinates and associated text across a large cohort of neuroimaging research papers. One of the fundamental aspects of such meta-analysis is text-mining. To date, word counts and more sophisticated methods such as Latent Dirichlet Allocation have been proposed. In this work we present an unsupervised study of the NeuroSynth text corpus using Deep Boltzmann Machines (DBMs). The use of DBMs yields several advantages over the aforementioned methods, principal among which is the fact that it yields both word and document embeddings in a high-dimensional vector space. Such embeddings serve to facilitate the use of traditional machine learning techniques on the text corpus. The proposed DBM model is shown to learn embeddings with a clear semantic structure.", "text": "able learn pre-speciﬁed number latent topics generated observed text. work present related approach based deep boltzmann machines motivation behind dbms alternative text-mining approaches twofold. first restricted boltzmann machines special case dbms recently shown outperform terms generalization performance hypothesized result rbms learning useful internal representations text corpus presence additional hidden layers dbms would serve facilitate learning internal representations. second advantage using dbms models yield embedding words documents high-dimensional vector space. embeddings crucial component modern natural language processing systems easily incorporated traditional machine learning pipelines. furthermore word embeddings employed learn joint models across text associated activation coordinates ultimate objective meta-analysis studies work demonstrate dbms effectively employed learn distribution neurosynth text corpus. further proposed model able learn embeddings individual words well entire documents. motivation table shows clusters obtained k-means clustering applied word embeddings obtained model. clusters display clear semantic context. section outline models employed work. begin introducing restricted boltzmann machines serve building blocks deeper architectures considered work. extensions rbms directly model word counts discussed considering deep boltzmann machines restricted boltzmann machines rbms class undirected graphical models specify probability distribution observed binary variables binary hidden variables formally rbms energy based models bipartite graph structure across visible hidden variables. structure imposed order facilitate learning models parameters discuss below. abstract—large-scale automated meta-analysis neuroimaging data recently established important tool advancing understanding human brain function. research pioneered neurosynth database collecting brain activation coordinates associated text across large cohort neuroimaging research papers. fundamental aspects meta-analysis text-mining. date word counts sophisticated methods latent dirichlet allocation proposed. work present unsupervised study neurosynth text corpus using deep boltzmann machines dbms yields several advantages aforementioned methods principal among fact yields word document embeddings high-dimensional vector space. embeddings serve facilitate traditional machine learning techniques text corpus. proposed model shown learn embeddings clear semantic structure. study human brain using functional magnetic resonance imaging advanced rapidly last decades. provided signiﬁcant insights relationship architecture function human brain. reﬂected number published studies grown exponentially time. consequently major challenge scientiﬁc community involves efﬁcient integration analysis knowledge across wide corpus studies challenge inspired attempts automatically aggregate analyze knowledge across ﬁeld fmri. particular neurosynth meta-analysis database collecting brain activation coordinates corresponding text across range thousand studies. important applications analysis interpretation fmri data facilitating quantitative reverse inference automated extraction information collection published neuroimaging studies based fundamental pillars; ﬁrst involves generating detailed statistical maps. work focus second pillar; extraction analysis semantic topics text methods look employ text-mining methodologies discover latent topics brain imaging literature. approaches subsequently combined activation coordinates examine underlying mapping cognitive neural states. associated vocabulary memory retrieval encoding hippocampus hippocampal episodic items recall memories recollection item familiarity autobiographical language semantic words speech word reading verbal phonological lexical linguistic naming ﬂuency verbs english adults children years older young development adolescents developmental aging sleep adult late younger blind childhood hearing adolescence emotional amygdala social negative faces face emotion neutral affective facial anxiety fear expressions regulation emotions valence personality arousal fearful trait threat happy mood empathy moral person traits communication patients controls schizophrenia disorder deﬁcits disease abnormalities symptoms impaired impairment adhd alterations dysfunction abnormal atrophy patient ptsd severity damage bipolar lesions impairments deﬁcit depressive mild syndrome symptom elderly dementia epilepsy poor pathophysiology total number words document. standard learning proceeds contrastive divergence. models interpreted learning distribution word histograms documents. deep boltzmann machines dbms extensions rbms allow multiple layers hidden variables. models capability learning internal representations data increasing complex throughout work consider two-layer multinomial visible variables binary hidden variables. model associated following energy function parameters wish estimate. probability given conﬁguration subsequently deﬁned normalizing constant. furthermore likelihood observation obtained summing binary hidden units write denote ﬁrst second layer binary hidden variables respectively. similarly parameters represent symmetric interaction terms between visible-to-hidden hidden-to-hidden variables. analogous equation probability assigned visible vector deﬁned parameter learning rbms typically achieved performing gradient descent log-likelihood observed data. equation training data log-likelihood negative term derivate respect positive term corresponds expectation data dependent distribution hidden variables easily computed bipartite structure rbms. however derivative negative term involves expectation distribution visible hidden units proposed model intractable. expectation typically approximated looking sample distribution using mcmc. starting visible units gibbs sampling applied times order obtain unbiased sample gradient procedure known contrastive divergence letting recovers maximum likelihood however practice shown empirically setting performs well. replicated softmax model aforementioned model employed objective learn probability binary visible variables. context modeling documents possible treat occurrence words speciﬁc locations text binary variables. case observations correspond binary incidence matrix }n×d vnd= word document takes value. approach able model order words explosion number parameters. replicated softmax takes parsimonious setting visible units correspond vector words counts document. note corresponds size vocabulary. furthermore bipartite across layers conditional distributions layers computed closed form. allows persistent markov chains estimate intractable model expectations. naive meanﬁeld variational inference used approximate datadependent expectations. details refer readers practice appropriate initialization parameters crucial success deep models. propose greedy layer-by-layer pretraining algorithm dbms. involves iteratively stacking rbms small caveat bottom contributions bottom layer doubled pretraining. model selection selecting number hidden units within layer non-trivial task. difﬁculty approach arises need estimate partition function entire model. depends parameters well number hidden units must calculated order perform model comparison. importance sampling often employed estimate properties distributions known normalizing constant using samples known distribution. however importance sampling yield reliable estimate known proposal distribution must resemble target distribution. context high-dimensional rbms ﬁnding proposal distribution challenging. order address challenge propose annealed importance sampling sequence auxiliary proposal distributions deﬁned iteratively approximate target distribution. work greedy layer-by-layer approach taken select bottom layer trained using range hidden units. architecture yielded maximum likelihood across held-out validation selected. hidden activation subsequently provided input layer process repeated. neurosynth text corpus employed work. original corpus contains word frequencies entire text publication work publication abstracts employed. served reduce range vocabulary employed motivated belief much semantic structure present publication would also present corresponding abstract. abstracts collected publications using pubmed resulting mean document length words standard preprocessing applied text corpus. stop words removed well words occur sufﬁcient frequency resulted vocabulary approximately thousand words words occurred frequently retained dataset split training consisting documents test remaining documents. one-step reconstruction memory working recall performance retrieval verbal load semantic recognition task social emotion emotional regions brain affective gray traits amygdala social facial faces face emotional processing regions functional brain cortex patients disorder adhd abnormalities controls brain matter alterations structural network default connectivity brain regions cognitive functional mode activity cortex further figure shows visualization word embeddings using t-sne three sections embedding highlighted. regions showcase embeddings terms relating emotion memory respectively. important note relevant brain regions contained sections meanwhile region contains terms relating development. finally alternative manner demonstrating model obtained good estimate probability distribution consider one-step reconstructions. examples provided table input words employed obtain distribution hidden units level. distribution employed obtain distribution words. words highest probability mass shown right column. two-layer employed consisting visible layer multinomial visible units followed binary hidden layers units each. pretraining model selection rbms trained using cd−. addition dropout employed form regularization hidden units retained probability architecture selected minimizing negative log-likelihood held validation dataset greedy manner described previously. brieﬂy employed estimate partition function rbm. five thousand auxiliary distributions employed estimates averaged hundred runs. finally initialized weights learnt pretraining trained described proposed model used obtain word well document embeddings high-dimensional vector space. remainder section study word document embeddings obtained proposed model. document embeddings obtained analogous fashion providing entire document word vector input dbm. clustering document embeddings leveraging activation maps within neurosynth database able study activations associated cluster. figure shows subset embeddings obtained using t-sne document embeddings. before kmeans clustering employed cluster documents according associated high-dimensional representations possible study reported functional activations publications within given cluster. following achieved convolving activations within cluster gaussian kernel. resulted mean activation documents within given cluster. peak activations together frequently occurring words shown clusters figure activation maps highlight functional networks regions example clusters four identify pain motor regions respectively. clusters also appear identify pathologies. example cluster appears related cognitive impairment atrophy. furthermore interesting note spatially adjacent clusters share similarities. example clusters show frontal activation. left panel shows result applying t-sne word embeddings obtained model. three regions highlighted fig. shown greater detail remaining three panels. seen regions correspond emotion memory related terms respectively region contains terms associated aging development. subset dimensional embedding obtained applying t-sne document embedding. iii) activation maps shown several highlighted clusters shown together frequently occurring terms paper demonstrated dbms modeling text corpus composed abstracts neuroscientiﬁc publications. proposed model able yield vector representation individual words well entire documents. representations advantageous many reasons example employed cluster words documents. further combining abstracts neurosynth corpus able study whether activation maps associated cluster. exploratory results presented work future work look simultaneously model text activations thereby facilitating formal inference. exciting application would leverage document embeddings inform novel machine learning applications neuroscience recently proposed automated neuroscientist abraham pedregosa eickenberg gervais muller kossaiﬁ gramfort thirion varoquaux. machine learning neuroimaging scikit-learn. arxiv preprint arxiv. lorenz monti violante anagnostopoulos faisal montana leech. automatic neuroscientist framework optimizing experimental design closed-loop realtime fmri. neuroimage", "year": 2016}