{"title": "Unsupervised Domain Adaptation by Backpropagation", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary).  As the training progresses, the approach promotes the emergence of \"deep\" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation.  Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.", "text": "test time. particularly important example synthetic semi-synthetic training data come abundance fully labeled inevitably distribution different real data learning discriminative classiﬁer predictor presence shift training test distributions known domain adaptation number approaches domain adaptation suggested context shallow learning e.g. situation data representation/features given ﬁxed. proposed approaches build mappings source target domains classiﬁer learned source domain also applied target domain composed learned mapping domains. appeal domain adaptation approaches ability learn mapping between domains situation target domain data either fully unlabeled labeled samples below focus harder unsupervised case although proposed approach generalized semi-supervised case rather straightforwardly. unlike previous papers domain adaptation worked ﬁxed feature representations focus combining domain adaptation deep feature learning within training process goal embed domain adaptation process learning representation ﬁnal classiﬁcation decisions made based features discriminative invariant change domains i.e. similar distributions source target domains. obtained feed-forward network applicable target domain without hindered shift domains. thus focus learning features combine discriminativeness domain-invariance. achieved jointly optimizing underlying features well discriminative classiﬁers operating features label predictor predicts class labels used training test time top-performing deep architectures trained massive amounts labeled data. absence labeled data certain task domain adaptation often provides attractive option given labeled data similar nature different domain available. here propose approach domain adaptation deep architectures trained large amount labeled data source domain large amount unlabeled data target domain training progresses approach promotes emergence deep features discriminative main learning task source domain invariant respect shift domains. show adaptation behaviour achieved alfeed-forward model augmenting standard layers simple gradient reversal layer. resulting augmented architecture trained using standard backpropagation. overall approach implemented little effort using deep-learning packages. method performs well series image classiﬁcation experiments achieving adaptation effect presence domain shifts outperforming previous state-ofthe-art ofﬁce datasets. introduction deep feed-forward architectures brought impressive advances state-of-the-art across wide variety machine-learning tasks applications. moment however leaps performance come large amount labeled training data available. time problems lacking labeled data still possible obtain training sets enough training large-scale deep models suffer shift data distribution actual data encountered domain classiﬁer discriminates source target domains training. parameters classiﬁers optimized order minimize error training parameters underlying deep feature mapping optimized order minimize loss label classiﬁer maximize loss domain classiﬁer. latter encourages domain-invariant features emerge course optimization. crucially show three training processes embedded appropriately composed deep feedforward network uses standard layers loss functions trained using standard backpropagation algorithms based stochastic gradient descent modiﬁcations approach generic used domain adaptation existing feed-forward architecture trainable backpropagation. practice non-standard component proposed architecture rather trivial gradient reversal layer leaves input unchanged forward propagation reverses gradient multiplying negative scalar backpropagation. below detail proposed approach domain adaptation deep architectures present results traditional deep learning image datasets svhn well office benchmarks proposed method considerably improves previous state-of-the-art accuracy. related work large number domain adaptation methods proposed recent years focus related ones. multiple methods perform unsupervised domain adaptation matching feature distributions source target domains. approaches perform reweighing selecting samples source domain others seek explicit feature space transformation would source distribution target ones important aspect distribution matching approach similarity distributions measured. here popular choice matching distribution means kernel-reproducing hilbert space whereas principal axes associated distributions. approach also attempts match feature space distributions however accomplished modifying feature representation rather reweighing geometric transformation. also method uses rather different measure disparity distributions based separability deep discriminatively-trained classiﬁer. several approaches perform gradual transition source target domain gradual change training distribution. among methods deep layerwise training sequence deep autoencoders gradually replacing source-domain samples target-domain samples. improves similar approach simply trains single deep autoencoder domains. approaches actual classiﬁer/predictor learned separate step using feature representation learned autoencoder. contrast approach performs feature learning domain adaptation classiﬁer learning jointly uniﬁed architecture using single learning algorithm therefore argue approach simpler method also achieves considerably better results popular office benchmark. approaches perform unsupervised domain adaptation approaches perform supervised domain adaptation exploiting labeled data target domain. context deep feed-forward architectures data used ﬁne-tune network trained source domain approach require labeled target-domain data. time easily incorporate data available. idea related described goal quite different measure minimize discrepancy distribution training data distribution synthesized data similar architecture measures minimizes discrepancy feature distributions domains. finally recent concurrent report also focuses domain adaptation feed-forward networks. techniques measures minimizes distance data means across domains. approach regarded ﬁrst-order approximation approach seeks tighter alignment distributions. deep domain adaptation model detail proposed model domain adaptation. assume model works input samples input space certain labels label space below assume classiﬁcation problems ﬁnite however approach generic handle output label space deep feedfigure proposed architecture includes deep feature extractor deep label predictor together form standard feed-forward architecture. unsupervised domain adaptation achieved adding domain classiﬁer connected feature extractor gradient reversal layer multiplies gradient certain negative constant backpropagationbased training. otherwise training proceeds standard minimizes label prediction loss domain classiﬁcation loss gradient reversal ensures feature distributions domains made similar thus resulting domain-invariant features. forward models handle. assume exist distributions referred source distribution target distribution distributions assumed complex unknown furthermore similar different ultimate goal able predict labels given input target distribution. training time access large training samples source target domains distributed according marginal distributions denote binary variable i-th example indicates whether come source distribution di=) target distribution di=). examples source distribution corresponding labels known training time. examples target domains know labels training time want predict labels test time. deﬁne deep feed-forward architecture input predicts label domain label decompose mapping three parts. assume input ﬁrst mapped mapping d-dimensional feature vector feature mapping also include several feed-forward layers denote vector parameters layers mapping i.e. then feature vector mapped mapping label denote parameters mapping finally feature vector mapped domain label mapping parameters learning stage minimize label prediction loss annotated part training parameters feature extractor label predictor thus optimized order minimize empirical loss source domain samples. ensures discriminativeness features overall good prediction performance combination feature extractor label predictor source domain. make features domain-invariant. want make distributions x∼s} similar. covariate shift assumption would make label prediction accuracy target domain source domain measuring dissimilarity distributions however non-trivial given high-dimensional distributions constantly changing learning progresses. estimate dissimilarity look loss domain classiﬁer provided parameters domain classiﬁer trained discriminate feature distributions optimal way. observation leads idea. training time order obtain domain-invariant features seek parameters feature mapping maximize loss domain classiﬁer simultaneously seeking parameters domain classiﬁer minimize loss domain classiﬁer. addition seek minimize loss label predictor. here loss label prediction loss domain classiﬁcation denote corresponding loss functions evaluated i-th training example. based idea seeking parameters deliver saddle point functional saddle point parameters domain classiﬁer minimize domain classiﬁcation loss minus sign) parameters label predictor minimize label prediction loss. feature mapping parameters minimize label prediction loss maximizing domain classiﬁcation loss parameter controls trade-off objectives shape features learning. below demonstrate standard stochastic gradient solvers adapted search saddle point learning rate updates similar stochastic gradient descent updates feed-forward deep model comprises feature extractor label predictor domain classiﬁer. difference factor although direct implementation possible highly desirable reduce updates form since main learning algorithm implemented packages deep learning. fortunately reduction accomplished introducing special gradient reversal layer deﬁned follows. gradient reversal layer parameters associated forward propagation acts identity transform. backpropagation though takes gradient subsequent level multiplies passes preceding layer. implementing layer using existing object-oriented packages deep learning simple deﬁning procedures forwardprop backprop parameter update trivial. deﬁned inserted feature extractor domain classiﬁer resulting architecture depicted figure backpropagation process passes partial derivatives loss downstream w.r.t. layer parameters upstream multieffectively replaced plied i.e. therefore running resulting model implements updates converges saddle point mathematically formally treat gradient reversal layer pseudo-function deﬁned equations describing forwardbackpropagation behaviour running updates implemented doing leads emergence features domain-invariant discriminative time. learning label predictor used predict labels samples target domain experiments perform extensive evaluation proposed approach number popular image datasets modiﬁcations. include large-scale datasets small images popular deep learning methods office datasets facto standard domain adaptation computer vision much fewer images. baselines. bulk experiments following baselines evaluated. source-only model trained withconsideration target-domain data train-ontarget model trained target domain class labels revealed. model serves upper bound methods assuming target data abundant shift domains considerable. addition compare approach recently proposed unsupervised method based subspace alignment simple setup test datasets also shown perform well experimental comparisons shallow methods. boost performance baseline pick important free parameter range test performance target domain maximized. apply setting train source-only model consider activations last hidden layer label predictor descriptors/features learn mapping source target domains since baseline requires train classiﬁer adapting features order compared settings equal footing retrain last layer label predictor using standard linear four considered methods office dataset directly compare performance full network recent approaches using previously published results. architectures. general compose feature extractor three convolutional layers picking exact conﬁgurations previous works. give exact architectures appendix domain adaptator stick three fully connected layers except mnist used simpler arsimple learning procedure outlined rederived/generalized along lines suggested relation h∆h-distance section give brief analysis method terms h∆h-distance widely used theory nonconservative domain adaptation. formally deﬁnes discrepancy distance distributions w.r.t. hypothesis using notion obtain probabilistic bound performance classiﬁer evaluated target domain given performance source domain source target distributions respectively depend particular consider ﬁxed representation space produced feature extractor family label predictors assume family domain classiﬁers rich enough contain symmetric difference hypothesis unrealistic assumption freedom pick whichever want. example architecture domain discriminator layerby-layer concatenation replicas label predictor followed layer non-linear perceptron aimed learn xor-function. given assumption holds easily show training closely related estimation dhp∆hp indeed pf∼s pf∼t maximized optimal thus optimal discriminator gives upper bound dhp∆hp time backpropagation reversed gradient changes representation space table classiﬁcation accuracies digit image classiﬁcations different source target domains. mnist-m corresponds difference-blended digits non-uniform background. ﬁrst corresponds lower performance bound last corresponds training target domain data known class labels methods show much lower upper bounds covered cases approach outperforms considerably covers portion gap. chitecture speed experiments. loss functions logistic regression loss binomial cross-entropy respectively. training procedure. model trained sized batches. images preprocessed mean subtraction. half batch populated samples source domain rest comprised target domain order suppress noisy signal domain classiﬁer early stages training procedure instead ﬁxing adaptation factor gradually change using following schedule experiments details training found appendix visualizations. t-sne projection visualize feature distributions different points network color-coding domains observe strong correspondence success adaptation terms classiﬁcation accuracy target domain overlap domain distributions visualizations. choosing meta-parameters. general good unsupervised methods provide ways metaparameters unsupervised i.e. without referring labeled data target domain. method assess performance whole system observing test error source domain domain classiﬁer error. general observed good correspondence success adaptation errors addition layer domain adaptator attached picked computing difference means suggested results discuss experimental settings results. case train source dataset test different target domain dataset considerable shifts between domains results summarized table table mnist mnist-m. ﬁrst experiment deals mnist dataset order obtain target domain blend digoriginal patches randomly extracted color photos bsds operation formally deﬁned images ijk| coordinates pixel channel index. words output sample produced taking patch photo infigure effect adaptation distribution extracted features ﬁgure shows t-sne visualizations cnn’s activations case adaptation performed case adaptation procedure incorporated training. blue points correspond source domain examples ones correspond target domain. cases adaptation method makes distributions features much closer. verting pixels positions corresponding pixels digit. human classiﬁcation task becomes slightly harder compared original dataset whereas trained mnist domain quite distinct background strokes longer constant. consequently source-only model performs poorly. approach succeeded aligning feature distributions successful adaptation results time improvement source-only model achieved subspace alignment quite modest thus highlighting difﬁculty adaptation task. synthetic numbers svhn. address common scenario training synthetic data testing real data street-view house number dataset svhn target domain synthetic digits source. latter consists images generated windows fonts varying text positioning orientation background stroke colors amount blur. degrees variation chosen manually simulate svhn however datasets still rather distinct biggest difference structured clutter background svhn images. proposed backpropagation-based technique works well covering thirds training source data training target domain data known target labels. contrast result signiﬁcant improvement classiﬁcation accuracy thus highlighting adaptation task even challenging case mnist experiment. mnist svhn. experiment increase distributions test mnist svhn signiﬁcantly different appearance. training svhn even without adaptation challenging classiﬁcation error stays high ﬁrst epochs. order avoid ending poor local minimum therefore learning rate annealing here. obviously directions equally difﬁcult. svhn diverse model trained svhn exmains amazon dslr webcam. unlike previously discussed datasets office rather small-scale labeled images spread across different categories largest domain. amount available data crucial successful training deep model hence opted ﬁne-tuning pre-trained imagenet done recent works make approach comparable using exactly network architecture replacing domain mean-based regularization domain classiﬁer. following previous works evaluate method using random splits transfer tasks commonly used evaluation. training protocol close number labeled source-domain images category. unlike works similarly e.g. dlid whole unlabeled target domain under transductive setting method able improve previously-reported state-of-the-art accuracy unsupervised adaptation considerably especially challenging amazon webcam scenario discussion proposed approach unsupervised domain adaptation deep feed-forward architectures allows large-scale training based large amount annotated data source domain large amount unannotated data target domain. similarly many previous shallow deep techniques adaptation achieved aligning distributions features across domains. however unlike previous approaches alignment accomplished standard backpropagation training. approach therefore rather scalable implemented using deep learning package. plan release source code gradient reversal layer along usage examples extension caffe evaluation larger-scale tasks semisupervised settings constitutes future work. also interesting whether approach beneﬁt good initialization feature extractor. this natural choice would deep autoencoder/deconvolution network trained domains vein effectively using initialization method. figure semi-supervised domain adaptation trafﬁc signs. labeled target domain data shown method achieves signiﬁcantly lower error model trained target domain data source domain data only. pected generic perform reasonably mnist dataset. this indeed turns case supported appearance feature distributions. observe quite strong separation domains feed trained solely mnist whereas svhn-trained network features much intermixed. difference probably explains method succeeded improving performance adaptation svhn mnist scenario opposite direction unsupervised adaptation mnist svhn gives failure example approach synthetic signs gtsrb. overall setting similar numbers svhn experiment except distribution features complex signiﬁcantly larger number classes source domain obtained synthetic images simulating various photoshooting conditions. again method achieves sensible increase performance proving suitability synthetic-to-real data adaptation. additional experiment also evaluate proposed algorithm semi-supervised domain adaptation i.e. additionally provided small amount labeled target data. purpose split gtsrb train validation validation part used solely evaluation participate adaptation. training procedure changes slightly label predictor exposed target data. figure shows change validation error throughout training. graph clearly suggests method used semi-supervised setting thorough veriﬁcation semi-supervised setting left future work. ofﬁce dataset. ﬁnally evaluate method office dataset collection three distinct doexists alternative construction leads updates rather using gradient reversal layer construction introduces different loss functions domain classiﬁer. minimization ﬁrst domain loss lead better domain discrimination second domain loss minimized domains distinct. stochastic updates deﬁned thus different parameters participate optimization different losses framework gradient reversal layer constitutes special case corresponding pair domain losses however pairs loss functions used. example would binomial cross-entropy indicates domain indices output predictor. case adversarial loss easily obtained swapping domain labels i.e. ld+. particular pair potential advantage producing stronger gradients early learning stages domains quite dissimilar. experiments however observe signiﬁcant improvement resulting choice losses. baktashmotlagh mahsa harandi mehrtash tafazzoli lovell brian salzmann mathieu. unsupervised domain adaptation domain invariant projection. iccv borgwardt karsten gretton arthur rasch malte kriegel hans-peter sch¨olkopf bernhard smola alexander integrating structured biological data kernel maximum mean discrepancy. ismb donahue jeff yangqing vinyals oriol hoffman judy zhang ning tzeng eric darrell trevor. decaf deep convolutional activation feature generic visual recognition shimodaira hidetoshi. improving predictive inference under covariate shift weighting log-likelihood function. journal statistical planning inference october srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research v´azquez david l´opez antonio manuel mar´ın javier ponsa daniel gomez david ger´onimo. virtual real world adaptationfor pedestrian detection. ieee trans. pattern anal. mach. intell. gong boqing grauman kristen fei. connecting dots landmarks discriminatively learning domain-invariant features unsupervised domain adaptation. icml goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. nips hoffman judy tzeng eric donahue jeff yangqing saenko kate darrell trevor. one-shot adaptation supervised deep convolutional models. corr abs/. yangqing shelhamer evan donahue jeff karayev sergey long jonathan girshick ross guadarrama sergio darrell trevor. caffe convolutional architecture fast feature embedding. corr abs/. netzer yuval wang coates adam bissacco alessandro andrew reading dignatural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning", "year": 2014}