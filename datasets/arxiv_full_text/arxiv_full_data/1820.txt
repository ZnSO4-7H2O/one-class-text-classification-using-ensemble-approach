{"title": "On-the-fly Operation Batching in Dynamic Computation Graphs", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing toolkits - both static and dynamic - require that the developer organize the computations into the batches necessary for exploiting high-performance algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations. On a variety of tasks, we obtain throughput similar to that obtained with manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually.", "text": "dynamic neural network toolkits pytorch dynet chainer offer ﬂexibility implementing models cope data varying dimensions structure relative toolkits operate statically declared computations however existing toolkits—both static dynamic—require developer organize computations batches necessary exploiting high-performance algorithms hardware. batching task generally difﬁcult becomes major hurdle architectures become complex. paper present algorithm implementation dynet toolkit automatically batching operations. developers simply write minibatch computations aggregations single instance computations batching algorithm seamlessly executes them using computationally efﬁcient batched operations. variety tasks obtain throughput similar obtained manual batches well comparable speedups singleinstance learning architectures impractical batch manually. modern cpus gpus evaluate batches arithmetic operations signiﬁcantly faster sequential evaluation operations. example performing elementwise operations takes nearly amount time whether operating tens thousands elements multiplying hundred different vectors matrix signiﬁcantly slower executing single matrix–matrix product using optimized gemm implementation either cpu. thus careful grouping operations batches execute efﬁciently parallel crucial making available hardware resources. today developers write code train neural networks responsible crafting batch handling hand. cases easy inputs outputs naturally represented ﬁxed sized tensors computations required process instance instance-invariant expressible standard operations tensors suitably ﬂexible tensor library figure computation graphs computing loss minibatch three training instances consisting sequence input vectors paired ﬁxed sized output vector. left conceptual computation graph shows operations associated computing losses individually sequence aggregating them. computation executed right-hand computation graph aggregates inputs order make better modern processors. comes price complexity—the variable length sequences requires padding masking operations. user specify conceptual computation left framework take care efﬁcient execution. provides efﬁcient implementations higher-order generalizations low-order operations makes manual batching straightforward. example adding leading trailing dimension tensors representing inputs outputs multiple instances straightforwardly represented single data structure. words scenario developer conceives writes code computation individual instance packs several instances tensor minibatch library handles executing efﬁciently parallel. unfortunately idealized scenario breaks working complex architectures. deep learning increasingly applied problems whose inputs outputs intermediate representations easily ﬁxed sized tensors. example images vary size sequences length; data structured trees graphs model select computation conditional input cases desired computation easy enough write single instance organizing computational operations make optimally efﬁcient hardware nontrivial. indeed many papers operate data structures complicated sequences avoided batching entirely fact last year published work recursive neural networks appears used single instance training. premise work operation batching responsibility user instead service provided framework. user responsible specifying large enough computation batching possible framework take care lower-level details operation batching much like optimizing compilers optimizers interpreted languages take large step towards goal introducing efﬁcient algorithm—and corresponding implementation—for automatic batching dynamically declared computation graphs. method relies separating graph construction execution using operator overloading lazy evaluation separation place propose fast batching heuristic performed real time training instance graph construction execution extend dynet toolkit capability. end-user’s computation graphs data structures used structure evaluation expressions reverse mode automatic differentiation compute derivatives broadly learning frameworks strategies construct these static dynamic. static toolkits tensorﬂow computation graph deﬁned compiled examples graph. contrast dynamic toolkits chainer pytorch construct computation graph training instance forward computation executed. dynamic declaration means minibatch computational architecture user still responsible batching operations themselves. perspective result simple mechanism exploiting efﬁcient data-parallel algorithms networks would cumbersome batch hand. user simply deﬁnes computation independently instance batch framework takes care rest. experiments show algorithm compares favorably manually batched code signiﬁcant speed improvements possible architectures straightforward manual batching design obtain better performance tensorflow fold alternative framework built simulate dynamic graph deﬁnition automatic batching tensorflow illustrate challenges batching consider problem predicting real-valued vector conditional sequence input vectors assume input sequence vectors read sequentially ﬁnal state used make prediction; training loss euclidean distance prediction target. compare algorithms computing code naïve developer-friendly reﬂects conceives batch loss computation computationally efﬁcient— conceptually complex—version batches computations executed parallel across sequences naïve batched implementation left part figure shows computations must executed compute losses associated three training instances implemented naïvely. pseudo-code constructing graph rnns left using dynamic declaration framework follows code simple understand uses basic control present programming language simple mathematical operations. unfortunately executing generally quite inefﬁcient since resulting computation graph operation performed sequentially without exploiting fact similar operations performed across training instances. efﬁcient manually batched implementation make good efﬁcient data-parallel algorithms hardware necessary batch operations sequences processed parallel. standard achieve aggregating inputs outputs altering code follows code computes value naïve implementation efﬁciently signiﬁcantly complicated. sequences processed rnns generally different lengths necessary input representation dummy values also mask resulting losses right times. techniques part inventory skills good engineer increase difﬁculty implementation probability bugs present code. implementation comparison naïve algorithm advantages manual batching. first easy implement conceive model implemented errors padding masking batching avoided. second naïve algorithm aggregates single instance loss whereas manual batching efforts generally problem speciﬁc. reasons strongly prefer ﬁrst algorithm; however efﬁciency reasons batching matters. next section turn problem efﬁciently execute naïve computation graphs take advantage efﬁcient batched implementations operations. provides best worlds developers code easy write execution fast. algorithm on-the-ﬂy batching manual batching discussed previous section mostly operates aggregating input instances feeding network. rnns means aggregating inputs share time step. often require padding masking input sizes differ. also restricts kinds operations batched. contrast method identiﬁes aggregates computation graph nodes executed batched fashion given graph. reduces need workarounds padding masking allows seamless efﬁcient execution also architectures hard conceptualize input-centric paradigm allows identiﬁcation batching opportunities apparent input-centric view. batching procedure operates three steps graph deﬁnition operation batching computation. here steps shared standard execution computation graphs corresponds proposed method. graph deﬁnition first deﬁne graph represents computation want perform. user’s perspective done simply performing computation interested performing deﬁned rnn-regression-loss function previous example. common dynamic graph frameworks interleave graph deﬁnition forward execution separate parts using lazy evaluation perform forward evaluation resulting value requested user calling forward function. graph extended call forward calls lazily evaluate delta computation. allows accumulation large graph chunks executing forward computations providing ample opportunities operation batching. operation batching next given computation graph left side figure proposed algorithm converts graph operations executed together batched together. done step process described below. computing compatibility groups ﬁrst partition nodes compatibility groups nodes group potential batching. done associating node signature nodes share signature guaranteed able executed single operation inputs ready. signatures vary depending operation node represents. example nodes representing element-wise operations nodes operation batched together signature simply operation name nodes dimensions information also relevant whether operations batched information also included signature. example node picks slice input matrix also dependent matrix size range slice signature look something like slice-x-. cases remember speciﬁc node inputs generalizing across inputs resulting signature would look something like matmul-node-x. thorough discussion given appendix determining execution order computation graph essentially dependency graph node depends input goal select execution order node executed dependencies; nodes signature depend scheduled execution step finding optimal execution order maximizes amount batching general case hard discuss heuristic strategies identifying execution orders satisfy requirements. depth-based batching used method automatic batching tensorflow fold done calculating depth node original computation graph deﬁned maximum length leaf node node itself batching together nodes identical depth signature. construction nodes depth dependent each-other nodes higher depth input thus batching strategy guaranteed satisfy condition above. however strategy also miss good batching opportunities. example loss function calculations figure different depths different-lengthed sequences similar problems occur recurrent neural network language models tree-structured neural networks myriad situations. agenda-based batching method propose depend solely depth. core method agenda tracks available nodes unresolved dependencies. node count unresolved dependencies maintained; initialized number inputs node. agenda initialized adding nodes incoming inputs iteration select node agenda together available nodes signature group single batch operation. nodes removed agenda dependency counter successors decremented. zero-dependency nodes added agenda. process repeated nodes processed. prioritize multiple available nodes agenda? intuitively want avoid prematurely executing nodes potential nodes signature added agenda later point resulting better batching. good example running example figure loss-calculating nodes added agenda different points becoming calculable different numbers time steps. capture intuition introduce heuristic method prioritizing nodes based average depth nodes signature nodes lower average depth executed earlier. general tends prioritize nodes occur earlier parts graph result nodes later parts graph loss calculations executed later hopefully batched together. finally non-trivial batching procedure must executed quickly overhead batch scheduling calculations doesn’t cancel efﬁciency gains operation batching. ensure this perform number optimizations implementation detail appendix determined execution order perform calculations values themselves. standard computation graphs forward computation done topological order calculate function itself backward calculation done reverse topological order calculate gradients. automatically batched evaluation calculation largely similar exceptions single→batch node conversion first necessary convert single nodes batched node also requires modiﬁcation underlying operations converting multiple matrixvector operations single matrix-matrix operation done internally library user-facing maintains original unbatched computation graph structure making process invisible user. ensuring contiguous memory ensure operations executed batch inputs operations must arranged contiguous memory cases necessary perform memory copy arrange inputs contiguous memory cases inputs already contiguous correct order cases omit memory copy inputs as-is. section describe experiments designed answer three main questions situations manual batching easy close proposed method approach efﬁciency program uses hand-crafted manual batching depth-based agenda-based approaches compare situations manual batching less easy proposed method capable obtaining signiﬁcant improvements efﬁciency proposed method compare tensorflow fold existing method batching variably structured networks within static declaration framework ﬁrst experiments stress-test proposed algorithm ideal case manual batching. speciﬁcally train model bi-directional lstm sequence labeler synthetic data every sequence labeled length this manual batching easy don’t padding adjustment sentences different lengths. network takes input size embedding vector vocabulary size layers hidden node lstms either direction predicts label classes. batch size within setting test various batching settings without manual mini-batching explicitly batch word vector lookup lstm update loss calculation time step. without on-the-ﬂy batching depth-based autobatching agendabased autobatching measure speed method ms/sec also break percentage computation time spent forward graph creation/on-the-ﬂy batching forward computation backward graph creation backward computation parameter update. experiments single tesla intel xeon .ghz cpu. control variance execution time perform three runs report fastest. report accuracy numbers functions calculated thus accuracies regardless batching strategy. figure computation time forward/backward graph construction computation well parameter update bilstm tagger without manual batching without depth-based agenda-based automatic batching. results found figure first comparing ﬁrst second proposed on-the-ﬂy batching strategy drastically reduces computation time sentence byagenda reducing per-sentence computation time resulting approximately -fold increase sentences processed second byagenda faster bydepth demonstrating sophisticated agenda-based strategy indeed effective batching together operations. next compared manual batching without automatic batching fully automatic batching manual batching competitive slightly slower. speed decrease attributed increased overhead computation graph construction batch scheduling. however even extremely idealized scenario manual batching competitive difference relatively small compared extreme difference case using batching all. given automatic batching major advantages ease implementation attractive alternative even situations manual batching relatively easy. finally compare fourth ﬁfth/sixth rows even manual batching automatic batching still provides gains computational efﬁciency processing sentences times faster without automatic batching. reason attributed fact bilstm implementation performs manual batching across sentences across time steps within sentence. contrast auto-batching procedure able batch word embedding lookup softmax operations across time-steps well reducing number calls increasing speed. case less gained batching less expensive operations. next extend experiments cases increasingly difﬁcult manually batch. realistic dimension sizes corresponding tasks batches size exact dimensions details training settings appendix bilstm w/char bilstm tagger above except additional bilstm characters calculate embeddings rare words. sorts character-based embeddings shown allow model generalize better also makes batching operations difﬁcult variably-lengthed encoding step occur words input. tree-structured lstms tree-lstm model here instance tree rather sequence network structure follows tree structures. discussed introduction architecture notoriously hard manually batch. transition-based dependency parsing challenging case evaluate transition-based system transition based parser lstm-based featureextraction exploration-based training here sequence encoded using lstm followed series predictions. prediction based subset encoded vectors vectors participate prediction well loss determined outcomes previous predictions. here batching harder nature computation interleaves sampling model training requires calling forward step leaving automatic-batcher little room play with. however small change computation different parsers parallel potentially share computation across different systems given time-step. concretely modiﬁed version bist parser results table cases automatic batching gives healthy improvements computation time .x–.× .–.× gpu. furthermore agenda-based heuristic generally effective depth-based one. compare tensorflow fold reference implementation stanford sentiment treebank regression task using treelstm architecture .figure shows many trees processed second relative performance byagenda algorithm dynet dynet performance better across board stratiﬁed hardware type. furthermore dynet greater throughput tensorflow fold batch sizes exceed additionally single instance training dynet’s sequential evaluation processes trees/second whereas autobatching processes trees/second. demonstrates complex architectures like treelstms opportunities batch operations inside single training instance exploited batching algorithm. addition noted dynet implementation advantage much straightforward relying simple python data structures control represent traverse trees fold implementation requires implementing traversal composition logic domain speciﬁc functional programming language optimization static algorithms widely studied plays important role numerical libraries used machine learning. work rather different since code/workload dynamically speciﬁed must executed rapidly precludes sophisticated statistic analysis. however review important related work here. automatic graph optimization selection kernels static computation graphs used variety toolkits including tensorflow theano dynamic creation optimally sized minibatches make good hardware resources also proposed optimizing convolutional architectures static nature computation makes tools closer optimizing compilers rather efﬁcient interpreters required cope dynamic workloads encountered dealing dynamically structured computations. related general technique automatic vectorization mainstay optimizing compilers. recent work begun explore vectorization context interpreted code cannot compiled autobatching variant dynet similarly provides vectorized primitives selected dynamically. aﬁeld problem scheduling batching decisions widely studied operations research since least although work deals similar problems standard algorithms ﬁeld computationally demanding execute inner loop learning algorithm. deep learning research relies empirical exploration architectures. rapid pace innovation seen last several years enabled largely tools automated error-prone aspects engineering writing code computes gradients. however contention operation batching increasingly becoming another aspect model coding error prone amenable automation. solution framework lets programmers express computations naturally relies smart lightweight interpreter ﬁgure execute operations efﬁciently. hope facilitate creation classes models better cope complexities real-world data. references martın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. michael bartholomew-briggs steven brown bruce christianson laurence dixon. automatic differentiation algorithms. journal computational applied mathematics james bergstra olivier breuleux frédéric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. theano math compiler python. proc. python science conf pages samuel bowman gauthier abhinav rastogi raghav gupta christopher manning christopher potts. fast uniﬁed model parsing sentence understanding. annual conference association computational linguistics pages chris dyer miguel ballesteros wang ling austin matthews noah smith. transitionbased dependency parsing stack long short-term memory. annual conference association computational linguistics pages stefan hadjis firas abuzaid zhang christopher caffe troll shallow ideas speed deep learning. proceedings fourth workshop data analytics scale eliyahu kiperwasser yoav goldberg. simple accurate dependency parsing using bidirectional lstm feature representations. transactions association computational linguistics faisal ladhak ankur gandhe markus dreyer lambert matthias ariya rastrow björn hoffmeister. latticernn recurrent neural networks lattices. proc. interspeech chengtao daniel tarlow alexander gaunt marc brockschmidt nate kushman. neural program lattices. international conference learning representations wang ling chris dyer alan black isabel trancoso ramon fermandez silvio amir luis marujo tiago luis. finding function form compositional character models open vocabulary word representation. conference empirical methods natural language processing pages moshe looks marcello herreshoff delesley hutchins peter norvig. deep learning dynamic computation graphs. international conference learning representations graham neubig chris dyer yoav goldberg austin matthews waleed ammar antonios anastasopoulos miguel ballesteros david chiang daniel clothiaux trevor cohn kevin manaal faruqui cynthia garrette yangfeng lingpeng kong adhiguna kuncoro gaurav kumar chaitanya malaviya paul michel yusuke matthew richardson naomi saphra swabha swayamdipta pengcheng yin. dynet dynamic neural network toolkit. arxiv preprint arxiv. joel nothman nicky ringland radford tara murphy james curran. learning multilingual named entity recognition wikipedia. artiﬁcial intelligence barbara plank anders søgaard yoav goldberg. multilingual part-of-speech tagging bidirectional long short-term memory models auxiliary loss. annual conference association computational linguistics pages noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc geoffrey hinton jeff dean. outrageously large neural networks sparsely-gated mixture-of-experts layer. international conference learning representations richard socher cliff chris manning andrew parsing natural scenes natural language recursive neural networks. international conference machine learning pages richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. conference empirical methods natural language processing sheng richard socher christopher manning. improved semantic representations tree-structured long short-term memory networks. annual conference association computational linguistics seiya tokui kenta oono shohei hido justin clayton. chainer next-generation open source framework deep learning. proceedings workshop machine learning systems twenty-ninth annual conference neural information processing systems dani yogatama phil blunsom chris dyer edward grefenstette wang ling. learning compose words sentences reinforcement learning. international conference learning representations node signature nodes identical signatures batched together. exceptions nodes batched together perform operation identity operation node performs necessary part signature. addition additional constraints nodes batched together based nature operation performed. demonstrate signatures illustrative classes operations below component-wise operations tanh perform exactly work regardless shape tensors involved. simple operations signature simply identity operation additional constraints. also true component-wise operations take multiple arguments sums component-wise multiplications long involve broadcasting discussed following items. dimension-sensitive operations require additional restrictions. example matrix multiplies generally performed inputs dimensions match several wihi operations able batch together dimension across elements cases explicitly specify necessary dimensions signature preventing inputs incompatible dimensions processed together. operations shared elements matrix–vector multiply matrix applied vectors common source potential gains operation batching. reason operations important perform explicit optimizations concatenating vectors matrix performing single matrix–matrix multiplication take advantage this represented node deﬁne signature mult-nw- operations share left side different right sides grouped together. matrix multiplication either shared un-shared signature. thing noted nodes like matrix multiplies example afﬁne transforms signature clear. elements shared parameters would preferable signature shares parameters take advantage efﬁcient implementations mentioned above. however elements multiply afﬁne transform unique better simpler dimension-sensitive operations. implementation simple heuristic multiplies afﬁne transforms neural networks tend parameters positions elements position tend input-dependent signatures share elements positions share elements position. order ensure increased complexity automatic batching introduce unacceptable overhead calculation took care efﬁciently implement different parts algorithm using sophisticated fairly standard optimization techniques. include minimizing number memory allocations preferring stack allocation ﬁxed-size implementing specialized linked-list-style data structures contiguous memory avoid computing node signatures integer hash values instead strings. implementing optimized kernels perform sparse-to-dense dense-to-sparse memory copies copying operations results to/from contiguous memory batched nodes. bilstm tagging data data named entity recognition task models trained tested wikiner english corpus words frequency less treated unknowns. network single-layer word embeddings lstms either direction containing nodes each. bilstm w/char settings bilstm tagger character embeddings above addition character-based lstms unknown words. character embeddings size character lstms directions. tree-structured lstms tree lstms trained stanford sentiment treebank regression task similarly word embedding node sizes models trained predict labels node tree. transition-based dependency parsing parser modiﬁed perform aggregate batching running several parsers in-parallel aggregating decisions given time-step across different parsers. contrast benchmarks paper implemented python-based implementation. measure training time iteration training publicly available english universal dependencies treebank containing sentences. default settings parser well ﬂags –-userlmost –-userl –-bibi-lstm. http//github.com/clab/dynet https//github.com/neulab/dynet-benchmark http//www.github.com/elikip/bist-parser/ https//github.com/universaldependencies/ud_english", "year": 2017}