{"title": "Learning Overcomplete HMMs", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We study the problem of learning overcomplete HMMs---those that have many hidden states but a small output alphabet. Despite having significant practical importance, such HMMs are poorly understood with no known positive or negative results for efficient learning. In this paper, we present several new results---both positive and negative---which help define the boundaries between the tractable and intractable settings. Specifically, we show positive results for a large subclass of HMMs whose transition matrices are sparse, well-conditioned, and have small probability mass on short cycles. On the other hand, we show that learning is impossible given only a polynomial number of samples for HMMs with a small output alphabet and whose transition matrices are random regular graphs with large degree. We also discuss these results in the context of learning HMMs which can capture long-term dependencies.", "text": "study problem learning overcomplete hmms—those many hidden states small output alphabet. despite signiﬁcant practical importance hmms poorly understood known positive negative results efﬁcient learning. paper present several results—both positive negative—which help deﬁne boundaries tractable intractable settings. speciﬁcally show positive results large subclass hmms whose transition matrices sparse well-conditioned small probability mass short cycles. hand show learning impossible given polynomial number samples hmms small output alphabet whose transition matrices random regular graphs large degree. also discuss results context learning hmms capture long-term dependencies. hidden markov models commonly used data natural sequential structure paper focuses overcomplete hmms number output symbols much smaller number hidden states example outputs natural language documents character time number characters quite small number hidden states would need large encode rich syntactic semantic discourse structure document. algorithms learning hmms provable guarantees assume transition rn×n observation rm×n matrices full rank hence apply overcomplete regime. notable exception recent work huang studied setting showed generic hmms learned polynomial time given exact moments output process though understanding properties generic hmms important ﬁrst step reality hmms large number hidden states typically structured non-generic transition matrices—e.g. consider sparse transition matrices transition matrices factorial hmms huang also assume access exact moments leaves open question learning possible efﬁcient sample complexity. summarizing interested following questions fundamental limitations learning overcomplete hmms? properties hmms make learning possible polynomial samples? structured hmms learned overcomplete regime? contributions. make progress three questions work sharpening understanding boundary tractable intractable learning. begin stating negative result perhaps explains difﬁculty obtaining strong learning guarantees overcomplete setting. theorem parameters hmms transition matrix encodes random walk regular graph nodes degree polynomial output alphabet polylog iii) output distribution hidden state chosen uniformly independently random cannot learned using polynomially many samples window length polynomial high probability choice observation matrix. theorem somewhat surprising parameters hmms transition matrices easily learned non-overcomplete regime. transition matrices full-rank condition numbers polynomial hence spectral techniques anandkumar applied. theorem also fundamentally different nature compared lower bounds based parity noise reductions hmms information-theoretic. also seems damning hard cases seemingly innocuous classes random walks dense graphs. lower bound also shows analyzing generic random hmms might right framework consider overcomplete regime might learnable polynomial samples even though identiﬁable. motivates need understanding hmms structured transition matrices. provide proof theorem explicitly stated conditions appendix positive results focus understanding properties structured transition matrices make learning tractable. disentangle additional complications choice observation matrix assume observation matrix drawn random throughout paper. long-standing open problems learning aliased hmms hint understanding learnability respect properties observation matrix daunting task itself perhaps best studied separately understanding properties transition matrix affect learning. positive result learnability depends natural graph-theoretic properties transition matrix. consider transition matrices sparse small probability mass cycles shorter logm states—and show hmms learned efﬁciently using tensor decomposition method moments given random observation matrices. condition prohibiting short cycles might seem mysterious. intuitively need condition ensure markov chain visits sufﬁcient large portion state space short interval time fact condition stems information-theoretic considerations. discuss sections also discuss results relate learning hmms capture long-term dependencies outputs introduce notion well captures long-term dependencies. discussed section also show identiﬁability results sparse hmms. results provide ﬁner picture identiﬁability huang hold sparse transition matrices generic. technical contribution. prove theorem show khatri-rao product dependent random vectors well-conditioned certain conditions. previously bhaskara showed khatri-rao product independent random vectors well-conditioned perform smoothed analysis tensor decomposition techniques however extend dependent case. dependent case show similar result using novel markov chain coupling based argument relates condition number best coupling output distributions random walks disjoint starting distributions. technique outlined section related work. spectral methods learning hmms studied anandkumar bhaskara allman results require allman authors show hmms identiﬁable given moments continuous observations requires bhaskara give another bound window size requires however output alphabet size specifying moments length continuous time interval requires time samples therefore approaches lead exponential runtimes constant respect also relevant work anandkumar guarantees learning certain latent variable models gaussian mixtures overcomplete setting tensor decomposition. mentioned earlier work closest huang showed generic hmms identiﬁable gives ﬁrst polynomial runtimes case constant. parity noise information theoretically easy given observations window length least number inputs parity. linear number hidden states parity noise whereas theorem says sample complexity must super polynomial polynomial sized window. outline. section introduces notation setup. also provides examples high-level overview proof approach. section states learnability result discusses assumptions hmms satisfy assumptions. section contains identiﬁability results sparse hmms. section discusses natural measures long-term dependencies hmms. state lower bound learning dense random hmms section conclude section provide proof sketches main body rigorous proofs deferred appendix. setup overview section ﬁrst introduce required notation outline method moments approach parameter recovery. also examples provide better understanding classes hmms learn give high level proof strategy. denote output time hidden state time number hidden states number observations assume output alphabet without loss generality. transition matrix observation matrix deﬁned columns one. matrix refer column deﬁned transition matrix time-reversed markov chain assume reversibility hence equal denote refer string length sequence outputs time time output alphabet denoting particular output sequence time deﬁne bijective mapping maps output sequence associated inverse mapping throughout paper assume transition matrix ergodic hence stationary distribution. also assume every hidden state stationary probability least /poly. necessary condition otherwise might even visit states poly samples. also assume output process stationary. stochastic process stationary distribution subset random variables invariant respect shifts time index—that string lτ−τ true initial hidden state chosen according stationary distribution. results depend conditioning matrix respect norm. deﬁne minimum gain transition matrix vectors unit norm also natural parameter measure long-term dependence hmm—if large preserves signiﬁcant information distribution hidden states time future time initial distributions time discuss section denotes column matrix denotes tensor product rd×d×d aibjck. refer different dimensions tensor modes tensor. denote mode matricization tensor ﬂattening tensor along direction obtained stacking matrix slices together. example denotes ﬂattening tensor rd×d×d matrix. recall denote khatri-rao product matrices denotes ﬂattening matrix vector. denote kruskal’s condition says full rank rows linearly dependent efﬁciently decomposed factors decomposition unique upto scaling permutation. simultaneous decomposition algorithm well known algorithm decompose tensors satisfy kruskal’s condition. algorithm learning hmms follows method moments based approach outlined example anandkumar huang contrast popular expectationmaximization approach suffer slow convergence local optima method moments approach ensures guaranteed recovery parameters mild conditions. method moments approach learning hmms high-level steps. ﬁrst step write tensor empirical moments data factors tensor correspond parameters underlying model. second step perform tensor decomposition recover factors tensor—and recover parameters model factors. fact enables second step tensors unique decomposition mild conditions factors example tensors unique decomposition factors full rank. uniqueness tensor decomposition permits unique recovery parameters model. learn using moments observation sequences yτ−τ time since output process assumed stationary distribution outputs contiguous time interval length interval setup convenience. call length observation sequences used learning window length since number samples required estimate moments window length desirable keep small. note ensure polynomial runtime sample complexity method moments approach window length must deﬁne moment tensor. given moments window length construct third-order moment tensor rmτ×mτ×m using mapping strings outputs indices tensor transition observation matrices recovered factors tensors goal analyze conditions tensor decomposition step works provably. note factor matrix likelihood observing sequence observations conditioned starting given hidden state. we’ll refer likelihood matrix reason. equivalent matrix time-reversed markov chain. show full rank columns same tensor unique decomposition learned provided exact moments using simultaneous diagonalization algorithm show property identiﬁability results. learnability results show matrices well-conditioned implies learnability polynomial samples. main technical contribution paper requires analyzing condition number khatri-rao product dependent random denotes khatri-rao product matrices matrices size khatri-rao product matrix whose column outer product ﬂattened vector. note sketch argument showing well-conditioned appropriate conditions. coupling random walks analyze khatri-rao product. mentioned introduction paper interested setting transition matrix ﬁxed observation matrix drawn random. could draw fresh random matrices time step recursion would well-conditioned smoothed analysis khatri-rao product bhaskara however setting signiﬁcantly difﬁcult access fresh randomness time step techniques bhaskara cannot applied here. pointed earlier condition number scenario depends crucially transition matrix even full rank instead analyze coupling argument. intuition this note full rank disjoint sets columns whose linear combinations equal combination weights used setup initial states random walks deﬁned transition matrix output distribution time steps. generally ill-conditioned random walks disjoint starting states similar output distributions. show random walks similar output distributions time steps randomly chosen observation matrix probability mass random walks coupled. hand min)τ sufﬁciently large total variational distance random walks starting different starting states must least min)τ time steps cannot good coupling well-conditioned. provide sketch argument simple case section provide simple examples illustrate classes hmms cannot learn. ﬁrst provide example class simple hmms handled results non-generic transition matrices hence framework huang consider transition matrix permutation cyclic shift hidden states results imply hmms learnable polynomial time figure examples transition matrices framework. proposition shows hmms transition matrix composed union cycles constant length even identiﬁable short windows length polynomial samples output distributions hidden states chosen random. provide intuition transition matrix fig. efﬁciently learnable. consider simple case outputs binary hidden state deterministically outputs labeled accordingly. labels assigned random high probability string labels continuous sequence hidden states cycle fig. unique. means output distribution time window unique every initial hidden state shown ensures moment tensor unique factorization. showing output distribution time window different different initial hidden states—in addition unique—we show factors moment tensor well-conditioned allows recovery efﬁcient sample complexity. another slightly complex example learn fig. depicts whose transition matrix random walk graph small degree short cycles. learnability result handle hmms structured transition matrices. example cannot learned framework consider transition matrix binary observations fig. case probability output sequence depends total number zeros ones sequence. therefore independent measurements windows length hence windows length instead necessary identiﬁability discussions case). generally prove proposition small transition matrix composed cycles constant length requires window length polynomial become identiﬁable. proposition consider hidden states observations transition matrix permutation composed cycles length windows length necessary model identiﬁable polynomial constant root cause difﬁculty learning hmms short cycles visit large enough portion state space steps hence moments time window carry sufﬁcient information learning. results cannot handle classes transition matrices also section discussion. learnability results overcomplete hmms section state learnability result discuss assumptions provide examples hmms satisfy assumptions. learnability results hold following conditions reversed markov chain well-conditioned -norm logm states logm time except probability /nc. transition distributions /nc. hence soft degree requirement. output distributions random small support exists every hidden state output distribution cumulative mass outputs /nc. also output distribution drawn uniformly outputs. constants made explicit example works. conditions show hmms learned using polynomially many samples theorem satisﬁes conditions high probability choice parameters learnable within additive error observations windows length logm sample complexity poly. proof sketch. refer reader section high level idea. here provide proof sketch much simpler case considered theorem recall main goal show likelihood matrix well-conditioned. assume simplicity output distribution hidden state deterministic output distribution support character. character output distribution hidden state supported assigned independently uniformly random output alphabet. also assume conditions theorem zero. proof steps roughly follows– ﬁrst show sample paths random walk logm time steps visit logm different states time steps never meet time steps emit different sequence observations high probability randomness using fact degree hidden state small perform union bound possible sample paths show high probability choice sample paths meet time steps emit different sequence observations. consider sample paths corresponding random walks emit sequence observations time steps. point above must meet time probability emitting random walks respectively show probability mass coupled sample paths intersect sample paths core argument. also refer fig. hence probability emitting sequence observations random walks similar every sequence good coupling random walks implies total variational distance distribution random walks time steps must small. contradiction min)τ large. contradiction stems fact distance time hence distance time least appendix also states corollary theorem terms minimum singular value σmin matrix instead min. discuss conditions theorem next subsequently provide examples hmms satisfy conditions. figure consider random walks time steps disjoint starting states sample paths visits states {abcd} {ebcf} times respectively. show sample paths output distribution must hidden state time step. example simultaneously states means probability mass random walks coupled hence variational distance random walks must small end. cannot case well-conditioned. hence sample paths must different output distributions means random walks start disjoint states must different output distributions implies well-conditioned. figure experiments study effect sparsity short cycles learnability hmms. condition number likelihood matrix determines stability sample complexity method moments approach. condition numbers averaged trials. transition matrix well-conditioned note singular transition matrices might even identiﬁable. moreover mossel roch showed learning hmms singular transition matrices hard learning parity noise widely conjectured computationally hard. hence necessary exclude least classes ill-conditioned transition matrices. transition matrix short cycles proposition know might even identiﬁable short windows composed union short cycles hence expect similar condition learning polynomial samples; though upper lower bounds terms probability mass allowed short cycles. performed simulations understand length cycles transition matrix probability mass assigned short cycles affects condition number likelihood matrix recall condition number determines stability method moments approach. take number hidden states cycle hidden states union short cycles length states take transition matrix different values fig. shows condition number becomes worse hence learning requires samples cycles shorter length probability mass assigned short cycles hinting conditions perhaps stringent. hidden states small degree condition theorem reinterpreted saying transition probabilities hidden state must mass /n+c hidden state except hidden states soft constraint weaker hard constraint degree natural whether sparsity necessary learn hmms. above carry simulations understand degree affects condition number likelihood matrix consider transition matrices hidden states combination dense part cycle. deﬁne cycle before. deﬁne adjacency matrix directed regular graph degree take transition matrix hence transition distribution every hidden state mass neighbors residual probability mass assigned permutation fig. shows condition number becomes worse degree becomes larger probability mass assigned dense part transition matrix providing weak evidence necessity condition also recall theorem shows hmms transition matrix random walk undirected regular graph large degree cannot learned using polynomially many samples constant respect however graphs eigenvalues except ﬁrst less hence clear hardness learning depends large degree ill-conditioned. concretely pose following open question open question consider transition matrix cyclic permutation hidden states random walk undirected regular graph large degree constant. learned using polynomial samples small respect example approximately preserves σmin addition permutation hence difﬁculty transition matrix large degree. output distributions random small support discussed introduction assume observation matrices random even simple hmms cycle permutation transition matrix might require long windows even become identiﬁable fig. hence assumptions output distribution seem necessary learning model short time windows though assumptions probably tight. instance assumption output distributions small support makes learning easier leads outputs discriminative hidden states clear necessary assumption. ideally would like prove learnability results smoothed model adversary allowed transition matrix pick worst-case random noise added output distributions limits power adversary. believe results hold smoothed setting aside future work. figure consider hmms transition matrices cycles states binary outputs outputs conditioned hidden states deterministic. states labeled always emit states labeled always emit hmms distinguishable windows length less hence worst case even simple hmms like cycle could require long windows even become identiﬁable. transition matrices markov chain permutation markov chain permutation cycles longer logm transition matrix obeys conditions theorem singular values permutation degree hidden states visit logm different states logm time steps. transition matrices random walks graphs small degree large girth directed graphs condition equivalently stated graph representation transition matrix large girth transition matrices factorial hmms factorial hmms factor latent state time dimensions independently evolves according markov process equivalent saying hidden states indexed labels represent transition matrices dimensions naturally models settings multiple latent concepts evolve independently. following properties easy show therefore factorial hmms learnable random underlying processes obey conditions similar assumptions theorem well-conditioned least short cycles either small degree learnable random figure graphical model factorial markov chains evolve independently output time step dependent current states markov chains time step. conditions learning transition matrices transfer cleanly conditions transition matrices underlying markov chains obvious requirements theorem necessary natural attempt derive stronger results identiﬁability hmms structured transition matrices. section state results identiﬁability hmms windows size huang showed hmms except belonging measure zero become identiﬁable windows length logm however measure zero might possibly contain interesting classes hmms example sparse hmms also belong measure zero set. reﬁne identiﬁability results section show natural sparsity condition transition matrix guarantees identiﬁability short windows. given transition matrix regard supported indices non-zero entries state result identiﬁability sparse hmms. theorem indices supports permutation cycles least logm hidden states. transition matrices support identiﬁable windows length logm observation matrices except measure zero transition matrices observation matrices proof sketch. recall section main task show likelihood matrix full rank. proof uses basic algebraic geometry main idea used analogous following fact polynomials either polynomial zero polynomial ﬁnitely many roots measure zero set. determinant likelihood matrix polynomial entries hence need show polynomial zero polynomial. show polynomial zero polynomial sufﬁcient instance variables makes polynomial non-zero. hence need particular determinant using fact supports permutation short cycles. hypothesize excluding measure zero transition matrices theorem necessary long transition matrix full rank unable show this. note result identiﬁability ﬂexible allowing short cycles transition matrices theorem closer lower bound identiﬁability proposition also strengthen result huang identiﬁability generic hmms. huang conjectured windows length logm sufﬁcient generic hmms identiﬁable. constant information theoretic bound hidden states outputs independent parameters hence needs observations window size logm uniquely identiﬁable. proposition settles conjecture proving optimal window length requirement generic hmms identiﬁable. number possible outputs window length size moment tensor section exponential window length. therefore even factor improvement window length requirement leads quadratic improvement sample time complexity. proposition hmms identiﬁable observations windows length logm except measure zero transition matrices observation matrices discussion long-term dependencies hmms section discuss long-term dependencies hmms show results overcomplete hmms improve understanding hmms capture long-term dependencies respect markov chain outputs. recall deﬁnition claim large transition matrix preserves signiﬁcant information distribution hidden states time future time initial distributions time consider distributions time distributions hidden states time given distribution time respectively. distance min)tp verifying claim. interesting compare notion mixing time transition matrix. deﬁning mixing time time distance starting distributions follows mixing time τmix min) large chain slowly mixing. however converse true—σ might small even chain never mixes example graph disconnected connected components quickly. therefore possibly better notion long-term dependence transition matrix requires information preserved past state directions. another reasonable notion long-term dependence long-term dependence output process instead hidden markov chain utility past observations making predictions distant future depend simple matrices note markov chain fast mixing output process certainly long-term dependencies. also note respect long-term dependencies output process setting seems much interesting comparable reason small output alphabet setting receive small amount information true hidden state step hence longer windows necessary infer hidden state make good prediction. also refer reader kakade related discussions memory output processes hmms. section state lower bound possible efﬁciently learn hmms underlying transition matrix random walk graph large degree small respect actually show stronger result show number bits information contained polynomial number samples negligible fraction total number bits information needed specify transition matrix cases showing approximate learning also information theoretically impossible. theorem consider class hmms hidden states outputs polylog transition matrix chosen d-regular graph least bits information needed specify choice transition matrix. however observation matrix randomly chosen columns chosen independently number bits information contained polynomially many samples window length poly high probability choice notation hides polylogarithmic factors proof sketch. proof consists steps–in ﬁrst step show information contained polynomially many observations windows length logm sufﬁcient learn hmm. proof part relies counting argument lower bound number random regular graphs given degree. show information contained polynomial samples longer windows much larger information contained polynomial samples window length main technical part need show hidden state time much inﬂuence hidden state time conditioned outputs time conditioning makes tricky probabilities hidden states longer evolve transition matrix markov chain. around showing probability hidden states conditioning observations evolves time-inhomogeneous markov chain transition matrices every time step related outputs time original transition matrix. analyze spectrum time-inhomogeneous transition matrices show inﬂuence hidden state time decays every step small time would like point techniques prove information theoretic lower bound appear generally useful analyzing inﬂuence hidden state time hidden state time conditioned outputs time measure much value observations time predicting observation time conditioned intermediate observations time natural notion memory output process. conclusion future work setting output alphabet much smaller number hidden states well-motivated practice seems several interesting theoretical questions lower bounds algorithms. though results obtained restrictive conditions seems necessary hope ideas techniques pave much sharper results setting. open problems think might particularly useful improving understanding relaxing condition observation matrix random structural constraint observation matrix thoroughly investigating requirement transition matrix sparse short cycles.", "year": 2017}