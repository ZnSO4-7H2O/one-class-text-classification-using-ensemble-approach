{"title": "Deep Recurrent Neural Networks for Acoustic Modelling", "tag": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "abstract": "We present a novel deep Recurrent Neural Network (RNN) model for acoustic modelling in Automatic Speech Recognition (ASR). We term our contribution as a TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with Time Convolution (TC), followed by a Bidirectional Long Short-Term Memory (BLSTM), and a final DNN. The first DNN acts as a feature processor to our model, the BLSTM then generates a context from the sequence acoustic signal, and the final DNN takes the context and models the posterior probabilities of the acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ) eval92 task or more than 8% relative improvement over the baseline DNN models.", "text": "network architecture. rnns include temporal memory component allows model store temporal contextual information directly model. relieves explicitly deﬁning size temporal contexts allows model learn directly. fact whole speech sequence accumulated temporal context. exist many implementations rnns lstm gated recurrent units particular implementations rnns easy train suffer vanishing exploding gradient problems performing backpropagation time lstms capability remember sequences long range temporal dependencies applied successfully many applications include image captioning end-to-end speech recognition machine translation lstms process sequential signals direction. natural extension bidirectional lstms composed lstms. forward lstm process sequence usual second processes input sequence backward order. outputs sequences concatenated. blstms distinct advantages lstms ﬁrst advantage forward backward passes sequence yields differing temporal dependencies model capture sets signal dependencies. second advantage higher level sequence layers using blstm outputs access information input directions. lstms grus recently successfully applied acoustic modelling timit phone sequences trained end-toend unsegmented sequence data using lstm transducer. lstms combined connectionist temporal classiﬁcation implicitly perform sequence training speech signal timit used grus generated explicit alignment model timit speech sequence data phone sequence. commercial speech system trained using lstm acoustic model entire speech sequence used context classifying context dependent phones. extend applied sequence training lstms. contribution paper novel deep acoustic model easy train archives relative improvement dnns wall street journal corpus. present novel deep recurrent neural network model acoustic modelling automatic speech recognition term contribution tc-dnn-blstm-dnn model model combines deep neural network time convolution followed bidirectional longshort term memory ﬁnal dnn. ﬁrst acts feature processor model blstm generates context sequence acoustic signal ﬁnal takes context models posterior probabilities acoustic states. achieve wall street journal eval task relative improvement baseline models index terms deep neural networks recurrent neural networks long-short term memory asynchronous stochastic gradient descent automatic speech recognition deep neural networks convolutional neural networks yielded many state-of-the-art results acoustic modelling automatic speech recognition tasks dnns cnns often accept spectral feature context window inputs trained supervised backpropagation softmax targets learning hidden markov model acoustic states. dnns make much prior assumptions input feature space consequently model architecture blind temporal frequency structural localities. cnns able directly model local structural localities usage convolutional ﬁlters. ﬁlters connect subset region feature space tied shared across entire input feature giving model translational invariance additionally pooling often added yields rotational invariance inherent structure cnns yields model much robust small shifts permutations. speech fundamentally sequence time signals. cnns capture time locality convolution ﬁlters however cnns able directly capture longer temporal signal patterns. example temporal patterns span frames however convolution ﬁlter width frames wide. model must rely higher level fully connected layers model long term dependencies. additionally size frame width phones temporal patterns varying lengths. optimizing convolution ﬁlter size expensive procedure corpora dependent figure tc-dnn-blstm-dnn architecture. model contains parts signal processing takes original fmllr acoustic features projects high dimensional space blstm models sequential signal produces context ﬁnal takes context generated blstm estimates likelihoods across acoustic states. length sequences context window. advantage model easily blstms online disadvantage however amount temporal information stored model limited context width however ofﬂine decoding also compute acoustic states parallel versus iterations needed iterative dependency lstm memory. model begins ﬁxed window context acoustic features similar standard acoustic model within context window overlapping time window features time convolution features timestep. similar approach used however used stride sake reducing computational cost however motivation time convolution rather performance stride model processes features independent columns dnns context window timesteps. refer tc-dnn component model. objective tcdnn component project original acoustic feature high dimensional feature space easily modelled consumed lstm. refers deep input-to-hidden function. bias peephole connections; initial experimentation observed negligible difference hence omitted work. additionally apply gradient clipping gradient projection however apply cell activation clipping prevent saturation sigmoid non-linearities. found cell activation clipping help remove convergence problems exploding gradients. also recurrent projection layer found lstm implementation train easily without exploding gradients even high learning rates. context manipulated projected second dnn. second adds additional non-linear transformations ﬁnally softmax layer model context dependent state posteriors. refers deep hidden-to-output function. model trained supervised backpropagation minimizing cross entropy loss. figure gives visualization entire model. found lstm models easy train converge. initialize lstm layers uniform distribution layers gaussian distribution clip lstm cell activations need apply gradient clipping gradient projection. train model stochastic gradient descent using minibatch size found using larger minibatches give slightly worse wers. used simple geometric decay schedule start learning rate multiply factor every epoch. learning rate ﬂoor experimented classical nesterov momentum however found momentum harm ﬁnal convergence slightly hence momentum. apply optimization hyperparameters experiments possible using slightly different decay schedule yield better results. best model took epochs converge around hours wall clock time nvidia tesla gpu. experiment dataset. approximately hours speech training development eval test set. observe development every epoch stop training development longer improves. report converged corresponding eval wers. fmllr features generated kaldi recipe decoding setup exactly recipe trib alignments training targets total acoustic states. trib baseline achieved test respectively. also built relu requires pretraining. relu consisted layers relu neurons followed softmax trained geometrically decayed sgd. also experimented deeper wider networks however found layer architecture best. relu much easier train achieves matching pretrained sigmoid dnn. relu results suggest pretraining necessary given sufﬁcient supervised data competitive acoustic modelling task. table summarizes wers baseline systems. table blstm wers wall street journal. larger recurrent models tend perform better without overﬁtting. deep blstm models yield substantial gains single layer counterparts. experimented single layer layer deep blstm models. cell size reported direction blstm models take longer train underperform compared relu model. large blstm models tend outperform smaller ones suggesting overﬁtting issue. however limited incremental gain performance additional cells. best single layer blstm bidirectional cells achieved compared relu model. deep blstm models give additional model performance since upper layers access information shallow layers directions additional layers non-linearities available. deep blstm models contain layers cell size reported direction layer deep blstm experiments give mixed results. number cells layer deep model performs slightly better. however ﬁxed number parameters single layer blstm model performs slightly better single layer bidirectional cells achieved deep layer blstm model bidirectional cells layer achieved table summarizes blstm experiment wers. experimented next dnn-blstm model. dnnblstm model time convolution input lacks second non-linearities context projection. layer neuron relu front blstm acts signal processor projecting original acoustic signal high dimensional space easily digested lstm. blstm module uses bidirectional cells. compared bidirectional cell blstm model model improves relatively. results experiment suggest fmllr features best features blstm models rather table ablation effects tc-dnn-blstm-dnn model. dnns time convolution used signal context projections. show components critical obtain best performing model. next experiment blstm-dnn model. here blstm accepts original acoustic feature without modiﬁcation emits context. context passed layer neuron relu provides additional layers non-linear projections classiﬁcation softmax layer. again blstm module uses bidirectional cells. model improves relatively compared original bidirectional cell blstm model context non-linearities. result experiment suggest lstm context used directly softmax phone classiﬁcation rather additional layers non-linearities needed achieve best performance. experimented dnn-blstm-dnn model layers relu neurons blstm layer cells direction. combine beneﬁts learnt signal processing context projection. compared bidirectional cell blstm model drops relatively. compared bidirectional cell blstm model essentially redistributed parameters wide shallow network deeper network. achieve relative improvement compared single layer bidirectional cell blstm suggesting deeper models much expressive powerful. finally tc-dnn-blstm-dnn model combines dnn-blstm-dnn input time convolution. model improves without time convolution time convolution. compared models achieve absolute reduction relatively. best knowledge best eval performance without sequence training hypothesize time convolution gives richer signal representation signal processor consequently blstm model consume. time convolution also relieves lstm computation power learning long term dependencies rather short term dependencies. table summarizes experiments section. results presented previous sections paper trained single sgd. reduce time required train individual model also experimented distributed asynchronous stochastic gradient descent across multiple gpus. implementation similar gpus system dedicated parameter server compute shards apply stale gradient decay warm starting exact learning rate schedule minibatch size hyperparameters tc-dnn-blstm-dnn baseline. applied distributed asgd optimization however applied cluster cpus rather gpus. additionally compare differential versus asgd. baseline tc-dnn-blstm-dnn system took epochs wall clock hours converge test distributed implementation converges epochs wall clock hours achieves test distributed optimization able match however test signiﬁcantly worse. unclear whether differential asynchronicity characteristic optimizer small datasets suspect larger datasets asgd shrink. conclusion draw asgd converge much quicker faster however impact ﬁnal performance. table figure summarizes results. paper presented novel tc-dnn-blstm-dnn acoustic model architecture. eval task report relative improvement baseline wer. model easy optimize implement suffer exploding gradients even high learning rates. also found pretraining necessary dnns pretrained achieved compared relu without pretraining wer. also experimented asgd tcdnn-blstm-dnn model able match however evaluation signiﬁcantly lower future work seek apply sequence training acoustic model improve model accuracy. povey ghoshal boulianne burget glembek goel hannenmann motlicek qian schwarz silovsky stemmer vesely kaldi speech recognition toolkit automatic speech recognition understanding workshop sainath kingsbury rahman mohamed dahl saon soltau beran aravkin ramabhadran improvements deep convolutional neural networks lvcsr automatic speech recognition understanding workshop sainath rahman mohamed kingsbury ramabhadran deep convolutional neural networks lvcsr ieee international conference acoustics speech signal processing chan lane deep convolutional neural networks acoustic modeling resource languages ieee international conference acoustics speech signal processing graves rahman mohamed hinton speech recognition deep recurrent neural networks ieee international conference acoustics speech signal processing chung gulcehre bengio empirical evaluation gated recurrent neural networks sequence modeling neural information processing systems workshop deep learning representation learning workshop chorowski bahdanau bengio end-to-end continuous speech recognition using attention-based recurrent first results neural information processing systems workshop deep learning representation learning workshop vinyals heigold senior mcdermott monga sequence discriminative distributed training long short-term memory recurrent neural networks interspeech hinton deng dahl rahman mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition ieee signal processing magazine november", "year": 2015}