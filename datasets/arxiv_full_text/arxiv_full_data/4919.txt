{"title": "Linear Time Computation of Moments in Sum-Product Networks", "tag": ["cs.LG", "cs.AI"], "abstract": "Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. To do so, they must compute moments of the model parameters under this distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. This unfortunate scaling makes Bayesian online algorithms prohibitively expensive, except for small or tree-structured SPNs. We propose an optimal linear-time algorithm that works even when the SPN is a general directed acyclic graph (DAG), which significantly broadens the applicability of Bayesian online algorithms for SPNs. There are three key ingredients in the design and analysis of our algorithm: 1). For each edge in the graph, we construct a linear time reduction from the moment computation problem to a joint inference problem in SPNs. 2). Using the property that each SPN computes a multilinear polynomial, we give an efficient procedure for polynomial evaluation by differentiation without expanding the network that may contain exponentially many monomials. 3). We propose a dynamic programming method to further reduce the computation of the moments of all the edges in the graph from quadratic to linear. We demonstrate the usefulness of our linear time algorithm by applying it to develop a linear time assume density filter (ADF) for SPNs.", "text": "bayesian online algorithms sum-product networks need update posterior distribution seeing single additional instance. must compute moments model parameters distribution. best existing method computing moments scales quadratically size although scales linearly trees. unfortunate scaling makes bayesian online algorithms prohibitively expensive except small tree-structured spns. propose optimal linear-time algorithm works even general directed acyclic graph signiﬁcantly broadens applicability bayesian online algorithms spns. three ingredients design analysis algorithm edge graph construct linear time reduction moment computation problem joint inference problem spns. using property computes multilinear polynomial give efﬁcient procedure polynomial evaluation differentiation without expanding network contain exponentially many monomials. propose dynamic programming method reduce computation moments edges graph quadratic linear. demonstrate usefulness linear time algorithm applying develop linear time assume density ﬁlter spns. sum-product networks recently attracted interest ﬂexibility modeling complex distributions well tractability performing exact marginal inference general-purpose inference machines perform exact joint marginal conditional queries linear time size network. shown discrete spns equivalent arithmetic circuits sense transform equivalent vice versa linear time space respect network size spns also closely connected probabilistic graphical models interpreting node network hidden variable product node rule encoding context-speciﬁc conditional independence every equivalently converted bayesian network compact data structures used represent local probability distributions relationship characterizes probabilistic semantics encoded network structure allows practitioners design principled efﬁcient parameter learning algorithms spns existing batch learning algorithms spns straightforwardly adapted online setting network updates parameters receives instance time step. online learning setting makes spns widely applicable various real-world scenarios. includes case either data large store once network needs adapt change external data distributions. recently rashwan proposed online bayesian moment matching algorithm learn probability distribution model parameters spns based method moments. later jaini extended algorithm continuous case leaf nodes network assumed gaussian distributions. high level understood instance general assumed density ﬁltering framework algorithm ﬁnds approximate posterior distribution within tractable family distributions method moments. speciﬁcally spns works matching ﬁrst second order moments approximate tractable posterior distribution exact intractable posterior. essential sub-routine algorithms efﬁciently compute exact ﬁrst second order moments one-step update posterior distribution rashwan designed recursive algorithm achieve goal linear time underlying network structure tree algorithm also used jaini continuous case. however algorithm works underlying network structure tree naive computation moments scale quadratically w.r.t. network size. often quadratic computation prohibitively expensive even spns moderate sizes. paper propose linear time algorithm able compute moments network parameters simultaneously even underlying network structure dag. three ingredients design analysis algorithm linear time reduction moment computation problem joint inference problem spns succinct evaluation procedure polynomial differentiation without expanding dynamic programming method reduce quadratic computation linear. differential approach used polynomial evaluation also applied exact inference bayesian networks. technique also implicitly used recent development concave-convex procedure optimizing weights spns essentially reducing moment computation problem joint inference problem spns able exploit fact network polynomial computes multilinear function model parameters efﬁciently evaluate polynomial differentiation even polynomial contain exponentially many monomials provided polynomial admits tractable circuit complexity. dynamic programming used trade constant factor space complexity reduce quadratic time complexity linear edge moments computed simultaneously passes network. demonstrate usefulness linear time sub-routine computing moments apply design efﬁcient assumed density ﬁlter learn parameters spns online fashion. runs linear time space efﬁcient sub-routine. additional contribution also show understood general framework moment matching difference lies moments chosen matched match chosen moments. sum-product network computational circuit random variables xn}. rooted directed acyclic graph. internal nodes sums products leaves univariate distributions simplest form leaves indicator variables ix=x also understood categorical distributions whose entire probability mass single value. edges nodes parameterized positive weights. node computes weighted children product node computes product children. interpret node function leaf nodes scope node deﬁned variables appear function. formally node terminal node indicator variable scope else scope ∪˜v∈chscope. complete node children scope decomposable every product node scope scope every pair children shown every valid converted complete decomposable quadratic increase size without changing underlying distribution. result work assume spns discuss complete decomposable. child list node graph edge weight associated node child node probability joint assignment computed value root input divided normalization constant vroot vroot/vroot vroot value root node values leaf nodes essentially corresponds marginalizing random vector ensure deﬁnes proper probability distribution. remarkably queries w.r.t. including joint marginal conditional answered linear time size network. provide alternative interpretations spns useful later design linear time moment computation algorithm. ﬁrst relates spns bayesian networks informally complete decomposable converted bipartite size construction internal node corresponds latent variable constructed leaf distribution node corresponds observable variable furthermore constructed simple bipartite graph layer local latent variables pointing layer observable variables observable variable child local latent variable observable variable appears descendant latent variable original spn. means understood number latent variables instance second perspective view mixture model exponentially many mixture components speciﬁcally decompose complete decomposable induced trees tree corresponds product univariate distributions. proceed ﬁrst formally deﬁne called induced trees deﬁnition given complete decomposable called induced tree root node exactly child corresponding edge product node children corresponding edges shown def. produces subgraphs trees long original complete decomposable useful result based concept induced trees theorem vroot. counts number unique induced trees unique induced tree univariate distribution leaf node. thm. shows vroot also computed efﬁciently setting edge weights general counting problems complexity class fact probabilistic inference counting problem tractable spns also implies spns work subsets distributions succinct/efﬁcient circuit representation. without loss generality assuming layers alternate product layers height hence mixture model represented number mixture components exponential height thm. characterizes number components form component mixture model well mixture weights. convenience later discussion call vroot network polynomial corollary network polynomial vroot multilinear function positive coefﬁcients monomial. corollary holds since monomial corresponds induced tree edge appears tree. property crucial useful derivation linear time algorithm moment computation spns. number nodes suppose given fully factorized prior distribution worth pointing fully factorized prior distribution well justiﬁed bipartite graph structure equivalent introduced section interested computing moments posterior distribution receive observation world. essentially bayesian online learning setting update belief distribution model parameters observe data world sequentially. note corresponds weight vector associated node vector satisﬁes assume prior distribution dirichlet i.e. observing instance exact posterior distribution pp/p. realize network polynomial also computes likelihood plugging expression prior distribution well network polynomial bayes formula since dirichlet conjugate distribution multinomial term summation updated dirichlet multiplicative constant. equation suggests exact posterior distribution becomes mixture dirichlets observation. data instances exact posterior become mixture components intractable maintain since hardness maintaining exact posterior distribution appeals approximate scheme sequentially update belief distribution time efﬁciently maintain approximation. assumed density ﬁltering framework algorithm chooses approximate distribution tractable family distributions observing instance. typical choice match moments approximation exact posterior. order approximate distribution match moments exact posterior need able compute moments exact posterior. problem traditional mixture models including mixture gaussians latent dirichlet allocation etc. since number mixture components models assumed small constants. however case spns effective number mixture components also depends input network ∈tte corresponds product leaf distributions ∈tte i.e. product tree edges prior distribution realizing posterior distribution needs satisfy normalization constraint have generally function applied edge weight spn. notation mean moment function evaluated distribution interested computing call one-step update posterior distribution. speciﬁcally edge weight would like compute following quantity note trivial compute involves terms. furthermore order conduct moment matching need compute moment edge node. naive computation lead total time complexity linear time algorithm compute moments designed rashwan underlying structure tree. algorithm recursively computes moments top-down fashion along tree. however algorithm breaks graph dag. follows present time space algorithm able compute moments simultaneously general spns structures. ﬁrst show linear time reduction moment computation joint inference problem proceed differential trick efﬁciently compute edge graph. ﬁnal component dynamic program simultaneously compute edges graph trading constant factors space complexity reduce time complexity. ﬁrst compute ﬁxed edge strategy partition induced trees based whether contain tree edge not. deﬁne words corresponds trees contain edge corresponds trees contain edge then words implies fact convex combination since computed closed form edge order compute need able compute coefﬁcients efﬁciently. recall αkj. term observation allows linear time reduction lies fact shares exactly functional form network polynomial difference speciﬁcation edge weights network. following lemma formalizes argument. clearly share functional form difference lies edge weight edge weight used given constrained positive locally normalized. means order compute bottom-up pass evaluation give desired result root network. linear time space complexity follows linear time space inference complexity spns. words reduce original moment computation problem edge joint inference problem weights determined ααα. efﬁcient polynomial evaluation differentiation induced trees contain edge again exponential lower bound number unique induced trees brute force computation infeasible worst case. observation ctut tractable circuit representation since ctut/∂wkj) computed time evaluate also need computett∈tt ctut efﬁciently subset differential trick solve problem realizing fact multilinear function αkj/ lemma tt∈tt ctut proof. deﬁne αkj/ second equality corollary network polynomial multilinear function third equality holds trees contain wkj. last equality follows simple algebraic transformations. summary lemma holds fact differential operator applied multilinear function acts selector remark. fact compute differentiation w.r.t. using original circuit without expanding underlies many recent advances algorithmic design spns. zhao used differential trick design linear time collapsed variational algorithm concave-convex produce parameter estimation spns. different related approach differential operator taken w.r.t. input indicators model parameters applied computing marginal probability bayesian networks junction trees ﬁnish discussion concluding polynomial computed network multilinear function terms model parameters input indicators differential operator w.r.t. variable used efﬁcient compute subset monomials contain speciﬁc variable. hence formula shows computes ratio induced trees contain edge network. roughly speaking measures important contribution speciﬁc edge whole network polynomial. result interpret follows important edge portion moment comes observation. visualize moment computation method single edge fig. remark. cccp spns originally derived using sequential convex relaxation technique iteration concave surrogate function constructed optimized. update iteration cccp given follows wkjvjdk/vroot r.h.s. exactly deﬁned above. perspective cccp also understood implicitly applying differential trick compute i.e. relative importance edge take updates according importance measure. order compute moments edge weights naive computation would scale edges graph cor. computation takes time. observation allows reduce complexity linear comes structure depends three terms i.e. forward evaluation value backward differentiation value original weight edge wkj. implies dynamic programming cache bottom-up evaluation pass top-down differentiation pass respectively. high level trade constant factor space complexity reduce quadratic time complexity linear. theorem edges computed time space. proof. bottom-up evaluation pass order compute value vroot root also obtain values node graph. instead discarding intermediate cache allocating additional space node bottom-up evaluation pass network also node cost additional copy network. similarly top-down differentiation pass network chain rule also obtain intermediate node again cache them. edge moments weighted edges simultaneously. whole process requires bottom-up evaluation pass top-down differentiation pass time complexity |s|. since additional copies space complexity |s|. algorithm linear time exact moment computation input prior moment input output mp)∀. αkj∀. compute bottom-up evaluation pass input record node top-down differentiation pass input record node compute exact moment λkjmp section alg. sub-routine develop bayesian online learning algorithm spns based assumed density ﬁltering approximate distribution minimizing divergence one-step update posterior approximate distribution. dir} i.e. space product dirichlet densities decomposable nodes note since fully decomposable natural choice approximate distribution minimizes kl-divergence i.e. hard show exponential family distribution case setting minimization problem corresponds solving following moment matching equation vector sufﬁcient statistics dirichlet understood taken elementwise. principle ﬁnding approximate distribution also known reverse information projection literature information theory comparison information projection corresponds minimizing within family distributions utilizing efﬁcient linear time algorithm exact moment computation propose bayesian online learning algorithm spns based moment matching principle called assumed density ﬁltering pseudocode shown alg. algorithm edge moment matching equation amounts solving following equation digamma function. system nonlinear equations r.h.s. equation computed using alg. time edges efﬁciently solve take sides equation approximate l.h.s. using fact exp) expanding r.h.s. equation using identity have approximately mean prior dirichlet posterior adding pseudo-count wkj. essentially ﬁnding posterior hyperparameter posterior mean approximately weighted geometric mean means given instead matching moments given sufﬁcient statistics also known natural moments tries approximate distribution matching ﬁrst order moments i.e. mean prior one-step update posterior. using notation want match following equation again interpret equation posterior hyperparameter posterior mean given weighted arithmetic mean means given weighted λkj. notice normalization constraint cannot solve directly equations order solve need equation added system. however line alg. need next iteration algorithm normalized version. additional equation update formula directly algorithm. using alg. sub-routine enjoy linear running time sharing order time complexity cccp. however since cccp directly optimizes data log-likelihood practice observe cccp often outperforms log-likelihood scores. propose optimal linear time algorithm efﬁciently compute moments model parameters spns online settings. techniques used design algorithm include liner time reduction moment computation joint inference differential trick able efﬁciently evaluate multilinear function dynamic programming reduce redundant computations. using proposed algorithm sub-routine able improve time complexity quadratic linear general spns structures. also proposed algorithm sub-routine design online algorithm adf. future direction hope apply proposed moment computation algorithm design efﬁcient structure learning algorithms spns. also expect analysis techniques develop might uses learning spns. references boutilier friedman goldszmidt koller. context-speciﬁc independence bayesian networks. proceedings twelfth international conference uncertainty artiﬁcial intelligence pages morgan kaufmann publishers inc. jaini rashwan zhao banijamali chen poupart. online algorithms sum-product networks continuous variables. proceedings eighth international conference probabilistic graphical models pages rashwan zhao poupart. online distributed bayesian moment matching parameter learning sum-product networks. proceedings international conference artiﬁcial intelligence statistics pages", "year": 2017}