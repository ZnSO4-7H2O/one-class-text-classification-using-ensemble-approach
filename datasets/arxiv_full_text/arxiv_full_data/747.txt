{"title": "Fitness inheritance in the Bayesian optimization algorithm", "tag": ["cs.NE", "cs.AI", "cs.LG", "G.1.6; G.3; I.2.6; I.2.8"], "abstract": "This paper describes how fitness inheritance can be used to estimate fitness for a proportion of newly sampled candidate solutions in the Bayesian optimization algorithm (BOA). The goal of estimating fitness for some candidate solutions is to reduce the number of fitness evaluations for problems where fitness evaluation is expensive. Bayesian networks used in BOA to model promising solutions and generate the new ones are extended to allow not only for modeling and sampling candidate solutions, but also for estimating their fitness. The results indicate that fitness inheritance is a promising concept in BOA, because population-sizing requirements for building appropriate models of promising solutions lead to good fitness estimates even if only a small proportion of candidate solutions is evaluated using the actual fitness function. This can lead to a reduction of the number of actual fitness evaluations by a factor of 30 or more.", "text": "paper describes ﬁtness inheritance used estimate ﬁtness proportion newly sampled candidate solutions bayesian optimization algorithm goal estimating ﬁtness candidate solutions reduce number ﬁtness evaluations problems ﬁtness evaluation expensive. bayesian networks used model promising solutions generate ones extended allow modeling sampling candidate solutions also estimating ﬁtness. results indicate ﬁtness inheritance promising concept population-sizing requirements building appropriate models promising solutions lead good ﬁtness estimates even small proportion candidate solutions evaluated using actual ﬁtness function. lead reduction number actual ﬁtness evaluations factor more. ensure reliable convergence global optimum genetic evolutionary algorithms must often maintain large population candidate solutions number iterations. however many real-world problems ﬁtness evaluation computationally expensive evaluating even moderately sized populations candidate solutions intractable. example ﬁtness evaluation include large ﬁnite element analysis consist complex traﬃc simulation require interaction human expert. leads interesting question would possible make geas evolve population candidate solutions also model ﬁtness could used evaluate certain proportion newly generated candidate solutions fortunately answer question positive studies made support argument. methods proposed ﬁtness inheritance simple genetic algorithm univariate marginal distribution algorithm purpose paper propose method uses models promising solutions developed bayesian optimization algorithm model ﬁtness landscape estimate ﬁtness newly generated candidate solutions. types models considered traditional bayesian networks full conditional probability tables used bayesian networks local structures used decision graphs hierarchical since model captures signiﬁcant nonlinearities ﬁtness landscape using model basis developing model ﬁtness landscape seems promising approach. course methods neural networks various regression models could used instead. proposed method examined decision trees three example problems onemax concatenated traps order concatenated traps order results indicate ﬁtness inheritance beneﬁcial even less candidate solutions evaluated using actual ﬁtness function. turns population sizing requirements creating correct model promising solutions ﬁtness inheritance better. paper starts discussing previous ﬁtness inheritance studies. section presents proposed method ﬁtness inheritance boa. section presents discusses experimental results. section summarizes concludes paper. probabilistic model-building genetic algorithms replace traditional variation operators genetic evolutionary algorithms building probabilistic model promising solutions sampling model generate candidate solutions. bayesian optimization algorithm uses bayesian networks model candidate solutions. evolves population candidate solutions given problem. ﬁrst population candidate solutions usually generated randomly according uniform distribution solutions. population updated number iterations using basic operators selection variation. selection operator selects better solutions expense worse ones current population yielding population promising candidates. variation operator starts learning probabilistic model selected solutions encodes features promising solutions inherent regularities. bayesian networks used model promising solutions bayesian networks among powerful tools capturing representing decomposition inherent feature complex real-world systems. variation operator proceeds sampling probabilistic model generate solutions incorporated original population. here simple replacement scheme used solutions fully replace original population. detailed description found pelikan bayesian networks among popular graphical models statistics modularity graph theory combined practical tool estimating probability distributions inference. bayesian network deﬁned components structure parameters. structure encoded directed acyclic graph nodes corresponding variables modeled data edges corresponding conditional dependencies. parameters represented conditional probability tables specifying conditional probability variable given instance variables variable depends directed edge relates variables encoded distribution variable corresponding terminal node conditioned variable corresponding initial node. incoming edges node result conditional probability variable condition containing parents. addition encoding dependencies bayesian network encodes independence assumptions. independence assumptions state variable independent antecedents ancestral ordering given values variable’s parents. learn bayesian networks greedy algorithm usually used eﬃciency robustness. greedy algorithm starts empty bayesian network. iteration adds edge network improves quality network most. network quality measured popular scoring metric bayesian networks bayesian dirichlet metric likelihood equivalence bayesian information criterion learning terminated improvement possible. conditional probability tables store conditional probabilities variable number conditional probabilities variable conditioned parents grows exponentially binary variables instance number conditional probabilities instances parents suﬃcient store probability variable instance. figure shows example nonetheless dependencies sometimes also contain regularities. furthermore exponential growth full cpts often obstructs creation models accurate eﬃcient. bayesian networks often extended local structures allow eﬃcient representation local conditional probability distributions full cpts tree variable associated edges connecting node children stand diﬀerent values variable. binary variables edges coming internal node; edge corresponds whereas edge corresponds values either edge used value values classiﬁed several categories category would create edge. path decision tree starts root tree ends leaf encodes constraints values variables leaf stores value conditional probability given condition speciﬁed path root tree leaf. decision tree encode full conditional probability table variable parents splits leaves corresponding unique condition. however decision tree enables eﬃcient ﬂexible representation local conditional distributions. figure example decision tree conditional probability table presented earlier. decision graph allows edges terminate single node. words internal nodes decision tree allowed share children result node parent. makes representation even ﬂexible. however experience indicates that decision graphs usually provide better performance decision trees. figure example decision graph. learn bayesian networks decision trees decision tree variable initialized empty tree univariate probability iteration leaf decision tree split determine quality current network improves executing split best split performed. learning ﬁnished splits improve current network anymore. quality model estimated using popular scoring metric. combination metrics score penalized number bits required encode parameters decision graphs merge operation introduced allow merging leaves decision graph. despite importance ﬁtness inheritance robust population-based search surprisingly studies ﬁtness inheritance found. section reviews important studies. smith dike stegmann proposed approaches ﬁtness inheritance simple ﬁrst approach compute ﬁtness oﬀspring average ﬁtness parents. second approach consider weighted average based similar oﬀspring parent. results indicated ﬁtness inheritance outperformed without inheritance. however study ﬁtness inheritance consider effects ﬁtness inheritance crucial parameters population size number generations. result speed-up achieved using ﬁtness inheritance could estimated properly. sastry goldberg pelikan considered univariate marginal distribution algorithm simplest pmbgas. using ﬁtness inheritance umda introduces challenges umda two-parent recombination therefore diﬃcult direct correspondence parents oﬀspring. instead sastry extend probabilistic model allow estimating ﬁtness newly sampled candidate solutions. umda models population promising solutions selection using probability vector stores probability position. probabilities used sample candidate solutions. incorporate ﬁtness inheritance probability vector extended include additional statistics string position term denotes average ﬁtness solutions analogously term denotes average ﬁtness solutions equal ﬁtness solution estimated sastry also developed theory ﬁtness inheritance umda onemax estimates number actual ﬁtness evaluations given proportion candidate solutions inherits ﬁtness whereas remaining candidate solutions evaluated using actual ﬁtness. basic idea start adapting population sizing time-to-convergence models umda ﬁtness inheritance relate quantities counterparts standard umda. optimal population size used cases sastry showed evaluations saved. however population size used cases number evaluations decreases factor three. section describes ﬁtness model built updated using bayesian networks candidate solutions evaluated using model. bayesian networks full cpts well ones local structures discussed. section also discusses statistics acquired built accurate ﬁtness model. umda probabilities position form probability vector coupled average ﬁtness position. analogically bayesian networks extended incorporate average ﬁtness statistic stored model. every variable possible value average ﬁtness solutions must stored instance xi’s parents binary case conditional probability table thus extended additional entries. figure shows example conditional probability table extended ﬁtness information based conditional probability table presented figure ﬁtness estimated similar method full cpts used incorporate ﬁtness information bayesian networks decision trees graphs. average ﬁtness instance variable must stored every leaf decision tree graph. figure shows example decision tree graph extended ﬁtness information based decision tree graph presented earlier figure ﬁtness averages leaf restricted solutions satisfy condition speciﬁed path root tree leaf. still faced following question obtain information compute statistics used ﬁtness inheritance? speciﬁcally instance instance xi’s parents must compute average ﬁtness solutions sources computing ﬁtness-inheritance statistics reason restricting computation ﬁtness-inheritance statistics selected parents oﬀspring probabilistic model used basis selecting relevant statistics represents nonlinearities population parents population oﬀspring. since best maximize learning data available seems natural populations compute ﬁtness-inheritance statistics. reason restricting input computing statistics solutions evaluated using actual ﬁtness function ﬁtness solutions estimated involves errors could mislead ﬁtness inheritance propagate generations. using solutions evaluated using actual ﬁtness function incorporating oﬀspring estimating inheritance statistics diﬀers previous ﬁtness inheritance studies denotes input binary string bits. onemax ﬁtness contribution independent context. simple model used umda considers variable independently variables suﬃces yields convergence optimum evaluations. however models bounded complexity work well practically crossover operator used standard also suﬃce. model ﬁtness developed average ﬁtness leaf approximately whereas average ﬁtness leaf approximately result solutions penalized would rewarded average ﬁtness vary throughout run. paper considers onemax bits. concatenated -bit traps input string ﬁrst partitioned independent groups bits each. partitioning unknown algorithm change run. -bit trap function applied group bits contributions traps added together form ﬁtness. -bit trap deﬁned follows unlike onemax depend state search distribution contexts changes time bits trap independent. context leaf also determines whether leaf. paper considers trap consisting copies -bit trap total number bits concatenated traps order deﬁned analogically traps order instead dealing groups bits groups bits considered. contribution group bits computed average ﬁtness values depend context similarly traps order thus follow similar dynamics. paper considers trap consisting copies -bit trap total number bits test problem following ﬁtness inheritance proportions considered step step step test problem ﬁtness inheritance proportion independent experiments performed. experiment consisted independent runs minimum population size ensure convergence solution within optimum runs. experiment bisection method used determine minimum population size number evaluations recorded. average runs experiments computed displayed function proportion candidate solutions ﬁtness estimated using ﬁtness model. speed-up also computed equal factor number evaluations decreases compared case inheritance. results onemax traps order traps order shown ﬁgures experiments number actual ﬁtness evaluations decreases inheritance proportion reaches optimum proportion candidate solutions ﬁtness inheritance means considering actual ﬁtness evaluations evaluating less candidate solutions actual ﬁtness seems beneﬁcial. number evaluations actual ﬁtness decreased factor onemax trap order trap order although actual savings depend problem considered expected ﬁtness inheritance enables signiﬁcant reduction ﬁtness evaluations many problems deceptive problems bounded diﬃculty bound large class important problems. considering actual ﬁtness evaluations ignores time complexity selection model construction generation candidate solutions ﬁtness estimation. combining factors complexity estimate actual ﬁtness evaluation used compute optimal proportion candidate solutions evaluate using ﬁtness inheritance. nonetheless results presented paper clearly indicate using ﬁtness inheritance reduce number solutions must evaluated using actual ﬁtness function factor more. consequently ﬁtness evaluation bottleneck space improvement using ﬁtness inheritance boa. fitness inheritance enables genetic evolutionary algorithms evaluate certain proportion candidate solutions using actual ﬁtness function ﬁtness remaining solutions computed using model ﬁtness landscape updated using ﬁtness models updated used eﬃciently signiﬁcantly speed solution problems ﬁtness evaluation paper showed ﬁtness inheritance yields moderate speed-ups simple umda beneﬁts using ﬁtness inheritance become signiﬁcant. rather large population-sizing requirements creating adequate probabilistic model promising solutions number actual function evaluations decreases even less candidate solutions evaluated using actual ﬁtness function ﬁtness remaining solutions estimated using model. important result advanced pmbgas often require large populations evaluating large populations become intractable problems computationally expensive ﬁtness evaluation. increasing proportion candidate solutions evaluated using ﬁtness model results greater population-sizing requirements optimal inheritance proportion depends complexity building sampling model promising solutions well evaluating solutions using actual ﬁtness function. good news complex evaluation function higher proportions candidate solutions evaluated using model ﬁtness instead actual ﬁtness function. important topic future work incorporate ﬁtness inheritance presence niching lead accumulation candidate solutions whose ﬁtness overestimated. resolving problem would enable ﬁtness inheritance hierarchical combines local structures niching. another important topic develop theory extends theoretical work ﬁtness inheritance umda competent gas. finally important apply proposed ﬁtness inheritance model solve challenging real-world problems expensive ﬁtness evaluation. authors would like thank david goldberg discussions comments. part work supported research award university missouri louis research board university missouri. experiments done asgard cluster institute theoretical physics swiss federal institute technology z¨urich. hboa software used pelikan developed martin pelikan david goldberg university illinois urbana-champaign.", "year": 2004}