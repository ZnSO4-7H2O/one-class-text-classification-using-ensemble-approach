{"title": "Synthesizing Novel Pairs of Image and Text", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "Generating novel pairs of image and text is a problem that combines computer vision and natural language processing. In this paper, we present strategies for generating novel image and caption pairs based on existing captioning datasets. The model takes advantage of recent advances in generative adversarial networks and sequence-to-sequence modeling. We make generalizations to generate paired samples from multiple domains. Furthermore, we study cycles -- generating from image to text then back to image and vise versa, as well as its connection with autoencoders.", "text": "generative adversarial networks generative image modeling seen tremendous improvements past years emergence deep learning techniques. generative adversarial networks pioneered goodfellow since opened doors image generation deep-convolution generative models paper generative adversarial text image synthesis reed provided multi-stage approach generating image text authors ﬁrst learned sequence encoder learns discriminative text feature representations. authors trained deep-convolution generative model conditioned text embeddings. approach provided performant generate images based text. side text image problem generating natural language descriptions images seen tremendous improvements clever amalgamation deep convolution networks recurrent nerual networks. techniques seqseq sutskever shown great potential natural language processgenerating novel pairs image text problem combines computer vision natural language processing. paper present strategies generating novel image caption pairs based existing captioning datasets. model takes advantage recent advances generative adversarial networks sequence-to-sequence modeling. make generalizations generate paired samples multiple domains. furthermore study cycles generating image text back image vise versa well connection autoencoders. scarcity availability image captioned datasets raises question whether it’d feasible synthesize artiﬁcial samples contain high quality pairs image text. application could help improve performance neural networks tasks image captioning image classiﬁcation. commercial applications also include content creation drug discovery news entertainment. recent advances generative adversarial networks sequence sequence modeling made image text text image feasible. work combines paradigms explore implications combining models. main accomplishments include best knowledge ﬁrst study synthesizing novel pairs image text analysis cycles image text back image comparison autoencoders. tasks machine translation show tell neural image caption generator vinyals demonstrated descriptive coherent captions generated feeding last convolutional activations image classiﬁcation network long short term memory network dataset augmentation inspired advances tackle problem dataset augmentation synthesizing novel image text pairs. plethora work detailing ways creating additional samples imbalanced learning literature examples created minority class order prevent overﬁtting. synthetic minority over-sampling technique examples created featurespace randomly selecting pairs original dataset adasyn sample generated combining existing sample weighted differences sample neighbors. parsimonious mixture gaussian trees samples created ﬁrst ﬁtting existing minority class mixture gaussians sampling take inspiration much literature imbalanced learning novel samples generated minority class order improve learning. section paper cover re-purposing reed al.’s gan-cls generate image text pairs well connection autoencoders. section paper show qualitative results analysis approach. paper utilize neural framework generate images text. recent advances generative adversarial network shown feasibility translating image text well translating text image combining gain ability generate novel image text pairs. case source domain text ﬁrst sample text generate image based text sample; vise versa ﬁrst sample image generate text caption based image sample. source domain generation source domain generation problem problem given examples v..n generate novel samples vnew. problem requires construct novel examples instead sampling existing dataset. high dimensionality image text domain instead generate novel embeddings instead. encoding text image learned training phase gangls. last convolutional activation discriminator encoder generator conditioned order construct φnew ψnew propose approaches inspired existing imbalance class learning literature prototype-based density-based. prototype based ﬁrst method make modiﬁcations existing samples prototypes. prototypes examples dataset. contribution draws heavily techniques smote synthesize examples based pairs existing examples. estimate using lstm. unlike vinyals instead training convolutional neural network take advantage discriminator trained conditional section utilizing last convolutional layer note gan-cls architecture concatenated textual information means contains information needed distinguish real images wrong captions wrong images right captions. leads conclude capability generating descriptive captions. g))) relation autoencoders traditional autoencoder tries minimize reconstruction loss input although directly minimizing loss formulation cycles close ties autoencoders. major advantage using existing prototypes prototypes easily identiﬁed. prototype method extended sampling conditioned additional information source class. density based density-based source domain generation learn generative model sample distribution source domain. sample learned distribution obtain additional examples source domain. target domain generation generating source domain want generate target domain conditioned source domain. leads formulation text image image text intuition behind loss forces discriminator learn discern generated images right captions real images wrong captions wrong images right captions therefore expect information loss within embedding space information shared domains minimal. means close original reconstructed images. however expect cos) also similar since maximize probability data conditioned shared information image text. experimented oxford ﬂowers dataset consists classes ﬂowers images along captions image. using model able successfully generate novel pairs image text. figure source domain text target domain images. figure interpolate text embeddings linearly interpolating demonstrates ability construct novel images based text. figure combining captions create image. example prototype based source domain generation. leftmost image generated based caption ﬂower rightmost image generated based caption ﬂower blue. images between linear interpolations text embedding figure combining images create caption. another example prototype based source domain generation. caption right generated combining embeddings images left figure creating ﬂower images sampling density model learned text embedding. example density based source domain generation. image contains examples sampled cluster gaussian mixture model. note homogeneity images within cluster. cycle generation figure section shows example cycle image text image. note caption image preserve semantic attributes ﬂowers instead pixel values. hand text image text figure shows similar behavior meaning text stays same exact words. propose novel generating pairs image text repurposing existing gancls architecture. method extended generating pairs novel samples multiple domains. additionally study implication cycles going image text image vise versa well compare model autoencoders. hong-cao vicent john pang parsimonious mixture gaussian trees model oversampling imbalanced multimodal timeseries classiﬁcation ieee volume issue dec.", "year": 2017}