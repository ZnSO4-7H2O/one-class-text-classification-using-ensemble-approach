{"title": "Analyzing sparse dictionaries for online learning with kernels", "tag": ["stat.ML", "cs.CV", "cs.IT", "cs.LG", "math.IT"], "abstract": "Many signal processing and machine learning methods share essentially the same linear-in-the-parameter model, with as many parameters as available samples as in kernel-based machines. Sparse approximation is essential in many disciplines, with new challenges emerging in online learning with kernels. To this end, several sparsity measures have been proposed in the literature to quantify sparse dictionaries and constructing relevant ones, the most prolific ones being the distance, the approximation, the coherence and the Babel measures. In this paper, we analyze sparse dictionaries based on these measures. By conducting an eigenvalue analysis, we show that these sparsity measures share many properties, including the linear independence condition and inducing a well-posed optimization problem. Furthermore, we prove that there exists a quasi-isometry between the parameter (i.e., dual) space and the dictionary's induced feature space.", "text": "abstract—many signal processing machine learning methods share essentially linear-in-the-parameter model many parameters available samples kernel-based machines. sparse approximation essential many disciplines challenges emerging online learning kernels. several sparsity measures proposed literature quantify sparse dictionaries constructing relevant ones proliﬁc ones distance approximation coherence babel measures. paper analyze sparse dictionaries based measures. conducting eigenvalue analysis show sparsity measures share many properties including linear independence condition inducing well-posed optimization problem. furthermore prove exists quasi-isometry parameter space dictionary’s induced feature space. advent data deluge data illustrated extensive literature compressed sensing references therein). sparsity promoting crucial signal processing machine learning gaussian processes kernel-based methods bayesian learning well neural networks pruning recent dropout principle deep learning many learning machines share essentially model linear nonlinear kernel form including support vector machines gaussian processes radial-basisfunction networks resource-allocating networks recently neural networks function approximation also seminal work poggio smale learning machines rely well-known representer theorem deﬁnes linear-in-the-parameters model many parameters training samples. sparse approximation model often required many interesting desirable properties enforcing interpretation results providing computational tractable problem large-scale datasets. issue studied within last years kernel-based machines recent developments sparse approximation compressed sensing open advances. moreover online learning brings challenges sparsity signal processing machine learning sample available instant thus leads incrementation number parameters. therefore needs control complexity growth selecting samples take part model formulation; literature contributing samples called atoms collected called dictionary. construction available samples pertinent dictionary measure relevance investigated literature several sparsiﬁcation criteria coupled sparsity measure deﬁnes diversity captured dictionary. oldest sparsity criterion distance introduced controlling complexity structure radial-basis-function networks resource-allocating networks also recent advances. criterion constructs dictionary lowerbounding pairwise distance atoms. another criterion approximation criterion explores deeper analysis atoms lower-bounding error approximating atom atoms investigated gaussian processes kernel recursive least squares algorithm recently kernel principal component analysis. third criterion takes advantage recent developments sparse approximation literature compressed sensing upper-bounding coherence pair atoms. initially introduced online learning kernels learning sensor networks extensively considered one-class classiﬁcation online learning multiple kernels multiple dictionaries multiple-output learning babel measure criterion provide comprehensive analysis dictionary structure limiting cumulative coherence best knowledge work studies sparsity measures criteria. independently sparsiﬁcation criterion resulting dictionary many algorithms introduced update model. might expected wide class linear adaptive ﬁlters extensively investigated online learning kernels revisiting popular algorithms least mean squares normalized afﬁne projection recursive least squares algorithms; instance review linear adaptive ﬁlters. exists frameworks develop adaptive algorithms online learning kernels thanks underlying linear-in-the-parameters model functional framework dual one. within functional framework optimization operated feature space estimating updating within subspace spanned atoms dictionary. framework widely investigated online learning kernels; instance well reference known work reference recent work eigenvalues lower bounds eigenvalues upper bounds linear independence condition number isometry property distances isometry property inner products evaluated sample using κih. property shows sample represented space also called feature space. moreover reproducing property leads so-called kernel trick pair samples commonly used kernels linear kernel polynomial kernel gaussian kernel theorem shows functional optimization problem equivalent estimation unknowns injecting expression dual problem. duality illustrated next kernel ridge regression problem. heoretical analysis comprehensive study. second framework based estimating parameters model thus solving optimization problem so-called dual space. framework extensively explored literature simplicity nlms algorithm algorithm algorithm overview framework references therein. best knowledge yukawa pointed distinction frameworks relationship frameworks studied before namely connecting feature space dual space. paper study aforementioned sparsity measures sparsiﬁcation criteria provide analysis eigenvalues associated sparse dictionary provide upper lower bounds terms sparsity measures show lower bounds provide conditions linear independence atoms moreover show condition number gram matrix associated sparse dictionary upper-bounded illustrating impact sparsity measures conditioning optimization problem major result provided paper connection dictionary’s induced feature space dual space showing exists quasi-isometry spaces dealing sparse dictionaries. results allow bridge gaps aforementioned frameworks picture illustrated table considering given loss function deﬁned measures error desired output estimated optimization problem consists minimizing regularized empirical risk follows space candidate functions controls tradeoff ﬁtness error regularity solution monotonically increasing function. examples loss functions quadratic loss hinge loss yi)+ support vector machines. using formalism reproducing kernel hilbert space space candidate functions kernelbased machines incorporate prior knowledge using kernel. reproducing kernel induced rkhs inner product. reproducing property states function order ﬁxed controlled chosen available samples instant namely `xm} xt}. denote dictionary atoms elements space spanned paper restrictive unit-norm atoms. throughout paper quantities associated dictionary accent case instance m-by- vector whose j-th entry gram matrix size m-by-m whose entry eigenvalues matrix denoted given non-increasing order. studying section dictionary terms sparsity measures sparsiﬁcation criteria constructing relevant dictionary assume dictionary known. problem determining model solved ways functional framework updated dual framework update parameter vector αt−. frameworks summarized next starting latter since vector-based formulation straightforward. consider sample selected available samples using notation paper opposed elements expansion need samples drawn distribution. difference investigated updating instant order minimize prediction error. throughout paper outline special case unit-norm atoms since setting often considered literature. unit-norm atoms arise dealing either linear kernel unit-norm kernel namely regularization dual optimization problem essentially tikhonov regularization form kγαk literature tikhonov matrix often chosen identity matrix multiplicative constant giving preference solutions smaller norms. kernel ridge regression becomes connections regularization functional space kψkh regularization dual space straightforward. result based α⊤kα therefore fact rayleigh’s quotient courant-fischer minimax theorem λmin λmax smallest largest eigenvalues gram matrix consequence minimizing yields upper bound norm parameter vector minimizing yields following upper bound norm functional space tighter bounds studied detail section representer theorem linear-in-the-parameters model constitutes bottleneck online learning required real-time system identiﬁcation big-data processing distributed optimization indeed online setting solution updated recursively based information available instant namely novel instant thus including pair training representer theorem dictates parameter added unknowns. consequence order linear-in-theparameters model continuously increasing. overcome drawback needs control growth model order instant keeping fraction kernel functions expansion reduced-order model instant takes form expression often simpliﬁed equivalence granted matrix nonsingular assumption unfortunately satisﬁed general. linear dependence training samples. also removal process latter case order provide ﬁxed-budget learning discarding atom least contribution diversity dictionary investigated instance threshold parameter sparsiﬁcation criterion related novelty criterion given sparsiﬁcation criterion without scaling factor followed prediction error mechanism. distance measure deﬁned relies atoms closest pair dictionary. comprehensive analysis dictionary composition capacity approximating atom linear combination atoms. dictionary designated δ-approximate following satisﬁed unfortunately formulations assume ﬁniteness training reported drawback fact model kernel function instant. order control growth restrict span dictionary replace current projection onto subspace spanned dictionary namely `κxt appendix details. leads expression iii. online sparsification sparsity measures independently investigated framework online learning algorithms coupled sparsiﬁcation scheme. instant dictionary updated necessary left unchanged. indeed dictionary augmented whenever novel kernel function increases diversity dictionary. exists several sparsity measures quantify diversity described following. norm perspective coherence essentially ∞-norm dealing unit-norm atoms. babel notion explores analogy norm operator thus providing complete description dictionary structure babel related -norm gram matrix deﬁnition corresponds maximum cumulative correlation atom atoms dictionary. easy that dealing unit-norm atoms coherence dictionary cannot exceed babel measure. babel criterion deﬁned follows. candidate function included dictionary given positive threshold deﬁnition viewed extension coherence criterion sense approximation extension distance criterion. babel measure sparsiﬁcation. since gram matrix fundamental analysis dictionary study following eigenvalues provide theoretical bounds. results provide analysis span deﬁned sparse dictionary given terms sparsity measure scrutiny. lower bounds used forthcoming linear independence analysis lower upper bounds investigated forthcoming study condition number main results derived next section eigenvalues matrix given non-increasing order namely `λm. proceeding bring mind well-known gerˇsgorin discs theorem revisited gram matrix sparse dictionary. also well known trace matrix equals eigenvalues. trace theorem cornerstone study described next providing upper lower bounds eigenvalues gram matrix associated sparse dictionary investigating sparsity measure. approximation criterion based constructing dictionary high approximation measure investigated gaussian processes kernel-based ﬁlter recently kernel principal component analysis kernel function added dictionary coherence fundamental measure characterize dictionary literature sparse approximation. corresponds largest correlation atoms given dictionary mutually atoms dictionaries. coherence measure investigated analysis quality representing signal dictionary initially work recently abundant publications compressed sensing work consider linear measure explore following coherence kernel functions order derive coherence criterion initially proposed coherence criterion constructs low-coherent dictionary includes candidate kernel function dictionary coherence latter exceed given threshold namely condition enforces upper bound cosine angle pair kernel functions. threshold controls level sparseness dictionary null value yields orthogonal basis. criterion computationally efﬁcient given expression denominator reduces unit-norm atoms thus becomes case important measure sensitivity respect variations within matrix resolution problem form unknown. gives bound inaccurate solution approximation. value small i.e. close solution robust perturbations opposed large values lead illconditioned problems even ill-posed. instant consider gradient descent procedure solve linear system shown error reduction iteration bounded upper bound proportional condition number matrix condition number studied recently kernelbased machine learning; instant next provide upper bound condition number terms sparsity measure dictionary. proof following theorem straightforward deﬁnition condition number aforementioned theorems lower upper bounds eigenvalues. case unit-norm atoms obtained relation yields instance upper bound γ-coherent dictionary. results demonstrate choice threshold value sparsiﬁcation criterion impacts conditioning system towards wellposed optimization problem. section show feature subspace dual space intimately related topologies feature subspace spanned atoms sparse dictionary. show section pairwise distances spaces almost preserved. quasiisometry property associated given sparse dictionary quantiﬁed terms sparsity measures presented section namely distance approximation coherence babel measures. results isometry extended section issue preserving pairwise inner-products spaces. results establish relevant construct dictionary linearly independent atoms condition allows represent feature unique linear way. dictionary kernel functions atoms linearly independent following satisﬁed linear combination zero element weighting coefﬁcients null. trivial dictionary nonzero approximation following show sparsity measures provide sufﬁcient conditions linear independence dictionary’s atoms. investigate duality linear independence singularity associated gram matrix essentially considered coherence linear dictionary unit-norm atoms extended kernel-based dictionaries. indeed courant-fischer minimax theorem used consequence prove linear independence atoms providing lower bound eigenvalues associated gram matrix. following theorem summarizes property different sparsity measures. exploring results derived section identify isometry constants dictionary terms distance approximation coherence babel measures. besides approximation measure expressions straightforward theorems thanks bounds eigenvalues symmetric even asymmetric bounds approximation measure given theorem easily identify expression isometry constant dealing non-unit-norm atoms expressions difﬁcult derive asymmetry bounds eigenvalues shown following theorem. lower bound upper bound order bounds symmetric deﬁnition divide term yields isometry constant rescaled atoms dictionary atom divided finally proof theorems show sparse dictionary provides quasi-isometry respect distances dual space subspace spanned atoms. following show property quasi-isometry extends inner products. worth noting that dealing total isometry isometry respect inner products extends naturally isometry respect distances vice versa. case using quasi-isometry deﬁnition. bridge following. linear operator inner product space another inner product space exists equivalence kauk u.this equivalence less obvious dealing quasi-isometry. worth noting results require atoms dictionary linear independent since condition uniquely repguarantees feature resented atoms dictionary. section iv-b particular theorem provides weak conditions terms sparsity measure dictionary. without limiting online learning comparing consider features feature space denoted representations dual space denoted respectively. exists isometry spaces distance pair features corresponds distance parameter vectors namely ψ′′kh α′′k. isometry property restrictive relax following deﬁnition quasi-isometry showing quotient distances close unity. denote ψ′−ψ′′ parameter vector α′−α′′. deﬁnition given dictionary kernel functions space spanned atoms spaces quasi-isometric exists isometry constant that vector entries feature satisﬁes following show quasi-isometry property satisﬁed sparse dictionaries relying investigated sparsity measure. generalizing theorem restrict theorem case unit-norm atoms often sufﬁcient work literature sparse approximation e.g. using gaussian kernel. theorem dictionary unit-norm atoms isometry constant deﬁned follows baraniuk candes nowak vetterli eds. ieee signal processing magazine special issue sensing sampling compression vol. ieee signal processing society march srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol. sch¨olkopf herbrich smola generalized representer theorem proc. annual conference computational learning theory european conference computational learning theory colt/eurocolt springer-verlag matrix second easy matrices share eigenvectors eigenvalue corresponds eigenvalue k−i. indeed eigenpair satisﬁes `k−i therefore eigenpair matrix paper provided framework based eigenvalue analysis study sparsity measures sparsiﬁcation criteria. proposed uniﬁed study well-conditioning optimization problem condition uniqueness solution. established quasi-isometry dual space dictionary’s induced feature space thus connecting functional dual frameworks illustrating impact sparsity measures topologies. future work extending framework include insights sparse dictionary analysis. sch¨olkopf mika burges knirsch k.-r. m¨uller r¨atsch smola input space versus feature space kernelbased methods ieee trans. neural networks vol. vukovi´c miljkovi´c growing pruning sequential learning algorithm hyper basis function neural network function approximation neural netw. vol. oct. honeine essoloh richard snoussi distributed regression sensor networks reduced-order kernel model proc. ieee globecom global communications conference honeine richard snoussi bermudez chen decentralized approach non-linear prediction time series data sensor networks journal wireless communications networking vol. special issue theoretical algorithmic foundations wireless sensor networks jan. ishida tanaka multikernel adaptive ﬁlters multiple dictionaries regularization signal information processing association annual summit conference asiapaciﬁc crit`ere coh´erence actes xxi-`eme colloque gretsi traitement signal images september honeine m´ethodes noyau pour l’analyse d´ecision environnement non-stationnaire. thesis m´emoire th`ese doctorat optimisation sˆuret´e syst`emes ecole doctoral ssto troyes france said´e lengell´e honeine richard achkar dictionary adaptation online prediction time series data kernels proc. ieee workshop statistical signal processing august said´e honeine lengell´e richard achkar adaptation ligne d’un dictionnaire pour m´ethodes noyau actes -`eme colloque gretsi traitement signal images september vaerenbergh santamaria principe fixedbudget kernel recursive least-squares acoustics speech signal processing ieee international conference march gilbert muthukrishnan strauss approximation functions redundant dictionaries using coherence proc. annual acm-siam symposium discrete algorithms society industrial applied mathematics gilbert muthukrishnan strauss tropp improved sparse approximation quasi-incoherent dictionaries international conference image processing vol. sept. paul honeine born beirut lebanon october received dipl.-ing. degree mechanical engineering m.sc. degree industrial control faculty engineering lebanese university lebanon. received ph.d. degree systems optimisation security university technology troyes france postdoctoral research associate systems modeling dependability laboratory since september assistant professor university technology troyes france. research interests include nonstationary signal analysis classiﬁcation nonlinear statistical signal processing sparse representations machine learning. particular interest applications sensor networks biomedical signal processing hyperspectral imagery nonlinear adaptive system identiﬁcation. co-author best paper award ieee workshop machine learning signal processing. past years published peerreviewed papers.", "year": 2014}