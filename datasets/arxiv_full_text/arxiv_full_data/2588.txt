{"title": "Missing Data Estimation in High-Dimensional Datasets: A Swarm  Intelligence-Deep Neural Network Approach", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In this paper, we examine the problem of missing data in high-dimensional datasets by taking into consideration the Missing Completely at Random and Missing at Random mechanisms, as well as theArbitrary missing pattern. Additionally, this paper employs a methodology based on Deep Learning and Swarm Intelligence algorithms in order to provide reliable estimates for missing data. The deep learning technique is used to extract features from the input data via an unsupervised learning approach by modeling the data distribution based on the input. This deep learning technique is then used as part of the objective function for the swarm intelligence technique in order to estimate the missing data after a supervised fine-tuning phase by minimizing an error function based on the interrelationship and correlation between features in the dataset. The investigated methodology in this paper therefore has longer running times, however, the promising potential outcomes justify the trade-off. Also, basic knowledge of statistics is presumed.", "text": "abstract. paper examine problem missing data high-dimensional datasets taking consideration missing completely random missing random mechanisms well arbitrary missing pattern. additionally paper employs methodology based deep learning swarm intelligence algorithms order provide reliable estimates missing data. deep learning technique used extract features input data unsupervised learning approach modeling data distribution based input. deep learning technique used part objective function swarm intelligence technique order estimate missing data supervised ﬁne-tuning phase minimizing error function based interrelationship correlation features dataset. investigated methodology paper therefore longer running times however promising potential outcomes justify trade-oﬀ. also basic knowledge statistics presumed. previous research across wide range academic ﬁelds suggests decisionmaking data analysis tasks made nontrivial presence missing data. such assumed decisions likely accurate reliable complete/representative datasets used instead incomplete datasets. assumption research data mining domain novel techniques developed perform task accurately research suggests applications various professional ﬁelds medicine manufacturing energy sensors instruments report vital information enable decision-making processes fail lead incorrect outcomes presence missing data. cases important system capable imputing missing data failed sensors high accuracy. imputation procedure require approximation missing values taking account interrelationships exist data sensors system. another instance presence missing data poses threat decision-making image recognition systems whereby absence pixel values renders image prediction classiﬁcation task diﬃcult such systems capable imputing missing values high accuracy needed make task feasible. consider high dimensional dataset mixed national institute standards technology dataset feature variables pixel values shown fig. above. assuming pixel values missing random observed bottom statistic analysis required classify dataset questions interest would impute degree certainty missing data high dimensional datasets high accuracy? techniques introduced approximation missing data correlation interrelationships variables considered? paper therefore aims deep learning technique built restricted boltzmann machines stacked together form autoencoder tandem swarm intelligence algorithm estimate missing data model created would cater mechanisms interest arbitrary pattern. dataset used mnist database handwritten digits yann lecun training sample images test sample images features. images show handwritten digits fact research discussed paper conducted time little interest dl-si missing data predictors high dimensional data paper seeks exploit technique mnist dataset. remainder paper structured follows section introduces missing data deep learning techniques used well swarm intelligence algorithm implemented. section also presents related work domain. section presents experimental design procedures used section focuses results ﬁndings experiments conducted article. discussions concluding remarks suggestions future research presented section article implements deep learning technique referred stacked autoencoder built using restricted boltzmann machines individually trained using contrastive divergence algorithm stacked together bottom-up manner. estimation missing values performed using fireﬂy algorithm swarm intelligence method. however article ﬁrst brieﬂy discuss methods used problem aims solve. missing data situation whereby features within dataset lacking components ensues problems application domains rely access complete quality data aﬀect every academic/professional ﬁelds sectors. techniques aimed rectifying problem area research several disciplines manner data points missing dataset determines approach used estimating values. exist three missing data mechanisms. article focuses investigating missing completely random missing random mechanisms. previous research suggests mcar scenario arises chances missing data entry feature dependent feature features dataset implies lack correlation cross-correlation features including feature interest hand arises missingness speciﬁc feature reliant upon features within dataset feature interest according main missing data patterns. arbitrary monotone missing data patterns. arbitrary pattern missing observations occur anywhere ordering variables importance. monotone missing patterns ordering variables importance occurrence random. based upon realization article focus arbitrary missing pattern. deep learning comprises several algorithms machine learning make cataract nonlinear processing units organized number layers extract transform features input data layers output previous layer input supervised unsupervised algorithm could used training phase. come applications supervised unsupervised problems like classiﬁcation pattern analysis respectively. also based unsupervised learning multiple levels features representations input data whereby higher-level features obtained lower level features yield hierarchical representation data learning multiple levels representations depict diﬀerent levels abstraction data obtain hierarchy concepts. article deep learning technique used stacked autoencoder. firstly boltzmann machine undirected network nodes possessing stochastic traits described neural network. used amongst things extract vital information unknown probability distribution using samples distribution generally diﬃcult process learning process made simple implementing restrictions network structure leading restricted boltzmann machines described undirected probabilistic parameterized graphical model also known markov random ﬁeld rbms became techniques interest suggested components multi-layer topologies termed deep networks idea hidden nodes extract vital information observations subsequently represent inputs next rbm. stacking rbms together objective obtaining high level representations data learning features features. also associated bipartite undirected graph consists visible nodes representing input data hidden nodes capturing interdependencies features input layer article features values values }m+n. distribution given gibbs distribution energy important part model real valued weight units bias terms visible hidden variables respectively. negative equal probability decreases leading high energy. contrary positive equal zero probability increases leading lower energy. negative increases leading probability. therefore preference instead however positive decreases leading high probability preference instead negative value decreases second term positive value increases second term. applies third term gibbs distributions probabilities obtained objective training minimize average negative loglikelihood without regularization using stochastic gradient descent algorithm scales well high-dimensional datasets. achieving objective requires partial derivative parameter loss function following equation ﬁrst term expectation data distribution referred positive phase represent variables second term expectation model distribution termed negative phase. phase hard compute also intractable exponential required furthermore many sampling steps needed obtain unbiased estimates log-likelihood gradient. however shown recently running markov chain steps leads estimates suﬃcient training model approach contrastive divergence algorithm. training method undirected probabilistic graphical models idea away double expectations negative phase instead focus estimation. basically implements monte-carlo estimate expectation single input data point. idea k-step rather second term approximated sample model distribution steps gibbs chain frequently gibbs chain starts training sample training data returns steps step entails sampling obtaining samples training pattern ping input output reveals vital information essential structure input vector otherwise abstract. autoencoder takes input vector maps hidden representation deterministic mapping function form parameter comprises matrix weights vector oﬀsets/biases sigmoid activation function expressed here parameter comprises transpose matrix weights vector biases encoder prior ﬁne-tuning phase aforementioned transposition weights done autoencoder said tied weights. explained rigorous regeneration instead nature-inspired metaheuristic algorithm based ﬂashing patterns behavior ﬁreﬂies based three main rules being fireﬂies unisex ﬁreﬂies attracted ﬁreﬂies attractiveness proportional brightness decrease distance increases. idea less brighter ﬁreﬂy move towards brighter one. obvious brighter ﬁreﬂy move randomly brightness ﬁreﬂy determined landscape objective function considering attractiveness proportional light intensity variation attractiveness deﬁned respect distance here positions ﬁreﬂies second term attraction ﬁreﬂies. represent diﬀerent time steps randomization parameter controlling step size third term vector random numbers obtained gaussian distribution. movement simple random walk movement reduces variant particle swarm optimization algorithm parameters used research number missing cases sample iterations parameters selected yielded optimal results experimemtation diﬀerent permutations combinations values. algorithm used although successfully applied number domains digital image compression eigenvalue optimization feature selection fault detection scheduling etc. eﬃciency investigated missing data estimation tasks high-dimensional datasets. present work done researchers address problem missing data. research done implements hybrid genetic algorithm-neural network system perform missing data imputation tasks varying number missing values within single instance creates hybrid k-nearest neighbor-neural network system purpose. hybrid auto-associative neural network autoencoder genetic algorithm simulated annealing particle swarm optimization model used impute missing data high levels accuracy cases feature variable missing input entries. cases neural networks used principal component analysis genetic algorithm robust regression imputation missing data presence outliers investigate eﬀectiveness. suggested information within incomplete cases instances missing values used estimating missing values. nonparametric iterative imputation algorithm proposed leads root mean squared error value least imputation continuous values classiﬁcation accuracy imputation discrete values varying ratios missingness. shell-neighbor method applied missing data imputation means shell-neighbor imputation algorithm observed perform better k-nearest neighbor imputation method terms imputation classiﬁcation accuracy takes account left right nearest neighbors missing data well varying number nearest neighbors contrary k-nn considers ﬁxed nearest neighbors. multi-objective genetic algorithm approach presented missing data imputation. observed results obtained outperform well known missing data methods accuracies percentile. novel algorithms missing data imputation comparisons existing techniques found papers design experiments matlab software used dell desktop computer intel core .ghz processor virtual -bit operating system running windows pro. additionally mnist database used contains nodes seven hidden layers varying number nodes. network ﬁne-tuned using backpropagation minimizing mean squared network error. error value obtained training training done using entire training data divided balanced mini-batches. weight bias updates done every mini-batch. training higher layers weights achieved real-valued activations visible nodes preceeding rbms transcribed activation probabilities hidden nodes lower level rbms. multilayer perceptron input output layer consisting nodes hidden layer consisting nodes obtained experimenting diﬀerent numbers nodes hidden layer observing architecture leads lowest lowest mean squared network error value hidden output layer activation function used sigmoid funtion. training done using scaled conjugate gradient descent algorithm epochs. missingness test data created random according mcar mechanisms well arbitrary pattern missing values approximated using swarm intelligence algorithm objective function minimizing loss function ﬁne-tuned network. tolerance error intially networks considered reasonable ﬁrst time investigation proposed method. overall approach consist four consecutive steps being train individual rbms training data complete records using greedy layer-by-layer pre-training algorithm described starting bottom layer. layer trained epochs learning rate weights visible unit biases hidden unit biases initial ﬁnal momentum respectively. ﬁnal parameter weight cost estimate missing data ﬁne-tuned deep network part objective function fireﬂy algorithm parsing known variable values objective function ﬁrst estimating unknown values parsing estimates objective function. estimation procedure terminated stopping criterion achieved either error tolerance maximum number function evaluations attained. investiagtion imputation technique used test data contained missing data entries accounting approximately data. present tables actual estimate squared error values proposed deep autoencoder system without tolerance autoencoder system distance estimate actual value added squared error parameters determine performance method. cases presented tables deep autoencoder system shows entries autoencoder shows respectively. show better performance proposed technique without error tolerance compared existing autoencoder. knowledge validated squared error always smaller proposed technique cases presented tables could consider enough conclude performance compared techniques need analyse processing time seems better existing method compared proposed deep autoencoder system. demonstrated fig. compare processing times techniques. evident setting error tolerance value makes estimation process faster observed fig. however expense accuracy main aspect task seen fig. bigger error tolerance value faster estimation missing data. paper investigates deep neural network swarm intelligence algorithm impute missing data high-dimensional dataset. according arbitrary missing data pattern mcar mechanisms missing data could occur anywhere dataset. experiment paper considers scenario test data missing. values estimated error tolerance well error tolerance. also proposed method compared autoencoder estimation system. results obtained reveal proposed system yields accurate estimates especially error tolerance value. made evident distance squared error values considered. systems yield better estimates system. however accurate estimates come longer running times observed become smaller error tolerance values set. bigger tolerance value smaller running time. based ﬁndings article intend perform in-depth parameter analysis future research order observe parameters optimal task generalize aspect using several datasets. another obstacle faced research computation time estimate missing values address this parallelize process multi-core system observe whether parallelizing task indeed speed process maintain eﬃciency.", "year": 2016}