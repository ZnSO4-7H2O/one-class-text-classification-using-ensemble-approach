{"title": "Non-Parametric Transformation Networks", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "ConvNets, through their architecture, only enforce invariance to translation. In this paper, we introduce a new class of deep convolutional architectures called Non-Parametric Transformation Networks (NPTNs) which can learn \\textit{general} invariances and symmetries directly from data. NPTNs are a natural generalization of ConvNets and can be optimized directly using gradient descent. Unlike almost all previous works in deep architectures, they make no assumption regarding the structure of the invariances present in the data and in that aspect are flexible and powerful. We also model ConvNets and NPTNs under a unified framework called Transformation Networks (TN), which yields a better understanding of the connection between the two. We demonstrate the efficacy of NPTNs on data such as MNIST and CIFAR10 where they outperform ConvNet baselines with the same number of parameters. We show it is more effective than ConvNets in modelling symmetries from data, without the explicit knowledge of the added arbitrary nuisance transformations. Finally, we replace ConvNets with NPTNs within Capsule Networks and show that this enables Capsule Nets to perform even better.", "text": "figure operation performed single non-parametric transformation network node nptns generalization convnets towards learning general invariances symmetries. node main components convolution transformation pooling. dot-product input patch number ﬁlters computed indicates transformation applied template ﬁlter resultant output scalars max-pooled produce ﬁnal output pooling operation spatially rather across channels encode non-parametric transformations. output invariant transformation encoded ﬁlters plane indicates single feature map/ﬁlter. target task given certain amount data. number ways exist achieve this. present transformed versions training data minimize auxiliary objectives promoting invariance training pool transformed versions representation convolutional networks beyond. towards goal ideas proposed introduction convolutional neural networks proved useful. weight sharing implemented convolutions helped regularize network vastly reduce number parameters learned. additionally resulted hard encoding translation invariances network making ﬁrst applications modelling invariance network’s architecture itself. mechanism resulted greater sample efﬁciency regularization form structural inductive bias network. motivation mind alnatural whether networks model complicated invariances symmetries perform better? convnets architecture enforce invariance translation. paper introduce class deep convolutional architectures called non-parametric transformation networks learn general invariances symmetries directly data. nptns natural generalization convnets optimized directly using gradient descent. unlike almost previous works deep architectures make assumption regarding structure invariances present data aspect ﬂexible powerful. also model convnets nptns uniﬁed framework called transformation networks yields better understanding connection two. demonstrate efﬁcacy nptns data mnist cifar outperform convnet baselines number parameters. show effective convnets modelling symmetries data without explicit knowledge added arbitrary nuisance transformations. finally replace convnets nptns within capsule networks show enables capsule nets perform even better. introduction fundamental problem. central problems deep learning machine learning general supervised classiﬁcation. instantiation which vision object classiﬁcation. core challenge towards problems encoding learning invariances symmetries exist training data. general classiﬁcation problem would require invariance withinclass transformations symmetries selective between-class transformations. also evidence sample complexity model inversely proportional amount invariance invoke towards nuisance transformations indeed methods incorporate known invariances promote learning powerful invariances learning problem perform better investigating architectures invoke invariances implicitly model’s functional explicitly architectural property seems important. dimensions network architecture. years deep convolutional networks enjoyed wide array improvements architecture. observed early larger number ﬁlters convnets improved performance though diminishing returns. another signiﬁcant milestone development maturity residual connections dense skip connections though advances network architecture many improvements derivatives ideas recently however capsule nets introduced presented anpotentially fundamental idea encoding properties entity object activity vector rather scalar. goal designing powerful networks ideas modelling general invariances framework convnets open potentially dimension architecture development. primary contribution. work explore architecture class called transformation networks introduce layer form class networks called non-parametric transformation networks networks ability learn invariances general transformations present data non-parametric nature. nptns named explicit handling transformation invariances symmetries data. easily implemented using standard off-the-shelf deep learning frameworks libraries. further optimized using vanilla gradient descent methods sgd. unlike methods enforce additional invariances convolutional architectures nptns need transform input ﬁlters stage learning/testing process. enjoy beneﬁts standard convolutional architecture speed memory efﬁciency powerful modelling invariances elegant operation. forced ignore learnable transformation invariances data gracefully reduce vanilla convnets theory practice. however allowed outperform convnets capture general invariances. properties nptns. architecture nptn allows able learn powerful invariances data offer better sample complexity shortens generalization gap. learning invariances data different powerful enforcing known speciﬁc invariances rotation symmetry networks. networks enforce predeﬁned symmetries force invariances layers strong prior. complex invariances left network learn using implicit functional opposed explicit architecture. proposed nptns ability learn different independent invariances different layers fact different channels themselves. standard convolution architectures enforce translation invariance convolution operation followed aggregation operation aspect node predeﬁned invariance needs learn ﬁlter instantiation. however nptn node needs learn independent entities. first instantiation ﬁlter second transformation particular node invariant towards. node learns entities independently allows ﬂexible invariance model opposed architectures replicate invariances across network. considerable interest past developing methods incorporate prior knowledge invariances symmetries data. although applications previously tackled speciﬁc relatively narrow development methods offers better understanding importance modelling symmetries data. cases architectures explicitly enforce structure different approaches. though work focus deep architectures interesting note number works modiﬁcations markov random fields restricted boltzman machines achieve rotational invariance incorporating known invariances using deep networks. convolutional architectures also seen efforts produce rotation invariant representations. rotate input before feeding stacks cnns generating rotation invariant representations gradual pooling parameter sharing across orientations. rotate convolution ﬁlters instead input. followed pooling operation invoke invariance. ﬁlters transform instead alleviates need transform inputs expensive. nonetheless requirement also remains considerably expensive training. similar approach explored scale propose network play incorporating reﬂective symmetry within ﬁlters weight tying. interesting direction research explored ﬁlters ﬁxed non-trainable. properties ﬁlter structure allowed rotation scale translation invariant. methods however make critical assumptions invariances present applications. although useful restrictive dealing general applications cases transformations present unknown complex nature general vision. presented method incorporate invariances towards parametric groups. recently proposed interesting warped convolution based layer implement general equivariance. transformations known apriori sample grids generated ofﬂine. also introduced steerable ﬁlters convolutional framework. require generating ﬁlters ofﬂine apriori hence limited capability learning arbitrary adaptive transformations. nptns need apriori knowledge learn arbitrary non-parametric transformations ﬁnally simpler elegant theory implementation. learning unknown invariances data. address previous shortcoming studies deep networks conducted towards learning general transformations. indeed real world problems nuisance transformations present data unknown complicated parameterized function. proposed theory group invariances called i-theory explored connection general classiﬁcation problems deep networks. based core idea measuring moments group invariant distribution multiple works demonstrated efﬁcacy ideas challenging real-world problems face recognition though neural network setting learning unknown invariances data using deep networks. introduced symnets ﬁrst model general invariances deep networks back propagation. utilize kernel based interpolation weights model general symmetries. consistent connection sample complexity invariance modelling symnets perform better fewer samples compared vanilla convnets. nonetheless approach complicated difﬁcult scale. best knowledge approach learned invariances data neural network setting albeit complicated approach. provide sufﬁcient conditions enforce learned representation symmetries learned data. modelled local invariances using pooling sparse coefﬁcients dictionary basis functions. achieved local invariance complex weight sharing. optimization carried topographic carried layer wise deep networks. separate approach towards modelling invariances also developed normalizing transformation applied every input independently. approach applied transforming auto encoders spatial transformer networks transformation network feed forward network architecture designed enforce invariance class transformations. core framework node whose structure enforces desirable invariance properties. network consists multiple nodes stacked layers. node analogous single channel output single channel input convnet. speciﬁcally vanilla convnet layer input channels output channels version layer would nodes arranged fashion. node internally consists operations convolution operation bank ﬁlters pooling operation across resultant convolution feature maps single input channel. pooling across transformed versions single input channel. contrast single convolution operation vanilla convolution node. note pooling operation superﬁcially standard pooling convnet pooling spatial nature. pooling spatial rather across channels originating input channel. fig. illustrates operation single node single input channel single patch. single channel illustrated ﬁgure takes single input feature convolves bank ﬁlters. cardinality transformations node invariant towards actual itself. next transformation pooling operation simply pools across feature values obtain single activation value. node replicated spatially standard convolution layers utilized. layer input channels output channels simply nodes output channel node connected input channels. fig. illustrates multi-channel figure comparison standard convolution layer layer layer depicted input output channels convolution layer therefore ﬁlters whereas layer ﬁlters. nptn structure layer. different shades ﬁlters layer denote transformed versions ﬁlter pooled operation denotes channel addition. experiments adjust input/output channels nptn layer number ﬁlters convnet baselines. obeying group axioms element unitary. weight template instantiation transformed weights therefore convolution kernel weights node simply transformed versions transformed unitary group node entities learned i.e. node tasked learning template instantiation transformations node invariant towards. sharp contrast vanilla convolutional node template instantiation learned hard coded translation group. theoretically node transform weight template according generate rest ﬁlters pooled trasformation pooling stage. practice however simply stored templates ﬁlters implicitly encodes gradient descent updates ﬁlter differently time facilitating encoding. thus forward pass generation transformed ﬁlters necessary signiﬁcantly reduces computational complexity comapred previous works invariances node. invariance node arises theory symmetry unitary group structure ﬁlters. operation simply measures inﬁnite moment invariant distribution leads invariance. demonstrate form following result. theorem given vectors unitary group maxg∈gx ﬁxed following true proof. consider distribution elements particular distribution characterizes vector projections onto unitarity g−gw. since group closure property. elements contains elements hence must also contain g−g. implies action group results reordering group leaving distribution unchanged. thus unchanged. speciﬁcally identity element thus sets invoke exact distribution results moments same. includes inﬁnite moment implies maxg∈g maxg∈g theorem shows input node output invariant transformation group interesting since need observe transformed version training reduces sample complexity. invariance invoked arbitrary input test thereby demonstrating good generalization properties. worthwhile note vanilla convnets pooling layers pool perfect unitary groups since translation unitary operation pooling structure enforced ﬁnite group. general non-group structure node. practice node explicitly enforce group structure templates convolution ﬁlters. although structure required theoretically generating invariance sufﬁcient approximate invariance observed empirical studies real-world data utilizing non-group structures found experiments observe architecture networks able perform better learning invariance towards group structured transformations translation rotation also towards general non-parametric transformations. convnets kind parametric transformation networks vanilla convolution layer simply performs convolution operation ﬁlters onto image. produce invariance transformation. however followed spatial pooling operation resultant feature explicitly invariant translation. convolution pooling operation modelled node group deﬁned ﬁnite translation group. following node operation translation group acts ﬁlter template transformed versions computed on-the-ﬂy convnet dot-producted input patch generates features. features max-pooled second operation node resulting translation invariance. straight-forward observe generalization vanilla convolution node would group parameterized model complicated transformations. networks general type parametric node called parametric transformation networks thereby vanilla convolution network pooling operation kind ptn. introduce main practical contribution transformation networks learn general nonparametric symmetries data. non-parametric transformation network network nodes lack parametric model transformation describes templates/ﬁlters nodes. able explicitly learn arbitrary invariances symmetries data better vanilla cnn. compared approaches model general invariances architecture natural generalization cnns elegantly simple. setting reduces nptn standard theory practice. nptn layer structure. nptns implemented using standard deep learning libraries convolutional routines. optimized using standard gradient descent methods sgd. replace convolution layer architecture making versatile. describe implement nptn layer input channels output channels models transformations cardinality note channels models ﬁlters independently. every input channel ﬁlters must learned would encode pre-transformed ﬁlter set. continuously maintained ﬁlters bypasses need transform ﬁlters on-the-ﬂy forward pass whenever weights change update. gradients encode invariance updates. input convolved ﬁlters sets feature maps pooled across. speciﬁcally number feature maps single input channel results intermediate feature pooling operation intermediate feature maps transformation invariant. feature maps results output feature channel. output channels performs operation independent channels. total number ﬁlters learned nptn layer input channels output channels models number transformations n|g|. importantly every training loss layer network figure training loss cifar. layered network. network number parameters second layer lower number parameters ﬁrst layer nptn variants. layered network. networks color number parameters. non-trivial nptn variants higher training error lower test error thereby shortening generalization gap. connection input output layer independently models invariance using separate number ﬁlters. non-unitary structure nptn node. nptns explicitly enforce unitary structure. preserving unitary structure would restrict complexity transformations modelled. nonetheless restriction might also bring regularization beneﬁts question left future exploration. nptns aspect approximation whereas convnets exactly maintain unitary group structure explicitly hard coded operations. despite fact experiments nptns perform better leading hypothesis modelling complex transformations important exactly preserving unitary group structure. relation maxout networks. nptns deviate signiﬁcantly motivation architecture maxout networks maxout networks introduced general activation function also applied pooling across channels. however channels pooled support input channels. relation invariance modelling single input feature map. nptns hand pool across channels take single channel input. ﬁlters pooled channels applied single input. pooling across responses results invariant description input. four operations performed sequence namely convolution volumettric pooling channel reordering ﬁnally volumetric mean pooling. inputs outputs transformations ﬁrst function standard convolution function inputs n|g| outputs groups option hence total n|g| ﬁlters learned. second function volumetric pooling pool across channels kernel size kernel size along height width increased spatial pooling desired well. note output convolution function channels sequence input channel replicated times. however want alternate conﬁguration volumetric mean pooling layer. instead require feature every input channel together order replicated times. solved channel reordering operation third operation. ﬁnal operation volumetric mean pooling kernel size resulting output channels required. ﬁrst experiments benchmark characterize behavior nptns standard convnets augmented batch normalization goal experiments observe whether learning non-parametric transformation invariance complex visual data helps object classiﬁcation. experiment utilize cifar dataset. networks experiment designed compete data rather throw light behavior nptns. therefore utilize shallow networks namely three layered network experiments. layer block baseline convnets consist convolution layer followed batch normalization non-linearity ﬁnally spatial pooling layer. corresponding nptn network replaces convolution layer nptn layer. thus nptn allowed model non-parametric invariance addition typically enforced translation invariance spatial pooling. layered nptn. ﬁrst pilot experiment works layered network baseline convnet channels total ﬁlters. nptn variants experiment keep number ﬁlters second layer constant channels denoted channels denoted channels fig. fig. show training testing losses. network experimented number parameters. layered network performs slightly worse convnet baseline training. however nptn variants learn non-trivial transformations lower training loss except nptns perform signiﬁcantly better. fig. shows nptn variants much lower test losses performing best. three layered nptn. second comparison experiment explore behavior three layered network different kernel sizes. maintain fair comparison approximately equal number ﬁlters corresponding baseline convnet nptn variant keeping number channels constant nptns instead increasing cardinality transformation thus transformation modelling capacity table test loss progressively transformed mnist random rotations random pixel shifts. nptns learn invariances arbitrary transformations data without apriori knowledge. network slowly increased. train/test networks cifar report results fig. table three layered nptn higher training loss compared convnet baseline demonstrates lower test loss. indicates nptn structural bias shortens generalization gap. behavior training contrast layered network line behavior test time decreased loss. nptns roughly computational complexity parameters generalize better convnets. note however observe beneﬁts kernel size fact many cases nptns perform worse. hypothesize that ﬁlter size small area activation exhibit meaningful structured spatial transformations learned. better generalization consistently observed larger kernel sizes contain structured transformations larger receptive ﬁeld sizes. effect training larger |g|. performance peaks around going higher offers less performance gains. believe forward pass nptn selects single transformation channel update operation. every back-propagation therefore updates single ﬁlter ﬁlters leading ﬁlter updated average total number iterations times factor |g|. results ﬁlters less optimized standard convnet ﬁlters resulting trade modelling invariance. effect seems pronounced higher |g|. standard convnet face problem every ﬁlter updated given backprop iteration efﬁcient learning unknown invariances data. demonstrate ability nptn networks learn invariances directly data without apriori knowledge. experiment augment mnist random rotations random translations training testing data thereby increasing complexity learning problem itself. sample random instantiation transformation applied. rotation angular range increased whereas translations pixel shift range. table presents results. networks table layered exact number parameters. expected nptns match performance vanilla convnets additional transformations added however transformation intensity increased nptns perform signiﬁcantly better convnets. trends consistent previous experiments observed highest performance observed nptn highlights main feature nptns i.e. ability model arbitrary transformations observed data without apriori information without changes architecture whatsoever. exhibit better performance settings rotation invariance stronger translation invariance required ability something previous deep architectures posses demonstrate. efﬁcacy depth. exploratory study examine relative depth nptn layer assists learning transformation invariance mnist. this train three networks number layers channels layer expect last layer channels. replace layers different depth single nptn layer starting depth last layer observe impact performance. keeping number parameters computation complexity exactly networks particular depth. enforce this number channels convnet baselines slightly increased maintain number parameters nptn versions. fig. shows performance networks cifar. term layer ratio signiﬁes relative depth nptn layer replacement that general trend nptns offer performance gains towards lower layers learn complicated level transformations translations. performance gains decrease towards layer ratio clear nptns perform slightly better convnets rotations rotation experiments small translations pixels applied training. figure test errors mnist capsule nets augmented nptns. denotes capsule network vanilla convnet. labels nptns number ﬁlters left right nptns signiﬁcantly outperform convnets capsule nets fewer ﬁlters. capsule networks dynamic routing recently introduced extension standard neural networks main motivation behind capsules represent entities allow capsules encode different properties object. however since architecture implemented using vanilla convolution layers invariance properties networks limited. goal ﬁnal experiment augment capsule nets nptns. replacing convolution layers primary capsule layer published architecture nptn layers maintaining number parameters effect convolutional capsules replaced nptn capsules. baseline proposed capsulenet layers using third party implementation pytorch. number output channels ﬁrst convolution layer kept baseline convolution capsule layer output channels. nptn variants progressively decreased number channels increased. hyperparameters preserved. networks trained -pixel shifted mnist epochs learning rate tested test shift. performance statistics runs reported fig. roughly number kernel ﬁlters capsule nets much gain nptn layers learning invariances within capsule signiﬁcantly increases efﬁcacy performance overall architecture. clear success convnets whole story towards solving perception. studies different aspects network design prove paramount addressing complex problem visual general perception. development nptns offer design aspect i.e. modelling non-parametric invariances symmetries simultaneously data. experiments found nptns indeed effectively learn general invariances without apriori information. further effective improve upon vanilla convnets even applied general vision data presented cifar complex unknown symmetries. seems critical requirement system aimed taking step towards general perception. assuming knowledge symmetries real-world data impractical succesful models would need adapt accordingly. experiments nptns compared vanilla convnet baselines number ﬁlters interestingly superior performance nptns fewer channels convnet baselines indicates better modelling invariances useful goal pursue design. explicit efﬁcient modelling invariances potential improve many existing architectures. experiments also capsule networks utilized nptns instead vanilla convnets performed much better. motivates justify attention towards architectures solutions efﬁciently model general invariances deep networks. endeavour might produce networks performing better practice promises deepen understanding deep networks perception general. references anselmi fabio leibo joel rosasco lorenzo mutch tacchetti andrea poggio tomaso. unsupervised learning invariant representations hierarchical architectures. arxiv preprint arxiv. dieleman sander willett kyle dambre joni. rotation-invariant convolutional neural networks galaxy morphology prediction. monthly notices royal astronomical society hadsell raia chopra sumit lecun yann. dimensionality reduction learning invariant mapping. computer vision pattern recognition ieee computer society conference volume ieee kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning kavukcuoglu koray fergus lecun yann learning invariant features topographic ﬁlter maps. computer vision pattern recognition cvpr ieee conference ieee liao qianli leibo joel poggio tomaso. learning invariant representations applications face veriﬁcation. advances neural information processing systems dipan kannan ashwin arakalgud gautam savvides marios. max-margin invariant features transformed unlabelled data. advances neural information processing systems dipan juefei-xu felix savvides marios. discriminative invariant kernel features bells-and-whistlesfree approach unsupervised face recognition pose proceedings ieee conference estimation. computer vision pattern recognition schmidt roth stefan. learning rotation-aware features invariant priors equivariant descriptors. computer vision pattern recognition ieee conference ieee schroff florian kalenichenko dmitry philbin james. facenet uniﬁed embedding face recognition clustering. proceedings ieee conference computer vision pattern recognition sifre laurent mallat st´ephane. rotation scaling deformation invariant scattering texture discrimination. proceedings ieee conference computer vision pattern recognition", "year": 2018}