{"title": "Visual Dialog", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog question-answer pairs.  We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network -- and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrieval-based evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first 'visual chatbot'! Our dataset, code, trained models and visual chatbot are available on https://visualdialog.org", "text": "georgia institute technology carnegie mellon university berkeley virginia tech {skottur khushig moura}andrew.cmu.edu {abhshkdz parikh dbatra}gatech.edu introduce task visual dialog requires agent hold meaningful dialog humans natural conversational language visual content. speciﬁcally given image dialog history question image agent ground question image infer context history answer question accurately. visual dialog disentangled enough speciﬁc downstream task serve general test machine intelligence grounded vision enough allow objective evaluation individual responses benchmark progress. develop novel two-person chat data-collection protocol curate large-scale visual dialog dataset visdial released contains dialog question-answer pairs images coco total dialog questionanswer pairs. introduce family neural encoder-decoder models visual dialog encoders late fusion hierarchical recurrent encoder memory network decoders outperform number sophisticated baselines. propose retrievalbased evaluation protocol visual dialog agent asked sort candidate answers evaluated metrics mean-reciprocal-rank human response. quantify machine human performance visual dialog task human studies. putting together demonstrate ﬁrst ‘visual chatbot’ dataset code trained models visual chatbot available https//visualdialog.org. figure introduce task visual dialog agent must hold dialog human visual content. introduce large-scale dataset evaluation protocol novel encoder-decoder models task. tion object detection ‘high-level’ tasks learning play atari video games answering reading comprehension questions understanding short stories even answering questions images videos lies next believe next generation visual intelligence systems need posses ability hold meaningful dialog humans natural language visual content. applications include aiding visually impaired users understanding surroundings social media content requires tion relevant region. co-reference resolution right?’ requires machine visual memory systems also need consistent outputs ‘how many people wheelchairs?’ ‘two’ ‘what genders?’ ‘one male female’ note number genders being speciﬁed two. difﬁculties make problem highly interesting challenging one. talk machines? prior work language-only dialog arranged spectrum following end-points goal-driven dialog goal-free dialog ends vastly differing purposes conﬂicting evaluation criteria. goal-driven dialog typically evaluated task-completion rate time task completion clearly shorter dialog better. contrast chit-chat longer user engagement interaction better. instance goal million amazon alexa prize create socialbot converses coherently engagingly humans popular topics minutes. believe instantiation visual dialog hits sweet disentangled enough spot spectrum. speciﬁc downstream task serve general test machine intelligence grounded enough vision allow objective evaluation individual responses benchmark progress. former discourages taskengineered bots ‘slot ﬁlling’ latter discourages bots personality avoid answering questions keeping user engaged contributions. make following contributions propose task visual dialog machine must hold dialog human visual content. develop novel two-person chat data-collection protocol curate large-scale visual dialog dataset upon completion visdial contain dialog images coco dataset total dialog question-answer pairs. compared visdial studies signiﬁcantly richer task overcomes ‘visual priming bias’ contains free-form longer answers order magnitude larger. visdial data coco-train cocoval already available download https// visualdialog.org. since dialog history contains ground-truth caption collecting dialog data coco-test. instead collect dialog data extra images coco distribution test set. figure differences image captioning visual question answering visual dialog. dialogs shown visdial dataset curated live chat amazon mechanical turk workers despite rapid progress intersection vision language particular image captioning visual question answering clear grand goal agent ‘see’ ‘communicate’. captioning human-machine interaction consists machine simply talking human dialog input human. takes signiﬁcant step towards human-machine interaction still represents single round dialog unlike human conversations scope follow-up questions memory system previous questions asked user consistency respect previous answers provided system step towards conversational visual introduce novel task visual dialog along large-scale dataset evaluation protocol novel deep models. task deﬁnition. concrete task visual dialog following given image history dialog consisting sequence question-answer pairs natural language follow-up question task machine answer question free-form natural language task visual analogue turing test. consider visual dialog examples fig. question ‘what gender white shirt?’ requires machine selectively focus direct atten– hierarchical recurrent encoder contains dialoglevel recurrent neural network sitting question-answer -level recurrent block. qa-level recurrent block also include attentionover-history mechanism choose attend round history relevant current question. propose retrieval-based evaluation protocol visual dialog agent asked sort list candidate answers evaluated metrics meanreciprocal-rank human response. vision language. number problems intersection vision language recently gained prominence image captioning video/movie description text-to-image coreference/grounding visual storytelling course visual question answering however involve single-shot natural language interaction dialog. concurrent work recent works also begun studying visually-grounded dialog. visual turing test. closely related work geman proposed fairly restrictive ‘visual turing test’ system asks templated binary questions. comparison dataset free-form openended natural language questions collected subjects chatting amazon mechanical turk resulting realistic diverse dataset dataset contains street scenes dataset considerably variety since uses images coco moreover dataset orders magnitude larger images images question-answer pairs image total pairs. text-based question answering. work related text-based question answering ‘reading comprehension’ tasks studied community. recent large-scale datasets domain include factoid question-answer corpus simplequestions dataset deepmind dataset artiﬁcial tasks babi dataset squad dataset reading comprehension visdial viewed fusion reading comprehension vqa. visdial machine must comprehend history past dialog understand image answer question. design answer question visdial present past dialog were question would asked. history dialog contextualizes question question ‘what else holding?’ requires machine comprehend history realize question talking excluded understand image answer question. conversational modeling chatbots. visual dialog visual analogue text-based dialog conversation modeling. earliest developed chatbots rule-based end-to-end learning based approaches actively explored recent large-scale conversation dataset ubuntu dialogue corpus contains dialogs extracted ubuntu channel internet relay chat perform study problems existing evaluation protocols free-form dialog. important difference free-form textual dialog visdial visdial participants symmetric person asks questions image see; person sees image answers questions role assignment gives sense purpose interaction allows objective evaluation individual responses. describe visdial dataset. begin describing chat interface data-collection process analyze dataset discuss evaluation protocol. consistent previous data collection efforts collect visual dialog data images common objects context dataset contains multiple objects everyday scenes. visual complexity images allows engaging diverse conversations. live chat interface. good data task include dialogs temporal continuity grounding image mimic natural ‘conversational’ exchanges. elicit responses paired workers chat real-time worker assigned speciﬁc role. worker sees single line text describing imfigure collecting visually-grounded dialog data amazon mechanical turk live chat interface person assigned role ‘questioner’ second person ‘answerer’. show ﬁrst questions collected interface turkers interact fig. fig. remaining questions shown fig. image remains hidden questioner. task questions hidden image ‘imagine scene better’. second worker sees image caption. task answer questions asked chat partner. unlike answers restricted short concise instead workers encouraged reply naturally ‘conversationally’ possible. fig. shows example dialog. process unconstrained ‘live’ chat exception questioner must wait receive answer posting next question. workers allowed conversation messages exchanged details ﬁnal interface found supplement. also piloted different setup questioner highly blurred version image instead caption. conversations seeded blurred images resulted questions essentially ‘blob recognition’ ‘what pink patch bottom right?’. full-scale data-collection decided seed captions since resulted ‘natural’ questions closely modeled real-world applications discussed section visual signal available human. building -person chat amt. despite popularity data collection platform computer vision setup design overcome unique challenges issue simply designed multi-user human intelligence tasks hosting live two-person chat meant none amazon tools could used developed backend messaging data-storage infrastructure based redis messaging queues node.js. support data quality ensured worker could chat maintaining pool worker paired. minimize wait time worker second searched ensured always signiﬁcant pool available hits. workers abandoned midway automatic conditions code kicked asking remaining worker either continue asking questions providing facts image till messages sent them. workers completed task fully compensated backend discarded data automatically launched image real two-person conversation could recorded. entire data-collection infrastructure publicly available. visual priming bias. difference visdial previous image question-answering datasets visual baidu lack ‘visual priming bias’ visdial. speciﬁcally previous datasets subjects image asking questions analyzed leads particular bias questions people clocktower picture?’ pictures actually containing clock towers. allows language-only models perform remarkably well results inﬂated sense progress particularly perverse example questions dataset starting blindly answering ‘yes’ without reading rest question looking associated image results average accuracy visdial questioners image. result bias reduced. analyzing visdial answers answer lengths. fig. shows distribution answer lengths. unlike previous datasets answers visdial longer descriptive mean-length words fig. shows cumulative coverage answers frequent answers difference visdial stark top- answers cover answers visdial ﬁgure signiﬁcant heavy tail visdial long strings unique thus coverage curve fig. becomes straight line slope total unique answers visdial answer types. since answers visdial longer strings visualize distribution based starting words interesting category answers emerges think can’t tell’ can’t see’ expressing doubt uncertainty lack information. consequence questioner able image asking contextually relevant questions questions answerable certainty image. believe rich data building human-like refuses answer questions doesn’t enough information answer. related complementary effort question relevance vqa. binary questions binary answers. binary questions simply ‘yes’ ‘no’ ‘maybe’ answers visdial must distinguish binary questions binary answers. binary questions starting ‘do’ ‘did’ ‘have’ ‘has’ ‘is’ ‘are’ ‘was’ ‘were’ ‘can’ ‘could’. answers questions contain ‘yes’ ‘no’ begin ‘yes’ ‘no’ contain additional information clariﬁcation involve ambiguity answer question without explicitly saying ‘yes’ ‘no’ call answers contain ‘yes’ ‘no’ binary answers answers subsets respectively. binary answers biased towards ‘yes’ yes/no answers ‘yes’. visdial trend reversed. ‘yes’ yes/no responses. understandable since workers image likely negative responses. analyzing visdial dialog section discussed typical dialog visdial. analyze quantitative statistics here. figure distribution lengths questions answers percent coverage unique answers answers train dataset compared vqa. given coverage visdial unique answers indicating greater answer diversity. distributions. fig. shows distribution question lengths visdial questions range four words. fig. shows ‘sunbursts’ visualizing distribution questions visdial vqa. similarities differences immediately jump out. binary questions visdial compared frequent ﬁrst question-word visdial ‘is’ ‘what’ vqa. detailed comparison statistics visdial datasets available table supplement. finally stylistic difference questions difﬁcult capture simple statistics above. subjects image asked stump smart robot. thus queries involve speciﬁc details often background visdial questioners original image asking questions build mental model scene. thus questions tend open-ended often follow pattern generally starting entities caption figure distribution ﬁrst n-grams visdial questions questions visdial answers. word ordering starts towards center radiates outwards length proportional number questions containing word. coreference dialog. since language visdial result sequential conversation naturally contains pronouns ‘he’ ‘she’ ‘his’ ‘her’ ‘it’ ‘their’ ‘they’ ‘this’ ‘that’ ‘those’ etc. total questions answers nearly dialogs contain least pronoun thus conﬁrming machine need overcome coreference ambiguities successful task. pronoun usage ﬁrst round picks frequency. ﬁne-grained perround analysis available supplement. temporal continuity dialog topics. natural conversational dialog data continuity ‘topics’ discussed. already discussed qualitative differences visdial questions vqa. order quantify differences performed human study manually annotated question ‘topics’ images chosen randomly set. topic annotations based human judgement consensus annotators topics asking particular object scene weather image exploration performed similar topic annotation questions images compared topic continuity questions. across rounds visdial question topics average conﬁrming independent questions. recall visdial questions image opposed vqa. therefore fair comparison compute average number topics visdial subsets successive questions. bootstrap samples batch size visdial topics lower mean suggests continuity visdial questions change topics often. fundamental challenge dialog systems evaluation. similar state affairs captioning machine translation open problem automatically evaluate quality free-form answers. existing metrics bleu meteor rouge known correlate poorly human judgement evaluating dialog responses instead evaluating downstream task holistically evaluating entire conversation evaluate individual responses round retrieval multiple-choice setup. speciﬁcally test time visdial system given image ‘ground-truth’ dialog history question list candidate answers asked return sorting candidate answers. model evaluated retrieval metrics rank human response recallk i.e. existence human response top-k ranked responses mean reciprocal rank human response evaluation protocol compatible discriminative models generative models ranking candidates model’s log-likelihood scores. candidate answers. generate candidate correct incorrect answers four sets correct ground-truth human response question. plausible answers similar questions. similar questions start similar tri-grams mention similar semantic concepts rest question. capture this questions embedded vector space concatenating glove embeddings ﬁrst three words averaged glove embeddings remaining words questions. euclidean distances used compute neighbors. since neighboring questions asked different images answers serve ‘hard negatives’. popular popular answers dataset e.g. ‘yes’ ‘no’ ‘white’ ‘grey’ ‘gray’ ‘yes is’. inclusion popular answers forces machine pick likely priori responses plausible responses question thus increasing task difﬁculty. random remaining answers random questions dataset. generate candidates ﬁrst union correct plausible popular answers include random answers unique found. question list candidate answers high level models follow encoder-decoder framework i.e. factorize parts encoder converts input vector space decoder converts embedded vector output. describe choices component next present experiments encoder-decoder combinations. decoders types decoders generative decoder encoded vector initial state long short-term memory language model. training maximize log-likelihood ground truth answer sequence given corresponding encoded representation evaluate model’s loglikelihood scores rank candidate answers. note decoder need score options during training. result models exploit biases option creation typically underperform models debatable whether exploiting biases really indicative progress. moreover generative decoders practical actually deployed realistic applications. discriminative decoder computes product similarity input encoding lstm encoding answer options. products softmax compute posterior probability options. training maximize log-likelihood correct option. evaluation options simply ranked based posterior probabilities. cases represent -normalized activations penultimate layer vgg- encoder experiment possible ablated versions late fusion encoder encoder treat long string entire history concatenated. separately encoded different lstms individual representations participating inputs concatenated linearly transformed desired size joint representation. hierarchical recurrent encoder encoder capture intuition hierarchical nature problem question sequence words need embedded dialog whole sequence question-answer pairs thus similar shown fig. propose model contains dialog-rnn sitting recurrent block recurrent block embeds question image jointly lstm embeds round history passes concatenation dialog-rnn dialog-rnn produces encoding round dialog context pass onto next round. also attention-over-history mechanism allowing recurrent block choose attend round history relevant current question. attention mechanism consists softmax previous rounds computed history question+image encoding. figure architecture encoder attention. current round model capability choose attend relevant history previous rounds based current question. attention-over-history feeds dialog-rnn along question generate joint representation decoder. memory network encoder develop encoder maintains previous question answer ‘fact’ memory bank learns refer stored facts image answer question. speciﬁcally encode lstm vector encode previous round history another lstm matrix. comtable performance methods visdial measured mean reciprocal rank recallk mean rank. higher better recallk lower better mean rank. performance visdial included supplement. however models better encode history perform better corresponding models with/without history models looking outperform corresponding blind models human studies. conduct studies quantitatively evaluate human performance task combinations {with image without image}×{with history without history}. without image humans perform better access dialog history. expected narrows access image. complete details found supplement. conclusions summarize introduce task visual dialog agent must hold dialog human visual content. develop novel two-person chat datacollection protocol curate large-scale dataset propose retrieval-based evaluation protocol develop family encoder-decoder models visual dialog. quantify human performance task human studies. results indicate signiﬁcant scope improvement believe task serve testbed measuring progress towards visual intelligence. pute inner product question vector history vector scores previous rounds softmax attention-over-history probabilities. convex combination history vectors using attention probabilities gives ‘context vector’ passed fc-layer added question vectorto construct encoding. language memory network ‘-hop’ encoding. ‘--’ convention refer model-input combinations. example ‘lf-qi-d’ late fusion encoder question+image inputs discriminative decoder. implementation details models found supplement. splits. visdial contains dialogs coco-train coco-val images. split training validation test. data preprocessing hyperparameters training details included supplement. baselines compare number baselines answer prior answer options test question encoded lstm scored linear classiﬁer. captures ranking frequency answers training withresolving exact string matching. nn-q given test question nearest neighbor questions train score answer options meansimilarity answers. nn-qi first nearest neighbor questions test question. then subset size based image feature similarity. finally rank options mean-similarity answers questions. finally adapt several state-of-art models hiecoatt visual dialog. since posed classiﬁcation ‘chop’ ﬁnal vqa-answer softmax models feed activations discriminative decoder train end-to-end visdial. note lf-qi-d model similar altogether form fairly sophisticated baselines. results. tab. shows results models baselines visdial takeaways expected learning based models signiﬁcantly outperform non-learning baselines. discriminative models signiﬁcantly outperform generative models discussed expected since discriminative models tune biases answer options. best generative discriminative models mn-qih-g mn-qih-d mrr. observe naively incorporating history doesn’t help much even hurt little building excellent framework. work funded part career awards awards grant n--- sloan fellowship awards allen distinguished investigator award paul allen family foundation ictas junior faculty awards google faculty research awards amazon academic research awards education research grant nvidia donations supported grant n---. views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied u.s. government sponsor. sec. presents analysis visdial question types question answer lengths question type. video interactive sunburst visualization dataset included. putting together compile video demonstrating visual chatbot answers sequence questions user image. demo uses best generative models main paper mn-qih-g uses sampling inference lstm decoder. note videos demonstrate ‘unscripted’ dialog sense particular sequence present visdial model provided list answer options. section exhaustive list differences visdial image question-answering datasets dataset serving representative. essence characterize makes instance visdial collection independent questionanswer pairs image makes dialog. order self-contained exhaustive list parts section repeat content main document. fig. shows distribution answer lengths visdial. tab. compares statistics visdial existing image question answering datasets. unlike previous datasets figure distribution lengths questions answers percent coverage unique answers answers train dataset compared vqa. given coverage visdial unique answers indicating greater answer diversity. fig. shows cumulative coverage answers frequent answers difference visdial stark top- answers cover answers visdial ﬁgure signiﬁcant heavy tail answers visdial long strings unique thus coverage curve fig. becomes straight line slope total unique answers visdial people conversing tend pronouns refer already mentioned entities. since language visdial result sequential conversation naturally contains pronouns ‘he’ ‘she’ ‘his’ ‘her’ ‘it’ ‘their’ ‘they’ ‘this’ ‘that’ ‘those’ etc. total questions answers nearly dialogs contain least pronoun thus conﬁrming machine need overcome coreference ambiguities successful task. comparison questions answers contain least pronoun. fig. pronoun usage lower ﬁrst round compared rounds expected since fewer entities refer earlier rounds. pronoun usage also generally lower answers questions also understandable since answers generally shorter questions thus less likely contain pronouns. general pronoun usage fairly consistent across rounds questions answers. ‘there’s blue fence background like enclosure’ enclosure inside outside?’. line questioning exist dataset subjects shown questions already asked image explicitly instructed different entities counting number topics. order quantify qualitative differences performed human study manually annotated question ‘topics’ images chosen randomly set. topic annotations based human judgement consensus annotators topics asking particular object scene weather image exploration performed similar topic annotation questions images compared topic continuity questions. across rounds visdial questions topics average conﬁrming independent questions. recall visdial questions image opposed vqa. therefore fair comparison compute average number topics visdial ‘sliding windows’ successive questions. bootstrap samples batch size visdial topics lower mean number topics suggests continuity visdial questions change topics often. transition probabilities topics. take analysis step computing topic transition probabilities topics follows. given sequential dialog exchange count number topic transitions between consecutive pairs normalized total number possible transitions rounds compute ‘topic transition probability’ visdial different settings in-order permuted sequence figure percentage pronouns different rounds. round pronoun usage questions rounds pronoun usage higher questions fairly consistent across rounds. qualitative example topics. stylistic difference questions asked visdial nature task assigned subjects asking questions. subjects image asked stump smart robot. thus queries involve speciﬁc details often background visdial questioners original image asking questions build mental model scene. thus questions tend openended often follow pattern generally starting entities caption qas. note visdial simply collection independent opposed dialog would expect topic transition probabilities similar inorder permuted variants. however permutations topic-annotated image-dialogs inorder-visdial average topic transition probability permuted-visdial contrast topic transition probability in-order permuted qas. observations in-order transition probability lower visdial permuting order questions results larger increase visdial around compared mere case observations establish smoothness temporal order topics visdial indicative narrative structure dialog rather independent question-answers. analysis goal measure whether visdial behaves like dialog dataset. particular compare visdial cornell movie-dialogs corpus cornell movie-dialogs corpus text-only dataset extracted pairwise interactions characters approximately movies widely used standard dialog corpus natural language processing dialog communities. popular evaluation criteria used dialog-systems research community perplexity language models trained dialog datasets lower perplexity model better learned structure dialog dataset. purpose analysis pick popular sequence-to-sequence language model perplexity model trained different datasets measure temporal structure dataset. standard dialog literature train seqseq model predict probability utterance given previous utterance i.e. cornell corpus. visdial train seqseq model predict probability question given previous question-answer pair i.e. dataset used train splits training hyperparameter tuning respectively report results test. test time conversations length cornell corpus fair comparison visdial three datasets created permuted versions table comparison sequences visdial cornell movie-dialogs corpus original ordering permuted ‘shufﬂed’ ordering. lower better perplexity higher better classiﬁcation accuracy. left absolute increase perplexity natural permuted ordering highest cornell corpus followed visdial indicative degree linguistic structure sequences datasets. right accuracy simple threshold-based classiﬁer trained differentiate original sequences permuted shufﬂed versions. higher classiﬁcation rate indicates existence strong temporal continuity conversation thus making ordering important. classiﬁer visdial achieves highest accuracy followed cornell note binary classiﬁcation task prior probability class design equal thus chance performance classiﬁer performs close chance. test either pairs utterances randomly shufﬂed disturb natural order. allows compare datasets natural ordering w.r.t. permuted orderings. hypothesis since dialog datasets linguistic structure sequence utterances contain structure signiﬁcantly affected permuting sequence. contrast collection independent question-answers signiﬁcantly affected permutation. tab. compares original unshufﬂed test shufﬂed testsets metrics perplexity compute standard metric perplexity token i.e. exponent normalized negative-logprobability sequence tab. shows perplexities original unshufﬂed test permuted test sequences. notice trends. first note absolute perplexity values higher cornell corpus datasets. hypothesize broad unrestrictive dialog generation task cornell corpus difﬁcult task question prediction images comparison restricted task. second three datasets shufﬂed test statistically signiﬁcant higher perplexity original test indicates shufﬂing indeed break linguistic structure sequences. third absolute increase perplexity natural permuted ordering highest cornell corpus followed visdial indicative degree linguistic structure sequences datasets. finally relative increases perplexity cornell visdial visdial suffers highest relative increase perplexity shufﬂing indicating existence temporal continuity gets disrupted. classiﬁcation second metric compare datasets natural permuted order test whether reliably classify given sequence natural permuted. classiﬁer simple threshold perplexity sequence. speciﬁcally given pair sequences compute perplexity seqseq model predict higher perplexity sequence permuted ordering sequence lower perplexity natural ordering. accuracy simple classiﬁer indicates easy difﬁcult tell difference natural permuted sequences. higher classiﬁcation rate indicates existence temporal continuity conversation thus making ordering important. tab. shows classiﬁcation accuracies achieved datasets. classiﬁer visdial achieves highest accuracy followed cornell note binary classiﬁcation task prior probability class design equal thus chance performance classiﬁers visdial cornell signiﬁcantly outperforming chance. hand classiﬁer near chance indicating lack general temporal continuity. summarize analysis experiments show visdial signiﬁcantly dialog-like behaves like standard dialog dataset cornell movie-dialogs corpus. difference visdial previous image question answering datasets visual baidu lack ‘visual priming bias’ visdial. speciﬁcally previous datasets subjects image asking questions described leads particular bias questions people clocktower picture?’ pictures actually containing clock towers. allows languagemodels perform remarkably well results inﬂated sense progress particularly perverse example questions dataset starting blindly answering ‘yes’ without reading rest question looking associated image results average accuracy visdial questioners image. result figure distribution answers visdial ﬁrst four words. ordering words starts towards center radiates outwards. length proportional number questions containing word. white areas words contributions small show. uncertainty answers visdial. since answers visdial longer strings visualize distribution based starting words interesting category answers emerges think can’t tell’ can’t see’ expressing doubt uncertainty lack information. consequence questioner able image asking contextually relevant questions questions answerable certainty image. believe rich data building human-like refuses answer questions doesn’t enough information answer. related complementary effort question relevance vqa. binary questions binary answers visdial. binary questions simply ‘yes’ ‘no’ ‘maybe’ answers visdial must distinguish binary questions binary answers. binary questions starting ‘do’ ‘did’ ‘have’ ‘has’ ‘is’ ‘are’ ‘was’ ‘were’ ‘can’ ‘could’. answers questions contain ‘yes’ ‘no’ begin ‘yes’ ‘no’ contain additional information clariﬁcation involve ambiguity generative models options presented humans. note setting humans machines since options. tab. bottom-half shows results comparison. that expected humans full information perform best large human machine performance even larger compared generative models unlike discriminative models actively trying exploit biases answer candidates furthermore humans outperform best machine even looking image simply basis context provided history perhaps expected access image history humans signiﬁcantly better best machines access history humans perform even better. in-house human studies worker feedback dialog history plays following roles humans provides context question paints picture scene helps eliminate certain answer choices gives cues answerer’s response style helps identify right answer among similar answer choices disambiguates amongst likely interpretations image again helping identify right answer among multiple plausible options. section show interface connect amazon mechanical turk workers live used collect data. instructions. ensure quality data provide detailed instructions interface shown fig. since workers know roles starting study provide instructions questioner answerer roles. pairing immediately pairing workers assign roles questioner answerer display role-speciﬁc instructions shown fig. observe ‘maybe’) answer question without explicitly saying ‘yes’ ‘no’ call answers contain ‘yes’ ‘no’ binary answers answers subsets respectively. binary answers biased towards ‘yes’ yes/no answers ‘yes’. visdial trend reversed. ‘yes’ yes/no responses. understandable since workers image likely negative responses. table human-machine performance comparison visdial measured mean reciprocal rank recallk mean rank. note higher better recallk lower better mean rank. conducted studies quantitatively evaluate human performance task combinations {with image without image}×{with history without history} random images rounds. speciﬁcally setting show human subjects jumbled list candidate answers question top- predicted responses ‘lf-qih-d’ model ground truth answer rank responses. task done human subjects. results study shown top-half tab. without access image humans perform better access dialog history compare human-qh human-q perhaps expected narrows humans access image compare human-qih humanqi note numbers directly comparable machine performance reported main paper models tasked ranking responses humans asked rank candidates. task fig. shows question lengths type round. average length question type consistent across rounds. questions starting ‘any’ tend shortest. fig. shows answer lengths type question said response round. contrast questions signiﬁcant variance answer lengths. answers binary questions tend short answers ‘how’ ‘what’ questions tend explanatory long. across question types answers tend longest middle conversations. fig. shows round-wise coverage question type. conversations progress ‘is’ ‘what’ ‘how’ questions reduce ‘can’ ‘do’ ‘does’ ‘any’ questions occur often. questions starting ‘is’ popular dataset. tab. shows results proposed models baselines visdial takeaways first expected learning based models signiﬁcantly outperform non-learning baselines. second discriminative models signiﬁcantly outperform generative models discussed expected since discriminative models tune biases answer options. improvement comes signiﬁcant limitation able actually generate responses recommend decoders viewed separate cases. third best generative discriminative models mn-qih-g mn-qih-d outperform suite models sophisticated baselines. fourth observe models perform better q-only models highlighting importance history visdial. fifth models looking outperform blind models least recall decoders. finally models best performance. dialog-level evaluation. using deﬁne round-level ‘success’ best discriminative model mn-qih-d gets rounds correct generative mn-qihg gets further mean ﬁrst-failure-round mn-qih-d mn-qih-g. fig. fig. show plots values figure percentage coverage question types round. conversations progress ‘is’ ‘what’ ‘how’ questions reduce ‘can’ ‘do’ ‘does’ ‘any’ questions occur often. questions starting ‘is’ popular dataset. late fusion encoder. encode image vgg- question concatenated history separate lstms concatenate three representations. followed fully-connected layer tanh nonlinearity vector used decode response. fig. shows model architecture encoder. hierarchical recurrent encoder encoder image representation vgg- early fused question. speciﬁcally image representation concatenated every question word lstm. qa-pair dialog history independently encoded another lstm shared weights. image-question representation computed every round concatenated history representation previous round constitutes sequence examples attention history facts encoder. model learns attend facts relevant question asked. example asked ‘what color kites?’ model attends people stand around ﬂying kites park.’ anyone bus?’ attends large yellow parked grass.’ note selected examples always attention weights interpretable. preprocessing. spell-correct visdial data using bing following lowercase questions answers convert digits words remove contractions tokenizing using python nltk construct dictionary words appear least times train giving vocabulary around hyperparameters. models implemented torch model hyperparameters chosen early stopping based mean reciprocal rank metric. lstms -layered -dim hidden states. learn -dim embeddings words images. word embeddings shared across question history decoder lstms. adam memory network. image encoded question lstm. concatenate representations follow fully-connected layer tanh non-linearity ‘query vector’. caption/qapair dialog history encoded independently lstm shared weights. query vector used compute attention facts inner product. convex combination attended history vectors passed fully-connected layer tanh non-linearity added back query vector. combined representation passed another fully-connected layer tanh non-linearity used decode response. model architecture shown fig. fig. shows", "year": 2016}