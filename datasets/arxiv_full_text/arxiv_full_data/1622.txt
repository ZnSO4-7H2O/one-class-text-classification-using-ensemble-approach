{"title": "Personalizing a Dialogue System with Transfer Reinforcement Learning", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users' data as a source domain and an individual user's data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose \"PETAL\"(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.", "text": "difﬁcult train personalized task-oriented dialogue system data collected individual often insufﬁcient. personalized dialogue systems trained small dataset likely overﬁt make difﬁcult adapt different user needs. solve problem consider collection multiple users source domain individual user target domain perform transfer learning source target domain. following idea propose personalized task-oriented dialogue system transfer learning framework based pomdp construct personalized dialogue system. petal system ﬁrst learns common dialogue knowledge source domain adapts knowledge target domain. proposed petal system avoid negative transfer problem considering differences source target users personalized q-function. experimental results real-world coffee-shopping data simulation data show proposed petal system learn different optimal policies different users thus effectively improve dialogue quality personalized setting. dialogue systems classiﬁed classes open domain dialogue systems task-oriented dialogue systems open domain dialogue systems limit dialogue topic speciﬁc domain typically clear dialogue goal. task-oriented dialogue systems solve speciﬁc task dialogues. paper focus dialogue systems assist users ﬁnish task ordering coffee. personalized task-oriented dialogue systems help user complete dialogue task better faster non-personalized dialogue systems. personalized dialogue systems learn preferences habits user interactions user utilize personalized information speed conversation process. personalized dialogue systems could categorized rule-based dialogue systems learning-based dialogue systems rulebased personalized dialogue systems dialogue state system speech user speech predeﬁned developers hence difﬁcult system dialogue state speech hard deﬁne manually. learning-based personalized dialogue systems could learn states actions training data without requiring explicit rules designed developers. however difﬁcult train personalized task-oriented dialogue system data collected individual often insufﬁcient. personalized dialogue system trained small dataset likely fail unseen common dialogue cases over-ﬁtting. solution consider collection multiple users source domain individual user target domain transfer common dialogue knowledges target domain. transferring dialogue knowledge challenge lies difference source target domains. works proposed transfer dialogue knowledge among similar users model difference different users might harm performance target domain. paper propose personalized taskoriented dialogue system transfer learning framework based pomdp learning personalized dialogue system. petal system ﬁrst learns common dialogue knowledge source domain adapts knowledge target user. achieve goal petal system models personalized policies personalized q-function deﬁned expected cumulative general reward plus expected cumulative personal reward. personalized q-function model differences source target users thus avoid negative transfer problem brought differences source target users. ﬂowchart petal system coffee-ordering task shown figure experimental results realworld coffee-shopping dataset simulation data show proposed petal system choose different optimal actions different users thus effectively improves dialogue quality personalized setting. contributions three-fold firstly tackle problem learning common dialogue knowledge source domain adapting target user personalized dialogue system. multi-turn dialogue systems learning optimal responses different situations non-trivial problem. naïve policy always choose previous seen sentences necessarily optimal. example online coffee ordering task naïve policy could incur many logical mistakes asking repeated questions conﬁrming order user ﬁnishes ordering. secondly propose transfer learning framework pomdp model preferences different users. unlike existing methods proposed petal system require manuallydeﬁned ground truth state space model personalized future expected reward. finally demonstrate effectiveness petal system real-world dialogue dataset well simulation data. personalized dialogue systems could categorized rule-based dialogue systems learningbased dialogue systems. rule-based personalized dialogue systems thompson propose interactive system users choose place interactive conversational process system could learn user preference improve future conversations. personalization frameworks proposed extract utilize user-related facts generate responses applying predeﬁned templates facts. different rule-based personalized dialogue systems learning-based personalized dialogue systems learn states actions training data without requiring explicit rules. casanueva propose initialize personalized dialogue systems target speaker data similar speakers source domain improve performance target speaker. work requires predeﬁned user similarity metric select similar source users selected similar users different target user performance target user degrade. genevay laroche propose select transfer optimized policy source users target user using multi-armed stochastic bandit algorithm require predeﬁned user similarity measure. however method high complexity since target user requires bandit selection operations number source users. moreover similar differences selected source users target user deteriorate performance. different works proposed method assume predeﬁned dialogue states system speech acts required rule-based systems explicitly models differences users. transfer learning applied tasks dialogue systems. gasic uses transfer learning extend dialogue system include previously unseen concept. gasic propose incremental scheme adapt existing dialogue management system extended domain. works transfer parameters policy source domain prior target domain. however models deal multiple source domains explicit personalized mechanisms different users. consequence negative transfer might occur differences users large. contrast proposed method explicit personalization mechanism alleviate negative transfer. argumentation agents works study personalized dialogue system. however works inﬂuence users’ goal different motivations formulations totally different ours. matrices denoted bold capital case vectors bold lower case scalars lower case. text dialogues denoted curlicue represented bag-of-words assumption. bag-of-words representations vector entry binary value. since current state dialogue observable ground truth dialogue states assumed unknown formulate dialogue pomdp deﬁned -tuple denotes hidden unobservable states denotes replies agent denotes users’ utterances state transition probability function reward function observation function discounted factor. i-th turn dialogue user reply deﬁne agent belief state vector represents probability distribution unobserved unlike previous work assume underlying ground truth state space provided. instead compact propose learn function dialogue history belief state vector inputs problem include order solve problem policy target user could choose appropriate action maximize cumulative reward deﬁned maxπ model belief states introduce state projection matrix dialogue history state q-function deﬁned expected cumulative reward according policy starting belief state choose value-based approaches usually small number training data target domain policy-based approaches generate responses word word require training data. order build personalized dialogue system target user need learn personalized q-function qπut user. however since training data {{out target user limited hardly estimate personalized q-function qπut order learn accurate qπut transfer common dialogue knowledge source domain i=}. however different users different data many users {{ous preferences hence directly using data source users would bring negative effects. propose model personalized q-function general q-function plus personal denotes general personal rewards user time respectively general q-function captures expected reward related general dialogue policy users parameters general q-function contains large amount parameters requires training data personal q-function transforms matrix vector columnwise manner rewrite linear function vect qgwt kronecker product multi-round dialogue systems different optimal actions different belief states. rationale kronecker product general q-function depend combination belief state personal q-function learns personalized preference user avoid negative effect brought transferring biased dialogue knowledge across users different preferences. denote possible choices j-th choice want collect choices proposed i-th agent response total number order choices hence exact choice example coffee-ordering task {latte cappuccino. could type coffees choice user j-th choice i-th dialogue turn. example could latte could iced. based assumption different choice sets independent other follows categorical distribution j-th choice probability user choose equals user made choice dialogue history otherwise. implies whether system receive personal reward rest dialogue q-function models cumulative future reward. controls importance personalized reward learned data. close zero q-function depend know nothing user show personal preference user vocabulary choices much smaller whole vocabulary estimate personal preference parameters dialogue data {{out combining general personal q-functions personalized q-function ﬁnally deﬁned personal reward received user conﬁrms suggestion agent negative reward received user declines suggestion agent. related personal information user. example user could conﬁrm address suggested agent. general reward received user proceeds payment. general reward received agent dialogue turn encourage shorter dialogue received agent generates non-logical responses asking repeated questions. total four sets parameters learned. denote parameters wp{pu}}. dealing real-world data training consists records optimal actions provided human hence loss function deﬁned follows value iteration method learn general personal q-functions. adopt online stochastic gradient descent algorithm learning rate optimize model. speciﬁcally state-action-reward-state-action algorithm. on-policy training simulation model decreasing probability choosing random reply candidate ensure sufﬁcient exploration number training dialogues seen algorithm. detailed petal algorithm shown algorithm train model user source domain. shared users separate user source domain. transfer target domain using initialize corresponding variables target domain train well target user limited training data. since source target users might different preferences learned source domain useful target domain. personal preference target user learned separately without modelling user different preferences source target users might interfere thus cause negative transfer. number parameters model around total vocabulary size dimension state vector. experiment number parameters general q-function personal q-function user hence parameters personalized q-function could learned accurately limited data target domain. compare proposed petal model baseline algorithms including nonetl trained data target users trained data target user similar user source domain bandit target user useful source user identiﬁed bandit algorithm priorsim target user policy similar user source domain used prior priorall target user dialogue policy trained users source domain used prior policy trained source users’ data. section evaluate model real-world dataset. real-world dataset collected july april coffee ordering service contains coffee dialogues consumers coffee makers. users order coffee providing coffee type temperature size delivery address hence order choices. select users dialogues source domain. remaining users used separately target domain. total coffee dialogues source domain coffee dialogues target domain. earlier dialogues target domain used training remaining dialogues form test set. statistics dataset shown table round testing conversation model rank ground truth reply among randomly chosen agent replies. label assigned randomly chosen agent replies following calculate score turn conversation performance algorithm measured average score dialogue every user test set. figure report mean standard deviation averaged score different random seeds used randomly sample agent replies candidates. performance nonetl priorsim priorall worse directly transfers training data ﬁtting target domain data cause overﬁtting. transferring data similar users good transferring data source users common knowledge learned data. proposed petal method performs best learns common knowledge users avoids negative transfer caused different preferences among source target users indicates proposed personalized model dialogues better demonstrates effectiveness petal real-world dataset. table case study real-world dataset. last case study shown table column shows candidate responses ground truth space limit show response marked ﬁrst second columns three candidates. results show predicted rewards petal proposed petal candidates. method ranks ground truth response ﬁrst place based predicted reward given learned personalized q-function method without personalization ranks wrong address higher demonstrates effectiveness proposed method. section compare model baseline models simulated coffee-ordering dialogue data. simulated users order coffee providing coffee type temperature size delivery address agents reply choosing predeﬁned candidate responses without knowing speech act. simulated users source domain users want coffee. before? tall macchiato deliver minsheng road pudong district shanghai? yes. please pay. payment completed. want coffee. before? tall macchiato deliver minsheng road pudong district shanghai? want iced mocha today. sure please pay. payment completed. coffee preferences rest preference. target domain users different preferences users source domain. simulator designed based real-world dataset used previous section. simulator order according preference probability otherwise simulator order coffee randomly. training user target domain dialogues test dialogues. reward experiment reward deﬁned section model choose reply candidates generated templates turn simulator react reply accordingly. model report mean standard deviation averaged reward averaged success rate averaged dialogue length possible target users repeated times different random seeds. results shown figure figure figure petal outperforms baselines obtains highest average reward highest success rate lowest dialogue length implies petal found better dialogue policy adapt behaviour according preference target users demonstrates effectiveness petal live environment. show typical case simulation data tables non-personalized dialogue system corresponding model users choices even frequent users table universal recommendation frequent users different preferences. however petal learned target users’ preferences previous dialogues. shown table response agent specially tailored target user personalized questions given petal method guide user complete coffee-ordering task faster general questions leading shorter dialogue higher averaged reward. user want everything usual shown second case table petal still react correctly shared dialogue knowledge transferred source domain. cases show petal choose different optimal actions different users effectively shorten conversation. paper tackle problem designing personalized dialogue system. propose petal system transfer learning framework based pomdp learning personalized dialogue system. petal system ﬁrst learns common dialogue knowledge source domain adapts knowledge target user. propose model personalized policy pomdp personalized q-function. framework avoid negative transfer problem brought differences source users target user. experimental results real-world coffee-ordering data simulation data show petal learn different optimal policies different users thus effectively improve dialogue quality personalized setting. future direction investigate transfer knowledge heterogeneous domains knowledge graphs images. references jeesoo bang hyungjong yonghee gary geunbae lee. example-based chatoriented dialogue system personalized long-term memory. proceedings international conference data smart computing pages inigo casanueva thomas hain heidi christensen ricard marxer phil green. knowledge transfer speakers personalised dialogue management. proceedings annual meeting special interest group discourse dialogue michel galley chris brockett alessandro sordoni yangfeng michael auli chris quirk margaret mitchell jianfeng bill dolan. deltableu discriminative metric generation tasks intrinsically diverse targets. arxiv preprint arxiv. milica gašic catherine breslin matthew henderson dongho martin szummer blaise thomson pirros tsiakoulis steve young. pomdp-based dialogue manager adaptation extended domains. proceedings annual meeting special interest group discourse dialogue milica gasic dongho pirros tsiakoulis catherine breslin matthew henderson martin szummer blaise thomson steve young. incremental on-line adaptation pomdpbased dialogue managers extended domains. proceedings annual conference international speech communication association pages takuya hiraoka graham neubig sakriani sakti tomoki toda satoshi nakamura. reinforcement learning cooperative persuasive dialogue policies using framing. proceedings international conference computational linguistics pages yonghee jeesoo bang junhwi choi seonghan sangjun gary geunbae lee. acquisition long-term memory personalized dialog systems. proceedings international workshop multimodal analyses enabling artiﬁcial agents human-machine interaction pages esther levin roberto pieraccini wieland eckert. learning dialogue strategies within markov decision process framework. proceedings ieee workshop automatic speech recognition understanding pages lili yiping song zhang jin. sequence backward forward sequences content-introducing approach generative short-text conversation. arxiv preprint arxiv. alan ritter colin cherry william dolan. data-driven response generation social media. proceedings conference empirical methods natural language processing pages ariel rosenfeld sarit kraus. providing arguments discussions basis prediction human argumentative behavior. transactions interactive intelligent systems yangqiu song erheng zhong qiang yang. transitive transfer learning. proceedings sigkdd international conference knowledge discovery data mining pages tsung-hsien milica gasic nikola mrksic pei-hao david vandyke steve young. semantically conditioned lstm-based natural language generation spoken dialogue systems. arxiv preprint arxiv. tsung-hsien milica gasic nikola mrksic lina rojas-barahona pei-hao stefan ultes david vandyke steve young. network-based end-to-end trainable task-oriented dialogue system. arxiv preprint arxiv.", "year": 2016}