{"title": "Empowerment for Continuous Agent-Environment Systems", "tag": ["cs.AI", "cs.LG"], "abstract": "This paper develops generalizations of empowerment to continuous states. Empowerment is a recently introduced information-theoretic quantity motivated by hypotheses about the efficiency of the sensorimotor loop in biological organisms, but also from considerations stemming from curiosity-driven learning. Empowemerment measures, for agent-environment systems with stochastic transitions, how much influence an agent has on its environment, but only that influence that can be sensed by the agent sensors. It is an information-theoretic generalization of joint controllability (influence on environment) and observability (measurement by sensors) of the environment by the agent, both controllability and observability being usually defined in control theory as the dimensionality of the control/observation spaces. Earlier work has shown that empowerment has various interesting and relevant properties, e.g., it allows us to identify salient states using only the dynamics, and it can act as intrinsic reward without requiring an external reward. However, in this previous work empowerment was limited to the case of small-scale and discrete domains and furthermore state transition probabilities were assumed to be known. The goal of this paper is to extend empowerment to the significantly more important and relevant case of continuous vector-valued state spaces and initially unknown state transition probabilities. The continuous state space is addressed by Monte-Carlo approximation; the unknown transitions are addressed by model learning and prediction for which we apply Gaussian processes regression with iterated forecasting. In a number of well-known continuous control tasks we examine the dynamics induced by empowerment and include an application to exploration and online model learning.", "text": "adaptive systems algorithms research groups school computer science university hertfordshire college lane hatﬁeld herfordshire united kingdom paper develops generalizations empowerment continuous states. empowerment recently introduced information-theoretic quantity motivated hypotheses efﬁciency sensorimotor loop biological organisms also considerations stemming curiositydriven learning. empowemerment measures agent-environment systems stochastic transitions much inﬂuence agent environment inﬂuence sensed agent sensors. information-theoretic generalization joint controllability observability environment agent controllability observability usually deﬁned control theory dimensionality control/observation spaces. earlier work shown empowerment various interesting relevant properties e.g. allows identify salient states using dynamics intrinsic reward without requiring external reward. however previous work empowerment limited case small-scale discrete domains furthermore state transition probabilities assumed known. goal paper extend empowerment signiﬁcantly important relevant case continuous vector-valued state spaces initially unknown state transition probabilities. continuous state space addressed monte-carlo approximation; unknown transitions addressed model learning prediction apply gaussian processes regression iterated forecasting. number well-known continuous control tasks examine dynamics induced empowerment include application exploration online model learning. goal research enable artiﬁcial agents intelligently complex difﬁcult environments. common view intelligent behavior engineered; either fully hand-coding necessary rules agent relying various optimizationbased techniques automatically generate example modern control dynamic programming human designer speciﬁes performance signal explicitly implicitly encodes goals agent. behaving optimizes quantity agent programmer wants many applications perfectly reasonable approach lead impressive results. however typically requires prior knowledge sometimes subtle design human developer achieve sensible desirable results. paper investigate approach embodiment agent generate preferred behaviors without resort specialized hand-designed solutions vary task task. research embraces related ideas self-organization self-regulation complex behavior derive simple generic internal rules. philosophy seemingly intentional goal-driven behavior emerges by-product agent trying satisfy universal rules rather optimizing externally deﬁned rewards. examples kind work include homeokinesis work second idea intrinsically motivated behavior artiﬁcial curiosity agent engages behavior inherently interesting enjoyable rather step towards solving speciﬁc goal. intrinsically motivated behavior directly help solving goal indications leads exploration allows agent acquire broad range abilities need arises easily molded goal-directed behavior. related relevant publications include example related work found consider principle empowerment information-theoretic quantity deﬁned channel capacity agent’s actions sensory observations subsequent time steps. empowerment regarded universal utility deﬁnes priori intrinsic reward rather value/utility states agent ﬁnds empowerment fully speciﬁed dynamics agent-environment coupling reward need speciﬁed. hypothesized greedy maximization empowerment would direct agent interesting states variety scenarios empowerment considered stochastic generalization concept mobility powerful heuristic many deterministic discrete puzzles games. state high empowerment gives agent wide choice actions conversely agent default mode poises priori highempowerment state best equipped quickly move variety target states emergency regard quantity empowerment allows agent automatically identify states even complex environments. present paper show that certain class continuous control problems empowerment provides natural utility function imbues states priori value without explicit speciﬁcation reward. problems typically tries keep system alive indeﬁnitely i.e. certain goal region long time possible. hand choosing wrong actions nothing would instead lead death system natural example pole-balancing. context smoothness system informs local empowerment gradients around agent’s state alive states are. choosing actions local empowerment score maximized would lead agent states. pole-balancing example means wide range initial conditions agent would made balance pendulum. previous studies empowerment showed promise various domains essentially limited case small-scale ﬁnite-state domains furthermore state transition probabilities assumed known priori. main contribution article extend previous work signiﬁcantly important case continuous vector-valued state spaces initially unknown state transition probabilities. ﬁrst property means able calculate empowerment values approximately; speciﬁcally montecarlo approximation evaluate integral underlying empowerment computation. second property considers case state space previously unexplored implies agent form online model learning estimate transition probabilities state-actionsuccessor state triplets encounters interacting environment. here approach model learning using gaussian process regression iterated forecasting. second part forms main technical portion. section starts formal deﬁnition empowerment continuous case gives algorithm computation based montecarlo approximation underlying high-dimensional integrals. section describes model learning using gaussian process regression however since rather complex subject matter brevity cannot beyond high-level description. third part examines empowerment empirically number continuous control tasks well known area reinforcement learning. experiments demonstrate empowerment form natural utility measure states high empowerment values coincide natural choice goal state respective domain. incorporate empowerment perception-action loop agent e.g. greedily choosing actions lead highest empowered states obtain seemingly goal-driven behavior. application this study problem exploration model learning using empowerment guide parts state-space exlore next agent quickly discover goal thus efﬁciently explore environment without exhaustively sampling state space. empowerment pole-balancing example ﬁrst investigated discretized state space priori known state transition probabilities. strongly extend example continuous case online learning. state transition probabilities initially known. instead agent learn transition probabilities interacting environment. logarithm effective number successor states agent induce actions. thus empowerment essentially measures extent agent inﬂuence environment actions zero regardless agent does outcome same. maximal every action distinct outcome. note empowerment speciﬁcally designed allow general stochastic environments deterministic transitions special case. example consider taxi-domain well-known problem reinforcement learning ﬁnite state action space stochastic transitions. environment shown left consists gridworld four special locations designated ’r’’y’’g’’b’. apart agent passenger wants four locations another state system coordinate agent location passenger destination overall distinct states. usually interest abstraction hierarchical learning factored representation state used explicitly exploits structure domain. purpose identifying salient states part problem assume structure domain known representation instead. agent possible elementary actions ﬁrst four move agent indicated direction resulting direction blocked wall movement occurs. agent also issue pick-up drop-off action require taxi correct location passenger car. issuing pick-up drop-off conditions result changes. passenger successfully delivered environment reset agent placed center passenger start destination generated. using state transition dynamics compute -step empowerment i.e. effective number successor states reachable action horizon steps every state system. figure shows results values ordered every subplot shows empowerment values correspond speciﬁc slice state space. example left subplot shows empowerment value locations passenger waiting destination labeling states corresponds states inspecting plots things become apparent general locations center high empowerment locations corners empowerment interesting empowerment value designated locations passenger waiting certain location empowerment neighbors steps away increases. similarly passenger empowerment destination neighbors steps away increases. reason situations agent additional previously unavailable ways affecting environment higher relative gain empowerment result episode teleports agent center). thus states stand interesting heuristic empowerment. incidentally also exactly subgoal states agent’s task transport passenger source destination. note specify external reward goals empowerment intrinsically computed transition dynamics alone. empowerment essentially discovers states additional degrees freedom available creates basin attraction around them indicating salient features environment interest agent. difﬁcult imagine agent uses empowerment guiding principle exploration; e.g. choosing state greedily action leads successor state highest empowerment. expect agent would traverse state space sensible figure plotting empowerment subset states taxi domain. clarity every plot shows mean-subtracted empowerment certain slice state space white corresponds empowerment black corresponds high empowerment blind random exploration following trail increasing empowerment would quickly lead discovery salient states environment. remainder paper develop methods carrying idea continuum demonstrate empowerment supersedes typical hand-designed rewards number established benchmark domains. empowerment deﬁned stochastic dynamic systems transitions arise result making decision e.g. agent interacting environment. assume vector-valued state space discrete action space na}. transition function given terms density denotes probability going state making decision assume system fully deﬁned terms -step interactions also interested general n-step interactions. thus consider sequence single-step actions induced probability density approach consider system evolving time-scale n-step actions n-step actions regarded -step actions higher level decision making. abstraction allows treat -step n-step actions equal footing simplify notation drop references time index. instead writing write denote transition irrespective whether n-step action -step action. furthermore symbol loop actions note consider stochastic transitions continuum. otherwise every action resulting successor states distinct empowerment always attains maximum value. practice usually case simulating continuous control tasks deterministic dynamics. case artiﬁcially zero mean gaussian noise small variance interpreted modeling limited action sensoric resolution depending take. also natural assumption robot realized hardware. strictly speaking entropies eqs. differential entropies probabilities read probability densities. however always using mutual information i.e. difference entropies well-deﬁned non-negative information values always ﬁnite limited resolution/noise assumed above. using summation). consider agent environment three states labeled possible actions denoted dynamics environment fully described -step transitions shown figure right side ﬁgure shows corresponding -step transitions derived -step transitions; example entry obtained resulting value depend individual probabilities actions transition probabilities ﬁxed given environment. natural choice action probabilities could uniform distribution. however empowerment assignment action probabilities resulting value maximimized among possible assignments calculated empowerment values example various time horizons i.e. -step -step etc. note that empowerment values logarithmic purpose illustration results given terms exp) ﬁrst column -step illustrates full range possible empowerment values. empowerment state zero actions outcome. empowerment state maximal action different outcome. state successor states overlap thus empowerment value extremes. time horizon increases make following observations. empowerment value always stays zero matter agent does outcome value goes down whereas value stays constant reason that time horizon increases number possible actions e.g. steps. however large number actions bring agent cannot escape. therefore actions contribute equal parts result lead zero empowerment dominate thus also result close zero. hand maximization suppress effect indistinguishable actions thus ensure distinct choices always correctly identiﬁed. summarize. empowerment measures extent agent inﬂuence environment actions. speciﬁcally works stochastic systems also apply deterministic systems empowerment zero regardless agent does outcome maximal every action distinct outcome brieﬂy discuss related information-theoretic quantity mutual information would largely properties would easier compute powerful channel capacity identifying interesting states environment. first comment idea modeling inﬂuence action channel deﬁne kind distribution actions. considering agent’s embodiment deﬁned controller default action distribution could use. therefore distinguish particular action distributions action channel measured. main natural choices choice action distribution equally distributed singling particular action maximizes i.e. achieves channel capacity. seen last section equidistribution actions fail resolve important properties action channel optimal distribution detect. obvious situation large number equivalent actions. mutual information assumes uniform distribution actions mislead large numbers actions lead outcome. another example consider following situation. assume agent different actions available state every action effect assume agent enters state shown left side actions still outcome action leads different state case mutual information equidistributed would still close zero indicating actions roughly effect whereas empowerment correctly identiﬁes distinct choices nats) since redistribute actions highlights additional degrees freedom attained blahut-arimoto algorithm em-like algorithm iterates distributions denotes k-th iteration step produce distribution achieves maximum since consider discrete action domain represented vector section reduce assumptions consider environment neither n-step -step transition probabilities readily available. instead assume could observe number -step transitions given triplets state performed action resulting successor state. using regression samples ﬁrst infer -step transition model. proceeding -step model obtain general n-step transition model iteratively predicting steps ahead time. general would many ways task regression could accomplished. gaussian process regression simple mathematically elegant powerful tools offer considerable advantages. directly produce predictive distribution target values exactly needed computation empowerment. furthermore predictive distribution gaussian hence easy draw samples monte-carlo approximation also nonparametric meaning model restricted certain class functions instead encompasses functions sharing degree smoothness. practice also easy solution found analytically closed form. bayesian framework allows nicely address problem hyperparameter selection principled makes process using virtually fully automated i.e. without adjust single parameter hand. learn state transition probabilities i.e. predict successor state performing -step action state combine multiple univariate gps. individual predicts j-th coordinate successor state action individual trained independently subset transitions action chosen desired target outputs regress change state variables since state variables actions treated separately need total independent gps. detailed description univariate regression work found training gives distribution j-th variable successor state exact equations mean variance found note every hyperparameters independently obtained associated training data bayesian hyperparameter selection. combining predictive models variables obtain desired distribution there also problem implementing efﬁciently dealing possible large number data points. brevity sketch particular implementation detailed information. implementation based subset regressors approximation. elements subset chosen stepwise greedy procedure aimed minimizing error incurred using rank approximation optimization likelihood done random subsets data ﬁxed size. avoid degenerate predictive variance projected process approximation used. figure learning state transition probabilities combining multiple univariate gps. individual predicts j-th coordinate successor state action individual trained independently corresponding subset training data hyperparameters turn -step model n-step model sequence -step actions integrate intermediate distributions. unfortunately solving integral analytically closed form possible. simple approach sampling methods like monte-carlo approximation numerically determine integral. alternatively could consider sophisticated approximate solution based laplace approximation proposed since experiments consider short prediction horizons naive approach predicting iteratively steps ahead using learned -step model. given state apply produce instead considering full distribution take mean ˆxt+ point estimate predict applying -step model produce repeating procedure until prediction horizon reached obtain steps approximation originally sought n-step transition model general approximation tend underestimate variance prediction produce slightly different mean since every time produce estimate ignore uncertainty preceding prediction case however procedure incur neglible error since prediction horizon consider short. details. indicated earlier empowerment shown intuitively appealing identiﬁcation salient states discrete scenarios ready study number intricate continuous scenarios. scenarios used benchmark typical learning algorithms however noted latter learning algorithms need instructed optimization criterion learning process. here always empowerment maximization criterion demonstrate resulting behaviors actually match closely optimization external quality criterion requested. observation behaviors match subtle point discussed detail discussion important side effect empowerment also used exploration driver scenarios particularly interesting since unlike optimal control algorithms empowerment fundamentally local opposed optimal control algorithms that informed decision need horizon extended encompass information desired target state sufﬁciently accurate extent. consider scenarios without model learning model learning. ﬁrst scenario demonstrate incorporating empowerment perception-action loop agent produces intuitively desirable behavior greedily choosing actions state lead highest empowered states. primary intent show empowerment relevant quantity considered simplicity assume transition probabilities system known. second scenario reduce assumptions consider longer case. agent starts knowing nothing environment combine empowerment model learning exploration while ﬁrst scenario agent chooses actions based empowerment underlying computations carried using learned model state transition probabilities. model continually updated transitions agent experiences thus gets continually better predicting effects actions have turn produce accurate empowerment values. comparison common model-based reinforcement learning rmax operates similar fashion actively optimizes external performance criterion concludes. testbeds experiments consider simulations three physical systems described below. reiterate that literature systems like usually used context control learning behavior goal externally deﬁned optimizing thus determined performance criterion system driven speciﬁcally reach goal. contrast empowerment used generic heuristic goal explicitly deﬁned operates innate characteristics system’s dynamic alone. turn empowerment intrinsically drives system states fact typically externally chosen goal states. however empowerment enforce goal external reward generic intrinsic quantity that domain generated exactly way. note that wider sense tasks belong class control problems goal choose actions system stays alive achieve this agent stay certain stable goal region. class problems believe empowerment particularly well-suited. dynamic equations system given appendix. force applied system returns stable equilibrium state initial condition. goal swing stabilize pole inverted position. however motor provide enough torque directly single rotation. instead pendulum needs swung back forth gather energy pushed balanced. creates somewhat difﬁcult nonlinear control problem. state space -dimensional angle angular velocity. since empowerment model deals ﬁnite number -step n-step actions control force discretized +.}. riding bicycle second domain involved consider bicycle riding task described depicted figure task bicycle-rider system moves constant speed horizontal surface. bicycle self-stabilizing actively stabilized prevented falling. goal keep bicycle stable continues move forward indeﬁnitely. detailed description dynamics system given appendix. problem -dimensional state variables roll angle roll rate angle handlebar angular velocity control space inherently -dimensional horizontal displacement bicycle-rider system vertical plane turning handlebar neutral position. since empowerment deal ﬁnite number -step n-step actions consider possible action vectors acrobot third domain acrobot proposed acrobot imagined gymnast swinging high bending hips. depicted figure acrobot two-link robot freely swings around ﬁrst joint exert force second joint controlling acrobot challenging problem nonlinear control; underactuated meaning dimensionality state space higher actuators informally degrees freedom actuators usually tasks considered acrobot literature ﬁrst easier swing lower link height upper link. second task signiﬁcantly difﬁcult ﬁrst task goal swing lower link; however time acrobot reach inverted handstand position close zero velocity actively balance remain highly unstable state long possible. detailed description dynamics system given appendix. initial state acrobot stable equilibrium links hanging vertically down. state space -dimensional since before empowerment deal ﬁnite number -step n-step actions continuous control discretized however actions alone sufﬁcient solve swing-up task sufﬁcient inverted balance since case control values extremes must chosen. therefore include third nonprimitive ’balance’ action chooses control values derived controller obtained linearizing system dynamics handstand position note ’balance’ action produces meaningful outputs close handstand state means cannot naively used direct acrobot balance arbitrary point state space. ﬁrst series experiments agent chooses actions greedily maximize empowerment. domains assume state transition probabilities known. control loop becomes following every time step agent observes current state using state transition function determine -step successor states possible -step actions. states compute empowerment value described section using iter adding gaussian white noise covariance smear otherwise deterministic state transitions. agent executes action corresponding successor state highest empowerment value advancing time producing next state xt+. note practice empowerment values meaningful usually require increased look-ahead horizon future single simulation step; thus instead -step empowerment usually need examine n-step empowerment values greater one. form n-step actions exhaustive enumeration; thus number possible -actions agent available number n-step actions consider computation empowerment experiment performed informally determined minimum time horizon lookahead necessary achieve desired effect. especially small simulation steps number -step actions needed given time horizon could grow relatively large turn would lead large number n-step actions rendering computational costs prohibitive. reduce number n-step actions still maintaining lookahead -step action action sequence held constant extended amount time multiple simulation step alternative would intelligently compress prune lookahead tree suggested discrete scenarios allows extend note metaparameters time horizon simulation step size amount noise independent must chosen judiciously domain seperately. example variance noise small relative average distance successor states empowerment always close maximal hand noise large relative average distance successor states empowerment longer distinguish effects different actions time full analytical understanding parameters interact best determine given domain disciplined trial error. horizon order magnitude similar complexity. here however going demonstrate even locally informed empowerment short lookahead horizons sufﬁcient treat aforementioned scenarios. results inverted pendulum figure shows phase plot behavior results starting initial condition following -step empowerment period seconds state transition noise plot demonstrates that empowerment alone makes agent drive pendulum successfully balance indeﬁnitely; agent accomplishes goal without explicitly told trajectory shows happens straight direct without wasting time note empowerment illuminates local potential future current state access global value trajectory opposed optimal control methods implicitly global information goal states must propagated back throughout system model controller take right decision. compare results different angle reformulate problem minimum-time optimal control task opposed before assume agent explicit externally speciﬁed goal step-wise cost function implements goal given since dimensionality state space dynamic programming directly determine optimal behavioral policy optimal means choosing actions accumulated costs minimized among possible behaviors comparing results figure using dynamic programming opposed using empowerment heuristic figure shows remarkable result empowerment achieve nearly behavior optimal control. result remarkable because unlike optimal value function underlying cost function tied particular goal empowerment generic heuristic operates innate characteristics dynamics system alone. results bicycle complex bicycle domain goal keep bicycle going forward preventing falling side other; angle vertical axis deviates much zero bicycle considered fallen. whenever happens bicycle stops moving forward matter action agent takes successor state future time steps consequently empowerment zero. examine behavior empowerment different initial conditions bicycle different trials varying angle interval interval initially zero cases. employ -step empowerment -step action action sequence held constant simulation steps state transition noise .i×. figure shows empowerment able keep bicycle stable wide range initial conditions; dots indicate bicycle successfully kept going forward seconds stars indicate not. note many cases failure would actually physically impossible prevent bicylce falling; example bicycle already strongly leaning left velocity pointing left. also note column corresponding zero angle shows outlier; empowerment able balance bicycle figure shows phase plot starting initial condition empowerment keeps bicycle stable brings system close point kept stable indeﬁnitely. based policy; shows empowerment able successfully balance bicycle large variety initial conditions; black vertical bars indicate failure states; value angle failure longer avoided. results acrobot highly challenging acrobot require deeper lookahead consider -step empowerment -step action action sequence held constant simulation steps state transition noise .i×. phase plot figure demonstrates empowerment leads successful swing-up behavior approaches unstable equilibrium particular makes agent actually balance inverted handstand position. figure illustrates numbers translate real physical system. figure shows corresponding empowerment shows every time step empowerment value state agent empowerment increase monotonically every single time step increases time reaches maximum outlier result inaccuracy produced monte-carlo approximation. repeating experiment larger number samples showed indeed bicycle balanced initial conditions. however note initial conditions already close boundary balancing becomes impossible regardless many samples used. handstand position. vertical ﬁgure indicates point ’balance’ action chosen ﬁrst time action highest empowerment. point choosing ’balance’ would sufﬁcient; however phase plot control variable reveals phase balance action always highest empowerment. note ’balance’ action appendix) produces values interval states close handstand position saturation behaves like actions otherwise. second experiment discuss scenarion empowerment extends potential applicability; interested model learning using empowerment extrapolate intelligently part state space explore next. particular consider case online model learning; i.e. learning state transition probabilities samples agent experiences interacting environment idea show empowerment avoid sampling state space exhaustively instead learn target behavior system-agent interactions. overview learning architecture depicted figure agent consists components. model learner stores history transitions seen current time implements multiple provide -step predictions n-step predictions second component action selector. given current state environment ﬁrst determine successor states possible -step actions using mean predictions successor state determine empowerment value using n-step predictions since predicted successor states depend accuracy adjust empowerment scores uncertainty associated -step prediction. uncertainty taken note that simplicity ignore gp-based model learner produces predictive distribution successor states thus naturally incorporates noise/stochasticity transitions. computationally would become quite unwieldy calculate every step expected empowerment value successor state closed form solution integral would evaluated approximately e.g. monte-carlo approximation. note simpliﬁcation change results. figure acrobot phase plot following empowerment-based policy. bottom right panel shows associated empowerment values. vertical shows ﬁrst time ’balance’ action chosen produced values extreme controls individual uncertainties state components employ called optimism face uncertainty less certain system want perform exploratory action. here linearly interpolate extremes maximum uncertainty minimum uncertainty concrete value maximum uncertainty minimum uncertainty depend hyperparameters implementing details agent executes highest ranked action observes outcome updates model accordingly summary control loop shown below experiment consider inverted pendulum domain comparatively easy dimensionality compute respective optimal behavior. dynamics domain modiﬁed obtain episodic learning task every steps state system reset initial condition episode starts. action selector computes empowerment using parameters previous section difference -step n-step successor states predicted current model. model learner updated every samples; employ kernel automatic selection hyperparameters. comparison consider rmax common model-based reinforcement learning algorithm also combines exploration model learning control operates unlike learning framework described section main difference rmax derived dynamic programming value iteration ﬁnds agent behavior optimizes given performance criterion. performance criterion before explicit cost function makes agent want reach goal fast possible. rmax learn model transitions environment cost function. former could done latter done gps. reason cost function every part state space except small region goal. since initial samples agent experiences region would rapidly conclude whole cost function ﬂat; since uncertainty model guides exploration would predict cost states high conﬁdence thus agent would miss goal long time usually done rmax therefore grid-based discretization estimate costs transitions. uncertainty prediction depends whether underlying grid-cell visited before. since rmax unvisited states attractive reaching goal agent tends explore environment exhaustively behave optimally. figure compare empowerment-based exploration rmax various spacings underlying grid examine division cells. every curve shows cumulative costs function episode. thus every curve parts transient agent still learning acting non-optimally steady-state agent acting optimally respect underlying bias either maximizing empowerment minimization costs. graph shows things ﬁner resolution grid longer takes rmax optimally. grid size agent reaches optimal performance episodes; grid size needs episodes; grid size needs episodes; grid size needs episodes. hand empowerment needs episodes steady-state behavior reached. steady-state performance empowerment somewhat worse rmax versus however surprising. empowerment consider externally deﬁned cost function making decisions whereas rmax speciﬁcally optimizes agent behavior performance respect particular cost function maximized. still behavior empowerment close would achieve explicitly optimizing cost function; however figure exploration model-based learning inverted pendulum domain. plot compares sample efﬁcieny ultimate performance learned behavior empowerment rmax different levels discretization grid sizes figure shows detail empowerment drives agent visit relevant part state space. ﬁgure compares empowerment rmax grid spacing state-action pairs visited learning various points time plots show that empowerment-based agent gp-based model learner accurately predict state transitions seen samples. accuracy predictions goes uncertainty predictions goes down becomes conﬁdent does. uncertainty turn means agent longer takes exploratory actions instead chooses highest empowerment. learned model accurate enough good knowing true transitions function agent behaves accordingly plot shows happens soon right within ﬁrst episode. rmax hand exhaustively sample state-action space essentially visit every grid-cell action. thus takes much longer even reach goal region learn desired behavior. central question need address empowerment actually carry intuitively desirable behaviour? previous work shown property spurious actually reappears number disparate scenarios figure distribution visited state-action pairs empowerment rmax. empowerment reaches goal region around point transitions right ﬁrst episode whereas rmax needs times long. empowerment agent explore limited parts state-action space model learned. rmax order also learn external cost function state-action space needs sampled exhaustively. oblique angle different upright position. even position sustainable position would clearly match state empowerment maximization strategy attain. nevertheless task placing pole arbitrary oblique position strikes unnatural nothing else speciﬁed task. words balancing inverted pendulum seems unbiased natural task scenario. however course scenarios preferred outcomes naturally arise system dynamics. obvious examples e.g. mazes needs reach particular goal state. goal state obviously arbitrary selected independently actual dynamics/topology system. even scenarios empowerment still mimics/approximates graph-theoretic notion centrality means empowerment maximization place agent location world expected distance randomly speciﬁed goal state minimal. words best guess agent place expectation unknown goal assuming wishes minimize number steps goal. completely omit discussion case different actions different costs different states obviously forces resort full-ﬂedged dynamic programming formalism. however clearly case speciﬁcation environmental structure dynamics sufﬁcient characterization task reward structure needs explicitly speciﬁed. issues balancing explicit rewards information-theoretic costs decision making intricate discussed detail elsewhere however performance scenarios even better natural goals would impose priori seem anticipated empowerment trying maximize. considered scenarios thing common survival-type scenarios. agent aims stay alive move away death states possible makes particularly interesting context continuous systems point concern present paper smoothness system informs local empowerment gradients around agent’s state alive states even discrete transition graphs display somewhat structured scenarios like grid-worlds small-world networks property attraction basins global good local empowerment optima visible distance. particularly striking since empowerment seems correlate well measures dominating states graphs hand-crafted purpose empowerment maximization coincides natural optimal control task computes local gradients towards right direction opposed optimal control/dynamic programming implicitly require global picture goal states are. open question properties required system provide relatively large attraction basins empowerment maxima visible local empowerment gradients. property seems present continuous environments environments degree globally homogeneous structures different however novel degrees freedom form gateways state space particular locations world grant access subregions state space otherwise inaccessable majority states. prime example taxi domain section actions picking dropping passenger open degrees freedom speciﬁc locations maze gateways usually irregular occurences state space typically detected empowerment reach action horizon. still intelligent action sequence extension algorithms suggested provide recourse larger effective action horizons even cases. however examples studied paper involve gateways require relatively short horizons virtue smooth structure. suggests signiﬁcant class dynamic control problems empowerment provide purely local exploration behaviour heuristic identiﬁes moves towards particularly interesting areas; present paper furthermore demonstrates implemented efﬁcient on-line fashion. paper discussed empowerment information-theoretic quantity measures agentenvironment system stochastic transitions extent agent inﬂuence environment actions. earlier work empowerment already shown various uses number different domains empowerment calculation previously limited case small-scale discrete domains state transition probabilities assumed known agent. main contribution paper relax assumptions. first paper extends calculation empowerment case continuous vector-valued state spaces. second discuss application empowerment exploration online model learning longer assume precise state transition probabilities priori known agent. instead agent learn interacting environment. addressing vector-valued state spaces model learning paper already signiﬁcantly advances applicability empowerment real-world scenarios. still computational point view open questions remain. question particular best deal continuous vectorvalued action spaces assumed paper action space could discretized. however higher dimensional action spaces naive discretization soon become infeasible. work partly taken place learning agents research group artiﬁcial intelligence laboratory university texas austin supported grants national science foundation darpa federal highway administration research partially supported european commission part feelix growing project contract ist-. views expressed paper authors necessarily consortium. refer schematic representation inverted pendulum given figure state variables angle measured vertical axis angular velocity control variable torque applied restricted interval motion pendulum described differential equation refer schematic representation acrobot domain given figure state variables angle ﬁrst link measured horizontal axis angular velocity angle second link ﬁrst link angular velocity motor allowed produce torques range since algorithm section allows compute empowerment ﬁnite possible -step actions discretized continuous control space. three actions ﬁrst correspond bang-bang control take extreme values however bang-bang control alone allow keep acrobot inverted handstand position unstable equilibrium. third action therefore introduce complex balance-action derived lqr. first linearize acrobot’s equation motion unstable equilibrium yielding constant gain matrix values resulting truncated stay inside valid range note controller works intended produces meaningful results state already close neighborhood handstand state; particular incapable swinging balancing acrobot initial state refer schematic representation bicycle domain given figure state variables roll angle bicycle measured vertical axis roll rate angle handlebar angular velocity control variables displacement bicycle-rider common center mass perpendicular plane bicycle torque applied handlebar. dynamic model bicycle system roll rate angular velocity kept interval saturation; roll angle restricted since algorithm section allows compute empowerment ﬁnite possible -step actions discretized continuous control space. consider following discrete actions meaning gravitation constant speed bicycle height ground common bicycle-rider center mass distance front back tire point touch ground radius tire vertical distance bicycle’s rider’s center mass horizontal distance front tire common center mass mass bicycle mass tire mass rider", "year": 2012}