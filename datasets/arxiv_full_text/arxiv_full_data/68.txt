{"title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "tag": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "abstract": "In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains $43.9/47.7\\%$ WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with $15.7\\%$ and $5.3\\%$ relative improvement respectively.", "text": "paper extend deep long short-term memory recurrent neural networks introducing gated direct connections memory cells adjacent layers. direct links called highway connections enable unimpeded information across different layers thus alleviate gradient vanishing problem building deeper lstms. introduce latency-controlled bidirectional lstms exploit whole history keeping latency control. efﬁcient algorithms proposed train novel networks using frame sequence discriminative criteria. experiments distant speech recognition task indicate train deeper lstms achieve better improvement sequence training highway lstms novel model obtains ./.% eval sets outperforming previous works. beats strong dlstm baselines relative improvement respectively. recently deep neural network -based acoustic models greatly improved automatic speech recognition accuracy many tasks improvements reported using advanced models convolutional neural networks long short-term memory recurrent neural networks although techniques help decrease word error rate distant speech recognition remains challenging task reverberation overlapping acoustic signals even sophisticated front-end processing techniques multi-pass decoding schemes. paper explore advanced back-end techniques dsr. reported deep lstm rnns help improve generalization often outperform single-layer lstm rnns. however dlstm rnns harder train slower converge. paper extend dlstm rnns introducing gated direct connection memory cells adjacent layers. direct links called highway connections provide path information layers directly without decay. alleviates gradient vanishing problem enables dlstm rnns training virtually arbitrary depth. here refer lstm highway connections hlstm rnn. ∗part work reported carried jelinek memorial summer workshop speech language technologies university washington seattle supported johns hopkins university grant gifts google microsoft research amazon mitsubishi electric merl. improve performance also introduce latency-controlled bidirectional lstm rnns. lc-blstm rnns past history fully exploited similar unidirectional lstm rnns. however unlike standard blstm rnns start model evaluation after seeing whole utterance lc-blstm rnns look ahead ﬁxed number frames limits latency. lc-blstm much efﬁciently trained standard blstm without performance loss. also trains decodes faster context-sensitive-chunk blstms access limited past future context. study conducted single distant microphone setup. compare standard lstm rnns highway lstm rnns layers. show highway lstm rnns dropout applied highway connection signiﬁcantly outperform standard lstm rnns. high connection helps train deeper networks better highway lstm rnns seem beneﬁt sequence discriminative training. overall proposed model decreased dnns cnns dlstm rnns relatively. best knowledge ./.% achieved eval sets best results reported task.. rest paper organized follows. section brieﬂy discuss related work. section introduce standard dlstm rnns blstm rnns highway lstm rnns. section describe lc-blstm train models efﬁciently frame sequence discriminative criteria. summarize experimental setup section report experimental results section conclusions reiterated section developing highway lstms independently noticed similar work done works share idea adding gated linear connections different layers. highway networks proposed adaptively carry dimensions input directly output information across layers much easily. however formulation different focus dnn. work share idea model structure. general uses generic form. however task text e.g. machine translation focus distant speech recognition. addition used dropout control highway connections turns critical dsr. long short-term memory rnns lstm initially proposed solve gradient diminishing problem rnns. introduces linear dependence memory cell state time cell’s state nonlinear gates introduced control information ﬂow. operation network follows equations iteratively logistic sigmoid function vectors represent values time input gate forget gate output gate cell activation cell output activation respectively. denotes element-wise product vectors. weight matrices connecting different gates corresponding bias vectors. matrices full except matrices cell gate vector diagonal. deep lstm rnns formed stacking multiple layers lstm cells. speciﬁcally output lower layer lstm cells although lstm layer upper layer input deep time since unrolled time become feedforward neural network layer shares weights deep lstm rnns still outperform single-layer lstm rnns significantly. conjectured dlstm rnns make better parameters distributing space multiple layers. note conventional dlstm rnns interaction cells different layers must output-input connection. highway lstm proposed paper illustrated figure direct gated connection memory cells lower layer memory cells upper layer carry gate controls much information lower-layer cells directly upper-layer cells. gate function layer time weight matrix connecting carry gate input layer. weight vector carry gate past cell state current layer. weight vector connecting carry gate lower layer memory cell. carry gate activation vectors layer thus depending output carry gates highway connection smoothly vary behavior plain lstm layer simply passes cell memory previous layer. highway connection cells different layers makes inﬂuence cells layer direct alleviate gradient vanishing problem training deeper lstm rnns. unidirectional lstm rnns described exploit past history. speech recognition however future contexts also carry information utilized enhance acoustic models. bidirectional rnns take advantage past future contexts processing data directions separate hidden layers. shown bidirectional lstn rnns indeed improve speech recognition results. study also extend hlstm rnns unidirection bidirection. note backward layer follows equations used forward layer except replaced exploit future frames model operates output forward backward layers concatenated form input next layer. nowadays gpus widely used deep learning leveraging massive parallel computations mini-batch based training. unidirectional models better utilize parallelization power card multiple sequences often packed mini-batch. truncated bptt usually performed parameters updating therefore small segment sequence packed minibatch. however applied sequence level training gpu’s limited memory restricts number sequences packed mini-batch especially lvcsr tasks long training sequences large model sizes. alternative speed using asynchronous based gpu/cpu farm section focused fully utilizing parallelization power single card. algorithms proposed also applied multi-gpu setup. speed training bi-direcctional rnns contextsensitive-chunk bptt proposed method sequence ﬁrstly split chunks ﬁxed length past frames future frames concatenated chunk left right context respectively. appended frames used provide context information generate error signals training. since trunk independently drawn trained stacked form large minibatches speed training. unfortunately model trained csc-bptt longer true bidirectional since history exploit limited left right context concatenated chunk. also introduces additional computation cost decoding since left right contexts need recomputed chunk. solve problems csc-bptt propose latencycontrolled bi-directional rnns. different csc-bptt model carry whole past history still using truncated future context. instead concatenating computing left contextual frames chunk directly carry left contextual information previous chunk utterance. every chunk training decoding computational moreover loading cost reduced factor history previous mini-batch instead ﬁxed contextual windows makes context exact compared uni-directional model. note standard blstm rnns come signiﬁcant latency since model evaluated seeing whole utterance. latency-controlled blstm rnns latency limited users. experiments process utterances parallel times faster processing whole utterances without performance loss. compared bptt approach times faster often leads better accuracy. increase number sequences mini-batch propose two-forward-pass solution sequence discriminative training recurrent neural networks. basic idea pretty straightforward. sequence training recurrent models mini-batch packaging method cross-entropy training case i.e. pack multiple sequences mini-batch small chunk ﬁrst forward pass collect log-likelihood frames mini-batch pool without updating model. collected log-likelihood certain number sequences. point able compute error signal sequence pool. roll back mini-batches computed error signals start second forward pass time update model using error signals pool. two-forward-pass solution able pack sequences mini-batch e.g. thus leading much faster training. evaluated models meeting corpus corpus comprises around hours meeting recordings recorded instrumented meeting rooms. multiple microphones used including individual headset microphones lapel microphones microphone arrays. work single distant microphone condition experiments. systems trained tested using split recommended corpus release training hours development test hours. training segments provided corpus including overlapped speech. models evaluated evaluation only. nist’s asclite tool used scoring. kaldi used feature extraction early stage triphone training well decoding. maximum likelihood acoustic training recipe used trains gmm-hmm triphone system. forced alignment performed training data triphone system generate labels neural network training. computational network toolkit used neural network training. start training -layer sigmoid units layer. -dimensional ﬁlterbank features together corresponding delta delta-delta features used feature vectors. training concatenated frames feature vectors leads dimension used force align training data generate labels lstm training. lstm models unless explicitly stated otherwise added projection layer layer’s output proposed trained -dimensional ﬁlterbank features. lstmp models hidden layer consists memory cells together -node projection layer. blstmp models hidden layer consists memory cells -node projection layer. highway companions share network structure except additional highway connections. learning rate halved gain observed. train unidirectional model truncated back-propagationthrough-time used update model parameters. bptt segment contains frames process utterances simultaneously. train latency-controlled bidirectional model also process utterances simultaneously. start learning rate minibatch used learning rate scheduler takes action. frame level cross-entropy training constraint regularization used. sequence training constraint regularization also applied whenever used corresponding cross-entropy trained model. ﬁxed sample learning rate sequence training lstm sequence training. performance various models evaluated using word error rate percent below. experiments conducted eval speciﬁed otherwise. since exclude overlapping speech segments model training addition results full eval also show results subset contains non-overlapping speech segments table gives performance -layer lstmp blstmp rnns well highway versions. performance network also listed comparison. table it’s clear highway version lstm rnns consistently outperform non-highway companions though small margin. dropout applied highway connection control high dropout rate essentially turns highway connection small dropout rate hand keeps connection alive. experiments early training stages small dropout rate increase epochs training. performance highway lstmp networks dropout shown table dropout helps bring highway networks. network goes deeper training usually becomes difﬁcult. table compares performance shallow deep networks. table normal lstmp network goes layers layers recognition performance degrades dramatically. highway network however increase little bit. table suggests highway connection lstm layers allows network much deeper normal lstm networks. perform sequence discriminative training networks discussed section detailed results shown table table suggests introducing highway connection lstmp layers beneﬁcial sequence discriminative training. example without highway connection sequence training -layer lstmp network brings relative improvement particular task. introducing highway connection dropout improvement relatively. relative improvement even larger non-overlapping segment subset roughly results suggest sequence training beneﬁcial highway connection deeper structure. presented novel highway lstm network applied far-ﬁeld speech recognition task. experimental results suggest type network consistently outperforms normal lstmp networks especially dropout applied highway connection control connection’s on/off state. experiments also suggest highway connection allows network much deeper larger beneﬁt sequence discriminative training. agarwal basoglu padmilac kamenev ivanov cyphers parthasarathi mitra huang zweig rossbach currey peng stolcke slaney huang introduction computational networks computational network toolkit microsoft technical report povey ghoshal boulianne burget glembek goel hannemann motl´ıˇcek qian schwarz silovsk´y stemmer vesel´y kaldi speech recognition toolkit asru seide chen feature engineering context-dependent deep neural networks conversational speech transcription proc. ieee workshop automfatic speech recognition understanding hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors available http//arxiv.org/abs/. dahl deng acero context-dependent pre-trained deep neural networks large-vocabulary speech recognition ieee transactions audio speech language processing vol. seide conversational speech transcription using context-dependent deep neural networks proc. annual conference international speech communication association hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine seltzer wang investigation deep neural networks noise robust speech recognition proc. international conference acoustics speech signal processing senior beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling fifteenth annual conference international speech communication association kumatani mcdonough microphone array processing distant speech recognition closetalking microphones far-ﬁeld sensors. ieee signal process. mag. vol. hain burget dines garner grzl hannani huijbregts karaﬁt lincoln transcribing meetings amida systems. ieee transactions audio speech language processing vol.", "year": 2015}