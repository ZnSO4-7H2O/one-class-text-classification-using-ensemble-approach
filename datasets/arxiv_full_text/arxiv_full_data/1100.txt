{"title": "Stochastic Gradient Descent: Going As Fast As Possible But Not Faster", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "When applied to training deep neural networks, stochastic gradient descent (SGD) often incurs steady progression phases, interrupted by catastrophic episodes in which loss and gradient norm explode. A possible mitigation of such events is to slow down the learning process. This paper presents a novel approach to control the SGD learning rate, that uses two statistical tests. The first one, aimed at fast learning, compares the momentum of the normalized gradient vectors to that of random unit vectors and accordingly gracefully increases or decreases the learning rate. The second one is a change point detection test, aimed at the detection of catastrophic learning episodes; upon its triggering the learning rate is instantly halved. Both abilities of speeding up and slowing down the learning rate allows the proposed approach, called SALeRA, to learn as fast as possible but not faster. Experiments on standard benchmarks show that SALeRA performs well in practice, and compares favorably to the state of the art.", "text": "applied training deep neural networks stochastic gradient descent often incurs steady progression phases interrupted catastrophic episodes loss gradient norm explode. possible mitigation events slow learning process. paper presents novel approach control learning rate uses statistical tests. ﬁrst aimed fast learning compares momentum normalized gradient vectors random unit vectors accordingly gracefully increases decreases learning rate. second change point detection test aimed detection catastrophic learning episodes; upon triggering learning rate instantly halved. abilities speeding slowing learning rate allows proposed approach called salera learn fast possible faster. experiments standard benchmarks show salera performs well practice compares favorably state art. machine learning algorithms require efﬁcient optimization techniques whether solve convex problems non-convex ones convex setting main focus order convergence rate non-convex case still experimental science. signiﬁcant efforts devoted devising optimization algorithms tailored typical regime models problem instances imagenet data size model dimensionality increase mainstream convex optimization methods adversely affected. hessian-based approaches optimally handle convex optimization problems however ill-conditioned scale approximations required overall stochastic gradient descent increasingly adopted convex non-convex settings good performances linear tractability within framework main issues know control learning rate objective reach satisfactory learning speed without triggering catastrophic event manifested sudden rocketing training loss gradient norm. finding \"how much much\" terms learning rate slippery game. depends current state system current mini-batch. often eventual convergence ensured decaying learning rate number mini-batches. learning rate decay effectively paper proposes novel approach adaptive called salera salera based conjecture that learning catastrophes well taken care learning process speed whenever successive gradient directions show general agreement direction frequent advent catastrophic episodes long observed neural practitioners raises question best mitigate impact. answer depends whether events could anticipated precision. framing catastrophic episodes random events adopt purely curative strategy detecting instantly curing catastrophic episodes. formally sequential cumulative change detection test page-hinkley test adapted used monitor learning curve reporting minibatch losses. change learning curve detected system undergoes instant cure halving learning rate backtracking former state. instant cure thought terms dichotomic approximation line search risk catastrophic episodes well addressed learning rate adapted agile manner alera process increases learning rate whenever correlation among successive gradient directions higher random comparing actual gradient momentum agnostic momentum built random unit vectors. contribution paper twofold. first proposes original efﬁcient control learning dynamics secondly opens approach handling catastrophic events salvaging signiﬁcant part doomed-to-fail runs experimental validation thereof compares favorably state mnist cifar- benchmarks revived last decade effective method training deep neural networks linear computational complexity size dataset faces limitations depending learning rate large learning trajectory leads catastrophic episodes; small convergence takes ages. dynamic adjustment learning rate therefore acknowledged issue since early days dealing catastrophic events deep learning exploding gradient problem described encounter steep cliff structures derivative landscape learning frequently training neural networks comes dealing events published work focuses creating conditions arise. among possibilities regularizations e.g. regularization max-norm regularization gradient clipping constraining gradient norm remain smaller constant another possibility. introduction batch normalization also helps diminishing frequency events. finally proper initialization unsupervised pre-training i.e. initializing optimization trajectory good region parameter space also diminish frequency events. information contained correlation successive gradient directions already heart delta-delta delta-bar-delta update rules proposed jacobs brieﬂy delta-bar-delta rule states parameter current gradient relaxed past gradients sign learning rate incremented additively; opposite sign learning rate decremented multiplicatively. decrementing learning rates faster increasing already advocated author adapt faster case catastrophic events. natural gradient descent approach considers riemaniann geometry parameter space using fisher information matrix precondition gradient. quadratic complexity dimension parameter space approximations designed deep networks notably approaches hessian-free martens interpreted adagrad also uses information past gradients precondition update parameter-wise manner dividing learning rate squared past gradients. several approaches proposed reﬁne adagrad mitigate learning rate decay including adadelta rmsprop adam adam based estimating ﬁrst second moments gradient w.r.t. parameter using ratio update parameters. moment estimates maintained exponential moving averages different weight factors default inertia ﬁrst moment higher orders magnitude second. seen salera also builds upon gradient second moment difference compared ﬁxed agnostic counterpart. learning rate computed time-step approximately maximally decrease expected loss loss function locally approximated parabola. finally andrychowicz address learning rate adaptation reinforcement learning problem exploiting evidence gathered current time steps infer would good decisions earlier accordingly optimizing hyper-parameter adjustment policy. remotely related momentum approaches classic nesterov versions derived sutskever rely relaxed past gradients indicating robust descent direction current gradient. salera involves components learning rate adaptation scheme ensures learning system goes fast can; catastrophic event manager charge detecting undesirable behaviors getting system back track. rationale basic idea proposed learning rate update compare current gradient descent random walk uniformly chosen gradient directions. indeed successive normalized gradient vectors referred cumulative path following larger norm uniformly drawn unit vectors gradient directions positively correlated. cases learning process global direction process afford speed opposite norm cumulative path smaller random equivalent gradient directions anti-correlated process alternating opposite directions learning rate decreased. alera scheme takes inspiration famed cma-es algorithms today considered among best-performing derivative-free continuous optimization algorithms. approaches facto implement natural gradient optimization instantiate information-geometric optimization paradigm space normal distributions ird. formally cma-es maintains normal distribution. variance normal distribution step-size updated basis comparison cumulative path algorithm random walk gaussian moves ﬁxed step-sizes. mechanism said agnostic makes assumption whatsoever properties optimization objective. alera algorithm partial adaptation cma-es scheme minimization loss function d-dimensional parameter space deﬁned follows. solution time gradient current loss current learning rate. computes solution time ηtgt. denote norm associated product. deﬁnition. exponential moving average normalized gradients weight random equivalent deﬁned denote limits respectively time step alera scheme updates cumulative path comparing norm distribution agnostic momentum deﬁned learning rate increased decreased depending normalized squared norm approach implemented alera algorithm given hyper-parameters well initial learning rate mini-batch size iteration mini-batch computes exponential moving average normalized gradient performs agnostic update learning rate updating parameter usual learning rate controlled layer-wise fashion independently maintaining exponentiated moving average updating learning rate layer neural network algorithm used experiments section parameter-wise learning rate adaptation noted kingma parameterwise control learning rate desirable contexts. scheme extended achieve parameter-wise update learning rate follows. denote squared coordinate straightforward show expectation expectation divided squared coordinate noted comparison random counterpart save possible backtracks compute gradient backward pass exponential moving average normalized gradients agnostic learning rate update standard parameter update healthy learning regime training error decrease along time recovery noise inter-batch variance unless learning system abruptly meets cliff structure usually blamed large learning rate uneven gradient landscape. convex noiseless optimization setting computationally tractable best strategy compute optimal learning rate line search. context thought experiment that used update resulting would yield performance yields worse performance continued optimization process likely diverge. yields performance improvement. yields performance improvement too; trajectory likely bounce back forth walls optimum valley. overall safety zone learning rate proposed safeguard strategy primarily aims detect steps outside safety zone apply correction back upon change detection mini-batch loss salera implements straightforward correction halving learning rate recovering last solution test triggering. halving process iterated needed sending back exponentially fast rationale halving trick based trade-off number successive dividing iterations could indeed made even smaller using larger dividing factor required standard alera iterations needed reach optimal learning rate reached safety zone again. choice dividing factor discussed appendix detection salera applies change detection test signal given minibatch loss detection test chosen provides optimal guarantees trade-off detection delay upon change mean time false alarms. maintains empirical mean signal cumulative deviation empirical mean ¯t)). finally records empirical bounds case stationary signal expectation construction; change test thus triggered empirical bounds higher problem-dependent threshold controls alarm rate. test implemented salera algorithm follows tenth empirical loss ﬁrst minibatch experiments variables lmax maintained learning context decrease loss signal welcomed expected. case increasing signal thus monitored. upon test triggering learning rate halved weight vector reset last solution test reinitialized goal following experiments validate algorithmic ideas introduced section comparing application widely used optimization techniques straightforward architectures. datasets experiments performed mnist cifar datasets respectively contain training examples. contain test examples classiﬁed classes. data normalized according mean standard deviation along coordinate training set. algorithms adagrad adam used baselines. agnostic adaptation rule change detection applied independently. order separate effect original algorithms studied here alera implements agnostic learning rate adaptation without change detection; ag-adam uses agnostic adaptation learning rate adam algorithm. finally change detection mechanism implemented agnostic adaptation yielding salera described algorithm well parameter-wise version spalera lenet-inspired convolutional models models contain convolutional layers max-pooling followed fully connected layers relu activation. respective sizes mnist cifar-. batch normalization used layer. experimental conditions computations performed gpus tesla using torch library double precision. typical titan mini-batches size cifar- takes minutes algorithms. metrics mnist cifar- classiﬁcation problems. therefore report classiﬁcation accuracy test epochs epochs well standard deviations independent runs. learning performances experimental evidence shows ag-adam quite often slightly statistically signiﬁcantly improves adam. possible explanation ag-adam ﬂexible adjustment learning rate adam many cases alera salera yield similar results; indeed whenever alera meet catastrophic episodes alera salera behaviors. representative alera salera undergo catastrophic episodes depicted fig. alera faces series catastrophic episodes training error reaches eventually stabilizes medium training loss test error meanwhile salera reacts upon ﬁrst catastrophic episode around epoch halving learning rate layer. faces catastrophic episode around epoch halves learning rates again. overall faces less frequent less severe accidents. eventually salera recovers acceptable train test errors. interesting note learning rates fig. constantly increasing contradiction common knowledge. practice learning rate behavior depends dataset neural architecture seed diverse diverse learning rate behavior viewed original feature proposed approach made possible ability detect recover from catastrophic explosions training loss. actual behavior algorithms depicted cifar- model fig. ag-adam alera salera respectively getting ﬁrst second third rank terms test error epochs. terms optimization salera reaches training error close epoch whereas ag-adam reaches plateau epoch meanwhile test error decreases test loss increases three algorithms. tentative interpretation fact neural yields crisp output close change error increasing loss. result suggests several perspectives work order determine extent best results table depend hyper-parameter settings deﬁne best conﬁgurations considered benchmarks sensitivity analysis performed comparing results models epochs setting choosing lowest ranks. results robust settings displayed table proposed hyper-parameters alera salera found interestingly enough optimal hyper-parameters adam instead suggested original paper proposed approaches still show advantage adam though seem sensitive parameter tuning. left work derive precise recommendations depending model characteristics. catastrophe management performances deﬁne failed attains test error epochs. alera observed failed runs parameter range deﬁned catastrophe management scheme makes possible salera avoid approximately failures reaching rate failure parameter range. would course possible diminish failure rate saleraby setting alarm threshold rather used experiments reported above. however would potentially interfere learning rate adaptation triggering learning rate halvings even serious alert made thus preventing bold possible. indeed setting causes decline half figure comparison representative mnist alera salera test error minibatch error learning rates ﬁrst catastrophic episode around epoch salera reacts dividing learning rate three layers. note catastrophic events salera rare less severe alera salera eventually yields better training loss considerably better test error alera. better seen color. salera performances even though manages approximately halve number failed runs. furthermore setting diminish failure rate therefore harming salera learning performances. hand setting signiﬁcantly improve best performances higher failure rate. tenth initial loss therefore good balance aggressive learning rate adaptation scheme braking counterpart least datasets architectures. ﬁrst proposed contribution relies comparison gradient momentum ﬁxed reference. meant estimate overall correlation among sequence gradients thought signal-to-noise ratio process generated current solution objective successive mini-batches. depending ratio process accelerated slowed down. alera procedure implements idea proves signiﬁcantly able increase decrease learning rate. furthermore process plugged adam performance improvement average. price ﬂexibility increases risk catastrophic episodes instant rocketing training loss gradient norm. proposed approach relies conjecture catastrophic episodes rigorously observed detected. second conjecture neural optimizer almost doomed face episodes along optimization process. events mostly detrimental optimization often chooses small learning rates prevent them mostly recovered from. based conjectures second contribution paper agnostic principled detect address episodes. detection relies page-hinkley change point detection test. soon event detected learning rates halved previous solution recovered. short-term perspective research apply proposed approach recurrent neural networks consider complex datasets. another perspective replace halving trick approximating line search e.g. exploiting gaps actual momentum reference several values momentum weight factor. third perspective regards adaptation detection threshold learning. runs test triggered over resulting small learning rate preventing improvement. goal adapt mechanism re-initialized current loss values. another perspective apply salera heartfully thank steve altschuler lani making work possible supporting well insightful discussions. also thank yann ollivier sigurd angenent insightful discussions anonymous reviewers preliminary version paper accurate constructive comments. andrychowicz denil colmenarejo hoffman pfau schaul freitas. learning learn gradient descent gradient descent. sugiyama luxburg guyon garnett editors nips pages bottou bousquet. tradeoffs large scale learning. platt koller singer roweis editors advances neural information processing systems volume pages defazio bach lacoste-julien. saga fast incremental gradient method support non-strongly convex composite objectives. ghahramani welling cortes lawrence weinberger editors nips pages duchi hazan singer. adaptive subgradient methods online learning stochastic optimization. technical report ucb/eecs-- eecs department university california berkeley http//www.eecs.berkeley.edu/pubs/techrpts// eecs--.html. erhan bengio courville p.-a. manzagol vincent bengio. unsupervised pre-training help deep learning? mach. learn. res. mar. issn http//dl.acm.org/citation.cfm?id=.. george powell. adaptive stepsizes recursive estimation applications approximate dynamic programming. mach. learn. oct. issn ./s---. http//dx.doi.org/./s---. glorot bengio. understanding difﬁculty training deep feedforward neural networks. proceedings thirteenth international conference artiﬁcial intelligence statistics aistats chia laguna resort sardinia italy pages http//www.jmlr.org/proceedings/papers/v/glorota.html. ioffe szegedy. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning icml lille france july pages http//jmlr.org/ proceedings/papers/v/ioffe.html. martens. deep learning hessian-free optimization. proceedings international conference machine learning june haifa israel pages http//www.icml.org/papers/.pdf. ollivier arnold auger hansen. information-geometric optimization algorithms unifying picture invariance principles. journal machine learning research http//jmlr.org/papers/v/-.html. pascanu mikolov bengio. difﬁculty training recurrent neural networks. proceedings international conference international conference machine learning volume icml’ pages iii––iii–. jmlr.org http//dl. acm.org/citation.cfm?id=.. srivastava hinton krizhevsky sutskever salakhutdinov. dropout simple prevent neural networks overﬁtting. mach. learn. res. jan. issn http//dl.acm.org/citation.cfm?id=.. sutskever martens dahl hinton. importance initialization momentum deep learning. proceedings international conference international conference machine learning volume icml’ pages iii––iii–. jmlr.org http//dl.acm.org/citation.cfm?id=.. proceedings twentieth international conference international conference machine learning icml’ pages aaai press isbn ---. http //dl.acm.org/citation.cfm?id=.. algorithm gives detailed implementation parameter-wise version alera salera brieﬂy described section line denotes coordinate-wise multiplication context notations section make even simpler assuming dimension minimizing parabola straightforward show that independently current solution optimal value learning rate value loss deteriorate assume current learning rate recovery phase salera used prevent catastrophic event dividing factor show best trade-off bringing back reaching optimal value discussed section test triggered ﬁrst phase brings successive divisions number divisions logζ second phase uses standard alera procedure reach value η−]. context assume simpliﬁed procedure updates multiplying small consider cases depending whether smaller greater compute expectation number iteration salera needed reach ηt+. hence standard update phase decreases multiplying becoming less length update phase increases thus construction minimal meanwhile length division phase decreases increases; thus optimal value multiplications needed close summarize different computational costs involved catastrophe detected grail reached. cost dividing iterations involves forward pass current minibatch. denote cost hand standard alera iterations larger cost involving forward pass plus backward pass weights update. denote cost looking value minimize total cost reaching catastrophic event detected i.e. minimizes easy empirically check global minimum depends value constant assumed small .]). increases asymptotic value. however value initially chosen historical reasons reference famed doubling trick frequently used different areas machine learning. light results simple case work investigate slightly larger values.", "year": 2017}