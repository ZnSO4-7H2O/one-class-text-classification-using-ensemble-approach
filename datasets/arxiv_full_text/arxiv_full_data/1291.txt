{"title": "Efficient Dictionary Learning with Sparseness-Enforcing Projections", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Learning dictionaries suitable for sparse coding instead of using engineered bases has proven effective in a variety of image processing tasks. This paper studies the optimization of dictionaries on image data where the representation is enforced to be explicitly sparse with respect to a smooth, normalized sparseness measure. This involves the computation of Euclidean projections onto level sets of the sparseness measure. While previous algorithms for this optimization problem had at least quasi-linear time complexity, here the first algorithm with linear time complexity and constant space complexity is proposed. The key for this is the mathematically rigorous derivation of a characterization of the projection's result based on a soft-shrinkage function. This theory is applied in an original algorithm called Easy Dictionary Learning (EZDL), which learns dictionaries with a simple and fast-to-compute Hebbian-like learning rule. The new algorithm is efficient, expressive and particularly simple to implement. It is demonstrated that despite its simplicity, the proposed learning algorithm is able to generate a rich variety of dictionaries, in particular a topographic organization of atoms or separable atoms. Further, the dictionaries are as expressive as those of benchmark learning algorithms in terms of the reproduction quality on entire images, and result in an equivalent denoising performance. EZDL learns approximately 30 % faster than the already very efficient Online Dictionary Learning algorithm, and is therefore eligible for rapid data set analysis and problems with vast quantities of learning samples.", "text": "therefore eligible rapid data analysis problems vast quantities learning samples. keywords sparse coding sparse representations dictionary learning explicit sparseness constraints sparseness-enforcing projections great variety classical machine learning problems sparse solutions attractive provide efﬁcient representations compared non-sparse solutions. overwhelming evidence mammalian brains respect sparseness principle holds true especially mammalian visual cortex suggests sparseness fundamental prior variety signal processing tasks. particular includes low-level image processing since natural images represented succinctly using structural primitives interesting biologically plausible sparse representations discovered computer simulations natural images related representations obtained analysis temporal image sequences stereo image pairs images chromatic information enforcing topographic organization sparseness alleviates effects random noise natural since prevents arbitrary combinations measured signals fact methods based sparse representations shown achieve state-of-the-art performance image denoising notable image processing applications beneﬁt efﬁciency gained sparseness diverse deblurring super-resolution compression depth estimation abstract learning dictionaries suitable sparse coding instead using engineered bases proven effective variety image processing tasks. paper studies optimization dictionaries image data representation enforced explicitly sparse respect smooth normalized sparseness measure. involves computation euclidean projections onto level sets sparseness measure. previous algorithms optimization problem least quasi-linear time complexity ﬁrst algorithm linear time complexity constant space complexity proposed. mathematically rigorous derivation characterization projection’s result based soft-shrinkage function. theory applied original algorithm called easy dictionary learning learns dictionaries simple fast-to-compute hebbian-like learning rule. algorithm efﬁcient expressive particularly simple implement. demonstrated despite simplicity proposed learning algorithm able generate rich variety dictionaries particular topographic organization atoms separable atoms. further dictionaries expressive benchmark learning algorithms terms reproduction quality entire images result equivalent denoising performance. ezdl learns approximately faster already efﬁcient online dictionary learning algorithm communicated julien mairal francis bach michael elad. thom driveu institute measurement control microtechnology university germany e-mail markus.thomuni-ulm.de fig. visualization hoyer’s sparseness measure abscissa ordinate specify entries two-dimensional vector obtained sparseness degree color coded. dashed lines contour levels intervals tasks needs model capable reproducing signals processed. linear generative model sparse coding sample features expressed approximately linear combination atoms larger dictionary rd×n sparsely populated. here dictionary ﬁxed samples sparse code word depends concrete sample. columns represent atoms also called bases ﬁlters. sparse coding framework well-suited overcomplete representations dictionary generated wavelets example adapted speciﬁc task solving optimization problem measurement data. latter also called dictionary learning context. sparseness acts regularizer. constrained sparseness trivial choices would sufﬁce perfect reproduction capabilities. sparse represented additive superposition small number bases thus preventing trivial solutions. fundamental problem working sparse representations decide function formally assesses sparseness vector. pseudo-norm simply counts nonzero entries argument. poor choice since non-continuous prone random noise fails fulﬁll desirable properties meaningful sparseness measures here denote manhattan norm euclidean norm respectively. normalization designed attains values zero one. satisﬁes entries vanish. conversely entries equal. function interpolates smoothly extremes fig. moreover scale-invariant sparseness degree obtained vector multiplied nonzero number. hence quantity given units example millivolts instead volts adjustments whatsoever made. sparseness degree respect change much small amount added entries vector whereas pseudo-norm would indicate vector completely non-sparse. properties render hoyer’s sparseness measure intuitive especially nonexperts. employed successfully dictionary learning smoothness results improved generalization capabilities classiﬁcation tasks compared pseudo-norm used common approach dictionary learning minimization reproduction error original samples learning approximations provided linear generative model sparseness constraints beneﬁcial practitioners endusers enforce explicit sparseness constraints demanding code words generative model possess target sparseness degree leads optimization problems form here objective function reproduction error implemented euclidean distance. implicit sparseness constraints hand augment reproduction error additive penalty term yielding optimization problems trading reproduction error additive sparseness penalty non-trivial since actual resulting code word sparseness cannot easily predicted. explicit constraints guarantee adjusted sparseness degree making tuning intransparent scale factors mathematical tool achieve explicit sparseness sparseness-enforcing projection operator. vectorvalued function maps given point euclidean space nearest point achieves pre-set target sparseness degree. case theory projected gradient methods given objective function optimized subject hard side conditions. replacing parameters best approximations lying certain update step ensures constraints satisﬁed optimization progress. paper studies dictionary learning explicit sparseness constraints respect hoyer’s sparseness measure major part work devoted efﬁcient algorithmic computation sparseness-enforcing projection operator integral part efﬁcient dictionary learning. several algorithms proposed past solve projection problem thom palm provided complete mathematically satisfactory proof correctness algorithm. moreover known algorithms least quasi-linear time complexity dimensionality vector projected. paper ﬁrst derive characterization sparseness projection demonstrate computation equivalent ﬁnding root real-valued auxiliary function constitutes much simpler problem. result used proposition algorithm projection operator asymptotically optimal sense complexity theory time complexity linear space complexity constant problem dimensionality. show experiments real computing machine newly proposed algorithm superior computational demands previously proposed techniques even small problem dimensionalities. existing approaches dictionary learning feature explicit sparseness constraints categorized ones hoyer’s ones employ pseudonorm. hoyer potluru considered matrix factorization frameworks subject constraints space requirements linear number learning samples prevents processing large data sets. thom palm designed sparse code words result sparseness-enforcing projection operator applied product dictionary samples. requires following approaches consider explicit pseudonorm constraints aharon skretting engan coates infer sparse code words iteration compatible data dictionary employing basis pursuit matching pursuit algorithms negative impact processing time. zelnik-manor consider block-sparse representations signals assumed reside union subspaces. duarte-carvajalino sapiro propose simultaneously learn dictionary sensing matrix example image data results improved reconstruction results compressed sensing scenarios. addition contributions sparseness projection computation paper proposes easy dictionary learning algorithm. technique aims dictionary learning explicit sparseness constraints terms hoyer’s sparseness measure using simple fast-tocompute biologically plausible hebbian-like learning rule. presented learning sample sparsenessenforcing projection operator carried out. ability perform projections efﬁciently makes proposed learning algorithm particularly efﬁcient less training time required comparison optimized online dictionary learning method mairal extensions easy dictionary learning facilitate alternative representations topographic atom organization atom sparseness includes example separable ﬁlters. furthermore demonstrate competitiveness dictionaries learned algorithm computed alternative sophisticated dictionary learning algorithms terms reproduction denoising quality natural images. since tasks deblurring super-resolution build upon optimization problem application phase reproduction denoising expected ezdl dictionaries exhibit performance degradations tasks either. remainder paper structured follows. section derives linear time constant space algorithm computation sparseness-enforcing projections. sect. easy dictionary learning algorithm explicitly sparseness-constrained dictionary learning proposed. section reports experimental results performance newly proposed sparseness projection algorithm easy dictionary learning algorithm. section concludes paper discussion appendix contains technical details mathematical statements sect. section proposes linear time constant space algorithm computation projections onto level sets hoyer’s formally denotes target sparseness degree arbitrary point point level minimizes euclidean distance sought. function computes argmins∈sx− also called sparseness-enforcing projection operator since situation particular interest. thom palm proved intersection scaled canonical simplex hypercircle. further demonstrated projections onto computed ﬁnite number alternating projections onto geometric structures. proof correctness method could rely classical alternating projection method lacking convexity rendering proof arguments quite complicated. remainder section proceeds follows. first succinct characterization sparseness projection given. shown computing sparseness projections equivalent ﬁnding zero monotone realvalued function. prove achieved optimal asymptotic complexity proposing algorithm solves projection problem. main theoretical result paper representation theorem characterizes projection onto gained analysis intermediate points emerge alternating projections onto simplex hypercircle. closed-form expression provided showing intermediate points projection algorithm satisfy loop-invariant certain property fulﬁlled step alternating projections. since mathematical rigorous treatment result technical deferred appendix. assume vector projected given projection unique. since guaranteed points except null exactly projection exclude points nonunique projections considerations restriction practice. state characterization projection outcome representation theorem denote unique projection onto exactly real number words projection point rescaled obtain norm vector computed input vector subtracting scalar entries afterwards setting negative values zero. remarkable projection admits simple representation although target projection non-convex geometrically quite complicated. function maps scalar constant offset called soft-shrinkage function. also called rectiﬁer function. because central element projection representation soft-shrinkage operation applied entry-wise input vector carrying projections onto level sets hoyer’s sparseness measure interpreted denoising operation projection problem hence reduced determining soft-shrinkage offset one-dimensional problem real line. further reasonable must smaller maximum entry since otherwise would null vector absurd. usual case conclude must non-negative. otherwise result projection using representation theorem would less sparse input impossible. therefore projection problem reduced ﬁnding number bounded interval xmax maxi∈{...n} next method efﬁciently deciding whether correct offset found required. similar projection onto canonical simplex design real-valued function vanishes exactly wanted offset. properties hoyer’s allow formulate function intuitive way. call fig. plot auxiliary function derivative random vector derivative scaled using positive number improved visibility. steps exactly places coincides entry enough neighboring entries exact solution computed closed-form expression. rationale concrete deﬁnition follows. representation theorem know projection onto merely scaled version point moreover scale-invariance hoyer’s follows essence ratio norm norm argument. hence scale constants make normalized interval omitted target norms used instead. therefore thusψ unique conclude offset soft-shrinkage operation leads correct solution. fact write therefore exemplary plot depicted fig. clearly auxiliary function continuous differentiable except isolated points strictly decreasing except ﬁnal part constant. since constant part always negative starts offset equal second-largest entry xnd-max max{xi xmax} feasible interval reduced even more. step discontinuities coincide exactly entries input vector appealing analytical properties greatly simplify computation zero since standard rootﬁnding algorithms bisection newton’s method employed numerically expression requires indices coordinates result projection positive known. around known indices exactly ones coordinates greater offset sufﬁciently close determined numerically appropriate index determined thus exact value decision candidate offset close enough true made quite efﬁciently since coupled index sets individual entries input vector hence sufﬁces right-most left neighbor entries analogously left-most right neighbor whenever zero must located continuity reasons. values greater exactly values greater equal determined simply scanning entries based upon considerations ﬂowchart proposed method computing sparseness-enforcing projections depicted fig. algorithm performs bisection continuously checks sign changes auxiliary function soon fulﬁlled computed result projection determined in-place. complete formal presentation discussion proposed algorithm given appendix. therefore show overall algorithm possesses asymptotic complexity proved number bisection iterations required ﬁnding independent length initial interval upperbounded xnd-max discussed earlier. bisection moreguaranteed stop latest interval length smaller minimum pairwise distance distinct entries n}}. fact enough sufﬁciently small range deduce analytical solution. required number bisection iterations less number bounded regardless dimensionality input vector since xnd-max upper-bounded lower-bounded ﬁnite machine precision hence sparseness-enforcing projection algorithm asymptotically optimal sense complexity theory. still hidden constants asymptotic notation render proposed algorithm less efﬁcient previously known algorithms small input dimensionality. experiments sect. demonstrate case. section proposes easy dictionary learning algorithm dictionary learning explicit sparseness constraints. first introduce ordinary formulation learning algorithm. sparse code word inference sparseness projection algorithm proposed sect. renders ezdl efﬁcient particularly simple implement. discuss extensions allow concentrating different aspects data investigation topographic organization atoms atom sparseness. little implementation effort required extensions computational demands low. description comprehensive ezdl learning algorithm accompanied several strategies improve optimization performance. costly optimization procedure updating dictionary. ezdl learns dictionary ﬁrst yielding sparse code words simple inference models tuning dictionary simple update step. inference model function accepts ﬁlter responses form product dictionary rd×n learning sample produces representation certain desirable properties. here combination sparseness-enforcing projection operator particularly interesting since provides natural method fulﬁll explicit sparseness constraints. call choice ordinary inference model denotes euclidean projection onto vectors achieve sparseness degree respect hoyer’s sparseness measure computation also interpreted trivial ﬁrst iteration projected landweber procedure sparse code word inference dictionary adapted data minimizing deviation learning sample approximation linear generative model. goodness approximation assessed differentiable similarity measure ezdl optimization problem becomes note variable optimization problem since deﬁned output inference model. reason need constrained inherently satisﬁes explicit sparseness constraints. although wide range similarity measures possible decided correlation coefﬁcient since invariant afﬁne-linear transformations context visual data analysis corresponds invariance respect components gain factors. moreover differentiability similarity measure facilitates gradientbased learning assumed constant neglected actually depends without assumption gradient would extended additive term comprises gradient inference model this turn requires computation sparseness-enforcing projection operator’s gradient. gradient computed simple vector operations show appendix. however constitutes signiﬁcant computational burden compared simple fast-to-compute ezdl update step. section demonstrates experiments simpliﬁcation still perfectly capable learning dictionaries. topographic organization dictionary’s atoms similar self-organizing maps topographic independent component analysis achieved straightforward ezdl using alternative inference models proposed section. this dictionary atoms interpreted arranged two-dimensional grid. spatial pooling operator subject circular boundary conditions used incorporate interactions atoms located adjacent grid. example realized averaging entry ﬁlter responses neighborhood grid. convolution operation expressed linear operator applied vector represented sparsely populated matrix rn×n. containing atom interactions call topographic inference model. alternative realization topographic organization enforcing structure sparseness ﬁlter responses reshaped matrix account grid layout. structure sparseness assessed application sparseness measure vector singular values matrix. pseudo-norm used rank matrix. matrix rank also measured robustly ratio schatten -norm schatten norm essentially hoyer’s sparseness measure applied singular values clearly matrix ra×b reshaped vector vectorization operator stacks columns linear operation inverted provided shape original matrix known vec− a×b. following denote projection onto matrices possess target rank min{ab}}. classical result states matrix projected onto low-rank matrices computation singular value decomposition applying pseudo-norm projection vector singular based considerations natural numbers describing topographic grid layout dimensionalities. rank-κh topographic inference model deﬁned composition words inference model operates follows. ﬁrst approximates ﬁlter responses sparsely populated vector attains sparseness respect hoyer’s sparsiﬁed ﬁlter responses laid grid replaced best approximating ﬁlter responses meet lower rank min{rc}}. finally grid layout reshaped vector since output type inference model. vectors vec− ﬁlter responses expressed dyadic product reshaped grid. demonstrate experiments results interesting topographic atom organization similar independent subspace analysis analogous non-negative matrix factorization sparseness constraints atoms enforced sparse well. achieve this sufﬁcient apply sparseness projection column dictionary learning epoch situation learning samples resemble image patches atoms also restricted fulﬁll structure sparseness using low-rank approximation. suppose image patches possess pixels following projection carried learning epoch here min{ pw}} denotes target rank atom reshaped pixels. dictionary used process images convolution small values beneﬁcial since computations speeded considerably. example atom expressed outer product vectors respectively. convolutional kernel separable case one-dimensional convolutions lead result two-dimensional convolution require fraction operations previous sections simple hebbian-like learning rule derived depends abstract inference models. core inference models sparseness-enforcing projection operator discussed sect. guarantees code words always attain sparseness degree easily controllable user. presentation learning sample update step dictionary hence consists determining ﬁlter responses feeding ﬁlter responses inference model intermediate steps carried efﬁciently. sparseness projection computed using resources neither gradient gradient entire inference model required. learning epoch possible impose additional constraints atoms atom-wise projections. several aspects data structure made explicit since atoms forced become simple feature detectors. example individual learning epoch given algorithm topographic inference model low-rank structure sparseness used. addition explicit expressions correlation coefﬁcient gradient given outcome parameterization applied patches natural images depicted fig. discussed sect. efﬁciency learning algorithm improved simple modiﬁcations described following. reduce learning time compared randomly initialized dictionary used dictionary initialized samples randomly chosen learning large learning sets millions samples available stochastic gradient descent proven result signiﬁcantly faster optimization progress compared degrees freedom updated learning epoch step size schedule denotes initial step size n\\{} learning epoch counter beneﬁcial true gradient available rather erroneous estimate learning epoch atoms dictionary normalized input algorithm accepts following parameters existing dictionary rd×n atoms. random access learning samples function \"pick_sample\". step size update steps number samples presented mepoch topographic inference model pick random sample. pick_sample compute filter responses. evaluate topographic inference model. compute approximation compute correlation coefficient unit scale. since learning rule hebbian-like prevents atoms growing arbitrarily large becoming arbitrarily small normalization step also common multitude alternative dictionary learning algorithms responses updated. result atoms never updated target sparseness degree large. behavior alleviated adding random noise inference model’s output prior updating dictionary forces atoms updated. used random numbers sampled zero-mean normal distribution standard deviation multiplicatively annealed learning epoch. optimization proceeds atoms well-distributed sample space lifetime sparseness approximately equal population sparseness randomness needed anymore. section reports experimental results techniques proposed paper. ﬁrst evaluate alternative solvers sparseness projection’s auxiliary function show algorithm sparseness-enforcing projection operator signiﬁcantly faster previously known methods. turn application projection easy dictionary learning algorithm. first morphology dictionaries learned natural image patches analyzed context previous methods. show resulting dictionaries well-suited reproduction entire images. achieve reproduction quality equivalent dictionaries trained alternative significantly slower algorithms. eventually analyze performance dictionaries employed image denoising task that analogous reproduction experiments performance degradation observed. proposed algorithm sparseness projections implemented program using scientiﬁc library ﬁrst analyzed whether solvers bisection locating zero auxiliary function would result improved performance. since differentiable except isolated points derivatives computed quite efﬁciently newton’s method halley’s method straightforward apply. further veriﬁed whether newton’s method applied slightly transformed variant auxiliary function minuend subtrahend squared would behave efﬁciently. methods based derivatives additionally safeguarded bisection guarantee positions always located within well-deﬁned intervals impairs theoretical property fig. number auxiliary function evaluations needed ﬁnal interval projection onto using four different solvers. error bars indicate standard deviation distance mean value. since newton’s method applied consistently outperforming solvers method choice practical applications. evaluate solver would provide maximum efﬁciency practical scenarios thousand random vectors problem dimensionality sampled corresponding sparseness-enforcing projections computed using four solvers. used random vectors input solvers counted number times auxiliary function needed evaluated solution found. results experiment depicted fig. number function evaluations required plain bisection grew linearly log. expected minimum difference distinct entries random vector gets smaller dimensionality random vector increased expected number function evaluations bisection needs increases problem dimensionality. either case length interval found always bounded machine precision number function evaluations bisection bounded regardless solvers based derivative respectively always required less function evaluations bisection. exhibited growth clearly sublinear log. newton’s method required function evaluations mean halley’s method needed iterations newton’s method applied found solution iterations. therefore practice always latter solver employed. another experiment time competing algorithms require computing sparseness projections real computing machine measured comparison. this algorithms proposed hoyer potluru thom palm implemented using used easy dictionary learning algorithm data analysis tool visualize structure patches natural images different aspects facilitates qualitative statements algorithm performance. experiments used mcgill calibrated color image database images either pixels pixels. images desaturated quantized -bit precision. extracted patches pixels images \"foliage\" collection. patches extracted random positions. patches vanishing variance omitted since carry information. total obtained million samples learning. learned dictionaries pixel values learning whitened image patches. learning samples normalized zero mean unit variance pre-processing pixel experiments. whitened data obtained using canonical preprocessing section hyvärinen principal components. ezdl applied using proposed combination inference model atom constraints. presented randomly selected learning samples thousand epochs corresponds sweeps entire learning set. initial step size noisy version ﬁnal dictionary could already observed ﬁrst epoch demonstrating effectiveness ezdl quick analysis. since optimization procedure probabilistic initialization selection training samples repeated training times parameter set. observed minor variations between dictionaries parameter set. following present details dictionaries optimized pixel values analyze ﬁlter characteristics. then consider dictionaries learned whitened data dictionaries separable atoms trained pixel values discuss underlying reasons special morphology. using ordinary inference model trained two-times overcomplete dictionaries atoms normalized pixel values dictionary sparseness degree varied resulted familiar appearance dictionaries gabor-like ﬁlters resemble optimal stimulus visual neurons small values ﬁlters mostly small sharply bounded high sparseness resulted holistic blurry ﬁlters. fig. speed-ups relative original algorithm hoyer obtained using alternative algorithms. linear time algorithm proposed paper superior terms execution time. time complexity competing methods least quasi-linear becomes noticeable especially large problem dimensionalities. scientiﬁc library means programs. intel core processor used algorithms single-threaded environment. random vectors sampled problem dimensionality initially projected attain sparseness respect hoyer’s initial projection better reﬂects situation practice completely random vectors processed. next four algorithms used compute projections target sparseness degree time measured. original algorithm hoyer slowest taking ratio times algorithms time slowest algorithm relative speed-up obtained. figure visualizes results experiment. tested problem dimensionalities proposed linear time algorithm dominated previously described methods. speed-up algorithms potluru thom palm relative original algorithm hoyer already signiﬁcant especially small medium dimensionalities relatively slow large vectors. surprising methods start sorting input vector store permutation undone end. algorithm proposed paper based rootﬁnding monotone function requires sorting. left right neighbors scalar vector found. achieved scanning linearly input vector particularly efﬁcient huge vectors processed. proposed algorithm times faster methods hoyer potluru thom palm appealing asymptotic behavior obstacle applying smooth sparseness methods large-scale problems. fig. mean values three inﬂuential parameters gabor functions ﬁtted atoms dictionaries learned ezdl dependence target degree dictionary sparseness increase sparseness results ﬁlters reduced spatial frequency signiﬁcant increase width gaussian envelope. fig. histograms spatial phase ﬁtted gabor functions sparseness yields dictionaries odd-symmetric ﬁlters dominate whereas high dictionary sparseness results signiﬁcant amount even-symmetric odd-symmetric ﬁlters. used methods developed jones palmer ringach detailed analysis. two-dimensional gabor function deﬁned equation ringach gaussian envelope multiplied cosine carrier wave dictionary atom using algorithm nelder mead veriﬁed gabor accurately described atoms conﬁrmed gabor-like nature ﬁlters. differences dictionaries varying sparseness degrees became apparent analysis parameters ﬁtted gabor functions. figure shows mean values three inﬂuential gabor parameters dependence parameters change continuously monotonically increasing sparseness. spatial frequency factor cosine wave gabor function constantly decreases increasing dictionary sparseness. width gaussian envelope broken standard deviation principal orientations monotonically increasing. envelope width direction increases earlier envelope width direction larger. concluded sparse code words result ﬁlters lower frequency larger envelope. since increased sparseness reduces model’s effective number degrees freedom prevents constrained dictionaries adapting precisely learning data. similar principal component analysis low-frequency atoms minimize reproduction error best effective atoms allowed histograms spatial phase ﬁlters additive term cosine wave gabor function depicted fig. peak corresponds odd-symmetric ﬁlters distribution clearly bimodal peaks corresponding even-symmetric odd-symmetric ﬁlters respectively. case small matches result ordinary sparse coding higher dictionary sparseness results ﬁlters characteristics optimal stimulus macaque visual neurons analysis proves ezdl’s minimalistic learning rule capable generating biologically plausible dictionaries constitute particularly efﬁcient image coding scheme. obtain dictionaries diverse characteristics enough adjust target degree dictionary sparseness normalized scale. major component learning algorithm sparseness projection enforcing local competition among atoms absolute order-preservation property whitening pre-processing step helps reduce sampling artifacts decorrelates input data also changes intuition similarity measure ezdl’s objective function linear features rather single pixels considered feature captures multitude pixels patches. results differences ﬁlter structure particularly emergence low-frequency ﬁlters. dictionary depicted fig. learned using topographic inference model average-pooling neighborhoods. dictionary sparseness degree number atoms arranged grid. dictionary closely resembles gained topographic independent component analysis invariant predictive sparse decomposition noted representation times overcomplete whitening procedure. overcomplete representations inherently possible plain independent component analysis limits expressiveness restriction hold ezdl. emergence topographic organization explained special design topographic inference model. pooling operator acts spatial low-pass ﬁlter ﬁlter responses smoothed ﬁlter response carries information neighboring ﬁlter responses. filter response locality retained sparseness projection hence adjacent atoms receive similar updates hebbian-like learning rule. hence small differences ﬁlters within vicinity. learning process similar self-organizing maps atom maximum ﬁlter response atoms direct surrounding updated. however ezdl simultaneously updates multiple clusters since sparse code words laid grid multimodal. further notable achieved topographic organization merely linear operator simple average-pooling. stands contrast discussion bauer memisevic necessity nonlinearity multiplicative interaction pooling respect euclidean norm assumed. result simple linear operator plugged ordinary inference model already produces smooth topographies proves linear interactions atoms sufﬁcient despite minimality. figure shows dictionary obtained rank- topographic inference model using sparseness degree ﬁlters grid. here sparse code words reshaped matrix required unit rank results specially organized ﬁlter layout. example rows three four almost exclusively contain low-frequency ﬁlters ﬁlters sixth column oriented vertically. grouping similar atoms rows columns related independent subspace analysis yields groups gabor-like ﬁlters ﬁlters group approximately frequency orientation rank- topographic inference model guarantees code words expressed dyadic product vectors factors sparsely populated code words sparse. causes code words possess sparse rectangular structure reshaped account grid layout non-vanishing activity always concentrated rows columns. hebbian-like learning rule therefore induces similar updates atoms located common rows columns explains obtained group layout. enforcing ﬁlters rank setting resulted bases similar discrete cosine transform also observed recently hawe considered tensor decomposition dictionary rigamonti minimized schatten -norm atoms. note ezdl merely replaces atoms learning epoch best rank- approximation. computational complexity operation negligible compared individual learning epoch using tens thousands samples hence slow actual optimization. lack details explained restriction topographic organization reduces effective number degrees freedom. analogously situation sect. dictionary sparseness increased reproduction error reduced large amount using ﬁlters frequencies. reason checkerboard-like ﬁlters minimization reproduction error ﬁrst achieved principal orientations checkerboards used encode details image patches. conclusion results show variety dictionaries produced ezdl vast simply adapting easily interpretable parameters. algorithm covers reproduces well-known phenomena literature allows precise visualization data set’s structure. ﬁrst impression ﬁnal dictionaries already obtained learning epoch takes second modern computer. demonstrated sect. easy dictionary learning algorithm produces dictionaries correspond efﬁcient image coding schemes. analyze suitability ezdl dictionaries trained pixel values reproduction entire images. prerequisite several image processing tasks image enhancement compression since essentially optimization problem solved reproduction given dictionary. analysis allows quantitative statements original images reproduction sparse coding numerically compared signal level. further ezdl dictionaries compared results online dictionary learning algorithm mairal recursive least squares dictionary learning algorithm skretting engan terms reproduction quality learning speed. evaluation methodology follows. first dictionaries different parameter sets trained million pixels natural image patches extracted \"foliage\" collection mcgill calibrated color image database sect. patches picked random positions desaturated quantized images. patches normalized zero mean unit variance training. dictionaries designed four times overcomplete representation atoms samples pixels. training dictionary quality atoms sparseness degree atoms diagonal structure cannot present constrained dictionaries diagonality requires ﬁlter rank greater one. although ﬁlters paraxial rank- constraint still resembled contrast ﬁelds grating-like appearance. representation’s sparseness known induce appearance. similar ﬁlter morphology obtained topographic ﬁlter ordering using grid though ﬁlters blurrier shown fig. here checkerboard-like ﬁlters located clusters ﬁlters vertical horizontal orientation spatially adjacent. expected sparse topography local blobs active entries code word causing similar atoms grouped together dissimilar reproduction quality dependence number landweber iterations carried inference. four curves show different combinations dictionary sparseness inference sparseness achieved reproduction quality dependence inference sparseness four different dictionary sparseness degrees large values yield best performance large fig. results reproduction entire images using easy dictionary learning dictionaries. landweber iteration sufﬁces learning better reproduction performance yielded landweber procedure convergence. relation dictionary sparseness inference sparseness inﬂuential. hardly information loss observed case shows dictionaries trained sparse code words demanding sparseness reproduction. single image subsequently divided nonoverlapping blocks pixels. blocks normalized zero mean unit variance optimized dictionaries used infer sparse code word block. resulting code word linear generative model using currently investigated dictionary mean value variance restored match original block. sparse code word inference achieved projected landweber procedure essentially projected gradient descent starting null vector. using sparseness-enforcing projection operator iteration code words tuned attain inference sparseness need necessarily equal sparseness degree used dictionary learning. correlation coefﬁcient used similarity measure inference beneﬁt invariance shifting scaling. automatic step size control carried bold driver heuristic note representation theorem sparseness projection process also understood iterative softthresholding reproduced image yielded applying procedure distinct blocks original image using single dictionary. deviation output image original image assessed ssim index yielded qualitatively similar results peak signal-to-noise ratio. ssim index however normalized values zero respects local structure images since examines pixels neighborhoods convolution. measures visual similarity images intuitive measures based pixel-wise squared error evaluation method yielded number interval image dictionary. parameterization dictionary learning used train dictionaries compensate probabilistic effects mean resulting ssims reported. figure visualizes results obtained dictionaries produced easy dictionary learning using dictionary sparseness degrees thousand learning epochs carried presenting samples epoch using initial step size dictionary learning ﬁrst trivial iteration landweber procedure carried sparse code word inference computing ordinary inference model. ﬁrst analyzed impact carrying landweber iterations image reproduction phase effect varying inference sparseness degree huge performance increase obtained using iteration inference almost optimal performance achieved iterations after hundred iterations method converged. performance dictionaries equal yielding maximum ssim one. value indicates visual difference reproduced images original images. fig. experimental results image reproduction alternative dictionary learning algorithms used. dictionaries behave similarly easy dictionary learning dictionaries dictionary sparseness parameter inference sparseness varied. inference sparseness increased difference choice dictionary sparseness becomes noticeable. performance already degrades degradation substantial intuitive relation. almost information lost reproduction enforcing lower inference sparseness dictionary sparseness performance worse plausible dictionary adapted higher sparseness degree. natural case performance mainly depends concrete value higher sparseness results worse reproduction capabilities. investigate behavior number landweber iterations varied smoothly interval four different dictionary sparseness degrees. results experiment depicted fig. hardly difference reproduction quality irrespective value difference training dictionaries different sparseness degrees ﬁrst becomes visible large values performance better using dictionaries sparse code words demanded learning. hence tasks require high code word sparseness dictionaries trained high values dictionary sparseness. comparison conducted experiments using online dictionary learning algorithm mairal recursive least squares dictionary learning algorithm skretting engan inference sparse code words minimizes reproduction error implicit sparseness constraints. rlsdla uses external vector selection algorithm inference hence explicit constraints target pseudonorm demanded easily. algorithms update dictionary sample presentation. require step sizes adjusted. crucial parameter dictionary sparseness number controls trade-off reproduction capabilities code word sparseness. trained dictionaries using atoms presenting learning samples thousand epochs. then evaluation methodology used assess reproduction capabilities. results shown fig. choice inﬂuential compared ezdl. small performance differences hardly difference observed. similar ezdl experiments large values result better performance large worse performance small rls-dla provides forgetting factor parameter behaves analogously step size gradient descent. used default forgetting factor schedule interpolates using cubic function thousand learning epochs. inference chose optimized orthogonal matching pursuit variant parameter controls number non-vanishing coordinates code words. trained dictionaries atoms thirty thousand randomly drawn learning samples presented learning epoch. resulting reproduction performance shown fig. again large values dictionaries sparse code words learning perform best trained small values fig. comparison dictionary performance dependency dictionary sparseness degrees eliminated rls-dla ezdl respectively. three algorithms produce dictionaries equally well-suited reproduction entire images. fig. reproduction quality dependence number samples presented online dictionary learning recursive least squares dictionary learning algorithm easy dictionary learning initial step size varied latter. approaches achieved performance learning epochs. compare three dictionary learning algorithms independent concrete dictionary sparseness took mean ssim value belonged best performing dictionaries feasible value also interpreted using convex hull results shown fig. fig. yielded curve dictionary learning algorithm depicted fig. minor difference curves entire range hence concluded algorithms learn dictionaries equally well-suited reproduction entire images. although ezdl uses simple learning rule sufﬁcient enough achieve performance competitive state-of-the-art dictionary learning algorithms. moreover compared learning speed methods investigated inﬂuence initial step size ezdl dictionaries. carried reproduction experiments entire images using dictionaries provided learning algorithms certain numbers learning epochs. thousand overall epochs learning samples input three algorithms. dictionary sparseness parameters ezdl online dictionary learning recursive least squares dictionary learning algorithm. timing measurements intel core processor shown ezdl learning epoch takes approximately less time learning epoch odl. rls-dla roughly times slower ezdl. although employed vector selection method present part optimized software library fairly unoptimized implementation actual learning algorithm inference sparseness hundred landweber iterations carried sparse code word inference. ssim index eventually used assess reproduction performance. figure visualizes results learning speed analysis averaged images evaluation dictionaries trained parameterization compensate probabilistic effects. online dictionary learning free step sizes dictionary performance increased instantly constantly learning epoch. small performance gains could observed learning epochs. performance dictionaries trained rls-dla initially better dictionaries improved slowly. hundred epochs however possible observe signiﬁcant performance gain. although tweaking forgetting factor schedule better early reproduction capabilities rls-dla achieved performance equivalent epochs. ezdl’s initial step size unity performance ﬁrst degraded started improve ﬁfth epoch. epochs performance identical odl’s three methods obtained equal reproduction capabilities epochs. reduction initial step size caused dictionaries perform better sample presentations. quality online dictionary learning achieved epochs ezdl dictionaries always better mean epoch. hardly quality difference epochs small initial step size resulted however slightly worse performance. ered signiﬁcant difference reproduction capabilities could observed epochs. since ezdl epoch faster online dictionary learning epoch proposed method produces better results earlier absolute time scale. demonstrated sect. dictionaries learned easy dictionary learning good obtained online dictionary learning algorithm mairal recursive least squares dictionary learning algorithm skretting engan terms reproduction quality entire images. ﬁnal experiment investigated suitability ezdl dictionaries image denoising using image enhancement procedure proposed mairal method carries semi-local block matching ﬁnds sparse code words imposing group sparseness penalty euclidean reproduction error. pre-learned dictionary used explains appearance uncorrupted images helps resolve ambiguities block matching fails provide large enough groups. linear generative model ﬁnally used estimators denoised image patches sparse code words. procedure quite robust input data noisy since sparseness provides strong prior well regularizes ill-posed inverse problem denoising approach mairal also provides possibility dictionary adaptation denoising concrete input data. option would hinder resilient statements dictionary’s eligibility would modiﬁed another dictionary learning algorithm denoising. methodology experiment follows. used four-times overcomplete dictionaries trained pixels image patches \"foliage\" collection mcgill calibrated color image database models uncorrupted images. dictionary sparseness parameters ezdl rls-dla. evaluation used images \"animals\" collection mcgill database converted -bit grayscales previous experiments. images synthetically corrupted additive white gaussian noise. five noisy images generated original image standard deviation fig. denoising performance terms peak signal-to-noise ratio using denoising method proposed mairal small performance difference dictionaries trained rls-dla ezdl. results depicted fig. denoising performance degrades noise strength increased. hardly difference dictionaries trained three algorithms. rls-dla ezdl dictionaries perform slightly better small synthetic noise levels improvement visually imperceptible. result surprising since sect. demonstrated three learning algorithms produce dictionaries equally well-suited reproduction entire images. denoising procedure mairal aims reproduction capabilities well modiﬁcation employing noisy samples input. image enhancement compression applications proposed yang dong skretting engan horev also problem formulations based reproduction error hence expected beneﬁt efﬁciently learned dictionaries well. paper proposed ezdl algorithm features explicit sparseness constraints respect hoyer’s smooth sparseness measure pre-deﬁned sparseness degrees ensured always attained using sparseness-enforcing projection operator. building upon succinct representation projection proved projection problem formulated root-ﬁnding problem. presented linear time constant space algorithm projection superior previous approaches terms theoretical computational complexity execution time real computing machines. proving theorem characterization projection onto ﬁrst notation. above denote i-th canonical basis vector furthermore vector entries one. note point resides non-negative orthant subscripts vectors denote corresponding coordinate except example abbreviate clear real number. index elements unique matrix {}d×n proof possible prove claim either constructively implicitly variants differ whether positive coordinates projection computed must assumed known. ﬁrst present constructive proof based geometric analysis conducted thom palm contributes deepening insight involved computations. alternative also provide rigorous proof using method lagrange multipliers greatly enhances unproven analysis potluru ﬁrst note β∗·max determined already holds ·max show claimed representation unique present different proofs existence representation. uniqueness since would violate impossible ﬁrst show distinct indices assume case argmini∈i index smallest coordinate index numbers deﬁne ∑i∈i\\{ like argument shown non-empty. follows ∑i∈i\\{ hence least good approximation violating uniqueness therefore impossible singleton. thus holds. moreover finally hyperplane points non-negative orthant foundation sparse code word inference. projection efﬁciency since complicated gradients required proposed learning algorithm signiﬁcantly faster even optimized algorithm. topographic atom organization atom sparseness realized simple extensions allowing versatile sparse representations data sets. simplicity efﬁciency hinder ezdl producing dictionaries competitive generated rls-dla terms reproduction denoising performance natural images. alternative image processing methods based sparse representations rely dictionaries subject criteria thus expected beneﬁt ezdl’s advantages well. acknowledgements authors grateful heiko neumann florian schüle michael gabb helpful discussions. would like thank julien mairal karl skretting making implementations algorithms available. parts work performed computational resource bwunicluster funded ministry science research arts universities state badenwürttemberg germany within framework program bwhpc. work supported daimler germany. appendix studies algorithmic computation euclidean projections onto level sets hoyer’s greater detail particular proves correctness algorithm proposed sect. non-empty subset euclidean space point call projm infz∈m euclidean projections onto since consider situations projm singleton also write projm. without loss generality compute projt vector within non-negative orthant instead projs arbitrary point yield sparseness-enforcing projections deﬁned sect. first actual scale irrelevant simply re-scale result projection second constraint projection lies non-negative orthant easily handled ﬂipping signs certain coordinates finally entries assumed non-negative corollary thom palm note non-convex constraint. moreover target sparseness degrees show construction point lies denotes i-th canonical basis vector. since coordinates positive always contains points pseudonorm used pseudo-norm measure sparseness would sparseness degree example vector entries equal unity. however close large value entries equaling small positive. simple example demonstrates situations presence noise cannot eliminated hoyer’s much robust sparseness measure pseudo-norm. turn last paragraph know j+/β projections onto lij+. hand r−mij+ proof lemma thom palm holds proof lemma thom palm implies ≤···≤ noted above separator projecting onto satisﬁes rearranging inequality using conclude want show coordinates original vector vanish iteration projecting onto already small. base case induction trivial since complement empty. induction step note complement partitioned since already know induction hypothesis shown since substituting explicit expression hence claim holds ij+. show holds claim theorem follows setting note construction coordinates positive requirement rectiﬁer applied nothing changes. indeed expression therefore holds completes constructive proof existence. existence existence projection guaranteed weierstraß extreme value theorem since compact. objective function constraints represented functions indices functions continuously differentiable. projt local minimum subject application method lagrange multipliers ﬁrst show regular means gradients evaluated must linearly independent jn−d} since hence vectors show linear independence. clearly rn−d ∑n−d then deﬁnition hence pre-multiplication yield therefore equation hand hence ﬁrst paragraph uniqueness proof shown distinct indices since thus conclude implies vn−d well shows regular. lagrangian γigi derivative respect ﬁrst argument given ·e+β ·s−γ proposition bertsekas guarantees existence lagrange multipliers assume since derivative must vanish. hence therefore singleton remark thom palm assumed unique permutation-invariant. seen earlier absurd must hold. norm scaled canonical simplex. further circle arbitrary index subset coordinates indexed vanish. theorem appendix thom palm exists ﬁnite sequence index sets projt result ﬁnite sequence intermediate projections yield unique results restricted unique. index sets contain indices entries survive projections onto words computed alternating projections sets non-convex expressions individual projections given lemma lemma proposition lemma respectively thom palm completeness deﬁne folsuppose holds want show also holds projc) deﬁnition proposition thom palm implies mean value entries survive simplex projection additive constant. note always nonnegative lemma thom palm need show since yield induction hypothesis therefore deﬁnition omit rectiﬁer using induction hypothesis expression ·vij+x projecting onto lij+ distance mij+ ∑i∈ij+ lemma thom palm applied. mij+ index palm therefore holds call well-deﬁned requirements deﬁnition met. note situation trivial sparseness-decreasing setup coordinates projection must positive. hence theorem computed provided formula. need notation describe properties point. write contains entries xmin short smallest entry xmax xnd-max \\{xmax} denote largest entries further denotes curve evolves application soft-shrinkage function manhattan norm euclidean norm points given respectively. therefore sect. written derivative expressed terms using chain rule. proof addition original claim also give explicit expressions derivative higher derivatives thereof part necessary show strictly decreasing constant respectively claimed intervals explicit implementation algorithm soft-shrinkage function differentiable everywhere except offset. therefore differentiable everywhere except argument coincides entry start deducing ﬁrst second derivative element them. allow xmin completeness. index nonvanishing coordinates constant derivative computed using closed-form expression. this denote number non-vanishing coordinates interval. ∑i∈i ∑i∈i obtain analogously ∂/∂α ∂/∂α ∑i∈i chain rule yields ∂/∂α product rule first exactly non-vanishing coordinate. situation thus constant consequence mean value theorem real analysis. because continuous constant even next ﬁrst paragraph. since xnd-max. furthermore inequality fact strict because least distinct nonzero entries. hence negative interval mean value theorem guarantees strictly decreasing interval. property holds entire interval continuity reals. discriminant b−ac −vix since dvix moreover numrem arbitrary. exists weierstraß extreme value theodλ hence must real solution equation above. solving equation shows suppose number arises square root number sign thus seen earlier distinct indices assume implies possible violates order-preservation property projections onto permutation-invariant sets thom palm thus choice correct ﬁrst place must stated claim. remains shown result soft-shrinkage function. shows still thus holds. therefore representation holds entries. finding containing indices positive coordinates projection result algorithmic computation projection. based constructive proof could example achieved carrying alternating projections whose run-time complexity quasi-linear quadratic problem dimensionality whose space complexity linear. alternative method proposed potluru input vector sorted possible candidate checked. sorting must form unknown here run-time complexity quasi-linear space complexity linear problem dimensionality since also sorting permutation stored. gets large algorithms smaller computational complexity mandatory. requirement implies thus arbitrary hence since must hold. existence follows intermediate value theorem uniqueness guaranteed strictly monotone relevant interval. deﬁne projt theorem max) since obtain uniqueness zero implies described sect. exact value zero found inspecting neighboring entries candidate offset neighbors element changes sign know root must located within interval. further know coordinates value greater must survive sparseness projection yields theorem thus explicit representation projection. next result gathers ideas shows easy verify whether change sign hand. lemma given well-deﬁned arbitrary. xmin xmin. otherwise max{xi left neighbor min{xi right neighbor exist sets maximum minimum taken nonempty. deﬁne |i|. then exactly number inequalities satisﬁed projt denotes projection onto hence computed exactly theorem proof claim obvious lemma ∑i∈i vix−dα ∑i∈i yields ∑i∈k ∑i∈i dxk. claim follows analogously. likewise hence follows ∑i∈j ∑i∈i value computed manner. condition claim equivalent case hence number note general. write projt theorem follows equivalent turn equivalent therefore must hold. thus already correct non-vanishing coordinates projection ﬁrst place computed exactly using formula claim theorem yields projection sect. informally described proposed algorithm carrying sparseness-enforcing projections provided simpliﬁed ﬂowchart fig. previous theoretical considerations propose prove correctness formalized algorithm input point projected target norms position evaluated. output values ﬁnished {true false} indicating whether correct interval found numbers needed exactly compute projection problem. here overall method split algorithm evaluates auxiliary function based derivative returns additional information required ﬁnding zero part algorithm implements root-ﬁnding procedure carries necessary computations yield result projection. furthermore returns information required computation projection’s gradient discuss below. theorem projt unique. algoproof start analyzing algorithm evaluates given position output includes values auxiliary function ﬁrst second derivative value transformed auxiliary function derivative. moreover boolean value indicates whether interval sign change found three additional numbers required compute zero soon correct interval found. denote indices entries larger blocks line line algorithm scans coordinates identiﬁes elements computes numbers line |i|. additionally clearly algorithm performs actual projection in-place outputs values needed gradient projection. uses algorithm sub-program calls function \"auxiliary\". algorithm ﬁrst checks whether fulﬁlled case coordinates survive projection computation straightforward theorem using otherwise lemma states numerically standard root-ﬁnding algorithms since continuous strictly decreasing interval concrete variant chosen parameter \"solver\" algorithm implementation details found traub press here root-ﬁnding loop starting line terminated algorithm indicates correct interval exact computation zero identiﬁed. therefore necessary carry root-ﬁnding numerical convergence enough come sufﬁciently close line computes based projection representation given theorem line either reached directly line statements lemma hold. block starting line computes stores point’s squared euclidean norm variable line computes number theorem multiplies every entry ﬁnally contains projection onto would also possible create vector projection result leave input vector untouched expense additional memory requirements linear problem dimensionality. solver bisection loop line repeated constant number times regardless since algorithm terminates time linear algorithm needs time linear further amount additional memory needed independent algorithm overall space requirements constant. therefore algorithm asymptotically optimal sense complexity theory. thom palm shown projection onto grasped function almost everywhere differentiable almost everywhere. explicit expression projection’s gradient derived depended number alternating projections required carrying projection. based characterization gained theorem derive much simpler expression gradient also efﬁcient compute theorem projt unique. α∗β∗ given theorem projt differentiable matrix rd×d given projt dvix here rd×d identity matrix point coordinates unity proof invariant local changes therefore projt differentiable composition differentiable functions. following derive claimed expression gradient projt application chain rule yields ∂vix algorithm linear time constant space algorithm projections onto auxiliary function evaluated calls \"auxiliary\" carried algorithm algorithm operates in-place input vector overwritten output vector upon completion. input point projected target norms output projection projt ﬁrst element replacing needed input vector numbers compute projection’s gradient algorithm skip root-finding decreasing sparseness. update bisection interval. else iteration root-finding. solver bisection else solvers based derivatives. solver newton else solver newtonsqr else solver halley next blocks spanning line line compute scalar numbers according lemma deﬁnition ﬁrst derivatives thereof given explicitly proof lemma deﬁnition derivative given chain rule. line checked whether conditions lemma hold using statements lemma result stored boolean variable \"ﬁnished\". algorithm starts computing sliced vectors computes \"sum equals \"scp equals line computes using numbers output algorithm line line product computed in-place scaling adding scaled version adding scalar coordinate. since remains invert slicing. complexity algorithm clearly linear time space. escaped notice corollary also used determine eigensystem projection’s gradient prove useful analysis gradient-based learning methods involving sparseness-enforcing projection operator.", "year": 2016}