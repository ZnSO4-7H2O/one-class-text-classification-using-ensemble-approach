{"title": "Grid Long Short-Term Memory", "tag": ["cs.NE", "cs.CL", "cs.LG"], "abstract": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to significantly outperform the standard LSTM. We then give results for two empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to define a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task.", "text": "paper introduces grid long short-term memory network lstm cells arranged multidimensional grid applied vectors sequences higher dimensional data images. network differs existing deep lstm architectures cells connected network layers well along spatiotemporal dimensions data. network provides uniﬁed using lstm deep sequential computation. apply model algorithmic tasks -digit integer addition sequence memorization able signiﬁcantly outperform standard lstm. give results empirical tasks. grid lstm achieves bits character wikipedia character prediction benchmark state-of-the-art among neural approaches. addition grid lstm deﬁne novel two-dimensional translation model reencoder show outperforms phrase-based reference system chinese-to-english translation task. long short-term memory networks recurrent neural networks equipped special gating mechanism controls access memory cells since gates prevent rest network modifying contents memory cells multiple time steps lstm networks preserve signals propagate errors much longer ordinary recurrent neural networks. independently reading writing erasing content memory cells gates also learn attend speciﬁc parts input signals ignore parts. properties allow lstm networks process data complex separated interdependencies excel range sequence learning domains speech recognition ofﬂine hand-writing recognition machine translation image-to-caption generation even non-sequential data recent success deep networks shown long chains sequential computation ﬁnding exploiting complex patterns. deep networks suffer exactly problems recurrent networks applied long sequences namely information past computations rapidly attenuates progresses chain vanishing gradient problem layer cannot dynamically select ignore inputs. therefore seems attractive generalise advantages lstm deep computation. extend lstm cells deep networks within uniﬁed architecture. introduce grid lstm network arranged grid dimensions. network lstm cells along dimensions grid. depth dimension treated like dimensions also uses lstm cells communicate directly layer next. since number dimensions grid easily more propose novel robust modulating n-way communication across lstm cells. n-dimensional grid lstm naturally applied feed-forward networks well recurrent ones. one-dimensional grid lstm corresponds feed-forward network uses lstm cells place transfer functions tanh relu networks related highway networks gated transfer function figure blocks form standard lstm form grid lstm networks dimensions. dashed lines indicate identity transformations. standard lstm block memory vector vertical dimension; contrast grid lstm block memory vector applied along vertical dimension. used successfully train feed-forward networks layers depth. grid lstm dimensions analogous stacked lstm adds cells along depth dimension too. grid lstm three dimensions analogous multidimensional lstm differs cells along depth dimension also using proposed mechanism modulating n-way interaction prone instability present multidimesional lstm. study learning properties grid lstm various algorithmic tasks. compare performance two-dimensional grid lstm stacked lstm computing addition -digit integers without curriculum learning memorizing sequences numbers settings cells along depth dimension effective them; similarly tying weights across layers also effective untying weights despite reduced number parameters. also apply grid lstm empirical tasks. architecture achieves bits-per-character characters wikipedia dataset outperforming neural networks. secondly grid lstm deﬁne novel neural translation model re-encodes source sentence based target words generated point. network outperforms reference phrase-based cdec system iwslt btec chinese-to-ensligh translation task. appendix contains additional results grid lstm learning parity functions classifying mnist images. outline paper follows. sect. describe standard lstm networks comprise background. sect. deﬁne grid lstm architecture. sect. consider experiments conclude sect. lstm network processes sequence input target pairs pair lstm network takes input produces estimate target given previous inputs past inputs determine state network comprises hidden vector memory vector computation figure stacked lstm grid lstm applied character prediction composed respective blocks note grid lstm signal ﬂows lstm cells along time depth dimensions. computation outputs hidden memory vectors comprise next state network. estimate target computed terms hidden vector functional lstm shorthand follows concatenates four weight matrices aspect lstm networks role gates forget gate delete parts previous memory vector whereas gate write content memory modulated input gate output gate controls read memory onto hidden vector mechanism important learning properties. memory vector obtained linear transformation previous memory vector gates; ensures forward signals step repeatedly squashed non-linearity tanh backward error signals decay sharply step issue known vanishing gradient problem mechanism also acts memory implicit attention system whereby signal input written memory vector attended parts across multiple steps retrieved part time. figure instances one-dimensional three-dimensional grid lstm. network left used parity results appendix. translation mnist models speciﬁc instances grid lstm right. model closely related standard lstm network stacked lstm stacked lstm adds capacity stacking lstm layers other. output hidden vector lstm taken input lstm place stacked lstm depicted fig. note although lstm cells present along sequential computation lstm network present vertical computation layer next. another related model multidimensional lstm inputs arranged sequence n-dimensional grid two-dimensional grid pixels image. input array network receives hidden vectors memory vectors computes hidden vector memory vector passed next state dimensions. network concatenates transformed input hidden vectors vector computes well forget gates gates used compute memory vector follows number paths grid grows combinatorially size dimension total number dimensions values grow rate unconstrained summation cause instability large grids adding cells along depth dimension increases exacerbates problem. motivates simple alternate computing output memory vectors grid lstm. grid lstm deploys cells along dimensions including depth network. context predicting sequence grid lstm cells along dimensions temporal sequence vertical along depth. modulate interaction cells dimensions grid lstm proposes simple mechanism values cells cannot grow combinatorially section describe multidimensional blocks combined form grid lstm. multidimensional lstm n-dimensional block grid lstm receives input hidden vectors memory vectors unlike multidimensional case block outputs hidden vectors memory vectors distinct. computation simple proceeds follows. model ﬁrst concatenates input hidden vectors dimensions rd×n applies standard transform distinct weight matrices lstm mechanism across respective dimension. note vector contains input hidden vectors shared across transforms whereas input memory vectors affect n-way interaction directly combined. n-dimensional blocks naturally arranged n-dimensional grid forming grid lstm. block grid sides incoming hidden memory vectors sides outgoing hidden memory vectors. note block receive separate data representation. data point projected network pair input hidden memory vectors along sides grid. n-dimensional block transforms dimensions computed parallel. useful dimension know outputs transforms dimensions especially outgoing vectors dimension used estimate target. instance prioritize ﬁrst dimension network block ﬁrst computes transforms dimensions obtaining output hidden vectors block concatenates output hidden vectors input hidden vector ﬁrst dimension vector follows grid lstm networks blocks along given dimension grid useful regular connections along dimension without cells. naturally accomplished inside block using dimension simple transformation nonlinear activation function instead transform lstm. given weight matrix rd×n ﬁrst dimension looks follows standard nonlinear transfer function simply identity. allows modulo differences mechanism inside blocks grid lstm networks generalize models sect. grid lstm applied temporal sequences cells temporal dimension vertical depth dimension corresponds stacked lstm. likewise grid lstm without cells along depth corresponds multidimensional lstm stacked layers. figure results -digit addition. left table gives results best performing networks type. right graph depicts learning curve -layer tied -lstm solves problem less examples. spike curve likely repetitions steps addition algorithm. picture n-dimensional block fig. sides block input vectors associated sides output vectors. blocks arranged grid separation extends grid whole; side grid either input output vectors associated certain tasks inputs different types model exploit separation projecting type input different side grid. mechanism inside blocks ensures hidden memory vectors different sides interact closely without conﬂated. case neural translation model introduced sect. source words target words projected different sides grid lstm. sharing weight matrices speciﬁed along dimension grid lstm useful induce invariance computation along dimension. translation image models multiple sides grid need share weights capacity added model introducing grid dimension without sharing weights. weights shared along dimensions including depth refer model tied n-lstm. ﬁrst experiment -lstm networks learning -digit integers. problem formulation similar number given network digit time result also predicted digit time. input numbers separated delimiter symbols end-of-result symbol predicted network; symbols well input target padding indicated example follows contrary work uses digits input integers number digits curriculum learning strategies digits partially predicted output back network forcing network remember partial predictions making task challenging. predicted output numbers either digits. compare performance -lstm networks standard stacked lstm train types networks either tied untied weights hidden units layers. train network stochastic gradient descent using figure three plots corresponds neural network respective type reached accuracy respectively memorization task. networks hidden units number layers indicated horizontal axis. vertical axis indicates number samples needed achieve threshold accuracy. deeper networks tend learn faster shallower ones -lstm networks effective stacked lstm networks tied untied settings. mini-batches size adam optimizer learning rate train networks million samples reach accuracy random sample unseen addition problems. note since training samples randomly generated samples seen possible network overﬁt training data. training test accuracies agree closely. figure relates results experiments addition problem. best performing tied -lstm layers deep learns perfectly solve task less training samples. tied -lstm networks generally perform better untied -lstm networks likely repetitive nature steps involved addition algorithm. best untied -lstm network layers learns slowly achieves per-digit accuracy million examples. -lstm networks turn perform better either tied untied stacked lstm networks stacked layers improve single-layer models. cells present clear advantage deep -lstm networks helping mitigate vanishing gradients along depth dimension. third algorithmic task analyze performance -lstm networks task memorizing random sequence symbols. sequences symbols long vocabulary symbols encoded one-hot vectors given network symbol step. setup similar addition above. network tasked reading input sequence outputting sequence unchanged train -lstm stacked lstm either tied untied weights memorization task. networks hidden units layers. mini-batches size optimize network using adam learning rate above train network million samples reach accuracy unseen samples. accuracy measured individual symbol sequence. curriculum learning training strategies. figure bits-per-character results various models measured wikipedia dataset together respective number parameters size alphabet used. note slight differences test data alphabet size. successful learn solve task smallest number samples. -layer tied lstm network learns solution less samples. although fairly high variance amid solving networks deeper networks tend learn faster. addition large difference performance tied -lstm networks tied stacked lstm networks. latter perform much lower accuracy stacked lstm networks layers reach accuracy optimization property cells depth dimension delivers large gain. similarly case addition problem untied lstm networks untied stacked lstm networks take signiﬁcantly longer learn respective counterparts tied weights advantage cells depth direction clearly emerges untied -lstm networks too. next test -lstm network hutter challenge wikipedia dataset successively predict next character corpus. dataset million characters. follow splitting procedure last million characters used testing. alphabet characters total. tied -lstm hidden units layers depth. fig. previous tasks characters projected form initial input hidden cell vectors softmax layer connected topmost output hidden cell vectors. model total parameters. usual objective minimize negative log-likelihood character sequence model. training performed sampling sequences characters processing order. back propagate errors every characters. initial cell hidden vectors temporal direction initialized zero beginning sequence; maintain forward propagated values update order simulate full back propagation. mini-batches thereby processing sequences characters parallel. network trained adam learning rate training proceeds approximately epochs. figure reports bits-per-character performance together number parameters various recently proposed models dataset. tied -lstm signiﬁcantly outperforms models despite fewer parameters. layers depth adding capacity untying weights likely enhance -lstm. next ﬂexibility grid lstm deﬁne novel neural translation model. neural approach machine translation trains neural network end-to-end source sentence target sentence mapping usually performed within encoder-decoder framework. neural network convolutional recurrent ﬁrst encodes source sentence computed representation source conditions recurrent neural network generate target sentence. approach yielded strong empirical results suffer bottleneck. encoding source sentence must contain information words order; decoder network turn cannot easily revisit unencoded source sentence make decisions based partially produced translations. issue alleviated soft attention mechanism decoder neural network uses gates focus speciﬁc parts source sentence grid lstm view translation novel fashion two-dimensional mapping. call reencoder network. dimension processes source sentence whereas dimension produces target sentence. resulting network repeatedly re-encodes source sentence conditioned part target sentence generated thus functioning implicit attention mechanism. size representation source sentence varies length source sentence repeatedly scanned based generated target word. represented fig. target word beginning start-of-target-sentence symbol network scans source sentence ﬁrst layer second layer; scan depends target words generated block layers communicate directly. note that like attention-based model two-dimensional translation model complexity respectively length source target; contrast recurrent encoder-decoder model complexity gives additional computational capacity former models. besides addressing bottleneck two-dimensional setup aims explicitly capturing invariance present translation. translation patterns languages invariant position scale pattern. instance reordering patterns maps english verb french verb sends part english verb german sentence detected applied independently occur source sentence number words involved instance pattern. capture this grid lstm translation model shares weights across source target dimensions. addition hierarchy stacked two-dimensional grids opposite directions used increase capacity help learning longer scale translation patterns. resulting model three-dimensional grid lstm hierarchy grows along third dimension. model depicted fig. evaluate grid lstm translation model iwslt btec chinese-to-english corpus consists pairs source target sentences training development testing. corpus words language source vocabulary chinese words target vocabulary english words target sentences average around words long. development test corpora come reference translations. -lstm uses two-dimensional grids -lstm blocks hierarchy. since network layers third dimension regular identity connections without nonlinear transfer function along third dimension figure ﬁrst table contains bleu- scores -lstm neural translation model cdec system depth-gated lstm attention mechanism; scores calculated either main reference translation available reference translations btec corpus. cdec state-of-the-art hierarchical phrase based system many component models. second table contains examples generated translations. deﬁned sect. source target dimensions tied weights lstm cells. processing bidirectional ﬁrst grid processes source sentence beginning second beginning. allows shortest distance signal travels input output target words constant independent length source. note second grid receives input coming grid -lstm block. train seven models vectors size apply dropout probability hidden vectors within blocks. optimization adam learning rate decoding output probabilities averaged across models. beam search size discard candidates shorter half length source sentence. results shown fig. best model reaches perplexity test data. baseline state-of-the-art hierarchical phrase-based system cdec grid lstm signiﬁcantly outperforms baseline system validation test data sets. introduced grid lstm network uses lstm cells along dimensions modulates novel fashion multi-way interaction. seen advantages cells compared regular connections solving tasks parity addition memorization. described powerful ﬂexible ways applying model character prediction machine translation image classiﬁcation showing strong performance across board. kyunghyun merrienboer bart g¨ulc¸ehre aglar bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. corr abs/. http//arxiv.org/abs/.. duch wlodzislaw. k-separability. kollias stefanos stafylopatis andreas duch wlodzislaw erkki artiﬁcial neural networks icann volume lecture notes computer science springer berlin heidelberg isbn ----. http//dx.doi.org/./_. duchi john hazan elad singer yoram. adaptive subgradient methods online learning stochastic optimization. technical report ucb/eecs-- eecs department university california berkeley http//www.eecs.berkeley.edu/pubs/techrpts// eecs--.html. dyer chris lopez adam ganitkevitch juri weese johnathan ture ferhan blunsom phil setiawan hendra eidelman vladimir resnik philip. cdec decoder alignment learning framework ﬁnite-state context-free translation models. proceedings association computational linguistics goodfellow warde-farley david mirza mehdi courville aaron bengio yoshua. maxout proceedings international conference machine learning icml atnetworks. lanta june http//jmlr.org/proceedings/ papers/v/goodfellow.html. hochreiter bengio frasconi schmidhuber gradient recurrent nets difﬁculty learning long-term dependencies. kremer kolen field guide dynamical recurrent neural networks. ieee press kiros ryan salakhutdinov ruslan zemel richard unifying visual-semantic embeddings multimodal neural language models. corr abs/. http//arxiv.org/abs/. chen-yu saining gallagher patrick zhang zhengyou zhuowen. deeply-supervised nets. proceedings eighteenth international conference artiﬁcial intelligence statistics aistats diego california http//jmlr.org/proceedings/ papers/v/leea.html. nair vinod hinton geoffrey rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning june haifa israel http//www.icml.org/papers/.pdf. simard patrice steinkraus david platt john best practices convolutional neural networks applied visual document analysis. international conference document analysis recognition -volume august edinburgh scotland icdar... http//dx.doi.org/./icdar... visin francesco kastner kyle kyunghyun matteucci matteo courville aaron bengio yoshua. renet recurrent neural network based alternative convolutional networks. corr abs/. http//arxiv.org/abs/.. zeiler matthew zhang sixin lecun yann fergus rob. regularization neural networks using dropconnect. icml volume jmlr proceedings jmlr.org http//dblp.uni-trier.de/db/conf/icml/icml.htmlwanzzlf. report additional results algorithmic empirical without special initialization training tricks -lstm network learn compute parity input bits -lstm network applied images obtains strong results mnist. apply one-dimensional grid lstm learning parity. given string bits parity generalized string deﬁned bits bits even. although manually crafted neural networks problem devised training generic neural network ﬁnite number examples generic random initialization weights successfully learn compute parity k-bit strings signiﬁcant values longstanding problem core problem k-bit string given neural network whole single projection; considering time remembering previous partial result recurrent figure results training tied -lstm networks compute k-bit parity input bits. left diagram contains solutions found -lstm networks hidden units whereas right diagram shows solutions found -lstm networks units. horizontal axis corresponds number input bits. vertical axis corresponds number layers networks. point diagram corresponds classiﬁcation accuracy respective network sample unseen k-bit strings. networks million strings training often solutions many fewer strings. missing points diagram indicate failure solution within training size time constraints. figure left table reports best performing networks k-bit parity. right ﬁgure heat activation values selected counter neurons -lstm network layers trained parity -bit strings. speciﬁc values obtained feed-forward pass network using input string different strings gave similar results. multi-step architecture reduces problem learning k-bit parity simple learning -bit parity. learning parity difﬁcult change single input changes target value decision boundaries resulting space highly non-linear. train -lstm networks tied weights compare fully-connected feedforward networks relu tanh activation functions either tied untied weights. search space hyper-parameters follows. -lstm networks trained either hidden units hidden layers. -lstm networks trained input strings bits increments feed-forward relu tanh networks trained units also hidden layers. latter networks trained input strings bits increments network trained maximum million samples four days computation tesla gpu. optimization mini-batches size adagrad rule learning rate network considered found solution network correctly computes parity randomly sampled unseen k-bit strings. nature problem training predicted accuracy never better random guessing network ﬁnds solution accuracy suddenly spikes figure depicts results experiments -lstm networks figure relates best performing networks type. feed-forward relu tanh networks either tied untied weights networks fail solutions bits beyond. networks search space solutions input bits. contrast represented fig. tied -lstm networks solutions bits. appears correlation length input strings minimum depth -lstm networks. minimum depth networks increases suggesting longer strings need operations applied them; however rate growth subfigure -lstm network applied non-overlapping patches image. patch projected form input hidden cell vectors depth dimension -lstm blocks. arrows across spatial dimensions indicate computation layer. subsampling pooling occurs networks topmost layer simply concatenates output hidden memory vectors depth dimension passes layer relus ﬁnal softmax layer. linear suggesting single input considered every step. visualized activations memory vectors obtained feed-forward pass -lstm networks using selected input strings revealed prominent presence counting neurons keep counter number layers processed far. aspects seem suggest networks using cells process string sequentially attending parts step computation seemingly crucial feature available relu tanh transfer functions. last experiment apply -lstm network images. consider non-overlapping patches pixels image forming two-dimensional grid inputs. -lstm performs computations lstm cells along three different dimensions. dimensions correspond spatial dimensions grid whereas remaining dimension depth network. like convolutional neural network three-way transform -lstm applied parts grid ensuring features extracted across parts input image. unbounded context size -lstm computations features image inﬂuenced features computed image within layer. cells along depth direction features present patch passed onto next layer either unprocessed processed layer function neighboring patches. construct network depicted fig. divide mnist image pixel patches small number patches linearized projected vectors size hidden layer -lstm; projected vectors input hidden memory vectors ﬁrst layer depth direction -lstm. layer computation -lstm starts corner image follows spatial dimensions ends opposite corner image. network layers depth layer starting computation corners image. current form pooling successive layers -lstm. topmost layer concatenates output hidden memory vectors parts grid. passed layer relus ﬁnal softmax layer. setup similarity original application multidimensional lstm images recently described renet architecture difference former apply multiple layers depth image three-dimensional blocks concatenate output vectors classiﬁcation. difference renet figure test error mnist dataset. approaches convolutional networks except visin uses stack single-direction recurrent neural networks. grid lstm non-lstm connections along depth uses relu instead. architecture -lstm processes image according inherent spatial dimensions; instead stacking hidden layers renet block also modulates directly information passed along depth dimension. training details follows. mnist dataset consists training images validation images test images. pixel values normalized dividing data augmentation performed shifting training images pixels horizontal vertical directions padding zero values. shift directions chosen uniformly random. validation samples used retraining best model settings found grid search. train -lstm without cells depth dimension. -lstm cells uses patches pixels four lstm layers hidden units relu layer units. -lstm without cells depth dimension input patches size obtained cropping image size also four lstm layers units relu layer units. latter model relu transfer function depth direction mini-batches size train models using adam learning rate figure reports test errors models competing approaches. even absence pooling -lstm cells performs near state-of-the-art. -lstm without cells also performs quite well; cells depth direction likely help feature extraction higher layers. approaches exception renet convolutional neural networks.", "year": 2015}