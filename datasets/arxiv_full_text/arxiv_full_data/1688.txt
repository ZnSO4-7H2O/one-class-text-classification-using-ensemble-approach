{"title": "Multidimensional counting grids: Inferring word order from disordered  bags of words", "tag": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "abstract": "Models of bags of words typically assume topic mixing so that the words in a single bag come from a limited number of topics. We show here that many sets of bag of words exhibit a very different pattern of variation than the patterns that are efficiently captured by topic mixing. In many cases, from one bag of words to the next, the words disappear and new ones appear as if the theme slowly and smoothly shifted across documents (providing that the documents are somehow ordered). Examples of latent structure that describe such ordering are easily imagined. For example, the advancement of the date of the news stories is reflected in a smooth change over the theme of the day as certain evolving news stories fall out of favor and new events create new stories. Overlaps among the stories of consecutive days can be modeled by using windows over linearly arranged tight distributions over words. We show here that such strategy can be extended to multiple dimensions and cases where the ordering of data is not readily obvious. We demonstrate that this way of modeling covariation in word occurrences outperforms standard topic models in classification and prediction tasks in applications in biology, text modeling and computer vision.", "text": "models bags words typically assume topic mixing words single come limited number topics. show many sets words exhibit diﬀerent pattern variation patterns eﬃciently captured topic mixing. many cases words next words disappear ones appear theme slowly smoothly shifted across documents examples latent structure describe ordering easily imagined. example advancement date news stories reﬂected smooth change theme certain evolving news stories fall favor events create stories. overlaps among stories consecutive days modeled using windows linearly arranged tight distributions words. show strategy extended multiple dimensions cases ordering data readily obvious. demonstrate modeling covariation word occurrences outperforms standard topic models classiﬁcation prediction tasks applications biology text modeling computer vision. machine learning research data samples often represented bags words without particular order. choice often motivated diﬃculty computational eﬃciency modeling known structure data e.g. language. striking example current computer vision research spatial structure visual features largely discarded object recognition algorithms. also examples data structure truly unknown. gene expression array modeled genes expression levels simply corresponding counts time little known cellular pathways employ genes. without knowledge clear gene ordering. biology also abundant situations data interest truly structure. example mammalian immune systems sees virus inside cell whole disordered peptides sampled inside viral proteins presented cellular surface immune surveillance. clustering dimensionality reduction techniques latent semantic indexing latent dirichlet allocation among popular approaches modeling disordered bags words. case subspace models words modeled mixture topics topic represented tight distribution words. paper point much variability many interesting datasets better modeled terms multidimensional thematic shifts rather outright mixing. model certain words/features dropped others added consequence movement hypothetical space. goal infer properties space embedding data useful ﬁrst consider example data spatial embedding directly available. figure shows count different words news stories cnn’s news blog ﬁrst days march text document often modeled words cnn’s daily news blogs modeled highlighted words would participate bags frequencies indicate thematic shifts induced timeline events. beginning month spike news launch apple’s ipad words sheen libya also abundant. week later seems interest american public quickly shifted events professional personal life actor charlie sheen overshadowing steady trickle news uprising middle east. catastrophic natural events japan caused large spike usage word earthquake followed days later signiﬁcant increase usage word nuclear reﬂecting problems nuclear plants started unfold consequence earthquake ensuing tsunami. expectation resolution libya word regains dominance towards period. signal processing perspective distribution word counts across timeline seems caused point sources excitation going averaging ﬁlter. news people used thematic shifts nature story enjoys limited life time suppressed interesting topics. machine learning perspective situation well modeled creating series relatively tight word distributions combining several consecutive distributions form expected histogram words given day. thematic shifts days simply modeled moving averaging window across timeline point sources. thus even though blog considered disordered words good model would fact order along line induce constraints word mixing gives rise observed bags words. figure example counting grid geometry. general data embedded hypercube wrapped around along dimension avoid local minima would caused abrupt cuts along dimension. paper provide simple model data much variation induced thematic shifts expressed movement inferred space. news example assume space tight distributions embedded distributions combined using windowing operation create resultant distribution observed bags words features generated. however assume mapping given priori. simplicity assume space discrete grid counts arbitrary dimension consider iterative estimation counts grid mapping data overlapping windows experiments indicate thematic shifts indeed present variety datasets result model outperforms standard topic mixing there. analyzed wide variety data types including text images gene expression viral peptides used learned counting grids perform regression classiﬁcation. optimize bound respect parameters note ﬁrst second term involves parameters requires another summation applying logarithm. summation grid positions within window bound using variational distribution jensen’s inequality distribution locations i.e. indexed normalization done diﬀerently window indexed diﬀerent diﬀerent features indexed term inside summation diﬀerent distribution could could thought information proportion features type contributed diﬀerent sources window however performing constrained optimization assuming ﬁxed parameters distribution maximizes bound independent i.e. consider distributions feature mapping counting grid result intuitive. know containing features type mapped grid section additional information proportions features contributed diﬀerent incremental counts best guess proportions follow proportions among inside window. assume distributions ﬁxed combining eqs. minimizing resulting bound parameters normalization constraint features obtain update rule found somewhere counting grid. particular using windows dimensions generated ﬁrst averaging counts hypercube window starting d-dimensional grid location extending direction grid positions form histogram generating relaxing terminology refer respectively counting grid window size. also often refer ratio window volumes capacity model terms equivalent number topics many nonoverlapping windows onto grid.fine variation achievable moving windows between close nonoverlapping windows useful expect smooth thematic shifts occur data illustrate experiments indeed does. finally indicate particular window placed location compute likelihood data need latent variables computing logarithm which mixture models epitomes much similar counting grids makes diﬃcult perform assignment latent variables also estimating model parameters. makes iterative exact variational algorithm necessary. bounding non-constant part shorthand variational distribution latent mapping onto counting grid t-th bag. variational distributions varied maximize bound. fact given counting grid bound maximized distribution equal exact posterior distribution. standard variational derivation exact step leads steps eqs. constitute step iterated till convergence ﬁrst step aligns bags features grid windows match bags’ histograms second re-estimates counting grid histogram matches even better. thus starting non-informative initialization iterative process jointly estimate counting grid align bags avoid severe local minima important however consider counting grid torus consider windowing operations accordingly previously proposed learning epitomes model quilts spatially-organized images videos. prevents problems grid boundaries otherwise could crossed space needed grow layout features. careful examination steps reveals efﬁcient cumulative sumsboth steps linear size counting grid. steps done ﬁrst computing linear time cumulative sums computing appropriate linear combinations. example case compute cumulative generalized associating vertex hypercube binary vector diﬀerent vertices share various coordinates along dimension vertex assume values deﬁne elements vector follows label embedding used classiﬁcation regression essentially nearestneighbor strategy data point embedded based words label simply read dominated training points mapped region. options using classiﬁcation/regression course available opted simplest experiments reasons discuss next. mostly interested quality unsupervised learning distribution bags words majority experiments compared following setting. data sample consists words label. bags used without labels train models capture covariation word occurrences mostly modeling thematic shifts modeling topic mixing. then label prediction task performed leave-one-out setting data point taken used estimate label embedding rest data left sample’s mapping used read appropriate location used similar fashion similarly mapped points used predict label left sample. also investigated linear regression based latent mapping. finally variety methods occasionally compared slightly diﬀerent evaluation methods described individual subsections appropriate. figure news- classiﬁcation results. stands bag-of-words movbf mixture von-mises fisher asterisk indicates method uses original feature set. describe particular specifying form e/w. value reported size dimensions embedding three class labels embedding three class labels easiest subset newsgroup dataset news--diﬀerent posts rec.sport.baseball sci.space alt.atheism details). date post available). following previous work reduced dataset subsets varying similarities among news groups. consider report results challenging news--same posts highly related groups comp.os.ms-windows comp.windows.x comp.graphics. ﬁrst compared embedding documents three classes provided based respectively label embedding function performance k-nearest neighbor classiﬁer using topic proportions employing divergence evaluate distances. comparisons performed training/split test reduced vocabulary words using feature reduction method training data save computation time. also evaluated simple words comparisons setting classiﬁcation. figure summarizes results learned several grids varying repeated tests trying take κfor cases evaluate document classes embed spaces diﬀerent dimensions. tests counting grids found perform slightly better suggesting dimensions needed embed complex classes. sake comparison also added recent results including test vonmf model best result model spherical admixture model models topics using vonmf distribution tf-idf features input. outperforms models features experiments rather complex derived features citation; example reduce sam’s classiﬁcation error setting illustrate label embedding training documents learned counting grids show image labels three classes dataset ended tests colored green blue based fractions documents three classes mapped locations grids rendered semi-transparent. ﬁgure demonstrates complex structure lots transitions among classes provides gradation window overlap coarse increasing size window beyond along dimension allow reﬁned overlaps increase performance. interesting model easily train. although example counting grid consists independent word distributions positive parameters summed large groups represent data groups large overlap. independent nonoverlapping windows designated grid cramming components expressible sums subsets positive numbers hard unless real structure data discovered. small window sizes start overtraining. additional results case reported table show quite robust choice grid size given enough room accommodate variation documents. case visual data modeled bags features model account misalignment scenes change location counting grid. smooth thematic motion corresponds motion camera. visual scene dataset introduced composed datasets composed four natural four artiﬁcial categories widely used vision community. class contains roughly images. following standard bag-of-visual -words approach extracted sift features pixel windows computed grid spaced pixels clustered descriptors visual words. describe image features. feature maps images case used window size original features really spatial arrangement window size. kept volume case setting varied grid size keeping cubic grids choosing form discussed above experiments compare representations terms power classiﬁer using unsupervised data embedding i.e. used training test images classes estimate thus embedding data space hidden variables models without using scene labels used hidden variables appropriate training/test splits. results including performed data shown ﬁgure bars repeated evaluation inverting training test sets training used /-th data cases outperform methods robust overtraining. also experiments dubbed supervised classiﬁcation trained model class varying cardinality training set. subsequently classiﬁed test samples using likelihood tests. repeated process times averaging results. compared ﬁgure dimensional case seems outperform higher dimensions general outperform large margin across training data cardinalities. ﬁgure compared similar approach m-step original structure image kept placed onto grid window size equal window size. results show even discarding spatial information accuracy drop much. finally compared state combining discriminative layer reached case case respectively natural artiﬁcial images matching statistical signiﬁcance performance well known method presented figure scene classiﬁcation. unsupervised embedding evaluation terms classiﬁcation based latent space. report result artiﬁcial dataset. kept ﬁxed varied number brackets coverage b-c) supervised image classiﬁcation results using diﬀerent counting grids models diﬀerent classes text details. immune system among interesting complex adaptive systems higher organisms. consists number interacting subsystems employing various infection clearing paths cellular presentation plays central role many them. cells present sample peptides derived cellular proteins means advertising states immune system. facilitates globally coordinated action viral infection. immune pressure depends cellular presentation variation cellular presentation across patients expected reﬂect variation viral load matically across patients variety reasons e.g. gender previous exposures related viruses etc. detection statistically signiﬁcant links cellular presentation viral load expected important consequences vaccine research considered dataset composed coli promoter gene sequences associated imperfect domain theory task recognize promoters strings represent nucleotides promoter genetic region initiates ﬁrst step expression adjacent gene input features sequential nucleotides special notation used simplify specifying locations sequence. biological literature counts locations relative site transcription begins. fifty nucleotides following location constitute example. transformed sequences features representations explained previous section. results obtained using leaveone-out validation show outperform methods based ﬁsher kernel specialized sequences. previous work interpreted microarray expression values counts bagsof-genes good classiﬁcation rates reached. following intuition perform microarray classiﬁcation. used dataset study prostate cancer consisting samples features. samples subdivided diﬀerent classes samples labelled benign prostatic hyperplasia normal adjacent prostate normal adjacent tumor localized prostate cancer prostatitis metastatic tumors classes divided three macro-classes non-cancer cancer metastatic tumor ﬁltered genes variance keep variable hundred. compare results previous experiments. classiﬁcation errors computed using -fold cross validation. learned several squared counting grids several capacities picking diﬀerent values width least extent analyzed predicted cellular presentation patients western australia cohort avoid confounding eﬀects clade analyzed clade infected patients. represented patient’s cellular presentation counts many -long peptides protein previously found targeted immune system. counts calculated based patients class types hla-peptide binding estimation procedure discussed trained counting grids models varying complexity data predicted patients viral load based representation data latent space leave-one-out cross validation framework discussed found counting grids outperformed slightly grids case outperformed lda. fig. also show evolution embedding viral loads ﬁrst patients iterations counting grid learning. discussed above bags words mapped counting grid iteratively grid estimated best model bags regression target viral load used learning models. however inferred mapping iteration used visualize embedded viral load evolves. emergence areas high viral load indicates structure cellular presentation discovered indeed reﬂect variation viral load. presented correlation factors true predicted viral loads computed convergence. correlation factors uniformly across range values indicate cellular presentation protein explains viral load. comparison targeting eﬃciency analysis could explain less viral load. although viral load varies drapacity factor comparable cases. compared results varying number topics results shown fig. demonstrate outperform across range choices worth noting best knowledge state-of-the dataset grids outperform value across range complexities. also classiﬁcation tasks brain colon cancer datasets consist classes respectively. case ﬁxed best values found previous microarray embed data single complexity performed classiﬁcation above. results summarized tab. compared results published cases performs better result colon cancer dataset currently state art. introduced multidimensional counting grid model outperforms subspace models modeling variety datasets found thematic shifts seem better capturing correlations word occurrence. found grids outperform grids visual scene classiﬁcation scenes indicates possibility model primarily captures image misfigure microarray classiﬁcation accuracy prostate cancer dataset various values capacity number topics. bottom table shows actual widths square windows counting grids uniform dimensions. alignment. hand topology needed capture variation cellular presentation better embedded counting grid. fact counting grids slightly better most applications research needed effect dimensionality well aspect ratio counting grids model quality various applications. another interesting observation warrants research fact optimal window size text seemed relatively small presentation modeling required larger patches text modeling thus straightforward ways combining aspects models investigated further. similar previously published model counting grids epitome model also based overlapping patches latent space also reaped beneﬁts shift-invariance. however epitomes relied data samples already ordered array model opens modeling strategy much wider data types.", "year": 2012}