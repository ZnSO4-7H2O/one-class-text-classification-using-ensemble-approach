{"title": "Combining Two And Three-Way Embeddings Models for Link Prediction in  Knowledge Bases", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "This paper tackles the problem of endogenous link prediction for Knowledge Base completion. Knowledge Bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships. Previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns, which unfortunately usually end up overfitting on rare relationships, or in approaches that trade capacity for simplicity in order to fairly model all relationships, frequent or not. In this paper, we propose Tatec a happy medium obtained by complementing a high-capacity model with a simpler one, both pre-trained separately and then combined. We present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving state-of-the-art results on four benchmarks of the literature.", "text": "paper tackles problem endogenous link prediction knowledge base completion. knowledge bases represented directed graphs whose nodes correspond entities edges relationships. previous attempts either consist powerful systems high capacity model complex connectivity patterns unfortunately usually overﬁtting rare relationships approaches trade capacity simplicity order fairly model relationships frequent not. paper propose tatec happy medium obtained complementing high-capacity model simpler pre-trained separately combined. present several variants model diﬀerent kinds regularization combination strategies show approach outperforms existing methods diﬀerent types relationships achieving state-of-the-art results four benchmarks literature. knowledge bases crucial tools deal rise data since provide ways organize manage retrieve digital knowledge. repositories cover kind area speciﬁc domains like biological processes generic purposes. freebase huge collaborative belongs google knowledge graph example latter kind provides expert/common-level knowledge capabilities users. example knowledge engine wolframalpha engine answers natural language question like saturn sun? human-readable answers using internal used question answering also natural language processing tasks like word-sense disambiguation co-reference resolution even machine translation formalized directed multi-relational graphs whose nodes correspond entities connected edges encoding various kinds relationship. hence also refer multi-relational data. following denote connections among entities triples facts entities head tail connected relationship label. information represented triple concatenation several ones. note multi-relational data present also recommender systems nodes would correspond users products edges diﬀerent relationships them social networks instance. main issue complete. freebase currently contains thousands relationships millions entities leading billions facts remains small portion human knowledge obviously. since question answering engines based like wolframalpha capable generalizing acquired knowledge missing facts facto limited search matches question/query internal information missing provide correct answer even correctly interpreted question. consequently huge eﬀorts devoted nowadays towards construction completion manual automatic processes both. mainly divided tasks entity creation extraction consists adding entities link prediction attempts connections entities. paper focuses latter case. performing link prediction formalized ﬁlling incomplete triples like predicting missing argument triple triple exist yet. instance given small example figure made entities diﬀerent relationships containing facts like would like able predict links using fact inﬂuenced singer michael buble instance. link prediction complex several issues. entities homogeneously connected links entities whereas others rarely connected. illustrate diverse characteristics present relationships take look subset freebase introduced indicating large number appear triples. besides roughly connections type -to- head connected tail around type many-to-many multiple heads linked tail vice versa. result diverse problems coexist database. another property relationships impact performance typing arguments. relationships strongly typed like /sports/sports team/location always expects football team head location tail less precise /common/webpage/category expects page adresses tail pretty much everything else head. link prediction algorithm able adapt diﬀerent settings. though exists symbolic approaches link prediction based markovlogic networks random walks learning latent features representations constituents so-called embedding methods recently proved eﬃcient performing link prediction e.g. works entities represented low-dimensional vectors embeddings relationships operators them embeddings operators deﬁne scoring function learned triples observed higher scores unobserved ones. embeddings meant capture underlying features eventually allow create links successfully. scoring function used predict links higher score likely triple true. representations relationships usually speciﬁc sharing parameters across relationships) embeddings entities shared relationships allow transfer information across them. learning process considered multi-task task concerns relationship entities shared across tasks. embedding models classiﬁed according interactions encode validity triple scoring function. joint interaction head label tail used dealing -way model; binary interactions head tail head label label tail core model -way model. kinds models represent entities vectors diﬀer model relationships -way models generally matrices whereas -way models vectors. diﬀerence capacity leads diﬀerence expressiveness models. larger capacity -way models beneﬁcial relationships appearing triples detrimental rare ones even regularization applied. capacity diﬀerence -way models information encoded paper introduce tatec encompass previous works combining wellcontrolled -way interactions high-capacity -way ones. capturing data patterns approaches separately pre-training embeddings -way -way models using diﬀerent embedding spaces them. demonstrate following otherwise pre-training and/or diﬀerent embedding spaces features cannot conveniently captured embeddings. eventually pre-trained weights combined second stage leading combination model outperforms previous works conditions four benchmarks literature umls kinships svo. tatec also carefully regularized since systematically compared diﬀerent regularization schemes adding penalty terms loss function hard-normalizing embedding vectors constraining norms. paper extension added much thorough study regularization combination strategies tatec. besides propose experiments several benchmarks complete comparison proposed method w.r.t. state-of-the-art. also give examples predictions projections obtained embeddings provide insights behavior tatec. paper organized follows. section discusses previous works. section presents model justiﬁes choices. detailed explanations training procedure regularization schemes given section finally present experimental results four benchmarks section simplest successful -way models transe model relationships represented translations embedding space holds embedding tail close embedding head plus vector depends label natural approach model hierarchical asymmetric relationships common knowledge bases freebase. several modiﬁcations transe proposed recently transh transr transh embeddings entities projected onto hyperplane depends translation. second algorithm transr follows idea except projection operator matrix general orthogonal projection hyperplane. shall next section transe corresponds bigrams model additional constraints parameters. -way models shown good performances datasets limited expressiveness fail dramatically harder datasets. contrast -way models perform form low-rank tensor factorization respect extremely high expressiveness depending rank constraints. context link prediction multi-relational data rescal follows natural modeling assumptions. similarly transe rescal learns low-dimensional embedding entity. however relationships represented bilinear operator embedding space i.e. relationship corresponds matrix. training objective rescal frobenius norm original data tensor low-rank reconstruction whereas tatec uses margin ranking criterion transe. another related -way model parameterization constrained version rescal also uses ranking criterion training objective. latent factor model neural tensor networks combinations -way model constrained -way model sense closer algorithm tatec. important diﬀerences algorithms tatec though. first share entity embeddings -way -way models learn diﬀerent entity embeddings. diﬀerent embeddings -way -way models increase model expressivity equivalent combination shared embeddings higher dimensional embedding space additional constrains relation parameters. show experiments however additional constraints lead signiﬁcant improvements. second main diﬀerence approach parameters relationships -way -way interaction terms also shared case tatec. indeed joint parameterization might reduce expressiveness -way interaction terms which argue section left maximum degrees freedom. general parameterization still uses entity embeddings -way -way interaction terms. also layers non-linearity ﬁrst layer model nonlinearity embedding step. order precise overview diﬀerences approaches give section formulas scoring functions related works. focus recently algorithms purely based learning embeddings entities and/or relationships many earlier alternatives proposed. discuss works carried bayesian clustering framework well approaches explicitly graph structure data. inﬁnite relational model nonparametric extension stochastic blockmodel bayesian clustering approach learns clusters entities kind i.e. groups entities similar relationships entities. work followed -way tensor factorization model based bayesian clustering entities within cluster share distribution embeddings. last line work discuss approaches structure graph symbolic way. line work path ranking algorithm estimates probability unobserved fact function diﬀerent paths subject object multi-relational graph; learning consists ﬁnding relationship weight associated kind path linking entities. used knowledge vault project conjunction embedding approach. thus even though consider symbolic approaches here could also combined embedding model desired. -way interaction term model similar slightly general contain constraint relationdependent vectors also seen relaxation translation model special case identity matrix entity embeddings constrained unit sphere. complex relationships. indeed said earlier introduction -way models basically represent kind interaction among entities. combination terms already used besides diﬀerent parameterization tatec contrasts additional freedom brought using diﬀerent embeddings interaction terms. constraints imposed relation-dependent matrix -way terms relation vectors constrained image matrix global constraints severely limited expressiveness -way model stringent regularization reduces expressiveness -way model which explain section left maximum degrees freedom. similar respect share parameter relations. overall scoring function similar model single layer fundamental diﬀerence diﬀerent embedding spaces non-linear transfer function results facilitated training study strategies combining bigram trigram scores indicated equation cases ﬁrst trained separately detail section combined. diﬀerence strategies depends whether jointly update parameters second phase not. parameters ﬁne-tuned second training phase accommodate combination. version could trained directly without pre-training separately show experiments detrimental. linear combination second strategy combines bigram trigram terms using linear combination without jointly ﬁne-tuning parameters remain unchanged pre-training. score hence deﬁned follows combination weights depend relationship learned optimizing ranking loss using l-bfgs additional quadratic penalization σℓ+ǫ contains combination weights relation conterm strained version tatec denoted tatec-lc common regression classiﬁcation collaborative ﬁltering biases model. instance critical step best-performing techniques netﬂix prize user item biases i.e. approximate user-rating according -way -way interaction model propose seen -mode tensor version biased version matrix factorization trigram term collective matrix factorization parameterization rescal algorithm thus analogue tensors term matrix factorization model exact form given corresponds speciﬁc form collective factorization ﬁber-wise bias matrices equation exactly learn bias ﬁber many ﬁbers little data while argue following speciﬁc form collective factorization propose allow share relevant information diﬀerent biases. important notice biases matrix factorization model bigram term overall scoring function aﬀect model expressiveness particular aﬀect main modeling assumptions embeddings rank. user/item-biases boil adding rank- matrices factorization model. since rank matrix hyperparameter simply hyperparameter slightly larger expressiveness before reasonably little impact since increase rank would remain small compared original value critical feature biases collaborative ﬁltering interfere capacity control terms rank namely -norm regularization instance terms trained using squared error measure regularization factor. kind regularization weighted trace norm regularization leaving aside weighted part idea equal times regularization applied user biases times singular value rank-one matrix thus pattern user+item biases exists data weakly hidden stronger factors less regularized others model able capture biases allowed data factors oﬀer opportunity relaxing control capacity parts model translates gains patterns capture indeed useful patterns generalization. otherwise ends relaxing capacity lead overﬁtting. addition bigram terms ensure capture useful patterns also capacity control terms less strict trigram terms. tensor factorization models especially -way interaction models parameterizations capacity control regularization individual parameters still well understood sometimes turns detrimental eﬀective experiments. eﬀective parameter admissible rank embeddings leads conclusion bigram term really useful addition trigram term higher-dimensional embeddings used. hence absence clear concrete eﬀectively controlling capacity trigram term believe diﬀerent embedding spaces used. part model less expressive less regularized part useful patterns learn meaningful prediction task hand. section give motivation -way interaction term task modeling multi-relational data. relationships multi-relational data knowledge bases like particular strongly typed sense well-deﬁned speciﬁc subsets entities either heads tails selected relationships. instance relationship like capital expects city head country tail valid relation. large knowledge bases huge amounts entities belong many diﬀerent types. identifying expected types head tail entities relationships appropriate granularity types likely ﬁlter -way interaction model corresponds low-rank factorization bias matrices head tail entities embeddings based assumption types entities predicted based features features predicting head-types instance capital france easily predicted looking city strongest overall connections france knowledge base. country city strongly linked geographical positions independent respective types. diagonal matrix allows re-weight features embedding space account fact features used describe types describe similarity objects diﬀerent types. diagonal matrix strictly equivalent using general symmetric matrix place reason using symmetric matrix comes intuition direction many relationships arbitrary model invariant arbitrary inversions directions relationships tasks invariance desirable diagonal matrix could replaced arbitrary matrix. training tatec carried using stochastic gradient descent ranking objective function designed give higher scores positive triples negative ones negative triples provided often need process turn positive triples corrupted ones carry discriminative training. simple approach consists creating negative examples replacing argument positive triple random element. simple eﬃcient practice introduce noise creating wrong negatives. margin hyperparameter deﬁnes minimum score positive triple negative one’s. stochastic gradient descent performed minibatch setting. epoch data shuﬄed split disjoint minibatches triples negative triples created every positive one. diﬀerent learning rates bigrams trigram model; kept ﬁxed whole training. interested bigrams trigram terms tatec capture diﬀerent data patterns using random initialization weights necessarily lead solution. hence ﬁrst pre-train separately learned weights initialize full model. training tatec hence carried phases pre-training either ﬁne-tuning tatec-ft learning combination weights tatec-lc. pre-training ﬁne-tuning stopped using early stopping validation follow training procedure summarized algorithm unregularized case. training linear combination weights tatec-lc stopped convergence l-bfgs. previous work embedding models used diﬀerent regularization strategies either constraining entity embeddings have most -norm value adding -norm penalty weights objective function former denote hard regularization regularization performed projecting entity embeddings minibatch onto -norm ball radius latter denote soft regularization penale]+ entity embeddings added. soft scheme control large capacity relation matrices trigram model adapted regularization schemes hard scheme force relation matrices have most frobenius norm value soft include penalization loss function result soft scheme hyperparameters weight importance term form expressing relational structure kinship system australian tribe alyawarra umls biomedical high-level concepts like diseases symptoms connected verbs like complicates affects causes. data sets whole possible triples positive negative observed. used area precision-recall curve metric. dataset split -folds cross-validation training validation last test. since number available negative triples much bigger number positive triples positive ones fold replicated match number negative ones. negative triples correspond ﬁrst setting negative examples section number training epochs ﬁxed bigrams trigram tatec models validated every epochs using precision-recall curve validation criterion randomly chosen validation triples keeping proportion negative positive triples. transe baseline validated every epochs well. introduced data subset freebase large database generic facts gathering billion triples million entities. evaluation used ranking metric. head test triple replaced entities dictionary turn score computed them. scores sorted descending order rank correct entity stored. procedure repeated removing tail instead head. mean ranks mean rank proportion correct entities ranked hits. called setting. setting correct positive triples ranked higher target hence counted errors. following order reduce noise measure thus granting clearer view ranking performance remove positive triples found either training validation testing except target ranking. setting called ﬁltered. since made positive triples negative ones generated. that epoch generate negative triples positive replacing single unit positive triple random entity corruption approach implements prior knowledge unobserved triples likely invalid widely used previous work learning embeddings knowledge bases words context language models. negative triples correspond second setting negative examples section training epochs transe bigrams trigram tatec using ﬁnal ﬁltered mean rank validation criterion. several models statistically similar ﬁltered mean ranks take hits secondary validation criterion. since dataset training validation test sets ﬁxed give conﬁdence interval results randomly split test subsets computing evaluation metrics. times ﬁnally compute mean standard deviation values mean rank hits. database nouns connected verbs subject-verb-direct object relations extracted wikipedia articles. introduced database perform verb prediction task assign correct verb given nouns acting subject direct object; words present results ranking label given head tail. ranking metrics computed mean rank hits% proportion predictions correct verb ranked total number verbs within task negative triples generated replacing label random verb. negative triples correspond third setting negative examples section transe bigrams trigram number epochs ﬁxed validated every epochs. tatec epochs validated each. mean rank chosen validation criterion random validation triples. rates selected among values learning bigrams trigram models isolation independent values chosen pre-training margin penalization terms soft regularization used. conﬁgurations model selected using performance validation given appendix training combination weights tatec-lc carried iterative alternating optimization parameters l-bfgs update parameters using α||δℓ|| value validated among previous models retrained transe hyperparameter grid tatec used running baseline datasets using either soft hard regularization. addition display results best performing methods literature dataset values extracted original papers. umls kinships also report performance -way models rescal -way sme. recent variants transe transh transr ctransr chosen main baselines. transh transr/ctransr optimal values hyperparameters dimension margin learning rate selected within similar ranges tatec. compare tatec three diﬀerent approaches counts -way model -way lfm. counts based direct estimation probabilities triples using number occurrences pairs training set. results models extracted followed experimental setting. since results paper available setting restricted experiments conﬁguration well. results knowledge bases provided table umls models performing well. combination bigrams trigram models slightly better trigram alone signiﬁcant. seems constituents tatec bigrams trigram encode complementary information combination bring much improvement. basically dataset many methods somewhat eﬃcient best lfm. diﬀerence transe bigrams dataset illustrates potential impact diagonal matrix constrain embeddings head tail entities triple similar. regarding kinships -way models like transe models like rescal. cause deterioration comes peculiarity positive triples entity appears times number entities head connected entities even once. important consequence -way models since highly rely information kinships interaction head-tail best irrelevant though practice interaction even introduce noise. poor performance bigrams model combined trigram model combination turn detrimental w.r.t. performance trigram isolation -way models quite noisy cannot take advantage them. side trigram model logically reaches similar performance rescal similar well. performance tatec versions based ﬁne-tuning parameters worse trigram bigrams degrades model. tatec-lc using potentially sparse linear combination models drawback since completely cancel inﬂuence bigram model. conclusion experiments components tatec quite noisy directly remove tatec-lc automatically. soft regularization setting seems slightly better also. table displays results fbk. unlike kinships -way models outperform -way models mean rank hits. simplicity -way models seems advantage something already observed cite combination bigrams trigram models tatec leads impressive improvement performance means information encoded models complementary. tatec outperforms existing methods except transe mean rank wide margin hits. bigrams-soft performs roughly like ctransr better counterpart bigrams-hard. though trigram-soft better trigram-hard well tatec-ft-soft tatec-ft-hard converge similar performances. fine-tuning parameters time better simply using linear combination even tatec-lc still performing well. tatec-ft outperforms variants tatec-shared tatec-no-pretrain wide margin conﬁrms pre-training diﬀerent embeddings spaces essential properly collect diﬀerent data patterns bigrams trigram models sharing embeddings constrain much model without pre-training tatec able encode complementary information constituents. performance tatec cases in-between performances soft version also broke results type relation classifying relationship according cardinality head tail arguments. relationship considered -to- -to-m m-to- regarding variety arguments head given tail vice versa. average number diﬀerent heads whole unique pairs given relationship considered around. number relations classiﬁed -to- -to-m m-to- respectively. results displayed table bigrams trigram models cooperate constructive types relationship predicting head tail. tatec-ft remarkably better m-to-m relationships. tatec achieves also good performance task since outperforms previous methods metrics. before regularization strategies lead similar performances soft setting slightly better. terms hits% tatec outperforms constituents however terms mean rank bigrams model considerably worse trigram tatec. performance trigram bigrams models conﬁrms fact sharing embeddings -way terms actually prevent make best types interaction. kinships since performance bigrams much worse trigram tatec-lc competitive. seems bigrams trigram perform well diﬀerent types relationships combining ﬁne-tuning allows best both; however consistently performing worse relationships seems happen kinships tatec-lc good choice since cancel inﬂuence model. however table depicting training times various models shows training tatec-lc around twice slow training tatec-ft. transe peculiar behavior performs well quite poorly datasets. looking detail noticed database made pairs symmetrical relationships /film/film/subjects /film/film subject/films /music/album/genre /music/genre/albums. simplicity translation model transe works well when predicting validity unknown triple model make symmetrical counterpart present training set. speciﬁcally test triples symmetrical triple training set. split test triples subsets containing test triples symmetrical triple used learning stage containing ones symmetrical triple exist training overall mean rank transe decomposed mean rank overall hits decomposed respectively. transe makes adequate particular feature. original transe paper algorithm shown perform well dataset extracted wordnet suspect wordnet dataset also contains symmetrical counterparts test triples training tatec also make information expected much better relations symmetrical counterparts train mean rank tatec-ft-soft relations symmetrical counterparts instead hits instead results datasets show tatec also able generalize complex information needs taken account. examples predictions tatec displayed table ﬁrst want know answer question location polish national football team?; among possible answers locations speciﬁcally countries makes sense national team. question topic film ’remember titans’? top- candidates potential topics. answers question religion noam chomsky belong typed religions. examples sides relationship clearly typed certain type entity expected head tail operators tatec operate speciﬁc regions embedding space. contrary relationship /webpage/category example non-typed relationship. could actually seen attribute rather relationship indicates entity head topic website oﬃcial website. since many types entities webpage little correlation among relationships predicting left-hand side argument nearly impossible. figures show projections embeddings selected entities trigram bigrams models trained respectively obtained projecting using t-sne projection carried freebase entities whose profession either singer attorney usa. observe figure attorneys clustered separated singers except corresponds multifaceted fred thompson. however embeddings singers clearly clustered since singers appear multitude triples layout result compendium categories. illustrate graphically diﬀerent data patterns bigrams trigram respond focus small cluster made japanese singers seen figure figure however entities diluted whole singers. looking neighboring embeddings japanese singers entities figure entities highly connected japan like yoko born japan mignogna table examples predictions svo. given nouns acting subject direct object test triple tatec predicts best ﬁtting verb. bold expected correct answer. greg ayres chris patton laura bailey worked dubbing industry japanese anime movies television series. shows impact interaction heads tails bigrams model tends push together entities connected triples whatever relation. case forms japanese cluster. table shows examples predictions svo. ﬁrst example though target verb pair verbs like provide offer good matches well. similarly non-target verbs like establish join lead participate attend good matches second third examples respectively. fourth ﬁfth instances show example heterogeneous performance relationship easily explained semantic point view transport good given pair whereas channel transports video natural express watch videos channel hence leads poor performance target verb ranked sixth example particularly interesting since even target verb colonize ranked list good candidates pair found top-. similar representation colonize almost synonyms ranked much higher. eﬀect verb frequency. illustrated figure frequent relationship higher frobenius norm hence verbs similar meanings unbalanced frequencies ranked diﬀerently explains rare verb colonize ranked much worse semantically similar words. consequence relation frobenius norm appearance frequency usual verbs tend highly ranked even though sometimes good matches inﬂuence norm score. ﬁgure frobenius norm relation matrices larger regularized case unregularized case. happens ﬁxed large value regularized case imposes strong constraint norm entities relationship matrices makes frobenius norm matrices absorb whole impact norm score thus impact verb frequency. could down-weight importance verb frequency tuning parameters enforce stronger constraint. breaking performance relationship translated strong relation performance relationship frequency however relation -norm entities embeddings frequency observed explained given entity appear left right argument unbalanced way. paper presents tatec tensor factorization method satisfactorily combines -way interaction terms obtain performance better best either constituent. diﬀerent data patterns properly encoded thanks diﬀerent embedding spaces two-phase training experiments four benchmarks diﬀerent tasks diﬀerent quality measures prove strength versatility model could actually seen generalization existing works. experiments also allow draw conclusions usual regularization schemes used embedding-based models achieve similar performances even soft regularization appears slightly eﬃcient extra-hyperparameter. optimal conﬁgurations umls transe-soft bigrams-soft trigram-soft tatec-soft transe-hard bigrams-hard trigram-hard tatec-hard tatec-linear-comb optimal conﬁgurations kinships transe-soft bigrams-soft trigram-soft tatec-soft transe-hard bigrams-hard trigram-hard tatec-hard tatec-linear-comb", "year": 2015}