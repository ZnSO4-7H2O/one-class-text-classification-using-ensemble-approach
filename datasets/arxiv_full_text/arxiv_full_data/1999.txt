{"title": "Learning of Coordination Policies for Robotic Swarms", "tag": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.NE"], "abstract": "Inspired by biological swarms, robotic swarms are envisioned to solve real-world problems that are difficult for individual agents. Biological swarms can achieve collective intelligence based on local interactions and simple rules; however, designing effective distributed policies for large-scale robotic swarms to achieve a global objective can be challenging. Although it is often possible to design an optimal centralized strategy for smaller numbers of agents, those methods can fail as the number of agents increases. Motivated by the growing success of machine learning, we develop a deep learning approach that learns distributed coordination policies from centralized policies. In contrast to traditional distributed control approaches, which are usually based on human-designed policies for relatively simple tasks, this learning-based approach can be adapted to more difficult tasks. We demonstrate the efficacy of our proposed approach on two different tasks, the well-known rendezvous problem and a more difficult particle assignment problem. For the latter, no known distributed policy exists. From extensive simulations, it is shown that the performance of the learned coordination policies is comparable to the centralized policies, surpassing state-of-the-art distributed policies. Thereby, our proposed approach provides a promising alternative for real-world coordination problems that would be otherwise computationally expensive to solve or intangible to explore.", "text": "contains main components action policy determines action agent performs given inputs communication protocol deﬁnes agents communicate other. achieve system-level coordination classical methods rely human-designed policies communication protocols usually predeﬁned manual design process especially difﬁcult complex coordination tasks large swarms. recently learningbased approaches deep reinforcement learning able successfully learn communication protocols coordination tasks biggest drawback approaches tremendous amount data computational resources required training. paper propose learning-based approach utilizes pre-designed centralized policies train distributed policies. agent follows identical distributed policy interprets agent’s observation communication information received determines action agent generates communication information broadcasted neighbouring agents. distributed policy modelled differentiable deep neural network called distributed policy network inputs outputs represented ﬁxed-sized vectors. distributed policy network serves part larger neural network maps agents’ multi-step observations multi-step actions refer augmented neural network multi-step multi-agent neural network since communication vectors sent agent agent hidden states msmann perform backpropagation msmann capture commuabstract— inspired biological swarms robotic swarms envisioned solve real-world problems difﬁcult individual agents. biological swarms achieve collective intelligence based local interactions simple rules; however designing effective distributed policies large-scale robotic swarms achieve global objective challenging. although often possible design optimal centralized strategy smaller numbers agents methods fail number agents increases. motivated growing success machine learning develop deep learning approach learns distributed coordination policies centralized policies. contrast traditional distributed control approaches usually based human-designed policies relatively simple tasks learning-based approach adapted difﬁcult tasks. demonstrate efﬁcacy proposed approach different tasks well-known rendezvous problem difﬁcult particle assignment problem. latter known distributed policy exists. extensive simulations shown performance learned coordination policies comparable centralized policies surpassing state-of-the-art distributed policies. thereby proposed approach provides promising alternative real-world coordination problems would otherwise computationally expensive solve intangible explore. biological swarms coordination perform tasks beyond capabilities individuals absence centralized control mechanism global observation swarm intelligence emerges local behaviour individual agents governed simple uniﬁed rules distributed structure swarm systems makes less vulnerable individual failures. robust nature well-realized robotic systems robotic swarms relied upon solve complex real-world problems search rescue object transportation mars exploration centralized control extremely costly impossible coordinate individual agents towards achieving mutual objective challenge relay local actions observations agent agent. distributed policy therefore nication protocols addition action policies completes distributed policy learn. since input output vectors communication information ﬁxed sizes distributed policy network must learn aggregate information effective communication. also enables control communication analyze communication information sent agent agent. regarded analogy controlling size representation layer auto-encoder analyzing learned representation. learning approach applied variety distributed robotic tasks provided well-designed centralized policy available. rendezvous task limited visibility approach consistently outperforms stateof-the-art distributed control systems consisting different numbers agents. generalizability proposed approach robotic tasks demonstrated task distributed solution exists. approach provides tangible alternative complex robotic tasks centralized policies designed. moreover analyze learned communication protocols provide insights meaning. analysis learned coordination policies could potentially beneﬁt manual design communication protocols complex robotic swarm systems. following sections begin discussions brief overview related literature section upon deﬁning control framework problem statement section learning approach discussed section simulation setup corresponding results presented section vii. distributed control literature work focused manual design analysis distributed control laws simple robotic tasks example study rendezvous ﬂocking tasks analyze performance various distributed control laws. however studies based simpliﬁed robot dynamics adapting approaches complicated distributed robotic tasks challenging. little human expertise required learning-based approaches provide alternatives challenging distributed robotic tasks. majority works focus learning action policy assume pre-deﬁned communication protocols communication among agents approaches lack ﬂexibility adapt communication information thus limiting capability distributed robotic system. approach learns control policy communication protocols among agents allowing emergence effective interagent communication. recent work explored learning communication protocols reinforcement learning. speciﬁcally perform independent learning agent learns solely local observations. others assume centralized learning performed particular assume communication channel differentiated backpropagation. employs multiple communication cycles time step similar control framework assumes communication emitted received next time step. different reinforcement learning approaches above approach makes pre-designed centralized policies learns communication protocols imitating behaviour centralized policies. compared reinforcement learning approaches approach learn communication action policies efﬁciently using guidance centralized policies. approach inspired performs supervised learning simple lever-pulling task using multiple communication cycles time step. perform learning distributed robotic domain using different framework assumes single communication cycle time step. consider group homogeneous agents complete task dynamic connectivity network deﬁned bidirectional graph discrete time index. v··· represents agent represents connection agent agent deﬁne {vj|e neighbours agent neighbour deﬁne communication information sent agent agent time work deﬁne communication vector size communication information. communication outﬂow means communication information sent previous time step received current time step agent communication vector special case allows agent leave information itself. work allow self-talking behaviour. time step agent obtains observation inforin. agent mation sends information corresponding neighbours performs action uniﬁed action space agents following equations holding true fig. communication framework three agents consecutive time steps. connectivity network indicated arrows among agents. show neighbours communication associated time step. time index agent communicate agent send information itself. time index agent receives information sent previous time step sends information neighbours itself. fig. multi-step multi-agent neural network consisting identical components example msmann three-agent example fig. grey arrows represent communication vectors also hidden states msmann; green arrows represent inputs neural network arrows represent outputs neural network. represents subset neighbouring nodes local used obtain discretization. apply discretization process communication inﬂow average communication vectors within group agents. also restrict communication outﬂow communication vectors sent agents group identical. formulate this deﬁne communication inﬂow outﬂow concatenations communication vectors discretization represents cardinality set. renders dimensions communication inﬂow outﬂow constant. thus distributed policy learn transformed fig. overall picture homogeneous distributed control framework. agent receives communication inﬂow neighbours previous time step observes surrounding local. given inputs policy provides communication outﬂow action probability distribution communication outﬂow sent agent’s neighbours action agent sampled based probability distribution consider agents homogeneous distributed control framework described section iii. problem targeted work learn mapping communication inﬂow local observations olocal communication outﬂow cout action probability distribution mapping referred distributed policy modelled interprets communication inﬂow local observations infers communication outﬂow action probability distribution. goal policy optimizes performance target task learning process robot actions observations centralized strategy available amount communication inﬂow outﬂow agent changing dynamically number neighbours changes. since learning model variable input output dimensions challenging perform neighbour discretization provide constant input/output dimensions. process agent’s neighbours fig. radial discretization used rendezvous task. visible area discretized components. vectors {∆sd agent center area represent discretized action space. order demonstrate proposed learning approach consider coordination tasks rendezvous problem limited vision range particle assignment task. former well-studied control literature solution compared state-of-the-art distributed control law; latter task solved framework distributed control before. task formulation consider homogeneous agents located -dimensional plane position vectors s··· sk}. agent governed double integrator dynamics position controlled pd-type controller. input controller desired position vector relative agent’s position corresponds action agent. deﬁne dynamic connectivity network based visibility {e|vi vsi−sj dlim} also deﬁne agent observe local. relative positions neighbours formulate action space observation space discretize -dimensional visibility space agent different components observation agent approximated number agents discretized component directly learning communication protocols among agents centralized policy difﬁcult centralized policy provide correct labelled communication protocols among agents. based problem setup communication vectors sent previous time step received current time step resulting communication information across multiple time steps. learn information require learning process operate across multiple time steps well. formulate learning target ﬁrst assume parametrized πdθ. course performing task time steps using distributed policy deﬁne overall mapping achieve mapping modelled neural network consisting identical components connected communication vectors among agents across multiple time steps recall neural network multi-step multi-agent neural network identical components distributed policy network every agent time step. backpropagation performed throughout msmann optimize distributed policy respect learning objective minimizing minimizing learn distributed policy behaves similarly centralized policy. guidance centralized policy hypothesize supervised learning process efﬁcient reward-based learning direct guidance. interesting possibility reward-based learning improvement completed supervised learning centralized policy. possibility left future work. objective task make agents converge common location quickly possible. assume dynamic connectivity network connected initially. task performance primarily evaluated based rendezvous time deﬁned smallest satisﬁes following condition small constant deﬁnes maximum distance farthest agents. constraint never satisﬁed classify failure converge evaluating task multiple trials deﬁne convergence rate follows nsuccess/ntot nsuccess number successful trials ntot number total trials. neighbour discretization neighbour discretization rule builds upon space discretization performed task formulation. using relative positions neighbouring agents deﬁne partition follows rule neighbouring agents discretized component belong discretization group. intuition behind neighbouring agents similar relative positions receive similar communication vectors communication vectors sending back also similar. centralized policy centralized policy computes optimal rendezvous coordinate soptimal minimizes trv. proved soptimal center smallest circle encloses agents moving towards optimal rendezvous coordinate optimal solution achieved. adapt discrete action space optimal action agent closest optimal coordinate existing distributed policies best knowledge circumcenter state-of-the-art distributed policy task guarantees convergence single integrator dynamics result simply extended second-order systems double integrator dynamics consider task. circumcenter deﬁned follows time step agents pursue circumcenter point consisting neighbours adapt control control framework described section choose agent’s action based relative position agent circumcenter equivalently communication inﬂow consist zero vectors time steps. existing distributed policies averaging cyclic pursuit. averaging requires agents pursue average coordinate neighbours itself. however suffers convergence issues cyclic pursuit usually assumes ﬁxed connectivity network given task also propose distributed robotic task existing solution best knowledge. objective move agents target points target points covered agent using agents’ local observations neighbouring agents target points communication. different cooperative navigation task deﬁned assumes unlimited visibility range assume limited visibility range. task formulation consider homogeneous agents position vectors s··· dimensional plane. agent dynamics pd-controller discrete action space connectivity network discretization rule described rendezvous task. adding these introduce target points ¯v··· ¯vk} position vectors ¯s··· ¯sk}. deﬁne connectivity target points based visibility {e|¯vi ¯si−¯sj dlim} dlim limited visibility range. assume connected initially least agent observe target point initially. agent also pre-deﬁned potential ﬁeld layer collision avoidance mechanism pd-controller. convenience deﬁne neighbouring target points follows fig. performance comparison centralized policy state-of-the-art distributed policy learned policy communication enabled learned policy without communication rendezvous task different number agents. data point average trials random initial conditions. explicitly exclude examples fail converge since including examples would blow average performance desired comparison purposes. instead show convergence rate. video simulations available online http //tiny.cc/dnnswarm. current time step number time steps back-propagate. work choose nbatch tasks rendezvous task particle assignment task. rendezvous task simplify communication inﬂow communication vectors discretization instead concatenation communication vectors allows better analyze communication content learned. rendezvous task train distributed policy network agents random initial positions limit size communication vector evaluate performance scenarios different number agents agent density similar training cases. agent density deﬁned ratio number agents area smallest circle encloses agents. compare performance learning approach state-of-the-art circumcenter distributed control centralized policy described section vi-a. also show performance learning approach without communication fig. demonstrate learning approach consistently outperforms state-of-the-art distributed policy different numbers agents. however learning approach without communication performs poorly almost circumstances demonstrates necessity inter-agent communications resembling behaviours centralized policy note circumcenter control require communication behaves qualitatively different centralized strategy. however convergence rate drop signiﬁcantly scenarios large number agents included training data. generalization issue learning might over-ﬁt simple situations model trained provided limitation learning approach centralized policy design centralized policy optimal assignment agents target points maximum distance travelled agent minimized. leads minimization completion time tpa. optimization done using hungarian algorithm tasks feedforward neural network probability distribution obtained softmax layer. loss function probability distributions cross entropy. train nbatch simulations parallel different initial setups. simulations agent’s action sampled action distribution predicted distributed policy network. every time steps construct msmann model based dynamic connectivity agents past time steps. using action probability distribution centralized policy perform adam algorithm backpropagation msmann minimize average loss functions nbatch parallel simulations cost function approximation overall learning objective fig. comparison centralized policy state-of-the-art distributed policy learned distributed policy communication enabled disabled -agent rendezvous task. dots agents; circles visible regions agents; line represents distance farthest agents. show learned distributed policy resemble behaviour centralized policy signiﬁcantly better communication available. fig. probability choosing discretized action given communication inﬂow values. show ﬁgure. channel represents channel represents observed discretized area direction increasing action probability always opposite desired relative position corresponding action discretized area. situation agent neighbours located exactly opposite direction. convenience deﬁne fig. hypothesize communication inﬂow values transformed -dimensional vector correlated tendency choosing action closest vector’s direction. interpret communication vector intent vector inﬂuences tendency moving direction agent receives intent vector. explains sudden drop observed performance since onedimensional communication vector cannot fully represent -dimensional direction vector. fig. performance learned policy different communication vector sizes rendezvous task. data point average trials different random initial conditions. sudden drop performance observed. demonstrate reducing size communication vectors leads decrease task performance signiﬁcant drop provide insights result choose analyze learned distributed policy communication size relatively easy visualize achieving comparable performance. keep observation input model constant observe change model output changing communication input. constant observation input assume hypothetical particle assignment task also train -agent scenarios test approach various numbers agents fig. demonstrates examples performance achieved agents emergence distributed behaviours. fig. also show average performance learned distributed policy comparable centralized policy fewer agents. approach suffers convergence issues larger swarms. hypothesize could attributed inherent complexity particle assignment task. multiple aspects task must achieved exploring resolving assignment conﬂict staying connected agents. achieving aspects much challenging number agents increases. present dnn-based approach learns distributed action communication policies well-designed centralized policies homogeneous distributed robotic system. main advantages proposed approach summarized approach applied various distributed robotic tasks given pre-designed centralized policies fig. performance comparison centralized policy learned distributed policy different numbers agents particle assignment task. data point average trials. video simulations available online http//tiny.cc/dnnswarm campo nouyan birattari groß dorigo negotiation goal direction cooperative transport proc. international conference colony optimization swarm intelligence chen overview recent progress study distributed multi-agent coordination ieee transactions industrial informatics vol. kasai tenmoto kamiya learning communication codes multi-agent reinforcement learning problem proc. ieee conference soft computing industrial applications giles k.-c. learning communication multiagent systems proc. innovative concepts agent-based systems first international workshop radical agent concepts chen everett decentralized noncommunicating multiagent collision avoidance deep reinforcement learning proc. ieee international conference robotics automation zhang lesser coordinating multi-agent reinforcement learning limited communication proc. international conference autonomous agents multi-agent systems fig. behaviour comparison centralized policy learned distributed policy -agent -agent scenarios. dots represent agents; lines represent trajectories agents; small circles represent cover range agents; large dashed circles represent visibility agents; diamonds represent target points. demonstrate agents controlled learned distributed policy able follow agents targets target sight explore neighbouring targets rather stopping nearest target resolve target assignment conﬂicts. distributed behaviours emerge learning centralized policy. available; requires little human expertise taskspeciﬁc control communication protocol designs; approach computationally efﬁcient compared reward-based learning approaches. moreover learned communication protocols reveal meaningful messages conveyed could potentially inspire coordination communication designs real-world distributed robotic systems. future work address observed convergence issues complex scenarios over-ﬁtting.", "year": 2017}