{"title": "Deep Kernelized Autoencoders", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "In this paper we introduce the deep kernelized autoencoder, a neural network model that allows an explicit approximation of (i) the mapping from an input space to an arbitrary, user-specified kernel space and (ii) the back-projection from such a kernel space to input space. The proposed method is based on traditional autoencoders and is trained through a new unsupervised loss function. During training, we optimize both the reconstruction accuracy of input samples and the alignment between a kernel matrix given as prior and the inner products of the hidden representations computed by the autoencoder. Kernel alignment provides control over the hidden representation learned by the autoencoder. Experiments have been performed to evaluate both reconstruction and kernel alignment performance. Additionally, we applied our method to emulate kPCA on a denoising task obtaining promising results.", "text": "abstract. paper introduce deep kernelized autoencoder neural network model allows explicit approximation mapping input space arbitrary user-speciﬁed kernel space back-projection kernel space input space. proposed method based traditional autoencoders trained unsupervised loss function. training optimize reconstruction accuracy input samples alignment kernel matrix given prior inner products hidden representations computed autoencoder. kernel alignment provides control hidden representation learned autoencoder. experiments performed evaluate reconstruction kernel alignment performance. additionally applied method emulate kpca denoising task obtaining promising results. autoencoders class neural networks gained increasing interest recent years used unsupervised learning eﬀective hidden representations input data representations capture information contained input data providing meaningful features tasks clustering classiﬁcation however eﬀective representation consists highly dependent target task. standard representations derived training network reconstruct inputs either bottleneck layer thereby forcing network learn compress input information over-complete representation. latter regularization methods employed e.g. enforce sparse representations make representations robust noise penalize sensitivity representation small changes input however regularization provides limited control nature hidden representation. matrix used prior reproduced inner products hidden representations learned hence addition minimizing reconstruction loss also minimize normalized frobenius distance prior kernel matrix inner product matrix hidden representations. note process resembles kernel alignment procedure proposed model called deep kernelized autoencoder related recent attempts bridge performance kernel methods neural networks speciﬁcally connected works interpreting neural networks kernel perspective information theoretic-learning auto-encoder imposes prior distribution hidden representation variational autoencoder addition providing control hidden representation method also several beneﬁts compensate important drawbacks traditional kernel methods. training learn explicit approximate mapping function input kernel space well associated back-mapping input space end-to-end learning procedure. mapping learned used relate operations performed approximated kernel space example linear methods input space. case linear methods equivalent performing non-linear operations non-transformed data. mini-batch training used proposed method order lower computational complexity inherent traditional kernel methods especially spectral methods additionally method applies arbitrary kernel functions even ones computed ensemble methods. stress fact consider experiments probabilistic cluster kernel kernel function robust regards hyperparameter choices shown often outperform counterparts kernel simultaneously learn functions. ﬁrst encoder provides mapping input domain code domain i.e. hidden representation. second function decoder maps back single hidden layer encoding function decoding function denotes suitable transfer function denote respectively sample input space hidden representation reconstruction; ﬁnally weights bias encoder decoder respectively. sake order minimize discrepancy original data reconstruction parameters typically learned minimizing usually stochastic gradient descent reconstruction loss diﬀerently stacked autoencoder consists several hidden layers deep architectures capable learning complex representations transforming input data multiple layers nonlinear processing optimization weights harder case pretraining beneﬁcial often easier learn intermediate representations instead training whole architecture end-to-end important application pretrained initialization layers deep neural networks pretraining performed diﬀerent phases consists training single applied input resulting representation used train next stacked architecture. layer trained independently aims capturing abstract features trying reconstruct representation previous layer. individual trained unfolded yielding pretrained sae. two-layer encoding function consists ﬁnal architecture ﬁne-tuned end-to-end back-propagating gradient reconstruction error. principal component analysis common dimensionality reduction technique projects data subspace preserves maximal amount variance kernel space requires compute eigendecomposireason kpca applicable large-scale problems. availability eﬃcient mapping functions however would reduce complexity thereby enabling methods applicable larger datasets furthermore providing approximation would possible directly control visualize data represented finding explicit inverse mapping central problem several applications image probabilistic cluster kernel adapts inherent structures data depend critical user-speciﬁed hyperparameters like width gaussian kernels. trained ﬁtting multiple gaussian mixture models input data combining models single kernel. particular gmms trained variety mixture components diﬀerent randomized initial conditions denote posterior distribution data point mixture components initial condition deﬁned intuitively posterior distribution mixture model contains probabilities given data point belongs certain mixture component model. thus inner products large data pairs often belong mixture component. averaging inner products range values kernel function large value data points similar global scale local scale reconstruction loss hyperparameter ranging loss function simpliﬁes traditional loss code loss distance measure matrices rn×n kernel matrix given prior rn×n inner product matrix codes associated input data. objective enforce similarity terms. first reconstruction error true input output dkae ˜xi. second term distance measure matrices target prior kernel matrix minibatch training matrix computed codes data mini-batch distance compared submatrix related current mini-batch. exactly kernel alignment cost function note distance implemented also advanced diﬀerentiable measures similarity matrices divergence mutual information however options explored paper left future research. fig. encoder maps input lies code space. dkaes code domain approximates space associated prior kernel linear method receives input produces output decoder maps back input space. result seen output non-linear operation input space. mini batches samples train dkae thereby avoiding computational restrictions kernel especially spectral methods outlined sec. making mini-batch training memory complexity computational complexity scales linearly regards parameters network. particular given mini batch samples dkae loss function deﬁned taking average per-sample reconstruction cost dimensionality input space subset contains rows columns related current mini-batch contains inner products codes speciﬁc mini-batch. note re-computed mini batch. scheme proposed dkae explicitly approximates function maps input onto kernel space. particular dkae feature vector approximated code following underlying idea kernel methods inspired cover’s theorem states high dimensional embedding likely linearly separable linear operations performed code. linear operation produces result code space relative input codes mapped back input space kernel space back input domain. unlike kernel methods explicit mapping deﬁned fact permits visualization interpretation results original space. section evaluate eﬀectiveness dkaes diﬀerent benchmarks. ﬁrst experiment evaluate eﬀect terms objective function varying hyperparameters size code layer. second experiment study reconstruction kernel alignment. compare dkaes approximation accuracy prior kernel matrix kpca number principle components increases. finally present application method image denoising experiments consider mnist dataset consisting images handwritten digits. however subset samples computational restrictions imposed illustrate dkaes ability learn arbitrary kernels even originate ensemble procedure. train ﬁtting gmms subset training samples parameters trained models applied remaining data calculate kernel matrix. data training validation testing respectively. network architecture used experiments demonstrated perform well several datasets including mnist supervised unsupervised tasks here refers dimensionality code layer. training performed using pretraining approach outlined sec. avoid learning identify mapping individual layer applied common regularization technique encoder decoder weights tied i.e. done pretraining ﬁne-tuning. unlike traditional saes account kernel alignment objective code layer optimized according also pretraining. size mini-batches training chosen randomly independently sampled data points; experiments epoch consists processing batches. pretraining performed epochs layer ﬁnal architecture ﬁne-tuned epochs using gradient descent based adam dkae weights randomly initialized according glorot experiment evaluate inﬂuence main hyperparameters determine behaviour architecture. note experiments shown section performed training dkae training evaluating performance validation set. evaluate out-of-sample reconstruction figure illustrates eﬀect ﬁxed value neurons code layer. observed reconstruction loss increases focus minimizing quantiﬁes empirically tradeoﬀ optimizing reconstruction performance kernel alignment time. similarly observed decreases increasing inspecting results speciﬁcally near constant losses range method appears robust changes hyperparameter analyzing eﬀect varying given ﬁxed observe losses decrease increases. could suggest even larger architecture characterized layers neurons w.r.t. architecture adopted might work well dkae seem overﬁt also regularization eﬀect provided kernel alignment. according previous results following experiments figure illustrates results sec. qualitatively displaying original images test reconstruction chosen value non-optimal one. similarly prior kernel dkaes approximated kernel matrices relative test data displayed diﬀerent values. notice that illustrate diﬀerence traditional values zero. clearly seen that reconstruction kernel matrix resemble original closely agrees plots figure inspecting kernels obtained figure compare distance between kernel matrices ideal kernel matrix obtained considering supervised information. build ideal kernel matrix elements belong class otherwise table illustrates kernel approximation produced dkae outperforms traditional regards kernel alignment ideal kernel. additionally seen kernel approximation actually improves slightly order quantify kernel alignment performance compare dkae approximation provided kpca varying number principal components. test take kernel matrix training compute eigendecomposition. select increasing number components project input data emλ/ approximation original kernel matrix given zmzt compute distance following compare dissimilarity evaluating out-of-sample performance nystr¨om approximation kpca compare dkae kernel approximation test set. figure shows approximation obtained means dkaes outperforms kpca using small number components i.e. note fig. comparing dkaes approximation kernel matrix kpca increasing number components. plot shows dkae reconstruction accurate number components. common spectral methods chose number components equal number classes dataset case classes mnist dataset dkae would outperform kpca. number selected components increases approximation provided kpca perform better. however shown previous experiment mean approximation performs better regards ideal kernel. fact experiment kernel approximation dkae actually performed least well prior kernel hint potential performing operations code space described sec. emulate kpca performing learned kernel space evaluate performance task denoising. denoising task requires mapping kernel space well back-projection. traditional kernel methods explicit back-projection exists approximate solutions called pre-image problem proposed chose method proposed bakir kernel ridge regression diﬀerent kernel used back-mapping. challenging good kernel captures numbers mnist dataset performed test class only. regularization parameter required backprojection found grid search best regularization parameter found median euclidean distances projected feature vectors. models ﬁtted training gaussian noise added test set. methods principle components used. tab. illustrates dkae+pca outperforms kpcas reconstruction regards mean squared error. however necessarily good measure denoising also visualize results fig. seen dkae yields sharper images denoising task. paper proposed novel model autoencoders based deﬁnition particular unsupervised loss function. proposed model enables learn approximate embedding input space arbitrary kernel space well projection kernel space back input space end-to-end trained model. worth noting that method able approximate arbitrary kernel functions inner products code layer allows control representation learned autoencoder. addition enables emulate well-known kernel methods kpca scales well number data points. rigorous analysis learned kernel space embedding well applications code space representation clustering and/or classiﬁcation tasks left future works. acknowledgments. gratefully acknowledge support nvidia corporation donation used research. work partially funded norwegian research council fripro grant developing next generation learning machines.", "year": 2017}