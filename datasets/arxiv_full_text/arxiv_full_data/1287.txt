{"title": "Visualization Regularizers for Neural Network based Image Recognition", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "The success of deep neural networks is mostly due their ability to learn meaningful features from the data. Features learned in the hidden layers of deep neural networks trained in computer vision tasks have been shown to be similar to mid-level vision features. We leverage this fact in this work and propose the visualization regularizer for image tasks. The proposed regularization technique enforces smoothness of the features learned by hidden nodes and turns out to be a special case of Tikhonov regularization. We achieve higher classification accuracy as compared to existing regularizers such as the L2 norm regularizer and dropout, on benchmark datasets without changing the training computational complexity.", "text": "success deep neural networks mostly ability learn meaningful features data. features learned hidden layers deep neural networks trained computer vision tasks shown similar midlevel vision features. leverage fact work propose visualization regularizer image tasks. proposed regularization technique enforces smoothness features learned hidden nodes turns special case tikhonov regularization. achieve higher classiﬁcation accuracy compared existing regularizers norm regularizer dropout benchmark datasets without changing training computational complexity. regularization important aspect deep neural network training prevent over-ﬁtting absence sufﬁcient data. usually regularizers restrict norms weight parameters. commonly used class regularizers norm regularizers penalize norms weight parameters. these norm regularizers popular. regularizers include soft-weight sharing layer-wise unsupervised pre-training dropout shown erhan layer-wise unsupervised pre-training regularizing effect training. also recent works adaptive dropout improvement original dropout. paper propose novel regularizer call visualization regularizer based visual quality features learned hidden nodes. introduce variants regularizer based norms respectively. vision tasks beneﬁt features consisting primitives recognized mid-level vision systems. deep neural networks known learn hierarchical layers feature representation observing features learned deep neural networks trained using back propagation seen contrast well deﬁned mid-level features node features often noisy. meaningful features favorable training network. proposed regularizer imposes constraint hidden nodes neural network learn smoother features. since deﬁnition regularizer depends visual property smoothness pertinent domains notion spatial locality images. show regularizer special case tikhonov regularization tikhonov matrix conventional norm regularizer corresponds identity matrix multiplied regularization weight. whereas tikhonov matrix regularizer generalised sparse. perform experiments regularizer benchmark datasets mnist cifar- observe regularizer aids learning improves classiﬁcation accuracy used alongside regularizers. however computational complexity regularized training algorithm remains unregularized training algorithm. paper organized sections. section gives brief introduction architecture deep neural networks notation used. notion visualization node formally deﬁned section iii. proposed regularizer training algorithm along complexity analysis described section section establishes relationship tikhonov regularization. section presents experimental results observations. finally section concludes summary achievements scopes future work. investigations presented paper based multi-class classiﬁcation setting. notation followed paper follows denotes input respectively denote weights biases corresponding layers denotes pre-activation layers denotes activation layers denotes output neural network. subscripts notation denotes hidden layer. following equations describe deep neural network hidden layers. denotes classiﬁcation loss output neural network true class labels denotes regularization term denotes regularizer weight. however dropout cannot included loss function. incorporated training algorithm. visualization node refers visualization features learned node. visualization nodes neural network studied erhan proposed activation maximization algorithm visualizing features learned node. following notion deﬁne visualization node follows. visualization node deﬁned input pattern activate node maximally restriction norm input equal unity. norm input restricted unity prevent input becoming unbounded. formally visualization node deﬁned denotes activation node input depending non-linearity used equation solution equation unique multiple. example exists unique solution invertible functions like tanh. whereas multiple solutions non-invertible functions also known rectiﬁed linear unit visualization internal node neural network computed using gradient-ascent described erhan activation maximization. observe pre-activation node ﬁrst hidden layer input vector denotes weights connections coming node pre-activation maximized aligned direction appropriate vector space. since activation function monotonically increasing function maximization pre-activation maximizes activation too. consequently following closed form possible solutions equation nodes ﬁrst hidden layer. regularizer based expression visualization given equation utilize equation produce appropriate regularization loss include ﬁnal training loss. following subsections give detailed description regularizer. intuitively determine whether image smooth noisy looking gradients image. image smooth small gradients. gradient image computed convolving high pass ﬁlter. examples high pass ﬁlters include ﬁrst order gradient ﬁlters sobel operator second order gradient ﬁlters laplacian operator. larger pixel values convolution larger gradients original image greater presence noise image. utilize intuition give formal deﬁnition smoothness. consider convolution image kernel high pass ﬁlter like laplacian kernel. deﬁne smoothness image negative squares pixel values equivalently deﬁne visualization loss image denotes element-wise product also known schur hadamard product. visualization loss negative smoothness image. lower visualization loss smoother image. table shows visualization loss example visualizations. classiﬁcation tasks using deep neural networks beneﬁt high-level abstractions achieved higher layers neural network. deep neural networks intended utilize low-level pixels learn mid-level features ﬁnally high-level features. propose visualization regularizer constrain nodes ﬁrst hidden layer learn features qualities similar mid-level visual features. constraint intended facilitate discovery high-level abstractions effectively. informally deﬁne regularizer regularizer reduce visualization loss nodes neural network. words regularizer makes nodes learn smooth less noisy features. denote nodes ﬁrst hidden layer neural network node denote weights connections incoming node. equation know visualization node ﬁrst hidden layer proportional weights connections coming node. hence visualization loss node proportional visualization loss weight vector coming node. therefore surrogate visualization loss visualization node difﬁculty computing algebraic expression visualization nodes higher hidden layers limits usage surrogate nodes ﬁrst hidden layer only. visualization loss used regularizer gradient must computed respect model parameters. automatic gradient computation libraries tensorﬂow theano obviate need compute gradients manually. however additional overhead computational resources hence manual computation gradients increased efﬁciency derive expression derivative regularizer. also useful case embedded computing computational resources limited. following paragraphs derive gradient general kernel size simplicity computing expression gradient index elements kernel relative central element shown equation element center indexed elements indexed according position relative central element. equation allows compute gradient efﬁciently scalable manner. popular programming frameworks provide libraries scalable convolutions. figure illustrates gradient computation visualization loss. training requires computing gradient regularized loss function respect parameters network. evident equation gradient loss function computed ﬁrst computing gradients computing sum. gradients computed using back-propagation partial derivatives respectively. equation gradient gradients note zero words gradient zero gradient belong weights incoming node hence need compute gradients computed using equation full algorithm described ﬁgure algorithm extended using dropout momentum. computing ﬁgure takes consequently computing gradient regularizer takes time order computing gradient norm regularizer. thus regularizer impose additional overhead computational complexity iteration. description regularizer deﬁned smoothness image squares convoluted image actually square -norm ﬂattened vector analogously also deﬁne -norm variant regularizer. distinguish -norm -norm variants denote respective losses similar deﬁnition equation deﬁned follows. tikhonov regularization originally developed solutions ill-posed problems example regularization special case tikhonov regularization used compute solutions regression problems rank deﬁcient matrices encountered computing solutions. solution regularized least squares regression given tikhonov matrix. regularizer corresponds show regularizer also special case tikhonov regularization. concatenation weights equation ctwt) squared terms form represented given form example zero. tikhonov matrix constructed follows. consider expression consisting squared terms. term practice assumed kernel matrix constant size. hence follows expression consists constant number non-zero since number non-zero bounded size kernel matrix hence number non-zero entries whereas total number entries p|w| concluding sparsity classiﬁcation mnist digits experimented fully connected architectures convolutional architectures experimented convolutional architectures classiﬁcation cifar- objects. fully connected architectures applied regularizer weights layer immediately input layer i.e. ﬁrst hidden layer. convolutions however applied regularizer weights fully connected layer immediately last convolutional layer. detailed layerwise descriptions architectures used experiments given table used mean cross-entropy loss mini-batches classiﬁcation loss. total training loss deﬁned equation trained neural network using stochastic gradient descent momentum experiment code theano available https//github.com/biswajitsc/visregdl. implementation tensorﬂow also available https//github.com/cvikasreddy/visreg. table description network architectures used experiments. denotes fully connected layer nodes dropout denotes dropout layer nodes dropped probability conv denotes convolutional layer kernel size output channels maxpool denotes maxpooling layer sized kernel. reduced factor every epochs. mnist reduced epoch reduced factor thereafter every epoch. ﬁnding optimal values regularization weights performed randomized hyper-parameter search manual ﬁnetuning. compared training various neural network models combinations dropout regularizers. accuracy optimal hyper-parameters various regularizer settings given table parameters respectively denote regularizer weights regularizer weight learning rate. paper introduced regularizer deep neural networks trained image tasks. formulated regularizer based notion smoothness visualization also derived expression gradient. experimentally observe regularizer aids learning leads improvement classiﬁcation accuracy. regularizer introduces class regularizers based domain assumptions kind regularizers also used domains audio video similar natural images transitions along dimensions data noisy mostly smooth. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research dumitru erhan yoshua bengio aaron courville pierre-antoine manzagol pascal vincent samy bengio. unsupervised pre-training help deep learning? journal machine learning research mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg man´e rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vi´egas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems http //tensorflow.org/. software available tensorﬂow.org. nesterov. method solving convex programming problem convergence rate {o}. soviet mathematics doklady http//www.core. ucl.ac.be/{˜}nesterov/research/papers/dan.pdf.", "year": 2016}