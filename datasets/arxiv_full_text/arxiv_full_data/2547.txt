{"title": "Thompson Sampling is Asymptotically Optimal in General Environments", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We discuss a variant of Thompson sampling for nonparametric reinforcement learning in a countable classes of general stochastic environments. These environments can be non-Markov, non-ergodic, and partially observable. We show that Thompson sampling learns the environment class in the sense that (1) asymptotically its value converges to the optimal value in mean and (2) given a recoverability assumption regret is sublinear.", "text": "discuss variant thompson sampling nonparametric reinforcement learning countable classes general stochastic environments. environments non-markov nonergodic partially observable. show thompson sampling learns environment class sense asymptotically value converges optimal value mean given recoverability assumption regret sublinear. reinforcement learning agent interacts unknown environment goal maximizing rewards. recently reinforcement learning received surge interest triggered success applications simple video games however theory lagging behind application theoretical analyses done bandit framework markov decision processes restricted environment classes fall short full reinforcement learning problem theoretical results usually assume ergocity visiting every state inﬁnitely often. needless assumptions satisﬁed simplest applications. goal lift restrictions; consider general reinforcement learning top-down approach understand fundamental underlying problems generality. approach general nonparametric assume true environment belongs given countable environment class. asymptotic optimality requires asymptotically agent learns optimally discounted value agent’s policy converges optimal discounted value environments environment class. convergence impossible deterministic policies since agent explore inﬁnitely often long stretches time policies converge almost surely ces`aro average bayes-optimal agents generally asymptotically optimal however asymptotic optimality achieved exploration component bayes-optimal agent optimism asymptotic optimality mean essentially weaker variant probably approximately correct comes without concrete convergence rate probability policy ε-suboptimal converges zero eventually probability less since environment class large non-compact concrete pac/convergence rates likely impossible. regret many expected rewards agent forfeits following best informed policy. different problem classes different regret rates depending structure difﬁculty problem class. multiarmed bandits provide worst-case regret bound number arms markov decision processes lower bound number states number actions diameter countable class environments given state representation functions histories states regret achievable assuming resulting weakly communicating problem class considered learnable algorithm sublinear regret guarantee. bayesian agent aixi proof satisﬁes various optimality guarantees recently revealed optimality notions trivial subjective bayesian agent explore enough lose prior’s bias particularly prior make agent conform arbitrarily policy long policy yields rewards. negative results bayesian approach question. paper remedy situation showing using bayesian techniques agent indeed optimal objective sense. agent consider known thompson sampling posterior sampling bayesian control rule samples environment posterior follows ρ-optimal policy effective horizon repeats. show agent’s policy asymptotically optimal mean furthermore using recoverability assumption environment assumptions discount function prove worst-case regret sublinear. ﬁrst time convergence regret bounds thompson sampling shown general conditions. thompson sampling originally proposed thompson bandit algorithm easy implement often achieves quite good results multi-armed bandits attains optimal regret thompson sampling also considered mdps model-free method relying distributions q-functions convergence guarantee model-based algorithm without theoretical analysis bayesian frequentist regret bounds also established guarantees established optimistic variant thompson sampling mdps general thompson sampling ﬁrst suggested resampling every time step. authors prove action probabilities thompson sampling converge action probability optimal policy alsurely require ﬁnite environment class technical assumptions behavior posterior distribution similarity environments class. convergence results require assumptions rely recoverability assumption regret bound. alphabet inﬁnite strings alphabet empty string denoted confused small positive real number given string denote length |x|. string length denote ﬁrst characters ﬁrst characters notation denotes probability distributions reinforcement learning agent interacts environment cycles time step agent chooses action receives percept consisting observation real-valued reward cycle repeats assume rewards bounded actions percepts ﬁnite. discount function history element denote interaction cycle denote history length treat action percepts histories outcomes random variables. policy function mapping history distribution actions taken seeing history; probability action denoted environment function mapping history action distribution percepts generated history; probability percept denoted policy environment generate probability measure inﬁnite histories deﬁned values cylinder sets æ<t} strens proposes following optimal policy episode related number state transitions agent likely need plan ahead follow strens’ suggestion resample effective horizon. note stochastic policy since occasionally sample distribution. assume sampling independent everything else. deﬁnition policy asymptotically optimal environment class different types asymptotic optimalities based type stochastic convergence convergence occurs almost surely called strong asymptotic optimality convergence occurs mean called asymptotic optimality mean; convergence occurs probability called asymptotic optimality probability; ces`aro averages converge almost surely called weak asymptotic optimality theorem immediately implies thompson sampling also asymptotically optimal probability convergence mean random variables stated theorem equivalent convergence probability sense random variables nonnegative bounded. however imply almost sure convergence satisﬁed eventually posterior converges -almost surely. note random limit depends history without loss generality assume true environment contained since -almost surely. follows lemma since convergence mean implies convergence probability. proof theorem denote sequence used deﬁne algorithm assume large enough small enough holds since µπ-almost surely policy stochastic process -martingale since remains show high probability value sample optimal policy sufﬁciently close µ-optimal value worst case draw worst sample twice row. denote sample environment draw time step denote time step probability sample following therefore -probability least true -probability least sample happens w-probability least sample events true simultaneously probability least hence bound transfers -probability therefore probability. value function bounded thus also converges mean dominated convergence theorem. totic optimality. however notions fundamentally different allow inﬁnite numµ mistakes asymptotic optimality mean allows mistakes long probability converges zero; weak asymptotic optimality allows mistakes long total time spent mistakes grows sublinearly. lattimore hutter show weak asymptotic optimality possible countable class deterministic environments using mdl-agent explores bursts random walks classes stochastic environments bayesexp weakly asymptotically optimal however requires additional condition effective horizon grows sublinearly theorem require condition. generally weak asymptotic optimality asymptotic optimality mean incomparable notions convergence incomparable random variables. first deterministic sequences convergence mean equivalent convergence implies convergence ces`aro average vice versa. second convergence probability imply almost sure convergence ces`aro averages leave open question whether policy weakly asymptotically optimal. strong asymptotic optimality known impossible deterministic policies whether possible stochastic policies open question. however show thompson sampling strongly asymptotically optimal. example deﬁne assume geometric discounting consider following class environments note deﬁnition demands becomes less costly recover mistakes time progresses. regarded effect discount function horizon grows recovery becomes easier optimal policy time perform recovery. moreover recoverability optimal policy contrast notion ergodicity mdps demands returning starting state regardless policy. remark effective horizon growing weakly communicating ﬁnite state partially observable satisﬁes recoverability assumption. deﬁnition regret policy environment note regret undiscounted always nonnegative. moreover supremum always attained policy optimal policy policy uses discounting) since space possible different policies ﬁrst actions ﬁnite since assumed actions percepts ﬁnite. assumption discount function problem geometric discounting makes recoverability assumption strong since horizon growing environment enable faster recovery time progresses; case weakly communicating partially observable mdps recoverable. choice satisﬁes assumption e−√t/√t discount function e−√t thus time step path state gets unlocked optimal policy take action twice state class class deterministic weakly communicating mdps optimal policy environment always take action optimal policy environment take action take action state action otherwise. suppose policy acting environment since asymptotically optimal class take actions inﬁnitely often environment indistinguishable posterior larger equal prior. hence always constant chance sampling taking actions point environments become falsiﬁed. policy decides explore take ﬁrst action state denote current history. ν∞-optimal action general environments classes worst-case regret linear agent caught trap unable recover achieve sublinear regret need ensure agent recover mistakes. formally make following assumption. deﬁnition environment satisﬁes recoverability assumption horizon grows linearly inﬁnitely often policy might spend constant fraction current effective horizon exploring incurs cost constant fraction total regret far. example two-armed bernoulli bandit means suppose using geometric discounting discount factor environment recoverable. policy pulls suboptimal exactly time steps regret logarithmic. however time steps value difference deterministically least implications following immediate consequence. corollary function satisﬁes assumption environment satisﬁes recoverability assumption proof. theorem since policy asymptotically optimal environment satisfy recoverability assumption regret linear even optimal policy optimal policy maximizes discounted rewards short-sightedness might incur tradeoff leads linear regret later environment allow recovery. corollary discount function satisﬁes assumption environment satisﬁes recoverability assumption thompson sampling policy paper introduced reinforcement learning policy based thompson sampling general countable environment classes proved asymptotic statements policy. theorem states asymptotically optimal mean value true environment converges optimal value. corollary states regret sublinear difference expected average rewards best informed policy converges statements come without concrete convergence rate weak assumptions made environment class. asymptotic optimality taken grain salt. provides incentive agent avoid traps environment. agent gets caught trap actions equally thus optimal asymptotic optimality achieved. even worse asymptotically optimal agent explore traps might contain hidden treasure. overall dichotomy asymptotic nature asymptotic optimality discounting prioritize present future. ideally would want give ﬁnite guarantees instead without additional assumptions likely impossible general setting. regret bound could step right direction even though asymptotic nature. bayesians asymptotic optimality means pos| æ<t) concentrates environterior distribution thompson sampling works optimal policy environment draw posterior will higher higher probability also optimal true environment. bayesian mixture inside class assign prior probability arbitrarily close since posterior prior thompson sampling according bayes-optimal policy time. means bayes-value thompson sampling made arbitrarily small thus thompson sampling near-optimal legg-hutter intelligence contrast bayes-value thompson sampling also suppose class -armed bandits indexed bandit gives reward reward reward arms. geometric discounting bayes-optimal pull thompson sampling explore average arms ﬁnds optimal arm. bayes-value thompson sampling contract achieved bayes. horizon bayes-optimal policy suffers regret thompson sampling regret much larger small exploration performed thompson sampling qualitatively different exploration bayesexp bayesexp performs phases exploration maximizes expected information gain. explores environment class completely even achieving off-policy prediction contrast thompson sampling explores optimal policies environment classes yield off-policy prediction. sense exploration mechanism thompson sampling reward-oriented maximizing information gain. possible avenues future research providing concrete convergence rates speciﬁc environment classes results uncountable environment classes. latter different analysis techniques true environment typically assigned prior probability proofs lemma theorem rely dividing takjohn asmuth lihong michael littman nouri david wingate. bayesian sampling approach exploration reinforcement learning. uncertainty artiﬁcial intelligence pages s´ebastien bubeck cesa-nicol`o bianchi. regret analysis stochastic nonstochastic multi-armed bandit problems. foundations trends machine learning marcus hutter. self-optimizing paretooptimal policies general environments based bayes-mixtures. computational learning theory pages springer emilie kaufmann nathaniel korda r´emi munos. thompson sampling asymptotically optimal ﬁnite-time analysis. algorithmic learning theory pages springer volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature phuong nguyen odalric-ambrym maillard daniil ryabko ronald ortner. competing inﬁnite models reinforcement learning. artiﬁcial intelligence statistics pages laurent orseau lattimore marcus hutter. universal knowledge-seeking agents stochastic environments. algorithmic learning theory pages springer deﬁned equal natural numbers starting probability distributions ﬁnite strings alphabet inﬁnite strings alphabet possible actions possible percepts different actions action time step percept time step reward time step bounded history time i.e. ﬁrst interactions aeae at−et− history length small positive real number discount function discount normalization factor ε-effective horizon deﬁned policy i.e. function optimal policy environment value policy environment natural numbers time step time step effective horizon countable class environments environments i.e. functions true environment bayesian mixture environments", "year": 2016}