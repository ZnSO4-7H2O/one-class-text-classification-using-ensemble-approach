{"title": "A Deep Learning Interpretable Classifier for Diabetic Retinopathy  Disease Grading", "tag": ["cs.LG", "cs.CV", "stat.ML", "68T10", "I.2; I.4; I.5"], "abstract": "Deep neural network models have been proven to be very successful in image classification tasks, also for medical diagnosis, but their main concern is its lack of interpretability. They use to work as intuition machines with high statistical confidence but unable to give interpretable explanations about the reported results. The vast amount of parameters of these models make difficult to infer a rationale interpretation from them. In this paper we present a diabetic retinopathy interpretable classifier able to classify retine images into the different levels of disease severity and of explaining its results by assigning a score for every point in the hidden and input space, evaluating its contribution to the final classification in a linear way. The generated visual maps can be interpreted by an expert in order to compare its own knowledge with the interpretation given by the model.", "text": "deep neural network models proven successful image classiﬁcation tasks also medical diagnosis main concern lack interpretability. work intuition machines high statistical conﬁdence unable give interpretable explanations reported results. vast amount parameters models make diﬃcult infer rationale interpretation them. paper present diabetic retinopathy interpretable classiﬁer able classify retine images diﬀerent levels disease severity explaining results assigning score every point hidden input space evaluating contribution ﬁnal classiﬁcation linear way. generated visual maps interpreted expert order compare knowledge interpretation given model. deep learning methods used extensively last years many automatic classiﬁcation tasks. case image analysis usual procedure consists extracting important features convolutional layers that make ﬁnal classiﬁcation features using fully connected layers. finally soft-max output layer gives result predicted output probabilities classes predeﬁned model. during training model parameters changed using gradient-based optimization classiﬁer trained quality classiﬁcation outputs predicted compared correct true values stored labeled dataset. data considered gold standard ideally coming consensus knowledge human experts committee. mapping allows classiﬁcation multidimensional objects small number categories. model composed many neurons organized layers blocks layers piled together hierarchical way. every neuron receives input predeﬁned neurons. every connection parameter corresponds weight connection. function every neuron make transformation received inputs calculated output value. every incoming connection weight multiplied input value received neuron aggregated value feeded activation function calculates output neuron. parameters usually optimized using stochastic gradient descent algorithm minimizes predeﬁned loss function. parameters network updated backpropagating loss function gradients network. hierarchical models able learn multiple levels representation correspond diﬀerent levels abstraction enables representation complex concepts compressed deep learning based models proven eﬀective trained enough labelled data main concern lack interpretability. every successful model tend millions parameters making diﬃcult rationale interpretation. medical diagnosis tasks important accuracy predictions also reasons behind decision. self-explainable models enable physicians contrast information reported model knowledge increasing information probability good diagnostic. diﬀerent attempts done order interpret results reported neural networks. network propagation technique used visualization features input space. used pixel-wise decomposition classiﬁcation decision. decomposition could done ways considering network global function disregarding topology using natural properties decomposition inherent topology function message passing technique propagating back pixel space decomposition. this used named deep taylor decomposition technique replace inherently intractable standard taylor decomposition using multitude simpler analytically tractable taylor decompositions. work similar approach used pixel-wise decomposition taking account compositional nature topology novel approach comes fact score globally conservative conservation hold layers. concept score paper similar concept relevance used layer-wise relevance propagation. apart input-space contribution also another coming every layer independent input-space depends parameters layer. attribute individual pixels back-propagated contribution receptive ﬁeld represents layer individual entity. propagate back part score depends precedent input every layer. model explanation consider constant part property every layer. approach allows exact propagation scores using deconvolutional approach. diﬀering also method allows integration batch normalization typical neural network block constituents score propagation. full score propagation blocks typical deep learning functional constituents derived order make easy possible porting paper results networks applications. interpretation model tested application research area diabetic retinopathy leading disabling chronic disease main causes blindness visual impairment developed countries diabetic patients. studies reported cases prevented early detection treatment. screening retinal images used physicians detect lesions related disease. increasing number diabetic people amount images manually analyzed becoming unaﬀordable. moreover training personnel type image-based diagnosis long requires acquire expertise daily practice. medical community establishes standardized classiﬁcation based four severity stages determined type number lesions present retine class referring apparent retinopathy class mild non-proliferative diabetic retinopathy class moderate npdr class severe npdr class proliferative design interpretable image classiﬁcation model grading level disease. model able report predicted class also score importance every pixel input image ﬁnal classiﬁcation decision. possible determine pixels input image important ﬁnal decision facilitate human experts explanation verify results reported model. section current work deep learning applied brieﬂy introduced then main works interpretation presented. section present complete mathematical formulation interpretable model describing score propagation model section describes classiﬁcation model section present results showing samples type visual interpretations ﬁnally section present ﬁnal conclusions work. many deep learning based classiﬁers published last years. deep learning classiﬁer published prediction diﬀerent disease grades. model trained using public available eyepacs dataset. training images test quadratic weighted kappa evaluation metric test using unique deep learning model without ensembling close reported human experts. deep learning classiﬁer published detection severe cases model trained using extended version eyepacs dataset mentioned before total images improving proper tagging images using experts chosen panel expert ophtalmologists. model surpassed human expert capabilities reaching ﬁnal operating point approximately sensitivity speciﬁcity test sets images detecting worse cases strength model ability predict severe cases sensitivity speciﬁcity greater human experts. drawback many deep learning based models lack interpretability. model acts like intuition machine highly statistical conﬁdence lacking interpretation foundations ﬁnal decisions making diﬃcult experts balance compare prior knowledge reasons behind ﬁnal conclusion even better diagnostics. last years diﬀerent approximations derived convert initial deep learning black classiﬁers interpretable classiﬁers. next sections introduce successful interpretation models existing today sensitivity maps layer-wise relevance propagation taylor type decomposition models. main concern models objective explain giving information local change function. high non-linear functions like deep neural networks local variation pointing nearest local optimum necessarily direction global minimum method next general constraints ﬁrst nature classiﬁcation function decomposable several layers computation second total relevance must preserved layer another relevance layer equals ones layers ﬁnally relevance every node must equal relevance messages incoming node also equal relevance messages outgoing node authors explain constraints assure unique splitting score diﬀerent nodes guarantee ﬁnal score distribution meaningful interpretation classiﬁer prediction. free parameter chosen case deﬁned function reports value greater belongs class lower otherwise. deﬁned express case maximum uncertainty image. finding allows express equation explanation dependent derivative main problem approach ﬁnding valid root close euclidean norm analyzed image approximating function order taylor expansion residuum proportional euclidean distance points. diﬀerent ways ﬁnding proposed. example unsupervised search training looking images reporting near averaging ﬁnding deep taylor decomposition uses approximation combines layerwise taylor type models. compositional nature deep learning models approach supposes also decomposability relevance function presuming existence every node partial relevance function depends activation. considers function unknown applies taylor decomposition root point. summing individual contributions using relevance conservation property deﬁned previous models makes possible propagation intermediate relevance eventually reach input space come heatmap total relevance prediction. section describe contribution explanation models. model based layer-wise relevance propagation model described above. reformulate properties relevance propagation. models previous section based fact relevance conservative layers. formulation consider score entering node combination parts transformed function dependent inputs another constant belongs node. ﬁnal score continues conservative layers. ﬁnal score contribution studied feature-space plus score contributions every following layer. contribution every following layer depends parameters layer output activations. propagated score depends solely individual activation inputs layer. able unique mapping score every output input space network. propagation model proposed makes diﬀerent treatment components hand depends input activation arriving original image propagation backwards separate show activation function node. input activation transformed know λoao substituting λoφ. order proposition true require also λiai. relu family functions max) continues verifying proposition. type activation functions calculating score particular image consider network parameterizable activation functions. particular image consider ﬁrst order taylor expansion activation function linear function forward propagation dimensional convolution image diﬀerent feature activations predeﬁned locality linearly combined output backpropagating score convolutional layer requires divide individual components. every component either positive negative. also bias part comes inherent nature layer attributable inputs must treated also property layer. nature convolution operator every input node contributes calculation diﬀerent outputs that’s every input receives contribution score diﬀerent outputs summed score propagation max-pooling layer straightforward. score propagation value score output copied input selected forward pass average pooling also straightforward. score propagation value score splitted equal parts number inputs fully connected layer linear combination input activities weights. ﬁnal score splitted individual elements leaving apart bias becomes score contribution layer seen every block score constituents dependent inputs easily forwarded another depends layer. point going transport back also values input-space. know eﬀective equal theoretical eﬀective acts like gaussian function points located borders contribute less center ones. using property possible make approximate conversion hidden-space full constant scores input space using gaussian prior. example hidden layer pixels know every points representation value input space. prior information statistical distribution input space pixels possible back. summing gaussian distributions mean equal values hidden space summing coincident points possible distribution input space. ﬁxed approximate distribution scores seems acceptable information gaussian inside normalize function information inside study eyepacs dataset diabetic retinopathy detection competition hosted internet kaggle platform. every patient right left images reported. images classiﬁed ophthalmologists according standard severity scale presented images taken variable conditions diﬀerent cameras illumination conditions resolutions. used hyper-parameter optimization images; class class class class class test used time generalization evaluation contains total images; class class class class class dataset rich well tagged used allows train models near human expertise useful show purposes work good performance results mainly study pixel interpretability conclusions given model. values last linear combinations features. probability calculated let’s call score class ﬁnal value output neuron applying tmax. function required calculating probability every class case interested argmax needn’t evaluate tmax argmax argmax). deep neural network model design know driven mainly experience. nowadays still science lacks systematic designing best architecture solving problem. previous works tested diﬀerent kinds architectures allow previous knowledge kind models work better solving particular classiﬁcation task. using previous experience works summarize guidelines ruled ﬁnal model selection. design principles applicable particular application explained below optimal image resolution image information available fully convolutional neural network small convolutions adapt combination convolution sizes number layers ﬁnal similar possible image size relu activation function batch normalization every layer loss function eﬃcient number features linear classiﬁer last layer. optimal image resolution. hand size input image great importance classiﬁcation results. problem papers like shown better results achieved retine diameters pixels ones obtained pixels. tests done using greater densities pixel/diameter seem improve signiﬁcantly classiﬁcation rates. hand hardware calculation devices limitation available resources. input image size great impact memory calculation time required training test deep neural network models. work tested models pixels retine diameter. dataset diameters greater seem report better results. optimal size used study retine diameter equal pixels. available image information. previous studies published hardware limitations classiﬁcation models designed using limited input information using part available input requiring ensembling solutions combine results evaluating diﬀerent parts retine. input image model used random selection rotated square retine information available used classiﬁcation prediction. test time rotated versions input averaged order better evaluation result. paper network receives input information available requiring ensembling test time. background located diameter removed fully convolutional neural network. convolutional neural networks computationally eﬃcient fully connected ones. cnns ideal exploiting typical high local pixel correlations present images. small size convolutions. stacking small size convolutions eﬃcient usage size convolutions. lower number parameters possible generate nonlinear relationships pixels using unique convolution higher size. following philosophy convolutions used feature layers. adapt convolution sizes number layers similar possible image size. important aspect cnns size. deﬁnes theoretical space covered convolution input space. ideal case last layer equal image size sure information available used. greater image size ineﬃcient that’s sometimes necessary slightly modify convolution sizes layer desired one. figure shows growth model. rectiﬁed linear unit activation function. relu computationally eﬃcient activation function suitable used deep convolutional neural networks. derivatives scores easily calculated. tested activation functions leakyrelu selu reporting similar even worse results introducing complexity model without clear advantage ﬁnal result. batch normalization every layer. batch normalization stabilize training accelerates convergence. problem great diﬀerence using batch normalization point using makes diﬃcult even impossible training. loss function. multi-class classiﬁcation standardized loss function logarithmic loss shown ordinal regression problems multi-class classiﬁcation taking place also possible establish sorting classes based hidden underlying causes qwk-loss also used better results. properties function loss function widely studied diﬀerence performance ﬁnal results high. optimizing directly allows getting better classiﬁcation results. linear classiﬁer last layer. simplicity interpretability model expect model disentangle completely features required classiﬁcation. ﬁnal classiﬁcation required linear combination features last layer. eﬃcient number features. inﬁnitely number resources network. case limited resources would like also able implement result devices resources. tested networks diﬀerent sizes order check redundancy information made principal component analysis feature space last layer arriving conclusion features explain features total variance. studied diﬀerent conﬁgurations using diﬀerent number features values showed reduction performance increased increasing features higher number features improve results. ﬁgure show variance explained ﬁnal feature vector space. model description. model input image obtained minimal preprocessing step external background borders trimmed later resized required input size figure shows block diagram model. parameters divided layers. layers divided groups feature extractor classiﬁer. feature extraction blocks layers. every layer stack convolution stride padding followed batch normalization relu activation function. every block max-pooling operation stride applied. blocks feature extraction network grown till reaching approximately equal input size afterwards classiﬁcation phase takes place using convolution. average-pooling reduces dimensionality ﬁnal feature vector linearly combined obtain output scores every class. soft-max function allows conversion scores probabilities feed values proper cost function optimization process. feature extractor ﬁlters ﬁrst block second other. training images validation used hyperparameter selection notice image highly imbalanced. order facilitate learning training artiﬁcially equalized using data random initialization based kaiming&he approach used networks. models optimized using batch based ﬁrst order optimization algorithm called adam loss function used optimizing model qwk-loss batch size learning rate training network linear classiﬁer formed combination features eyes patient trained. possible increase prediction performance model using information available patient. model trained epochsreaching evaluation metric validation value achieved never seen test using linear classiﬁer combining features eyes ktest reaches expert ophthalmologist report inter-rating agreement values training model multi-class classiﬁcation model facilitates encoding required features distinguishing diﬀerent severity levels disease. training model aggregated detection sure increment accuracy prize missing coding important features separate positive classes case want model learn diﬀerences visualize explanation model that’s case better information available gradation disease order force model encode required features separating intermediate classes prize obviously reducing accuracy correct predictions. back-propagating explanations could scores model gives evaluation diﬀerent classes image allowing expert include expertise ﬁnal decision. figures show aggregated scores every hidden layer ﬁnal output layer. individual feature scores ﬁrst calculated receptive ﬁeld-wise summed mapped input-space done figures show hidden layers. fig. show part score depend exclusively input. score inputs combined constant scores deﬁne unique input score scores equal last layer inference score determines relative importance every pixel ﬁnal decision. density plot standard deviation also calculated. order determine importance pixels possible restrict visualization positive scores also even restrictive visualize pixels score greater predeﬁned threshold example score maps useful building explanations detecting cause non-expected classiﬁcations example pixels excessive importance ﬁnal decision conclusions based partial incorrect information etc. fig. shows three diﬀerent score maps generated images belonging diverse classes. appropriate analysis score maps every class considered diﬀerent threshold maps analyzed. ﬁgure space limitations predicted class shown. future publications study best method extract conclusions generated maps. paper presented model explanation deep learning classiﬁcation models based distribution last layer scores input pixels image. presented general theoretical derivation score calculation typical deep learning building blocks make possible generation score propagation networks type applications based deep learning models. additionally applied model design human expert performance level interpretable classiﬁer. model able classify retine images standardized levels disease severity able also report every class score importance pixel maps providing human expert possibility inference interpretation. score generation done using modiﬁed version pixel-wise relevance propagation algorithm diﬀerence back-propagating part score depends inputs leaving constant part contribution score considered layer. able generate scores unique exact way. additionally developed technique consisting applying d-gaussian prior mapping constant hiddenspace scores input generating unique score representative class making possible distribute score class information last layer.", "year": 2017}