{"title": "Relation Classification via Recurrent Neural Network", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Deep learning has gained much success in sentence-level relation classification. For example, convolutional neural networks (CNN) have delivered competitive performance without much effort on feature engineering as the conventional pattern-based methods. Thus a lot of works have been produced based on CNN structures. However, a key issue that has not been well addressed by the CNN-based method is the lack of capability to learn temporal features, especially long-distance dependency between nominal pairs. In this paper, we propose a simple framework based on recurrent neural networks (RNN) and compare it with CNN-based model. To show the limitation of popular used SemEval-2010 Task 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al., 2014). Experiments on two different datasets strongly indicates that the RNN-based model can deliver better performance on relation classification, and it is particularly capable of learning long-distance relation patterns. This makes it suitable for real-world applications where complicated expressions are often involved.", "text": "deep learning gained much success sentence-level relation classiﬁcation. example convolutional neural networks delivered competitive performance without much effort feature engineering conventional patternbased methods. thus works produced based structures. however issue well addressed cnn-based method lack capability learn temporal features especially long-distance dependency nominal pairs. paper propose simple framework based recurrent neural networks compare cnn-based model. show limitation popular used semeval- task dataset introduce another dataset reﬁned mimlre. experiments different datasets strongly indicates rnn-based model deliver better performance relation classiﬁcation particularly capable learning long-distance relation patterns. makes suitable real-world applications complicated expressions often involved. paper focuses task sentence-level relation classiﬁcation. given sentence contains pair nominals goal task predict relation nominals pre-deﬁned relations tags part speech name entities dependency path often involved. highlevel features require extra modules increase computational cost introduce additional errors. also manually designing patterns always time-consuming coverage. recently deep learning made signiﬁcant progress natural language processing. collobert proposed general framework derives task-oriented features learning text data using convolutional neural networks idea ‘learning scratch’ fundamentally different conventional methods require careful tedious feature engineering. collobert evaluated learning-based approach several tasks including tagging semantic role labelling. without human-designed features obtained close even better performance state-of-the-art systems involve complicated feature engineering. multitude researches proposed apply deep learning methods neural models relation classiﬁcation. representative progress made zeng proposed cnn-based approach deliver quite competitive results without extra knowledge resource modules. following success valuable models multi-window cr-cnn ns-deplcnn proposed recently based structure. though models also based structures like mv-rnn sdp-lstm occupies leading position. eling temporal patterns. though sdp-lstm algorithm utilize recurrent structure dependency parsing analysis shown compare models. note semantic meaning relation formed context target nominals including word sequence window preceding following words. additionally relation fact ‘directional’ means order context words matter. therefore relation learning essentially task temporal sequence learning modelled temporal model. models static models potentially weak especially learning long-distance relation patterns. example model learn local patterns hard deal patterns outside window convolutional ﬁlter. dependency path alleviate problem removing noise compared natural sequence input computation cost adding error caused dependency parser inevitable. limitation still exists dependency path long. paper propose simple framework based recurrent neural networks tackle problem long-distance pattern learning. compared models temporal model particularly good modeling sequential data main framework shown figure described details section veriﬁed advantages recurrent structure used semeval- task dataset also dataset reﬁned miml-re’s annotated data obtained distinct gain compared cnn-based model. mentioned conventional approaches relation classiﬁcation based pattern matching categorized feature-based methods kernel-based methods former category relies human-designed patterns require expert experience time consuming latter category suffers data sparsity. additionally methods rely extra tools derive linguistic features. alleviate difﬁculties pattern design also lack annotated data distant supervision drawn attention since technique combines resources text data knowledge graph uses relations knowledge graph discover patterns automatically text data. work follows line automatic feature learning neural models largely fostered collobert closely related work proposed zeng employed learn patterns relations text data pure feature learning approach. potential problem model learn local patterns suitable learning long-distance patterns relation learning. particularly simply increasing window size convolutional ﬁlters work lose strength cnns modeling local short-distance patterns. tackle problem nguyen grishman proposed model multiple window sizes ﬁlters allows learning patterns different lengths. although method promising involves much computation tuning window sizes trivial. rnn-based approach could solve difﬁculty models learning long-distance variable-distance patterns elegant way. order tackle long-distance dependency patterns works proposed based dependency trees eliminate irrelevant words sentence. early work mvrnn model proposed socher difference based different rnns mv-rnn model based recursive work based recurrent temporal model. recently exploits dependency path learn assignments subjects objects using straightforward negative sampling method adopts shortest dependency path object subject negative sample. recently proposed model based lstm recurrent neural network similar model. however works rely syntactic parsing makes process complicated. sentence becomes longer syntax becomes complex error dependency tree appear thus inﬂuence ﬁnal performance. addition work related principle framework decomposes sentences substructures factorizes semantic meaning contributions multiple annotations regarded general form mv-rnn models recursive hierarchy max-pooling replaced general composition function. nevertheless still static model shares disadvantage modeling temporal data. santos also convolutional network. propose ranking-based cost function elaborately diminish impact class. compared studies signiﬁcant difference model predicting targets time step supervision available sequence. similar semantic embedding model proposed palangi though made several important modiﬁcations presented next section. shown figure model proposed paper contains three components word embedding layer maps word sentence dimension word vector; bidirectional recurrent layer models word sequence produces word-level features pooling layer merges word-level features time step sentence-level feature vector selecting maximum value among word-level features dimension. sentence-level feature vector ﬁnally used relation classiﬁcation. components presented detail section. word embedding layer ﬁrst component proposed model projects discrete word symbols low-dimensional dense word vectors words modeled processed following layers. denote one-hot representation t-th word r|d|×|v projection matrix. since one-hot fact stores representations words word embedding widely studied context semantic learning. work ﬁrst train word vectors using wordvec tool large amount data general domains vectors initialize word embedding layer model. knowledge general domains used. shown pre-training improves model training e.g. bi-directional network second component model recurrent layer part modeling sequential data long-distance patterns. start simple one-directional forward rnn. given sentence words projected sequence word vectors denoted number words. word vectors recurrent layer step step. step network accepts word vector output previous step input produces current output linear transform followed non-linear activation function given output t-th step regarded local segment-level features produced word segment note dimension feature vector rm×d rm×m model parameters. used hyperbolic function tanh non-linear transform help back propagate error easily symmetry solution bi-directional architecture makes predictions based past future words seen figure. architecture demonstrated work well sequential labeling e.g. bi-directional architecture prediction step obtained simply adding output forward backward formulated follows rm×d rm×m parameters backward rnn. note forward backward rnns trained simultaneously addition possible even without parameter sharing structures. max-pooling sentence-level relation classiﬁcation requires single sentence-level feature vector represent entire sentence. cnn-based models pooling approach often used structure since semantic meaning sentence learned word word segment-level feature vector produced sentence actually represents entire sentence. accumulation approach used sentence-level semantic embedding. practice found accumulation approach suitable relation learning many long-distance patterns training data. accumulation recurrent connections tends forget long-term information quickly supervision sentence hard propagated early steps model training annoying problem gradient vanishing max-pooling approach models. argument segment-level features although strong representing entire sentence represent local patterns well. semantic meaning parameters around linear region activation function helps propagating gradients back early steps easier. moreover also balances learning speed parameters different layers discussed pre-training word embedding layer word vectors trained extra large amount corpus improves performance. approach employed experiments. position indicators relation learning essential algorithm know target nominals. cnn-based approach zeng appended position feature vector word vector i.e. distance word nominals. found highly important gain high classiﬁcation accuracy works followed technique. since model learns entire word sequence relative positional information word obtained automatically forward backward recursive propagation. therefore sufﬁcient annotate target nominals word sequence without necessity change input vectors. choose simple method uses four position indicators specify starting ending nominals. following example people </e> moving back downtown </e>. note people downtown nominals relation ‘entity-destination’ </e> </e> four position indicators regarded single words training testing process. positionembedded sentences used input train model. compared position feature approach model position indictor method straightforward. section experiment results show circumstances helpful database different datasets. ﬁrst dataset provided semeval- task directional relations additional ‘other’ relation resulting relation classes total. note chosen max-pooling rather mean-pooling. hypothesis several words associated patterns important relation classiﬁcation max-pooling appropriate promote informative patterns. model training training model figure involves optimizing parameters {win bbw}. training objective that given sentence output feature vector achieves best performance task relation classiﬁcation. simple logistic regression model classﬁer. formally model predicts posterior probability input sentence involves relationship follows train model follow training method proposed collobert utilizes stochastic gradient descent algorithm. back propagation time employed compute gradients layer layer fan-in technique proposed plaut hinton used initialize parameters. found initialization locate model number training data number development data number test data number relation types peralternate names perorigin perspouse pertitle peremployee percountries residence perstateorprovinces residence percities residence percountry birth relation given sentence target nominals prediction counted correct relation direction correct. performance evaluated terms score deﬁned semeval- task data evaluation tool publicly available. second dataset revision miml-re annotation dataset provided gabor angeli ofﬁcial document collections well july dump wikipedia text corpus annotation. sentences annotated. make dataset suitable task made several reﬁnement first direction relation names ‘peremployee splited ‘peremployee ‘peremployee relation’. according description task replace ‘orgparents’ replace ‘orgmember ‘orgmember’ leads relations dataset. that dataset randomly shufﬂed sentences relation split three groups training development test. finally remove sentences development test whose entity pairs relation appeared training sentence simultaneously. rest paper call second dataset sake simplicity. statistics relation types shown table contains directional relations additional relation’ relation resulting relation classes. notice different semeval- task several aspects experimental setup order compare work socher zeng word vectors proposed turian initialize embedding layer main experiments. additionally compare recent models additional experiments also conducted word vectors pretrained mikolov dimensional. ofﬁcial development dataset semeval- task dataset tune hyper-parameters -fold cross validation. hyper-parameters optimized training data used train model best conﬁguration. turian’s -dimensional word vectors best dimension feature vector mikolov’s dimensional word vectors best feature dimension learning rate conditions. fast convergence learning rate ﬁrst iterations. iteration time since split development convenient tune hyperparameters. test process development data also used choose best model among different iterations. turian’s -dimensional word vectors. best dimension feature vector iteration time table presents results proposed model contribution offered modiﬁcation. seen basic signal directional output last step sentence-level features performs poor. attributed lack position information target nominals difﬁculty training. max-pooling table comparing scores different neural models different position usage different datasets. -dimensional word vectors provided turian used pretraining. stands position features stands position indicators. number parentheses shows best dimension hidden layer. offers signiﬁcant performance improvement indicating local patterns learned neighbouring words highly important relation classiﬁcation. position indicators also produce highly signiﬁcant improvement surprising model would puzzled pattern learn without positional information. contribution positional information demonstrated zeng positional features lead nearly percentiles improvement similar gain obtained model. second experiment compares three representative neural models different datasets different position information added. results presented table -dimensional word vectors employed table. experiments though rather close didn’t reproduce value reported zeng third fourth column shows results cnn-based model. compared among different rows come conclusion model outperforms mvrnn model proposed model proposed compared among different columns results show model obtains improvement comparing dataset also indicates difference table draw another conclusion that recurrent structure effective contributes even experiments implemented dataset. former phenomenon caused ambiguous accumulation recurrent process disrupt information. latter probably attributes shortcoming tends less accurate nouns become phrases instead single words. discussion held section model performs better mv-rnn model uses syntactic parse extra resources. indicates relation patterns effectively learned rnns text without explicit linguistic knowledge. argued particular advantage model compared model deal long-distance patterns effectively. verify argument split test datasets subsets according length context. context deﬁned words between nominals plus words prior ﬁrst nominal words second nominal exist. position indicator count. clearly long contexts lead long-distance patterns. order compare performance models produce cnnbased method modiﬁcation ensures models learn input sequence representation. results subsets reported figure seen context length small models perform similar whereas context length large model clearly superior. conﬁrms suitable learn long-distance patterns. results also shows used clear trend means suitable models suitable models. since performs better kbp. attributed small proportion long contexts test data. limitation semeval- dataset distribution context lengths calculated test dataset. comparison york time corpus entities relations selected subset freebase recommended riedel also presented. statistics shown table observed long contexts exist three datasets. particularly proportion long contexts semeval- task dataset rather small compared datasets. suggests strengths different models fully demonstrated implementing experiments semeval- task dataset. since recent works relation classiﬁcation implemented single dataset comparison among different models dataset needed. semantic accumulation another interesting analysis show ‘semantic meaning’ sentence formed. first notice models sentence-level features produced local features dimension-wise max-pooling. figure semantic distribution words sentence skype free software allows hookup </e> multiple computer users </e> join online conference call without incurring telephone costs. measure contribution particular word segment sentence-level semantic meaning sentence count number dimensions local feature word step contributes output max-pooling. number divided number total dimensions feature vector resulting ‘semantic contribution’ word sequence. figure figure show examples semantic contributions. ﬁgure results models presented. highlight temporal model accumulates semantic meanings word word peak using’ actually contribution words ‘witch’. contrast model learns local patterns therefore splits semantic meaning separate word segments. similar observation obtained second example shown figure again model accumulates semantic meaning sentence word word model learn local patterns merge together. interesting observation rnnbased semantic distribution tends smoother produced model. fact calculated average variance semantic contribution neighbouring words sentences semeval- task dataset found variance model number model. smoother semantic distribution certainly temporal nature model. paper proposed simple rnn-based approach relation classiﬁcation. compared deep learning models model deal long-distance patterns particular suitable learning relations within long context. several important modiﬁcations proposed improve basic model including max-pooling feature aggregation position indicator approach specify target nominals bi-directional architecture learn forward backward contexts.", "year": 2015}