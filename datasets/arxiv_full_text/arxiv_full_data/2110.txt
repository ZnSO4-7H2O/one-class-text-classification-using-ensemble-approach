{"title": "Projective simulation with generalization", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "The ability to generalize is an important feature of any intelligent agent. Not only because it may allow the agent to cope with large amounts of data, but also because in some environments, an agent with no generalization capabilities cannot learn. In this work we outline several criteria for generalization, and present a dynamic and autonomous machinery that enables projective simulation agents to meaningfully generalize. Projective simulation, a novel, physical approach to artificial intelligence, was recently shown to perform well in standard reinforcement learning problems, with applications in advanced robotics as well as quantum experiments. Both the basic projective simulation model and the presented generalization machinery are based on very simple principles. This allows us to provide a full analytical analysis of the agent's performance and to illustrate the benefit the agent gains by generalizing. Specifically, we show that already in basic (but extreme) environments, learning without generalization may be impossible, and demonstrate how the presented generalization machinery enables the projective simulation agent to learn.", "text": "ability generalize important feature intelligent agent. allow agent cope large amounts data also environments agent generalization capabilities cannot learn. work outline several criteria generalization present dynamic autonomous machinery enables projective simulation agents meaningfully generalize. projective simulation novel physical approach artiﬁcial intelligence recently shown perform well standard reinforcement learning problems applications advanced robotics well quantum experiments. basic projective simulation model presented generalization machinery based simple principles. allows provide full analytical analysis agent’s performance illustrate beneﬁt agent gains generalizing. speciﬁcally show already basic environments learning without generalization impossible demonstrate presented generalization machinery enables projective simulation agent learn. ability upon stimulus based previous experience similar distinct stimuli sometimes denoted generalization used extensively daily life. simple example consider driver’s response traﬃc lights driver need recognize details particular traﬃc light order respond correctly even though traﬃc lights appear different another. property matters color whereas neither shape size play role driver’s reaction. learning react traﬃc lights thus involves aspect generalization. learning agent capable meaningful useful generalization expected following characteristics ability categorization ability classify ideally generalizations relevant success agent learned correct actions associated relevant generalized properties generalization mechanism ﬂexible. illustrate mean ﬂexible generalization back driver. learning handle traﬃc lights correctly driver tries follow arrow signs nearby airport. clearly shape category signal guide driver rather color category. situation would even confusing traﬃc signalization would suddenly based shape category alone square lights mean stop whereas circle lights mean drive. adapt environmental changes driver give color-based generalization build shape-based generalization. generalizations must therefore ﬂexible. refs. provide broad account generalization artiﬁcial intelligence. reinforcement learning agent learns interaction rewarding environment generalization often used technique reduce size percept space potentially large. useful recent summaries found refs. example q-learning sarsa algorithms common function approximation methods realized e.g. tile coding neural networks decision trees constructive function approximation support vector machines implement generalization mechanism. alternatively learning classiﬁer systems generalization facilitated using wildcard character which roughly speaking means particular category irrelevant present task environment relates recent works state abstraction maps diﬀerent states agent react similarly single abstract temporal abstraction grouping actions addressed known hierarchical-rl methods usually applied problems states and/or actions factored categories modern approaches generalization include relational-rl feature-rl work contribute development recently introduced model projective simulation exhibiting approach generalization within framework. physical approach intelligence based stochastic artiﬁcial processing experience. uses particular type memory denoted episodic compositional memory structured directed weighted network clips clip represents remembered percept action sequences thereof. percept observed network activated invoking random walk clips action clip couples real action agent. generalization process within achieved work roughly speaking generation abstracted clips represent commonalities percepts precisely subsets percept space. these turn inﬂuence behavior situations based similar previous experiences. method introduce step toward advanced approaches generalization within suitable medium-scale task environments. complicated environments large number possible abstractions harm performance agent. address question could combated discussion section. approach artiﬁcial intelligence arguably stands promising diﬀerent perspectives first random walks constitute basic internal dynamics model well-studied context randomized algorithm theory probability theory thus providing extensive theoretical toolbox analyzing related models; second model design represents stochastic physical system points possible physical realizations. relates framework embodied artiﬁcial agents last physics aspects model oﬀer route toward research quantumenhanced variants underlying random walk already shown naturally extend quantum many-body master equation related this fact deliberation agent centers around random walk process opens route advances using quantum random walks instead. quantum random walks roughly speaking probabilistic repositioning walker replaced quantum superposition moves switching stochastic coherent quantum dynamics. allows exploit quintessential quantum phenomena including quantum interference quantum parallelism. quantum walks increasingly employed recent times framework development quantum algorithms. course last decade polynomial exponential improvements computational complexity reported classical counterparts utilizing methodology context reinforcement learning recently shown authors collaborators quantum variant agent exhibits quadratic speed-up deliberation time classical analogue leads similar speed-up learning time active learning scenarios quantum advantage decisionmaking process quantum agent recently experimentally demonstrated using small-scale quantum information processor based trapped ions model learning realized internal modiﬁcation clip network terms structure weights edges. interactions rewarding environment clip network adjusts dynamically increase probability performing better subsequent time steps learning thus based trial error approach making model especially suitable solving tasks. indeed recent studies showed agent perform well comparison standard models basic problems standard benchmark tasks grid world continuous-domain mountain problem ﬂexibility framework also used contexts beyond textbook recent applications instance problem learning complex haptic manipulation skills problem learning design complex quantum experiments present simple dynamical mechanism allows network evolve experience network represents exploits similarities perceived percepts i.e. network generalize. using network address problem task environments require aspects generalization solved. standard approaches task environments rely external machinery function approximator combined otherwise tabular machinery e.g. temporal diﬀerence learning model-free methods q-learning sarsa. contrast achieve goal elaborate structure model represented table directed graph rather external machinery e.g. case q-learning naturally proposed machinery also internally realizes function approximator structure updates arise basic learning rules model. generalization mechanism inspired wildcard notion based process abstraction systematic autonomous importantly requires explicit prior knowledge agent. contrast common models function approximator often require additional prior knowledge terms additional input moreover show agent provided machinery allows categorize classify rest expected characteristics listed follow directly basic learning rules agent. particular show relevant generalizations learned agent associates correct actions generalized properties entire generalization scheme ﬂexible required. generalization comparison solutions rely external machinery. instead homogeneous approach generalization mechanism based basic principles case speciﬁcally dynamic growth clip network. several advantages approach first allows relatively straightforward theoretical treatment performance including analytic formulas characterizing performance generalization learning. second rely powerful classifying machinery signiﬁcantly increase model complexity agent undesirable. finally agenda relevant sticking basic random walk mechanism oﬀers natural systematic route quantization overall dynamics. mentioned earlier lead improvements computational complexity principle also space complexity model. contrast heterogeneous approaches e.g. q-learning combined neural network exist clear routes useful quantization results proving improvements established quantization either alone combination. literature elements generalization considered means tackling curse dimensionality coined bellman discussed above also strictly necessary agent learn certain environments consider type environment where irrespective available resources agent generalization ability canlearn i.e. performs better fully random agent. following this show model enhanced generalization mechanism capable learning environment. along numerical illustrations provide detailed analytical description agent’s performance respect successlearning-rates analysis feasible simplicity model terms number free parameters underlying equations property extensively exploit. main contribution paper thus demonstrate inherent features model used solve problems require nontrivial notions generalization importantly without relying external classiﬁer machinery. also possible sacriﬁce homogeneity combine external machinery work strive develop theory model own. remainder paper structured follows. ﬁrst begin completeness description model formal comparison model standard techniques. present proposed generalization mechanism examine performance simple case illustrate gives rise meaningful generalization deﬁned above. next study central scenario paper generalization absolute condition learning. describing scenario showing agent cope analyze performance analytically. finally study scenario arbitrary number categories observe categorize beneﬁcial proposed mechanism. central component agent so-called clip network abstractly represented directed graph node clip directed edges represent allowed transitions depicted fig. whenever agent perceives input corresponding percept clip excited excitation marks beginning random walk clips action clip corresponding action performed. random walk carried according time-dependent probabilities node another. clarify glow edges whose transition result obtaining reward still obtain fraction later issued reward proportional current glow value. latter corresponds past relative later rewarded time-step particular edge used. glow mechanism thus allows future reward propagate back previously used edges enables agent perform well also settings reward delayed and/or contingent immediate history agent-environment interaction n-ship game presented ref. described learning mechanisms speciﬁed fully deﬁne basic agent ﬁxed learning parameters values learning parameters meta-parameters properly agent performs optimally certain task. however shown ref. model naturally extended account self-monitoring meta-parameters agent reach near-optimal optimal success rates diﬀerent kinds reinforcement learning problems. work present extensions make capacity model generate clips dynamically purpose generalization. focus study performance agent introduce simplest environmental scenarios highlight critical aspects generalization analyze generalization performs. simple settings contextual bandit framework hence require glow mechanism course readily applied complex task environments analogous generalization results hold. next section brieﬂy reﬂect relation basic model standard reinforcement learning machinery work context. basic model viewed explicitly reward-driven model. unlike standard models include explicit approximation state-value action-value function. consequence this also simpler sense value function policy optimized separately rather optimization occurs concurrently. despite structural diﬀerences model related standard approaches. quantitively model shown perform similar beginning h-values initialized ﬁxed value ensures that initially probability clip neighbors completely uniform. conditional probabilities deﬁned used throughout paper unless stated otherwise. also deﬁne diﬀerent function conditional probabilities known softmax function learning takes place dynamical strengthening weakening internal h-values correspondence external feedback i.e. reward coming environment. speciﬁcally update h-values done according following update rule reward non-negative added h-values edges traversed last random walk. update rule also handle negative rewards given probability function deﬁned weakens h-values edges allows agent forget previous experience important feature changing environments damping term nonzero however needed stationary environments contextual bandit task contextual bandit task agent update rule realizes optimal policy limit. note although update rule explicit tunable learning rate eﬀective separation diﬀerent h-values tunable changing inverse temperature parameter probability function. comparison standard models benchmark tasks addition quantitative relationship formal relationship h-value matrix action-value q-matrix derived formal level many fundamental properties algorithms qualitatively recovered well. instance setting stationary environments immediate rewards update rule basic two-layered model given percept action reads number realized transitions history converge value transition. moreover given policy-generating rule given holds probability outputting action rewarded transition converges unity. equivalent employing greedy policy converged actionvalue function standard hence also implies basic handles stationary contextual bandit probrepresent action values actually stem descriptions physical dynamics going steps further setting general environments contrast basic standard sarsa q-matrix update latter expression hand h−value plays active role step transition clip required. interval edge traversed last clip encountered again edge accumulates discounted rewards. time step k=t+t−kλ closely related γrlq ﬁrst term captures discounted future rewards experienced particular agent future realized steps starting next step. term γrlq captures current approximation future rewards also starting next step discounted γrl. words sarsa plays functional role g−value decay rate clarify note γrlq corresponds k=t+t−kλ term computed averaged agent averaged sequences subsequent moves given agent’s policy. note however case edge traversed many times course learning leading eﬀective averaging future-reward term note heuristic analysis certainly constitute formal statement relationship basic reinforcement learning models. full analysis goes beyond scope paper already heuristic suggests expected performance diﬀer qualitatively models. conﬁrmed empirically benchmarking models occasions outperformed q-learning dyna-planning underperformed global trends comparable. discussed similarities basic model standard algorithms turn attention diﬀerences topic paper. unlike mentioned models learning revolves around estimation value functions learning embodied re-conﬁguration clip network. includes update transition probabilities also dynamical network restructuring applied generalization). latter analog standard approaches discussed previously makes sense since clip network manifestly representation value functions conceptually diﬀerent object. work explore capacity model showing utilized handle problems generalization. possibilities related action ﬁne-graining previously studied refs. here approach diverges standard methodology which knowledge without exception tackles problem using external machinery like classiﬁers generally function approximators. reiterate additional machinery could also used comes cost elaborated previously. next section focus simple tasks require generalization resolved using dynamic clip generation. enhanced network built driver scenario ﬁrst four time steps. following sequence signals shown left-green right-green right-red left-red four percept clips ﬁrst connected layers wildcard clips generalization usually applicable perceptual input composed single category. framework model translates case percept space. particular stimuli similar i.e. share common features precisely values categories useful process similar way. enhance model simple eﬀective generalization mechanism based idea. feature mechanism dynamical creation class abstracted clips call wildcard clips. whenever agent encounters stimulus corresponding percept clip created compared pairwise existing clips. pair clips whose layer particular percept. perceptwildcard-clip direct edges action clips matching higher-level wildcard clips. matching higher-level wildcard clips mean wildcard clips wildcard symbols whose explicit category values match lower-lever wildcard clip. essence matching higher-level wildcard clip generalizes lower-level wildcard clip green color left right direction. time step agent thus perceives four possible combinations colors arrows randomly chosen environment chooses possible actions. setup basic agent described previous section would two-layered network clips composed four percept clips action clips shown fig. would associate correct action four percepts separately. generalization hand much richer playground addition connect percept clips intermediate wildcard clips associate wildcard clips action clips elaborate below. development enhanced network shown step step fig. ﬁrst four time steps driver scenario left-green signal perceived time corresponding percept clip initial weight second time step right-green signal shown. time addition creation corresponding percept clip wildcard clip also created placed ﬁrst layer network. simplify visualization draw wildcard clip green circle direction general omit symbol ﬁgures. newly created edges shown fig. solid lines whereas previously created edges shown dashed lines. next time right-red signal presented. leads figure enhanced network conﬁgurations built four phases driver scenario indirect strong edges shown. diﬀerent wildcard clips allow network realize diﬀerent generalizations. categorization classiﬁcation realized structure network whereas relevance correctness ﬂexibility come update rule mechanism described realizes construction ﬁrst characteristics meaningful generalization categorization classiﬁcation. particular categorization ability recognize common properties achieved composing wildcard clips according similarities coming input. example natural think wildcard clip representing common property redness. spirit could interpret full wildcard clip representing general perceptual input. likewise classiﬁcation ability relate stimulus group similar past stimuli fulﬁlled connecting lower-level wildcard clips matching higher lever wildcard clips described note classiﬁcation done therefore level percept clips also level wildcard clips. categorization classiﬁcation realized structure clip network remaining list requirements namely relevant correct ﬂexible generalization fulﬁlled update h-values. illustrate simple domain next confront agent four diﬀerent environmental scenarios other. scenario lasts time steps following sudden change rewarding scheme agent adapt. diﬀerent scenarios listed below figure average reward obtained agent generalization simulated four diﬀerent phases driver scenario beginning phase agent adapt rules environment. average reward drops revives again thereby exhibiting mechanism’s correctness ﬂexibility. damping parameter used average taken agents. figure sketches four diﬀerent network conﬁgurations typically develop phases. strong edges relative large h-values depicted ignore direct edges percepts actions clarity explained later. stage diﬀerent conﬁguration develops demonstrating relevant wildcard clips play important role strong connections figure basic enhanced networks built neverending-color scenario. percept clip ﬁrst independently connected action clips second row. perceptwildcardclip connected higher-level matching wildcard clips action clips. clarity one-level edges wildcard clips solid edges semitransparent. thickness edges reﬂect weights. action clips. moreover wildcard clips connected strongly correct action clips. relevant correct edges built update rule strengthens edges that traversed lead rewarded action. finally presented ﬂexibility network’s conﬁguration reﬂects ﬂexible generalization ability existence possible wildcard clips network; update rule allows network non-zero damping parameter adapt fast changes environment. note fig. displays idealized network conﬁgurations. practice strong edges exist e.g. direct edges percepts actions rewarded well. next section address alternative conﬁgurations analyze inﬂuence agent’s success rate. figure shows agent’s performance average reward obtained agent driver scenario function time averaged agents. reward given correct actions damping parameter used. model trade adaptation time maximally achievable average reward. high damping parameter leads faster relearning lower averaged asymptotic success rates also ref. chose allow network adapt within time steps. shown average agent managed adapt phases imposed environment learn correct actions. also asymptotic performance agent slightly better last phase correct action independent input. understand this note that relevant edge rewarded time step thus less aﬀected non-zero damping parameter; wildcard clip necessarily leads correct action. observation indicates agent’s performance improves stimuli generalized greater extent. encounter feature next section analytically veriﬁed. driver’s scenario given explain demonstrate underlying principles proposed generalization mechanism within problem course solved basic alone well sometimes helpful plain necessity agent mechanism generalization otherwise chance learn. consider example situation agent perceives stimulus every time step. option have trying similarities among stimuli upon act? section consider scenario analyze detail. speciﬁcally environment presents diﬀerent arrows time step background color diﬀerent. agent move corresponding directions environment rewards agent whenever follows direction arrow irrespective color. call neverending-color scenario. scenario basic agent two-layered clip network structure presented fig. time step percept clip created random walk leads single transition possible action clips agent perform. problem even agent takes correct direction rewarded edge never take part later time steps symbol shown twice. basic agent thus option choose action random correct probability even inﬁnitely many time steps. contrast basic generalization show learning behavior. full network shown fig. percept clips wildcard clips connected matching wildcard clips actions. note wildcard clips never created color seen twice. subnetwork corresponding left-arrow shown. weight edge wildcard clip correct action clip goes time. hopping clip leads rewarded action certainty. otherwise marked ﬁgure dashed blue line. seen cases asymptotic performance achieved already tens hundreds time steps simulations carried agents zero damping parameter since asymptotic performance independent reward ease following analytical analysis chose high value setting smaller reward would amount slower learning curve qualitative diﬀerence fig. reward fig. whenever reward normalize average reward obtained agent order compare probability obtaining maximum reward given therefore shown generalization mechanism leads clear qualitative advantage scenario without agent learn whereas can. quantitative diﬀerence fig. indicate performance large. nonetheless ampliﬁed. idea network conﬁguration probability take correct action larger probability take action success probability ampliﬁed unity majority voting i.e. performing random walk several times choosing action clip occurs frequently. ampliﬁcation rapidly increases agent’s performance whenever fully random agent negligible given full wildcard clip used. fig. show simulations agent majority voting. results demonstrate convergence optimal performance regardless value reward. presented performance model neverending-color scenario principle contrasted standard models. however clearly reasonable comparison because standard models e.g. sarsa q-learning probability agent generalization better take closer look clips. clips will eventually strong edges correct action clip. fact case zero damping consider here h-values edges tend inﬁnity time implying clip probability correct action clip becomes unity. illustrated left-arrow case fig. time step agent confronted certain colored arrow. corresponding percept clip created random walk network begins. determine exact asymptotic performance agent generalization consider possibilities either wildcard corresponding clip not. ﬁrst case occurs probability excitation correct action unit probability agent rewarded. second case action preferred others correct action reached probability possible edge full wildcard clip action clip previously rewarded averaging agents still averaged success probability overall performance agent generalization thus given fig. average performance agent generalization obtained numerical simulation plotted function time solid curves several values initially average performance i.e. completely random grows indicating agent begin learn respond correctly reaches asymptotic value given fig. probability environment presents arrow probability percept clip wildcard clip probability wildcard clip action clip finally take account fact that time analytical approximation time-dependent performance given plotted fig. dotted black shown match simulated curves well diﬀerence detail fig. beginning caused assumption wildcard clips present network beginning whereas real agent needs several time steps create them thus reducing initial success probability. nonetheless certain number time-steps simulated agent starts outperforming prediction given analytic approximation agent rewarded transition wildcard clip full wildcard clip leading higher success probability. note despite described diﬀerence early time steps shown fig. curves converge value fig. cannot learn anything task environment without external classifying machinery side capacity combination basic models classiﬁer machinery beat performance model. again arguably instructive comparison combine external classiﬁer machinery. latter could principle done solutions come price loss homogeneity increase model complexity avoid work reasons elaborated earlier. analyze learning curves predict agent’s behavior arbitrary take following simplifying assumptions first assume possible wildcard clips present network beginning; second technical reasons assume edges partial wildcard clips full wildcard clip never rewarded; account following arguments note time arrow shown possible network conﬁgurations conceptually diﬀerent either edge clip action clip already rewarded inﬁnite weight not. note edge must eventually rewarded ﬁnite promised. plearn probability figure learning curves agent generalization augmented majority voting neverending-color scenario actions. curves obtained averaging agents majority voting consists random walks. order achieve shorter learning times creation -clip suppressed simulation. high reward used. reward used. lead correct action clip unit probability illustrated fig. hand choosing clip results correct action averaged probability accordingly either random walk wildcard clip arrow not. averaged asymptotic performance categories actions hence written lustrated fig. means although categories provided environment irrelevant generalization machinery exploit make larger number relevant generalizations thereby increase agent’s performance. moreover large next generalize neverending-color task case arbitrary number categories color category take inﬁnite values whereas category take ﬁnite values number possible actions given before category important namely arrow direction agent rewarded following irrespective input. irrelevant categories environment thus overloads agent unnecessary information would aﬀect agent’s performance? also conceptual sense. particular enhanced network allows emergence clips represent abstractions abstract properties like redness property rather merely remembered percepts actions. moreover enhanced network multilayered allows involved dynamics random walk which shown gives rise sophisticated behavior agent. although clip network evolve complicated structures before overall model preserves inherent simplicity enables analytical characterization performance. approach assumes percept space underlying cartesian product structure established categories. natural assumption many settings instance categories stem distinct sensory devices case embodied agents. generalization procedure produce functional generalizations however also vital reward function environment indeed reﬂect structure. implies status categories matters categories matter. nonetheless settings underlying similarity structure percept space follow cartesian feature axes agent still learn provided percept space inﬁnite. case improvements come generalization procedure. restricted provided notion generalization still captures exponentially large collection subsets percept space number categories. regarding computational complexity model identify main contributions. ﬁrst deliberation time model number transitions within occur time-step interactions. eﬃcient number steps upper bounded number layers scales linearly number categories. second contribution stems updates similarly note none categories relevant i.e. environment agent expected take action irrespective stimulus receives agent performs even better ario every wildcard clip eventually connects rewarded action. accordingly since percept clip leads wildcard clips high probability correct action clip likely reached. fact case environment confronts agent stimulus every time step agent chance coping unless presented stimuli common features agent grasp. recognition common features i.e. categorization classifying stimuli accordingly ﬁrst steps toward meaningful generalization characterized beginning paper. presented simple dynamical machinery enables model realize abilities showed latter requirements meaningful generalization relevant generalizations learned correct actions associated relevant properties generalization mechanism ﬂexible follow naturally model itself. numerical analytical analysis also arbitrary number categories showed agent learn even extreme scenarios percept presented once. network number wildcard clips generated also immediate impact expected learning time agent. principle total number wildcard clips grow exponentially number categories note necessity exponentially many subspaces percept space agent must explore relevant generalizations. conﬁning space smaller subspace would necessarily restrict generality generalization procedure. thus unavoidable trade-oﬀ general generalization procedure versus large generalization space balance. approach generalization presented built upon capacity model dynamically generate novel clips restriction mentioned could mitigated employing external classiﬁcation machinery. latter approach norm approaches clariﬁed earlier comes types issues avoided external machinery increases complexity model makes inhomogeneous. contrast homogeneity approach allows high interpretability results including analytic analyses advantages respect embodied implementations. moreover approach clear route quantization model become increasingly relevant quantum technologies further develop. thus great interest develop method deal issue combinatorial space complexity maintaining homogeneity. particular clip-deletion mechanism used deal combinatorial growth number clips. mechanism deletes clips thereby maintaining stable population controlled size prioritizing deletion less used hence less useful clips. size population constitutes sparsity parameter model makes combinatorial explosion clip number controllable still allowing agent explore complete space axes-speciﬁed generalizations. proof principle approach given ref. detailed analysis deletion mechanism ongoing work. wish thank markus tiersch browne elham kasheﬁ helpful discussions. work supported part austrian science fund grant foqus templeton world charity foundation grant twcf/ab. wiering otterlo reinforcement learning state-of-the-art vol. adaptation learning optimization otterlo logic adaptive behavior knowledge representation algorithms markov decision process framework ﬁrst-order domains. ph.d. thesis univ. twente enschede netherlands ponsen taylor tuyls abstraction generalization reinforcement learning summary framework. taylor tuyls adaptive learning agents vol. lecture notes computer science chap. pyeatt howe decision tree function approximation reinforcement learning. proc. int. symposium adaptive systems evolutionary computation probabilistic graphical models ernst geurts wehenkel tree-based batch mode reinforcement learning. mach. learn. res. utgoﬀ precup constructive function apliu motoda feature proximation. extraction construction selection vol. springer international series engineering computer science holland escaping brittleness possibilities general-purpose learning algorithms applied parallel rule-based systems. michalski carbonell mitchell machine learning artiﬁcial intelligence approach vol. urbanowicz moore learning classiﬁer systems complete introduction review roadmap. journal artiﬁcial evolution applications tadepalli givan driessens relational reinforcement learning overview. proc. int. conf. mach. learn. workshop relational reinforcement learning nguyen sunehag hutter feature reinforcement learning practice. sanner hutter recent advances reinforcement learning vol. lecture notes computer science kempe discrete quantum walks exponentially faster. probab. theory relat. field krovi magniez ozols roland quantum walks marked element graph. algorithmica", "year": 2015}