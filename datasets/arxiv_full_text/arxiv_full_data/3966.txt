{"title": "A Step Forward in Studying the Compact Genetic Algorithm", "tag": ["cs.NE", "cs.AI"], "abstract": "The compact Genetic Algorithm (cGA) is an Estimation of Distribution Algorithm that generates offspring population according to the estimated probabilistic model of the parent population instead of using traditional recombination and mutation operators. The cGA only needs a small amount of memory; therefore, it may be quite useful in memory-constrained applications. This paper introduces a theoretical framework for studying the cGA from the convergence point of view in which, we model the cGA by a Markov process and approximate its behavior using an Ordinary Differential Equation (ODE). Then, we prove that the corresponding ODE converges to local optima and stays there. Consequently, we conclude that the cGA will converge to the local optima of the function to be optimized.", "text": "compact genetic algorithm estimation distribution algorithm generates offspring population according estimated probabilistic model parent population instead using traditional recombination mutation operators. needs small amount memory; therefore quite useful memory-constrained applications. paper introduces theoretical framework studying convergence point view which model markov process approximate behavior using ordinary differential equation then prove corresponding converges local optima stays there. consequently conclude converge local optima function optimized. famous optimization procedures combinatorial optimization genetic algorithm maintaining population solutions viewed implicitly modeling solutions seen search process. standard solutions generated applying randomized recombination operators high-quality individuals current population recombination operators one-point two-point uniform crossover randomly select non-overlapping subsets parent solutions form children solutions. using crossover operator preserves groups parameters parents children attempts capture dependencies parameters implicitly. poor behavior genetic algorithms problems sometimes attributed designed operators development types algorithms. probabilistic model building genetic algorithms estimation distribution algorithms class algorithms developed recently preserve building blocks principal concept technique prevent disruption partial solutions contained solution building probabilistic model. edas classiﬁed three classes based interdependencies variables solutions dependencies model bivariate dependencies model multiple dependencies model. name instances algorithms include population-based incremental learning bit-based simulated crossover univariate marginal distribution algorithm compact genetic algorithm learning automata based estimation distribution algorithm dependencies model mutual information maximization input clustering combining optimizer mutual information trees bivariate dependencies model ﬁnally factorized distribution algorithm bayesian optimization algorithm multiple dependencies model. researchers studied working mechanism edas. behavior umda pbil studied discuss convergence separable additively decomposable functions. zhang mhlenbein proven class edas inﬁnite population size globally converges. carried study time complexity edas inﬁnite population size. although algorithms dependencies model efﬁciency solving difﬁcult problems still important study simplicity terms memory usage computational complexity respect fact computational complexity bivariate dependencies model multiple dependencies model high. simplest algorithms dependencies model compact genetic algorithm algorithm initializes probability vector component follows bernoulli distribution parameter solutions randomly generated using generated solutions ranked based ﬁtness values. then updated based solutions. process adaptation continues converges. represents population solutions operationally mimics order-one behavior simple uniform crossover. confronted easy problems achieves performance terms number ﬁtness evaluations variations introduced utilized real world applications behavior studied details. best knowledge authors papers analytical analysis done proven elitism-based equal evolution strategy droste presented ﬁrst rigorous runtime analysis linear pseudo-boolean functions. shown linear functions asymptotical runtime. paper study recursive stochastic algorithm. model markov process approximate learning step small. then study behavior obtained determine convergence stability properties. work organized follows. section describes precisely. section formulation required deﬁnitions lemmas stated. section analysis markov process done stages. ﬁrst stage derive whose solution approximates asymptotic behavior cga. second stage prove corresponding therefore surely converge local optima function optimized stays them. finally section concludes paper. iteration manages population number genes thereby mimics order-one behavior uniform crossover value measures proportion allele locus simulated population. figure describes pseudocode cga. initialized represent randomly generated population. generation competing solutions generated basis current updated favor better solution probability increased learning step locus winner allele locus loser allele winner loser allele locus probability remains same. scheme denote solution belongs consider injective pseudo-boolean function maximized goal maximize using cga. iteration optimization process solutions generated basis then updated follows deﬁnition solution called local maximum function solution whose hamming distance solution i.e. local maximum called strict inequality strict. proof. iteration solutions sampled probability ﬁrst sampled solution equal winner solution probability second sampled solution winner solution equal therefore y|p) equal probabilities. hence proof. algorithm speciﬁed markov process. analysis process done stages. ﬁrst stage derive whose solutions approximate asymptotic behavior sufﬁciently small learning step used second stage characterize solutions thus obtain long-term behavior algorithm given represented interpolated process {pα}t≥ sequence random variables takes values space right continuous functions left hand limits deﬁned takes values bounded subset objective study limit behavior sequence {pα}t≥ tends zero good approximation asymptotic behavior tends zero written following interested characterizing long-term behavior hence asymptotic behavior show sequence interpolated processes {pα} weakly converges solution initial conﬁguration implies asymptotic behavior obtained solution theorem enables understand long-term behavior weak convergence theorem implies tends zero trajectory closely follow solution high probability ﬁnite interval. length time interval increases tends zero trajectory spends time required optimization process small neighborhood solution ode. thus eventually spend time small neighborhood well. tends zero follows trajectory time interval tends inﬁnity. point summarized following lemma. lemma large small enough value asymptotic behavior approximated solution initial conﬁguration. proof. solution initial condition sufﬁciently close asymptotically stable conﬁguration positive deﬁne function continuous theorem states tends limit zero since value trajectories limit process zero probability one. thus distance original sequence goes zero probability tends inﬁnity. particular initial condition used stationary conﬁguration solution converges. using nature interpolation given implied given initial conﬁguration integers exists based theorem conclude never stay conﬁguration local maximum still leaves question unanswered. possible converge local maximum example algorithm exhibits limit cyclic chaotic behavior? regarding question provide necessary condition converge local maximum proven theorem below. thus non-decreasing along trajectories ode. also nature algorithm given initial conﬁguration solution conﬁned compact subset hence lasalle’s invariance principle theorem asymptotically trajectories therefore stationary conﬁguration ode. thus solution converge stationary conﬁguration. since theorem stationary conﬁgurations local maxima unstable theorem follows. theorems together characterize long-term behavior function injective. theorem states local maxima asymptotically stable stationary conﬁgurations algorithm. addition theorem shows cannot converge point local maximum. function injective function theorem invalid cannot make sure local maxima stable stationary conﬁgurations cga. case converge non-deterministic conﬁgurations stay them. estimation distribution algorithm. simple easily implemented hardware. using small amount memory many applications memory constraint problems. paper mathematical framework based weak convergence non-linear systems theories proposed consequently convergence behavior studied. proven local maxima injective function asymptotically stable stationary points shown converges local maxima. results obtained paper interesting also serve ﬁrst steps towards using analysis edas evolutionary algorithms. open questions planned study future. first interested extending framework non-injective functions determining convergence rate different functions. recently theorems developed stochastic approximation theory useful regard since optimization algorithm chosen solve problem shape size basin attractions different also would like compute basin attractions local maxima determined function. comparing basin attractions local maximum basin attractions local maxima algorithms help choosing better algorithm optimization determined function. example could show basin attraction global maximum bigger basin attraction point pbil predict different initial values converge global maxima higher probability.", "year": 2009}