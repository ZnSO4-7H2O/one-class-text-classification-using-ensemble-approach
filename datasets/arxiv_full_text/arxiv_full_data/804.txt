{"title": "Long Timescale Credit Assignment in NeuralNetworks with External Memory", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "Credit assignment in traditional recurrent neural networks usually involves back-propagating through a long chain of tied weight matrices. The length of this chain scales linearly with the number of time-steps as the same network is run at each time-step. This creates many problems, such as vanishing gradients, that have been well studied. In contrast, a NNEM's architecture recurrent activity doesn't involve a long chain of activity (though some architectures such as the NTM do utilize a traditional recurrent architecture as a controller). Rather, the externally stored embedding vectors are used at each time-step, but no messages are passed from previous time-steps. This means that vanishing gradients aren't a problem, as all of the necessary gradient paths are short. However, these paths are extremely numerous (one per embedding vector in memory) and reused for a very long time (until it leaves the memory). Thus, the forward-pass information of each memory must be stored for the entire duration of the memory. This is problematic as this additional storage far surpasses that of the actual memories, to the extent that large memories on infeasible to back-propagate through in high dimensional settings. One way to get around the need to hold onto forward-pass information is to recalculate the forward-pass whenever gradient information is available. However, if the observations are too large to store in the domain of interest, direct reinstatement of a forward pass cannot occur. Instead, we rely on a learned autoencoder to reinstate the observation, and then use the embedding network to recalculate the forward-pass. Since the recalculated embedding vector is unlikely to perfectly match the one stored in memory, we try out 2 approximations to utilize error gradient w.r.t. the vector in memory.", "text": "neural networks external memories such neural turing machine memory networks often compared human hippocampus ability maintain episodic information long timescales networks trained tasks requiring memory storage comparable minutes whereas hippocampus stores information order years. well known sort long-term episodic storage vital overcoming problem catastrophic interference central challenge continual learning. discrepancy attributed short-term nature current benchmark tasks major bottleneck comes form reliance using back-propagation-through-time learn embedding function maps high-dimensional observations onto lower dimensional representations suitable storage external memory. problematic memories might used extremely long time embedding function called back-propagation-through-time would require hold onto forward activations function entire duration want back-propagate considering continual learning setup external memory store order hundreds thousands even millions separate embeddings could needed storing intermediate embedding activations woefully intractable. naive solution would simply store memories observation space addition embedding space recompute embeddings needed credit assignment signiﬁcant computational burden would result redundant forward-passes. importantly high dimensionality observation space would make storing additional memories take unreasonable amount memory. indeed blundell noted even relatively modest atari domain storing million observations pixel-space would require upwards gigabytes. credit assignment traditional recurrent neural networks usually involves back-propagating long chain tied weight matrices. length chain scales linearly number time-steps network time-step. creates many problems vanishing gradients well studied contrast nnem’s architecture recurrent while work utilizes long timescale external memory address problems raised here embedding function optimized usage external memory. rather either random projection pre-trained separate objective. indeed performance latter suggests end-to-end optimization might necessary adaptive embedding functions worthwhile context. activity doesn’t involve long chain activity rather externally stored embedding vectors used time-step messages passed previous time-steps. means vanishing gradients aren’t problem necessary gradient paths short. however paths extremely numerous reused long time thus forward-pass information memory must stored entire duration memory. problematic additional storage surpasses actual memories extent large memories infeasible back-propagate high dimensional settings. figure computational graphes nnem differing credit assignment mechanisms. forward-pass information shown black true gradients approximate gradients blue. standard method back-propagating external memory. note left half graph must frozen memory leaves. synthetic gradient model assigns credit upon ﬁrst entering memory learning older memories’ gradient. exact gradient reinstatement. dashed memories online recalculations. approximate gradient reinstatement. memories used inference credit assignment different. synthetic gradients recent technique created deal situation whether part network ‘blocked’ usage case previously discussed whereby information must stored gradient calculations complete. around problem treat gradient signal another function approximated. namely function mapping activation vector contextual information onto error gradient w.r.t. activation vector. auxiliary network used calculate estimate gradient true gradient available. synthetic gradient producing network trained purely supervised manner comparing estimates true gradient. shown figure apply approach nnems updating embedding network synthetic gradient immediately embedding vector calculated. true gradient signals available previously embedded memories used train synthetic gradient network. around need hold onto forward-pass information recalculate forwardpass whenever gradient information available. however previously mentioned even observations large store domain interest prevents direct reinstatement forward pass occurring. instead rely learned autoencoder reinstate observation embedding network recalculate forward-pass. since recalculated embedding vector unlikely perfectly match stored memory non-obvious utilize error gradient w.r.t. vector memory. distinct possibilities ignore mismatch directly memory’s gradient fresh embedding instead memory inference. methods illustrated figure aren’t mutually exclusive treated purposes paper interpreting initial results. latter option true gradient signal whereas former approximation relies good enough reconstruction harm inference process. test model used standard mnist benchmark objective correctly classify images numerals integer represent. task trivial modern networks appropriate architecture represents simplest tasks could certain conditions still encounter credit assignment problem previously described. since tractably train embedding functions within network external memory constructed simple nnem consisting embedding network external store holds embeddings recently encountered images along correct labels. classiﬁcations made taking cosine similarity embedding current image embeddings memory using normalized result weight class labels associated memorized embeddings. focus weight updates derived memorized embeddings stop gradient going embedding function coming current image sufﬁcient good performance simpliﬁed setting. addition found completely unsupervised learning autoencoder sufﬁcient yield decent performance. since we’re primarily interested getting supervised signal travel stored embeddings allow reconstruction error modify decoder’s weights decoder isn’t used inference like encoder/embedding function results shown figure. conditions utilize network architecture hyperparameters different mechanism credit assignment. reinstatement method exact gradients performed best requires additional computation. synthetic gradient performance perhaps lower expected possible network architecture. surrounding memories affect gradient individual memory can’t figure performance results averaged across runs. exact gradient reinstatement shown blue approximate gradient reinstatement shown green synthetic gradients shown red. accuracy results seperate validation set. input synethic gradient network future memories aren’t observable. approximate gradient version reinstatement method prone divergence makes sense given approximation involved. absolute performance mnist task isn’t useful itself performance purposely sabotaged isolate novel mechanisms discussion. however relative performance changes demonstrate promise reinstatement based credit assignment contrived domain necessitates work conﬁrm results large scale problems. possibility would tackle reinforcement learning problems like atari using variant model-free episodic control network modiﬁed embedding function learned on-line. exact modiﬁcations needed make setting compatible approach credit assignment outside scope paper clear domain present current literature calls long term episodic store. complex environment additional nuances emerge might improve credit assignment performance. given reliance accurate decoding proper reinstatement pre-training autoencoder ofﬂine might crucial. vanilla autoencoders also tend overﬁt might slow performance used assign credit examples. variational autoencoders shown robust also provide uncertainty estimates could useful deciding embeddings discard. hybrid approach utilizing synthetic gradients forms based credit assignment might also worthwhile approaches appear complementary weaknesses. focus credit assignment adaptive long-term episodic memories raise concerns fact stored embedding ‘stale’ rapidly embedding function’s parameters change. rate change slow enough operating memories queue limit impact performance. many setups embeddings discarded function usage frequently needed embeddings kept around long enough staleness become problematic. speculative autoencoder could also used ‘freshen’ embeddings re-encoding using current parameters.", "year": 2017}