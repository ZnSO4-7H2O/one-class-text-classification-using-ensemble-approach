{"title": "Noisy Parallel Approximate Decoding for Conditional Recurrent Language  Model", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Recent advances in conditional recurrent language modelling have mainly focused on network architectures (e.g., attention mechanism), learning algorithms (e.g., scheduled sampling and sequence-level training) and novel applications (e.g., image/video description generation, speech recognition, etc.) On the other hand, we notice that decoding algorithms/strategies have not been investigated as much, and it has become standard to use greedy or beam search. In this paper, we propose a novel decoding strategy motivated by an earlier observation that nonlinear hidden layers of a deep neural network stretch the data manifold. The proposed strategy is embarrassingly parallelizable without any communication overhead, while improving an existing decoding algorithm. We extensively evaluate it with attention-based neural machine translation on the task of En->Cz translation.", "text": "recent advances conditional recurrent language modelling mainly focused network architectures learning algorithms novel applications hand notice decoding algorithms/strategies investigated much become standard greedy beam search. paper propose novel decoding strategy motivated earlier observation nonlinear hidden layers deep neural network stretch data manifold. proposed strategy embarrassingly parallelizable without communication overhead improving existing decoding algorithm. extensively evaluate attention-based neural machine translation task en→cz translation. since ﬁrst language model recurrent neural network become facto choice implementing language model appealing properties approach language modelling refer recurrent language modelling recurrent language model generate long coherent sentence ability recurrent neural network capture long-term dependencies. property come spotlight recent years conditional version recurrent language model began used many different problems require generating natural language description high-dimensional complex input. tasks include machine translation speech recognition image/video description generation many references therein. much recent advances conditional recurrent language model focused either network architectures learning algorithms novel applications references therein). hand notice much research decoding algorithms conditional recurrent language models. work using recurrent language models common practice either greedy beam search likely natural language description given input. paper investigate whether possible decode better conditional recurrent language model. speciﬁcally propose decoding strategy motivated earlier observations nonlinear hidden layers deep neural network stretch data manifold neighbourhood hidden state space corresponds semantically similar conﬁgurations input space observation exploited proposed strategy injecting noise hidden transition function recurrent language model. proposed strategy called noisy parallel approximate decoding meta-algorithm runs parallel many chains noisy version inner decoding algorithm greedy beam search. parallel chains generate candidates npad selects highest score. effectively communication overhead decoding wall-clock performance proposed npad comparable single inner decoding algorithm distributed setting improves performance inner decoding algorithm. empirically evaluate proposed npad greedy search beam search well stochastic sampling diverse decoding attention-based neural machine translation. language model aims modelling probabilistic distribution natural language text. recurrent language model language model implemented recurrent neural network symbols preceding t-th symbol sentence note conditional dependency structure necessary preferred possible structures naturalness well fact length given sentence often unknown advance. neural language model neural network used compute conditional probability terms difﬁculty input neural network variable size. recurrent neural network cleverly addresses difﬁculty reading symbol time maintaining internal memory state internal memory state time vector representation t-th symbol input sentence. internal memory state effectively summarizes symbols read t-th time step. recurrent activation function simple afﬁne transformation followed point-wise nonlinearity complicated function long short-term memory gated recurrent units latter often preferred effectively avoid issue vanishing gradient given internal hidden state recurrent neural network computes conditional distribution next symbol xt+. assuming ﬁxed vocabulary linguistic symbols straightforward make parametric function returns probability symbol vocabulary given eqs. recurrent neural network reads symbol given sentence time left right computes conditional probability symbol sequence reached. probability sentence given product conditional probabilities. call recurrent neural network recurrent language model. conditional recurrent language model recurrent language model turned conditional recurrent language model distribution sentences conditioned another modality including another language. words conditional recurrent language model estimates although sentence here absolutely necessary level text phrase paragraph chapter document used unit language modelling. furthermore natural language text sequence speech video actions. anything sentence another language image video clip speech cases previously described recurrent language model requires slightest tweak order take account maximization often done stochastic gradient descent gradient computed backpropagation instead scalar learning rate adaptive learning rate methods adadelta adam often used. clear formulation eqs. exact decoding intractable state space grows exponentially respect length sequence i.e. without trivial structure exploited. thus must resort approximate decoding. greedy decoding perhaps naive approximately decode conditional recurrent language model. time step greedily selects likely symbol conditional probability greedy approach computationally efﬁcient likely crude. early choice based high conditional probability easily turn unlikely conditional probabilities later issue closely related garden path sentence problem among top-k hypotheses consider ones whose last symbols special marker sequence complete stop expanding hypotheses. hypotheses continue expanded however reduced number complete hypotheses. reaches beam search ends best among complete hypotheses returned. section introduce strategy used conjunction decoding strategies discussed earlier. strategy motivated fact deep neural network including recurrent neural network learns stretch input manifold hidden state space implies neighbourhood hidden state space corresponds semantically similar conﬁgurations input space regardless whether conﬁgurations close input space words small perturbation hidden space corresponds jumping plausible conﬁguration another. case conditional recurrent language model achieve behaviour efﬁciently exploration across multiple modes injecting noise transition function recurrent neural network. words replace time-dependent standard deviation selected reﬂect uncertainty dynamics conditional recurrent language model. recurrent network models target sequence direction uncertainty often greatest predicting earlier symbols gradually decreases context becomes available conditional distribution naturally suggests strategy start high level noise anneal decoding progresses. scheduling scheme noisy decoding processes parallel. done easily efﬁciently communication parallel processes except decoding processing. denote sequence decoded m-th decoding process. among hypotheses select highest probability assigned non-noisy model computational complexity clearly proposed decoding strategy times expensive i.e. computational complexity either greedy beam search however important note proposed npad embarrassingly parallelizable well suited distributed parallel environments modern computing. utilizing multicore machines practical cost computation reduces simply running greedy beam search contrary instance comparing beam search greedy search case beneﬁt parallelization limited heavy communication cost step. quality guarantee major issue proposed strategy resulting sequence worse running single inner-decoder stochasticity. however easily avoided setting decoding processes. even noisy decoding processes resulted sequences whose probabilities worse non-noisy process proposed strategy nevertheless returns sequence good single inner decoding algorithm. formulation conditional recurrent language model implies generate exact samples model directed acyclic graphical model. time step sample categorical distribution given samples previous time steps generated. procedure done iteratively either time steps another type stopping criterion similarly proposed npad sampling procedures parallel. major difference sampling-at-the-output proposed npad npad exploits hidden state space neural network data manifold highly linearized. words training neural network tends hidden state space much possible valid data points consequently point neighbourhood valid hidden state plausible point output space. contrary actual output space fraction output space plausible. perturb-and-map perturb-and-map algorithm reduces probabilistic inference sampling energy minimization markov random ﬁeld instance instead gibbs sampling perturb-and-map algorithm multiple instances conﬁgurations minimize perturbed energy function. instance perturb-and-map works ﬁrst injecting noise energy function i.e. followed maximum-a-posterior step i.e. minx connection perturb-and-map proposed npad clear. deﬁne energy function conditional recurrent language model log-probability i.e. then noise injection hidden state process similar injecting noise energy function. connection arises fact npad perturb-and-map share goal energy states chance diverse decoding view proposed npad generate diverse likely solutions conditional recurrent language model. variant beam search proposed modiﬁes scoring function time step beam search promote diverse decoding. done penalizing ranked hypotheses share previous hypothesis. approach however applicable beam search parallelizable proposed npad. noted npad diverse decoding used together. earlier batra proposed another approach enables decoding multiple diverse solutions mrf. method decodes solution time regularizing energy function diversity measure solution currently decoded previous solutions. unlike perturb-and-map npad deterministic algorithm. major downside approach inherently sequential. makes impractical especially neural machine translation already major issue behind deployment computational bottleneck decoding. paper evaluate proposed noisy parallel approximate decoding strategy attention-based neural machine translation. speciﬁcally train attention-based encoderdecoder network task english-to-czech translation evaluate different decoding strategies. encoder single-layer bidirectional recurrent neural network gated recurrent units decoder consists attention mechanism recurrent neural network gru’s. source target words projected -dimensional continuous space. used code dlmt-tutorial available online training. source target sentences represented sequences subword symbols trained model large parallel corpus approximately sentence pairs available wmt’ weeks. training adadelta used adaptively adjust learning rate parameter norm gradient renormalized exceed training early-stopped based validation perplexity using newstest- wmt’. model tested held-out sets newstest- newstest-. evaluation metric main evaluation metric negative conditional log-probability decoded sentence lower better. additionally bleu secondary evaluation metric. bleu de-facto standard metric automatically measuring translation quality machine translation systems case higher better. evaluate four decoding strategies. choose strategies comparable computational complexity core/machine assuming multiple cores/machines available. selection left greedy search beam search stochastic sampling diverse decoding proposed npad. greedy beam search greedy beam search widely used decoding strategies neural machine translation well conditional recurrent language models tasks. case beam search test beamwidths script made available dlmt-tutorial. stochastic sampling naive baseline injecting noise decoding simply sample output distribution time step instead taking top-k entries. test three conﬁgurations samplers parallel. number resulted typo originally intended https//github.com/nyu-dl/dlmt-tutorial/tree/master/session http//www.statmt.org/wmt/translation-task.html space constraint report result newstest-. however observed diverse decoding diverse decoding strategy hyperparameter search suggested authors based validation performance. also vary beam width included deterministic counter-part npad. effect noise injection first analyze effect noise injection comparing stochastic sampling proposed npad deterministic greedy decoding. used parallel decoding processes stochastic sampling npad. varied amount initial noise well. table present average negative log-probability bleu cases. expected proposed npad improves upon deterministic greedy decoding well stochastic sampling strategy. important notice improvement npad signiﬁcantly larger stochastic sampling conﬁrms efﬁcient effective inject noise hidden state neural network. effect number parallel chains next effect parallel decoding processes proposed npad. show table translation quality average negative log-probability bleu improves parallel decoding processes used signiﬁcantly better greedy strategy even chains. observed trend noise levels. important observation implies performance decoding easily improved without sacriﬁcing delay receiving input returning result simply adding cores/machines. npad beam search described earlier npad used deterministic decoding strategy. hence test together beam search strategy. table observe proposed npad improves deterministic strategy. however beam table observe difference greedy beam search strategies much smaller npad used outer loop. instance comparing greedy decoding beam search differences without npad conﬁrms proposed npad potential make neural machine translation suitable deploying real world. npad diverse decoding table present result using diverse decoding. diverse decoding proposed improve translation quality accordingly present best approaches based validation bleu. unlike reported able substantial improvement diverse decoding. fact jurafsky used additional translation/language models re-rank hypotheses collected diverse decoding. additional models trained selected speciﬁc application machine translation proposed npad generally applicable diverse decoding however worthwhile note diverse decoding also beneﬁt npad outer loop. paper proposed novel decoding strategy conditional recurrent language models. proposed strategy called noisy parallel approximate decoding exploits hidden state space recurrent language model injecting unstructured gaussian noise transition. multiple chains noisy decoding process parallel without communication overhead makes npad appealing practice. empirically evaluated proposed npad widely used greedy beam search well stochastic sampling diverse decoding strategies. empirical evaluation conﬁrmed npad indeed improves decoding improvement especially apparent inner decoding strategy existing strategies approximate. using npad outer loop signiﬁcantly closed fast approximate greedy search slow accurate beam search increasing potential deploying conditional recurrent language models neural machine translation practice. future work consider work ﬁrst step toward developing better decoding strategy recurrent language models. success simple npad suggests number future research directions. first thorough investigation injecting noise training done terms learning optimization also context inﬂuence decoding. conceivable exists noise injection mechanism training better noise injection process decoding second must study relationship different types scheduling noise npad addition white gaussian noise annealed variance investigated paper. lastly npad validated tasks neural machine translation image/video caption generation speech recognition references therein.)", "year": 2016}