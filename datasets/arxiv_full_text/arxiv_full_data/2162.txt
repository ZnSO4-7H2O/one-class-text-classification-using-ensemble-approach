{"title": "Reinforcement Learning in Rich-Observation MDPs using Spectral Methods", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Designing effective exploration-exploitation algorithms in Markov decision processes (MDPs) with large state-action spaces is the main challenge in reinforcement learning (RL). In fact, the learning performance degrades with the number of states and actions in the MDP. However, MDPs often exhibit a low-dimensional latent structure in practice, where a small hidden state is observable through a possibly large number of observations. In this paper, we study the setting of rich-observation Markov decision processes (\\richmdp), where hidden states are mapped to observations through an injective mapping, so that an observation can be generated by only one hidden state. While this mapping is unknown a priori, we introduce a spectral decomposition method that consistently estimates how observations are clustered in the hidden states. The estimated clustering is then integrated into an optimistic algorithm for RL (UCRL), which operates on the smaller clustered space. The resulting algorithm proceeds through phases and we show that its per-step regret (i.e., the difference in cumulative reward between the algorithm and the optimal policy) decreases as more observations are clustered together and finally, matches the (ideal) performance of an RL algorithm running directly on the hidden MDP.", "text": "designing effective exploration-exploitation algorithms markov decision processes large state-action spaces main challenge reinforcement learning fact learning performance degrades number states actions mdp. however mdps often exhibit low-dimensional latent structure practice small hidden state observable possibly large number observations. paper study setting rich-observation markov decision processes hidden states mapped observations injective mapping observation generated hidden state. mapping unknown priori introduce spectral decomposition method consistently estimates observations clustered hidden states. estimated clustering integrated optimistic algorithm operates smaller clustered space. resulting algorithm proceeds phases show per-step regret decreases observations clustered together ﬁnally matches performance algorithm running directly hidden mdp. reinforcement learning framework studies agent-environment interaction agent learns maximize given reward function long beginning interaction agent uncertain environment’s dynamics must explore different policies order gain information agent fairly certain knowledge environment exploited compute good policy attaining large cumulative reward. designing algorithms achieve effective trade-off exploration exploitation primary goal reinforcement learning. trade-off commonly measured terms cumulative regret difference rewards accumulated optimal policy obtained learning algorithm. practice often deal environments large observation spaces case regret tends large increases size observations. nonetheless many domains underlying dimensional latent space summarizes large observation space dynamics rewards mapping latent states observations known priori trivial exploit design learning algorithm operates low-dimensional latent space rather large observation space. however mapping typically unknown needs learnt agent. crucial develop contributions. paper focus rich-observation markov decision processes small number hidden states mapped large number observations injective mapping observation generated hidden state hidden states viewed clusters. mapping known could directly e.g. ucrl obtained regret scaling number hidden states instead number observasetting show indeed possible devise algorithm starting observations progressively cluster smaller states eventually converge hidden mdp. introduce sl-uc integrate spectral decomposition methods upper-bound algorithm algorithm proceeds epochs estimated mapping between observations hidden state computed optimistic policy computed constructed samples collected estimated mapping. mapping computed using spectral decomposition tensor associated observation process. prove method guaranteed correctly cluster observations together high probability. result dimensionality auxiliary decreases observations clustered thus making algorithm efﬁcient computationally effective ﬁnding good policies. suitable assumptions derive regret bound showing per-step regret decreases epochs prove worst-case bound number steps full mapping states observations computed. regret cumulated period actually constant time correct clustering increase number steps result sl-uc asymptotically matches regret learning directly latent mdp. also notice improvement regret comes equivalent reduction time space complexity. fact observations clustered space store auxiliary decreases complexity extended value iteration step ucrl decreases related work. assumption existence latent space often used reduce learning complexity. multi-armed bandits gheshlaghi-azar maillard mannor assume bandit problem generated unknown ﬁnite show regret signiﬁcantly reduced learning set. gentile consider general scenario latent contextual bandits contexts belong underlying hidden classes. show uniform exploration strategy contexts combined online clustering algorithm achieve regret scaling number hidden clusters. extension recommender systems considered contexts users items unknown priori. again uniform exploration used together spectral algorithm anandkumar learn latent classes. romdp model considered generalization latent contextual bandits actions inﬂuence contexts objective maximize long-term reward. romdps studied pac-mdp setting episodic deterministic environments using algorithm searching best q-function given function space. result extended general class contextual decision processes ortner proposes algorithm integrating state aggregation ucrl resulting algorithm signiﬁcantly reduce computational complexity ucrl analysis show improvement regret. finally notice romdps special class partially observable mdps azizzadenesheli recently proposed algorithm leverages spectral methods learn hidden dysub-optimal romdps). computation optimal memoryless policy relies optimization oracle general np-hard computing optimal policy romdps amounts solving standard mdp. finally develops pac-mdp analysis learning episodic pomdps obtain bound depends size observations. figure graphical model romdp. example observation matrix since state observation labelling arbitrary arranged non-zero values display diagonal structure. example clustering achieved policy x}). using action recover partial clusterings corresponding auxiliary states {s..s} clusters remaining elements singletons rich-observation tuple sets hidden states observations actions. denote cardinality enumerate elements {..x} {..y {..a}. assume hidden states much fewer observations i.e. consider rewards bounded depend hidden states actions reward matrix ra×x dynamics deﬁned hidden states ti′il rx×x×a transition tensor. observations generated observation matrix minimum non-zero entry omin. model strict subset pomdps since observation generated hidden state thus seen denote observations cluster cluster observation belongs structure implies existence observable f′ti reward observation-action pair hidden state-action pair dynamics obtained ′xj′ measure performance observation-based policy starting hidden state mapping romdp hidden optimal policy hidden-state policy steps measured regret recovered identifying non-zero entries need ﬁrst assumption romdp. assumption markov chain induced hidden policy ergodic. assumption policy stationary distribution hidden states stationary distribution conditional action assumption given action slice transition tensor full rank. asm. implies action dynamics expansive i.e. words number hidden states policy take action number states reached executing action allows reconstruct clustering structure romdp. consider trajectory observations actions generated arbitrary policy focus three consecutive observations step customary multi-view models vectorize observations three one-hot view vectors means observation ﬁrst view remap time indices onto notice views indeed independent random variables conditioning state action thus deﬁning {yi}i∈) sufﬁcient compute columns fact non-zero entry matrix hidden state construct cluster accurate re-labelling states. formally exists mapping function pair observations nonetheless illustrated fig. -right clustering minimal. fact zero policy even since mapping function changes actions unable correctly align clusters obtain clusters hidden states. deﬁne lemma given policy action hidden state tions clustered together according total number elements show recover factor matrix introduce mixed second third order moments permutation exploiting conditional independence views second moments written employ standard machinery tensor decomposition methods orthogonalize tensor lemma action views orthogonalize using second moment spectral decomposition compute exact factor matrix hidden state estimate second mixed moments used recover rank. actual calculate efﬁcient rank intricate postpone details app. andevt compute estimate second third lemma asm. letbv ]·ik≤ ]·i− estimate could directly used construct clustering observations noise pair prevents generating meaningful clustering. hand guarantee lem. single-out empirical estimates might lead entries ofbv relies fact aggregate observations correctly high-probability. denote \\sil corollary auxiliary states composed clusters tained clustering observations according toev together exists hidden state finally tends inﬁnity. describe spectral learning ucrl obtained integrating spectral method ucrl strategy. learning process split epochs increasing length. beginning epoch trajectory generated previous procedure fig. observations clustered together auxiliary space generated clustered together using labelling auxiliary states arbitrary observations preserve labels across epochs thus safely conclude observations belong hidden state. similarly construct cluster which case returns exact hidden space abuse notation write denote fact observation clustered auxiliary state epoch similarly compute number visits auxiliary state-action pair reward cumulated time return spectral decomposition runs space size instead thus reducing computation complexity. since clustering monotonic computed incrementally without storing statistics observation level thus signiﬁcantly reducing space complexity algorithm. optimistic auxiliary constructed using conﬁdence intervals extended value iteration resulting optimal optimistic policyeπ executed complexity scales o)a) thus gradually reducing complexity ucrl observation space soon observations clustered together. theorem consider romdp diameter sl-uc time steps asm. probability suffers total regret remark. bound shows per-step regret decreases epochs. first notice regret ﬁrst epochs actually depends number observations diameter soon observations start clustered auxiliary states regret depends number auxiliary states diameter since decreases every time observation added cluster monotonically decreasing auxiliary states reduces epochs longer. furthermore recall even clustering returned spectral method minimal merging minimal clustering. thm. shows performance sl-uc improves epochs relate performance could achieved hidden space known. order provid minimal clustrting integrate alg. clustering technique similar used epoch proceed merging together claim learned true clustering less ignore temporary clustering proceed next epoch. worth note procedure requires knowledge spectral method not. explicit rate clustering difﬁcult determine derive worst-case bounds number steps needed start clustering least observation well known algorithms. since sl-uc proposed speciﬁcally inﬁnite horizon environment able compare episodic setting e.g. psrl implemented three hidden-layers feed forward network equipped rmsprop replay buffer. tune introduced sl-uc novel algorithm learn romdps combining spectral method recovering clustering structure problem ucrl effectively trade exploration exploitation. proved theoretical guarantees showing sl-uc progressively reﬁnes clustering regret tends regret could achieved hidden structure known advance showed sl-uc gradually constructs smaller mdps therefore lower cost computing optimistic policy encounters fewer number epochs suffers lower computation cost. finally work opens several interesting directions extend results variety state aggregation topologies furthermore aggregate proposed method regret analyses e.g. leverage current bounds. azizzadenesheli supported part career award ccf-. lazaric supported part grant cper nord-pas calais/feder data advanced data science technologies cristal french national research agency project extra-learn n.anr-ce--. anandkumar supported part microsoft faculty fellowship google faculty award adobe grant career award afosr fa---.", "year": 2016}