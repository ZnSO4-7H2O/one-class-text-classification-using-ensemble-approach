{"title": "Uncertainty Estimates for Efficient Neural Network-based Dialogue Policy  Optimisation", "tag": ["stat.ML", "cs.CL", "cs.LG", "cs.NE"], "abstract": "In statistical dialogue management, the dialogue manager learns a policy that maps a belief state to an action for the system to perform. Efficient exploration is key to successful policy optimisation. Current deep reinforcement learning methods are very promising but rely on epsilon-greedy exploration, thus subjecting the user to a random choice of action during learning. Alternative approaches such as Gaussian Process SARSA (GPSARSA) estimate uncertainties and are sample efficient, leading to better user experience, but on the expense of a greater computational complexity. This paper examines approaches to extract uncertainty estimates from deep Q-networks (DQN) in the context of dialogue management. We perform an extensive benchmark of deep Bayesian methods to extract uncertainty estimates, namely Bayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and alpha-divergences, combining it with DQN algorithm.", "text": "statistical dialogue management dialogue manager learns policy maps belief state action system perform. efﬁcient exploration successful policy optimisation. current deep reinforcement learning methods promising rely ε-greedy exploration thus subjecting user random choice action learning. alternative approaches gaussian process sarsa estimate uncertainties sample efﬁcient leading better user experience expense greater computational complexity. paper examines approaches extract uncertainty estimates deep q-networks context dialogue management. perform extensive benchmark deep bayesian methods extract uncertainty estimates namely bayes-by-backprop dropout concrete variation bootstrapped ensemble α-divergences combining algorithm. statistical approaches dialogue modelling allow automatic optimisation spoken dialogue systems typically designed according structured ontology deﬁnes domain system talk about. domain presented using slots variables user either specify domain. system also comprises various statistical components. includes spoken language understanding module takes sentence input gives dialogue output slot-value pair specify arguments act. example inform dialogue type inform user informing system would like constrain search luxury venues. components include dialogue belief state tracker predicts user intent track dialogue history dialogue policy determine dialogue natural language generator convert conceptual representations system responses. pomdp framework mitigates problem noisy estimates spoken language understanding maintaining distribution possible hypothesises called belief state dialogue policy employed belief state appropriate system action every dialogue turn. ability generalise across different noise levels essential successful dialogue policy operation. although supervised learning dialogue corpora used learn human decision-making data action selection take future outcomes dialogue considersation leads sub-optimal behaviour. alternative reinforcement learning maximizes expected rewards received course dialogue much suitable method learning dialogue policies reward case measures degree dialogue successful. need learning real users online interactions efﬁcient exploration state-action space critical. q-function state-action pair augmented estimate uncertainty guide exploration achieve higher performance efﬁcient learning uncertainty estimates policy allow system generalise across different noise levels mitigate errors incurred speech recognition therefore resulting robust dialogue manager. gaussian processes provide explicit estimate uncertainty computational intensive preclude large action spaces deep neural network hand scale much better data computationally less expensive gps. many studies shown suitable dialogue management tasks however application noisy environments relatively under-explored paper perform benchmark uncertainty estimates dialogue domain using bayesian deep learning experiment without noise added simulated user input examine generalization capabilities different approaches compare state-of-the-art gpsarsa algorithm. bayes-by-backprop algorithm achieves best performance among neural networks approaches. one-step reward received given time deep q-network algorithm models action-value function using deep neural network weight vector iteratively improve prediction minimizing following loss obtain uncertainty estimates neural network bayesian neural networks employed instead single ﬁxed value weights neural networks weights represented probability distributions possible values given observed dialogues uncertainty hidden units allows expression uncertainty predictions exploration thompson sampling used instead \u0001-greedy consists performing single stochastic forward pass network every time action needs taken. q-values given input belief state given taking expectation posterior distribution equivalent using ensemble uncountably inﬁnite number neural networks intractable resort sampling-based stochastic variational inferences. variational inference intractable posterior approximated variational distribution parameters learnt minimizing kullback-leibler divergence variational approximation true posterior weights resulting cost function termed variational free energy test four algorithms casted variational approximate framework namely bayes-by-backprop dropout concrete dropout α-divergence deep bbq-learning. implement bayes backprop method dqn. propagate error layer samples reparameterization trick used choose gaussian diagonal covariance variational parameter given mean covariance weight sample obtained ﬁrst sampling computing point-wise multiplication. ensure strictly positive softplus function log) used free parameter variational parameters ρi}d d-dimensional weight vector resulting gradient estimator variational objective unbiased lower variance. exact cost approximated monte carlo sample drawn variational posterior q|θ). likelihood term objective function expected square loss. α-divergences. approximate inference technique described bayes backprop method corresponds variational bayes particular case α-divergence α-divergence measures similarity distributions take form hernandez-lobato found using performs better case approximation cover modes true distribution case local mode assuming true posterior multi-modal achieves balance shown perform best applied regression classiﬁcation tasks. experiment objective function based black α-divergence energy. reparametrization proposed designates energy designates approximation corresponds number datapoints minibatch. dropout. another method obtain uncertainty estimates deep neural networks bayesian inference dropout dropout consists randomly dropping units neural network training previous methods dropout analyzed variational inference perspective comes fact applying stochastic mask equivalent multiplying weight matrix given layer random noise. resulting stochastic weight matrix seen draws approximate posterior weights replacing deterministic weight matrix concrete dropout. obtain well-calibrated uncertainty estimates method grid-search dropout probabilities necessary. however treat dropout part optimization task obtaining automatic method tuning mask. method continuously relax dropout’s discrete masks optimize dropout probability using gradient methods dropout probability becomes optimized parameters. concrete distribution relaxation bernoulli random variable becomes uncertainty estimates obtained random initialization several neural networks predict ensemble uncertainty estimates neural networks improve efﬁciency networks share architecture different last layer computing q-values. algorithm obtained highest scores non-bootstrapped case networks share memory replay. employ ensemble variant. obtain uncertainty estimates gpsarsa needs steps total number data points training number representative data points training complexity dropout concrete dropout bootstrapped dqns number neural network parameters. complexity bbqn tripled requires three sets parameters. experiments conducted using cambridge restaurant domain pydial toolkit goal-driven user simulator semantic level user simulator replicates user behavior sufﬁcient accuracy optimize model parameters acceptable level performance cost-effective development evaluation purposes. error model confusions simulated user input added. error model outputs n-best list possible user responses. input models full dialogue belief state size output action space consists possible actions. linear kernel used gpsarsa. used hidden layers size maximum dialogue length turns adam optimiser used initial learning rate results averaged three different runs. figure shows learning curves benchmarked models function training dialogues. analysed algorithms bbqn reached performance comparable state-of-the-art non-parametric approach terms efﬁciency exploration well ﬁnal performance. moreover thanks implicit regularization constraint learning becomes much stable comparing vanilla dqn. three analyzed methods dropout concrete dropout bootstrapped approach help improving learning rate vanilla \u0001-greedy algorithm neither stabilize exploration. although concrete dropout tuning dropout probability automatic help improve efﬁciency. also optimize number heads bootstrapped however performance vary substantially yielding best results heads. α-divergences settings perform better general samples. clarity show learning curves. taking samples decreases variance gradient estimates averaged loss updates closer loss obtained taking sample close mean variational distribution implies updates necessary move direction true posterior distribution resulting slower convergence optimal policy. also investigated impact noise training models simulated user semantic error rate evaluated semantic error rate examine generalisation capabilities different algorithms. ﬁnal success rates given figure function training dialogues. results show gpsarsa performs best terms success rate followed closely bbqn. shows bbqn generalizes better ε-greedy algorithms. bbqn potential robust performance performs well even conditions different training conditions. methods fall behind substantially vanilla able reach similar performance training. chen xiang zhou cheng chang runzhe yang agent-aware dropout safe efﬁcient on-line dialogue policy learning. proceedings conference empirical methods natural language processing pages lucie daubigney milica gaši´c senthilkumar chandramohan matthieu geist olivier pietquin steve young. uncertainty management on-line optimisation pomdp-based largescale spoken dialogue system. interspeech pages yarin zoubin ghahramani. dropout bayesian approximation representing model uncertainty deep learning. international conference machine learning pages milica gasic steve young. gaussian processes pomdp-based dialogue manager optimization. ieee/acm transactions audio speech language processing josé miguel hernández-lobato yingzhen mark rowland daniel hernández-lobato thang richard turner. black-box α-divergence minimization. international conference machine learning esther levin roberto pieraccini wieland eckert. stochastic model human-machine interaction learning dialog strategies. ieee transactions speech audio processing zachary lipton jianfeng lihong xiujun faisal ahmed deng. efﬁcient exploration dialogue policy learning networks replay buffer spiking. nips workshop deep reinforcement learning osband charles blundell alexander pritzel benjamin roy. deep exploration bootstrapped dqn. advances neural information processing systems pages nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research pei-hao pawel budzianowski stefan ultes milica gasic steve young. sampleefﬁcient actor-critic reinforcement learning supervised data dialogue management. proceedings sigdial jason williams kavosh asadi geoffrey zweig. hybrid code networks practical efﬁcient end-to-end dialog control supervised reinforcement learning. association computational linguistics", "year": 2017}