{"title": "Principled Hybrids of Generative and Discriminative Domain Adaptation", "tag": ["cs.LG", "cs.AI"], "abstract": "We propose a probabilistic framework for domain adaptation that blends both generative and discriminative modeling in a principled way. Under this framework, generative and discriminative models correspond to specific choices of the prior over parameters. This provides us a very general way to interpolate between generative and discriminative extremes through different choices of priors. By maximizing both the marginal and the conditional log-likelihoods, models derived from this framework can use both labeled instances from the source domain as well as unlabeled instances from both source and target domains. Under this framework, we show that the popular reconstruction loss of autoencoder corresponds to an upper bound of the negative marginal log-likelihoods of unlabeled instances, where marginal distributions are given by proper kernel density estimations. This provides a way to interpret the empirical success of autoencoders in domain adaptation and semi-supervised learning. We instantiate our framework using neural networks, and build a concrete model, DAuto. Empirically, we demonstrate the effectiveness of DAuto on text, image and speech datasets, showing that it outperforms related competitors when domain adaptation is possible.", "text": "zhao†∗ zhenyao zhu‡ junjie adam coates‡ geoff gordon† †school computer science carnegie mellon university pittsburgh ‡silicon valley baidu research sunnyvale propose probabilistic framework domain adaptation blends generative discriminative modeling principled way. framework generative discriminative models correspond speciﬁc choices prior parameters. provides general interpolate generative discriminative extremes different choices priors. maximizing marginal conditional log-likelihoods models derived framework labeled instances source domain well unlabeled instances source target domains. framework show popular reconstruction loss autoencoder corresponds upper bound negative marginal log-likelihoods unlabeled instances marginal distributions given proper kernel density estimations. provides interpret empirical success autoencoders domain adaptation semi-supervised learning. instantiate framework using neural networks build concrete model dauto. empirically demonstrate effectiveness dauto text image speech datasets showing outperforms related competitors domain adaptation possible. making accurate predictions relies heavily existence labeled data desired tasks. however generating labeled data learning tasks often time-consuming. result poses obstacle applying machine learning methods broader application domains. domain adaptation focuses situation access labeled data source domain assumed different target domain want apply model goal domain adaptation algorithms setting generalize better target domain exploiting labeled data source domain unlabeled data target domain. paper propose probabilistic framework domain adaptation combines generative discriminative modeling principled way. start simple general generative model show special choice prior distribution model parameters leads usual discriminative modeling. generative nature framework provides principled unlabeled instances source target domains. framework non-parametric kernel density estimators marginal distribution instances show popular reconstruction loss autoencoders corresponds part work done intern svail baidu research. upper bound negative marginal log-likelihoods unlabeled instances. provides novel probabilistic interpretation unsupervised training general autoencoders help discriminative tasks though interpretations exist speciﬁc variants autoencoders e.g. denoising autoencoders contractive autoencoders interpretation also used explain recent success autoencoders semi-supervised learning instantiate framework ﬂexible neural networks powerful function approximators leading concrete model dauto. dauto designed achieve following three objectives simultaneously uniﬁed model learns representations informative main learning task source domain. learns domain-invariant features indistinguishable source target domains. learns robust representations reconstruction loss instances domains. demonstrate effectiveness dauto ﬁrst conduct synthetic experiment using mnist dataset showing superior performance adaptation possible. compare dauto state-of-the-art models amazon benchmark dataset. another contribution extend dauto also applied time-series modeling. evaluate speech recognition task showing effectiveness improving recognition accuracies trained utterances different accents. also provide qualitative analysis case domain adaptation hard methods test fail. recently availability rich data powerful computational resources non-linear representations hypothesis classes domain adaptation increasingly explored line works focuses building common robust feature representations among multiple domains using either supervised neural networks unsupervised pretraining using denoising auto-encoders works focus learning feature transformations feature distributions source target domains close practice observed unsupervised pretraining using stacked denoising auto-encoders often improves generalization accuracy limitations msda needs explicitly form covariance matrix input features solves linear system computationally expensive high dimensional settings. hand also clear extend msda also applied time-series modeling. domain adversarial neural networks discriminative model learn domain-invariant features formulated minimax problem feature transformation component tries learn representation confuse following domain classiﬁcation component. dann also enjoys nice theoretical justiﬁcation learn feature decrease a-distance measure source target domains. distance measures distributions also applied. tzeng long propose similar models maximum mean discrepancy domains minimized. recently bousmalis propose model orthogonal representations shared domains unique domain learned simultaneously. achieve goal incorporating similarity difference penalties features objective function. finally domain adaptation also viewed semi-supervised learning problem ignoring domain shift source instances treated labeled data target instances unlabeled data dauto improves msda dimension feature vectors high result covariance matrix cannot explicitly formed. later dauto also naturally extended modeling time-series domain. hand compared dann dauto clear probabilistic generative model interpretation provides principled unlabeled data source target domains training. generative models provide principled labeled data source domain unlabeled data domains. section start general probabilistic framework domain adaptation using principled hybrid generative discriminative models. provide concrete instantiation framework show leads popular reconstructionbased domain adaptation models. derivation also used explain prevalence success autoencoders domain adaptation semi-supervised learning section proposing dauto model follows probabilistic framework combines minimax adversarial loss regularizer domain adaptation. probabilistic framework domain adaptation input instance target variable classiﬁcation setting regression setting. fully generative model speciﬁed follows model parameter governs generation process model parameter conditional distribution prior distribution model parameters. ﬁrst glance might generative model unsuitable domain adaptation setting since implicitly assumes marginal distribution shared among instances domains. point makes framework still valid domain adaptation lies possible richness generation process parametrized although marginal distribution different input space still proper transformation induced marginal distributions domains similar fact necessary implicit assumption underlies recent success regularization based discriminative models better understand this illustrate generative process fig. using joint model assume prior distribution factorizes have note case ﬁrst term r.h.s. concerned prediction means unsupervised learning help generalization prediction. words independence assumption equivalently reduces joint model discriminative models contain parameters care prediction accuracy. emphasize reduction assumption crucial otherwise optimal still depend i.e. extreme corresponds prior constrains shared generative processes base distribution denotes kronecker delta function. seen formulation exactly reduces usual inference scheme generative model given parameter discussion shows formulation given general enough incorporate discriminative generative modelings extreme cases depending choice prior distribution easily recover both. nutshell independent recover discriminative modeling; otherwise exactly same recover generative modeling. however practice sweet spot often lies models discriminative training usually wins predictive accuracy generative modeling provides principled unlabeled data. achieve best worlds consider case common subspace i.e. model parameters shared generation process clearly case factorization assumption prior distribution hold anymore cannot hope recover discriminative model simply optimizing make discussion concrete think φ\\ζ) ψ\\ζ) shared parameters domain adaptation possible setting whenever forms rich class transformations unlabeled instances domains similar induced marginal distribution. necessary condition domain adaptation succeed framework. generative model also allows algorithms unlabeled instances domains optimize marginal likelihood function also helps predictive task shared component instantiation using kernel density estimation probabilistic framework instantiate proper choices marginal distributions well conditional distributions. hand would like make assumptions possible generation process hand model rich enough even though instances source target domains different distributions model contains ﬂexible transformation induced distributions similar enough domains. taking considerations account propose nonparametric kernel density estimator model speciﬁcally chosen kernel {xi}n transformations applied identity deﬁnition reduces original one. note applied source target domains separately original give similar density estimations empirical distributions other exactly case domain adaptation. figure density estimator additional transformations input instances source target domains respectively. transforms close transforms back close density estimations conditional distribution depending whether typical choices include linear regression logistic regression. models linear limited ﬁrst augment rich nonlinear transformation applied input instance. note transformation along parameters shared finally model completed specifying prior distribution follows constrains common parameter shared base distribution chosen prior corresponds usual criterion; forms distributions effectively introduce regularizations advantages generative model lends principled using unlabeled data labeled data source domain {xj}n unlabeled data domains. domains. instead maximizing conditional likelihood using labeled data jointly maximize conditional likelihood marginal likelihood depends bandwidth constant depend upper bound becomes tight note derivation omit prior distribution assuming constant distribution. design choice made e.g. gaussian corresponding regularization term model parameters putting together maximizing combination conditional marginal likelihoods correspond following unconstrained minimization problem remark. maximum joint likelihood estimator also maximum-a-posteriori estimator leading fully probabilistic interpretation second term objective function interesting interpretation essentially measures reconstruction error transformation based interpret encoder corresponding decoder minimizing reconstruction loss autoencoder exactly corresponds maximizing lower bound marginal probability distribution given kernel density estimation. furthermore squared measure reconstruction error restricted example instead gaussian kernel chose laplacian kernel would measure reconstruction loss interpretation also used explain practical success autoencoders semi-supervised learning neural networks ﬂexible function approximators desired transformations speciﬁcally fully-connected neural networks parametrize softmax function parametrize simply change softmax function afﬁne function output. simplicity discussion assume one-layer fully connected network represent rd×d rd×d element-wise nonlinear activation function e.g. rectify linear unit. ease notation also omit translation terms afﬁne transformations. softmax softmax layer compute conditional probability class assignment. although model capacity learn shared transformation unlabeled data domains similar marginal distributions objective function discussed necessarily induce transformation. purpose domain adaptation necessary regularizer enforces constraint. popular effective choice a-distance introduced shown a-distance approximated binary classiﬁcation error domain classiﬁer discriminates figure model architecture dauto computation learning phase. dauto contains three components autoencoder domain classiﬁer label predictor autoencoder instantiates marginal distribution encoder label predictor instantiates conditional probability domain classiﬁer works regularizer. model contains three different objectives domain classiﬁcation loss reconstruction loss well label prediction loss. inference phase used. instances source target domain intuition given ﬁxed class binary labeling functions exists function easy tell instances source domain target domain distance domains large. hand labeling functions confused task think distance domains small. following dann take approach softmax domain classiﬁer shared representation constructed encoder regularizer takes form convex surrogate loss binary error. common choice cross-entropy loss. putting together optimization problem joint model given prediction loss reconstruction loss domain classiﬁcation loss. illustrate model architecture fig. show above cross-entropy loss learning task reconstruction loss essentially negative joint log-likelihoods labeled instances unlabeled instances. domain classiﬁcation loss works regularizer incorporate prior knowledge encoder form invariant transformation. result dauto designed achieve following three objectives simultaneously uniﬁed framework learns representations informative main learning task source domain. learns robust representations reconstruction loss. learns domain-invariant features indistinguishable source target domains. ﬁrst evaluate dauto synthetic experiments mnist compare state-ofthe-art models including msda ladder network dann. report experimental results amazon benchmark dataset large-scale time-series dataset speech recognition. synthetic experiment mnist. experiment contains tasks task binary classiﬁcation problem judging whether given image speciﬁc number not. choose digits tasks similar many handwritten images quite different. clearly domain adaptation always possible. would like verify dauto succeeds domains close other also show failure case domains sufﬁciently different. task recognizing digit sample images training digit others digits sample images original test images digit others digits pairs experiments altogether possible pair source target domains. design well-controlled experiment compare dauto standard dann algorithms share network structure. also apply training procedure algorithms difference performance explained additional domain regularizer well reconstruction loss dauto. networks used experiment contains hidden layers contain hidden units respectively. input layer contains units output layer single unit parametrized logistic regression model. except output layer hidden layers relu nonlinear activation function. network structure used dann dauto. training adadelta optimization algorithm models. dauto hyperparameters chosen following range held-out test combined early stopping model selection. experiments learning rate multi-class classiﬁcation. dauto easily applied multi-class classiﬁcation well. this test model mnist usps svhn contain images digits. mnist contains train/test instances; usps contains train/test instances svhn contains train/test instances. training preprocess instances gray scale single-channel images size used network. again perform controlled experiment ensure fair comparison evaluated models. network structures exactly approaches hidden layers units following softmax output layer units. training batch size dropout rate also ﬁxed among experiments. hence again difference performance explained different objective functions different models. sentiment analysis. amazon dataset consists reviews products amazon task predict polarity text review i.e. whether review speciﬁc product positive negative. dataset contains text reviews following four categories books dvds electronics kitchen appliances product contains text reviews training data reviews test data. text review described feature vector dimensions dimension correspond word dictionary. dataset benchmark data frequently used purpose sentiment analysis source-target pair train corresponding models completely labeled source instances access unlabeled target instances. classiﬁcation accuracy target domain main metric. ladder dann dauto share network structure input layer contains units followed fully-connected hidden layer units output layer logistic regression model. correspondingly msda pretrains two-layer stacked auto-encoder hence feature representation pretrained msda experiment dimensions. ladder dauto also unlabeled instances pretrain model parameters purely unsupervised way. again neural network based models relu nonlinear activation function adadelta optimization algorithm training. dauto hyperparameters speech recognition. dauto naturally extended time-series modeling. experiment apply dauto speech recognition recurrent neural network trained ingest speech spectrograms generate text transcripts. model variant deepspeech composed convolution layer followed stacked bidirectional lstm layers fully connected layer softmax layer. time step input network spectrogram feature output character blank. network trained end-to-end using connectionist temporal classiﬁcation loss negative log-likelihood training utterances. extend dauto sequential models besides global loss regularize time step reconstruction loss autoencoder well adversarial loss domain classiﬁer. evaluate dauto compare algorithms adaptation task across three different accented datasets recorded native english speakers recorded speakers mandarin indian accents respectively. dataset contains hours labeled audio data user utterances randomly sample percent training rest used test set. model structure recurrent network used experiment follows time step input feature spectrogram feature dimensions. followed convolution layer kernels stacked bidirectional lstm layers contains hidden units. output last bidirectional lstm layer connected fullyconnected softmax layer outputs represents characters three special characters space apostrophe blank. publicly available loss implementation https //github.com/baidu-research/warp-ctc experiments performed public codebase https//github.com/baidu-research/ba-dls-deepspeech. compare dauto following methods no-adapt. baseline model ignores possible shifts domains. order make comparison fair possible experiments dauto shares prediction model no-adapt. msda. msda pretrains unlabeled instances source target domains build feature input space. constructed representations msda used train classiﬁer suggested original paper experiments corruption level training msda stack number layers autoencoders dauto. ladder network ladder network novel structure aiming semi-supervised learning. hierarchical denoising autoencoder reconstruction errors pair hidden layers incorporated objective function. dann. again exactly inference structure dann no-adapt ladder dauto. experiments early-stop avoid overﬁtting. implement models ensure preprocessing data algorithms differences experimental results explained differences models themselves. defer detailed description models used experiment supplementary material. results analysis mnist. list classiﬁcation accuracies pairs tasks using no-adapt dann dauto table besides pairs tasks domain adaptation setting also show additional tasks training test sets domain. scores tasks used empirical upper bounds compare performance domain adaptation algorithms. dauto signiﬁcantly improves no-adapt baseline total possible pairs showing indeeds desired capability domain adaptation. hand would also like highlight domain adaptation tasks successful prediction accuracies three algorithms task marginally random guesses digits similar observe successful domain transfer using dauto even algorithm instances target category training. qualitatively study successful failure cases project representations without dauto adaptation onto dimensional space using shown fig. several interesting observations made fig. domain adaptation successful principal directions learned representations domains well-aligned other; hand adaptation fails representations share principal directions. special case source target domains share distribution dauto still works degrade. multi-class classiﬁcation. results multi-class classiﬁcation digits shown table highlight successful domain adaptations using green colors failure cases using colors. datasets correspond source domains columns correspond target domains. dann dauto contain failure case explained intrinsic difference svhn datasets two. however three datasets whenever dann dauto succeed domain adaptation dauto usually outperforms dann around percent accuracy. note experiment dann dauto share exactly experimental protocol hence difference explained different objective functions. words adaptation beneﬁt reconstruction error autoencoders works unsupervised regularizer. table classiﬁcation accuracy digits experiment no-adapt dann dauto. improvements baseline method highlighted green decreases performance shown red. table best viewed color. amazon. show effectiveness different domain adaptation algorithms labeled instances scarce evaluate algorithms tasks gradually increasing size labeled training instances still whole test dataset measure performance. speciﬁcally fraction available labeled instances source domain training. successful domain adaptation algorithm able take advantage unlabeled instances target domain help generalization even amount labeled instances available small. plot results fig. domain adaptation algorithms able unlabeled instances target domain help generalization. however differences baseline dauto getting smaller increase size training data. phenomenon supports probabilistic framework showing dauto effectively unlabeled data. fig. also observe given small fraction data large gaps dauto baseline tasks. interestingly gaps become smaller certain tasks e.g. kitchen->electronics books->kitchen. besides observe consistently fraction training data increases. convinces conjecture dauto effective baseline learning informative representations low-resource data semi-supervised learning effect generative nature. also report classiﬁcation accuracies test data pairs tasks thorough comparisons among models table dauto outperforms competitors tasks whereas dann achieves best test accuracy ladder scores highest msda performs best emphasize dauto performs consistently better competitors. note difference dann dauto autoencoder regularizer forces feature learning part learn robust features conclude dauto successfully helps build robust representations. check whether accuracy differences methods signiﬁcant perform paired t-test report two-sided p-value null hypothesis related speech recognition. avoid possible external error decoding process usually stable noisy data directly report loss obtained different algorithms. pairs domain adaptation tasks source/target domain ranges speech accents. show results table again observe decreases performance applying domain adaptation algorithms transfer speech indian accents fails. however domains similar adaptation possible dauto consistently outperforms dann. want bring readers’ attention dann dauto improve baseline method training unlabeled instances domain means dann dauto effect semi-supervised learning algorithms source target domains indeed same dann dauto beneﬁt using unlabeled instances. generative interpretation probabilistic framework dauto achieves goal maximizing marginal probability unlabeled instances shared component helps training discriminative model. table loss tasks lstm dann dauto. lower loss indicates better speech model. improvements baseline method highlighted green decreases performance shown red. table best viewed color. propose probabilistic framework incorporates generative discriminative modeling principled also helps interpolate generative discriminative extremes speciﬁc choice prior distribution subset model parameters shared. instantiated model dauto allows unlabeled instances domains principled way. instantiation also shows empirical success autoencoders semi-supervised learning domain adaptation explained maximizing marginal log-likelihoods unlabeled data kernel density estimators used model marginal distributions. provides ﬁrst probabilistic justiﬁcation joint training autoencoders practice. experimentally show dauto successfully applied domain adaptation problems natural extension time series well. would like thank speech recognition team svail baidu research especially vinay jesse engel sanjeev satheesh helpful discussions speech recognition experiment. also wants thank zhuoyuan chen valuable suggestions encouragements internship. supported part collaborative participation robotics consortium sponsored army research laboratory collaborative technology alliance program cooperative agreement wnf---. views conclusions contained document authors interpreted representing ofﬁcial policies either expressed implied army research laboratory u.s. government. u.s. government authorized reproduce distribute reprints government purposes notwithstanding copyright notation herein.", "year": 2017}