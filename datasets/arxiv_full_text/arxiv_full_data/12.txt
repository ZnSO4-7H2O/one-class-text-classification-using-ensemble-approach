{"title": "Efficient Neural Architecture Search via Parameter Sharing", "tag": ["cs.LG", "cs.CL", "cs.CV", "cs.NE", "stat.ML"], "abstract": "We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.", "text": "propose efﬁcient neural architecture search fast inexpensive approach automatic model design. enas controller discovers neural network architectures searching optimal subgraph within large computational graph. controller trained policy gradient select subgraph maximizes expected reward validation set. meanmodel corresponding selected subgraph trained minimize canonical cross entropy loss. sharing parameters among child models allows enas deliver strong empirical performances using much fewer gpuhours existing automatic model design approaches notably less expensive standard neural architecture search. penn treebank dataset enas discovers novel architecture achieves test perplexity establishing state-of-the-art among methods without post-training processing. cifar- dataset enas ﬁnds novel architecture achieves test error test error nasnet neural architecture search successfully applied design model architectures image classiﬁcation language models controller trained loop controller ﬁrst samples candidate architecture i.e. child model trains convergence measure performance task desire. controller uses performance guiding signal promising architectures. process repeated many iterations. de*equal contribution google brain language technology institute carnegie mellon university department computer science stanford university. correspondence hieu pham <hyhieucmu.edu> melody guan <mguanstanford.edu>. spite impressive empirical performance computationally expensive time consuming e.g. zoph gpus days meanwhile using less resources tends produce less compelling results observe computational bottleneck training child model convergence measure accuracy whilst throwing away trained weights. main contribution work improve efﬁciency forcing child models share weights eschew training child model scratch convergence. idea apparent complications different child models might utilize weights differently encouraged previous work transfer learning multitask learning established parameters learned particular model particular task used models tasks little modiﬁcations empirically show sharing parameters among child models possible also allows strong performance. speciﬁcally cifar- method achieves test error compared nas. penn treebank method achieves test perplexity signiﬁcantly outperforms nas’s test perplexity state-of-the-art among penn treebank’s approaches utilize post-training processing. importantly experiments single nvidia search architectures takes less hours. compared reduction gpu-hours efﬁciency name method efﬁcient neural architecture search central idea enas observation graphs ends iterating viewed sub-graphs larger graph. words represent nas’s search space using single directed acyclic graph figure illustrates generic example architecture realized taking subgraph dag. intuitively enas’s sufigure example recurrent cell search space computational nodes. left computational corresponds recurrent cell. edges represent information graph. middle recurrent cell. right outputs controller result cell middle left. note nodes never sampled results averaged treated cell’s output. create recurrent cell controller samples blocks decisions. illustrate enas mechanism simple example recurrent cell computational nodes input signal recurrent cell output previous time step. sample follows. node controller samples previous index activation function. example chooses previous index activation function relu. thus node cell computes relu output simply average loose ends i.e. nodes selected inputs nodes. example since indices never sampled input node recurrent cell uses average output. words example above note pair nodes independent parameter matrix shown example choosing previous indices controller also decides parameter matrices used. therefore enas recurrent cells search space share parameters. figure graph represents entire search space arrows deﬁne model search space decided controller. here node input model whereas nodes model’s outputs. perposition possible child models search space nodes represent local computations edges represent information. local computations node parameters used particular computation activated. therefore enas’s design allows parameters shared among child models i.e. architectures search space. following facilitate discussion enas example illustrates design cell recurrent neural networks speciﬁed controller explain train enas derive architectures enas’s controller finally explain search space designing convolutional architectures design recurrent cells employ nodes nodes represent local computations edges represent information nodes. enas’s controller decides edges activated computations performed node dag. design search space cells different search space cells zoph authors topology architectures binary tree learn operations node tree. contrast search space allows enas design topology reward computed validation rather training encourage enas select models generalize well rather models overﬁt training well. language model experiment reward function c/valid perplexity computed minibatch validation data. image classiﬁcation experiments reward function accuracy minibatch validation images. deriving architectures. discuss derive novel architectures trained enas model. ﬁrst sample several models trained policy sampled model compute reward single minibatch sampled validation set. take model highest reward re-train scratch. possible improve experimental results training sampled models scratch selecting model highest performance separated validation done works however method yields similar performance whilst much economical. figure example recurrent cell search space computational nodes represent layers convolutional network. output controller rnn. bottom left computational corresponding network’s architecture. arrows denote active computational paths. bottom right complete network. dotted arrows denote skip connections. discuss search space convolutional architectures. recall search space recurrent cell controller samples decisions decision block previous node connect activation function use. search space convolucontroller network lstm hidden units lstm samples decisions softmax classiﬁers autoregressive fashion decision previous step input embedding next step. ﬁrst step controller network receives empty embedding input. enas sets learnable parameters parameters controller lstm denoted shared parameters child models denoted training procedure enas consists interleaving phases. ﬁrst phase trains shared parameters child models whole pass training data set. penn treebank experiments trained steps minibatch examples gradient computed using back-propagation time truncated time steps. meanwhile cifar- trained training images separated minibatches size computed using standard back-propagation. second phase trains parameters controller lstm ﬁxed number steps typically experiments. phases alternated training enas. details follows. training shared parameters child models. step controller’s policy perform stochastic gradient descent minimize expected loss function em∼π here standard cross-entropy loss computed minibatch training data model sampled gradient computed using monte carlo estimate mi’s sampled described above. provides unbiased estimate gradient em∼π however estimate higher variance standard gradient ﬁxed. nevertheless perhaps surprising works i.e. update using gradient single model sampled mentioned train entire pass training data. tional models controller also samples sets decisions decision block previous nodes connect computation operation use. decisions construct layer convolutional model. decision previous nodes connect allows model form skip connections speciﬁcally layer mutually distinct previous indices sampled leading possible decisions layer provide illustrative example sampling convolutional network figure example layer controller samples previous indices outputs layers concatenated along depth dimension sent layer meanwhile decision computation operation sets particular layer convolution average pooling pooing. operations available controller convolutions ﬁlter sizes depthwise-separable convolutions ﬁlter sizes pooling average pooling kernel size recurrent cells operation layer enas convolutional network distinct parameters. making described decisions total times sample network layers. since decisions independent networks experiments resulting search space. possible networks. rather designing entire convolutional network design smaller modules connect together form network figure illustrates design convolutional cell reduction cell architectures designed. discuss enas search architectures cells. utilize enas computational nodes represent computations happen locally cell. node node treated cell’s inputs outputs previous cells ﬁnal network remaining nodes controller make sets decisions previous nodes used inputs current node operations apply sampled nodes. available operations identity separable convolution kernel size average pooling pooling kernel size node previous nodes corresponding operations sampled operations applied previous nodes results added. figure example controller search space convolutional cells. controller’s outputs. search space convolutional cells node node cell’s inputs controller design node node bottom left corresponding edges represent activated connections. bottom right convolutional cell according controller’s sample. reduction cell also realized search space discussed simply sampling computational graph search space applying operations stride reduction cell thus reduces spatial dimensions input factor following zoph sample reduction cell conditioned convolutional cell hence making controller total blocks. finally estimate complexity search space. node controller select nodes previous nodes operations operations. decisions independent possible cells. since independently sample convolutional cell reduction cell ﬁnal size search space experiments search space realize ﬁnal networks making signiﬁcantly smaller search space entire convolutional networks ﬁrst present experimental results employing enas design recurrent cells penn treebank dataset convolutional architectures cifar- dataset. present ablation study asserts role enas discovering novel architectures. dataset settings. penn treebank well-studied benchmark language model. standard pre-processed version dataset also used previous works e.g. zaremba since goal work discover cell architectures employ standard training test process penn treebank utilize post-training techniques neural cache dynamic evaluation additionally collins established models parameters learn store information limit size enas cell parameters. also tune hyper-parameters extensively like melis train multiple architectures select best based validation perplexities like zoph therefore enas advantage compared zoph yang melis improved performance cell’s architecture. training details. controller trained using adam learning rate prevent premature convergence also tanh constant temperature sampling logits controller’s sample entropy reward weighted additionally augment simple transformations nodes constructed recurrent cell highway connections instance instead relu shown example section relu sigmoid denotes elementwise multiplication. shared parameters child models trained using learning rate decayed factor every epoch starting epoch total epochs. clip norm gradient using large learning rate whilst clipping gradient norm small threshold makes updates stable. utilize three regularization techniques regularization weighted variational dropout tying word embeddings softmax weights details appendix results. running single nvidia enas ﬁnds recurrent cell hours. table present performance enas cell well baselines employ post-training processing. enas cell achieves test perplexity existing state-of-the-art achieved mixture softmaxes note apply enas cell. importantly enas cell outperforms perplexity points whilst search process enas terms hours faster. enas cell visualized figure interesting properties. first non-linearities cell either relu tanh even though search space also functions identity sigmoid. second suspect cell local optimum similar observations made zoph randomly pick table test perplexity penn treebank enas baselines. abbreviations recurrent highway network variational dropout; weight tying; weight penalty; averaged weight drop; mixture contexts; mixture softmaxes. nodes switch non-linearity identity sigmoid perplexity increases points. similarly randomly switch relu nodes tanh vice versa perplexity also increases points. third shown figure output enas cell average nodes. behavior similar mixture contexts enas independently discover also learns balance number contexts increases model’s expressiveness depth recurrent cell learns complex transformations dataset. cifar- dataset consists training images test images. standard data pre-processing augmentation techniques i.e. subtracting channel mean dividing channel standard deviation centrally padding training images randomly cropping back randomly ﬂipping horizontally. training details. shared parameters trained nesterov momentum learning rate follows cosine schedule lmax lmin tmul architecture search epochs. initialize initialization also apply weight decay train architectures recommended controller using settings. policy parameters initialized uniformly trained adam learning rate similar procedure section apply tanh constant temperature controller’s logits controller entropy reward weighted additionally macro search space enforce sparsity skip connections adding reward divergence between skip connection probability layers chosen probability represents prior belief skip connection formed. divergence term weighted training details appendix results. table summarizes test errors enas approaches. table ﬁrst block presents results densenet highest-performing architectures designed human experts. trained strong regularization technique shake-shake data augmentation technique cutout densenet impressively achieves test error second block table presents performances approaches attempt design entire convolutional network along number gpus time methods take discover ﬁnal models. shown enas ﬁnds network architecture visualize figure achieves test error. test error better error achieved second best model keep architecture increase number ﬁlters network’s highest layer test error decreases away nas’s best model whose test error impressively enas takes hours architecture reducing number table classiﬁcation errors enas baselines cifar-. table ﬁrst block presents densenet state-ofthe-art architectures designed human experts. second block presents approaches design entire network. last block presents techniques design modular cells combined build ﬁnal network. third block table presents performances approaches attempt design modules connect together form ﬁnal networks. enas takes hours discover convolution cell reduction cell visualized figure convolutional cell replicated times enas achieves test error error nasnet-a cutout enas’s error decreases compared nasnet-a. replace separable convolutions normal convolutions adjust model size number parameters stay same test error increases similarly randomly change several connections cells enas ﬁnds micro search space test error increases behavior also observed enas searches recurrent cells well zoph thus believe controller learned enas good controller learned performance enas fact sample multiple architectures trained controller train them select best architecture validation data. extra step beneﬁts nas’s performance. disabling enas search. addition random search attempt train shared parameters without updating controller. conduct study macro search space effect untrained random controller similar dropout rate skip connections drop-path operations convergence model error rate validation ensemble monte carlo conﬁgurations trained model reach test error. therefore conclude appropriate training enas controller crucial good performance. growing interest improving efﬁciency nas. concurrent work promising ideas using performance prediction using iterative search method architectures growing complexity using hierarchical representation architectures table shows enas signiﬁcantly efﬁcient methods hours. enas’s design sharing weights architectures inspired concept weight inheritance neural model evolution additionally enas’s choice representing computations using inspired concept stochastic computational graph introduces nodes stochastic outputs computational graph. enas’s utilizes stochastic decisions network make discrete architectural decisions govern subsequent computations network trains decision maker i.e. controller ﬁnally harvests decisions derive architectures. closely related enas smash designs architecture uses hypernetwork generate weight. usage hypernetwork smash inherently restricts weights smash’s child architectures low-rank space. hypernetwork generates weights smash’s child models tensor products suffer low-rank restriction arbitrary matrices always inequality rank {rank rank}. limit smash architectures perform well restricted low-rank space weights rather architectures perform well normal training setups weights longer restricted. meanwhile enas allows weights child models arbitrary effectively avoiding restriction. suspect question regarding enas’s importance whether enas actually capable ﬁnding good architectures design search spaces leads enas’s strong empirical performance. comparing guided random search. uniformly sample recurrent cell entire convolutional network pair convolutional reduction cells search spaces train convergence using settings architectures found enas. macro space entire networks sample skip connections activation probability effectively balancing enas’s advantage divergence term reward random recurrent cell achieves test perplexity penn treebank worse enas’s perplexity random convolutional network reaches test error random cells reache cifar- reason behind enas’s superior empirical performance smash. addition seen experiments enas ﬂexibly applied multiple search spaces disparate domains e.g. space cells text domain macro search space entire networks micro search space convolutional cells image domain. important advance automatizes designing process neural networks. however nas’s computational expense prevents widely adopted. paper presented enas novel method speeds terms hours. enas’s contribution sharing parameters across child models search architectures. insight implemented searching subgraph within larger graph incorporates architectures search space. showed enas works well cifar- penn treebank datasets. authors want thank jaime carbonell zihang lukasz kaiser azalia mirhoseini ashwin paranjape daniel selsam xinyi wang suggestions improving paper.", "year": 2018}