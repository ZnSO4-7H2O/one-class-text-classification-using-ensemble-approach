{"title": "Robust, Deep and Inductive Anomaly Detection", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "PCA is a classical statistical technique whose simplicity and maturity has seen it find widespread use as an anomaly detection technique. However, it is limited in this regard by being sensitive to gross perturbations of the input, and by seeking a linear subspace that captures normal behaviour. The first issue has been dealt with by robust PCA, a variant of PCA that explicitly allows for some data points to be arbitrarily corrupted, however, this does not resolve the second issue, and indeed introduces the new issue that one can no longer inductively find anomalies on a test set. This paper addresses both issues in a single model, the robust autoencoder. This method learns a nonlinear subspace that captures the majority of data points, while allowing for some data to have arbitrary corruption. The model is simple to train and leverages recent advances in the optimisation of deep neural networks. Experiments on a range of real-world datasets highlight the model's effectiveness.", "text": "abstract. classical statistical technique whose simplicity maturity seen widespread anomaly detection. however limited regard sensitive gross perturbations input seeking linear subspace captures normal behaviour. ﬁrst issue dealt robust variant explicitly allows data points arbitrarily corrupted; however resolve second issue indeed introduces issue longer inductively anomalies test set. paper addresses issues single model robust autoencoder. method learns nonlinear subspace captures majority data points allowing data arbitrary corruption. model simple train leverages recent advances optimisation deep neural networks. experiments range real-world datasets highlight model’s eﬀectiveness. common need analysing real-world datasets determining instances stand dramatically dissimilar others. instances known anomalies goal anomaly detection determine instances data-driven fashion anomalies caused errors data sometimes indicative previously unknown underlying process; fact hawkins deﬁnes outlier observation deviates signiﬁcantly observations arouse suspicion generated diﬀerent mechanism. principal component analysis core method range statistical inference tasks including anomaly detection. basic idea many data sets high-dimensional tend inhabit lowdimensional manifold. thus operates projecting data lower-dimensional space separate signal noise; data point away projection deemed anomalous. point completely change orientation projection often leading masking anomalies. variant known robust limits impact anomalies using clever decomposition data matrix discuss rpca detail section note still carries linear projection cannot used make predictions test instances; cannot perform inductive anomaly detection. paper relax linear projection limitation rpca using deep robust autoencoder diﬀerence rpca deep autoencoder nonlinear activation function potential several hidden layers autoencoder. modiﬁcation conceptually simple show yields noticeable improvements anomaly detection performance complex real-world image data linear projection cannot capture suﬃcient structure data. further robust autoencoder capable performing inductive anomaly detection unlike rpca. sequel provide overview anomaly detection methods speciﬁc emphasis matrix decomposition techniques robust extensions. proceed describe proposed model based autoencoders present experiment setup results finally describe directions future work points number features point. example could number images photo collection number pixels used represent image. goal anomaly detection determine rows anomalous sense dissimilar rows. denote anomaly detection widely researched topic data mining machine learning community primary strands research design novel algorithms detect anomalies design eﬃcient means discovering anomalies large dataset. latter strand starting work schwabacher several optimisations proposed discover anomalies near linear time former strand primary focus emphasis non-parametric methods like distance density based outliers example distance-based methods deﬁne domain-dependent dissimilarity metric deem point anomalous relatively away neighbours another popular approach one-class learns smooth boundary captures majority probability mass data recent years matrix factorization methods anomaly detection measure anomalous particular point reconstruction close input deemed normal; else anomalous. describe several popular examples approach beginning principal component analysis here reconstruction matrix xuut rd×k number latent dimensions interpret projection k-dimensional subspace application inverse projection back original dimensional space. intuitively equation separates signal matrix noise matrix signal matrix low-rank structure noise assumed overwhelm signal matrix entries. trace norm before matrix captures anomalies captures signal. unlike rpca explicitly constraints low-rank rather merely trace norm; explicitly constraints maximal number nonzeros rather merely bounded norm. lack convexity objective requires bespoke algorithm optimisation. another overcome linear assumption robust kernel approach feature mapping reproducing kernel hilbert space projection operator point kpca subspace robust measure reconstruction error tuning parameter. rkpca explicitly handle gross outliers unlike rpca; however choosing rich feature mapping capture nonlinear anomalies. choice feature mapping must pre-speciﬁed whereas autoencoder methods implicitly learn good mapping. form robust autoencoder encodes input latent representation decoded additional term captures gross outliers data robust pca. purposes anomaly detection reconstruction matrix model reduces standard autoencoder possible solution generic predictor parameters regularisation function. observe could pick convolutional autoencoder would suitable dealing image data; model studied extensively experiments. further regulariser could involve general matrix norms norm objective function model equation non-convex unconstrained sub-diﬀerentiable. several ways performing optimisation. example diﬀerentiable activation could compute subgradients respect model parameters apply backpropagation. however leverage existing advances training deep networks observe that thus alternately optimise change overall objective threshold. stochastic optimisation ﬁrst step simplicity optimisation second step means easily train model data arrives online streaming fashion. convenient property model anomaly detector inductive i.e. generalise unseen data points. interpret model learning robust representation input unaﬀected gross noise; representation thus able accurately model unseen points manifold data used train model. formally given simply computes score point. larger likely point deemed anomalous. emphasise inductive predictions simply possible robust method estimates parameters robust autoencoder equation clear conceptual similarity robust seem choices penalty somearbitrarily used place trace norm. show objective fact naturally derived extension rpca. natural context standard since satisfying observe derived equation linear activation function robust autoencoder thus extends model employing nonlinear activation. contribution nonlinear extension rpca anomaly detection. noted above advantages rpca ability capture nonlinear structure data well ability detect anomalies inductive setting. price lack convexity objective function unlike rpca; nonetheless shall demonstrate model eﬀectively trained using procedure described section works employed deep networks anomaly detection without explicitly accounting gross anomalies. example recent work employed autoencoder-inspired objective train probabilistic neural network extensions structured data; rpca-style noise matrix useful explore conjunction methods. method also distinct denoising autoencoders wherein noise explicitly added instances whereas infer noise automatically. approaches slightly diﬀerent goals dnas extract good features data identify anomalies. section show empirical eﬀectiveness robust convolutional autoencoder state-of-the-art methods real-world data. primary focus non-trivial image datasets although method applicable context autoencoders useful e.g. speech. truncated zero-mean features equivalent pca. robust equation robust kernel equation autoencoder equation convolutional autoencoder convolutional autoencoder with robust convolutional autoencoder proposed model restaurant comprising video background modelling activity detection usps comprising usps handwritten digits cifar- consisting colour images classes anomaly detection unsupervised learning problem model evaluation challenging. restaurant dataset ground truth anomalies perform qualitative analysis visually comparing anomalies ﬂagged various methods done original robust paper datasets follow standard protocol wherein anomalies explicitly identiﬁed training set. evaluate predictive performance method measured ground truth anomaly labels using three standard metrics auprc auroc measure ranking performance former preferred class imbalance measures classiﬁcation performance fraction scored instances actually anomalous. https//github.com/raghavchalapathy/rcae http//perception.csl.illinois.edu/matrix-rank/sample_code.html http//www.cs.stonybrook.edu/~minhhoai/downloads.html dogs images cats; good anomaly detection method thus cats anomalous. similarly usps dataset created mixture images images ‘’as details datasets summarised table although observed deeper rcae networks tend achieve better image reconstruction performance exist four fold options related network parameters chosen number convolutional ﬁlters ﬁlter size strides convolution operation activation applied. tuned grid search additional hyper-parameters including number hidden-layer restaurant dataset work restaurant video activity detection dataset consider problem inferring background videos removal foreground pixels. estimating background videos important tasks anomalous activity detection. however diﬃcult variability background presence foreground objects moving objects people. parameter settings. rpca rank used. success batch normalization architecture exponential linear units found convolutional+batch-normalization+elu layers provide better representation convolutional ﬁlters. hence experiment rcae adopts four layers encoder part four layers decoder portion network. rcae network parameters chosen ﬁrst second layers third fourth layers encoder decoder layers. results. ground truth anomalies dataset qualitative analysis reveals rcae outperforms counterparts capturing foreground objects. figure compares anomalous images rcae rpca. anomalous images contain high foregound activity visually background reconstruction produced rpca contains blemishes cases rcae backgrounds smooth. usps dataset usps handwritten digit dataset create dataset mixture images images intuitively latter images treated anomalous corresponding images table comparison baseline state-of-the-art systems results mean standard error performance metrics random training draws. highlighted cells indicate best performer. results. table near certainty accurately identiﬁed outliers. figure shows anomalous images rcae indeed correctly placed list. contrast rpca also placed top. cifar- dataset create dataset anomalies combining random images dogs images cats illustrated figure scenario cats anomalies goal detect cats unsupervised manner. parameter settings. rpca methods rank used. trained three-hidden-layer autoencoder middle hidden layer size rank model trained using adam decoding layer uses sigmoid function order capture nonlinearity characteristics latent representations produced hidden layer. finally obtain feature vector image obtaining latent representation hidden layer. parameter requested kpca spectrum rkpca runtime prohibitive full sample resorted subsample dogs cats. rcae architecture experiment restaurant containing four layers encoder part four layers decoder portion network. rcae network parameters chosen ﬁrst second layers third fourth layers encoder decoder. figure illustrates anomalous images rcae method compared rpca. owing latter involving learning linear subspace model unable eﬀectively distinguish cats dogs; contrast rcae eﬀectively determine manifold characterising dogs identiﬁes cats anomalous respect this. conduct experiment assess detection inductive anomalies. recall capability rcae model e.g. rpca. consider following setup train model images evaluate test comprising dogs images. before wish methods accurately determine cats anomalies. table summarises detection performance methods inductive task. lower values compared table indicative problem challenging anomaly detection single dataset; nonetheless rcae method manages convincingly outperform baselines. conﬁrmed qualitatively figure rcae correctly identiﬁes many cats test anomalous basic method suﬀers. finally test ability model de-noise images form anomaly detection individual pixels experiment train models images dogs cifar-. image salt-and-pepper noise rate goal recover original image accurately possible. figure illustrates anomalous images presence noise contain images variations class images further figure illustrates various methods mean square error reconstructed original images. rcae eﬀectively suppresses noise evident error. improvement modest suggests beneﬁt explicitly accounting noise. fig. illustration mean square error boxplots obtained various models image denoising task cifar- dataset. setting rcae suppresses noise detects background foreground images eﬀectively. minute train rpca minutes train rkpca compared minutes rcae method. ability leverage recent advances deep learning part optimisation believe salient feature approach. note rkpca method fast train smaller datasets larger datasets suﬀers complexity kernel methods; example takes hour train cifar- dataset. plausible could leverage recent advances fast approximations kernel methods studying would interest future work. note issue using ﬁxed kernel function would remain however. extended robust model nonlinear autoencoder setting. best knowledge ﬁrst approach robust nonlinear inductive. robustness ensures model over-sensitive anomalies; nonlinearity helps discover potentially subtle anomalies; inductive makes possible deploy model live setting. autoencoders powerful mechansim data representation suﬀer black-box nature. growing body research outlier description i.e. explain reason data point anomalous. direction future reason extend deep autoencoders outlier description.", "year": 2017}