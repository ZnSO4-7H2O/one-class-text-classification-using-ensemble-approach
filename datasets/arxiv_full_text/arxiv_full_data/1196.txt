{"title": "Learning to Compare Image Patches via Convolutional Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "In this paper we show how to learn directly from image data (i.e., without resorting to manually-designed features) a general similarity function for comparing image patches, which is a task of fundamental importance for many computer vision problems. To encode such a function, we opt for a CNN-based model that is trained to account for a wide variety of changes in image appearance. To that end, we explore and study multiple neural network architectures, which are specifically adapted to this task. We show that such an approach can significantly outperform the state-of-the-art on several problems and benchmark datasets.", "text": "software) large datasets contain patch correspondences images begs following question make proper datasets automatically learn similarity function image patches goal paper afﬁrmatively address question. thus able generate patch similarity function scratch i.e. without attempting manually designed features instead directly learn function annotated pairs image patches. inspired also recent advances neural architectures deep learning choose represent function terms deep convolutional neural network also interested addressing issue network architecture best used task like this. thus explore propose various types networks architectures exhibit different trade-offs advantages. cases train networks using sole input large database contains pairs image patches allows improve performance method simply enriching database samples paper show learn directly image data general similarity function comparing image patches task fundamental importance many computer vision problems. encode function cnn-based model trained account wide variety changes image appearance. explore study multiple neural network architectures speciﬁcally adapted task. show approach signiﬁcantly outperform state-ofthe-art several problems benchmark datasets. comparing patches across images probably fundamental tasks computer vision image analysis. often used subroutine plays important role wide variety vision tasks. range low-level tasks structure motion wide baseline matching building panoramas image super-resolution higher-level tasks object recognition image retrieval classiﬁcation object categories mention characteristic examples. course problem deciding patches correspond quite challenging exist many factors affect ﬁnal appearance image include changes viewpoint variations overall illumination scene occlusions shading differences camera settings etc. fact need comparing patches given rise development many hand-designed feature descriptors past years including sift huge impact computer vision community. manually designed descriptors unable take account optimal manner aforementioned factors determine appearance patch. hand nowadays easily gain access explore propose variety different neural network models adapted representing function highlighting time network architectures offer improved performance. apply approach several problems benchmark datasets showing signiﬁcantly outperforms state-of-the-art leads feature descriptors much better performance manually designed descriptors learnt descriptors importantly convolutional nature resulting descriptors efﬁcient compute even dense manner. conventional approach compare patches descriptors squared euclidean distance. feature descriptors hand-crafted sift daisy recently methods learning descriptor proposed simonyan proposed convex procedure training tasks. approach however inspired recent success convolutional neural networks although models involve highly non-convex objective function training shown outstanding results various tasks fischer analysed performance convolutional descriptors alexnet network wellknown mikolajczyk dataset showed convolutional descriptors outperform sift cases except blur. also proposed unsupervised training approach deriving descriptors outperform sift imagenet trained network. zbontar lecun recently proposed cnn-based approach compare patches computing cost small baseline stereo problem shown best performance kitti dataset. however focus work comparing pairs consist small patches like ones narrow baseline stereo. contrast similarity function account broader appearance changes used much wider challenging applications including e.g. wide baseline stereo feature matching image retrieval. already mentioned input neural network considered pair image patches. models impose limitations respect number channels input patches i.e. given dataset colour patches networks could trained increase performance. however able compare approach state-of-the-art methods existing datasets chose grayscale patches training. furthermore exception model described section cases patches given input network assumed ﬁxed size several ways patch pairs processed network information sharing take place case. reason explored tested variety models. start section describing three basic neural network architectures studied i.e. -channel siamese pseudo-siamese offer different trade-offs terms speed accuracy essentially architectures stem different attempts address following question composing similarity function comparing image patches ﬁrst choose compute descriptor patch create similarity descriptors perhaps choose skip part related descriptor computation directly proceed similarity estimation? addition basic models also describe section extra variations concerning network architecture. variations mutually exclusive other used conjunction basic models described section overall leads variety models possible used task comparing image patches. siamese type network resembles idea descriptor branches network share exactly architecture weights. branch takes input patches applies series convolutional relu max-pooling layers. branch outputs concatenated given network consists linear fully connected relu layers. tests used network consisting linear fully connected layers separated relu activation layer. branches siamese network viewed descriptor computation modules network similarity function. task matching sets patches test time descriptors ﬁrst computed independently using branches matched network figure three basic network architectures -channel left siamese pseudo-siamese right color code used cyan conv+relu purple pooling yellow fully connected layer ture considered in-between siamese -channel networks. speciﬁcally structure siamese described except weights branches uncoupled i.e. shared. increases number parameters adjusted training provides ﬂexibility restricted siamese network much -channel network described next. hand maintains efﬁciency siamese network test time. -channel unlike previous models direct notion descriptor architecture. simply consider patches input pair -channel image directly ﬁrst convolutional layer network. case bottom part network consists series convolutional relu maxpooling layers. output part given input module consists simply fully connected linear decision layer output. network provides greater ﬂexibility compared models starts processing patches jointly. furthermore fast train general test time expensive requires combinations patches tested brute-force manner. deep network. apply technique proposed simonyan zisserman advising break bigger convolutional layers smaller kernels separated relu activations supposed increase nonlinearities inside network make decision function discriminative. also report might difﬁcult initialise network however observe behavior train network scratch usual. case applying technique model convolutional part ﬁnal architecture turns consist convolutional layer convolutional layers layers separated relu activations. shall also later experimental results change network architecture contribute improving performance accordance analogous observations made central-surround two-stream network. name suggests proposed architecture consists separate streams central surround enable processing spatial domain takes place different resolutions. speciﬁcally central high-resolution stream receives input patches generetad cropping central part input patch. furthermore surround low-resolution stream receives input patches generated downsampling half original pair input patches. resulting streams processed using basic architectures described section reason make two-stream architecture multi-resolution information known important improving performance image matching. furthermore considering central part patch twice implicitly focus pixels closer center patch less focus pixels periphery also help improving precision matching note total input dimenionality reduced factor case. result training proceeds faster also asgd constant learning rate momentum weight decay used train models. training done mini-batches size weights initialised randomly models trained scratch. combat overﬁtting augment training data ﬂipping patches pairs horizontally vertically rotating degrees. don’t notice overﬁtting training manner train models certain number iterations usually days test performance test set. training dataset size allows store images directly memory efﬁciently retrieve patch pairs training. images augmented on-the titan torch convolution routines taken nvidia cudnn library siamese descriptors times slower computing sift descriptors times faster imagenet descriptors according ﬁrst evaluation models used standard benchmark dataset consists three subsets yosemite notre dame liberty contains image patches sampled around difference gaussians feature points. patches scale orientation normalized. subsets generated using actual correspondences obtained multi-view stereo depth maps. maps used produce ground-truth feature pairs dataset equal number positive negative matches. evaluating models evaluation protocol generate curves thresholding distance feature pairs descriptor space. report false positive rate recall combinations training test sets well mean across combinations. also report mean denoted mean combinations figure network siamese architecture layers inserted immediately branches network decision layer input ﬁxed dimensionality size input patches. spatial pyramid pooling network comparing patches. point assuming network requires input patches ﬁxed size requirement comes fact output last convolutional layer network needs predeﬁned dimensionality. therefore need compare patches arbitrary sizes means ﬁrst resize spatial dimensions. however look example descriptors like sift instance another possible deal patches arbitrary sizes adjusting size spatial pooling regions proportional size input patch still maintain required ﬁxed output dimensionality last convolutional layer without deteriorating resolution input patches. also idea behind recently proposed sppnet architecture essentially amounts inserting spatial pyramid pooling layer convolutional layers fully-connected layers network. layer aggregates features last convolutional layer spatial pooling size pooling regions dependent size input. inspired this propose also consider adapting network models section according spp-architecture. easily achieved considered models table reports performance several models also details architecture brieﬂy summarize conclusions drawn table. ﬁrst important conclusion -channel-based architectures exhibit clearly best performance among models. something indicates important jointly information patches right ﬁrst layer network. ch-stram network top-performing network dataset ch-deep following closely fact ch-stream managed outperform previous state-of-the-art large margin achieving times better score difference sift even larger model giving times better score case according regarding siamese-based architectures manage achieve better performance existing state-ofthe-art systems. quite interesting because e.g. none siamese networks tries learn shape size placement pooling regions instead utilizes standard max-pooling layers. among siamese models two-stream network best performance verifying importance multi-resolution information comes comparing image patches. furthermore pseudo-siamese network better corresponding siamese tested performance siamese models decision layer replaced euclidean distance convolutional descriptors produced branches network case prior applying euclidean distance descriptors l-normalized pseudo-siamese branch used extract descriptors. expected case two-stream network computes better distances siamese network which turn computes better distances pseudo-siamese model fact siam-stream-l network manages outperform even previous state-of-the-art descriptor quite surprising given siamese models never trained using distances. provide corresponding curves fig. furthermore show table performance imagenettrained features among these conv gives best score equal makes better sift still much worse models. fig. displays ﬁlters ﬁrst convolutional layer learnt siamese network. furthermore fig. shows left right parts subset ﬁrst layer ﬁlters -channel network worth mentioning corresponding left right parts look like negative other basically means network learned compute differences features between patches last show fig. ranking false correct matches computed ch-deep network. observe false matches could easily mistaken even human table performance several models local image patches benchmark. models architecture follows ch-stream consists branches c-relu-p-c-relu-p-c-relu-c-relu central surround parts followed f-relu-f ch-deep c-stack-p-stack-f stack c-relu-c-relu-c-relu. c-relu-p-c-relup-c-relu-f-relu-f siam branches c-relu-p-c-relu-pc-relu decision layer f-relu-f siam-l reduces single branch siam pseudo-siam uncoupled version siam pseudo-siam-l reduces single branch pseudo-siam siam-stream branches c-relup-c-relu-c-relu-c-relu decision layer f-relu-f siam-stream-l consists central surround branch siam-stream. shorthand notation used following convolutional layer ﬁlters spatial size applied stride max-pooling layer size applied stride denotes fully connected linear layer output units. evaluation chose dataset strecha contains several image sequences ground truth homographies laser-scanned depthmaps. used fountain herzjesu sequences produce rectiﬁed stereo pairs respectively. baselines sequences chose increasing image making matching difﬁcult. goal show photometric cost computed neural network competes favorably costs produced state-ot-the-art hand-crafted feature descriptor chose compare daisy depth maps produced -channel architectures. results without global optimization also show estimated depth maps contain much details daisy. exhibit sparse errors case siamese-based networks errors easily eliminated global optimization. fig. also shows quantitative comparison focusing case siamese-based models efﬁcient. ﬁrst plot ﬁgure shows distribution deviations ground truth across range error thresholds furthermore plots ﬁgure summarize corresponding distributions errors stereo pairs increasing baseline error thresholds pixels plots seen siamese models perform much better daisy across error thresholds baseline distances also test networks mikolajczyk dataset local descriptors evaluation dataset consists images sequences camera viewpoint changes blur compression lighting changes zoom gradually increasing amount transfomation. known ground truth homographies ﬁrst image sequence. testing technique brieﬂy test pair images detectors applied images extract keypoints. following mser detector. ellipses provided detector used exctract patches input images. ellipse size magniﬁed factor include context. then depending type network either descriptors meaning outputs siamese pseudo-siamese branches extracted patch pairs given -channel network assign score. quantitative comparison dataset shown several models fig. also test network siam-spp-l spp-based siamese architecture used inserted layer spatial dimension seen provides boost matching performance suggesting great utility architecture comparing image patches. regarding rest models observed results fig. reconﬁrm conclusions already drawn previous experiments. simply note good performance siam-stream-l compute network outputs resulting cost worth noting costs computed efﬁciently using speed optimizations similar essentially means treating fully connected layers convolutions computing branches siamese network once furthermore computing outputs branches well ﬁnal outputs network locations using number forward passes full images minimized using algorithm based fastpd show fig. qualitative results terms computed depth maps fountain image global optimization results visually verify photometric cost computed neural network much robust hand-crafted features well high quality figure quantitative comparison wide-baseline stereo fountain dataset. distribution deviations ground truth expressed fraction scene’s depth range. distribution errors stereo pairs increasing baseline without taking account occluded pixels paper showed learn directly image pixels general similarity function patches encoded form model. studied several neural network architecures speciﬁcally adapted task showed exhibit extremely good performance signiﬁcantly outperforming state-of-the-art several problems benchmark datasets. among architectures note -channel-based ones clearly superior terms results. therefore worth investigating accelerate evaluation networks future. regarding siamese-based architectures -stream multi-resolution models turned extremely strong providing always signiﬁcant boost performance verifying importance multi-resolution information comparing patches. conclusion applies spp-based siamese networks also consistently improved quality results. figure evaluation mikolajczyk dataset showing mean average precision averaged types transformations dataset detailed plots provided supplemental material lack space. razavian azizpour sullivan carlsson. features off-the-shelf astounding baseline ieee conference computer vision recognition. pattern recognition cvpr workshops columbus june pages simonyan vedaldi zisserman. learning local feature descriptors using convex optimisation. ieee transactions pattern analysis machine intelligence", "year": 2015}