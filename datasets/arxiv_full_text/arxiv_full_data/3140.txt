{"title": "Separable Dictionary Learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Many techniques in computer vision, machine learning, and statistics rely on the fact that a signal of interest admits a sparse representation over some dictionary. Dictionaries are either available analytically, or can be learned from a suitable training set. While analytic dictionaries permit to capture the global structure of a signal and allow a fast implementation, learned dictionaries often perform better in applications as they are more adapted to the considered class of signals. In imagery, unfortunately, the numerical burden for (i) learning a dictionary and for (ii) employing the dictionary for reconstruction tasks only allows to deal with relatively small image patches that only capture local image information. The approach presented in this paper aims at overcoming these drawbacks by allowing a separable structure on the dictionary throughout the learning process. On the one hand, this permits larger patch-sizes for the learning phase, on the other hand, the dictionary is applied efficiently in reconstruction tasks. The learning procedure is based on optimizing over a product of spheres which updates the dictionary as a whole, thus enforces basic dictionary properties such as mutual coherence explicitly during the learning procedure. In the special case where no separable structure is enforced, our method competes with state-of-the-art dictionary learning methods like K-SVD.", "text": "many techniques computer vision machine learning statistics rely fact signal interest admits sparse representation dictionary. dictionaries either available analytically learned suitable training set. analytic dictionaries permit capture global structure signal allow fast implementation learned dictionaries often perform better applications adapted considered class signals. imagery unfortunately numerical burden learning dictionary employing dictionary reconstruction tasks allows deal relatively small image patches capture local image information. approach presented paper aims overcoming drawbacks allowing separable structure dictionary throughout learning process. hand permits larger patch-sizes learning phase hand dictionary applied efﬁciently reconstruction tasks. learning procedure based optimizing product spheres updates dictionary whole thus enforces basic dictionary properties mutual coherence explicitly learning procedure. special case separable structure enforced method competes state-of-the-art dictionary learning methods like k-svd. exploiting fact signal sparse representation dictionary rn×d backbone many successful signal reconstruction data analysis algorithms. sparse representation means linear combination columns referred atoms. formally reads transform coefﬁcient vector sparse i.e. entries zero small magnitude. performance algorithms exploiting model crucial dictionary allows signal interest represented accurately coefﬁcient vector sparse possible. basically dictionaries assigned classes analytic dictionaries learned dictionaries. analytic dictionaries built mathematical models general type signal represent. used universally allow fast implementation. popular examples include wavelets bandlets curvlets among several others. well known learned dictionaries yield sparser representation analytic ones. given representative training signals dictionary learning algorithms ﬁnding dictionary training admits maximally sparse representation. formally rn×m matrix containing training samples arranged columns rd×m contain therein rd×m function promotes sparsity reﬂects noise power predeﬁned admissible solutions. common dictionary learning approaches employing optimization problems related include probabilistic ones like clustering based ones k-svd comprehensive overview. dictionaries produced techniques unstructured matrices allow highly sparse representations signals interest. however dimension signals sparsely represented consequently possible dictionaries’ dimensions inherently restricted limited memory limited computational resources. furthermore used within signal reconstruction algorithms many matrix vector multiplications performed dictionaries computationally expensive apply. paper present method learning dictionaries efﬁciently applicable reconstruction tasks. crucial idea allow dictionary separable structure separable means dictionary given kronecker product smaller dictionaries rh×a rw×b i.e. relation signal sparse representation given accordingly vecb) vector space isomorphism ra×b deﬁned operation stacks columns other. employing separable structure instead full unstructured dictionary clearly reduces computational costs learning algorithm computational burden reduces apparent approach applies principle class signals. however focus signals inherently dimensional structure images. however worth mentioning sedil straightforwardly extended signals higher dimensional structure volumetric d-signals employing multiple kronecker products. notation rest work above dimensional signal rh×w sparse representation ra×b i.e. axb. proposed dictionary learning scheme sedil based adaption problem product unit spheres. furthermore incorporates regularization term allows control dictionary’s mutual coherence. arising optimization problem solved riemannian conjugate gradient method combined nonmonotone line search. general separable case method able learn dictionaries large patch dimensions conventional learning techniques fail deﬁne sedil yields algorithm learning standard unstructured dictionaries. denoising experiment given shows performance separable non-separable dictionary learned sedil -dimensional image patches. experiment seen separable dictionary outperforms analytic counterpart overcomplete discrete cosine transform non-separable achieves similar performance state-of-the-art learning methods like k-svd. besides that show learned separable dictionary able extract recover global information contained training data separable dictionary learned face database face image resolution pixels. dictionary applied face inpainting experiment large missing regions recovered solely based information contained dictionary. instead learning dense unstructured dictionaries costly apply reconstruction tasks unable deal high dimensional signals techniques exist learning dictionaries bypass limitations. following shortly review existing techniques focus learning efﬁciently applicable high dimensional dictionaries followed introducing approach. different algorithms proposed following idea ﬁnding dictionary atoms sparse ﬁxed analytic base dictionary. algorithm proposed enforces atom ﬁxed number non-zero coefﬁcients suggested imposes less restrictive constraint enforcing sparsity entire dictionary. however algorithms employ optimization problems capable ﬁnding large dictionary high dimensional signals. alternative structure dictionaries proposed. called signature dictionary small image itself every patch varying locations size possible dictionary atom. advantages structure include near-translation-invariance reduced overﬁtting less memory computational requirements compared unstructured dictionary approaches. however small number parameters model also makes dictionary restrictive structures. approach extended learn real translational-invariant atoms. hierarchical frameworks tackling high dimensional dictionary learning presented latter work uses framework conjunction screening technique random projections. like mention approach potential combined hierarchical frameworks. learning separable dictionary given training samples rh×w×m solving problem related denote collection sparse representations measure overall sparsity constraint commonly employed various dictionary learning procedures avoid scale ambiguity problem i.e. entries tend inﬁnity entries tend zero global minimizer unconstrained sparsity measure matrices normalized columns admit manifold structure known product spheres denote soft constraint requiring moderate mutual coherence dictionary well known regularization procedure dictionary learning motivated compressive sensing theory. roughly speaking mutual coherence measures similarity dictionary’s atoms value exposes dictionary’s vulnerability closely related columns confuse pursuit technique. common mutual coherence measure dictionary normalized columns rest paper follow notation denote column matrix corresponding lower case character order relax worst case measure measures introduced literature suited practical purpose example averaging largest entries considering squares elements work introduce alternative mutual coherence measure proven extremely useful experiments. explicitly measure mutual coherence since measure differentiable integrated smooth optimization procedures. furthermore used within dictionary learning scheme log-barrier function avoids algorithm producing dictionaries contain repeated identical atoms. proof. first notice since columns unit norm diagonal entries equal mutual coherence given largest off-diagonal absolute value respectively. analogously largest off-diagonal absolute value matrix deﬁnition kronecker product unit diagonal entry reappears off-diagonal entries yields inequalities combined therein weighs sparsity accurately axjb reproduces training samples. using parameter sedil handle perfect noise free training data well noisy training data. second weighting factor controls mutual coherence learned dictionary. knowing feasible solutions problem restricted smooth manifold allows apply methods ﬁeld geometric optimization learn dictionary. provide necessary notation shortly recall required concepts optimization matrix manifolds. in-depth introduction optimization matrix manifolds refer interested reader every point assign tangent space real vector space containing possible directions tangentially pass element called tangent vector tangent space associated inner product inherited surrounding euclidean space denote corresponding norm riemannian gradient element tangent space points direction steepest ascent cost function manifold. case globally deﬁned entire surrounding euclidean space riemannian gradient simply orthogonal projection gradient onto tangent space reads geometric optimization method proposed work based iterating following line search scheme. given iterate search direction step size iteration iterate lying found following concretize concepts situation hand present ingredients necessary implement proposed geometric dictionary learning method. given formulas regarding geometry derived e.g. considering product manifold ra×b×m riemannian submanifold ra×b×m×rh×a×rw×b element denoted tangent space given product structure tangent space point simply product individual tangent spaces i.e. ra×b×m consequently accordance equation orthogonal projection arbitrary point ra×b×m rh×a rw×b onto tangent space ﬁnal required ingredient compute geodesics. general closed form solution problem ﬁnding certain geodesic case hand allows efﬁcient implementation. point sphere tdsn− tangent vector geodesic direction great circle solve optimization problem employ geometric conjugate gradient method offers superlinear rate convergence still applicable large scale optimization problems acceptable computational complexity. therein initial search direction equal negative riemannian gradient i.e. subsequent iterations linear combination gradient previous search direction since addition vectors different tangent spaces deﬁned need done so-called parallel transport transports tangent vector along geodesic tangent space similar derived closed form solution geodesic consider geometry ﬁrst. parallel transport tangent vector tdsn− along great circle update following hybrid optimization scheme proposed shown excellent performance practice. authors combine hestenes-stiefel dai-yuan update formulas given order appropriate step size propose riemannian adaption nonmonotone line search algorithm proposed like nonmonotone line search schemes potential improve likelihood ﬁnding global minimum well increase convergence speed contrast standard armijo rule standard nonmonotone schemes generally function value previous iterate maximum previous iterates particular method utilizes convex combination function values previous iterations. pseudo code version line search scheme adapted geometric optimization problem found algorithm line search initialized finally complete method learning dictionary separable structure summarized algorithm show dictionaries learned sedil perform real applications present results achieved denoising images corrupted additive white gaussian noise different standard deviation σnoise case study. images noise levels chosen excerpt commonly used literature. peak signal-to-noise ratio ground-truth image recovered image used quantify reconstruction quality. additional quality measure mean structural similarity index computed parameters originally suggested ssim ranges zero meaning perfect image reconstruction. compared psnr ssim better reﬂects subjective visual impression quality. here present denoising performance universal unstructured dictionary i.e. universal separable dictionary learned training data using sedil. universal mean dictionary speciﬁcally learned certain image class universally applicable image content. without loss generality choose square image patches accordance patch-sizes mostly used literature. unstructured dictionary separable choose i.e. equal size dimension unstructured counterpart. training phase extracted image patches four images random positions vectorize them. course images considered within performance evaluations. training patches normalized zero mean unit -norm. figure learned atoms unstructured dictionary separable dictionary patch size atom shown block black pixel corresponds smallest negative entry gray zero entry white corresponds largest positive entry. initialized random matrices normalized columns. global convergence local minimum always observed regardless initialization. weighting parameters empirically shown figure respectively. employing fast iterative shrinkage-thresholding algorithm regularization parameter depends noise level σnoise that clean image patch computed sparse last overlapping image patches taken account several solutions coefﬁcients pixel exist ﬁnal clean image built averaging overlapping image patches. achieved results given table compare rank learned dictionaries among existing state-of-the-art techniques present denoising performance universal dictionary dksvd learned using k-svd training used sedil equal dimension unstructured dictionary table seen employing always yields slightly better denoising results compared employing dksvd. employing separable dictionary leads results slightly worse compared employing unstructured counterpart. tribute paid predeﬁned structure. however separability allows fast implementation popular also separable overcomplete discrete cosine transform here observed separable dictionary learned sedil outperforms odct images requiring exactly computational cost. second advantage besides computational efﬁciency comes along capability learning separable dictionary sedil allows learn sparse representations image patches whose size lets table psnr ssim denoising test images corrupted noise levels. cell presents results respective image noise level different methods left fista+k-svd dictionary right fista+unstructured sedil middle left fista+odct middle right fista+separable sedil bottom bmd. unstructured dictionary learning methods fail numerical reasons. order demonstrate capability sedil domain separable dictionary learned training consisting images dimension showing frontal face views different persons. training images randomly extracted faces cropped labeled faces wild database remaining images used following inpainting experiments. note face positions pictures arbitrary figure exemplary chosen training faces. dimensions resulting matrices parameters required learning procedure chosen above. ability separable dictionary capture global structure training samples illustrated inpainting experiment face images size large regions missing. images course included training set. assume image region ﬁlled given. inpainting procedure conducted applying fista inverse problem excerpt achieved results given figure like mention experiment seen highly sophisticated face inpainting method rather supply evidence sedil able properly extract global information underlying training set. propose dictionary learning algorithms called sedil able learn unstructured dictionaries well dictionaries separable structure. employing separable structure dictionaries reduces figure five exemplary large scale inpainting results. ﬁrst shows original images large regions removed second row. last shows inpainting results achieved sedil. computational complexity compared employing unstructured dictionaries considered signal dimension. this separable dictionaries learned using larger signal dimensions compared used learning unstructured dictionaries applied efﬁciently image reconstruction tasks. another advantage sedil allows control mutual coherence resulting dictionary. therefore introduce mutual coherence measure relation classical mutual coherence. sedil algorithm propose geometric conjugate gradient algorithm exploits underlying manifold structure. numerical experiments image denoising show practicability approach ability learn sparse representations large image-patches demonstrated face inpainting experiment.", "year": 2013}