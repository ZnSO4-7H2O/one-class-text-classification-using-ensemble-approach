{"title": "Encoder Based Lifelong Learning", "tag": ["cs.CV", "cs.AI", "stat.ML"], "abstract": "This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-of-the-art", "text": "main challenge make learned model adapt data similar different environment without losing knowledge previously seen task. classical solutions challenge suffer important drawbacks. feature extraction model representation learned task re-used extract features data without adapting model parameters highly conservative task suboptimal one. fine-tuning adapts model task using optimal parameters task initialization. result model driven towards newly seen data forgets learned previously. joint training method model trained jointly previous current tasks data. solution optimal tasks requires presence data time. requirement hard meet especially data. overcome drawbacks without constraint storing data previously seen tasks main approaches found literature. ﬁrst presented proposes train convolutional networks shared model used different tasks separate classiﬁcation layers. task presented classiﬁcation layer added. then model ﬁnetuned data task additional loss incorporates knowledge tasks. loss tries keep previous task predictions data unchanged. solution reduces forgetting heavily relying task data. consequence suffers build-up errors facing sequence tasks work presented recently tackles problem different way. rather dataoriented analysis consider knowledge gained model itself transfer task another bayesian update fashion. method relies approximating weight distribution training model task. gaussian distribution mean given optimal weights ﬁrst task varipaper introduces lifelong learning solution single model trained sequence tasks. main challenge vision systems face context catastrophic forgetting tend adapt recently seen task lose performance tasks learned previously. method aims preserving knowledge previous tasks learning using autoencoders. task under-complete autoencoder learned capturing features crucial achievement. task presented system prevent reconstructions features autoencoders changing effect preserving information previous tasks mainly relying. time features given space adjust recent environment projection dimension submanifold controlled. proposed system evaluated image classiﬁcation tasks shows reduction forgetting state-ofthe-art. intelligent agents able perform remarkably well individual tasks. however exposed task environment agents retrained. process learn speciﬁcity task tend loose performance tasks learned before. instance imagine agent trained localize defects factory products. then products introduced agent learn detect anomalies products faces risk forgetting initial recognition task. phenomenon known catastrophic forgetting occurs datasets tasks presented goal train single model perform well multiple tasks tasks learned sequentially. problem intersection joint training lifelong learning. standard multi-task learning aims learn jointly data multiple tasks uses inductive bias order integrate knowledge different domains single model. however requires presence data tasks training. lifelong learning scenario hand tasks treated sequential manner. exploit knowledge previous tasks learning one. knowledge used preserve performance previously seen data improve knowledge using inductive bias task data regularizer task beneﬁcial performance. work preserving knowledge previous tasks possibly beneﬁting knowledge learning task without storing data previous tasks. despite potential beneﬁts problem explored. learning without forgetting introduced proposes preserve previous performance knowledge distillation loss introduced consider shared convolutional network different tasks last classiﬁcation layer task speciﬁc. encountering task outputs existing classiﬁcation layers given task data recorded. training outputs preserved modiﬁed cross-entropy loss softens class probabilities order give higher weight small outputs. details loss method found sec. method reduces forgetting especially datasets come related manifolds. nevertheless shown icarl incremental classiﬁer representation learning suffers build errors sequential scenario data comes environment. similarly expertgate shows performance drops model exposed sequence tasks drawn different distributions. proposes store selection previous tasks data overcome issue something avoid. goal obtain experts different tasks suggest model lifelong learning experts individual tasks added network models sequentially. challenge decide expert launch based input. interestingly also undercomplete autoencoders case capture context task based decision made task test sample belongs work build reduce cumulated errors using undercomplete autoencoders learned optimal representaance given diagonal fisher information matrix used approximation. solution based strong principle gives interesting results. however requires store number parameters comparable size model itself. work propose compromise methods. rather heavily relying task data requiring huge amount parameters stored introduce autoencoders tool preserve knowledge task learning another. task undercomplete autoencoder trained training task model. captures important features task objective. facing task autoencoder used ensure preservation important features. achieved deﬁning loss reconstructions made autoencoder explain following sections. manner restrict subset features unchanged give model freedom adapt task using remaining capacity. figure displays model propose use. below ﬁrst give short description relevant related work sec. then describe autoencoders avoid catastrophic forgetting motivate choice short analysis relates proposed objective joint training scheme. sec. describe experiments conducted report discuss results concluding sec. ﬁrst step want understand limitations work suggested replace obtained training network ﬁrst task. suppose model enough capacity integrate knowledge ﬁrst task small generalization error consider however order able compute measure using samples conditions need satisﬁed. terms consider tries learn encoding data target space loss information generated instead function kullback-leibler divergence related probability distributions equivalently cross-entropy. thus data distributions related likely lead high performance. condition relatedness data distributions fails direct guarantee result important loss information ﬁrst task. indeed shown empirically signiﬁcantly different data distributions result signiﬁcant decrease performance lwf. even recently another interesting solution train shared models without access previous data somesimilar spirit work proposed context reinforcement learning. main idea method called elastic weight consolidation constrain weights training second task optimal weights ﬁrst task diagonal terms fisher information matrix. fisher matrix prevents weights important ﬁrst task change much. point view method despite success drawbacks. first method keeps weights neighborhood possible minimizer empirical risk ﬁrst task. however could another solution give better compromise tasks. second needs store large number parameters grows total number weights number tasks. reasons rather constraining weights choose constrain resulting features enforcing important previous tasks change much. constraining sub-manifold features allow weights adjust optimize features task preserving ensure good performance previous tasks. work consider problem training supervised deep model useful multiple tasks situation stage data network come always single task tasks enter training scenario successively. best performance tasks simultaneously achieved network trained data considered tasks time performance course limited capacity used model considered upper bound achieved lifelong learning setting data previous tasks longer accessible learning one. following notations random variables dataset task sampled data samples. access data tasks jointly network training aims control statistical risk figure preservation features important task training task training enforce projection submanifold captures important features stay close projection optimal features ﬁrst task. part meaningful ﬁrst task allowed adjust variations second task. based knowledge distillation loss introduced reduce resulting different distributions. work build method. order make used approximation less sensitive data distributions opportunity controlling mild conditions model functions namely lipschitz continuity control allows instead better approximate ﬁrst task loss note condition continuity observation based restrictive practice. indeed commonly used functions deep models satisfy condition main idea learn submanifold representation space contains informative features ﬁrst task. submanifold identiﬁed projection features onto submanifold change much training second task consequences follow stay informative ﬁrst task training time room adjust second task projection learned submanifold controlled. figure gives simpliﬁed visualization mechanism. next paragraphs propose method learn submanifold informative features given task using autoencoders. features training main idea preserve features informative ﬁrst task giving ﬂexibility features order improve performance second task. autoencoder trained representation ﬁrst task data obtained optimized model used capture important features task. autoencoder neural network trained reconstruct input network operates projection decomposed encoding decoding function. optimal weights usually obtained minimizing mean distance inputs reconstructions. dimension code smaller dimension input autoencoder captures submanifold represents best structure input data. precisely choose two-layer network sigmoid activation hidden layer wdecσ. figure shows general scheme autoencoder. beginning train second task feature extractor model optimized ﬁrst task. feature extraction type approach would keep operator unchanged order preserve performance previous task. however overly conservative usually suboptimal task. rather preserving loss function used train model ﬁrst task data. hyper-parameter controls compromise terms loss. manner autoencoder represents variations needed reconstruct input time contain information required task operator. second constraint rather controlling distance reconstructions constrain codes sub-multiplicity frobenius norm have wdecfσ−σ. advantage using codes lower dimension. codes reconstructions need recorded beginning training second task using codes result better usage memory. choice parameter done model selection. analysis objective given appendix giving bound difference ∗))] statistical risk joint-training setting shows effectively controls bound. training procedure proposed method sec. generalizes easily sequence tasks. autoencoder trained task. even needed memory grow linearly number tasks memory required autoencoder small fraction required global model. example case alexnet base model autoencoder comprises around memory. explain autoencoders start simple case task operator shared among tasks model composed common feature extractor task speciﬁc operator task time task presented model corresponding task operator optimized. however order achieve lifelong learning want also update feature extractor without damaging performance model previously seen tasks. task scenario training ﬁrst task optimized task. then train undercomplete autoencoder using minimizing empirical risk corresponding optimal performance ﬁrst task knowing operator kept equal nevertheless preventing changing lead suboptimal performance second task. idea keep projection manifold represented autoencoder unchanged. second term explicitly enforces represent submanifold needed good performance task thus controlling distance preserve necessary information task undercompleteness encoder projects features lower dimensional manifold controlling distance reconstructions give features ﬂexibility adapt second task variations consider model presented figure part task operator shared among tasks setting used clearly preferrable architecture lifelong learning setting memory increase adding task much lower. main idea start loss used method additional term coming idea presented sec. thus task scenario addition loss used second task propose constraints input shared feature extractor; shared task operator; {tt}t=..t previous task operators; {wenct}t=..t previous task encoders; training data ground truth task //hyper parameters initialization //record data codes training done using stochastic gradient descent autoencoder training also done adaptive gradient method adadelta alleviates need setting learning rates nice optimization properties. algorithm shows main steps proposed method. compare method state-of-the-art several baselines image classiﬁcation tasks. consider sets tasks learned sequentially settings ﬁrst task large dataset ﬁrst task small dataset. architecture experiment alexnet network architecture widespread similarity popular architectures. feature extraction block corresponds convolutional layers. default shared task operator corresponds last fully connected layers task-speciﬁc part contains last classiﬁcation layer training used imagenet rest tasks. note parameter sets trade allowed forgetting previous task performance task. autoencoders shallow architecture keep memory footprint low. encoding well decoding consist single fully connected layer sigmoid non-linearity between. dimensionality codes datasets except imagenet code size size figure training alexnet based autoencoder imagenet objective makes code loss classiﬁcation loss decrease. training stopped observe convergence classiﬁcation loss. autoencoder compared size network model. training autoencoders done using adadelta explained sec. training autoencoders hyperparameter compromise reconstruction error classiﬁcation error. tuned manually order allow convergence code loss classiﬁcation loss training data. figure shows evolution losses training validation samples imagenet training autoencoder based conv features extracted alexnet. experiments cases. datasets multiple datasets moderate size scenes indoor scene classiﬁcation caltech-ucsd birds ﬁne-grained bird classiﬁcation oxford flowers ﬁnegrained ﬂower classiﬁcation excluded pascal similar imagenet subcategories labels corresponding imagenet classes. datasets also used scenario based large initial dataset start imagenet million training images. small dataset scenario start oxford flowers training validation samples. reported results obtained respect test sets scenes birds flowers validation imagenet. need record targets tasks starting training procedure task. here perform ofﬂine augmentation variants sample setting differs slightly done explains higher performance individual tasks experiments. therefore compare stronger baseline accuracies reported table classiﬁcation accuracy tasks scenario starting imagenet. ﬁrst task reference performance given feature extraction. second task consider finetuning reference best achieved task alone. baselines finetuning model learned task using previous task model initialization feature extraction weights previous task model ﬁxed classiﬁcation layer learned task. further also report results variant method separate share representation layers task fully connected layers variant aims ﬁnding universal representation current sequence tasks allowing task fully connected layers. less sharing risk forgetting reduced cost higher memory consumption less regularization tasks. note case task autoencoders used test time activate fully connected layers task test sample belongs similar manner done setups consider sequences tasks. tasks setup given model trained previously seen task second task learn. follows experimental setup work tested scenarios start large dataset imagenet. also study effect starting small dataset flowers. further also consider setup involving three tasks. first sequence tasks starting imagenet i.e. imagenet scenes birds. additionally consider flowers ﬁrst task sequence flowers scenes birds. note different conducted sequences composed splits dataset i.e. task overall. results table shows different compared methods achieved performance tasks scenario imagenet ﬁrst task. finetuning optimal second task shows forgetting ﬁrst task. performance second task average comparable methods except feature extraction. since feature extraction baseline doesn’t allow weights model change optimizes small size flowers dataset network pretrained imagenet initialization training ﬁrst task model. main difference hence lies fact case care forgetting imagenet. last fully connected layers performance second task suboptimal signiﬁcantly lower methods. naturally performance previous task kept unchanged case. separate shows best compromise tasks. performance ﬁrst task highly preserved time performance second task comparable better methods shared fcs. variant method higher capacity allocates separate fully connected layers task memory consumption increases rapidly tasks added. method complete shared model systematically outperforms method previous task average achieves similar performance second task. start smaller dataset flowers trends observed larger differences accuracy performance second task lower achieved imagenet starting point compared methods. explained fact representation obtained imagenet meaningful different tasks ﬁnetuned flowers. differently imagenet starting case separate achieves considerably better performance second task preserving previous task performance. finetuning shows best performance second task suffering severe forgetting previous task. indeed pair tasks different distribution ﬁnding compromise between challenging problem. previous case reduces forgetting achieving similar average performance second task. overall ours-separate achieves best performance different pairs tasks. however requires allocating seprate fully connected layers task requires memory. thus sequential experiments focus shared model scenario. table report performance achieved ours finetuning sequence imagenet scenes birds. expected finetuning baseline suffers severe forgetting previous tasks. performance imagenet drops ﬁnetuning third task. baseline consider previous tasks training procedure advantage achieving best performance table classiﬁcation accuracy tasks scenario starting flowers. ﬁrst task reference performance given feature extraction. second task consider finetuning reference best achieved task alone. figure distance representation obtained training birds task given flowers samples original representation flowers network. starting scenes network trained using method flowers. continually reduces forgetting compared previous tasks showing comparable performance task sequence. example achieves imagenet compared lwf. similar conclusions drawn regarding sequential scenario starting flowers reported table behavior analysis examine effect representation control learning process perform analysis distance representation obtained learning procedure optimal ﬁrst task. flower scenes→ birds test case compute distance epoch current features flowers dataset obtained initial flowers network shown figure beginning training leading term loss related task thus ﬁrst stage model driven towards optimizing performance recent task. results quick loss performance previous tasks increase loss terms objective. then second stage kicks stage terms contribute model pushed towards recovering performance previous tasks continuing improving recent one. gradually gets closer equilibrium reached. strategies efﬁcient lifelong learning still open research problem. work tackled problem learning sequence tasks using data recent environment aiming obtaining reasonable performance whole sequence. existing works consider solutions preserve knowledge previous tasks either keeping corresponding system predictions unchanged training task keeping model parameters neighborhood sequence previous optimal weights. ﬁrst suffers difference task distributions second needs store large number parameters. solution presented reduces forgetting earlier tasks controlling distance representations different tasks. rather preserving optimal weights previous tasks propose alternative preserves features crucial performance corresponding environments. undercomplete autoencoders used learn submanifold represents important features. method tested image classiﬁcation problems sequences three tasks starting either small large dataset. improvement performance state-of-the-art achieved tested scenarios. especially showed better preservation tasks. despite demonstrated improvements work also identiﬁes possible developments. direction worth exploring autoencoders data generators rather relying data. would give m.-e. nilsback zisserman. automated ﬂower classiﬁcation large number classes. proceedings indian conference computer vision graphics image processing russakovsky deng krause satheesh huang karpathy khosla bernstein imagenet large scale visual recognition challenge. international journal computer vision silver mercer. task rehearsal method life-long learning overcoming impoverished data. conference canadian society computational studies intelligence pages springer stronger solution situation data represent previous distributions well. acknowledgment second equal author’s funded scholarship. work partially funded internal funds leuven fp-mc-cig amazon academic research award.", "year": 2017}