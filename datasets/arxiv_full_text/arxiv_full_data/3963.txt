{"title": "An Evolutionary-Based Approach to Learning Multiple Decision Models from  Underrepresented Data", "tag": ["cs.AI", "cs.NE"], "abstract": "The use of multiple Decision Models (DMs) enables to enhance the accuracy in decisions and at the same time allows users to evaluate the confidence in decision making. In this paper we explore the ability of multiple DMs to learn from a small amount of verified data. This becomes important when data samples are difficult to collect and verify. We propose an evolutionary-based approach to solving this problem. The proposed technique is examined on a few clinical problems presented by a small amount of data.", "text": "paper anticipate generalization ability enhanced using leave-one-out technique learn training data. hope technique allow mitigate negative effect overcomplicating usually affects ability generalize evolutionary-based algorithms learning growing complexity shown efficient small amount data allow gradually grown algorithms employ basic operations generating candidate-models selecting best described within approach considered multiple logically interpretable models consist processing units linked attributes outputs previous units. model domain data independently others order enhance models’ diversity. making ensemble models diverse therefore improve generalization ability averaging multiple models. multiple models allows also naturally estimate confidence decisions. many practical cases domain data represented many attributes small amount data available collect. learning data cannot efficient therefore loose ability generalize. however efficiency learning small amount data improved decomposing models many attributes several reference models attributes evolutionary-based method proposed proved efficiency approach several problems. thus novelty approach improve learnt small amount data leave-one-out technique selection reference models composing make interpretable reference models represented logical form. within approach evolutionary composition reference models allows combine ensemble providing better performance. abstract multiple decision models enables enhance accuracy decisions time allows users evaluate confidence decision making. paper explore ability multiple learn small amount verified data. becomes important data samples difficult collect verify. propose evolutionary-based approach solving problem. proposed technique examined clinical problems presented small amount data. multiple decision models enables enhance generalization ability models. time multiple allows users evaluate confidence decision making. properties important applications medical diagnostics data collected learning diagnostic models represented small amount patients’ cases reliably verified. ideally satisfy following requirements provide maximal performance; provide estimates confidence decisions finally decision models interpretable domain experts models learnt verified data large enough represent problem. however practice domain data collected users often underrepresented used learning decision models acceptable predictive accuracy. example clinical practice domain data collected small number patients difficulty diagnostic verification. cases resultant models become unacceptably dependent variations collected data. moreover cross-validation overfitting problem becomes inefficient small amount data affecting ability models generalize. however generalization ability still evaluated within leave-one-out technique ithin approach model units trained one-byone added model model performance number processing units associated complexity model increases gradually. training need find threshold model represented decomposition provide best performance given complexity model convert numeric attributes attempt search desired threshold context current exploiting attributes. search procedure aims minimise entropy model unit follows. attribute training data value entropy search procedure creates candidate model input linked either attribute previous model. case probability calculated ratio |si|/|s| portion training samples assigned model class total number training samples. given threshold model divided subsets calculate conditional entropy trained small amount data selected number errors training data. however selection favours overfitted poor ability generalise. enhance generalisation general models learn small amount data performance becomes poor loss generalization ability performance becomes especially poor models dependent many arguments. however cases performance models improved decomposing functions several functions dependent smaller number arguments particularly arguments paper attention focused logical functions interpretability. ttributes arguments nominal numeric. nominal variables easily convertible logical numeric attributes need converted minimal losses information. losses reduced attribute taken combination attributes determine so-called context problem however determination context even attributes trivial task combinatorial problem. other therefore arrange ensemble order enhance reliability decisions; e.g. final decision made majority voting. ensemble allows also evaluate confidence decisions. resultant ensemble consists several outcomes calculated given input inconsistent. real-world problems inconsistency caused noise corruption data. therefore determine confidence decisions terms consistency calculated ratio mi/m number models voted class clearly estimates confidence range section first describe experiments learning differentiate infectious endocarditis system lupus data collected penza hospital rheumatology department details. second describe experiments trauma data data sets repository spect heart wisconsin prognostic breast cancer differentiate expert collected verified cases represented results clinical laboratory distinguishing diseases. among tests represented numeric values remaining nominal values. total data consists cases. trauma data represented variables categorical numerical. spect data comprise variables categorical. wpbc data represented variables numerical. deal underrepresented data experiments small amount data training namely data samples equal ratio negative positive outcomes. compare proposed evolving decision model technique common artificial neural network ensemble technique. trained back-propagation. comparisons made terms performances provided best single models ensembles within two-fold cross validation. ensembles comprise anns including hidden neurons trained within leave-one-out validation technique reduce negative effect overfitting underrepresented data. make ensemble effective randomly initialised exploits random variables. comparison enable ability attempt select using leave-one-out cross validation technique allowing evaluate generalisation ability terms average number errors. small amount data technique runs reasonable computational time. selecting need also exclude different model’s structures provide outcomes; redundant useless repetitions so-called tautologies. within approach selection goals achieved follows. average numbers errors admitted candidate model previous models respectively. inputs model linked outputs models candidate model selected following condition min. indeed candidate model admits fewer errors models generalisation ability model becomes better. applying heuristic rule iteratively candidate model therefore achieve near-optimal generalisation ability theory maximum ability cannot guaranteed using heuristics search. learning process continues performance improved. performance stabilises assume trained provide best generalisation ability. performance improvement becomes unlikely almost candidate-models rejected selection criterion ccording criteria last generation provide minimal number errors. several seems reasonable select minimal complexity i.e. comprise minimal number processing units. complexity models made restricted achieve best performance; complexity simply counted number model’s units. experiments complexity restricted units mentioned data sets. fig. shows fitness function values complexity models number units accepted evolutionary learning. dashed line upper plot depicts performance units within leave one-out technique dashed line lower plot depicts maximal complexity models. figure fitness function reaches maximum fast becomes slightly oscillated around complexity models also grown fast reaches maximum around units. fig. depicts performances single models performance ensemble number models accepted learning. important figure observe performance ensemble tends increase. results experiments listed table presenting mean standard deviation values performances calculated runs within -fold cross validation. table proposed technique superior ensemble technique terms predictive accuracy. additional advantage proposed technique derive best models ensemble create diagnostic table clinicians conveniently make decisions evaluate consistency. example differentiation problem found nine represented form if-then rules circulating immune complex less articular syndrome absent anhelation absent erythema absent noises heart absent hepatomegaly absent myocarditis absent igure shows ranks eight attributes selected used dms. important contribution made attributes whilst attributes make weakest contribution. therefore attempt exclude remaining attributes keeping performance high. given input outcomes inconsistent final decision made majority voting. resultant diagnostic table applied patients misdiagnosed rate less kwedlo kretowski discovery decision rules databases evolutionary approach principles data mining knowledge discovery second european symposium nantes france september springer lecture notes computer science nikolaev automated discovery polynomials inductive genetic programming zutkow ranch principles data mining knowledge discovery springer berlin presented evolutionary-based approach learning underrepresented data frequently appear practice. aimed find proving best performance keeping form interpretable users. based evolutionary approach proposed technique starts learn consisting processing unit step-by-step evolves performance increases. processing unit pair inputs allows uncertainty model parameters learnt data reduced. theory proposed technique provide nearoptimal complexity models. however practice dealing underrepresented data cannot substantial portion data validation consequence needed restrict maximum models’ complexity. restriction viewed additional overfitting problem. within approach combined leave-one-out validation technique complexity restriction achieve near-optimal performance ensemble. experiments clinical datasets show proposed technique outperforms common ensemble technique term predictive accuracy. additionally proposed technique shown capable excluding redundant clinical tests decision models performance kept high.", "year": 2008}