{"title": "Asking the Difficult Questions: Goal-Oriented Visual Question Generation  via Intermediate Rewards", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "Despite significant progress in a variety of vision-and-language problems, developing a method capable of asking intelligent, goal-oriented questions about images is proven to be an inscrutable challenge. Towards this end, we propose a Deep Reinforcement Learning framework based on three new intermediate rewards, namely goal-achieved, progressive and informativeness that encourage the generation of succinct questions, which in turn uncover valuable information towards the overall goal. By directly optimizing for questions that work quickly towards fulfilling the overall goal, we avoid the tendency of existing methods to generate long series of insane queries that add little value. We evaluate our model on the GuessWhat?! dataset and show that the resulting questions can help a standard Guesser identify a specific object in an image at a much higher success rate.", "text": "despite signiﬁcant progress variety vision-andlanguage problems developing method capable asking intelligent goal-oriented questions images proven inscrutable challenge. towards propose deep reinforcement learning framework based three intermediate rewards namely goal-achieved progressive informativeness encourage generation succinct questions turn uncover valuable information towards overall goal. directly optimizing questions work quickly towards fulﬁlling overall goal avoid tendency existing methods generate long series insane queries little value. evaluate model guesswhat? dataset show resulting questions help standard guesser identify speciﬁc object image much higher success rate. although visual question answering attracted attention visual question generation much difﬁcult task. obviously generating facile repetitive questions represents challenge generating series questions draw useful information towards overarching goal however demands consideration image content goal conversation thus far. could generally also seen requiring consideration abilities motivation participant conversation. figure illustrative examples potential conversations human robot. left conversation clearly makes people frustrated right makes people happy robot achieves goal quicker less informative questions. well-posed question extracts informative answer towards achieving particular goal thus reﬂects knowledge asker estimate capabilities answerer. although information would beneﬁcial identifying particular object image little value agent asking human exact values particular pixels statistics gradients aspect ratio corresponding bounding box. fact answerer incapable providing requested information makes questions pointless. selecting question signiﬁcant probability generating answer helps achieve particular goal complex problem. asking questions essential part humans communicate learn. intelligent agent seeks interact ﬂexibly effectively humans thus needs able questions. ability intelligent questions even important receiving intelligent actionable answers. robot example fig. given task realized missing critical information required carry needs question. limited number attempts human gets frustrated carries task themselves. scenario applies equally intelligent agent seeks interact humans surprisingly little tolerance agents unable learn asking questions many. result above visual question generation started receive research attention primarily vision-to-language problem methods approach problem manner tend generate arbitrary sequences questions somewhat related image bare relationship goal. reﬂects fact methods means measuring whether answers generated assist making progress towards goal. instead paper ground problem goal-oriented version game guesswhat? introduced method presented play guesswhat game made three components questioner asks questions oracle guesser tries identify object oracle referring based answers. quality generated questions thus directly related success rate ﬁnal task. goal-oriented training uses game setting used visual dialog generation previously however work focus generating human-like dialogs helping agent achieve goal better question generation. moreover previous work uses ﬁnal goal reward train dialog generator might suitable dialog generation rather weak undirected signal control quality effectiveness informativeness generated question goal-oriented task. words cases want talk robot want ﬁnish speciﬁc task hold meaningless boring chat. therefore paper intermediate rewards encourage agent short informative questions achieve goal. moreover contrast previous works consider overall goal reward assign different intermediate rewards posed question control quality. achieved ﬁtting goal-oriented reinforcement learning paradigm devising three different intermediate rewards main contributions paper explicitly optimize question generation. ﬁrst goal-achieved reward designed encourage agent achieve ﬁnal goal asking multiple questions. however different considering whether goal achieved additional rewards awarded agent fewer questions achieve reasonable setting need robot ﬁnish task hundreds questions. second reward proposed progressive reward established encourage questions generated agent progressively increase probability right answer. intermediate reward individual question reward decided change ground-truth answer probability. negative reward given probability decreases. last reward informativeness reward used restrict agent ‘useless’ questions example question leads identical answer candidate objects show whole framework fig. evaluate model guesswhat? dataset pre-trained standard oracle guesser show novel questioner model outperforms baseline state-of-the-art model large margin. also evaluate reward respectively measure individual contribution. qualitative results show produce informative questions. related works visual question generation recently visual question generation problem brought computer vision community aims generating visual-related questions. works treat standalone problem follow image captioning style framework i.e. translate image sentence case question. example mora cnn-lstm model generate questions answers directly image visual content. zhang focus generating questions grounded images. densecap region captioning generator guide question generation. mostafazadeh propose dataset generate natural questions images beyond literal description image content. view dual learning process jointly training end-to-end framework. although works generate meaningful questions related image motivation asking questions rather weak related goals. moreover hard conduct quality measurement type questions. instead work develop agent learn realistic questions contribute achieving speciﬁc goal. goal-oriented visual dialogue generation attracted many attentions recently. introduce reinforcement learning mechanism visual dialogue generation. establish agents corresponding question answer generation respectively ﬁnally locate unseen image images. question figure framework proposed agent plays whole game environment. target object assigned oracle unknown guesser. generates series questions answered oracle. training oracle answer question based objects round measure informativeness reward also guesser generate probability distribution measure progressive reward. finally consider number rounds goal-achieved reward based status success. intermediate rewards adopted optimizing agent reinforce. agent predicts feature representation image reward function given measuring close representation compared true feature. however focus encouraging agent generate questions directed towards ﬁnal goal adopt different kinds intermediate rewards achieve question generation process. moreover question generation agent model asks questions based dialogue history involve visual information. florian propose employ reinforcement learning solve question generation guesswhat game introducing ﬁnal status success sole reward. share similar backbone idea several technical differences. signiﬁcant differences previous work considers using whether achieving ﬁnal goal reward assign different intermediate rewards posed question push agent short informative questions achieve goal. experimental results analysis section show model outperforms state-of-art also achieves higher intelligence i.e. using questions possible ﬁnish task. question generation long history works grammar question generation text domain natural language processing authors focus automatically generating gapﬁll questions crowdsourcing templates manually built templates used question generation respectively. works focus constructing formatted questions text corpus. reinforcement learning reinforcement learning adopted several vision language problems including image captioning aforementioned visual dialogue system etc. policy network value network collaboratively generate image captions different optimization methods image captioning explored called spider self-critical sequence training. introduce knowledge source iterative employ learn query policy. authors learn parameters model images structured knowledge bases. works solve related problems employing optimization method focus using carefully designed intermediate rewards train agent goal-oriented tasks. ground goal-oriented problem guess game speciﬁcally guesswhat? dataset guesswhat? three-role interactive game roles observe image rich visual scene contains multiple objects. view game three parts oracle questioner guesser. game random object scene assigned oracle process hidden questioner. questioner series yes/no questions locate object. list objects also hidden questioner question-answer rounds. questioner gathered enough information guesser start guess. game considered successful guesser selects right object. questioner part game goal-oriented problem; question generated based visual information image previous rounds question-answer pairs. goal successfully objects’ spatial features categories mlp. perform dot-product dialogue object features softmax operation produce ﬁnal prediction. baseline given image history question-answer pairs requires generating question build baseline based generator. recurrently produces series state vectors transitioning previous state lstm transition function case state vector condisj tioned whole image previous questionanswer tokens. softmax operation produce probability distribution vocabulary m−). baseline conducted appended dialogue history. repeat loop dialogue token sampled number questions reaches maximum. finally guesser takes whole dialogue object list inputs predict object. consider goal reached selected. otherwise failed. state action policy view markov decision process noted agent. dialogue generated based image time step state agent deﬁned image visual content history ﬁnish game case locate right object. paper goal-oriented reinforcement learning paradigm propose three different intermediate rewards namely goal-achieved reward progressive reward informativeness reward explicitly optimize question generation. goal-achieved reward established lead dialogue achieve ﬁnal goal progressive reward used push intermediate generation process towards optimal direction informativeness reward used ensure quality generated questions. better express generation process ﬁrst introduce notations guesswhat? game used throughout rest sections. game deﬁned tuple observed image dialogue rounds question-answer pairs list objects image target object. question also introduce supervised learning model referred baseline rest paper. oracle oracle requires generating answers kinds questions objects within image scene. build neural network architecture oracle referring bounding object encoded eight dimensional vector represent spatial feature ospa indicates coordinates width height. category embedded using learned look-up table current question encoded lstm three features concatenated single vector hidden layer followed softmax layer produce answer probability guesser given image series questionanswer pairs guesser requires predicting right object list objects. referring consider generated dialogue sequence tokens encode lstm. last hidden state extracted feature represent dialogue. also embed question-answer pairs tokens current question generated action agent select next output token vocabulary depends actions agent takes transition states falls following cases maximum length question mmax maximum rounds dialogue jmax. therefore number time steps dialogue mmax jmax. model stochastic policy represents parameters deep neural network used baseline produces probability distributions state. goal policy learning estimate parameter components signiﬁcant aspect deﬁne appropriate reward function state-action pair emphasized before goal-oriented aims generate questions lead achieving ﬁnal goal. therefore build three kinds intermediate rewards push agent optimized towards optimal direction. whole framework shown fig. goal-achieved reward basic rule appropriate reward function cannot conﬂict ﬁnal optimal policy primary purpose agent gather enough information soon possible help guesser locate object. therefore deﬁne ﬁrst reward reﬂect whether ﬁnal goal achieved. moreover take number rounds consideration accelerate questioning part reward nonzero game successful. wise. based want ﬁnal goal motivate generate useful questions. moreover intermediate process considered reward function rounds question-answer pairs guarantees efﬁciency generation process; fewer questions generated reward agent game quite useful setting realistic want fewer orders guide robot ﬁnish tasks. weight balance contribution successful reward dialogue round reward. progressive reward based intuition observation human interactive dialogues questions successful game ones progressively achieve ﬁnal goal i.e. long questions asked answered conﬁdence referring target object becomes higher higher. therefore round deﬁne intermediate reward state-action pair improvement target probability guesser outputs. speciﬁc interact guesser round obtain probability predicting target object. probability increases means generated question positive question leads dialogue towards right direction. intermediate reward called progressive reward encourage agent progressively generate positive questions. round record probability pjj) returned guesser compare last round difference probabilities used intermediate reward. pjj) pj−j−) question considered high-quality positive reward leads higher probability guess right object. otherwise reward negative. human questions expect answer help eliminate confusion distinguish candidate objects. hence imagine posed question leads answer candidate object question useless. example candidate objects ‘red’ posed question red?’ answer ‘yes.’ however question-answer pair cannot help identify target. want avoid kind questions non-informative. case need evaluate question based answer oracle. baseline function help reduce gradient variance chosen arbitrarily. one-layer takes state input agent outputs expected reward. baseline trained mean squared error section present results conduct comprehensive ablation analysis intermediate reward. mentioned above proposed method evaluated guesswhat? game dataset pre-trained standard oracle guesser. comparing image current question target object inputs outputs answer oracle answer question objects image. answers different other consider useful locating right object. otherwise contribute ﬁnal goal. therefore reward positive called informativeness reward useful questions. formally round oracle receives image current question list objects outputs answer {ajo ajon} element corresponds object. informativeness reward deﬁned considering large action space game setting adopt policy gradient method train agent proposed intermediate rewards. goal policy gradient update policy parameters respect expected return gradient descent. since episodic environment given policy generative network agent case policy objective function takes form guesswhat? dataset composed dialogues grounded images unique objects. question-answer pairs dialogues vocabulary size standard split training validation test following report accuracies games evaluation metric. given j-round dialogue target object located guesser game noted successful indicates agent generated qualiﬁed questions serve ﬁnal goal. kinds test runs training test respectively named newobject newimage. newobject randomly sampling target objects training images newimage sampling objects test images report three inference methods namely sampling greedy beam-search test runs. initialize training environment standard oracle guesser baseline start train agent proposed reward functions. train models epochs stochastic gradient descent learning rate batch size respectively. baseline function trained time. epoch training image sampled once objects inside randomly assigned target. maximum round jmax maximum length question mmax weight dialog round reward progressive reward section give overall analysis proposed intermediate reward functions. better show effectiveness reward conduct comprehensive ablation studies. moreover also carry human interpretability study evaluate whether human subjects understand generated questions well human question-answer pairs achieve ﬁnal goal. note agent trained goal-achieved reward vqg-rg trained goal-achieved progressive rewards vqg-rg+rp trained goal-achieved informativeness rewards vqg-rg+ri. ﬁnal agent trained three rewards noted vqg-rg+rp+ri. overall analysis tab. show comparisons between agent optimized proposed intermediate rewards state-of-the-art model proposed noted sole-r uses indicator whether reaching ﬁnal goal sole reward function. proposed intermediate rewards combinations agents outperform compared models evaluation metrics. speciﬁcally ﬁnal vqg-rg+rp+ri agent surpasses sole-r accuracy newobject sampling greedy beam-search respectively obtains higher accuracy newimage sampling greedy beam-search respectively. moreover agents outperform supervised baseline signiﬁcant margin. fully show effectiveness proposed intermediate rewards train three agents using rg+rp rg+ri rewards respectively conduct ablation analysis. vqg-rg already outperforms baseline state-of-the-art model means controlling dialogue round push agent wise questions. combination reward respectively performance agent improved. improvement gained reward higher reward suggests intermediate progressive reward contributes experiment. ﬁnal agent combines rewards achieves best results. fig. shows qualitative results. results found supplementary material including figure qualitative results agent comparisons baseline sole-r model elements middle array indicate probabilities successfully locating target object round. better viewed color. question informativeness also investigate informativeness questions generated different models. oracle answer questions objects round count percentage high-quality questions successful game. deﬁne high-quality question lead answer candidate objects. experimental results show agent high-quality questions higher baseline sole-r conﬁrms contribution reward. human study conduct human study well human guess target object based questions generated models. show human subjects images generated question-answer pairs baseline sole-r ﬁnal agent guess objects i.e. replacing guesser real human. three human subjects play split game recognized successful least give right answer. based experiments averagely subjects achieve highest accuracy based agent achieves accuracies baseline sole-r questions respectively. results indicate agent generate higher qualitative questions beneﬁt human achieve ﬁnal goal. conclusion ability devise concise questions lead parties dialog satisfying shared goal effectively possible important practical applications theoretical implications. introducing suitably crafted intermediate rewards deep reinforcement learning framework shown possible achieve result least particular class goal. figure comparisons success ratio agent soler well baseline model different dialogue round. left y-axis indicates number successful dialogues corresponds chart. right y-axis indicates success ratio corresponds line chart. better viewed color. dialogue round conduct experiment investigate relationship dialogue round game success ratio. speciﬁcally guesser select object round calculate success ratio given round comparisons different models shown fig. agent achieve goal fewer rounds compared models especially round three. progressive trend prove agent learn progressive trend generated questions count percentage successful game progressive trend target object observing probability distributions generated guesser round. agent achieves baseline sole-r respectively indicates agent better generating questions progressive trend considering introduce progressive reward qualitative results ‘progressive trend’ shown fig. i.e. probability right answer progressively increasing. ﬁxed inaccurate certain extent. consider main objective paper show effectiveness proposed intermediate rewards problem leave work train three components jointly reinforcement learning framework.", "year": 2017}