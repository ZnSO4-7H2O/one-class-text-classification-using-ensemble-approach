{"title": "When is a Convolutional Filter Easy To Learn?", "tag": ["cs.LG", "cs.AI", "cs.CV", "math.OC", "stat.ML"], "abstract": "We analyze the convergence of (stochastic) gradient descent algorithm for learning a convolutional filter with Rectified Linear Unit (ReLU) activation function. Our analysis does not rely on any specific form of the input distribution and our proofs only use the definition of ReLU, in contrast with previous works that are restricted to standard Gaussian input. We show that (stochastic) gradient descent with random initialization can learn the convolutional filter in polynomial time and the convergence rate depends on the smoothness of the input distribution and the closeness of patches. To the best of our knowledge, this is the first recovery guarantee of gradient-based algorithms for convolutional filter on non-Gaussian input distributions. Our theory also justifies the two-stage learning rate strategy in deep neural networks. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.", "text": "analyze convergence gradient descent algorithm learning convolutional ﬁlter rectiﬁed linear unit activation function. analysis rely speciﬁc form input distribution proofs deﬁnition relu contrast previous works restricted standard gaussian input. show gradient descent random initialization learn convolutional ﬁlter polynomial time convergence rate depends smoothness input distribution closeness patches. best knowledge ﬁrst recovery guarantee gradient-based algorithms convolutional ﬁlter non-gaussian input distributions. theory also justiﬁes two-stage learning rate strategy deep neural networks. focus theoretical also present experiments justify theoretical ﬁndings. deep convolutional neural networks achieved state-of-the-art performance many applications computer vision natural language processing reinforcement learning applied classic games like despite highly non-convex nature objective function simple ﬁrst-order algorithms like stochastic gradient descent variants often train networks successfully. hand success convolutional neural network remains elusive optimization perspective. input distribution constrained existing results mostly negative hardness learning -node neural network non-overlap convolutional ﬁlter recently shamir showed learning simple one-layer fully connected neural network hard speciﬁc input distributions. negative results suggest that order explain empirical success learning neural networks stronger assumptions input distribution needed. recently line research assumed input distribution standard gaussian showed gradient descent able recover neural networks relu activation polynomial time. major issue analysis rely specialized analytic properties gaussian distribution thus cannot generalized non-gaussian case real-world distributions fall into. general input distributions techniques needed. generated known function ﬁlter size stride i-th pixels. since convolutional ﬁlters need focus patches instead input following deﬁnitions theorems refer input distribution relu activation function) figure architecture network considering. given input extract patches {zi} send shared weight vector outputs sent relu summed yield ﬁnal label conditions proposed convergence. want data highly correlated concentrated direction aligned ground truth vector figure graphical illustration. architectures used ﬁrst layer many works computer vision address realizable case training data generated unknown teacher parameter learn input distribution consider loss gradient descent i.e. learnability filters show input patches highly correlated i.e. small gradient descent stochastic gradient descent random initialization recovers ﬁlter polynomial time. furthermore strong correlations imply faster convergence. best knowledge ﬁrst recovery guarantee randomly initialized gradient-based algorithms learning ﬁlters non-gaussian input distribution answering open problem smoothness input distribution convergence rate ﬁlter weights recovery smoothness paper deﬁned ratio largest least eigenvalues second moment activation region show smoother input distribution leads faster convergence gaussian distribution special case leads tightest bound. theoretical ﬁnding also justiﬁes twostage learning rate strategy proposed step size allowed change time. recent years theorists tried explain success deep learning different perspectives. optimization point view optimizing neural network non-convex optimization problem. pioneered class non-convex optimization problems satisfy strict saddle property optimized perturbed gradient descent polynomial time motivates research studying landscape neural networks however results cannot directly applied analyzing convergence gradient-based methods relu activated neural networks. learning theory point view well known training neural network hard worst cases recently shamir showed either niceness target function input distribution alone sufﬁcient optimization algorithms used practice succeed. additional assumptions many works tried design algorithms provably learn neural network polynomial time sample complexity however algorithms tailored certain architecture cannot explain gradient based optimization algorithms work well practice. focusing gradient-based algorithms line research analyzed behavior gradient descent gaussian input distribution. tian showed population gradient descent able true weight vector random initialization one-layer one-neuron model. brutzkus globerson showed population gradient descent recovers true weights convolution ﬁlter non-overlapping input polynomial time. yuan showed recover true weights one-layer resnet model relu activation assumption spectral norm true weights bounded small constant. methods explicit formulas gaussian input enable apply trigonometric inequalities derive convergence. gaussian assumption soltanolkotabi shows true weights exactly recovered projected gradient descent enough samples linear time number inputs less dimension weights. approaches combine tensor approaches assumptions input distribution. zhong proved sufﬁciently good initialization implemented tensor method gradient descent true weights -layer fully connected neural network. however approach works known input distributions. soltanolkotabi used gaussian width concentrations approach cannot directly extended learning convolutional ﬁlter. paper adopt different approach relies deﬁnition relu. show long input distribution satisﬁes weak smoothness assumptions able true weights polynomial time. using conclusions justify effectiveness large amounts data two-stage adaptive learning rates used szegedy etc. paper organized follows. section analyze simplest one-layer one-neuron model state observation establish connection smoothness convergence rate. section discuss performance gradient descent learning convolutional ﬁlter. provide empirical illustrations section conclude section place detailed proofs appendix. denote euclidean norm ﬁnite-dimensional vector. matrix λmax denote largest singular value λmin smallest singular value. note positive semideﬁnite matrix λmax λmin represent largest smallest eigenvalues respectively. denote standard big-o big-theta notations hide absolute gradient descent guaranteed converge local minima polynomial time joint activation region joint activation region −w∗. figure graphical illustration. simple algebra derive population gradient. initialization satisﬁes gradient descent algorithm recovers ﬁrst assumption non-degeneracy input distribution. case assumption fails input distribution supported low-dimensional space degenerated. second assumption initialization ensure gradient descent converge gradient undeﬁned. general convergence theorem holds wide class input distribution initialization points. particular includes theorem special case. input distribution degenerate i.e. holes input space gradient descent stuck around saddle points believe data needed facilitate optimization procedure also consistent empirical evidence data helpful optimization. previous section showed distribution regular weights initialized appropriately gradient descent recovers true weights converges. practice also want know many iterations needed. characterize convergence rate need quantitative assumptions. note different assumptions lead different rate possible choice. paper following quantities. conditions quantitatively characterize angular smoothness input distribution. given angle difference large direction large probability mass direction small probability mass meaning input distribution smooth. hand close directions similar probability mass means input distribution smooth. smoothest input distributions rotationally invariant distributions analogy think lipschitz constant gradient strong convexity parameter optimization literature also allow change angle. also observe intersection measure monotonically decreasing. next assumption growth aw−w∗ intersection measure. also aw−w∗ becomes larger. following assume operator norm aw−w∗ increases smoothly respect angle. intuition long input distribution bounded probability density respect angle operator norm aw−w∗ bounded. show theorem rotational invariant distribution theorem standard gaussian distribution. assumption assume exists l−w∗ maxwθ≤φ λmax increases decreases choose constant step size theorem implies \u0001-close solution iterations. also suggests direct relation smoothness distribution convergence rate. smooth distribution close small relatively small need fewer iterations. hand much larger need iterations. verify intuition section able choose step sizes adaptively like using methods proposed xiao improve computational complexity justiﬁes two-stage learning rate strategy proposed szegedy beginning need choose learning small small later choose large learning rate angle becomes smaller theorem requires initialization satisfying achieved random initialization constant success probability. section detailed discussion. main difference simple one-layer one-neuron network convolution ﬁlter patches appear different regions. given sample exists patch interaction plays important role convergence gradient descent. assume second moment interaction i.e. cross-covariance also grows smoothly respect angle. assumption assume exists lcross first note measure assumption models growth cross-covariance. next note lcross represents closeness patches. similar joint probability density small implies lcross small. extreme setting lcross case events measure ready present result learning convolutional ﬁlter gradient descent. theorem theorem suggests initialization satisﬁes lcross obtain linear convergence rate. section give concrete example showing closeness patches implies large small lcross. similar theorem step size chosen \u0001-close solution proof also similar theorem practicewe never true population gradient stochastic gradient following theorem shows also recovers underlying ﬁlter. least unlike vanilla gradient descent case convergence rate depends instead randomness need robust initialization. choose average ease presentation. apparent proof require close proof relies constructing martingale azuma-hoeffding inequality idea previously used different one-layer one-neuron model also requires lipschitz constant closeness lcross relatively small relatively large. natural question input distributions satisfy condition? give example. show patches close input distribution small probability mass around decision boundary assumption theorem satisﬁed. figure graphical illustrations. theorem denote zavg assume exists several comments sequel. view quantitative measure closeness different patches i.e. small means similar. lower bound monotonically decreasing function note σmin upper bond lcross represents upper bound probability density around small sumption usually satisﬁed real world examples like images image patches usually close decision boundary. example computer vision local image patches often form clusters evenly distributed appearance space. therefore linear classiﬁer separate cluster centers rest clusters near decision boundary probability mass low. one-layer one-neuron model need initialization convolution ﬁlter need stronger initialization following theorem this condition relaxed norm angle patch independent norm shows uniformly random initialization constant probability obtain good initialization. note theorem hand boost success probability arbitrary close random restarts. proof similar theorem uniformly sample p-dimensional ball radius apply general initialization theorem convolution ﬁlter case choose therefore simple algebra following corollary. corollary suppose uniformly sampled ball center radius probability least assumption corollary satisﬁed patches close discussed previous section. section simulations verify theoretical ﬁndings. ﬁrst test smoothness affect convergence rate one-layer one-neuron model described section construct input distribution different patch unit norm mixture truncated gaussian distribution model angle around around speciﬁcally probability density sampled note deﬁnitions probability mass centered around distribution spiky large. hand input distribution close rotation invariant distribution small. figure veriﬁes prediction initialization step size. also test whether learn ﬁlter real world data. choose mnist data generate labels using ﬁlters. random ﬁlter entry sampled standard gaussian distribution gabor ﬁlter figure figure show convergence rates different initializations. here better initializations give faster rates coincides theory. note report relative loss logarithm squared error divided square mean data points instead difference learned ﬁlter true ﬁlter found often cannot converge exact ﬁlter rather ﬁlter near zero loss. believe data approximately lying dimensional manifold learned ﬁlter true ﬁlter equivalent. justify conjecture interpolate learned ﬁlter true ﬁlter linearly result ﬁlter similar loss lastly visualize true ﬁlters learned ﬁlters figure similar patterns. paper provide ﬁrst recovery guarantee gradient descent algorithm random initialization learning convolution ﬁlter input distribution gaussian. analyses used deﬁnition relu mild structural assumptions input distribution. list future directions. possibility extend result deeper wider architectures. even two-layer fullyconnected network convergence gradient descent random initialization known. existing results either requires sufﬁciently good initialization figure convergence rates different smoothness larger smoother; different closeness patches smaller closer; learning random ﬁlter different initialization mnist data; learning gabor ﬁlter different initialization mnist data. another direction consider agnostic setting label equal output neural network. lead different dynamics gradient descent need analyze robustness optimization procedures. problem also related expressiveness neural network underlying function equal close neural network. believe analysis extend setting. anna choromanska mikael henaff michael mathieu g´erard arous yann lecun. loss surfaces multilayer networks. artiﬁcial intelligence statistics antoine gautier quynh nguyen matthias hein. globally optimal training generalized polynomial neural networks nonlinear spectral methods. advances neural information processing systems kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition majid janzamin hanie sedghi anima anandkumar. beating perils non-convexity guaranteed training neural networks using tensor methods. arxiv preprint arxiv. alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems qihang xiao. adaptive accelerated proximal gradient method homotopy continuation sparse optimization. international conference machine learning fausto milletari nassir navab seyed-ahmad ahmadi. v-net fully convolutional neural networks volumetric medical image segmentation. vision fourth international conference ieee david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature mahdi soltanolkotabi adel javanmard jason lee. theoretical insights optimization landscape over-parameterized shallow neural networks. arxiv preprint arxiv. yuchen zhang jason michael jordan. l-regularized neural networks improperly learnable polynomial time. international conference machine learning proof theorem assumption input distribution ensures aww∗ aw−w∗ gradient descent converges following theorem. assumption since gradient descent decreases function value converge note critical points lemma have without loss generality −αw∗ assumption know aw−w∗ second equation becomes aw−w∗ therefore proof theorem proof relies following simple crucial observation lcross)wt ﬁrst inequality used deﬁnitions regions; second inequality used deﬁnition operator norm; third inequality used fact fourth inequality used deﬁnition lcross ﬁfth inequality used next upper bound norm gradient using similar argument lcross last step used choice proof theorem consists parts. first show chosen properly high probability iterates stat neighborhood next conditioning this derive rate. lemma denote number iterations failure probability denote arcsin step size ﬁrst inequality used last assumption. second inequality used probability event upper bound superset event. third used lemma union bound. fourth used markov’s inequality. figure show loss linear interpolation learned ﬁlter ground truth ﬁlter interpolation form winter interpolation ratio. note interpolation ratios loss remains low.", "year": 2017}