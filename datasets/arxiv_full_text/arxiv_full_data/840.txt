{"title": "Principal Graphs and Manifolds", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "In many physical, statistical, biological and other investigations it is desirable to approximate a system of points by objects of lower dimension and/or complexity. For this purpose, Karl Pearson invented principal component analysis in 1901 and found 'lines and planes of closest fit to system of points'. The famous k-means algorithm solves the approximation problem too, but by finite sets instead of lines and planes. This chapter gives a brief practical introduction into the methods of construction of general principal objects, i.e. objects embedded in the 'middle' of the multidimensional data set. As a basis, the unifying framework of mean squared distance approximation of finite datasets is selected. Principal graphs and manifolds are constructed as generalisations of principal components and k-means principal points. For this purpose, the family of expectation/maximisation algorithms with nearest generalisations is presented. Construction of principal graphs with controlled complexity is based on the graph grammar approach.", "text": "many fields science meets multivariate distributions vectors representing observations. distributions often difficult analyse make sense nature human brain able visually manipulate objects dimension three. distributions objects lower dimension and/or complexity retaining important information structures contained initial full complex data point cloud. notion mean point generalized approximating data complex types objects. pearson proposed approximate multivariate distributions lines planes principal component analysis invented nowadays basic statistical tool. principal lines planes ‗middle‘ multivariate data distribution correspond first modes multivariate gaussian distribution approximating data. tarting proposed approximate complex multidimensional dataset several ‗mean‘ points. thus k-means algorithm suggested nowadays used clustering methods machine learning decades following major directions linear manifolds generalised non-linear ones links ‗mean‘ points introduced. appearance several large families statistical methods; famous principal curves principal manifolds self-organising maps quickly realized objects constructed methods tightly connected theoretically. observation allows develop common framework called ―construction principal objects‖. geometrical nature objects different serve data approximators controllable complexity. allows using tasks dimension complexity reduction. machine learning direction connected terms ‗unsupervised learning‘ ‗manifold learning.‘ chapter overview major directions field principal objects construction. formulate problem classical approaches k-means unifying framework show naturally generalised principal graphs manifolds general types principal objects principal cubic complexes. systematically introduce used ideas algorithms developed field. form definition mean point goes back fréchet notice definition mean point fréchet non-unique. however definition allows multiple useful generalisations including using abstract metric spaces. easy show case complete data constructing various approximations dataset almost cases principal objects represented finite infinite vectors approximates finite dataset sense minimisation answers regularity conditions discussed below. original works pearson followed principle reality data analysis dataset embedded multidimensional metric space. approach called geometrical. century probabilistic interpretation statistics actively developed. accordingly interpretation dataset particular i.i.d. sample multidimensional probability distribution defines probability appearance sample point possible calculate conditional mean since typically points zero point projected means finite datasets develop coarse-grained self-consistency notion. usually means every point defines kind neighbourhood introduces modified self-consistency respect neighbourhood instead itself. concrete implementations idea described chapter. cases effective size neighbourhood fundamental parameter controlling complexity resulting approximator approximators constructed linear manifolds embedded fact corresponds original definition principal lines planes pearson however method re-invented fields even obtained different names hotteling transform proper orthogonal decomposition others. formulate four equivalent ways define principal components user meet different applications. vectors given construct one-dimensional distribution define empirical variance along standard empirical variance. problem consists finding empirical variances along would maximal linear manifolds dimension embedded maximisation) problem consists finding sequence mean point-to-point squared distance orthogonal projections data points maximal linear manifolds dimension embedded three mentioned definitions basis vectors defined arbitrary rotation change manifold. make choice less ambiguous method following principle applied given ‗embedded‘ linear manifold smaller dimension form must linear principal manifold dimension definitions given finite datasets definition sensible finite datasets random vector finite datasets empiric correlation cancelled. empiric principal components annul empiric correlations could considered approximation principal components random vector. equivalence above-mentioned definitions case complete data euclidean space follows pythagorean theorem elementary algebra. however practice definition useful computations generalisations approach. thus definitions suitable working incomplete data since defined distance function easily calculated ‗gapped‘ data vectors definition generalized weighting data points definition generalized weighting pairs data points details generalisations could found fundamental book jollliffe algorithms finding principal objects given dataset constructed accordingly classical expectation/maximisation splitting scheme first formulated generic method dempster decomposition easy check vectors correspond principal vectors eigenvectors empirical covariance matrix whereas contain projections points onto corresponding principal vector. eigenvalues mathematical basis introduced sylvester represents solid mathematical foundation although formally problems spectral decomposition eigen decomposition equivalent algorithms performing singular decomposition directly efficient robust thus iterative algorithm calculating first principal component described previous chapter indeed performs singular decomposition finds right singular left singular vectors one. macqueen another extreme simplicity case finding principal object. case simply unstructured finite vectors solution searched k-means algorithm principal points demonstrated special construction probabilities instead equidistribution gives serious advantages. first centre select equiprobable centres y..yj chosen squared shortest distance data point closest centre already chosen. then select next centre probability example book mirkin based idea ‗data recovering‘). computationally expensive step algorithm partitioning dataset proximity centroids significantly accelerated using kd-tree data structure analysis effectiveness algorithm k-means problem given ostrovsky otice case principal points chapter selfconsistency coarse-grained self-consistency coincide centroid conditional mean point data points belonging voronoi region associated fukunaga olsen denote simplest cluster-wise approach consists applying k-means type clustering dataset calculating principal components cluster separately. however simple idea performs rather poorly applications interesting approach consists generalizing k-means introducing principal hyperplane segments proposed diday called ‗k-segments‘ local subspace analysis verbeek proposed variant ‗k-segment‘ approach onedimensional segments accompanied strategy assemble disconnected line segments global piecewise linear principal curve. einbeck proposed iterative cluster splitting joining approach helps select optimal number configuration disjoined segments. consists calculating local mean points local principal directions following starting seed points. locality introduced using kernel functions defining effective radius neighborhood data space. thus delicado introduced principal oriented points based variance maximisation-based definition pops different principal points introduced defined independently another principal points defined globally set. pops assembled principal curves oriented points einbeck proposed simpler approach based local tracing principal curves calculating local centers mass local first principal components. ohonen seminal paper proposed modify k-means approach introducing connections centroids change position centroid would also change configuration neighboring centroids. thus self-organizing maps algorithm developed. algorithm take finite metric space metric combinations criteria best preservation initial structure image best approximation dataset soms give popular approximations principal manifolds take fragment regular s-dimensional grid consider resulting approximation s-dimensional principal manifold goals. original formulation kohonen start usually approximation lies initial approximation s-dimensional linear principal manifold. k-th step algorithm chosen datapoint current approximation approximation w)). step size monotonically decreasing neighborhood function. process proceeds several epochs neighborhood radius decreasing next epoch. multiple generalizations algorithms constructing soms type described above batch algorithm includes projecting step exactly k-means maximization step modified simultaneously. attempt resolve issue bishop developed optimizationbased generative topographic mapping method. setting supposed observed data i.i.d. sample mixture gaussian distributions centers aligned along two-dimensional grid embedded data space. parameters mixture determined em-based maximization likelihood function distribution introduced thesis trevor hastie self-consistent onetwo-dimensional globally parametrisable smooth manifolds without self-intersections. parameterized without self-intersections. principal surface probability distribution self-consistent. using classical splitting. provide algorithm finite dataset directly applied typical point zero data point projected hence calculate expectation. mentioned above case kind coarse-grained self-consistency. original approach hastie done introducing non-linear principal manifolds constructed algorithm usually called hastie-stuelze principal manifolds. however global optimality principal manifolds guaranteed example second principal component sample normal distribution self-consistent correct principal curve course optimal one. also underline view object constructed algorithm dataset depends probabilistic interpretation nature chosen heuristic approach coarse-grained selfconsistency. suppose dataset generated i.i.d. sampling definition principal manifold purely operational principal manifold result application algorithm finite datasets. analogous remark applicable principal manifold approximators constructed finite datasets described chapter. independent fact straight line self-consistent principal curves claimed ‗biased‘. inspired tibshirani introduce alternative definition principal curve based directly continuous mixture model maximising regularized likelihood. approximators opposite principal curves self-consistent. however attempts construct practical algorithm finding globally optimal principal curves length successful. instead kégl developed efficient heuristic polygonal line algorithm constructing piecewise linear principal curves. length segment difference maximal minimal projection value onto first principal component. segment positioned contains projected data points. thus initial approximation vertices {yy} segment optimisation step. given partitioning obtained step functional optimised gradient technique. fixing partitioning needed calculate gradient otherwise differentiable function respect position vertices {yi}. adaptation step. choose segment largest number points projected onto segment exists longest chosen. vertex inserted midpoint segment; segments renumerated accordingly. smola proposed regularized principal manifolds framework based minimization quantization error functional large class regularizers used universal em-type algorithm. algorithm convergence rates analyzed showed regularizing series works authors chapter used metaphor elastic membrane plate construct one- twothree-dimensional principal manifold approximations various topologies. mean squared distance approximation error combined elastic energy membrane serves functional optimised. elastic algorithm extremely fast optimisation step simplest form smoothness penalty. implemented several programming languages software libraries front-end user graphical interfaces freely available web-site http//bioinfo.curie.fr/projects/vidaexpert. software found applications microarray data analysis visualization genetic texts visualization economical sociological data fields central vertex vertices connected). consider describes embedding graph multidimensional space. elastic energy graph embedding euclidean space defined historically first explored used applications. avoid confusion notice term elastic independently introduced several groups solving traveling salesman problem context principal manifolds recently context regularized regression problem three notions completely independent denote different things. euclidean space apply algorithm estimating elastic principal manifold finite dataset. based turn general algorithm estimating locally optimal embedding arbitrary elastic graph described below. usual algorithm described gives locally optimal solution. expect number local minima energy function grows increasing ‗softness‘ elastic graph this order obtain solution closer global optimum softening strategy proposed used algorithm estimating elastic principal manifold. constructed manifold provides regularization distances node positions initial steps softening. final stage softening zero little effect manifold configuration. pproximating datasets dimensional principal curves satisfactory case datasets intuitively characterized branched. principal object naturally passes ‗middle‘ data distribution also branching points missing simple structure principal curves. introducing branching points converts principal curves principal graphs. extension one-dimensional principal curves context skeletonisation hand-written symbols. important part definition form penalty imposed onto deviation configuration branching points embedment ‗ideal‘ configuration assigning types vertices serves definition penalty total deviation graph ‗ideal‘ configuration proposed universal form non-linearity penalty branching points. form penalty defined previous chapter elastic energy graph embedment. naturally generalizes simplest three-point second derivative approximation squared k-star penalty equals zero position central node coincides mean point neighbors. embedment ‗ideal‘ penalties equal zero. primitive elastic graph means embedment harmonic function graph value non-terminal vertex mean value closest neighbors vertex. non-primitive graphs consider stars include neighbors centers. example square lattice create elastic graph using -stars vertical -stars horizontal -stars. elastic non-boundary vertex belongs stars. general elastic graph sets k-stars harmonic functions linear; consider cubic lattice primitive graph correspondent pluriharmonic functions harmonic ones; create cubic lattice standard elastic -stars pluriharmonic functions linear. luriharmonic functions many attractive properties example satisfy following maximum principle. vertex elastic graph called corner point extreme point centre k-star onvex functions achieve maxima corner points. even particular case theorem linear functions quite useful. linear functions achieve maxima minima corner points. theory principal curves manifolds penalty functions introduced penalise deviation linear manifolds proposed pluriharmonic embeddings ‗ideal objects‘ instead manifolds introduce penalty deviation ideal form. configuration; elastic principal graphs explicitly measure deviation ‗ideal‘ pluriharmonic graph elastic energy structural complexity measure non-decreasing function number vertices edges k-stars different orders sc=sc; function penalises number structural elements; construction complexity defined respect grammar elementary transformation. graph grammars provide welldeveloped formalism description elementary transformations. elastic graph grammar presented production rules. rule form elastic graphs. rule applied elastic graph copy removed graph together incident edges replaced copy edges connect graph. full description language need notion labeled graph. labels necessary provide proper connection graph approach based graph grammars constructing effective approximations elastic principal graph recently proposed o={o..os}. possible applications graph grammar operation graph gives transformations initial graph number possible applications also define sequence different graph grammars choose grammar elementary transformations predefined boundaries structural complexity scmax construction complexity ccmax elasticity coefficients initialize elastic graph vertices connected edge. chosen belong initial first principal line data points projected onto principal line segment defined j=…r repeat steps collection candidate graph transformations separate permissible forbidden transformations; permissible transformation scmax scmax predefined structural complexity ceiling; class primitive elastic graphs consists operations transformation ‗remove leaf‘ applied vertex connectivity degree equal remove remove edge connecting tree; transformation ‗remove edge‘ applicable pair graph vertices connected edge delete edge delete vertex merge k-stars central nodes make k-star central node neighbors union neighbors k-stars illustration simple node node bisect edge graph grammar. start simple -star generate three distinct graphs shown. operation adding node node operations edge bisections illustration suppose operation gives biggest elastic energy decrement thus optimal operation. graph obtained generate distinct graphs choose optimal one. process continued definite number nodes inserted. efinition. „add node bisect edge‟ graph grammar applicable class primitive elastic graphs consists operations transformation node applied vertex node edge transformation bisect edge applicable pair graph vertices connected edge delete edge vertex edges transformation elastic structure induced change topology elastic graph primitive. consecutive application operations grammar generates trees i.e. graphs without cycles. manifold projected first principal components data points shown projected closest vertex elastic net; visualization data points internal coordinates classes represented form hinton diagrams size diagram proportional number points projected shape diagram denote three different point classes; data points shown projected closest point piecewise linearly interpolated elastic map; based projection shown e)-g) first iterations principal tree algorithm tree shown projected onto principal plane; metro representation iris dataset. estimating elastic principal graph gives approximation principal trees. introducing ‗tree trimming‘ grammar allows produce principal trees closer global optimum trimming excessive tree branching fusing k-stars separated small ‗bridges‘. project points multidimensional space closest node tree. tree construction one-dimensional object projection performs dimension reduction multidimensional data. question produce planar tree layout? course many ways layout tree plane without edge intersection. would useful local tree properties global distance relations would represented using layout. require two-dimensional layout k-stars represented equiangular; small penalty configuration; edge lengths proportional length multidimensional embedding; thus represent between-node distances. defines tree layout global rotation scaling also changing order leaves every k-star. change order eliminate edge intersections result guaranteed. order represent global distance structure found good approximation order k-star leaves taken projection every k-star linear principal plane calculated data points local principal plane vicinity k-star calculated points close star. resulting layout optimized using greedy optimization methods. point projections represented diagrams size diagram reflects number points projected corresponding tree node. sectors diagram allow show proportions points different classes projected node data display called metro since schematic ―idealized‖ representation tree data distribution inevitable distortions made produce layout. however using still estimate distance point point passing points. inherently unrooted useful compare metaphor trees produced hierarchical clustering metaphor closer ―genealogy tree‖. approximate multidimensional data r-dimensional object number points object grows exponentially. obstacle grammar–based algorithms even modest analysis rule copies introduction cubic complex useful factorization principal object allows avoid problem. factorized graphs application transformations. factorized graphs apply factors. approach significantly reduces amount trials selection optimal application. simple grammar rules ―add node node bisect edge‖ also powerful here produces products primitive elastic trees. product elastic structure defined topology factors. applied incomplete data minimal modifications subject random dense distribution missing values example iterative method incomplete data matrix developed gorban rossiev geometrical objects embedded calculate distance object generalize ―data approximation‖-based ―variation-maximization‖-based definitions linear pca. also exists whole family methods briefly mention here generalize ―distance distortion minimization‖ definition dissimilarity) matrix construct configuration points lowdimensional euclidean space distance matrix space reproduce maximal precision. fundamental series metric multidimensional scaling next kernel approach takes advantage fact linear algorithm needs matrix pairwise scalar products explicit values coordinates allows apply kernel trick substitute gramm matrix scalar products calculated kernel functions. kernel method tightly related classical multidimensional scaling somap laplacian eigenmap methods start construction neighbourhood graph i.e. graph close sense data points connected edges. weighted graph represented form weighted adjacency matrix {wij}. graph isomap constructs distance matrix based path lengths points neighbourhood graph multidimensional scaling applied laplacian solves eigenproblem trees algorithms standard iris dataset expected twodimensional approximation principal manifold case close linear principal plane. also principal tree illustrates well fact almost complete separation classes data space. molecule. example van-der-waals molecular surface formed surrounding every atom molecule sphere radius equal characteristic radius van-der-waals force. interior points eliminated forms complicated non-smooth surface practice surface sampled finite number points. molecular surface small piece molecule first made approximation dataset principal curve. interestingly curve followed backbone molecule forming helix second approximated molecular surface manifold. topology surface expected spherical applied spherical topology elastic optimisation. molecule. stick-and-balls model stretch initial molecular surface onetwo-dimensional spherical principal manifolds molecular surface; simple principal cubic complex branching case. spherical grid corrections performed edge elasticities grid initialization -multidimensional space. color codes denote point classes corresponding possible frameshifts random fragment overlaps coding gene black color corresponds non-coding regions. every genome principal tree shown together projection data distribution. note clusters appear mixed plot escherichia coli well separated metro map. proves well-separated dataset constructed string sequence using short word frequency dictionary approach following notion word defined; possible short words defined them; number text fragments certain width sampled text; fragment frequency occurrences possible short words calculated thus fragment represented vector multidimensional space whole text represented dataset vectors embedded case defined word sequence three letters {acgt} alphabet evidently possible triplets {acgt} alphabet; sampled fragments width genomic sequence; calculated frequencies nonoverlapping triplets every fragment. constructed datasets interesting objects data-mining non-trivial cluster structure usually contains various configurations clusters class labels assigned points accordingly available genome annotations; case information presence absence coding information current position genome; using data mining techniques immediate applications field automatic gene recognition others example fig. show application classical metro methods several bacterial genomes. look http//www.ihes.fr/~zinovyev/clusters web-site information. expository overview provided leung cavalieri technology found numerous applications understanding various biological processes including cancer. allows screen simultaneously expression genes cell exposed specific conditions obtaining sufficient number observations construct table \"samples genes\" containing logarithms expression levels typically several thousands genes typically several tens samples. fig. provide comparison data visualization scatters projection breast cancer dataset provided wang onto linear twonon-linear two-dimensional principal manifold. latter constructed elastic maps approach. point represents patient treated cancer. dimension reduction represented vector containing expression values genes tumor sample. linear non-linear principal manifolds provide mappings drastically reducing vector dimensions allowing data visualization. form shape size point fig. represent various clinical data extracted patient‘s disease records. practical experience bioinformatics studies shows two-dimensional data visualization using non-linear projections allow catch signals data linear projections fig. good example ivakhno armstrong figure visualization breast cancer microarray dataset using elastic maps. initio classifications shown using points size shape color configuration nodes projected three-dimensional principal linear manifold. clear feature dataset curved mapped adequately onto two-dimensional principal plane. distribution points internal non-linear manifold coordinates shown together estimation two-dimensional density points. linear two-dimensional manifold. notice ``basal'' breast cancer subtype much better separated non-linear mapping features distribution become better resolved. addition that gorban zinovyev performed systematic comparison performance low-dimensional linear non-linear principal manifolds microarray data visualization using following four criteria mean-square distance error; distortions mapping distances points; local point neighbourhood preservation; compactness point class labels projection. demonstrated non-linear two-dimensional principal manifolds provide systematically better results accordingly criteria achieving performance threefourdimensional linear principal manifolds interactive vimida vidaexpert software allowing microarray data visualization non-linear principal manifolds available web-site institut curie http//bioinfo.curie.fr/projects/vidaexpert http//bioinfo.curie.fr/projects/vimida. chapter gave brief practical introduction methods construction principal objects i.e. objects embedded ‗middle‘ multidimensional data set. basis took unifying framework mean squared distance approximation finite datasets allowed look principal graphs manifolds generalizations mean point notion. arthur vassilvitskii k-means++ advantages careful seeding. proceedings eighteenth annual acm-siam symposium discrete algorithms soda orleans louisiana january data using localized principal components application astronomical data. gorban kégl wunch zinovyev principal manifolds data visualization dimension reduction lecture notes computational science engineering springer berlin-heidelberg. manifolds principal trees metro maps elastic cubic complexes. gorban kégl wunsch zinovyev principal manifolds data visualization dimension reduction lecture notes principal manifolds application microarray data visualization. gorban kégl wunsch zinovyev principal manifolds data visualization dimension reduction lecture notes computational science engineering springer berlin-heidelberg. gorban kégl wunsch zinovyev principal manifolds data visualization dimension reduction lecture notes computational science engineering springer berlinheidelberg. principal components. journal educational psychology ivakhno armstrong j.d. non-linear dimensionality reduction signalling networks. systems biology probabilistic interpretation spectral embedding clustering algorithms. gorban kégl wunch zinovyev principal manifolds data visualization dimension reduction lecture notes computational science engineering springer berlinheidelberg. effectiveness lloyd-type methods k-means problem. proceedings annual ieee symposium foundations computer science ieee computer society washington usa. pearson lines planes closest systems points space. philosophical magazine series smola a.j. mika schölkopf williamson r.c. regularized principal manifolds. journal machine learning research steinhaus division corps materiels parties. bull. maps gorban kégl wunsch zinovyev principal manifolds data visualization dimension reduction lecture notes computational science engineering springer berlinheidelberg. rof. alexander gorban obtained degree differential equations mathematical physics degree biophysics holds chair applied mathematics university leicester chief scientist institute computational modelling russian academy sciences scientific interest include interdisciplinary problem model reduction topological dynamics physical chemical kinetics mathematical biology data mining. andrei zinovyev obtained university formation theoretical physics obtained degree computer science institute computational modelling russian academy sciences defending moved france institut hautes etudes scientifiques bures-sur-yvette work post-doc years mathematical biology group professor misha gromov. since january head computational systems biology cancer team institut curie paris. main area scientific expertise bioinformatics systems biology cancer dimension reduction highthroughput data analysis model reduction dynamical models. principal manifold intuitively smooth manifold going middle data cloud; formally exist several definitions case data distributions hastie stuelze‘s principal manifolds self-consistent curves surfaces; kegl‘s principal curves provide minimal mean squared error given limited curve length; tibshirani‘s principal curves maximize likelihood additive noise data model; gorban zinovyev elastic principal manifolds minimize mean square error functional regularized addition energy manifold stretching bending; smola‘s regularized principal manifolds minimize form regularized quantization error functional; definitions. principal graph graph embedded multidimensional data space providing minimal mean squared distance dataset combined deviation ―ideal‖ configuration exceeding limits complexity expectation/maximisation algorithm generic splitting algorithmic scheme almost algorithms estimating principal objects constructed; consists basic steps projection step data projected onto approximator maximization step approximator optimized given projections obtained previous step.", "year": 2008}