{"title": "TensorLog: Deep Learning Meets Probabilistic DBs", "tag": ["cs.AI", "cs.LG", "I.2.4; I.2.6"], "abstract": "We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.", "text": "present implementation probabilistic ﬁrst-order logic called tensorlog classes logical queries compiled diﬀerentiable functions neuralnetwork infrastructure tensorﬂow theano. leads close integration probabilistic logical reasoning deep-learning infrastructure particular enables high-performance deep learning frameworks used tuning parameters probabilistic logic. experimental results show tensorlog scales problems involving hundreds thousands knowledge-base triples tens thousands examples. recent progress deep learning profoundly aﬀected many areas artiﬁcial intelligence. exception probabilistic ﬁrst-order logical reasoning. paper seek closely integrate probabilistic logical reasoning powerful infrastructure developed deep learning. goal enable deep learners incorporate ﬁrst-order probabilistic conversely enable probabilistic reasoning outputs deep learners. motivation consider program figure could plausibly used answering simple natural-language questions director apocalyse now? main predicate answer takes question produces answer predicates actedin directed purpose performing natural-language analysis also extended facts text composes training test data stores information word n-grams contained question strings possible names entity words contained names n-grams. underlined predicates indicateslabel important popular soft predicates goal learning appropriate weights soft-predicate facts—e.g. learn indicateslabel high weight. ideally weights would learned indirectly observing inferences made using case would like learn question-answer pairs rely indirectly predicates like actedin rather hand-classiﬁed questions judgements speciﬁc facts soft predicates. main technical obstacle integration probabilistic logics deep learners existing ﬁrst-order probabilistic logics easily adapted evaluation gpu. superﬁcial problem computations made theorem-proving numeric also fundamental problem discuss. common approach ﬁrst-order inference ground ﬁrst-order logic converting zeroth-order format boolean formula probabilistic graphical model. instance context particular rule boolean disjunction embedded neural network e.g. initialize architecture regularizer probabilistic ﬁrst-order languages grounding typically results directed graphical model survey work). target architecture modern deep learners based gpus limited memory hence grounding approach used small short rules. example describes experimental results rules dozen facts largest datasets considered contain examples. although probabilistic logic implementations require explicit grounding similar problem arises using neural-network platforms implement probabilistic logic computationally hard. many probabilistic logics answering queries p-complete worse. since networks constructed modern deep learning platforms evaluated time polynomial size polysize network implement logic unless p=p. paper addresses obstacles several interrelated contributions. first section identify restricted family probabilistic deductive databases called polytree-limited stochastic deductive knowledge graphs tractable still reasonably expressive. formalism variant stochastic logic programs also show ptree-sdkgs sense maximally expressive cannot drop polytree restriction switch conventional possible-worlds semantics without making inference intractible. next section present algorithm performing inference ptree-sdkgs. algorithm performs inference dynamic-programming method formalize belief propagation certain factor graph random variable factor graph correspond possible bindings logical variable proof factors correspond database predicates. words random variables multinomials constants database factors constrain bindings consistent database predicates relate corresponding logical variables. although simple idea knowledge novel. also discuss detail implementation logic called tensorlog. section review usual deﬁnitions logic programs deductive databases also introduce term deductive knowledge graph deductive databases containing unary binary predicates. section omitted readers familiar logic programming. figure example database theory. uppercase symbols universally quantiﬁed variables clause read logical implication database constants child infant proved status also proved. figure example proof tree. root second level uses rule next level uses unit clause childleft unit clause childright; ﬁnal level uses brotherright. written called head clause body bi’s called literals. literals must form predicate symbol xi’s either logical variables database constants. database constants written number arguments literal called arity. paper focus case literals binary unary i.e. arity two. call database knowledge graph program deductive knowledge graph also assume constants appear database theory pair written model smallest superset deductively closed respect every clause least model unique usual semantics ground fact considered true model. ﬁnite. restrict discussion theories ﬁnite proof graphs. case answers query found systematically searching proof tree solution vertices. number strategies exist this popular used prolog uses depth-ﬁrst search ordering edges picking ﬁrst empty list indicates solution vertex reached.) finally assume answers exist produce conditional probability distribution answers query normalizing i.e. following terminology pure unnormalized slp. slps originally deﬁned fairly expressive class logic programs namely programs fail free sense dead ends proof graph prior work slps also considered special case normalized slps weights outgoing edges every vertex one. normalized fail-free slps simple slps closely connected several well-known types probabilistic reasoners. slps deﬁned introducing probabilistic choices top-down theorem-proving process since top-down theorem-proving logic programs analogous program execution ordinary programs slps thought logic-program analogs probabilistic programming languages like church normalized slps also conceptually quite similar stochastic grammars pcfgs except stochastic choices made theorem-proving rather rewriting string. form—i.e. consists theory contains function-free clauses database second restrict predicates unary binary. third restrict clauses theory weight meaningful weights sdkgs ﬁnal connection logics made considering logic program grounded conversion boolean formulae. simple approach implementing soft extension boolean logic evaluate truth falsity formula bottom-up deriving numeric conﬁdence subexpression conﬁdences associated subparts. instance might rules approach implementing soft logic sometimes called extensional approach common practical systems uses extensional approach several recent neural approaches slps relatively simple proof procedure informally inference requires computing weighted count proofs query weight particular proof computed quickly. natural question whether computationally eﬃcient theoremproving schemes exist slps. similarity slps probabilistic context-free grammars suggests eﬃcient schemes might exist since eﬃcient dynamicprogramming methods probabilistic parsing. unfortunately case even proof appears appendix. result especially surprising easy small theories exponentially many proofs e.g. clause equation exponentially many proofs naive proof-counting methods expensive clause. graph vertex edge share variable. graph polytree path pair vertices i.e. strongly connected component graph tree. finally deﬁne theory polytree-limited inﬂuence graph every clause polytree. figure contains examples polytree-limited clauses. proof follows correctness dynamic-programming algorithm sdkg inference present below detail section brief algorithm based belief propagation certain factor graph. construct graph random variables multinomials database constants random variable corresponds logical variable proof graph. logical literals proof correspond factors constrain bindings variables make literals true. importantly goal compilation deep-learning frameworks messagepassing steps used belief propagation deﬁned numerical operations given predicate input/output mode message-passing steps required perform belief propagation unrolled function diﬀerentiable. constants theory. assume constants appear database theory. relax this note possible introduce constant theory creating special unary predicate holds constant e.g. constant tired could create database predicate assign tired contains fact assign tired introduce variable bound constant tired needed. instance clause figure would rewritten rule weights rule features. sdkg weights associated facts databases rules theory however standard trick used lift weights database rules simply introduces special clause-speciﬁc fact clause body example weighted version clause could re-written indicates ages children used features determine rule succeeds. equivalent rule status -child weighted experiments below compare proppr construction. problog well-known logic programming language adopts semantics large literature approaches tractibly estimating naively requires marginalizing result well known instance suciu olteanu show p-hard compute probabilities one-rule theory completeness appendix paper contains proof emphasizes fact reasonable syntactic restrictions unlikely make inference tractible. particular theory used construction extremely simple predicates unary contain three literals body. section present eﬃcient dynamic-programming method inference polytreelimited sdkgs. formalize method belief propagation certain factor graph random variables factor graph correspond possible bindings logical variable proof factors correspond database predicates. words random variables multinomials constants database factors constrain bindings consistent database predicates related corresponding logical variables. although using belief propagation simple idea knowledge novel method ﬁrst-order probabilistic inference. certainly quite diﬀerent common formulations ﬁrst-order probabilistic inference random variables typically bernoulli random variables correspond potential ground database facts ultimate goal integration neural networks implement reasoning deﬁning series numeric functions ﬁnds answers particular family queries. convenient encode database numerically. assume constants mapped integers. constant deﬁne onehot row-vector representation i.e. vector dimension also represent binary predicate sparse matrix unary predicate analogous vector since goal reasoning system correctly answer queries using functions also introduce notation functions answer particular types queries particular predicate symbol denotes query response function queries predicate mode deﬁne query response function query form function which given one-hot encoding returns appropriate conditional probability vector figure algorithm unrolling belief propagation polytree sequence message-computation operations. notes replace wise) product all-ones vector returned. taining non-recursive polytree-limited clause obeys restrictions above. build factor graph follows logical variable body random variable every literal body clause factor potentials linking variables finally factor graph disconnected factors components connected. figure gives examples. variables appearing clause’s head starred. make ﬁnal step toward integration algorithm neural-network platforms must ﬁnally compute explicit diﬀerentiable query response function computes unroll message-passing steps series operations. figure shows algorithm used current implementation tensorlog code found convenient extend notion input-output modes query follows variable appearing literal clause body nominal input appears input position head literal left body nomimal output otherwise. prolog convention nominal inputs appear ﬁrst argument predicate tensorlog user respects convention forward message-passing steps rather code contains mutually recursive routines invoked requesting message output variable ﬁctional output literal. result emit series operations return name register contains unnormalized conditional probability vector output variable. instance sample clauses functions returned shown table extend idea theories many clauses. ﬁrst note several clauses predicate symbol head simply unnormalized query response functions e.g. predicate uncle deﬁned rules would deﬁne complex situation clauses predicate second theory predicate body example would case aunt also deﬁned theory rather database. theory recursion replace message-passing operations vxmq function call likewise operation vxmt shown equivalent taking factor graph splicing graph also possible allow function calls recurse ﬁxed maximum depth must simply extra argument tracks depth recursively-invoked functions make sure returns all-zeros vector depth bound exceeded. currently implemented marking learned functions predicate mode depth argument ensuring function calls inside computationally algorithm describe quite eﬃcient. assuming matrices exist additional memory needed factor-graph linear size clause hence compilation response functions linear theory size number steps ptree-sdkgs tree number message-passing steps also linear. message size limited often smaller practice compilation execution. current implementation tensorlog operates ﬁrst unrolling belief-propagation inference intermediate form consisting sequences abstract operators suggested examples table unrolling code performs number optimizations sequence in-line important operator sequences cross-compiled expressions possible back deep learning frameworks tensorﬂow theano operator sequences also evaluated diﬀerentiated local infrastructure implemented scipy sparse-matrix package includes operations actually needed inference simple gradient-descent optimizer. local infrastructure’s main advantage makes sparse-matrix representations. implementations matrices correspond relations sparse. messages corresponding one-hot variable binding possible bindings variable sparse vectors local infrastructure dense vectors tensorﬂow theano versions allow implementations multiplication dense vectors sparse matrices. typed predicates. practically important extension language tensorﬂow theano targets include machinery declaring types arguments database predicates inferring types logic programs instance sample program figure might include declarations like actedin indicateslabel. typing reduces size message vectors large constant factor increases potential minibatch size speeds run-time similar factor. constraining optimizer. tensorlog’s learning changes numeric score every soft fact using gradient descent. proof-counting semantics used tensorlog fact score could semantically meaningful instance costar might plausibly number movies actors appeared together. however semantically meaningful allow negative. prevent this learning parameter replace occurrence function unconstrained optimization performed optimize value regularization. default tensorlog trains minimize unregularized cross-entropy loss. however modern deep-learning frameworks quite powerful relatively easy crosscompiled functions produced tensorlog slight variants learning problem—often requires lines code. instance figure illustrates lregularization tensorlog’s loss function using tensorﬂow backend. also relatively easy extend tensorlog ways. discuss several possible extensions experimented extensively although veriﬁed implemented current framework. learning tensorlog’s training data consists queries corresponding desired outputs |cm. possible train examples multiple predicates instance example program figure could include training examples answer matches. alternative semantics query responses. natural extension would address limitation semantics namely weighting answers relative query sometimes leads loss information. example suppose answers father facts father father weight answer distinguish world joe’s paternity uncertain world fathers. possible solution learn parameters appropriate soft threshold element e.g. redeﬁne tlog tensorlog.simple.compiler train data tlog.load dataset test data tlog.load dataset data stored dictionary mapping function speciﬁcation like pair rows possible inputs desired outputs. function spec train data.keys assume function spec train data construct tensorﬂow version loss function function used inference unregularized loss tlog.loss tlog.inference regularization terms loss regularized loss unregularized loss weight tlog.trainable variables optimizer inputs optimizer optimizer tf.train.adagradoptimizer train step optimizer.minimize inputs dictionary keys name appropriate variables used loss function extension call host infrastructure. second extension allow tensorlog functions call backend language. suppose example wish replace classification predicate example program figure tensorﬂow model e.g. multilayer perceptron buildmlp function constructs expression evaluates input instruct compiler include model place usual function gclassification date experimentally explored capability depth; however would appear useful able write logical rules arbitrary neurally-deﬁned lowlevel predicates rather merely facts. note compilation approach also makes easy export tensorlog predicate deep learner function maps question possible answers conﬁdences. might useful building still complex model non-logical model long tradition embedding logical expressions neural networks purpose learning generally done indirectly conversion logic boolean formula rather developing diﬀerentiable theorem-proving mechanism considered here. embedding logic lead useful architecture regularizer recently proposed diﬀerentiable theorem prover proof example unrolled network. system includes representation-learning component well template-instantiation approach allowing structure learning well. however published experiments system limited small datasets. another recent paper describes system non-logical compositionally deﬁned expressions converted neural components questionanswering tasks. many ﬁrst-order probabilistic models implemented grounding i.e. conversion traditional representation. context deductive rule considered ﬁnite disjunction ground instances instance rule example markov logic networks widely-used probabilistic ﬁrst-order model bernoulli random variable associated potential ground database fact facts database binary predicate) ground instance clause factor. markov ﬁeld inference markov ﬁeld also expensive motivated development probabilistic similarity logic variant uses tractible hinge loss well lifted relational neural networks logic tensor networks recent models grounds ﬁrstorder theories neural network. however grounded model ﬁrst-order theory large limiting scalability techniques. noted above tensorlog closely related stochastic logic programs probabilistic process associated top-down theoremprover i.e. clause used derivation assocated probability number times used deriving explanation slps prslp probability distribution generated tensorlog slps normalized unnormalized normalized slps deﬁned r∈sp tensorlog represent normalized unnormalized slps normalized slps generalize probabilistic context-free grammars unnormalized slps express bayesian networks markov random ﬁelds proppr variant slps stochastic proofgeneration process augmented reset transitional probabilities based normalized soft-thresholded linear weighting features. ﬁrst extension slps easily modeled tensorlog second cannot equivalent proppr also includes approximate grounding procedure generates networks bounded size. asymptotic analysis suggests proppr faster large database small numbers training examples tensorlog faster large numbers training examples moderate-sized databases. compared tensorlog’s inference time problog mature probabilistic logic programming system implements tuple independence semantics inference problems described version friends smokers problem simpliﬁed model social inﬂuence. small graphs artiﬁcially generated using preferential attachment model details described; instead used small existing network dataset displays preferential-attachment statistics. inference times report inference tasks subset randomly-selected entities. shown table spite querying times many entities tensorlog many times faster. also compare path-ﬁnding task intended test performance deeply recursive tasks. goal compute ﬁxed-depth transitive closure grid -by- grid used maximum path length tensorlog shows much faster performance better scalability shown table times larger -by- grid. tensorlog’s maximum path length larger grid. also compared experimentally proppr several standard benchmark learning tasks. chose traditional relational learning tasks proppr outperformed plausible competitors mlns. cora citation-matching task hand-constructed rules.. second learning common relation aﬀects umls using rule learned algorithm finally motivated recent comparisons proppr embedding-based approaches knowledge-base completion also compared proppr relation-prediction tasks involving wordnet using rules theories used table shows accuracy systems learning quite comparable even rather simplistic learning scheme. proppr course well suited tight integration deep learners. results section demonstrate tensorlog’s approximation problog’s semantics eﬃcient useful. demonstrate tensorlog eﬃciently usefully approximate deeply recursive concepts posed learning task -by- grid maximum depth trained tensorlog approximate distribution task. dataset consists grid cells connected edges example queries form path particular grid cell. picked queries test remainder train trained single positive answer query path extreme corner closest a—i.e. corners initial weights edges uniformly training epochs local backend ﬁxed-rate gradient descent learner using learning rate brings accuracy test cases learning takes less sec/epoch. learning query times still quite fast shown table. table also includes visualization learned weights small grid. every pair adjacent grid cells weights learn edge converse. weight pair show single directed edge colored magnitude diﬀerence. observe problog addition implementing full tuple-independence semantics implements much expressive logic considered here including large portion full prolog contrast tensorlog includes subset datalog. extent comparison unfair. also observe although task seems simple quite diﬃcult probabilistic logics deeply recursive theories lead large deep proofs. tensorlog’s inference schemes scale well task still challenging optimize parameters especially larger grid sizes. problem unrolling inference leads large graphs especially compiled relatively ﬁne-grained operations used deep-learning infrastructure. table shows size networks compilation tensorﬂow various extensions -by- depth task. although growth linear depth constants large e.g. tensorﬂow -by- depth network memory gpu. second problem constructed networks deep leads problems optimization. smaller task local optimizer required careful tuning initial weights learning rate reliably converge. size complexity task suggested second experiments varied task complexity ﬁxing parameters optimizers. local optimizer ﬁxed parameters used -by- depth task tensorﬂow backend used adagradoptimizer default learning rate running epochs. results shown table illustrate several advantages using mature deep-learning framework backend tensorlog. sophisticated optimizers available tensorﬂow appear robust. particular adagrad performs well depth around ﬁxed-rate optimizer performs well depths larger scale experiment used wikimovies question-answering task proposed task similar shown figure consists tuples containing information relations movies. sample questions answers below double quotes identifying entities. learning times local infrastructure quite variable larger sizes numerical instabilities often cause optimizer fail. computing times discard runs overﬂow underﬂow harder detect. high variance accounts anomolously average time grid size encoded questions extending additional relations mentionsentity true question mentions entity hasfeature true question contains feature entities mentioned question extracted looking every longest match name features question simply words question theory variant given example figure main diﬀerence simple longest-exact-match heuristic described identiﬁes entities accurately dataset made mentionsentity hard predicate. also extended theory handle questions answers either movie-related entities movies tensorﬂow backend minibatches size adagrad optimizer learning rate running epochs regularization. compare accuracy results prior neural-network based methods applied task. shown table tensorlog performs better prior state-of-the-art task quite eﬃcient. paper described scheme integrate probabilistic logical reasoning powerful infrastructure developed deep learning. goal enable deep learners incorporate ﬁrst-order probabilistic conversely enable probabilistic reasoning outputs deep learners. tensorlog system describe here makes possible reasonable scale using conventional neural-network platforms. paper contains several interrelated technical contributions. first identiﬁed family probabilistic deductive databases called polytree-limited stochastic deductive knowledge graphs tractable still reasonably expressive. language variant slps maximally expressive cannot drop polytree restriction switch possible-worlds semantics without making inference intractible. argue logics tractable unlikely practically incorporated neural networks. second presented algorithm performing inference ptree-sdkgs based belief propagation. computationally algorithm quite eﬃcient. assuming matrices exist additional memory needed factor-graph linear size clause hence compilation linear theory size recursion depth. knowledge ﬁrst-order inference setting novel. finally present implementation logic called tensorlog. implementation makes possible call tensorlog inference within neural models conversely call neural models within tensorlog. current implementation tensorlog includes number restrictions. backends implemented tensorﬂow theano tensorﬂow backend extensively tested evaluated. also exploring compilation pytorch supports dynamic networks. also plan implement support stable optimization better support debugging. noted above tensorlog also makes possible replace components logic program submodels learned deep-learning infrastructure. alternatively export answer predicate deﬁned logic deep learner function maps question possible answers conﬁdences; might useful building still complex model non-logical model future work hope explore capabilities. also note although experiments paper assume theories given problem learning programs tensorlog also great interest. early results authors problem discussed elsewhere thanks william wang providing datasets used here; william wang many colleagues contributed technical discussions advice. author greatful google ﬁnancial support also support work grants ccf- iis-. reduce counting proofs psat computing probabilities sdkgs. psat p-hard task goal count number satisfying assignments formula literals clause positive hence psat formula form database contains facts assign assign weight facts binary binary weights also contains deﬁnition predicate either containing following three weight facts either either either total facts database. ﬁrst line ﬁrst rule assignment xi’s selected uniform easy literal either succeed i-th probability. clause made true assignment. hence ﬁrst rule theory succeed exactly times number satisfying assignments formula. second clause succeeds once note variable drawn database distribution contain either possible interpretations. corresponds boolean assignment assignment exactly corresponding interpretation. clearly clausei predicate succeeds exactly i-th clause satisﬁed hence pr|db|t thus exactly theory quite simple contains binary predicates rules short. thus diﬃcult identify syntactic restrictions might make proof-counting tractible possible-worlds scenario.", "year": 2017}