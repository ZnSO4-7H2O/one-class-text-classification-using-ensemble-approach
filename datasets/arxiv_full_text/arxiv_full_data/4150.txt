{"title": "DISCO Nets: DISsimilarity COefficient Networks", "tag": ["cs.CV", "cs.AI"], "abstract": "We present a new type of probabilistic model which we call DISsimilarity COefficient Networks (DISCO Nets). DISCO Nets allow us to efficiently sample from a posterior distribution parametrised by a neural network. During training, DISCO Nets are learned by minimising the dissimilarity coefficient between the true distribution and the estimated distribution. This allows us to tailor the training to the loss related to the task at hand. We empirically show that (i) by modeling uncertainty on the output value, DISCO Nets outperform equivalent non-probabilistic predictive networks and (ii) DISCO Nets accurately model the uncertainty of the output, outperforming existing probabilistic models based on deep neural networks.", "text": "present type probabilistic model call dissimilarity coefﬁcient networks disco nets allow efﬁciently sample posterior distribution parametrised neural network. training disco nets learned minimising dissimilarity coefﬁcient true distribution estimated distribution. allows tailor training loss related task hand. empirically show modeling uncertainty output value disco nets outperform equivalent non-probabilistic predictive networks disco nets accurately model uncertainty output outperforming existing probabilistic models based deep neural networks. interested class problems require prediction structured output given input complex applications often large uncertainty correct value example consider task hand pose estimation depth images wants accurately estimate pose hand given depth image depth image often occlusions missing depth values results uncertainty pose hand. therefore natural probabilistic models capable representing uncertainty. often capacity model restricted cannot represent true distribution perfectly. case choice learning objective inﬂuences ﬁnal performance. similar lacoste-julien argue learning objective tailored evaluation loss order obtain best performance respect loss. details denote ∆training loss function employed model training ∆task loss employed evaluate model’s performance. present simple example illustrate point made above. consider data distribution mixture bidimensional gaussians. consider models capture data probability distribution. model able represent bidimensional gaussian distribution diagonal covariance parametrised case neither models able recover true data distribution since ability represent mixture gaussians. words cannot avoid model error similarly real data scenario. model uses training loss ∆training. model employs loss emphasises ﬁrst dimension data speciﬁed ))/. model opposite employs loss function ))/. model performs grid search best parameters values figure shows contours mixture gaussians distribution data contour gaussian ﬁtted model detailed setting example available supplementary material. expected ﬁtted gaussian distributions differ according ∆training employed. table shows loss test evaluated ∆task minimised ∆training ∆task. simple example illustrates advantage able tailor model’s training objective function ∆training ∆task. contrast commonly employed learning objectives present section agnostic evaluation loss. order alleviate aforementioned deﬁciency state-of-the-art introduce disco nets class probabilistic model. disco nets represent true posterior distribution data distribution parametrised neural network. design learning objective based dissimilarity coefﬁcient dissimilarity coefﬁcient employ ﬁrst introduced deﬁned non-negative symmetric loss function. thus loss incorporated setting allowing user tailor disco nets needs. finally contrarily existing probabilistic models presented section disco nets require speciﬁc architecture training procedure making efﬁcient easy-to-use class model. related work deep neural networks particular convolutional neural networks comprised several convolutional layers followed fully connected layers interleaved non-linear function pooling. recent probabilistic models cnns represent non-linear functions data. observe models separate types. ﬁrst type model explicitly compute probability distribution interest. rather models allow user sample distribution feeding noise among models generative adversarial networks presented goodfellow popular used several computer vision applications example denton radford springenberg model consists networks simultaneously trained adversarial manner. generative model referred generator trained replicate data noise adversarial discriminative model referred discriminator trained identify whether sample comes true data training objective based minimax game networks approximately optimizes jensen-shannon divergence. however mentioned goodfellow radford models require careful design networks’ architecture. training procedure tedious tends oscillate. models generalized conditional mirza osindero additional input information generator discriminator. example mirza osindero cgan model generates tags corresponding image. gauthier applies cgan face generation. reed propose generate images ﬂowers cgan model conditional information word description ﬂower generate. application cgan promising little quantitative evaluation done. furthermore cgan models suffer difﬁculties mentioned gan. another line work developed towards statistical hypothesis testing learn probabilistic models. dziugaite authors propose train generative deep networks objective function based maximum mean discrepancy criterion. method statistical hypothesis test assessing probabilistic distributions similar. mentioned dziugaite test seen playing role adversary. second type model approximates intractable posterior distributions variational inference. variational auto-encoders presented kingma welling composed probabilistic encoder probabilistic decoder. probabilistic encoder input produces posterior distribution possible values noise could generated probabilistic decoder learns noise back data space training uses objective function based kullback-leibler divergence. models combined makhzani authors propose regularise autoencoders adversarial network. adversarial network ensures posterior distribution matches arbitrary prior hand pose estimation imagine user wants obtain accurate positions thumb index ﬁnger need accurate locations ﬁngers. task loss ∆task might based weighted l-norm predicted ground-truth poses high weights thumb index. existing probabilistic models cannot tailored task-speciﬁc losses propose dissimilarity coefﬁcient networks alleviate deﬁciency. disco nets begin description model specifying used generate samples posterior distribution samples turn employed provide pointwise estimate. subsequent subsection describe estimate parameters model. prediction sampling. disco consists several convolutional dense layers possibly pooling) takes input pair input data random noise. given pair disco produces value output example hand pose estimation input depth image convolutional layers. output last convolutional layer ﬂattened concatenated noise sample resulting vector several dense layers last dense layer outputs pose single depth image using different noise samples disco produces different pose candidates depth image. process illustrated figure importantly disco nets ﬂexible choice architecture. example noise could concatenated stage network including start. figure single depth image using different noise samples disco nets output different candidate poses depth image hand pose dataset tompson preprocessed oberweger best viewed color. denote distribution parametrised disco net’s neural network. given input disco nets provide user samples drawn without requiring expensive computation partition function. remainder paper consider rdz. pointwise prediction. order obtain single prediction given input disco nets principle maximum expected utility similarly premachandran prediction y∆task maximises expected utility rather minimises expected task-speciﬁc loss ∆task estimated using sampled candidates. formally prediction made follows learning disco nets objective function. want disco nets accurately model true probability words similar possible similarity evaluated respect loss speciﬁc task hand. given non-negative symmetric loss function outputs employ diversity coefﬁcient expected loss samples drawn randomly distributions. formally diversity coefﬁcient deﬁned intuitively minimise div∆ similar possible however uncertainty output predict given words diverse diverse well. thus encourage provide sample outputs given diverse minimising following dissimilarity coefﬁcient dissimilarity disc∆ difference diversity afﬁne combination diversity distribution given coefﬁcients introduced used latent variable models kumar need consider term div∆ constant problem thus disco nets objective function deﬁned follows optimisation. consider training dataset composed examples input-output pairs ..n}. order train disco nets need compute objective function equation knowledge true probability distributions overcome deﬁciency construct estimators diversity term div∆ div∆. first take empirical distribution data taking ground-truth pairs estimate distribution sampling outputs model gives unbiased estimate diversity term deﬁned candidate output sampled disco nets parameters disco nets. important note second term equation summing unbiased estimate therefore compute loss pairs different samples parameters learned gradient descent. algorithm shows training disco nets. steps algorithm draw random noise vectors input example generate candidate outputs input. allow compute unbiased estimate gradient step clarity remainder paper explicitely write parameters write strictly proper scoring rules. scoring rule learning. scoring rule deﬁned gneiting raftery evaluates quality predictive distribution respect true distribution using scoring rule ensure proper means maximised scoring rule said strictly proper unique maximiser hence maximising proper scoring rule ensures model aims predicting relevant forecast. gneiting raftery deﬁne score divergences corresponding proper scoring rule proper valid non-negative divergence function value example criterion mentioned section example type divergence. case loss expressed universal kernel deﬁne disco nets’ objective function divergence example theorem gneiting raftery take loss function y||β |i|)β/ excluding training objective strictly proper scoring rule employing proper scoring rule ensure objective function minimised true distribution expect disco nets generalise better unseen data. show strictly proper scoring rules also relevant assess quality distribution captured model. discriminative power proper scoring rules. observed fukumizu kernel density estimation fails high dimensional output spaces. goal compare quality distribution captured models setting better models according scoring rule associated divergence noted pinson tastu proper ensure observations drawn however scoring rule strictly proper scoring rule property ensured neighbourhood true distribution. experiments hand pose estimation given depth image often contains occlusions missing values wish predict hand pose hand pose dataset tompson estimate efﬁciency disco nets task. experimental setup hand pose dataset. hand pose dataset tompson contains testing training frames captured rgbd data ground-truth hand pose information. training composed images person whereas testing gathers samples persons. frame rgbd data kinects provided frontal view side views. experiments depth data frontal view. ground truth contains annotated joints follow evaluation protocol oberweger subset joints. also perform data preprocessing oberweger extract ﬁxed-size metric cube around hand depth image. resize depth values within cube patch normalized pixels deeper back cube missing depth values depth methods. employ loss functions outputs form energy score ∆training y||β ﬁrst goal assess advantages disco nets respect non-probabilistic deep networks. model referred discoβγ disco nets probabilistic model dissimilarity coefﬁcient equation taking noise injected model capacity discoβγ=. model baseβ non-probabilistic model taking objective function equation noise concatenated. corresponds classic deep network given input generates single output note write since noise concatenated. evaluation metrics. report classic non-probabilistic metrics hand pose estimation employed oberweger taylor mean joint euclidean error joint euclidean error fraction frames within distance refer reader supplementary material detailed expression metrics. metrics euclidean distance prediction ground-truth require single pointwise prediction. pointwise prediction chosen method among candidates. added probabilistic metric probloss. probloss deﬁned equation euclidean norm divergence associated strictly proper scoring rule. thus probloss ranks ability models represent true distribution. probloss computed using candidate poses given depth image. non-probabilistic model baseβ single pointwise predicted output available. construct candidates adding gaussian random noise mean diagonal covariance refer model baseβσ. architecture. novelty disco nets resides objective function. require speciﬁc network architecture. allows design simple network architecture inspired oberweger architecture shown figure input depth image convolutional layers ﬁlters kernels size stride followed rectiﬁed linear units pooling layers kernel size third last convolutional layer ﬁlters kernels size stride followed rectiﬁed linear unit. ouput convolution concatenated random noise vector size drawn uniform distribution result concatenation dense layers output size relus third dense layer outputs candidate pose r×j. non-probabilistic baseβσ model noise concatenated pointwise estimate produced. training. examples training frames construct validation dataset train examples. back-propagation used stochastic gradient descent batchsize learning rate ﬁxed momentum also l-regularisation controlled parameter relevant range comparative model baseβ best performing note disco nets report consistent performances across different values contrarily baseβ. different random seeds initialize model network parameters. report performance model best cross-validated seed train models epochs results change less value loss validation dataset baseβ. refer reader supplementary material details setting. results. quantitative evaluation. table reports performances test dataset parameters crossvalidated validation set. versions disco model outperform baseβ model. among different values better captures true distribution retaining accurate performance standard pointwise metrics. interestingly using all-zero noise test-time gives similar performances pointwise metrics. link observation mean method perform equivalently metrics qualitative evaluation. figure show candidate poses generated discoβ=γ=. testing examples. left image shows input depth image right image shows ground-truth pose candidate outputs model predict joint locations interpolate joints edges. edge thinner opaque means different predictions overlap uncertainty location edge’s joints low. discoβ=γ=. captures relevant information structure hand. figure visualisation discoβ=γ=. predictions examples testing dataset. left image shows input depth image right image shows ground-truth pose grey candidate outputs superimposed transparent red. best viewed color. comparison existing probabilistic models. best knowledge conditional generative adversarial networks mirza osindero applied pose estimation. order compare cgan disco nets several issues must overcome. first must design network architecture discriminator. ﬁrst disadvantage cgan compared disco nets require adversary. second mentioned goodfellow radford require careful design networks’ architecture training procedure. order fair comparison followed work mirza osindero practical advice presented larsen sønderby cgan initialising layers randomly cganinit ﬁxed initialising convolutional layers trained best-performing discoβ=γ=. section keeping layers ﬁxed. convolutional parts ﬁxed feature extractors depth image. setting similar employed tag-annotation images mirza osindero details setting found supplementary material. table shows cgan model obtains relevant results convolutional layers initialised trained model kept ﬁxed cganinit ﬁxed. results still worse disco nets performances. better architecture cgan experiments demonstrate difﬁculty training cgan disco nets. reference state-of-the-art values. train best-performing discoβ=γ=. section entire dataset compare performances state-of-the-art methods table figure state-of-the-art methods speciﬁcally designed hand pose estimation. oberweger constrained prior hand model referred nyu-prior reﬁned hand joint position increase accuracy referred nyu-prior-reﬁned. oberweger input depth image ﬁrst network nyu-init outputs pose used synthesize image second network. synthesized image used input depth image derive pose update. refer whole model nyu-feedback. contrary disco nets uses single network whose architecture similar nyu-prior accurately modeling distribution pose given depth image disco nets obtain comparable performances nyu-prior nyu-prior-reﬁned. without extra effort disco nets could embedded presented reﬁnement feedback methods possibly boosting state-of-the-art performances. discussion. presented disco nets family probabilistic model based deep networks. disco nets employ prediction training procedure based minimisation dissimilarity coefﬁcient. theoretically ensures disco nets accurately capture uncertainty correct output predict given input. experimental results task hand pose estimation consistently support theoretical hypothesis disco nets outperform non-probabilistic equivalent models existing probabilistic models. furthermore disco nets tailored task perform. allows possible user train tackle different problems interest. novelty resides mainly objective function disco nets require speciﬁc architecture easily applied problems. contemplate several directions future work. first apply disco nets prediction problems uncertainty output. second would like extend disco nets latent variables models allowing apply disco nets diverse dataset ground-truth annotations missing incomplete. acknowlegements. work funded microsoft research scholarship programme. would like thank pankaj pansari leonard berrada ondra miksik useful discussions insights. tilmann gneiting larissa stanberry eric grimit leonhard held nicholas johnson. assessing probabilistic forecasts multivariate quantities application ensemble predictions surface winds. test polyak. methods speeding convergence iteration methods. premachandran tarlow batra. empirical minimum bayes risk prediction extract extra few% performance vision models three parameters. cvpr", "year": 2016}