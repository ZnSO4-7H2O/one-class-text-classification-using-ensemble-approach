{"title": "Generating Focussed Molecule Libraries for Drug Discovery with Recurrent  Neural Networks", "tag": ["cs.NE", "cs.AI", "cs.LG", "physics.chem-ph", "stat.ML"], "abstract": "In de novo drug design, computational strategies are used to generate novel molecules with good affinity to the desired biological target. In this work, we show that recurrent neural networks can be trained as generative models for molecular structures, similar to statistical language models in natural language processing. We demonstrate that the properties of the generated molecules correlate very well with the properties of the molecules used to train the model. In order to enrich libraries with molecules active towards a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target.  Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.", "text": "novo drug design computational strategies used generate novel molecules good aﬃnity desired biological target. work show recurrent neural networks trained generative models molecular structures similar statistical language models natural language processing. demonstrate properties generated molecules correlate well properties molecules used train model. order enrich libraries molecules active towards given biological target propose ﬁne-tune model small sets molecules known active target. chemists designed whereas plasmodium falciparum reproduced test molecules. coupled scoring function model perform complete novo drug design cycle generate large sets novel molecules drug discovery. introduction chemistry language nature. chemists speak ﬂuently made discipline true contributors human well-being change live die. particularly true medicinal chemistry. however creating novel drugs extraordinarily hard complex problem. many challenges drug design sheer size search space novel molecules. estimated drug-like molecules could possibly synthetically accessible. chemists select examine molecules large space molecules active towards biological target. active means example molecule binds biomolecule causes eﬀect living organism inhibits replication bacteria. modern high-throughput screening techniques allow test molecules order lab. however larger experiments prohibitively expensive. given practical limitation vitro experiments desirable computational tools narrow enormous search space. virtual screening commonly used strategy search promising molecules amongst mil*correspondence marwin.segleruni-muenster.de institute organic chemistry center multiscale theory computation westfälische wilhelms-universität münster germany full list author information available article lions existing billions virtual molecules. searching carried using similarity-based metrics provides quantiﬁable numerical indicator closeness molecules. contrast de-novo drug design aims directly create novel molecules active towards desired biological target. here like molecular design task computer task generation novel molecules usually solved diﬀerent protocols. strategy build molecules predeﬁned groups atoms fragments. unfortunately approaches often lead molecules hard synthesise. therefore another established approach conduct virtual chemical reactions based expert coded rules hope reactions could also applied practice make molecules laboratory. systems give reasonable drug-like molecules considered solution structure generation problem. generally share view. however recently shown predicted reactions rulebased expert systems sometimes fail. also fotask scoring molecules ﬁltering undesired structures solved substructure ﬁlters undesirable reactive groups conjunction established approaches docking machine-learning approaches. approaches split branches target prediction classiﬁes molecules active inactive quantitative structure-activity relationships seek quantitatively predict real-valued measure eﬀectiveness substance molecular descriptors signature fingerprints extended-connectivity atom pair ﬁngerprints fuzzy variants de-facto standard today. convolutional networks graphs recent addition ﬁeld molecular descriptors. random forests neural networks currently widely used machine learning models target prediction. leads task search molecules right binding aﬃnity combined optimal molecular properties. earlier work performed classical global optimisation techniques example genetic algorithms ant-colony optimisation. furthermore novo design related inverse qsar. novo design design regular qsar mapping molecular descriptor space properties used scoring function global optimizer inverse qsar aims explicit inverse mapping maps back optimal points descriptor space valid molecules. however well deﬁned molecules inherently discrete. several protocols developed address this example enumerating structures within constraints hyper-rectangles descriptor space. gómez-bombarelli proposed learn continuous representations molecules variational auto-encoders based model bowman perform bayesian optimisation vector space optimise molecular properties. nevertheless approach applied create active drug molecules succeed optimising complex molecular properties emission color delayed ﬂuorescence decay rate work suggest novel completely datadriven novo drug design approach. relies generative model molecular structures based recurrent neural network trained large sets molecules. generative models learn probability distribution training examples; sampling distribution generates examples similar training data. intuitively generative model molecules trained drug molecules would \"know\" valid reasonable drug-like molecules look like could used generate druglike molecules. however molecules models studied rarely rigorously traditional models gaussian mixture models recently recurrent neural networks emerged powerful generative models diﬀerent domains natural language processing speech images video formal languages computer code generation music scores. work highlight analogy language chemistry show rnns also generate reasonable molecules. furthermore demonstrate rnns also transfer learned knowledge large molecule sets directly produce novel molecules biologically active retraining models small sets already known actives. test models reproducing hold-out test sets known biologically active molecules. representing molecules connect chemistry language important understand molecules represented. usually modeled molecular graphs also called lewis structures chemistry. molecular graphs atoms labeled nodes. edges bonds between atoms labeled bond order could therefore envision model reads outputs graphs. several common chemistry formats store molecules manner. however models natural language processing input output model usually sequences single letters strings words. therefore employ smiles format encodes molecular graphs compactly human-readable strings. smiles formal grammar describes molecules alphabet characters example aromatic aliphatic carbon atoms oxygen single double triple bonds indicate rings number introduced atoms ring closed. example benzene aromatic smiles notation would cccccc. side chains denoted round brackets. generate valid smiles generative model would learn smiles grammar includes keeping track rings brackets eventually close them. morphine complex natural product number steps ﬁrst second indicating ring established link molecules language discuss language models. initial state vector returns sequence state vectors sequence output vectors consists recursively deﬁned function takes state vector input vector returns state vector hi+. another function maps state vector output vector rent network also unrolled ﬁnite sequences unrolled seen deep neural network parameters shared among layers hidden state passed additional input next layer. training unrolled parameters simply done using backpropagation compute gradients respect loss function categorical cross-entropy work. speciﬁc function work long short term memory introduced hochreiter schmidhuber. used successfully many natural language processing tasks example google’s neural machine translation system. excellent in-depth discussions lstm refer articles goldberg graves olah greﬀ encode smiles symbols input vectors employ \"one-hot\" representation. means symbols symbol input step construct input vector length whose entries zero except k-th entry one. assume restricted symbols input would correspond language models recurrent neural networks given sequence words language models predict distribution word wi+. example language model receives sequence \"chemistry would assign diﬀerent probabilities possible next words. \"fascinating\" \"important\" \"challenging\" would receive high probabilities \"runs\" \"potato\" would receive probabilities. language models capture grammatical correctness meaning language models implemented example message autocorrection many modern smartphones. interestingly language models words. also based characters letters. case receiving sequence characters chemistr would assign high probability probability model molecules instead language simply swap words letters atoms practically characters smiles alphabet form language. example model receives sequence cccccc high probability next symbol would closes ring yields benzene. parameters learned training set. work recurrent neural network estimate probabilities equation contrast regular feedforward neural networks rnns maintain state needed keep track symbols seen earlier sequence. abstract terms takes sequence input vectors figure symbol generation sampling process. start random seed symbol gets converted one-hot vector input model. model updates internal state outputs probability distribution next symbols. here sampling yields converting feeding model leads updated hidden state output sample again. iterative symbol-by-symbol procedure continued long desired. example stop observing symbol obtain smiles benzene. hidden state allows model keep track opened brackets rings ensure closed later. corresponds k-th element vector sampling distribution would allow generating novel molecules sampling smiles symbol next time step construct input vector model equation yields sampling latter generates serves also model’s input next step symbol-bysymbol sampling procedure repeated desired number characters generated. indicate molecule \"completed\" molecule training data ﬁnishes line symbol case single character thus system outputs generated molecule ﬁnished. however simply continue sampling thus generating regular smiles contains molecule line. work used network three stacked lstm layers using keras library. model trained back propagation time using adam optimizer standard settings. mitigate problem exploding gradients training gradient norm clipping applied. transfer learning many machine learning tasks small datasets available might lead overﬁtting powerful models neural networks. situation transfer learning help. here model ﬁrst trained large dataset diﬀerent task. then model retrained smaller dataset also called ﬁne-tuning. transfer learning learn general features bigger data also might useful second task smaller data regime. generate focussed molecule libraries ﬁrst train large general molecules perform ﬁne-tuning smaller speciﬁc molecules start sampling procedure. target prediction verify whether generated molecules active desired targets standard target prediction employed. machine learning-based target prediction aims learn classiﬁer decide whether molecule molecular descriptor space active target. molecules split actives inactives using threshold measure substance eﬀectiveness. widely used metrics purpose. half maximal inhibitory concentration concentration drug required inhibit biological target’s function vitro. predict whether generated molecules active towards biological target interest target prediction models trained tested targets evaluated random forest logistic regression neural networks gradient boosting trees models ecfp molecular descriptor. found gbts slightly outperformed models used virtual assay studies ecfp ﬁngerprints generated version scikitlearn xgboost keras used machine learning libraries. -hta plasmodium molecules considered active reported chembl translates whereas staphylococcus used pmic data chemical language model trained smiles containing million molecules chembl database contains molecules measured biological activity data. smiles strings molecules canonicalized training chemoinformatics library yielding smiles contained molecule line. noted chembl contains many peptides natural products complex scaﬀolds michael acceptors benzoquinones hydroxylamines hydrazines etc. reﬂected generated structures corresponds million individual characters vocabulary size unique characters. characters subset smiles symbols since molecules chembl contain many heavy elements. number symbols hyperparameter model construction model learn distribution symbols present training data implies molecules smiles symbols seen training generated sampling. number reproduced molecules sampling molecules ﬁne-tuned generative model number reproduced molecules sampling molecules generic unbiased generative model trained large dataset. intuitively indicates much better ﬁne-tuned models work compared general model. work address points first want generate large sets diverse molecules virtual screening campaigns. second want generate smaller focussed libraries enriched possibly active molecules speciﬁc target. ﬁrst task train model large general molecules learn smiles grammar. sampling model would generate sets diverse unfocused molecules. address second task obtain novel active drug molecules target interest perform transfer learning select small known actives target reﬁt pre-trained chemical language model small data-set. epoch sample model generate novel actives. furthermore investigate model actually beneﬁts transfer learning comparing model trained scratch small sets without pre-training. training recurrent network employed recurrent neural network three stacked lstm layers dimensions followed dropout layer dropout ratio regularise neural network. model trained convergence using batch size unrolled steps. parameters. generating novel molecules generate novel molecules smiles symbols sampled model symbol-by-symbol. corresponded lines valid molecules parsing toolkit. removing molecules already seen training yielded structures. ﬁltering duplicates obtained novel molecules. generated molecules randomly selected depicted figure supporting information contains structures. created structures figure t-sne projection physicochemical descriptors random molecules chembl molecules generated neural network trained chembl unitless dimensions. distributions sets overlap signiﬁcantly. furthermore analysed bemis-murcko scaffolds training molecules sampled molecules. bemis-murcko scaﬀolds contain ring systems molecule moieties link ring systems removing side chains. represent scaﬀold core molecule series drug molecules often common. number common scaﬀolds sets divided union scaﬀolds sets indicates language model modify side chain substituents also introduces modiﬁcations molecular core. targeting -hta receptor generate novel ligands -hta receptor ﬁrst selected molecules tested -hta chembl ﬁne-tuned pre-trained chemical language model set. epoch sampled chars canonicalised molecules removed sampled molecules already contained training set. following this evaluated generated molecules round retraining -hta target prediction model order check novo compounds could considered valid starting points drug discovery program applied internal astrazeneca ﬁlters. astrazeneca ﬂagging system used determine compound suitable part high-throughput screening collection restricted particular ﬁlters applied generated molecules ﬂagged them either core backup. since ratio core backup compounds observed chembl collection therefore conclude algorithm generates preponderantly valid screening molecules faithfully reproduces distribution training data. determine whether properties generated molecules match properties training data chembl followed procedure kolb computed several molecular properties namely molecular weight bertzct number hdonors h-acceptors rotatable bonds logp total polar surface area randomly selected subsets sets rdkit library version then performed dimensionality reduction t-sne shown figure sets overlap alcompletely indicates generated molecules well recreate properties training molecules. targeting plasmodium falciparum plasmodium falciparum parasite causes dangerous form malaria. probe model important target used challenging validation strategy. wanted investigate whether model could also propose molecules medicinal chemists chose evaluate published studies. test this ﬁrst known actives plasmodium falciparum selected chembl. then split randomly training test chemical language model ﬁne-tuned training set. molecules sampled epochs reﬁtting. yielded unique molecules. interestingly found model able \"redesign\" unseen molecules test set. comparison molecules sampled unspeciﬁc untuned model enrichment random obtained. smaller training molecules model still reproduce test test reliance chose another cut-oﬀ took molecules training test set. test could recreated visually explore model populates chemical space figure shows t-sne plot ecfp ﬁngerprints test molecules generated molecules predicted active target prediction model plasmodium falciparum. indicates model generated many similar molecules around test examples. targeting staphylococcus aureus evaluate diﬀerent target furthermore conducted series experiments reproduce known active molecules staphylococcus aureus. here used actives pmic mean inhibitory concentration lowest concentration compound prevents visible growth microorganism. above actives split training test set. however here availability data allows larger test sets used. ﬁnetuning training molecules model could retrieve test molecules. scaling smaller training molecules model generates almost exclusively inactive molecules. already epochs ﬁne-tuning model produced molecules predicted active. diversity analysis order assess novelty novo molecules generated ﬁne-tuned model nearest neighbor similarity/diversity analysis conducted using commonly used ﬁngerprint based similarity method figure shows distribution nearest neighbor tanimoto index generated comparing novel molecules training molecules before epochs ﬁne-tuning. white bars indicate molecules generated unbiased general model darker bars indicate molecules several epochs ﬁne-tuning. within bins corresponding lower similarity number molecules decreases bins higher similarity populated increasing numbers molecules. plot thus shows model starts output similar molecules target-speciﬁc training set. notably rounds training highly similar molecules produced also molecules covering whole range similarity indicating method could deliver close analogs chemotypes scaﬀold ideas drug discovery project. best worlds diverse focussed molecules therefore suggest sample epoch retraining ﬁnal epoch. figure nearest-neighbour tanimoto similarity distribution generated molecules -hta epochs ﬁne-tuning known actives. generated molecules distributed whole similarity range. generated molecules medium similarity interesting scaﬀold-hopping. data) still reproduce test performs times better unbiased model using lower learning rate ﬁne-tuning often done transfer learning work well standard learning rate additionally examined whether model beneﬁts transfer learning. trained scratch model performs much worse pretrained subsequently ﬁne-tuned model pretraining large dataset thus crucial achieve good performance staphylococcus aureus. simulating design-synthesis-test cycles experiments conducted applicable already knows several actives. however drug discovery often start with. therefore high throughput screenings conducted identify hits serve starting point typical cyclical drug discovery process molecules designed synthesised tested assays. then best molecules selected based gained knowledge molecules designed closes cycle. therefore ﬁnal challenge model simulated cycle iterating molecule generation selection best molecules machine learning-based target prediction retraining language model best molecules staphylococcus aureus target. thus known actives start structure generation procedure started sampled molecules unbiased chemical language model. then using target prediction model extracted molecules classiﬁed actives. that ﬁnetuned epochs actives sampling molecules epoch. resulting molecules figure scheme novo design cycle. molecules generated chemical language model scored target prediction model inactives ﬁltered retrained. here machine learning model could also robot conducting synthesis biological assays docking program. model work? results presented section show general model trained large molecule learned smiles rules output valid druglike molecules resemble training data. however sampling model help much want generate actives speciﬁc target would generate large sets actives target among diverse range molecules model creates indicated high scores experiments. figure histogram levenshtein distances smiles reproduced molecules nearest neighbour training many cases model makes changes symbols smiles resembling typical modiﬁcations applied exploring series compounds distribution distances indicates also performs complex changes introducing larger moieties generating molecules structurally diﬀerent isofunctional training set. figure diﬀerent training strategies staphylococcus aureus dataset training test examples. fine-tuning pretrained model performs better training scratch better). already iterations model reproduced test molecules previous task exhibits higher model retrained directly actives additionally obtained unique molecules target prediction model classiﬁed active. demonstrates combination target prediction scoring model model also perform complete novo-design cycle. speciﬁc biological target enables creation novel molecules desired activity. iterating cycles structure generation language model scoring target prediction model retraining model increasingly larger sets highly scored molecules showed even need active known active molecules start procedure with could also docking program robot conducting synthesis biological testing. three main advantages method. first conceptually orthogonal established molecule generation approaches learns generative model molecular structures. second method simple setup train adapted diﬀerent datasets without modiﬁcations model architecture depend handencoded expert knowledge. furthermore merges structure generation optimisation model. weakness model interpretability. contrast existing de-novo design methods settled virtual reactions generate molecules advantages minimises chance obtaining \"overﬁt\" weird molecules increases chances synthesizable compounds. extend work small step cast molecule generation reinforcement learning problem pre-trained lstm generator could seen policy encouraged create better molecules reward signal obtained target prediction model. addition diﬀerent approaches target prediction example docking could evaluated. deep learning panacea join gawehn expressing some healthy skepticism regarding application drug discovery. generating molecules almost right enough because chemistry miss good mile drug discovery needle haystack problem also needle looks like hay. nevertheless given shown work model rediscover needles recent developments believe deep neural networks complimentary established approaches drug discovery. complexity problem certainly warrants investigation novel approaches. eventually success determine wave neural networks prevail. shifted towards molecules active towards target. study this compare levenshtein distance generated smiles nearest neighbours training figure levenshtein distance e.g. benzene cccccc pyridine cccncc would figure shows model often seems made small replacements underlying smiles many cases also made complex modiﬁcations even generated completely diﬀerent smiles. supported also distribution nearest neighbour ﬁngerprint similarities training rediscovered molecules many rediscovered molecules medium similarity regime. perform transfer learning ﬁnetuning model \"forget\" learned. plausible explanation model works therefore transfer modiﬁcations regularly applied series molecules studied molecules seen ﬁne-tuning. figure violin plot nearest-neighbour ecfp-tanimoto similarity distribution training molecules rediscovered molecules table entry distribution suggests model learned make typical small functional group replacements also reproduce molecules similar training data. work shown recurrent neural networks based long short term memory applied learn statistical chemical language model. model generate large sets novel molecules similar physico-chemical properties training molecules. used generate libraries virtual screening. furthermore demonstrated model performs transfer learning ﬁne-tuned smaller sets molecules active towards project conducted research stay m.s. astrazeneca gothenburg. thank chen engkvist valuable discussions feedback manuscript klambauer helpful suggestions. author details institute organic chemistry center multiscale theory computation westfälische wilhelms-universität münster germany. external sciences discovery sciences astrazeneca gothenburg sweden. department medicinal chemistry imed astrazeneca gothenburg sweden. department physics international centre quantum molecular structuresshanghai university china.", "year": 2017}