{"title": "Generalized Deep Image to Image Regression", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We present a Deep Convolutional Neural Network architecture which serves as a generic image-to-image regressor that can be trained end-to-end without any further machinery. Our proposed architecture: the Recursively Branched Deconvolutional Network (RBDN) develops a cheap multi-context image representation very early on using an efficient recursive branching scheme with extensive parameter sharing and learnable upsampling. This multi-context representation is subjected to a highly non-linear locality preserving transformation by the remainder of our network comprising of a series of convolutions/deconvolutions without any spatial downsampling. The RBDN architecture is fully convolutional and can handle variable sized images during inference. We provide qualitative/quantitative results on $3$ diverse tasks: relighting, denoising and colorization and show that our proposed RBDN architecture obtains comparable results to the state-of-the-art on each of these tasks when used off-the-shelf without any post processing or task-specific architectural modifications.", "text": "tions witness extension dcnns output dense pixel wise predictions approaches used either resnet backbone introduced architectural changes skip layers deconvolutional networks hypercolumns laplacian pyramids facilitate retention/reconstruction local input-output correspondences. approaches performed well segmentation benchmarks introduced trade-off locality context. since task still remained classiﬁcation trade-off skewed favor incorporating context subsequently reconstructing local correspondences global activations. perhaps approaches rely ancillary methods conditional random fields enhance granularity predictions. image-to-image regression entails generation dense continuous pixel-wise predictions locality-context trade-off highly task-dependent several dcnn based approaches proposed speciﬁc imim regression tasks denoising relighting colorization etc. approaches typically involve highly taskpresent deep convolutional neural network architecture serves generic image-to-image regressor trained end-to-end without machinery. proposed architecture recursively branched deconvolutional network develops cheap multicontext image representation early using efﬁcient recursive branching scheme extensive parameter sharing learnable upsampling. multi-context representation subjected highly non-linear locality preserving transformation remainder network comprising series convolutions/deconvolutions without spatial downsampling. rbdn architecture fully convolutional handle variable sized images inference. provide qualitative/quantitative results diverse tasks relighting denoising colorization show proposed rbdn architecture obtains comparable results state-of-the-art tasks used off-the-shelf without post processing taskspeciﬁc architectural modiﬁcations. last years generic deep convolutional neural network architectures variants resnet immensely successful tackling diverse range classiﬁcation problems achieve state-of-the-art performance benchmarks used box. feature architectures extremely high model capacity along robustness minor unwanted variations. given suitable training data models discriminatively trained reliable end-to-end fashion. however since classiﬁcation tasks require single class label corresponding entire image early architectures focused solely developing strong global image features. speciﬁc architectures coupled ﬁne-tuned ancillary post processing methods. however unlike classiﬁcation dcnns truly generic architecture imim regression proposed performs consistently well diverse range tasks. perhaps taskdependent locality-context trade-off coupled habitual trend incorporating vgg/resnet architectures non-classiﬁcation tasks impeded progress regard. propose generic imim dcnn architecture rbdn eliminates trade-off automatically learns much locality/context needed based task hand early development cheaply computed rich multi-scale image representation using recursive multi-scale branches learnable upsampling extensive parameter sharing. ﬁrst describe recently proposed imim dcnn approaches also fairly generic architecture compare similarities differences proposed rbdn approach. describe related work speciﬁc relighting denoising colorization. generic imim regression deep end--end voxel--voxel prediction proposed video-to-video regressor solving tasks semantic segmentation optical colorization. architecture consists style network branches upsample merge activations. unlike hypercolumns make upsampling learnable perform efﬁcient weight sharing. upsampling recover local correspondences dncnn hand entirely eliminate downsampling simple layer fully convolutional network residual connections handling tasks denoising super-resolution jpeg-deblocking. proposed rbdn architecture viewed hybrid utilize multi-scale activations like early network generate cheap composite multi-context representation image. subsequently pass composite linear convolution network like face relighting ﬁeld face recognition/veriﬁcation illuminationrelatively less invariant directly making illuexplored alternative image. mination traditional retinex /lambertian reﬂectance theory used spherical /hemispherical harmonics subspacebased dictionary-based illumination corrections. deep lambertian networks encoded lambertian models/illumination corrections directly network architecture. however limited expressive power network particularly strong lambertian assumptions isotropicity absence specular highlights seldom hold true face images. section show possible train well-performing relighting model without making lambertian assumptions using generic rbdn architecture. denoising approaches typically assume additive white gaussian noise known/unknown variance. traditional denoising approaches include clusteringsr epll nl-bayes ncsr wnnm among these popular well engineered still widely used state-of-the-art denoising approach. early dcnn based denoising approaches required different model trained noise variance limited practical use. recently gaussian-crf based dcnn approach proposed could explicitly model noise variance. dcgrf could however reliably model noise levels within reasonable range models low-noise dcgrf high-noise dcgrf section show single model proposed rbdn approach trained wide range noise levels achieves competitive results outperforms previously proposed approaches noise levels inherent color ambiguity majority objects makes colorization hard ill-posed problem. early works colorization required reference color image color local patches input image inferred parametric/non-parametric approaches. recently dcnn approaches used solve colorization imim classiﬁcation/regression problem grayscale color without requiring auxiliary inputs. hypercolumns complex dual-stream architecture simultaneously identiﬁes/classiﬁes object classes within image uses class labels colorize input greyscale image. classiﬁcation branch network identical colorization branch network mimics deconvnet architecture. best colorization results however obtained despite using fairly simple style architecture dilated convolutions. figure architecture proposed generic rbdn approach branches. various branches extract features multiple scales. learnable upsampling efﬁcient parameter sharing used recursively upsample activations branch merges pool output leading cheap multi-context representation input. multi-context subjected series convolutions supply ample non-linearity automatically choose much context needed based task hand. contribution novel classiﬁcation-based loss function quantized probability distribution values color space. class re-balancing scheme pushes predictions away statistically likely gray colors resulting colorful colorizations. section loss function replace vgg-style architecture proposed rbdn architecture obtain excellent colorizations. many imim approaches vgg/resnet backbone effectiveness availability. however leads suboptimal architectures types tasks inherent bias towards including context expense sacriﬁcing locality. instead propose rbdn uses recursive branches obtain cheap multi-context locality-preserving image representation early network. sections describe network architecture detail analyze various components. classiﬁcation dcnns typically contain multitude interleaved downsampling layers ultimately squash image vector. memory major bottleneck training dcnns downsampling layers enable exploration deep architectures providing natural translational invariance. however problems arise attempting directly port networks imim regression tasks. design changes needed retention/recovery local correspondences muddled across channels middle layers. recovery repeated upsampling inevitably lossy process particularly harmful regression tasks demanding continuous pixel-wise predictions. alternatively local correspondences retained merging activation maps earlier layers penultimate layer. downside approach activations early layers poor capability model nonlinearity limits overall capacity network modeling localized non-linear transformations. dcnn successful generic imim regressor would necessarily need maintain local pixel-wise features develop strong global representations across pipeline independently preserving local information. figure shows architecture proposed recursively branched deconvolutional network three branches. high level network ﬁrst extracts features scales merges activations early yield composite subjected series convolutions followed deconvolution yield output image. feature network multi-scale composite efﬁciently generated using recursive branching learnable upsampling. training network broad locality-context spectrum work early series convolution layers follow suit choose amount context based task hand apply ample nonlinearity. translates range modeling capabilities anywhere context-aware regression maps highly localized non-linear transformations activation function batch normalization layer convolution/deconvolution. independently experimented values performing relighting experiments found increasing yields miimprovement increasing network depth yielded signiﬁcant monotonic improvement convolution layers performance saturated. ﬁnal network gave best results shown ﬁgure denote network base network gives decent performance relighting limitations ﬁeld view. unlike conventional dcnns cannot downsampling midway since would corrupt local correspondences. result keep local correspondences intact instead branch network ﬁrst pooling layer. within convb+poolb+convb computes features half generalize multiple branches ...bk. order start deﬁning recursive branch module ﬁgure corresponds branch n-branch network. note branch bk+n originates merges within branch advantage recursive construction two-fold activations deeper branches would upsampled many times merging main branch. recursive construction helps deeper branches partially beneﬁt learnable upsampling machinery shallow branches. aside beneﬁt parameter sharing recursive construction forces activations deeper branches traverse longer path thus accruing many relu activations. enables deeper branches model non-linearity beneﬁcial since cover larger field view correspond global features. train generic rbdn architecture three diverse tasks relighting denoising colorization. train models nvidia titan-x caffe deep learning framework. denoising/colorization experiments augment caffe utility layers noise policies image conversions streamline training procedure enable practically image dataset without pre-processing. relu activation function perform batch normalization every convolution/deconvolution layer rbdn models. figure analysing effect learnable upsampling recursive branching. error plots cmu-multipie validation show positive inﬂuence learnable upsampling recursive branching. size learning rate minibatch size step-size train model iterations using stochastic gradient descent momentum weight decay. inference network virtue fully convolutional handle variable sized inputs. cmu-multipie face images subjects recorded sessions. within session face images subject exhibiting pose illumination expression variations. used images subjects appear sessions training relighting rbdn images subjects validation. imagenet ilsvrc million training imtrain relighting rbdn images cmu-multipie takes input frontal face image varying illumination outputs image ambient lighting. used crop size step-size trained model iterations. compared base network k-branch rbdn major additions recursive branching learnable upsampling. perform sets relighting experiments independently observe efﬁcacy k-branch rbdn follows figure relighting rbdn results subject cmumultipie validation set. input images output rd-th rbdn outputs branches respectively. results improve increase number branches branches. network starts overﬁtting branches. removed concat layers merge different branches. resulted linear network similar structure deconvolutional networks used semantic segmentation figure shows error plots reconstruction error cmu-multipie validation training iterations experiments. plots show learnable upsampling recursive branching independently positive impact performance. train single -branch rbdn model denoising takes input grayscale image corrupted additive standard deviation uniformly randomly chosen range evaluation protocol image test test images pascal dataset). precomputed noisy test images quantized range used compare various approaches fair realistic evaluation. ﬁrst transform color image ycbcr color space predict chroma channels luminance input using rbdn. input y-channel combined predicted cbcr channels converted back yield predicted color image. denote model rbdn-ycbcr. figure relighting results cmu-multipie validation set. goal render faces various unknown lighting conditions ﬁxed lighting condition. rows inputs even rows -branch rbdn output -branch rbdn table mean psnr various denoising approaches test images. single denoising model used report results rbdn dncnn comparison approaches note best performing model noise level used report results. inspired recently proposed colorful colorizations approach train another rbdn model takes input l-channel color image space tries predict -dimensional vector probabilities pixel subsequently problem treated multinomial classiﬁcation softmax-cross-entropy loss class re-balancing instead adam solver training learning rate step-size mini-batch size train model iterations. inference annealed-mean softmax distribution obtain predicted ab-channels denote model rbdn-lab. relighting figure shows rbdn outputs branches subject cmu-multipie validation set. improvement results -branch rbdn prominent gradual improvement increase number branches results deteriorate transitioning -branch rbdn figure shows results validation -branch rbdn achieves near perfect relighting subjects. denoising table shows mean psnr various denoising approaches benchmark test images. besides rbdn dncnn dcgrf approaches train separate model noise level. dcgrf results reported noise model test high noise model test results dncnn -branch rbdn however correspond single model trained automatically handle noise levels. model outperforms approaches test noise figure shows visual comparison various denoising approaches test image bsd. figure highlights single rbdn model’s denoising capability across range noise levels. figure illustrates generalization ability rbdn reliably denoise high noise level fact -layer rbdn outperforms -layer residual dncnn suggests cheap early recursive branching beneﬁcial added depth. colorization figure shows colorizations various models ms-coco test set. -branch rbdn-ycbcr models produce decent colorizations dull highly under-saturated. however architectural limitation rather loss function tends push results towards average. colorization inherently ambiguous large majority objects cars people animals doors utensils etc. several take wide range permissible colors. hand based models able reasonably color grass water typically take ﬁxed range colors. softmax crossentropy loss based models class rebalancing -branch rbdn-lab) able overcome undersaturation problem posing problem classiﬁcation task forcibly pushing results away average. finally difference -branch rbdnlab linear dilated convolutional network architecture. models give good colorizations appearing better certain images figure illustrating rbdn’s ability reliably denoise outside training bounds -layer dncnn outperformed -layer rbdn. yellow green boxes show psnr. proposed dcnn architecture imim regression rbdn gives competitive results diverse tasks relighting denoising colorization used off-the-shelf without task-speciﬁc architectural modiﬁcations. feature rbdn development cheap multi-context image representation early network means recursive branching learnable upsampling alleviates locality-context trade-off concerns inherent design imim dcnns. tentially beneﬁt residual connections dilated convolutions possibly activation functions besides relu. secondly used network ﬁxed depth across tasks prove insufﬁcient complex tasks suboptimal simple tasks. recently proposed structured sparsity approach allows networks simultaneously optimize hyperparameters highly efﬁcient training means group lasso regularization. thirdly known extremely poor loss function tasks demanding perceptually pleasing image outputs. loss function used colorization overcame mse’s limitations speciﬁc colorization problem. loss functions based adversarial networks hand generic replacement. research based upon work supported ofﬁce director national intelligence intelligence advanced research projects activity iarpa contract views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied odni iarpa u.s. government. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon.", "year": 2016}