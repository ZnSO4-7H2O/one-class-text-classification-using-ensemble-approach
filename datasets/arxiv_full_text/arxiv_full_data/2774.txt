{"title": "Teaching a Machine to Read Maps with Deep Reinforcement Learning", "tag": ["cs.RO", "cs.AI", "cs.LG", "stat.ML", "I.2.0; I.2.6; I.2.9; I.2.10"], "abstract": "The ability to use a 2D map to navigate a complex 3D environment is quite remarkable, and even difficult for many humans. Localization and navigation is also an important problem in domains such as robotics, and has recently become a focus of the deep reinforcement learning community. In this paper we teach a reinforcement learning agent to read a map in order to find the shortest way out of a random maze it has never seen before. Our system combines several state-of-the-art methods such as A3C and incorporates novel elements such as a recurrent localization cell. Our agent learns to localize itself based on 3D first person images and an approximate orientation angle. The agent generalizes well to bigger mazes, showing that it learned useful localization and navigation capabilities.", "text": "accordingly. addition agent gets maze also image seen figure location marked agent’s target. crux agent know currently several locations might correspond well current view. thus agent needs move around learn position move target illustrated figures equip agent approximate orientation angle i.e. agent roughly knows direction moving looking. always north. training agent learns approximate orientation corresponds north. complex multi-stage task navigating maze help naturally decomposed several subtasks agent needs observe environment compare determine likely position. agent needs understand case associate symbols rewards thereby gain understanding wall navigable space target finally agents needs learn follow plan order reach target. contribution follows present novel modular reinforcement learning architecture consists reactive agent several intermediate subtask modules. modules designed solve speciﬁc subtask. modules contain neural networks alternatively implement exact algorithms heuristics. presented agent capable ﬁnding target random mazes roughly three times size largest mazes seen training. ability navigate complex environment quite remarkable even difﬁcult many humans. localization navigation also important problem domains robotics recently become focus deep reinforcement learning community. paper teach reinforcement learning agent read order shortest random maze never seen before. system combines several state-of-theart methods incorporates novel elements recurrent localization cell. agent learns localize based ﬁrst person images approximate orientation angle. agent generalizes well bigger mazes showing learned useful localization navigation capabilities. main success factors human evolution ability craft complex tools. ability give motivation social interaction teaching others different tools also enhanced thinking capabilities since understand ever complex tools. take example; helps navigate places never seen before. however ﬁrst need learn read i.e. need associate content two-dimensional threedimensional surroundings. algorithms becoming increasingly capable learning complex relations make machines intelligent teach already existing tools. paper teach machine read deep reinforcement learning. agent wakes maze. agent’s view image maze rendered agent’s perspective like dungeon ﬁrst person video game. rendered image provided deepmind environment agent controlled human case complex deep reinforcement learning architecture. agent move rotate view image change learning method sutton basis actor-critic algorithms q-learning techniques first works using artiﬁcial neural networks reinforcement learning include in-depth overview reinforcement learning refer interested readers current deep learning boom started among contributions backpropagation algorithm advances computing power frameworks. however deep learning could applied effectively reinforcement learning recently. mnih introduced deep-q-network uses experience replay target networks stabilize learning process. since then several extensions architecture proposed double deepq-network dueling network architecture networks based using replay buffers stabilize learning prioritized experience replay state-of-the-art relies asynchronous actor-learners stabilize learning. system learning modiﬁed network architecture train reactive agent localization module on-policy manner. also make replay buffers train agent policy. major challenge reinforcement learning environments delayed sparse rewards. agent never gets reward never learn good behavior. thus jaderberg mirowski introduced auxiliary tasks agent learn based intermediate intrinsic pseudo-rewards predicting depth image simultaneously trying solve main task e.g. ﬁnding exit maze. policies learned auxiliary tasks directly used agent solely serve purpose helping agent learn better representations improves performance main task. idea auxiliary tasks inspired prior work temporal abstractions options whose focus learning temporal abstractions improve high-level learning planning. work introduce modularized architecture incorporates intermediate subtasks localization local estimation global interpretation. contrast reactive agent directly uses outputs modules solve main task. note auxiliary task inside localization module improve local estimation. kulkarni introduced hierarchical version tackle challenge delayed sparse rewards. system operates different temporal scales allows deﬁnition goals using entity relations. policy learned reach goals. similar approach make agent follow plan north. input uses plan path environment. subsequent works combined approaches computer vision techniques images input. machine learning techniques used solve mapping planning separately later also tackled joint mapping planning problem instead separating mapping planning phases reinforcement learning methods aimed directly learning good policies robotic tasks e.g. learning human-like motor skills recent advances deep reinforcement learning spawned impressive work area mapping localization. unreal agent uses auxiliary tasks replay buffer learn navigate maze. mirowski came agent uses different auxiliary tasks online manner understand navigation capabilities manifest biproduct solving reinforcement learning problem. tackled problems generalization across tasks data inefﬁciency. realistic environment physics engine gather training data efﬁciently. model capable navigating visually speciﬁed target. contrast approaches memoryless feed-forward model instead recurrent models. gupta simulated robot navigates real environment. focus architectural problem learning mapping planning joint manner phases proﬁt knowing other’s needs. agent capable creating internal representation local environment similar local visible map. work global given agent learns interpret read reach certain target location. thus agent capable following complicated long range trajectories approximately shortest path manner. furthermore system trained fully supervised manner whereas agent trained reinforcement learning. bhatti augment standard semantic maps vizdoom environment. semantic maps constructed rgb-d input employ techniques standard computer vision based object recognition slam. showed results better learned policies. task agent eliminate many opponents possible dying. contrast agent needs escape complex maze. furthermore environments designed provide little semantic information possible make task difﬁcult agent; agent needs construct local visible based purely shape surroundings. many complex tasks divided easier intermediate tasks solved individually solve complex task. principle apply neural network architecture design. section ﬁrst introduce concept modular intermediate tasks discuss implement modular tasks reading architecture. figure visible local network pixel input passed convolutional neural network layers fully connected layer being concatenated discretized angle processed fully connected layers gating operation. integrates local estimation. local estimation compared global probability distribution discretized possible locations. third module called interpretation network; learns interpret global outputs short term target direction estimated position. last module reactive agent learns follow estimated short term target direction ultimately exit maze. allow agent access discretized angle describing direction facing comparable robot access compass. furthermore limit ourself completely unsupervised learning allow agent discretized version actual position training. could implemented robot training network help signal. robot could train long accuracy signal certain threshold trained network soon signal gets inaccurate totally lost. leave practical implementation algorithm future work focus algorithmic structure itself. visible local network visible local network preprocesses visual input environment convolutional neural network layers followed fully connected layer. adapted preprocessing architecture thereby generated features concatenated -hot discretized encoding orientation angle i.e. input angle n-dimensional vector dimension represents discrete state angle three vector components represent discrete angle values closest actual angle remaining components zero e.g. used -hot instead -hot encoding smooth input. note encoding average quantization error degrees. figure architecture overview interplay four modules. discretized angle last action taken last reward received {ploc estimated location probability distribution possible discrete locations entropy estimated location probability distribution sttd short term target direction suggested interpretation network estimated state value policy output next action sampled. modular intermediate tasks intermediate task module information processing unit takes input either sensory input and/or output modules. module deﬁned designed intermediate task solves consist trainable hard coded parts. since dealing neural networks output therefore input module erroneous. module adjusts trainable parameters reduce error independent modules. achieve stopping error back-propagation module boundaries. note separation advantages drawbacks module performance evaluated debugged modules cannot adjust output input needs next module. achieved interface design i.e. intermediate task speciﬁcation. neural network architecture consists four modules dedicated speciﬁc subtask. ﬁrst give overview interplay modules describing detail following sections. architecture overview sketched figure ﬁrst module visible local network; takes visual input environment creates frame dimensional excerpt currently visible surroundings. second module recurrent localization cell takes stream visible local excerpts current position. agent predict egomotion shift egocentric estimated local accordingly. refer figure sketch architecture described hereafter. current output visible local network discretized -hot encoded orientation angle -hot encoded last action taken extrinsic reward received taking action estimated local time step feedback local time step est+mf estimated local feedback time step estimated necessary shifting time step {ploc discrete estimated location probability distribution. describe functionality recurrent localization cell following equations here layer feed forward neural network denotes dimensional discrete convolution stride dimensions +.−. denotes clipping trainable feedback parameter extracts local around location interpretation network goal interpretation network rewarding locations construct plan locations. achieve three stages first network passes convolutional layers followed rectiﬁed linear unit activation create -channel reward map. channels trained represent wall locations navigable locations target locations respectively. reward area averaged rectiﬁed passed parameter free shortest path planning module outputs discrete locations distribution {north east south west} i.e. short term target direction well measure distance nearest target location. plan multiplied estimated location probability distribution smooth sttd target distance currently estimated location. note planning possible location querying plan full location probability distribution helps resolve exploitation-exploration dilemma reactive agent uncertain location probability distribution close uniform distribution result uncertain sttd distribution {north east south west} thereby encouraging exploration. figure sketch information recurrent localization cell. last egomotion estimation discretized angle last action reward passed fully connected layers combined dimensional convolution former local estimation current visible local input egomotion estimation egomotion estimation used shift previously estimated local previous feedback local weighted clipped combination local estimations est+mf convolved full estimated location probability distribution {ploc recurrent connections marked empty arrows. current ﬁeld view used gate estimated excerpt estimates line sight make visible local map. gating crucial reduce noise visible local output. figure sketch visible local network recurrent localization cell moving around environment agent generates stream visible local excerpts like output figure visible local input figure recurrent localization cell builds egocentric local stream compares actual estimate training losses train agent combination on-policy losses data generated rollouts environment off-policy losses sample data replay memory. speciﬁcally total loss four module speciﬁc losses lvlm off-policy visible local loss lloc on-policy localization loss off-policy reward loss on-policy reactive agents acting loss train agent asynchronous advantage actor critic additional losses; similar deepmind’s unreal agent training iteration every thread rolls steps environment accumulates localization loss lloc acting loss step experience frame pushed experience history buffer ﬁxed length. experience frame contains inputs network requires well current discretized true position. experience history frames sampled inputs replayed network calculate visible local loss lvlm reward loss lrm. describe loss detail. output visible local network trained match visible excerpt constructed discretized location angle. training iteration experience frames uniformly sampled experience history visible local loss calculated distances visible local outputs targets here denotes sampled frame indices. localization loss lloc trained policy rollouts environment. step compare estimated position actual position ways results cross entropy location loss llocxent distance location loss llocd. cross entropy location loss cross entropy location probability distribution {ploc -hot encoding actual position. distance loss llocd calculated step distance actual dimensional cell position coordinates cpos estimated centroid possible cells weighted corresponding probability ploc addition training location estimation directly also assign auxiliary local loss lloclm help local construction. calculate local loss training iteration distance last estimated local actual local point time. location probability distribution locations similar sttd accumulate similarities result clear sttd agent even though location might still unclear reactive agent intrinsic reward mentioned reactive agent faces partially contradicting goals following sttd improving localization generating information rich visual input e.g. excessive staring walls. agent learns trade reinforcement learning i.e. maximizing expected rewards. rewards provide extrinsic rewards environment well intrinsic rewards linked short term goal inputs reactive agent. short term goal inputs sttd distribution {north east south west} measure distance nearest target location interpretation network well normalized entropy discrete location probability distribution {ploc represents measure location uncertainty linked need exploration. intrinsic reward consists parts encourage exploration exploitation. exploration intrinsic reward explor timestep difference location probability distribution entropy previous timestep exploitation intrinsic reward measure well egomotion agent aligns sttd. calculate approximate dimensional egomotion vector egomotion probability distribution estimation similarly calculate sttd vector sttd distribution orth east south est} previous timestep. calculate exploitation intrinsic reward exploit product vectors input reactive agent concatenate discretized -hot angle last extrinsic reward location probability distribution entropy sttd distribution estimated target distance. agent simple feed-forward network consisting fully connected layers rectiﬁed linear unit activation followed fully connected layer policy fully connected layer estimated state value respectively. agents next action sampled softmax-distribution policy outputs. figure results successful tests maze size. every single test represented line connects arithmetic averages maze size. distance origin target grows linearly maze size number steps. mazes. train agent starting small mazes increase maze sizes agent gets better. speciﬁcally asynchronous agent training threads start smallest training mazes training threads started sizes prevents visible local network overﬁtting small mazes. thread agents placed randomly sampled maze currently associated maze size exit counting steps. step interaction environment i.e. sampling action agents policy receiving corresponding next visual input discretized angle extrinsic reward environment. step location maze grid cell; agents accelerate direct correlation steps actual walked distance. consider sampled maze episode start. episode ends successfully agent manages target steps needed stored. agent exit steps episode ends successful. episode ends episode started i.e. maze sampled. note setting agent always placed newly sampled maze maze thread calculate moving average steps needed episodes. moving average falls maze size speciﬁc threshold thread transferred train mazes next bigger size. thread’s moving average steps needed biggest training mazes falls threshold thread stopped training considered successful. threads reach stage overall training considered successful agent fully trained. calculate moving average last episodes steps threshold maze sizes respectively. figure shows training performance actor threads. agents sometimes overﬁt policies results temporarily decreased performance even though maze size increase. however threads reach good performance. space locations target locations respectively. this leverage setting running wall gives negative extrinsic reward moving open space gives extrinsic reward ﬁnding target gives positive extrinsic reward. therefore problem transformed estimating extrinsic reward. training iteration sample frames experience history. sampling independent visible local loss sampling skewed expectation equally many frames positive negative zero extrinsic reward. frame frames passed convolution layers interpretation network create corresponding reward visual input localization state saved frame network estimated location probability distribution. reward loss cross entropy prediction error reward estimated position. reactive agent’s acting loss equivalent learning described mnih also adapted action repeat frame rate fps. whole network trained rmsprop gradient descent gradient back propagation stopped module boundaries i.e. module trained module speciﬁc loss. evaluate architecture created training test mazes corresponding black white maps deepmind environment. mazes quadratic grid mazes maze cell either wall open space target spawn position. training consists mazes different sizes; mazes sizes maze cells. test consists mazes; sizes note outermost cells mazes always walls therefore maximal navigable space maze maze cells. thus navigable space biggest test mazes roughly times larger biggest training mazes. figure four example frames illustrate typical behavior agent line trace actual position shades blue represent position estimate. darker blue conﬁdent agent location. frame shows agent’s true starting position frame shows several similar locations identiﬁed turning frame agent starts understand true location frame moved. presented deep reinforcement learning agent localize based observations surroundings. agent manages exit mazes high success rate even mazes substantially larger ever seen training. agent often ﬁnds shortest path showing agent continuously retain good localization. architecture system built modular fashion. module deals subtask maze problem trained isolation. modularity allows structured architecture design complex task broken subtasks subtask solved module. modules consist general architectures e.g. mlps task-speciﬁc networks recurrent localization cell. also possible deterministic algorithm modules shortest path planning module. architecture design aided possibility easily replace module ground truth values available sources performance. agent designed speciﬁc task. plan make modular architecture general apply tasks playing games. since modules swapped arranged differently would interesting equip agent many modules learn module situation. figure example trajectories walked agent. note agent walks close shortest path continuous localization planning lets agent path target even took wrong turn. figure comparison agent agent perfect position information optimal short term target direction input solid lines count steps solid blue line average line figure dashed lines count steps agent turns. ﬁgure shows overhead mostly turning agent needs look around localize itself. number required steps maze size plotted figure stop test steps even biggest test mazes agent found targets within steps. table percentage exits found maze sizes. agent ﬁnds exit almost shortest path manner seen figure however agent needs considerable number steps localize itself. evaluate localization overhead trained agent consisting solely reactive agent module access perfect location optimal short term target direction plotted average performance test figure ﬁgure shows large full agent agent access perfect position. turning actions full agent performs localize itself visible local network passes visual input convolutional layers fully connected layer extract visual features. visual features concatenated discretized angle input passed anfully connected layer intermediate representation local excerpt currently visible ﬁeld estimated. estimated excerpt constructed passing intermediate representation fully connected layer clipping nonlinearity clips output visible ﬁeld estimation achieved well passing intermediate representation fully connected layer rectiﬁed pseudo-sigmoidal nonlinearity output component lies within achieve rectiﬁed pseudosigmoidal activation clipping output adding rectiﬁed pseudo-sigmoidal activation able fully close open estimated gate. visible local estimation multiply estimated excerpt component-wise visible ﬁeld estimation i.e. gate estimated excerpt visible ﬁeld estimation. note directly train network estimate visible ﬁeld rather used term give logical intuition gating operation effective. estimate egomotion discrete probability distribution grid. grid cells represent estimated probability agent moved north-west north north-east cell middle grid represents estimated probability agent stayed place. rough estimate egomotion using visible local input dimensional convolution ﬁlter shift previously estimated local map. rough estimate tuned outputs small feed forward neural network takes inputs previously estimated egomotion probability distribution current discretized angle last action taken last extrinsic reward received softmax estimated egomotion logits estimated egomotion probability distribution estimated egomotion probability distribution dimensional convolution ﬁlter shift pret−) feedback viously estimated local previously estimated position. visible local shifted previously estimated clip range shifted feednew local estimation back weighted trainable parameter added local estimation local estimation feedback est+mf clipped note differentiate estimated local recurrently passed next step estimated local feedback est+mf used immediate localization. include feedback directly estimated local since otherwise feedback incorrect position could alter construction estimated local map. feedback merely used complete estimated local map. rasterize scale full desired location granularity range i.e. scale discrete cells range representing black representing white. localize agent simply estimated local feedback dimensional convolution ﬁlter slide zero-padded rasterized possible location cell correlation surrounding local local estimation. softmax correlation outputs location probability distribution {ploc finally feedback extracted next localization step location cell rasterized corresponding local surrounding local maps weighted probability agent corresponding cell. present implementation shortest path planning algorithm. note deterministic shortest path planner main focus work. algorithm takes grid maze input outputs location maze shortest path direction next step nearest exit would work. ﬁrst replaced reward estimation location reward average corresponding maze cell used sharp softmax classify location cell either target cell navigable cell wall cell. assigned values cells target cells navigable cells wall cells respectively. used module recursive multiplicative assign location cell value corresponding distance nearest target cell. precisely iterations iteration calculated cell here denotes current iteration denotes neighboring cells north east south west. ﬁnal iteration return cell sharp softmax four values desired short term target direction desired measure distance nearest target. figure example agent moving maze size testing. visual input pixel input agent receives. maze showing agent’s position estimation walked trajectory agent sees reward shows agent’s estimation reward. displayed shows positive reward estimations green reward estimations negative reward estimations blue. agent successfully learned reaching gives positive reward walking walls yields negative reward. right agent estimates local map. uses local estimate localize within maze map. agent currently conﬁdent position localization indeed accurate seen comparing estimated position actual position bottom right shows probability distribution possible actions current state", "year": 2017}