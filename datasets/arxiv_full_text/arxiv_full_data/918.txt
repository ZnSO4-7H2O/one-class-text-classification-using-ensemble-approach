{"title": "Techniques for Learning Binary Stochastic Feedforward Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Stochastic binary hidden units in a multi-layer perceptron (MLP) network give at least three potential benefits when compared to deterministic MLP networks. (1) They allow to learn one-to-many type of mappings. (2) They can be used in structured prediction problems, where modeling the internal structure of the output is important. (3) Stochasticity has been shown to be an excellent regularizer, which makes generalization performance potentially better in general. However, training stochastic networks is considerably more difficult. We study training using M samples of hidden activations per input. We show that the case M=1 leads to a fundamentally different behavior where the network tries to avoid stochasticity. We propose two new estimators for the training gradient and propose benchmark tests for comparing training algorithms. Our experiments confirm that training stochastic networks is difficult and show that the proposed two estimators perform favorably among all the five known estimators.", "text": "guillaume alain laurent dinh department computer science operations research universit´e montr´eal montr´eal canada guillaume.alain.umontrealgmail.com dinhlauriro.umontreal.ca stochastic binary hidden units multi-layer perceptron network give least three potential beneﬁts compared deterministic networks. allow learn one-to-many type mappings. used structured prediction problems modeling internal structure output important. stochasticity shown excellent regularizer makes generalization performance potentially better general. however training stochastic networks considerably difﬁcult. study training using samples hidden activations input. show case leads fundamentally different behavior network tries avoid stochasticity. propose estimators training gradient propose benchmark tests comparing training algorithms. experiments conﬁrm training stochastic networks difﬁcult show proposed estimators perform favorably among known estimators. feedforward neural networks multi-layer perceptron networks model mappings inputs outputs hidden units typically network output deﬁnes simple distribution isotropic gaussian fully factorial bernoulli distribution. case hidden units deterministic conditionals belong family simple distributions. stochastic feedforward neural networks advantage conditionals complicated. conﬁguration hidden units produces simple output mixture approximate distribution including multimodal distributions required one-to-many type mappings. extreme case using empty vectors input used unsupervised learning outputs another potential advantage stochastic networks generalization performance. adding noise stochasticity inputs deterministic neural network found useful regularization method introducing multiplicative binary noise hidden units regularizes even better. binary units additional advantages certain settings. instance conditional computations require hard decisions addition harwdare solutions restricted binary outputs mean-ﬁeld approximation inefﬁcient optimize lower bound likelihood loose. recent work proposes simply drawing samples feedforward phase guarantees independent samples unbiased estimate standard back-propagation using stochastic continuous-valued units back-propagation longer possible discrete units. several ways estimating gradient case. bengio proposes estimators unbiased estimator large variance biased version approximates back-propagation. tang salakhutdinov propose unbiased estimator lower bound works reasonably well hybrid network containing deterministic stochastic units. approach relies using sample training example paper provide theory show using sample important requirement. also demonstrate interesting applications mapping face person varying expressions mapping silhouette object color image object. tang salakhutdinov argue choice hybrid network structure based ﬁnite number hidden conﬁgurations fully discrete however offer alternate hypothesis much easier learn deterministic network around small number stochastic units might even important train stochastic units properly. extreme case stochastic units trained deterministic units work. work take step back study rigorously training problem fully stochastic networks. compare different methods estimating gradient propose estimators. approximate back-propagation less bias bengio modiﬁcation estimator tang salakhutdinov less variance. propose benchmark test setting based well-known mnist data toronto face database. study model maps inputs outputs stochastic binary hidden units equations given hidden layer extension multiple layers easy. activation probability computed like activation function deterministic multilayer perceptron networks probabilistic training criterion deterministic networks gradient respect model parameters computed using back-propagation algorithm based chain rule derivatives. stochasticity brings difﬁculties estimating training criterion estimating gradient. training criterion stochastic network requires summation exponential number conﬁgurations also derivatives respect discrete variables cannot directly deﬁned. review propose solutions problems below. interpreted performance ﬁnite mixture model samples drawn could hope using sample like many stochastic networks would work well enough. however show case network always prefers minimize stochasticity instance increasing input weights stochastic sigmoid unit behaves deterministic step-function nonlinearity. theorem maximizing expectation equation using hidden unit never prefers stochastic output deterministic one. however maximizing expectation equation hidden unit prefer stochastic output deterministic ones. proof. expected data distribution upper-bounded denotes data distribution. value last inequality achievable selecting distribution dirac delta around value maximizes deterministic function done every expectation independent way. analogous idea game theory since performance achieved linear combination performances deterministic choices mixed strategy cannot better best deterministic choice. look situation expectation differs case particle. original training criterion written expectation kl-divergence. fact expression features negative kl-divergence means maximum achieved conditionals match exactly. maximized value give simple example take values deﬁne following conditions show deterministic maximizing criterion maximized regardless distribution purposes comparing solutions simply take case expected takes value hand deterministic solutions yield lower value exploring different estimators gradient training criterion wrt. parameters however share following gradient training incoming signal activation function ﬁnal output layer. training compute gradient training criterion equation plug training criterion. estimate numerator denominator exponential moving average. second estimator biased lower variance. based back-propagation gives biased estimate gradient since ignore fact structure noise depends input signal note however noise zero-mean input help keep bias relatively small. inequality holds distribution usefulness choosing serves good approximation start noting importance sampling express terms proposal distribution draw samples. would interesting line research train auxiliary model proposal distribution following ideas kingma welling rezende mnih gregor call equivalent recognition model inference network. however pursue line paper follow tang salakhutdinov chose case importance weights simplify tang salakhutdinov generalized algorithm compute gradient lower bound given ﬁxed thus train using target outputs. turns resulting gradient exactly section despite rather different obtaining importance weights role responsibilities mixture model notation them. proposed unbiased estimator gradient propose gradient estimator applying variance reduction technique estimator tang salakhutdinov first note figure left norm gradient weights ﬁrst hidden layer function equation corresponds norm averaged proposed mini-batch epochs training mnist classiﬁcation experiment varying changes variance estimator minimum norm corresponds minimum variance. right function number particles used test time mnist structured prediction task proposed models trained propose experiments benchmarks stochastic feedforward networks based mnist handwritten digit dataset toronto face database experiments output distribution likely complex multimodal. ﬁrst experiment predicted lower half mnist digits using upper half inputs. mnist dataset used experiments binarized preprocessing step sampling pixel independently using grey-scale value expectation. second experiment followed tang salakhutdinov predicted different facial expressions toronto face database data used individuals least different facial expression pictures binarize. input mean images subject output predicted distribution different expressions subject. randomly chose subjects training data remaining subjects test data data second problem continuous assumed unit variance gaussian noise thus trained network using squares error. used network structure ﬁrst second problem respectively. running experiments simple viability check gradient estimators training network mnist classiﬁcation. based results kept performed signiﬁcantly better results viability experiment found appendix comparison also trained four additional networks addition stochastic feedforward networks. network deterministic network network used weights trained produce deterministic values hidden units instead using deterministic values test time stochastic equivalent. therefore trained network network tests network would stochastic network. network hybrid network inspired tang salakhutdinov hidden layer consists binary stochastic neurons deterministic neurons. however stochastic neurons incoming connections deterministic input previous layer outgoing connections deterministic neurons layer. original paper network trained using gradient estimator network hybrid network difference stochastic neurons constant activation probability hence incoming weights biases learn. experiments used stochastic gradient descent mini-batch size momentum used learning rate schedule learning rate increases linearly zero maximum ﬁrst epochs back zero remaining epochs. maximum learning rate chosen among best test error method reported. models trained test time always used seen table excluding comparison methods proposed biased estimator performs best tasks. notable performance increased signiﬁcantly using particles could predicted theorem figure plot objective test time based number particles theory larger number particles always better figure shows objective estimated accurately networks tested best performing network tasks however comparison network i.e. deterministic network added binary stochastic neurons constant activation probability especially interesting note network also outperformed hybrid network output probabilities stochastic neurons learned. network seems gain able model stochasticity without need propagate errors binary stochastic variables. results give support hypothesis hybrid network outperforms stochastic network easier learn deterministic network around small number stochastic units learning full stochastic network although stochastic units trained properly. results could possibly improved making networks larger continuing training longer given enough computational capacity. might case especially experiments toronto face dataset deterministic network outperforms stochastic networks. however critical difference stochastic networks deterministic network observed figure stochastic networks able generate reconstructions correspond different digits ambiguous input. clearly deterministic network cannot model distribution. discussion proposed estimator gradient equation positive negative weights various particles positive weights interpreted pulling probability mass towards particle negative weights pushing probability mass away particle. although showed variance gradient estimate smaller using positive negative weights difference ﬁnal performance estimators substantial challenge structured outputs samples give reasonably large probability reasonably small sample size training separate proposal distribution looks like promising direction addressing issue. might still useful particles subtract constant weights latter ones. approach would yield particles explain well particles negative weights. mnist experiments used separate validation select learning rate. however chose hyperparameter fairly sparse grid report best test error experiments without separate validation set. figure samples drawn prediction lower half mnist test data digits based upper half models trained using deterministic network leftmost column original mnist digit followed masked image samples. ﬁgures illustrate stochastic networks able model different digits case ambiguous inputs. table results obtained mnist structured prediction using various number samples training various estimators gradient error margins standard deviations runs. using stochastic neurons feedforward network computational trick train deterministic models. model deﬁned terms stochastic particles hidden layers shown many valid alternatives usual gradient formulation. proposals gradient involve particles hidden layers normalized weights represent well particles explain output targets. showed theoretically experimentally involving particle signiﬁcantly enhances modeling capacity. demonstrated validity techniques three sets experiments trained classiﬁer mnist achieved reasonable performance network could missing information deleted bottom part mnist digits network could output individual expressions face images based mean expression. hope provided insight properties stochastic feedforward neural networks theory applied contexts study dropout important techniques give stochastic ﬂavor deterministic models. references bastien fr´ed´eric lamblin pascal pascanu razvan bergstra james goodfellow bergeron arnaud bouchard nicolas bengio yoshua. theano features speed improvements. deep learning unsupervised feature learning nips workshop bengio yoshua l´eonard nicholas courville aaron. estimating propagating gradients stochastic neurons conditional computation. arxiv preprint arxiv. bergstra james breuleux olivier bastien fr´ed´eric lamblin pascal pascanu razvan desjardins guillaume turian joseph warde-farley david bengio yoshua. theano math expression compiler. proceedings python scientiﬁc computing conference june oral presentation. esser steve andreopoulos alexander appuswamy rathinakumar datta pallab barch davis amir arnon arthur john cassidy andrew flickner myron merolla paul cognitive computing systems algorithms applications networks neurosynaptic cores. neural networks international joint conference ieee hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. raiko tapani valpola harri lecun yann. deep learning made easier linear transformations perceptrons. international conference artiﬁcial intelligence statistics weaver nigel. optimal reward baseline gradient-based reinforcement learning. proceedings seventeenth conference uncertainty artiﬁcial intelligence morgan kaufmann publishers inc. mnist classiﬁcation well studied problem performances huge variety approaches known. since output class label advantage able model complex output distributions applicable. still benchmark useful comparing training algorithms other used paper test viability gradient estimators. used network structure dimensionalities ---. input data ﬁrst scaled range mean pixel subtracted. regularization method gaussian noise standard deviation added pixel separately epoch models trained epochs. table gives test error rate method. seen table deterministic networks give best results. excluding comparison networks best result obtained proposed biased gradient followed proposed unbiased gradient based results gradient estimators left structured prediction experiments.", "year": 2014}