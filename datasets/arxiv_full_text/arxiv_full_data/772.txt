{"title": "Polyphonic Music Generation by Modeling Temporal Dependencies Using a  RNN-DBN", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "In this paper, we propose a generic technique to model temporal dependencies and sequences using a combination of a recurrent neural network and a Deep Belief Network. Our technique, RNN-DBN, is an amalgamation of the memory state of the RNN that allows it to provide temporal information and a multi-layer DBN that helps in high level representation of the data. This makes RNN-DBNs ideal for sequence generation. Further, the use of a DBN in conjunction with the RNN makes this model capable of significantly more complex data representation than an RBM. We apply this technique to the task of polyphonic music generation.", "text": "abstract. paper propose generic technique model temporal dependencies sequences using combination recurrent neural network deep belief network. technique rnn-dbn amalgamation memory state allows provide temporal information multi-layer helps high level representation data. makes rnn-dbns ideal sequence generation. further conjunction makes model capable signiﬁcantly complex data representation rbm. apply technique task polyphonic music generation. creative machine learning extremely relevant research area today. generative models form basis creative learning reason high level sophistication required models. also identifying features subjective ﬁelds literature music arduous task made diﬃcult elaborate learning desired. deep architectures therefore present ideal framework generative models inherently stochastic support increasingly complex representations added layer. recurrent neural networks also used great success regards generative models particularly handwriting generation used achieve current state-of-the-art results. internal feedback memory state neural networks makes suitable technique sequence generation tasks like polyphonic music composition. many attempts generate polyphonic music past variety techniques applied discussed here. matic used neural networks cellular automata generate melodies. however dependent feature based emotions requires specialized knowledge musical theory. similar situation observed work done maeda kajihara used genetic algorithms music generation. elman networks chaotic inspiration used compose music. rnns excellent technique model sequences used chaotic inspiration external input instead real stochasticity compose original music. deep belief network sliding window mechanism create jazz melodies proposed however lack temporal information many instances repeated notes pitches. recently boulanger-lewandowski used rnn-rbms polyphonic music generation obtained promising results. propose generic technique combination deep belief network sequence generation apply automatic music composition. technique rnn-dbn eﬀectively combines superior sequence modeling capabilities high level data modeling enables produce rich complex melodies feature signiﬁcant repetition. moreover composition melodies require feature selection input data. model presesnted generic technique make assumptions nature data. apply technique variety datasets achieved excellent results current state-of-the-art. rest paper organized follows section discusses various deep neural network architectures serve motivation technique described section demonstrate application rnn-dbns task polyphonic music generation section present results. section discusses possible future work regarding technique concludes paper. samples obtained performing block gibbs sampling visible units sampled simultaneously given ﬁxed values hidden units. similarly hidden units sampled simultaneously given visible unit values. single step markov chain thus taken follows represents sigmoid function acting activations hidden visible units. several algorithms devised rbms order eﬃciently sample learning process eﬀective well-known contrastive divergence algorithm recurrent neural networks particular family neural networks network contains feedback connections activation cluster neurons loop. property allows network eﬀectively model time series data learn sequences. interesting property rnns modeled feedforward neural networks unfolding time. rnns trained using backpropogation time technique. network training sequence starts time instant ends time total cost function simply standard error function esse/ce time step contributions multiple instances weight {wvh dependent inputs hidden unit activations previous time instants. errors must back-propagated time well network. rbms stacked trained greedily form deep belief networks dbns graphical models learn extract deep hierarchical representation training data model joint distribution observed vector hidden layers follows principle greedy layer-wise unsupervised training applied dbns rbms building blocks layer begin training ﬁrst layer models input visible layer. using ﬁrst layer obtain representation input used data second layer. common solutions exist here representation chosen mean activations samples p|h). train second layer taking transformed data training examples vein continue adding many hidden layers required time propagating upward either samples mean values. recurrent temporal restricted boltzmann machine sequence conditional rbms whose parameters time-dependent depend sequence history time denoted mean-ﬁeld value seen rtrbm formally deﬁned joint probability distribution h|a) joint probability whose parameters deﬁned below eqn. eqn. parameters rbms usually depend previous time instants consider case biases depend gives rtrbm parameters{w general scenario derived similar fashion. hidden units binary inference sample generation mean-ﬁeld value transmitted successors important distinction makes exact inference easy improves eﬃciency training. rtrbm thought sequence conditional rbms whose parameters output deterministic constraint hidden units must describe conditional distributions convey temporal information sequence generation. single layer greatly constricts expressive power model whole. constraint lifted combining full distinct hidden units rtrbm graphical model replacing structure much powerful model dbn. call model rnn-dbn. deﬁned arbitrarily given eqn. simplicity consider rnn-dbn parameters {wvh hidden layer rnndbn i.e. biases variable single-layer whose hidden units connected direct predecessor relation demonstrate technique applying task polyphonic music generation. used rnn-dbn hidden layers binary units binary units layer. visible layer binary units corresponding full range piano implemented technique four datasets chorales musedata nottingham piano-midi.de. none preprocessing techniques mentioned applied data data given input rnn-dbn. evaluate models qualitatively generating sample sequences quantitatively using log-likelihood performance measure. results presented table results indicate technique current state-of-theart. believe diﬀerence performance technique current best attributed lack preprocessing. instance transposing sequences common tonality normalizing tempo beats minute preprocessing eﬀect generative quality model. also helps pretraining initialization parameters independent rbms fully shuﬄed frames initializing parameters auxiliary cross-entropy objective either stochastic gradient descent preferably hessian-free optimization subsequently ﬁnetuning signiﬁcantly helps density estimation prediction performance rnns would otherwise perform worse simpler multilayer perceptrons optimization techniques like gradient clipping nesterov momentum nade conditional density estimation also improve results. proposed generic technique called recurrent neural network-deep belief network modeling sequences generative models demonstrated successful application polyphonic music generation. used four datasets evalutaing technique obtained results current state-of-the-art. currently working improving results paper exploring various pretraining optimization techniques. also looking showcasing versatility technique applying diﬀerent problem statements.", "year": 2014}