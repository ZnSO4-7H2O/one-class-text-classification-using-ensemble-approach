{"title": "Adaptive Monte Carlo via Bandit Allocation", "tag": ["cs.AI", "cs.LG"], "abstract": "We consider the problem of sequentially choosing between a set of unbiased Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final combined estimate. By reducing this task to a stochastic multi-armed bandit problem, we show that well developed allocation strategies can be used to achieve an MSE that approaches that of the best estimator chosen in retrospect. We then extend these developments to a scenario where alternative estimators have different, possibly stochastic costs. The outcome is a new set of adaptive Monte Carlo strategies that provide stronger guarantees than previous approaches while offering practical advantages.", "text": "consider problem sequentially choosing unbiased monte carlo estimators minimize mean-squared-error ﬁnal combined estimate. reducing task stochastic multi-armed bandit problem show well developed allocation strategies used achieve approaches best estimator chosen retrospect. extend developments scenario alternative estimators different possibly stochastic costs. outcome adaptive monte carlo strategies provide stronger guarantees previous approaches offering practical advantages. monte carlo methods pervasive approach approximating complex integrals widely deployed areas science. widespread adoption development dozens specialized monte carlo methods given task tunable parameters. consequently usually difﬁcult practitioner know approach corresponding parameter setting might effective given problem. paper develop algorithms sequentially allocating calls unbiased estimators minimize expected squared error combined estimate. particular formalize class adaptive estimation problem learning combine monte carlo estimators. scenario given monte carlo estimators approximate expectation function interest. assume initially estimator unbiased unknown variance. practice estimators could include unbiased method and/or variance reduction technique unique instantiations importance stratiﬁed rejection sampling; antithetic proceedings international conference machine learning beijing china jmlr w&cp volume copyright author. variates; control variates problem design sequential allocation procedure interleave calls estimators combine outputs produce combined estimate whose decreases quickly possible. analyze performance meta-strategy formalize notion mseregret time-normalized excess combined estimate compared best estimator selected hindsight i.e. knowledge distribution estimates produced base estimator. ﬁrst main contribution show meta-task reduced stochastic multi-armed bandit problem bandit arms identiﬁed base estimators payoff given negative square sampled estimate. particular show mseregret meta-strategy equal bandit-regret procedure used play corresponding bandit problem. consequence conclude existing bandit algorithms well bounds banditregret immediately applied achieve results adaptive monte carlo estimation. although underlying reduction quite simple resulting adaptive allocation strategies provide novel alternatives traditional adaptive monte carlo strategies providing strong ﬁnite-sample performance guarantees. second consider general case alternative estimators require different costs produce sampled estimates. develop suitably designed bandit formulation yields bounds mse-regret cost-aware estimation. develop algorithms generalized form adaptive monte carlo provide explicit bounds mse-regret compare performance state-of-the-art adaptive monte carlo method. instantiating viable base estimators selecting algorithmically rather tuning parameters manually discover computation experimentation time reduced. work closely related complementary work adaptive stratiﬁed sampling strategy designed allocate samples between ﬁxed strata achieve mse-regret bounds relative requirement payoff distributions particular parametric form. instead need make much weaker assumption rewards bounded; particular auer proved that ﬁnite number actions ucb’s regret bounded various improvements algorithm since proposed. approach particular interest ucb-v algorithm takes empirical variances account constructing conﬁdence bounds. speciﬁcally ucb-v uses bound etkt denotes empirical variance payoffs pulls exploration function required non-decreasing function ﬁxed constant ucb-v procedure constructed substituting conﬁdence bound algorithm yields regret bound scales true variance constant relating worst case bound slightly worse ucb’s bound; however usually better practice particularly small recent algorithm kl-ucb conﬁdence bound based solving best allocation proportion chosen hindsight. work since extended optimizing number structure strata differentiable functions. method proposed paper however applied broadly base estimation strategies potentially even combination approaches. multi-armed bandit problem sequential allocation task agent must choose action step maximize long term payoff payselected action observed stochastic problem payoff action assumed generated independently identically ﬁxed unknown distribution performance allocation policy analyzed deﬁning cumulative regret sequence actions given random variable giving payaction denotes action taken i{is policy time-step denotes number times action chosen policy time here i{p} indicator function predicate true otherwise. objective agent maximize total payoff equivalently minimize cumulative regret. rearranging conditioning regret rewritten analysis stochastic problem pioneered robbins showed that payoff distributions deﬁned single parameter asymptotic regret sub-polynomially consistent policy lower bounded particular bernoulli payoffs fast respective sample means converge goal design sequential estimation procedure works discrete time steps round based previous observations procedure selects estimator whose observation used outer procedure update estimate based values observed far. common monte carlo literature evaluate accuracy mean-squared error deﬁne loss sequential method round measures excess loss initial ignorance estimator quality. implicit deﬁnition assumption time select next estimator negligible compared time draw observation. note also excess loss multiplied ensures that standard settings sublinear regret implies loss asymptotically matches best estimator. next sections adopt simple strategy combining values returned base estimators simply returns average estimate sophisticated approach weight samples inversely proportional respective variances. however adaptive procedure quickly identify ignore highly suboptimal arms savings weighted estimator diminish rapidly. interestingly argument immediately translate nonuniform cost case considered section shown empirically section main assumption section following assumption estimator produces sequence i.i.d. random observations common mean ﬁnite variance; values different estimators independent. denote distribution samples estimator note ≤k≤k completely determines sequential estimation problem. since samples coming estimator i.i.d. min≤k≤k furthermore hence min≤k≤k ∗/t. ﬁrst main result section. theorem consider estimators assumption holds arbitrary allocation procedure. then mse-regret explicit constants higher order terms apart higher order terms bound matches lower bound general kl-ucb expected better ucb-v except large sample sizes small variances. note that given algorithms apply tightest upper conﬁdence union bound price small additional constant regret. another approach received signiﬁcant recent interest thompson sampling bayesian method actions chosen randomly proportion posterior probability mean payoptimal. known outperform ucb-variants payoffs bernoulli distributed indeed ﬁnite time regret bernoulli payoff distributions closely matches lower bound every problem-dependant constant. however since possible bernoulli distributed payoffs mean different variances analysis directly applicable setting. instead consider general version thompson sampling converts realvalued bernoulli-distributed payoffs resampling step shown obtain formalize main problem consider paper. assume given ﬁnite number monte carlo estimators base estimator produces sequence real-valued random variables whose mean converges unknown target quantity observations different estimators assumed independent other. assume initially drawing sample estimator takes constant time hence estimators differ terms using theorem also establish bounds mse-regret algorithms mentioned section theorem assumption hold assume supported then rounds achieves mse-regret bound using ucb; using ucb-v using =ucb-kl; using =ts. additionally theorem also obtain bounds minimax mse-regret exploiting lower bound bandits ucb-based bandit algorithms shown logarithmic facachieve minimax rate {ucb ucb-v kl-ucb} order appendix provides discussion alternative ranges observations handled methods still applied payoff distribution unbounded satisﬁes moment conditions. next consider case base estimators take different amounts time generate observations. consequence non-uniform estimator times refer non-uniform costs deﬁnitions loss regret must modiﬁed accordingly. intuitively estimator takes time produce observation less useful another estimator produces observations identical variance less time. develop appropriate notion regret case introduce additional notation. denote time needed estimator produce observation xkm. before denote index estimator chooses round denote time observes sample note apply regret bounds section proof follows simple calculation given appendix essentially rewrite loss centered observations cross-terms shown cancel independence wald’s second identity algebra gives result. tight connection sequential estimation bandit problems revealed allows reduce sequential estimation design bandit strategies vice versa. furthermore regret bounds transfer ways. theorem assumption hold deﬁne corresponding bandit problem assigning distribution given arbitrary allocation strategy bandit bandit strategy consults select next obtaining reward based feeding observations copying choices. then banditregret bandit bandit problem mse-regret estimation problem conversely given arbitrary bandit strategy allocation strategy consults select next estimator observing based feeding rewards copying choices. mse-regret estimation problem bandit-regret bandit problem uses average observations estimate). proof theorem result follows theorem since furthermore bandit problem ensures regret procedure e]∆k vk−v max≤k≤k denote variance min≤k≤k family distributions reals dinf v)<v radonnikodym derivative dψ/dφ exists otherwise. note dinf measures distinguishable distributions smaller variance further denote regret estimation problem speciﬁed using distributions theorem distributions supported assume allocates subpolynomial fraction suboptimal estimators means limt→∞ intuitively estimator produce approximately t/δk independent observations hence variance average approximately implies min≤k≤k thus allocation strategy competmin≤k≤k best estimator must draw observations satisfying δkvk δkvk simplicity assume min≤k≤kδkvk unique vk∗. before consider adaptive strategies estimate using mean observations hence estimate time bound regret overall algorithm bounding number times allocation strategy chooses suboptimal estimators. generalizing nonuniform-cost case unlike equality obtained theorem provide upper bound. theorem assumption hold assume bounded unique. estimate time deﬁned sample mean assume assume furthermore appropriate constants depend problem parameters upper bound |xkm| constants proof theorem given appendix several comments order. first recall optimal regret setting logarithmic factors. rate order obtain rate need achieve attained stochastic even adversarial bandit algorithms receiving rewards expectation −δkvk well-concentrated number samples. moment condition also restrictive; example estimators rejection samplers sampling times geometric distribution satisﬁes polynomial tail condition. furthermore dkm≥ t/δ− ensures moment condition ximtim thus +diti distis convenience deﬁne note round starts time choosing estimator ﬁnishes time observation received updates estimate. thus time estimate becomes available estimate renewed. denote estimate available time assuming produces default estimate ﬁrst observation etc. denotes round index time etc.) time before scaling chosen that condition sublinear regret implies learning. note deﬁnition generalizes previous section make following assumption. assumption i.i.d. sequence furthermore assume sequences different independent other. note assumption allows deterministic value; case holds estimators deterministic algorithms produce observations. another situation arises stochastic correlated. case biased estimate however independent unbiased. indeed case independent partial sums although sufﬁcient negative second moment instead variance bandit reward under uniform costs simpliﬁcation longer possible costs nonuniform since δkvk whose expectation δkvk. similar constructions using slightly data used dependent case. note ensuring nontrivial. typical guarantees ucb-type algorithms ensure expected number pulls suboptimal rounds bounded function however dependence cannot generally bounded gk]). nevertheless example t/δ− hence used. finally need ensure last terms remain small follows concengeneral trate around means. achieve regret. however ensure concentration allocation strategy must also select optimal estimator time. example audibert show default parameters ucb-v select suboptimal arms probability making constant parameter ucb-v follows chance using suboptimal times made smaller outside small probability event optimal used times sufﬁcient show concentration summary conclude regret achieved theorem reasonable assumptions. conduct experimental investigations number scenarios better understand effectiveness multi-armed bandit algorithms adaptive monte carlo estimation. ﬁrst consider performance allocation strategies simple -estimator problem. note evaluation differs standard evaluations stochastic bandits absence single-parameter payoff distribufigure left tile plot indicating approach achieved lowest regret estimator scaled-bernoulli setting time x-axis variance optimal estimator y-axis additional variance second estimator. right log-plot illustrating expected number suboptimal selections highlighted case error bars indicate empirical percentiles. tions bernoulli cannot identical means different variances. important detail since stochastic bandit algorithms kl-ucb often evaluated single-parameter payoff distributions advantages scenarios might extend adaptive monte carlo estimation. particular consider problems standard bernoulli separate scale parameter design permits maximum range variance around mean within bounded interval. evaluated four bandit strategies detailed section ucb-v kl-ucb ucb-v used settings used uniform beta prior i.e. relative performance approaches reported figure appears best suited scenarios either estimator high variance whereas ucb-v effective faced medium variance estimators. additionally kl-ucb out-performs ucb-v high variance settings cases eclipsed next consider practical application adaptive monte carlo estimation problem pricing ﬁnancial instruments. particular following consider problem pricing european call options assumption interest rate evolves time according cox-ingersoll-ross model popular model mathematical ﬁnance nutshell model assumes interest rate function time follows square root diffusion model. price european caplet option strike price nominee amount maturity figure plots showing normalized different adaptive strategies estimating price european caplet option cox-ingersol-ross interest rate model using different strike prices results statistically signiﬁcant visual resolution. naive approach estimating simulate independent realizations however simulation interest rate lands strike price ignored since payoff zero. therefore common estimation strategy importance sampling introducing drift parameter proposal density meaning drift; encourages simulations higher interest rates. importance weights simulations efﬁciently calculated function importantly task adaptively allocating trials different importance sampling estimators previously studied problem using unrelated technique known d-kernel population monte carlo space restrictions prevent providing full description method roughly speaking method deﬁnes proposal density mixture {θk} drift parameters considered. time step samples drift value according mixture simulates interest rate. ﬁxed number samples mixture coefﬁcient drift parameter adjusted setting proportional importance weights sam. pled parameter proposal used generate next population. approximated option prices parameter settings namely different strike prices however consider wider proposals given results averaged simulations given figure results generally indicate effective bandit approaches signiﬁcantly better suited allocation task approach particularly longer term. among bandit based strategies clear winner which given conclusions previous experiment likely high level variance introduced option pricing formula. despite strong showing bandit methods remains surprisingly competitive task uniformly better better bandit allocation strategies early however believe advantage stems fact explore entire space mixture distribution remains interesting area future work bandit-based allocation strategies extend existing methods continuously parameterized settings. many important applications monte carlo estimation occur bayesian inference particularly challenging problem evaluating model evidence latent variable model. evaluating quantities useful variety purposes bayesian model comparison testing/training evaluation however desired quantities notoriously difﬁcult estimate many important settings small part fact popular high-dimensional monte carlo strategies markov chain monte carlo methods cannot directly applied nevertheless popular approach approximating values annealed importance sampling nutshell combines advantages importance sampling mcmc deﬁning proposal density sequence mcmc transitions applied sequence annealed distributions slowly blend proposal target technique offer impressive practical advantages often requires considerable effort parameters; particular practitioner must specify number annealing steps annealing rate schedule underlying mcmc method number mcmc transitions execute annealing step. even parameters appropriately tuned preliminary data assurance choices remain effective deployed larger slightly different data sets. figure plots showing average regret bandit allocators logistic regression model. training sample size indicates ﬁxed estimator; missing ﬁgure best setting. solid lines indicate performance combining observations uniformly whereas dashed lines indicate performance combining observations using inverse variance weights. model different sized subsets -dimensional pima indian diabetes data consider problem allocating resources three estimators differ number annealing steps use; namely steps. case annealing schedule using power heuristic suggested single slice sampling mcmc transition used step appendix details. challenge scenario computational costs associated differ substantially slice sampling uses internal rejection sampler costs stochastic. account costs directly elapsed cpu-time drawing sample estimator reported java choice reﬂects true underlying cost particularly convenient since require practitioner implement special accounting functionality. since expect cost correlate sample returns independent costs payoff formulation section results different allocation strategies training sets size shown figure perhaps striking result performance improvement achieved nonuniformly combined estimators indicated dashed lines. estimators change underlying allocation; instead improve ﬁnal combined estimate weighting observation inversely proportional sample variance estimator produced performance improvement artifact nonuniform cost setting since arms close terms vkδk still considerably different variances especially true ais. also observe optimal three training sizes consequently bandit allocation able outperform static strategy. practice implies even exhauspaper introduced sequential decision making strategy competing best consistent monte carlo estimator ﬁnite pool. base estimator produces unbiased values cost shown sequential estimation problem maps corresponding bandit problem allowing future improvements bandit algorithms transfered combining unbiased estimators. also shown weaker reduction problems different estimators take different time produce observation. expect work inspire research area. example consider combining ﬁnitely many inﬁnitely many estimators using appropriate bandit techniques and/or exploit fact observation estimator reveal information variance others. case example samplers importance sampling leading stochastic variant problem known bandits side-observations however much work remains done studying detail variance weighted estimators dealing continuous families estimators thorough empirical investigation alternatives available. work supported alberta innovates technology futures nserc. part work done cs.sz. visiting technion haifa microsoft research redmond whose support hospitality greatly acknowledged. leslie simulation studies optimistic bayesian sampling contextual-bandit problems. technical report statistics group department mathematics university bristol section provide proof theorem proof need following lemmas lemma sequence i.i.d. random variables t)t∈n subsequence decision whether include subsequence independent future values sequence i.e. sequence handling unknown ranges algorithms bounds section easily extended setting priori known. option scale common range using ˜xkt feed bandit algorithms constant translation reward change regret). however algorithms sensitive overestimation range would lead unnecessary deterioration performance. better option scale variable separately. then upper-conﬁdence bound based algorithms must modiﬁed scaling rewards respect range bounds needs scaled back original range. thus method computes reward upper bounds must amin min≤k≤k then method returns upper bound bound used selecting common estimators hence ﬁnding maximizes since conﬁdence bounds loser range larger thus algorithm sensitive ranges amin. particular /n-term bound used ucb-v scale dominate bound smaller values fact ucb-v needs samples term becomes negligible means estimators upper conﬁdence bound ucb-v steps. even upper conﬁdence bounds ucb-v keep using arms large range defending possibility sample variance crudely underestimates true variance. since bound kl-ucb range kl-ucb less exposed problem. based theorem alternative bandit algorithms minimize cost cost deﬁned variance samples arm. advantageous ranges become large unequal lower bounds implement idea needs develop bandit algorithms cumulative variance minimization fact explored section context estimation nonuniform-costs. finally note assumption samples belong known bounded interval necessary fact upper-conﬁdence based bandit algorithms mentioned also applied payoffs subgaussian known subgaussian coefﬁcient even tail heavier fact weaker assumptions multi-armed bandit problem ﬁnitely many arms analyzed assumes moment payoff ﬁnite known known moment bound strategies must replace sample means robust estimators also modify upper conﬁdence bounds calculated. setting condition moment transfers lemma useful concentrates around mean. consider upper bound general expect log) numerical constant thus setting clog results di{e log}. imagine choose then soon last indicator becomes zero. further develop lower bound sampler used every time step. lemma assumption hold assume random variables |xkm µ|km a.s. bounded constant consider case individual sampler used time xkm. then random variables a.s. bounded constant choose common upper bound cancel third term otherwise need select strike good balance ﬁrst third terms. note makes ﬁrst order δkvk/t error term order β/t. thus reasonable choice makes error term order second term lower bound. used follows assumption ﬁnishing proof upper bound. lower bound deﬁnition wald’s identity thus ﬁnishing proof turn proving would like apply wald’s second identity −−)µ). however stopping time w.r.t. ﬁltration not. deﬁne apply wald’s identity since recall deﬁnitions m=... )m=...≤k≤k m=... m=... i{is number samples obtained sampler round ximtim sample distis time observes sample round lasts time period min{m index round time thus number samples observed time period further remember combining lemmas proof theorem straightforward. apply ﬁrst inequality lemma ˆµ−µ s/−)−µ second inequality lemma ˆµk∗ sk/−)−µ. easy decay exponentially fast difference ﬁrst terms negligible. difference second terms handled lemmas bounds simpliﬁed using δmax) introducing obtain used assumption <t/. using last term bound lemma becomes ct−enk∗ indicator third line bound bounded δmaxc} zero since assumption <t/. summing bounds keeping probability following lemma gives variance estimate based kullback-leibler divergence. lemma independent sequences independent identically distributed random variables taking values furthermore considering alternative bounded payout distributions evaluated natural choices truncated normal uniform scaled-bernoulli. latter non-standard distribution transformation bernoulli described three parameters midpoint scale well bernoulli parameter speciﬁcally bernoulli random variable parameter transformation provides corresponding scaled-bernoulli sample. conducted experiments using different distributions keeping variances same attempt ascertain whether shape distribution might affect performance various bandit allocation algorithms. however unable uncover instance shape distribution non-negligible effect performance. result decided conduct experiments using scaled-bernoulli distributions permit maximum range variance bounded interval. approximate using important detail value option exactly zero interest rate strike price maturity. consequently comes monte carlo simulation less interested simulating trajectories lower interest rates. standard exploiting intuition importance sampling variant known exponential twisting here instead sampling noise directly target density uses skewed proposal density deﬁned single drift parameter deriving importance weights consider standard bayesian logistic regression model using multivariate gaussian prior. model σx)yx))−y denotes logit function i.e. density dimensional gaussian mean covariance labeled training examples assumed generated follows first chosen. sequence assumed i.i.d. given labels assumed satisfy demonstrations consider approximating integral using monte carlo integration drawing i.i.d. samples using annealed importance sampling. ﬁrst deﬁne annealed target distribution posterior values using power heuristic suggested given ﬁxed number annealing steps additionally requires specify sequence mcmc transitions annealing step. found slice sampling moves effective moves) additional effectively parameter-free. denote slice sampling transition operator meets detailed balance w.r.t. target algorithm given following procedure", "year": 2014}