{"title": "Telugu OCR Framework using Deep Learning", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "cs.NE"], "abstract": "In this paper, we address the task of Optical Character Recognition(OCR) for the Telugu script. We present an end-to-end framework that segments the text image, classifies the characters and extracts lines using a language model. The segmentation is based on mathematical morphology. The classification module, which is the most challenging task of the three, is a deep convolutional neural network. The language is modelled as a third degree markov chain at the glyph level. Telugu script is a complex alphasyllabary and the language is agglutinative, making the problem hard. In this paper we apply the latest advances in neural networks to achieve state-of-the-art error rates. We also review convolutional neural networks in great detail and expound the statistical justification behind the many tricks needed to make Deep Learning work.", "text": "abstract paper address task optical character recognition telugu script. present end-to-end framework segments text image classiﬁes characters extracts lines using language model. segmentation based mathematical morphology. classiﬁcation module challenging task three deep convolutional neural network. language modelled third degree markov chain glyph level. telugu script complex alphasyllabary language agglutinative making problem hard. paper apply latest advances neural networks achieve state-ofthe-art error rates. also review convolutional neural networks great detail expound statistical justiﬁcation behind many tricks needed make deep learning work. introduction. limited study development end-to-end system telugu script. availability huge online corpus scanned documents warrants necessity system complex script agglutinative grammar make problem hard. building system works well real-world documents containing noise erasure even challenging. task mainly split segmentation recognition. design guided other. robust segmentation easier task recognizer becomes vice-versa. techniques used segmentation somewhat similar across scripts. because usually connected component extracted give unit written text. principle applies roman scripts exceptions; hold complex scripts like devanagari arabic words letters written contiguous piece ink. telugu script intermediate complexity consonant-vowel pairs written unit. keywords phrases machine learning deep learning convolutional neural networks optical character recognition gradient based learning document recognition telugu natural language processing early telugu systems used features encode curves trace letter compare encoding predeﬁned templates ﬁrst attempt neural networks telugu knowledge sukhaswami seetharamulu pujari train multiple neural networks pre-classify input image based aspect ratio feed corresponding network. reduces number classes sub-network needs learn. likely increase error rate failure pre-classiﬁcation recoverable. neural network employed hopﬁeld down-sampled vectorized image. later work telugu primarily followed featurization-classiﬁcation paradigm. combinations like ink-based features nearest class centroid ink-gradients nearest neighbours principal components support vector machines wavelet features hopﬁeld nets used. recent work ﬁeld centred around improving supporting modules like segmentation skew-correction language modelling. work review google drive added functionality works telugu world languages. although details public seems based tesseract multi-lingual system augmented neural networks. paper improves previous work signiﬁcant ways. previous work restricted using handful fonts develop robust font-independent system using training data ﬁfty fonts four styles. data publicly released benchmark future research. training test data diverse enough reliable estimates accuracy. classiﬁer achieves near human classiﬁcation rate. also integrate much advanced language model also helps recover broken letters. system performs better oﬀered google best knowledge publicly available telugu. work break mentioned ‘featurize classify’ paradigm. employ convolutional neural network learns tasks tandem. addition also exploits correlation adjacent pixels dimensional space originally introduced digit classiﬁcation cnns adapted classify arbitrary colour images even thousand classes aided part better regularization techniques like training data augmentation architecture. results discussed regularization language model including recovery broken letters described section conclude evaluation end-to-end system section problem. telugu dravidian language million speakers mainly southern india state andhra pradesh. strong consonant-vowel structure i.e. consonants immediately followed vowel. consonant vowel combine give syllable. example syllable word sakarma figure entity ﬁgure another. syllable written contiguous ligature. alphasyllabic form writing called abugida opposed alphabet. vowels consonants combine give simple syllables. commonly used. nearly sixty symbols including vowel-less consonants punctuation numbers. brings total number symbols used common writing approximately figure demonstrates telugu calligraphy example. goal develop end-to-end system takes image telugu text converts unicode text. would mean ﬁrst need detect lines image. lines detected need segment line’s sub-image individual letters. paper refer extracted connected components glyphs. deﬁnition glyph contiguous segment typically maps syllable like figure syllable composed connected component like ﬁgure think glyph visual syllable. glyph obtained needs recognized representing classes. candidates glyph obtained infer likely line text using language model. segmentation. given binary image text ‘ink’ unity background zero ﬁrst row-ink-marginal i.e. running count number pixels image skew corrected maximize variance ﬁrst diﬀerential marginal. rotation results sudden rises falls rowpixel-counts figure sample telugu poem along row-ink-marginal. detected lineseparations top-lines blue bottom-lines green. smoothened rowink-marginal best ﬁtting harmonic shown right. lines detected looking sudden jumps marginal using wavelength harmonic heuristic. next estimate number lines page novel way. done taking discrete fourier transform mean-centred marginal. wavelength harmonic highest amplitude gives estimate distance baselines. heuristic look sudden drops row-ink-marginal baselines. detect zero baselines identify line separations. additionally also identify toplines point highest gain pixel count line-separation baseline. thus line processed extract connected components using leptonica image-processing library. thus have input classiﬁer arbitrary-sized glyph location reference base lines figure figure shows glyphs ready classiﬁed. generating training data. publicly available training data train classiﬁer with generate mechanized inexpensive manner. assemble moderate sized corpus unicode telugu text internet. based generate sample text containing possible glyphs. text rendered onto digital images four diﬀerent styles ﬁfty diﬀerent fonts. glyphs recovered images segmentation process described above. results nearly unique rendering glyph. hence labelled samples. figure shows renderings sample class. also know location information glyph i.e. position relative line baselines. expect information better classiﬁcation; since glyph placed diﬀerent locations relative base lines results diﬀerent classes. figure shows example. training data used train machine convolutional neural networks. convolutional neural networks shown successful image recognition tasks broad spectrum problems digit recognition imagenet classiﬁcation recent chinese hand-written character recognition task motivates cnns telugu recognition task. problem diﬀers digit recognition many classes also diﬀers chinese task need incorporate location information classiﬁer. cnns introduced exploit two-dimensional correlation structure image data. vanilla neural network ignores structure well suited pattern recognition tasks. convolutional pooling layers operate three dimensional image data heart architecture. full generality input dimage spatial dimensions frequency dimension. colour images sample stack three maps; blue green. hyper-spectral images could many more gray-scale images map. case input single-map binary image. given convolution operation employed cnns complicated begin explaining simple convolution operation image. d-convolution. d-convolution operation parametrised dkernel size typically three pixel interior input image kernel applied. take dot-product kernel l-neighbourhood target pixel. convolution random kernel thus performs local weighted-averaging input image. useful kernels tend various kinds edge/feature detectors. convolution kernel reduces side image might lead loss information borders. avoid this original image zero-padded pixels. figure shows kernels applied binary image. d-convolution. image stack images cnns intermediate representations input image fact images possibly thousands maps. maps tend smaller size input. shown figure constituent input image convolved corresponding kernel. dot-products summed generate single output map. hence input maps gets convolved kernel size give ‘single’ output map. dout output maps required employ many kernels. thus convolution operation kernel size dout convolution kernel kdindout parameters. think operation series dout feature extractors. input tensor convolution operation kernel written figure convolution input color image maps convolutional kernels output maps. many output maps generated form output image. note however maps lose interpretation ﬁrst layer. here weight parameter tensor learned training network. explain role non-linearities shortly; thought thresholding identify interesting features. pool layer. typical convolution layer outputs many maps takes also high correlation adjacent output values map. hence makes sense somehow ‘scale’ maps down generally factor co-ordinate. operation called pooling. typically pooling done grid. maximum four pixels grid extracted output. taking maximum gives neural network translational invariance good classiﬁcation. increase number maps four-fold convolutional layer decrease area image-maps factor four succeeding pool layer. preserves size image time transforming diﬀerent useful space. max-pooling operation tensor written fully-connected layer. cnns anywhere twenty convolutional pooling layers. ﬁnal output image ﬂattened vector. fully-connected layers follow. fully-connected layer basically simple matrix multiplication followed non-linearity. transformation used multilogit model produces positive values one. k-vector obtained matrix multiplication exponentiated make positive normalized belong k-simplex. values interpreted class probabilities. summary think convolutional pool layers non-linearities. non-linearities applied output layer neural network. neural network non-linear hidden layer universal function approximator i.e. approximate continuous function compact space input subjected three convolution-pool operations maps respectively). ﬁnal tensor ﬂattened nodes fully connected hidden layer units followed multi-class logistic layer classes. sigmoid node hidden layer simulates step function different size diﬀerent location. output node combines steps approximate desired mapping. without non-linear activations neural regardless depth linear transformer. despite simplicity zero gradient half domain improve performance network signiﬁcant way. however rectiﬁer activated network needs multiple starts successful training instance. propensity gradients either blow linear region region. observed performance improved allowing choice arbitrary. experiments learn amount ‘leakage’ demonstrate desirable decrease move input output layers. network instead trying learn leakages follows. start ﬁrst convolutional layer decrease layer ﬁnally last hidden layer. figure shows various non-linearities table summarizes eﬀect choices classiﬁcation error application. neural network. glyphs obtained segmentation scaled square image supplied input cnn. preserve aspect ratio scaling. addition binary pixel values numbers representing locations baselines later incorporate network. traditional architecture based lecun reference point compare various design choices regularizations. three pairs convolutional-pool layers employ convolution kernel max-pooling window. followed fully connected layers. leaky-relus place tanh activations lecun last layer’s soft-max activation yields ﬁnal class probabilities using notation developed section model written term composition layer cnn. represent convolutional pooling fully-connected layers respectively parametrized weight vector fwig parameters learned. figure shows traditional model. figure shows output layer sample input. figure shows intermediate ﬁnal features sample inputs. discussion results. table summarizes error rates sizes speeds diﬀerent architectures. k-nearest neighbour classiﬁer poorly data test error. justiﬁes need sophisticated models. multi-class logistic regression test figure letter goes ‘slim’ network table ﬁrst shows input convolution pooling second convolutions pooling third three convolutions followed pooling softmax layer last images hence input output softmax classiﬁcation layer single dark indicating high probability class figure intermediate features ﬁnal transformations various input images notice similar looking images similar representations transformed space. error comes reduces simple linear robust classiﬁer. around million parameters considered point reference model size. conventional neural network hidden layers thousand nodes takes error using nearly million parameters. architectures ignore structure input data specialized problem hand. structure-exploiting three conv-pool operations followed fully connected layers gives error rate nearly many parameters multi-class logistic model. deeper networks convolutional layers fully connected layer give better near human accuracies problem keeping network size small. make network size small parameters still achieving test error rates deeper networks half parameters last ‘classiﬁcation’ layer. rest network eight convolutional layers uses half parameters. thus network parameters equally split classiﬁcation feature-extraction tasks. however convolution operation computationally expensive signiﬁcantly increase time required classify glyph. millisecond range acceptable. even deepest neural networks trained classify much faster classiﬁer. also context larger telugu problem viterbi algorithm employed language model main speed bottle neck. regularization. represents binary input image convolutional layer output maps max-pooling layer fully connected layer units softmax output layer classes. speed speciﬁed design choices. fourteen layered neural network table requires speciﬁcation nearly hundred hyper-parameters. take quite experimentation obtain hyperparameters lead good prediction. ‘slim’ architecture table carefully hand-crafted improve size speed traditional architecture. could spend countless hours tuning hyper-parameters. diﬃculty picking right architecture hyper-parameters gives neural networks mystery. brieﬂy point important design choices attempt demystify process. dataset binary images nearly one-ﬁfth pixels rest background. consider background zero mean image instead consider reverse increase mean higher mean help keep relu units activated non-issue leaky relus never really ﬂat. addition successful training instances leaky relus also improve performance using traditional tanh activation increases computational complexity also reduces performance table summarizes eﬀects. kernel size. traditional architecture based lecun pool layer convolutional layer. kernel size decreases size image decreases pooling. modern architectures tend kernels through. bigger kernels needed multiple convolutional layers used instead. e.g. pair back back convolutional layers kernel sizes three each together result receptive ﬁeld size ﬁve. need weights convolutional layer kernel size needs weights less non-linearity. however price increased computational complexity. regularization. given ﬁtting model million parameters using training samples easy overﬁt making regularization component training process. forms regularization seem work well problem dropout input distortion. distort input classiﬁer albeit diﬀerent ways. input distortion. corrupting input data stabilize predictions employed various machine learning techniques addition shown adding gaussian noise data results l-type regularization much farther gaussian noise apply following distortions input image translation elastic deformation zoom rotation salt pepper noise. although original training data better approximates test real-world data notice distortion proves eﬀective form regularization. network less prone over-ﬁtting given exact sample twice. forced learn invariant features instead memorizing image patches. figure shows sample figure three distortions applied seven original images row. distortion combination random amount translation rotation zoom elastic deformation noise. dropout. dropout employs multiplicative noise input either doubled zero equal probability. else multiplied factor unless explicitly speciﬁed multiplicative noise results penalty data dependent thus incorporating distribution input data regularization process. alternate view dropout inspired invention encourage network learn features useful rather features useful presence another. dropping random half features gradient descent step preventing co-adaptation features. words consider simple case linear regression highly correlated predictors. coeﬃcients co-adapt minimize training error generalization matters. dropout prevents always updating coeﬃcients together thus breaking correlation. helps generalization. another intuition weakly training huge number models averaging test time. much sense dropout convolution layers. dropout applied input layer salt pepper noise. dropout works best applied ﬁnally extracted features feeding softmax layer. hypothesize dropping feature equivalent distorting input image signiﬁcantly. thus dropout input distortions similar eﬀects. also apparent table summarizes performance using various forms regularization. regularization variations traditional architecture as-is input distortion dropout input distortion dropout depth input distortion dropout depth salt pepper noise elastic aspect distortion traditional architecture line table gives test error employs dropout distortion. without them test error shoots using dropout using distortions might seem strange dropout aﬀects performance adversely presence distortions together forms regularization much. motivates examine eﬀect using lower dropout rate. figure summarizes results. absence distortions dropout rate around works best; presence little dropout needed. without salt pepper noise obtain better error rate leaves model susceptible data-drift expect stray pixels real-world data. regularization. given depth gradient loss respect parameters initial layers tends high even high phenomenon dubbed gradient explosion. exponentially many paths initial layers output. thus makes sense impose constraint weights equivalently gradients. form regularization leads figure eﬀect dropout test error. represents training attempt. solid lines show median performance given dropout rate eleven attempts. presence input distortion little dropout required. absence however helps. location information. recall extract glyph’s image also know position relative baselines feeding location information neural network lowers errorrate traditional model signiﬁcant gain gain mostly glyphs like figure change meaning depending located respect baseline. however straight-forward augment input image real numbers representing relative position baselines. fact numbers features need processing. therefore treat append input output layer ensure values scale rest features softmax layer. implementation training. given image full model speciﬁed gives output probabilities. calculate log-likelihood training data according negative log-likelihood cost wish minimize along regularization penalty weights. augmented sophisticated l-equivalent penalties like dropout input distortion early stopping incorporated training process. could usual back-propagation algorithm gradients cost respect network parameters modern approach symbolic diﬀerentiation. network parameters initialized symbols python based software package theano ﬁnal cost speciﬁed terms network parameters given equation. theano gives gradient cost respect network parameter symbols. traditionally back-propagation used eﬃciently apply chain-rule layer time desired gradients. symbolic differentiation packages apply chain-rule internally give gradient function-object evaluated point interest space. addition liberating write complicated back-propagating code theano also leverages processing power graphics processor units available modern computers. also optimize gradient function-object algebraic simpliﬁcations possible. space. mini-batch stochastic gradient descent gradients averaged ‘mini-batch’ training samples. also momentum nesterov averaging step along direction exponentially weighted running average recent gradients randomly initialized neural network trains hours going entire dataset times system nvidia titan black give test error rate training times slower without gpus. convergence faster leaky relus relus former gradients larger magnitudes reason convergence faster relus tanh activations. language model. list probable candidates extracted glyph remains likely line text. could consider likely candidate glyph output sequence best-matches ﬁnal extracted text. might best thing strong dependency structure human speech text. reason good accuracy training test data might able improve using language model classiﬁcation. using language model helps resolve ambiguities similar looking glyphs. spurious attach glyphs together erasure glyph pieces. language model help address problems. further incorporate biases system according frequency occurrence various glyphs letters words etc. here estimate probability estimate given classiﬁer shown image intuitively formula reﬂects human reading mechanism reader combines likelihood sequence letters language glyph looks like might surprising note formula. drops chose train neural network number samples class detailed derivation formula given now. start bayes’ rule simplifying math makes problem harder. additionally applicable case incorporate font based dependencies across glyphs neural network using bayes’ rule unconditional probability glyph appearing language. probability class given image. know approximate probability given classiﬁer shown image class probability estimates however learned showing network diﬀerent distribution denoted network’s estimate probabilities given image biased hence need perform case-control correction estimate assuming known n-gram model. although humans employ much complicated language models infer text sake character recognition simple n-gram model n-gram model order markov model. current unit language independent previous units. unigram model calculates probability sequence units product individual probabilities. seldom applies since know example likely followed english lack agglutination makes possible look words making natural choice language units. telugu however agglutinative. sample expression from-in-between-those-two-persons grammatically written word lack solid sense word identity exacerbated lack standard convention writing long words. expressions from-in-between those-two-persons in-between those-two-persons also accepted modern verse. addition language strongly phonetic diﬀerent accents word rendered diﬀerently. hence word level dictionaries limited use. glyph basic language unit. number glyphs sequence here notation represents beginning sentence ignore negative index. problem employ trigram model glyph level. since trigram supported points rich enough stand dictionary. given syllabic nature writing language impossible glyphs occur glyphs. renders trigram incidence matrix sparse. take candidates given glyph corresponding scores viterbi algorithm likely sequence glyphs according formula heuristic over-segmentation. major problem scanned text erasure ink. results glyphs like figure appearing neural network trained recognize whole glyphs. even glyph parts neural network still tries classify parts whole glyphs. section explain broaden scope problem include text glyphs broken parts. techniques presented section easily applied extending scope problem direction i.e. recover glyphs joined together. original segmentation algorithm works looking connected components. pixels said connected belongs fourneighbourhood other. could also consider eight neighbours instead four. four-connection results broken glyphs eightconnection. latter higher propensity mistakenly join glyphs. employ four connection rejoin broken glyphs. approach called heuristic over-segmentation segmentation module outputs series glyphs ordered left right glyphs represented linear graph edge glyph. glyphs linear graph nodes. figure shows example. combining broken glyphs. initial graph linear hence node child. graph parsed bottom starting last node. given node consider consecutive edges self child child grandchild. check edges need combined. repeated grandchildren children. although node child begin with time decide edge grandchild current node becomes child pieces combined give candidate glyph assign weight rule total score exceeds threshold combine. heuristics developed looking various failure cases sample texts. alternately could sparse logistic regression model large heuristically developed rules subset important rules needs additional training data. better over-combine under since original pieces combination considered processing. example considered ﬁnal viterbi decoder. viterbi decoder. ﬁnal segmentation graph directed acyclic graph generate recognition graph segmentation graph take corresponding image pass classiﬁer candidates build graph replacing segmentation graph weighted arcs carrying diﬀerent candidate-character. seems suﬃcient practice. figure shows example matches image. need path recognition graph corresponds highest probability deﬁned note terms glyph probability candidate label given image n-gram probability candidate label given previous former incorporated recognition graph however latter not. finding strongest path recognition graph would correspond picking top-match glyph. would incorporate linguistic information. hence need pick among paths highest path probability deﬁned could computationally expensive. suppose string thirty glyphs matches glyph paths consider. solve seemingly intractable problem famous viterbi algorithm eﬃcient dynamic programming algorithm ﬁnds shortest path graph. viterbi algorithm need augment recognition graph n-gram probabilities. length edge likelihood glyph instead need multiply n-gram probability current character-candidate given candidates previous edges recognition graph replaced edges order n-gram. thus node children edges. call n-gram graph viterbi algorithm graph likely path. figure recognition graph image figure replaced three arcs corresponding three matches image corresponding percentage probabilities also shown edge weights. top-matches shown blue. figure n-gram graph shown matches image bigram probabilities application leading much complicated graph. viterbi algorithm ﬁnds strongest path graph. ‘c’‘o’ edge-label denote candidate previous image current. %probability given image; %probability given ‘c’. edge strength represents beginning sentence. _c–co–or–rn strongest path making corn ﬁnal transcription. recalibrating probabilities. recall estimate correct bias probability given classiﬁer. trained model asymptotically consistent considerations approximation true model choice particular conﬁguration rather arbitrary; gives diﬀerent probability estimate given pair. huge model like prone over-ﬁt values input softmax layer deﬁne weight matrix could scale coeﬃcients ﬁxed quantity predictions neural network change conﬁdences deﬁne biased probabilities here multiplication scales norm vectors thus acting quadratic penalty. classes tend equal probability. class completely dominate rest. work well n-gram model important multiply log-probabilities gives correct amount thus becomes additional tuning parameter combined model. higher give classiﬁer power end-to-end system; similarly smaller give n-gram model power. work well scenario processing text uses font seen written dialect well approximated n-gram. processing page sanskrit text written telugu script might want give inﬂuence relative comprehensive evaluation. test integrity three modules working together evaluation suite sample texts. texts varying levels degradation conform scope problem various levels. results summarized table sample texts shown figure evaluation traditional architecture augmented location information uses parameters give test error language model learn trigram probabilities modern corpus unicode telugu characters obtained internet. perfect recovery well printed scanned ‘nudi’ text uses modern font. however realistic specimen. better evaluate performance target documents test fonts seen neural network. seen table network susceptible data/font-drift error-rates higher unseen fonts implementation recognition-graph also gives signiﬁcant improvement performance extends scope problem text erasure ink. recover well scenario high four glyphs broken also implementation figure sample texts used end-to-end evaluation summarized table ﬁrst sample artiﬁcially rendered rest real-world specimens scanned printed books. n-gram model reduces error-rates telugu texts. however introduces errors sanskrit text ‘rāmayaṅa’ written telugu script. understandable given corpus modern telugu learn trigram probabilities. could recalibrate probability estimates given according similar scanned text going modern internet telugu. thanks deep learning seem consistently better google system however seem better sanskrit text ‘rāmayaṅa’ think system gives satisfactory performance diverse looking documents. performance might enough replace human transcriber good enough enable search facility digital corpus scanned texts. conclusion. took challenging task telugu designed implemented end-to-end framework solve system generalizable brahmic alphasyllabic scripts like kannada malayalam gujarati punjabi etc. ones headline. heuristics speciﬁc telugu script played important role solving problem. questions framework’s generalizability. heuristics used detect lines easily generalizable ones used combine glyphs need redesigned. latest advances deep learning design good classiﬁer heart system. classical techniques like n-gram modelling viterbi algorithm equip model good language prior. although scope problem originally restricted well scanned documents without erasure later expand handle documents erasure. similarly extend model handle joined glyphs. this needs initially generate candidate splits edges original linear graph figure however even expanded scope expect achieve human-level performance. language models employed humans complex framework generalizes scripts like devanagari arabic modiﬁcations segmentation algorithm. scripts written words connected components. modern deep learning techniques like recurrent neural networks connectionist temporal classiﬁcation bypass segmentation task altogether. need data segmented one-to-one correspondence input samples output labels. software. software end-to-end system freely available github.com/teluguocr. software data generation along required fonts corpora also available there. bastien lamblin pascanu bergstra goodfellow bergeron bouchard bengio theano features speed improvements. deep learning unsupervised feature learning nips workshop. bergstra breuleux bastien lamblin pascanu desjardins turian warde-farley bengio theano math expression compiler. proceedings python scientiﬁc computing conference graves liwicki fernández bertolami bunke schmidhuber novel connectionist system unconstrained handwriting recognition. pattern analysis machine intelligence ieee transactions zhang delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. arxiv preprint arxiv.. hinton srivastava krizhevsky sutskever salakhutdinov improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv.. kumar bhagvati negi agarwal deekshatulu towards improving accuracy telugu systems. document analysis recognition international conference ieee. pujari naidu jinaga intelligent character recognizer telugu scripts using multiresolution analysis associative memory. image vision computing simard steinkraus platt best practices convolutional neural networks applied visual document analysis. international conference document analysis recognition ieee computer society. simard lecun denker victorri transformation invariance pattern recognition—tangent distance tangent propagation. neural networks tricks trade springer.", "year": 2015}