{"title": "Proactive Message Passing on Memory Factor Networks", "tag": ["cs.AI", "cs.CV"], "abstract": "We introduce a new type of graphical model that we call a \"memory factor network\" (MFN). We show how to use MFNs to model the structure inherent in many types of data sets. We also introduce an associated message-passing style algorithm called \"proactive message passing\"' (PMP) that performs inference on MFNs. PMP comes with convergence guarantees and is efficient in comparison to competing algorithms such as variants of belief propagation. We specialize MFNs and PMP to a number of distinct types of data (discrete, continuous, labelled) and inference problems (interpolation, hypothesis testing), provide examples, and discuss approaches for efficient implementation.", "text": "introduce type graphical model call memory factor network show mfns model structure inherent many types data sets. also introduce associated message-passing style algorithm called proactive message passing performs inference mfns. comes convergence guarantees eﬃcient comparison competing algorithms variants belief propagation. specialize mfns number distinct types data inference problems provide examples discuss approaches eﬃcient implementation. keywords machine learning optimization pattern recognition models vision scene understanding image restoration paper introduce memory factor networks proactive message passing algorithm. objective combine capability message passing algorithms make large-scale inferences highly eﬃcient manner ability machine learning algorithms generalize experience. factor graphs message passing proved extremely eﬀective combination faced inference task wherein global problem structure decomposes large local constraints. applications error correction decoding local constraints january analog devices lyric labs cambridge simple characterization makes local inference computationally simple. message-passing algorithms iterate making local inferences combining local decisions global estimate. however many settings local problem structure known ahead time engineered error correcting codes rather must learned. instance number examples provided initial task deduce underlying problem structure exemplars. following that make inferences based problem structure learned. tasks—of deducing problem structure making inferences based structure—are central many areas data analysis statistics machine learning. mfns variables describe problem conﬁguration. collection memory factors learned examples encode constraints overlapping subsets variables. high-level goal conﬁguration variables minimally conﬂicting local problem structure represented memory factors evidence obtained world. conﬂicts occur constraints overlap general cannot satisﬁed exactly. description somewhat generic innovation mfns design memory factors encode learned local problem structure manner easily accessible inference. guiding philosophy joint design graphical model message passing algorithm make necessary computations eﬃcient. analogy biology need memories easily accessible thinking. thus engineer detailed statistical structure problem engineer store knowledge statistical structure order make easy exploit inference tasks. framework memories correspond learned local problem structure encoded memory factors. second innovation simple methodology producing local inferences guarantees global convergence. learned problem structure encoded memory factors extremely rich—in comparison factor nodes used graphical models belief propagation typically applied to—a simple methodology producing local inference important maintain computational tractability. accomplishes intuitive voting mechanism that iteration considers current messages coming neighboring memory factors order make choices reduce global objective. thus proactive sense considers eﬀect possible local choice global situation deciding choice implement. proactivity ensures global objective decreases iteration guarantees converge locally optimum conﬁguration. order ensure converges good local optimum powerful heuristic scheduling factor updates derived novel notion factor conﬁdence. general idea combining message-passing inference algorithm graphical model built using example-based learning approach introduced freeman vista approach main application considered using vista approach example-based super-resolution vista approach used markov random ﬁeld graphical models built dynamically response inference problem encountered inference algorithms used belief propagation eﬃcient one-pass algorithms. think paper providing formal generalization vista idea. whereas vista work primarily concerned ﬁnding good solution particular application super-resolution primary concern carefully deﬁne demonstrate general approach learning-based inference based memory factor network data structure proactive message-passing algorithm. believe approach applied wide range problems going beyond encountered computer vision. section introduce memory factor networks. section describe proactive message passing algorithm. then section specialize discussions section speciﬁc types variable nodes cost functions section turn memory factors describe distinct types memory factors. ﬁrst consider memory tables memory factor consists database fragments observed exemplars. consider subspace factors memory factor constrains local problem structure reside learned lowdimensional subspace. section provide illustrative applications include face reconstruction missing noisy data music reconstruction handwritten digit classiﬁcation. examples compare contrast performance memory tables subspace factors. make concluding remarks section memory factor network contains variables constraints variables. constraints encode problem structure induced observations. capture types constraints memory factor evidence factor nodes respectively. seek conﬁguration variables minimizes cost function variables induced factors. following single factor. refer vector variables neighboring factor similarly deﬁned denote vote vector weight vector. occasionally need refer variables write refer set. also need refer votes regarding variable subset neighbors; denote perhaps edge weights description standard; quite neatly formalism normal factor graph forney forney’s normal representation variable factor nodes would correspond factors votes would correspond variables. objective minimize global cost function factored based graphical structure problem. include types local costs mismatch costs selection costs. mismatch costs penalize diﬀerence variable setting factor’s vote variable contribution mismatches global cost additive variable factor indices contribution weighted particular vote’s weight. make mismatch costs function variable index factor index. reason that weighting votes pertaining particular variable compatible sense therefore measure mismatch apply all. selection costs make various choices factor’s vote vector less costly thus function pertinent factor’s index unlike mismatch costs selection costs general additive factor example local mismatch costs absolute diﬀerence squared diﬀerence indicator function discrete set. selection costs }i∈na belong discrete certain subspace design limit practice selection costs arise result training phase examples shown system likely frequent conﬁgurations learned. later system trying understand sample world tries minimize overall eﬀorts minimize introduce proactive message passing algorithm. proactive message passing works sequence iterative subproblems attempt minimize algorithm understood analogy idealized political convention objective convention determine party platform. party platform consists number stances issues issue corresponds variable analogy stance value variable. delegate corresponds factor delegate concerned subset issues. close convention want identify optimal vector stances results minimal dissatisfaction amongst delegates; dissatisfaction measured analogy political convention exact hope help elucidate algorithm develop intuition. convention sequence rounds balloting. round corresponds iteration algorithm. round delegates vote issues concern them others abstain voting. delegates vote initially ones conﬁdent opinions. delegates temporarily abstain delay voting become suﬃciently conﬁdent opinions. round balloting delegates reconsider opinions changed based recent votes cast. forming opinions delegate takes account current votes issue interest delegate. opinions formed correspond votes that cast would reduce maximally cost function derived algorithm termed proactive delegate preemptively evaluates change votes would impact dissatisfaction delegates voted issues. next round balloting subset delegates chosen change votes cast initial votes. subset chosen delegates conﬁdent opinions; delegates least likely opinions change future delegates’ votes. cost function structured that eventually delegates must cast votes abstain. convention ends delegates’ votes align suﬃciently opinions wants change votes. mathematical framework around intuition described above. description factor current opinion value variable neighboring opinion diﬀer vote already cast. opinion vector connect original objective minimizes cost tuple ﬁrst tuples assert lexicographic order either ordering global optimum tuple minimizes algorithm feature iteration cost decrease ultimately factors abstain although algorithm converge local optimum rather global one. feature guaranteed convergence least local optimum shared message-passing algorithms operating factor graphs containing cycles vote-changing factor voted previously removed abstaining note never factors therefore cardinality abstaining decreases monotonically iteration count. reacting changed votes ∪a∈v factors that based dissatisﬁed contains factors whose recomputed opinion vector fact match vote vector given chance cast votes match latest opinions. note abstaining plays fundamental role algorithm role reactive dissatisﬁed sets purely reduce computation. algorithm could deﬁned without tracking latter sets number redundant computations would repeated iteration. initialize algorithm start factors want vote ﬁrst iteration. natural choice evidence nodes votes factors equal observations. abstaining if\\v reacting ∪a∈v dissatisﬁed note selection mismatch costs nonnegative. therefore elements cost tuple associated non-negative. given iteration factor node stops abstaining ﬁrst element tuple decreases reducing cost. hand abstaining change second element tuple change. since design choice votes decrease then abstaining change second element tuple least small previous iteration. thus cost tuple monotonically decreasing. overall proactive message passing algorithm summarized algorithm step algorithm optimal opinion vector factor computed using step opinion vector chosen factor minimize factors neighbor variables factor neighbors. proactively choose value neighboring variables optimize combined costs second third terms given current state factors’ votes proactivity reminiscent cavity method introduced statistical mechanics study spin glasses another step highlighted choose conﬁdent factor change votes. simply want greedily choose factor reduce overall cost most might diﬀerent choices values variables connected factor reduce overall cost similar large amount committing prematurely could lead convergence poor local optimum. particular easily happen factors initially isolated factors including evidence nodes. could reduce overall cost signiﬁcantly selecting memory voting early would lead premature commitments leading ultimately convergence poor local optima. instead want prioritize factors choice variable values signiﬁcantly better choice—what call conﬁdent factors. factors typically share variables several factors already voting given votes choice memory would reduce overall cost much other. equation written general form want manipulate form reveals message passing interpretation. particular want obtain messages factor node variable nodes connected informing factor preferences rest network variable node form suﬃcient statistic. begin deﬁning optimal choice based votes variable non-abstaining neighbors factor i.e. compute opinion vector factor opinion vector chosen minimize combination selection cost opinion vector aggregate mismatch costs appropriately weighted computed respect optimal choice variable settings vector possible opinion vector. following vector dimension alphabet many reasonable choices mismatch cost functions variable alphabets minimization requires small amount summary information think summary information external vote weight vectors message interpreted incremental cost factor casting vote variable always possible reexpress simply setting equal transformation thus real interest dimension message small easy transformation enables perform opinion computation relatively eﬃciently still accounts overwhelming fraction total running time particularly candidate opinions minimization runs large. luckily computation entirely independent factor aﬀect state external factor lines algorithm executed parallel diﬀerent form parallelism actually changes behavior algorithm multiple factors change votes simultaneously. simplest version single conﬁdent factor changes vote iteration. version easily guarantee convergence iteration memory factor changes vote vector make guaranteed non-increasing change overall objective. memory factors able adjust change computing opinions. contrast could multiple factors neighboring variables. simultaneous voting nearby factors change vote simultaneously factor react total rather twice. since every factor reacting participates opinion updates reducing number times factor enters reacting reduces total number opinion updates algorithm consumes less time even single core computer. simultaneous voting also signiﬁcantly reduces overall number iterations required converge shall later. terms quality optimum found clear whether simultaneous voting superior inferior serial voting. simultaneous voting potential advantage make less sensitive whims single factor. conﬁdent factor disagrees next four conﬁdent factors letting vote likely move system non-optimal direction; factors change opinions based ﬁrst factor’s votes chance express majority opinion. hand factors vote simultaneously system move consistent majority. empirically found converged slightly better optima mnist handwritten digit classiﬁcation task simultaneous voting used hand simultaneous voting slightly degraded empirical performance restoration corrupted images simultaneous voting introduce complication longer true cost must monotonically non-increasing iteration. updating factors share variables change factor makes vote vector could alter opinion another factor. practice however observe using procedure cost function increases extremely rarely. further possible recover guarantee convergence following simple modiﬁcation. observe whether changes vote vectors particular iteration result cost increase interference vote-changing factors. cost increase observed roll back vote changes iteration return serial procedure allowing single conﬁdent factor change votes. practice rollback occurs rarely; mnist task fewer simultaneous votes retracted. computing ﬁnal variable settings return computing opinion factor version given message-passing interpretation. thus great importance minimization tractable. indeed choices alphabets cost functions obtain explicit solutions allows write simple message-passing form discussion sec. messages come neighboring variable nodes summarize votes memory factors neighboring variables. consider situation mismatch cost absolute diﬀerence i.e. further consider setting weights identical. argue taking median value votes minimizes cost. median actually values example four votes values median would generally median show elements median equal cost. fact multiple elements even-degree variable node. case half elements equal least element half least equal largest element set. change value among elements median mismatch cost increase half votes amount decreases half. thus votes balanced range median conﬁrm elements median equal cost. next understand choice outside median incurs greater cost consider case equal largest value median set. increase mismatch cost |˜xi would increase least fact least elements values equal median could decrease cost increases take anything larger largest element median set. similar logic holds choose smaller smallest element median set. case summary message variable passes memory factor pair integers message suﬃcient factor determine impact choosing particular opinion variable global cost function. impact summed across variables memory factor neighbors. occurs frequently. pick vote value cost would increase decrease number votes. thus optimal pick median singleton multiple distinct elements. thus write optimal section describe ways construct memory factors—a simple approach using memory tables somewhat complicated perhaps scalable approach using reduction lower dimensional subspace. memory table simply database exemplars exemplar encoding valid conﬁguration variables neighbor factor question. memory factors memory tables trivial train. given number exemplars choice simpliﬁes algorithm. outside restricts optimization opinions exist memory table selection cost zero. conﬁdence memory table factor diﬀerence total cost best second-best distinct sets opinions corresponding memories. intuitively memory table factor high conﬁdence choice memory correct even memory costly next best alternative much worse. therefore makes sense factor cast votes allow less conﬁdent factors react decision. another particular form learned structure utilize memory factors reduction lower dimensional subspace refer memory factors enforce dimensionality reduction subspace factors. particular consider linear subspace factors learned data variety methods e.g. nonnegative matrix factorization case cone mentioned above. apply sparsity assumptions hidden variables generally enforce subspace factor indeed provides low-dimensional representation visible context suppose particular memory factor subspace factor maps hidden variables require neighboring variables include visible variables represented interest problem. paper assume precisely visible variables subspace factor thus |na| note restricts variables alphabet restriction could lifted allowing cost simply requires vote subspace deﬁned factor allows hidden variables take value domain. selection cost either zero inﬁnite replace opinion update optimization feasibility constraint problem becomes system parameter choose balance term representing mismatches opinion incoming messages term representing number votes messages represent. intuitively factor conﬁdent information rest system opinion closely matches information. scale score total number adjacent variables penalty variables votes relative impact subspace factors variables total. section describe number applications illustrate basic mechanisms abilities memory factor networks. also compare behavior mfns using memory tables using subspace factors. showing single face manually aligned facial features pixel locations image. images training hold testing. describe memory factor network represents images used variety tasks similar images. variables interested green blue pixel values pixel position image additional variables representing gray value pixel added examples. variables therefore nonnegative real numbers. also normalized enforce explicitly simply truncate rare cases returns larger value. type. corners edges follow pattern truncate factors necessary. structure essentially creates three parallel factor networks representing three color linked factors size single channel factors greatly simpliﬁes connections factors. implementation convenient group variables sets connections sets variables factors rather directly variables factors. choice size linked factors sets case memory tables implementation factors simple table includes appropriate pixel values face training set. subspace factors must learn matrix since pixel values nonnegative choose nonnegative matrix factorization learning step. precise learn given matrix subspace factor variables training images choose value implementation nmf.jl package julia programming language alternate least square using projected gradient descent algorithm. matrix desired subspace matrix representing hidden variable values correspond training samples. note region corners missing region evidence corners updates done serial fashion evidence given weight memory factor weight memory table solution mean square error subspace solution mean square error approach works arbitrary collection scalar variables dependent square samples subspace matrices particular size variables particular meaning. simply need transform whatever portion image interested vector scalars stack vectors matrix factorize. restriction comes choice smaller avoid trivial factorizations. generally choose much smaller subspace sense representing features image segment learned training set. primary example problem test image portion image erased example eyes general algorithm able make reasonably plausible reconstructions missing pixels based pixels memory face images. naturally test image training set. case provide network evidence nearby sections image algorithm generate pixel values occluded region. case assume evidence erased matches ground truth weight evidence pixels made suﬃciently large dominate factor votes determining ﬁnal variable values. without modiﬁcation tendency blur distort images particularly subspace case since must represented dimensions. subspace networks tend generate average eyes eyes match surrounding face lack signiﬁcant notable features. memory tables drawing directly wide sample eyes likely generate unique-looking eyes match other. also lead color anomalies variation sorts results factor votes for. subspace factors. factors cover entire image. updates done using simultaneous voting conﬁdent subspace factors wanting update iteration. evidence given weight memory factors given weight memory table solution mean square error subspace solution mean square error image rather given section. data loss isolated little gained correcting data provided data loss spread among sections image data present treating evidence absolute truth result anomalies pixels receive smoothed values adjacent pixels keep variable original. cases make sense simply smooth whole image usually results blurry consistent-looking face rather blotches. fig. example blurring image randomly missing evidence. another example missing data case given evidence grayscale pixel values rather color data. given grayscale image face would like color requires introduction variables gray values attached subset factors. best results come including gray factors contain three color channels. recall gray value pixel found taking linear combination values evidence given weight factor weight subspace factors used hidden variables. color output scaled match gray evidence. memory table solution mean square error subspace solution mean square error factor. factors cover entire image. updates done using simultaneous voting conﬁdent factors wanting update iteration. evidence memory factors given weight memory table solution mean square error subspace solution mean square error resemble grayscale image though blurred. subspace factors also often leave sections colorized image gray apparently little distinguish color options whereas memory tables always choose color memories. case rarely ever make sense privilege given evidence case noisy data. known pixel values perturbed prefer smoothed result given applying algorithm generally equal weights factors evidence fig. example results case noisy data. described settings problems described test images computing mean square error color-pixel values statistics summarizing results presented table images best worst examples problem available appendix another application mfns processing audio ﬁles. question interest reconstructing missing data smoothing noisy data design decisions diﬀerent depending application ﬁrst describe general process transforming audio format appropriate mfn. data second long clips randomly generated music downloaded otomata training samples test samples. clips sample rate memory table factors limit usage random subset training samples used sample included independently probability mfns work spectrogram representation audio ﬁles short-time fourier transform process audio samples. process begins splitting audio series overlapping frames given length. overlap varied changing size determines distance starting time consecutive frames. frame length size splitting frames data frame multiplied window function hanning window. windowed frame individually fourier transformed. fourier transforms matrix complex values column represents frame time represents signal frequency. frequency range resolution determined sample rate audio length frames respectively. particular number frequencies represented equal half number data points frame plus frames audio sampled frequencies represented ranging rather entire matrix decrease size grouping frequencies bins. given number frequencies desired number bins compute coeﬃcient values included frequencies. logarithmic binning scheme reduces size network work maintaining resolution lower frequencies importance original signal. binned matrix prepared build memory factor network. variables complex valued learn subspace factors follow work baldi approach. speciﬁcally matrix formed eigenvectors greatest magnitude eigenvalues perceived quality sound generally depends frequencies present sound consider memory factors cover entire frequency spectrum small period time. allows factor learn entire frequency proﬁle given sound often includes many distant frequencies. main test case randomly generated music also note absolute time position little meaning priori expectation that example last seconds sample diﬀerent ﬁrst seconds. this instead learning diﬀerent memory factors diﬀerent parts spectrogram learn single subspace matrix memories time-positions training examples share memory factors network. note results large training even relatively small number music samples. note time-independence feature particular choice audio signal would hold true something structured time like short piece speech. examples would closely resemble face example speciﬁc features generally appear time position every sample. figure example output music missing evidence. pixel wide factors covering entire frequency range hidden variables subspace factors. updates done serial fashion. evidence given weight memory factors given weight memory table solution mean square error subspace solution mean square error present preferring maintain original signal possible. also generally advantageous pick relatively wide memory factors maximize connection newly created signal original signal adjacent example spectrograms reconstruction problem fig. constructing missing music memory tables signiﬁcantly better properties subspace factors testing. memory table network selects full notes sequences notes attempts choose best surrounding music whereas subspace factors generally match dominant frequencies surrounding music recreate shape notes rather creating sort buzz often volume spike. believe part reason behavior subspace factors trained variety shifts time note rather forced choose shifts able combine them creating smoother blended sections spectrogram quite unlike patterns notes training set. another problem consider removing noise music sample. simulate problem normal random noise test sample. many regions spectrogram value zero absence noise evidence given signiﬁcant weight relative memory factors certain degree noise still present algorithm. combat eﬀect twice ﬁrst large weight evidence votes factors little weight evidence taking ﬁnal votes factors initial votes second run. allows factors take advantage evidence allowing dominate. factors primarily need match signal underneath noise rather coordinating time intervals narrow factors noise problems. example noisy music problem seen fig. memory tables better able maintain rich structure notes though subspace factors maintain much structure music attempting create figure example output music noisy evidence. pixel wide factors covering entire frequency range hidden variables subspace factors. updates done serial fashion. evidence originally given weight memory factors given weight evidence weight second initialized ﬁnal factor votes ﬁrst. memory table solution mean square error subspace solution mean square error stretches better able exactly match main structures memory tables must matching notes among memories. combining notes diﬀerent training samples results somewhat disjointed reconstruction whereas subspace factors somewhat fuzzier smoother. described settings problems described test clips computing mean square error spectrogram pixel values. statistics summarizing results presented table images best worst examples problem available appendix work images original images centered order facilitate hierarchy pixel label variables. addition level- variables representing image’s pixels hidden level- variables corresponding lower-resolution version image hidden level- variables corresponding even lower resolution version hidden level- variables representing entire image. hierarchy label variables exists parallel hierarchy pixel variables level- variables level- variables level- variables single level- variable characterizing entire image. pixel variables integer variables linear cost weights label variables indicator cost described section weights label variables higher weight pixel variables fewer natural scale mismatch lower. hierarchy useful properties. firstly allows locally inferred labels synthesized global label read classiﬁcation entire image. secondly provides fast lane information propagate part image remote part. inferences pixels labels hierarchy faster linearly diﬀuse network. network hierarchy constructed following manner. factor level connected pixel variables label variables level well pixel variables label variable level sample variables connected single memory table factor shown fig. testing time hidden pixel label variables connected evidence factors; acquire values result voted memory table factors. level- factor thinks group pixels characteristic cast votes neighboring level- pixel variables blurred version memory cast vote neighboring level- label variable. factors mismatch cost disagreeing pixel variables label variables thus factor isn’t sure whether looking swayed fact neighboring factor cast vote label algorithm converged classify image according single label hierarchy. simple memory factor network suﬃcient classify test images mnist data accuracy single factor voting time accuracy using simultaneous votes factors average single-vote implementation required iterations opinion updates simultaneous-vote implementation required iterations opinion updates. natural application mfns using memory tables reconstruct previously-seen images presented versions corrupted noise erasure. similar network face reconstruction application data images read previously-seen images selected random gaussian noise applied every color channel value every pixel standard deviation addition randomly-generated blob pixels completely erased center image image presented restores image original version removing applied noise ﬁlling correct pixels erased region. application particularly convenient quantitative metric assessing quality result; simply measure distance computed image original. makes natural choice evaluating eﬀect changes algorithm. thus assess eﬀects simultaneous voting algorithm test images using standard serial algorithm simultaneous voting version described section running factors every iteration. simultaneous voting version less likely reproduce initial image exactly average error comparable serial version. serial version restored images perfectly average total error across color channels simultaneous voting version restored images perfectly average total error introduced approach combining inference learning experience. memory factor networks provide simple store examples learned experience proactive message passing using conﬁdence-based scheme prioritizing factor updates gives reliable converge good optima memory factor network cost function. consider algorithms applications demonstrated initial foray possibilities raised approach. thus might consider factor graphs combined memory factor nodes conventional factor nodes encoded known statistical dependencies constraints. algorithm well-suited mfns might nevertheless consider using approaches based belief propagation variational approaches based alternating directions method multipliers many potential applications might attack approach including computer vision problems inferring depth single images motion videos. finally important open question well approach described take advantage massive amounts data often available reader reasonable sense range possible results including nature artefacts obtained using proactive message passing memory factor networks include images best worst solutions variety problems type factor ﬁrst reconstructing face images reconstructing music spectrograms.", "year": 2016}