{"title": "Verifying Properties of Binarized Deep Neural Networks", "tag": ["stat.ML", "cs.AI", "cs.CR", "cs.LG"], "abstract": "Understanding properties of deep neural networks is an important challenge in deep learning. In this paper, we take a step in this direction by proposing a rigorous way of verifying properties of a popular class of neural networks, Binarized Neural Networks, using the well-developed means of Boolean satisfiability. Our main contribution is a construction that creates a representation of a binarized neural network as a Boolean formula. Our encoding is the first exact Boolean representation of a deep neural network. Using this encoding, we leverage the power of modern SAT solvers along with a proposed counterexample-guided search procedure to verify various properties of these networks. A particular focus will be on the critical property of robustness to adversarial perturbations. For this property, our experimental results demonstrate that our approach scales to medium-size deep neural networks used in image classification tasks. To the best of our knowledge, this is the first work on verifying properties of deep neural networks using an exact Boolean encoding of the network.", "text": "understanding properties deep neural networks important challenge deep learning. paper take step direction proposing rigorous verifying properties popular class neural networks binarized neural networks using well-developed means boolean satisﬁability. main contribution construction creates representation binarized neural network boolean formula. encoding ﬁrst exact boolean representation deep neural network. using encoding leverage power modern solvers along proposed counterexample-guided search procedure verify various properties networks. particular focus critical property robustness adversarial perturbations. property experimental results demonstrate approach scales medium-size deep neural networks used image classiﬁcation tasks. best knowledge ﬁrst work verifying properties deep neural networks using exact boolean encoding network. deep neural networks become ubiquitous machine learning applications ranging computer vision speech recognition natural language processing. neural networks demonstrate excellent performance many practical problems often beating specialized algorithms problems rapid adoption industrial applications. wide adoption important questions arise regarding understanding neural networks robust networks perturbations inputs? critical choice architecture other? certain ordering transformations matter? recently line research understanding neural networks emerged look wide range questions interpretability neural networks verifying properties work focus important class neural networks binarized neural networks networks number important features useful resource constrained environments like ∗part work done author samsung reembedded devices mobile phones. firstly networks memory efﬁcient parameters primarily binary. secondly computationally efﬁcient activations binary enables specialized algorithms fast binary matrix multiplication. networks shown achieve performance comparable traditional deep networks ﬂoating point precision several standard datasets recently bnns deployed various embedded applications ranging image classiﬁcation object detection goal work analyze properties binarized networks lens boolean satisﬁability main contribution procedure constructing encoding bnn. important feature encoding exact rely approximations network structure. means encoding allows investigate properties bnns studying similar properties domain mapping properties back neural network domain exact. best knowledge ﬁrst work verifying properties deep neural networks using exact boolean encoding network. construction exploit attributes bnn’s functional e.g. parameters networks binary structural e.g. modular structure networks. encodings could directly handled modern solvers show exploit structure encodings solve resulting formulas efﬁciently based idea counterexample-guided search deep neural networks shown susceptible crafted adversarial perturbations force misclassiﬁcation inputs. adversarial inputs used subvert fraud detection malware detection mislead autonomous navigation systems pose serious security risk therefore certiﬁably verifying robustness networks adversarial perturbation question paramount practical importance. using encoding certiﬁably establish whether problems verifying whether networks equivalent operations regularly arise dealing network alterations input preprocessing. again encodings check network equivalence produce instances networks differ operation. experimentally show techniques verify properties medium-sized bnns. section present experiments mnist dataset variants. example fully connected layers able prove absence adversarial perturbation perturbation considered images. notation. denote vectors column-wise fashion denoted boldface letters. vector denote elements. denote lp-norm boolean formula vars denote variables unsatisﬁable exists assignment variables vars evaluates formula true otherwise satisﬁable. boolean formulas. denote negation implies satisﬁable equivalent next deﬁne supervised image classiﬁcation problem focus given training images drawn unknown distribution represents size individual images. image associated label generated unknown function possible labels. training phase given labeled training goal learn neural network classiﬁer used inference given image drawn distribution classiﬁer predict true label. inference phase network ﬁxed. work study properties ﬁxed networks generated post training. properties neural networks section deﬁne several important properties neural networks ranging robustness properties related network structure. properties deﬁned section speciﬁc bnns consider general feedforward neural network denoted represent output input ground truth label adversarial network robustness. robustness important property guards network adversarial tampering outcome perturbing inputs. many techniques generating adversarial inputs e.g. therefore natural question understand susceptible network form adversarial perturbation informally network robust input small perturbations input lead misclassiﬁcation. formally deﬁnition feedforward neural network -robust input exist since ﬁxed vector lp-norms monotonically decrease increase network -robust also )-robust case bounds maximum perturbation applied entry especially interesting considered frequently literature adversarial attacks deep learning another popular deﬁnition robustness comes notion universal adversarial perturbation deﬁned intuitively universal adversarial perturbation leads misclassiﬁcation inputs images. absence perturbation captured following deﬁnition robustness. denote images. deﬁnition feedforward neural network -universally robust inputs exist network equivalence. similar robustness property commonly veriﬁed equivalence networks. informally networks equivalent networks generate outputs inputs drawn domain. denote domain inputs drawn. case images deﬁnition feedforward neural networks equivalent network alteration consider scenario part trained network altered form network. change could arise model reduction operations commonly performed deep networks make amenable execution resource-constrained devices could arise sources noise including adversarial corruption network. question whether altered network equivalent original network? augmentation reordering consider scenario input preprocessed supplied network. examples preprocessing include geometrical transformations blurring etc. transformation functions want know sensitive network order applications example given network network output block. suppose blocks blkd− placed consecutively output block input next block list. input blkk output. input ﬁrst block input network. assume input network vector integers true image classiﬁcation task images standard format. note integers encoded binary values using standard encoding. therefore keep notations uniform layers assuming inputs binary. output last layer passed output block obtain label. deﬁnition binarized neural network feedforward network composed blocks blkd− formally given input .)). section consider encodings bnns boolean formulae. encode building blocks network sat. encoding conjunction encodings blocks. binblkk boolean function encodes block input output xk+. similarly bino boolean function encodes takes input outputs entire input encoded boolean formula mixed integer linear program encoding start mixed integer linear programming encoding blocks. milp encoding ﬂavor encodings literature nonbinarized networks e.g. encoding blkk. encode layer blkk milp separately. matrix denote input blkk. linear transformation. ﬁrst layer consider linear layer following simplicity focus linear transformation applied fully connected layer note similar encoding also done convolution layer linear constraint case simply binarized neural network feedforward network weights activations predominantly binary convenient describe structure terms composition blocks layers rather individual layers. block consists collection linear non-linear transformations. blocks assembled sequentially form bnn. internal block. internal block performs collection transformations binary input vector outputs binary vector. input output binary vectors internal layers produce real-valued intermediate outputs. common construction internal composed three main operations linear transformation batch normalization binarization table presents formal deﬁnition transformations. ﬁrst step linear transformation input vector. linear transformation based fully connected layer convolutional layer. linear transformation followed scaling performed batch normalization operation finally binarization performed using sign function obtain binary output vector. figure shows blks connected sequentially. output block. output block produces classiﬁcation decision given image. consists layers ﬁrst layer applies linear transformation maps input vector integers output label class. followed argmax layer outputs index largest entry vector predicted label. network blocks. deep feedforward network formed assembling sequence internal blocks table structure internal outputs blocks stacked together form binarized neural network. training phase might additional hard tanh layer batch normalization. parameters layer whereas parameters layer. correspond mean standard deviation computed training phase. layer parameter free. perform encoding linear transformation. constraints xd−xd−. outputs introduce four boolean variables note always true. need consider non-diagonal variables. hence following constraints finally compute output neural network encoding section show real-valued variables milp encoding pure encoding smaller compared milp encoding. encoding blkk. input output blkk integer values functional variables hence substitute based respectively. consider left part equation. notice left-hand equation integer value binary vectors. hence right-hand side real value rounded safely. deﬁne following implication constraint encode idea similar deﬁne value eliminate constraints vi’s based sign γki’s. example consider internal block example following transformation following constraints since constraints implication constraints standard trick big-m formulation encode deﬁne example consider internal block inputs output. suppose following parameters first apply linear transformation second apply batch normalization finally apply binarization. case encoding block encoding linear layer straightforward case construction vector input vector construction output vector deﬁned above. example consider internal block example following transformation replace constraint reexpressed encoded using sequential counters. encoding consider constraint observe transformation developed internal block applied too. perform variable substitutions encoding next consider output block. milp encoding introduce boolean variables avoid using intermediate variables wi’s. translates denote rows matrix constraints reexpressed rounding sound integer entries. constraints reused variables integers. example consider output block example constraints follow transformation reexpressed encoding next step pure encoding. trivial encode blocks directly translate encoding deﬁned inefﬁcient resulting encoding large formula blow even logarithmic encoding integers. section exploit properties binarized neural networks construct compact encoding. ﬁrst recall deﬁnition sequential counters used encode cardinality constraints. sequential counters. consider cardinality constraint boolean variable constant. sequential counter encoding denote following formula encoding blkk. looking constraint handed similarly) note type constraint need encode constraint form moreover recall values ai’s binary binary simplify equation noting verifying adversarial robustness property need sort outputs need ensure label ordering. verifying universal adversarial robustness. simple extension create copy adversarial robustness property encoding bnnadxi verify least ρ-fraction inputs misclassiﬁed expressed verifying network equivalence. check equivalence networks need verify networks produce outputs valid inputs domain. assuming entry valid image lies range formulate network equivalence problem following decision problem variables counterexample-guided search procedure given formulas constructed previous section could directly solver verify desired properties. however resulting encodings could large large networks making hard tackle even state-of-the-art solvers. however take advantage modular structure network speedsearch procedure. particular idea counterexample-guided search extensively used formal veriﬁcation observe encoding follows modular structure network. illustrate approach simple network consisting internal blocks output block figure suppose want verify adversarial robustness property network network encoded conjunction boolean formulas encodes ﬁrst block network encodes rest network. therefore bnnadx) verx) cnfτ∞ binblk; verx) binblk bino cnf= generator veriﬁer share variables encode activations shared block block rest encoding constraints similar using sequential counters. ﬁnishes construction bino. example consider output block example following transformation replace xd−xd follows finally rewrite sequential counters encode expression. section encoding constructed previous section investigate properties bnns deﬁned section note since encoding exact allows investigate properties bnns domain. verifying adversarial robustness. need encode norm restriction perturbation adversarial condition. consider image perturbed image encoding scheme works norm constraint linear function input include ll∞-norms. discussed section common norm assumption l∞-norm focus here. norm constraint need ensure size images. note integer variable valid image integer values. therefore standard conversion linear constraints integers boolean variables clauses additionally constraint ensure valid image. this make natural assumption exists lower bound upper bound entries valid image within constraint ensure entries within adversarial constraint recall encoding contains integer output variable value predicted label. hence need encode boolean formula true label denote standard conversion constraint integers boolean formula. putting together checking robustness translates checking assignments formula bnnadx) deﬁned bnnadx) cnfτ∞ \u0001)∧bnn cnf= network. variables usually small compared variables formula. exploit property using craig interpolants build efﬁcient search procedure. deﬁnition boolean formulas shared variables formula unsatisﬁable. exists formula called interpolant vars vars vars unsatisﬁable general exist multiple interpolants given search procedure ﬁrst generates satisfying assignment variables generator formula gen. denote assignment check extend assignment satisfying assignment veriﬁer formula. found adversarial perturbation otherwise generate interpolant verx) extracting unsatisﬁable core verx) since none satisfying assignments extended valid satisfying assignment bnnadx) block redeﬁning repeat procedure. reducing solution space iteration algorithm terminates. formula becomes unsatisﬁable valid perturbation i.e. network \u0001-robust image wide spread success deep neural network models line research understanding neural networks emerged investigate wide range questions interpretability deep neural networks verifying properties encode neural network sigmoid activation functions boolean formula linear constraints spirit milp encoding. piecewise linear functions approximate non-linear activations. main issue approach scalability verify properties small networks e.g. hidden neurons. recently propose solver verify properties neural network. particular show robustness adversarial perturbations encoded system constraints. computational feasibility authors propose approximation constraint system linear program. approach hand exact encoding used verify property binarized neural network robustness. consider neural networks relu activation functions show simplex algorithm used deal them. important property method works system constraints directly rather approximation. however method tailored relu activation function. scalability also concern work relu introduces branching choice authors work networks relus total inputs. finally perform discretization real-valued inputs counterexample-guided abstraction reﬁnement procedure search adversarial perturbations. authors assume existence inverse functions layers perform abstraction reﬁnement step. method also utilizes counterexample-guided search. however consider blocks layers rather individual layers need abstraction reﬁnement step. also interpolants guide search procedure rather inverse functions. finally best knowledge milp encoding complete search procedure ﬁnding adversarial perturbations previously proposed literature. experiments baseline encoding efﬁcient encoding milp bnns. existing adversarial perturbation approaches incomplete search procedures either greedy-based e.g. fast gradient method tailored particular class non-linearity e.g. relu used torch machine learning framework train networks titan pascal gpu. trained networks mnist dataset modern variants mnist mnist-rot mnistback-image mnist-rot variation dataset digits rotated angle generated uniformly radians. mnist-back-image variation dataset patch black-and-white image used background digit image. experiments focused important problem checking adversarial robustness l∞-norm. next describe architecture. network consists four internal blocks block containing linear layer ﬁnal output block. linear layer ﬁrst block contains neurons linear layers blocks contain neurons. batch normalization binarization layers block detailed section also mentioned earlier additional hard tanh layer internal block used training. process inputs layers ﬁrst layers network perform binarization gray-scale inputs. simpliﬁes network architecture reduces search space. addition decreases accuracy original network less percent. accuracy resulting network mnist mnist-rot mnist-back-image datasets respectively. check adversarial robustness three datasets randomly picked images correctly classiﬁed network classes. resulted images dataset consider remaining experiments. reduce search space ﬁrst focus important pixels image deﬁned notion saliency particular ﬁrst perturb highly salient pixels image. cannot valid perturbation leads misclassiﬁcation among pixels search pixels image. experimented three different maximum perturbation values varying timeout seconds instance solve. compare three methods searching adversarial perturbations. ﬁrst method method used scip solver encoding problem second method pure method based proposed encoding used glucose solver solve resulting encoding third method used encoding speedingsearch utilized counterexample-guided search procedure described section core produced veriﬁer interpolant. call method ceg. average generated formulas contain million variables million clauses. largest instance contains million variables million clauses. table presents results different datasets. column show number instances solved corresponding method selected instances average time seconds solve instances solved context means either determine valid perturbation leading misclassiﬁcation image network concretely establish exists solution means network \u0001-robust image. clear methods outperform approach. method solves fewer instances compared across datasets. demonstrate effectiveness encoding compared encoding. comparing methods results mixed. mnist-rot mnistback-image datasets solves instances mnist dataset opposite situation. observe faster compared across datasets. generally noticed images mnistrot dataset easiest perturb whereas images mnist-back-image dataset hardest perturb. major advantage complete search procedure certify \u0001-robustness exists adversarial perturbation technique fool network images. guarantees cannot provided incomplete adversarial search algorithms previously considered literature last three columns table present number images mnist-backimage dataset network certiﬁably \u0001-robust found three methods. sat-based approaches outperform approach. increasing number images network \u0001-robust decreases adversary leverage larger value construct adversarial images. decrease also reﬂected table terms number solved instances decreases instances lack solution certiﬁed method cannot always accomplished within given time limit. examples perturbed images. table shows examples original successfully perturbed images generated method. images dataset table. seen pictures differences sometime small original perturbed images indistinguishable human eye. examples illustrate essential certiﬁable search procedure ensure network robust adversarial inputs. proposed exact boolean encoding binarized neural networks allows verify interesting properties networks robustness equivalence. proposed counterexample-guided search procedure leverages modular structure underlying network speed-up property veriﬁcation. experiments demonstrated feasibility approach mnist variant datasets. future work focus improving scalability proposed approach enable property veriﬁcation even larger neural networks exploiting structure formula. another interesting direction verify properties recurrent neural networks number additional structural properties e.g. repetitive blocks layers exploited reformulation search.", "year": 2017}