{"title": "Art of singular vectors and universal adversarial perturbations", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been attracting a lot of attention in recent studies. It has been shown that for many state of the art DNNs performing image classification there exist universal adversarial perturbations --- image-agnostic perturbations mere addition of which to natural images with high probability leads to their misclassification. In this work we propose a new algorithm for constructing such universal perturbations. Our approach is based on computing the so-called $(p, q)$-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns, and by using only 64 images we were able to construct universal perturbations with more than 60 \\% fooling rate on the dataset consisting of 50000 images. We also investigate a correlation between the maximal singular value of the Jacobian matrix and the fooling rate of the corresponding singular vector, and show that the constructed perturbations generalize across networks.", "text": "recent work moosavi shown exist universal adversarial perturbations imageagnostic perturbations cause natural images misclassiﬁed. constructed iterating dataset recomputing worst direction space images solving optimization problem related geometry decision boundary. universal adversarial perturbations exhibit many interesting properties universality across networks means perturbation constructed using perform relatively well dnns. present algorithm constructing universal perturbations based solving simple optimization problems correspond ﬁnding so-called singular vector jacobian matrices feature maps dnn. idea based observation since norm adversarial perturbations typically small perturbations non-linear maps computed reasonably well approximated jacobian matrix. -singular vector matrix deﬁned solution following optimization problem desire instead sufﬁcient multiply solution universal adversarial perturbations typically generated bound ∞-norm motivates usage general construction. obtain -singular vectors modiﬁcation standard power method adapted arbitrary p-norms. main contributions paper vulnerability deep neural networks adversarial attacks attracting attention recent studies. shown many state dnns performing image classiﬁcation exist universal adversarial perturbations image-agnostic perturbations mere addition natural images high probability leads misclassiﬁcation. work propose algorithm constructing universal perturbations. approach based computing socalled -singular vectors jacobian matrices hidden layers network. resulting perturbations present interesting visual patterns using images able construct universal perturbations fooling rate dataset consisting images. also investigate correlation maximal singular value jacobian matrix fooling rate corresponding singular vector show constructed perturbations generalize across networks. deep neural networks great success applied many practical problems computer vision audio text processing however discovered many state-of-the-art dnns vulnerable adversarial attacks based adding perturbation small magnitude image. perturbations carefully constructed order lead misclassiﬁcation perturbed image moremay attempt force speciﬁc predicted class opposed class different ground truth potential undesirable usage adversarial perturbations practical applications autonomous driving systems malware detec investigate correlation largest singular value fooling rate generated adversarial examples; suggests singular value used quantitative measure robustness given neural network principle incorporated regularizer dnns. analyze various properties computed adversarial perturbations generalization across networks dependence fooling rate number images used construction perturbation. conclude perturbations small magnitude order sufﬁciently perturb output hidden layer sufﬁcient maximize right-hand side seems reasonable suggest propagating network dramatically change predicted label homogeneity problem deﬁned sufﬁcient solve solution deﬁned multiplication called -singular vector computation general case well-known problem several cases e.g. algorithms ﬁnding exact solution problem known based ﬁnding element maximal absolute value matrix. however approach requires iterating elements matrix thus complexity matrix size typical size matrix appearing setting e.g. taking vgg- network output ﬁrst pooling layer batch size would requires roughly memory store makes algorithms completely impractical. order avoid problems switch iterative methods. instead evaluating storing full matrix matvec function function given input vector computes ordinary product without forming full matrix typically complexity. many applications deal extremely large matrices using matvec functions essentially mandatory. computing -singular vectors exists well-known power method algorithm originally developed boyd explain next section. also present modiﬁcation method order construct universal adversarial perturbations. applies vectors element-wise. usual also deﬁne then given initial condition apply following algorithm obtain solution case becomes familiar power method obtaining largest eigenvalue corresponding eigenvector applied matrix discussion applies ﬁnding adversarial perturbation instance produce universal adversarial perturbation would like maximize left-hand size uniformly across images dataset introduce optimization problem note compute matvec functions sufﬁces able compute individual matvec functions present algorithm next section assume matvec functions given. apply algorithm matrix obtaining stochastic power method note algorithm could principle change batch iterations power method compute more general singular vectors. however experiments discovered almost affect fooling rate generated universal perturbation. matrices involved algorithm typical dnns large formed explicitly. however using automatic differentiation available deep learning packages possible construct matvec functions evaluated fraction second. compute matvecs follow well-known approach based pearlmutter’s roperator could brieﬂy explained follows. suppose given operation gradx computes gradient scalar function respect vector variable point summarize approach generating universal perturbations. suppose dataset natural images ﬁxed deep neural network trained perform image classiﬁcation. ﬁrst choose ﬁxed random batch images specify hidden layer dnn. using algorithm construct matvec functions matrix deﬁned finally algorithm obtain perturbation rescale necessary. experiments chose computed singular vectors various layers vgg- vgg- resnet chosen smoothen optimization problem effectively serves replacement highest fooling rates reported also investigate values section batch size algorithm chosen used images construct adversarial perturbations. computed singular vectors presented ﬁgs. observe computed singular vectors look visually appealing present interesting visual patterns. possible interpretation patterns given note extremely similar images computed relation feature visualization. namely various layers googlenet images activate particular neuron computed. particular visualization layer convd corresponds edge detection looks surprisingly similar several adversarial perturbations. informally speaking might indicate adversarial perturbations constructed -singular vectors attack network ruining certain level image understanding particular ﬁrst layers correspond edge detection. partly supported fact approach used feature visualization based computing jacobian matrix hidden layer maximizing response ﬁxed neuron spirit related method. measure strongly -singular vector disturbs output hidden layer based constructed evaluate corresponding singular value. computed layers vgg- vgg- resnet. results given note general singular values layers resnet much smaller magnitude nets shown roughly correspond obtained fooling rates. convergence algorithm analyzed observe relatively number iterations required achieve good accuracy. particular evaluation matvec functions takes operations total complexity iterations small improvement compared exact algorithm. vectors. choose recall achieved multiplying computed singular vector factor results given tables using images allowed achieve fooling rate investigated networks dataset containing images different classes. means analyzing less dataset possible design strong universal adversarial attacks generalizing many unseen classes images. similar fooling rates reported required roughly images achieve examples images addition adversarial perturbation highest fooling rate vgg- given predicted classes various adversarial attacks reported tables note top- class probability images adversarial attack relatively cases might indicate images moved away decision boundary. test behavior computing top- probabilities several values ∞-norm adversarial perturbation. results given top- probability decreases signiﬁcantly becomes roughly equal top- probability. similar behavior noticed cases adversarial example failed fool top- probability still decreased signiﬁcantly. also interesting note adversarial attack indeed introduces many edges image supports claim made previous section. figure top- probabilities predicted vgg- w.r.t ∞-norm universal adversarial perturbation. tests image universal adversarial perturbation highest fooling rate table chosen. regularizer indicates current learning batch. plan analyze approach future work. finally investigate adversarial perturbations generalize across different networks. chosen adversarial perturbation highest fooling rate tables tested networks. results given table adversarial perturbations indeed doubly universal reasonably well generalizing architectures. surprisingly cases fooling rate adversarial perturbation constructed using network higher network’s adversarial perturbation. universality might explained fact deep neural networks independently speciﬁcs architecture indeed learn detect low-level patterns edges adding edge-like noise high chance ruin prediction. interesting note adversarial perturbation obtained using block pool layer vgg- efﬁcient correspondence interesting edgelike structure. dependence perturbation analysis chosen approximate however value used constructing adversarial perturbations subsection investigate choice affects fooling rate generated perturbations perturbations computed several different values presented corresponding creasing batch size signiﬁcantly affect fooling rate using images possible construct adversarial perturbations fooling rate. suggests singular vector constructed using stochastic power method reasonably well approximates solution general optimization problem appears higher singular value layer necessarily indicate higher fooling rate corresponding singular vector. however shown singular values various layers vgg- general larger vgg- vgg- general larger singular values resnet roughly correspondence maximal fooling rates obtained networks. moreover layers closer input seem produce better adversarial perturbations closer end. based observation hypothesize defend kind adversarial attack choose figure adversarial perturbations constructed various values presented images correspond values uniformly increasing block pool layer vgg- used. table generalization adversarial perturbations across networks. columns indicate adversarial perturbation computed rows indicate network tested. adversarial perturbations highest fooling rates tables chosen. fooling rates reported observe bigger values produce clear edge-like patterns reﬂected increase fooling rate. however maximal fooling rate seems achieved probably ’smoother’ substitute might important large scale problems. subsection perform comparison algorithm presented moosavi refer method. former python implementation https//github.com/lts/universal/. since main features method extremely number images used constructing perturbation decided compare fooling rates universal perturbations constructed using methods various batch sizes. results presented note method indeed captures universal attack vector relatively fast fooling rate stabilizes roughly fooling rate perturbation constructed method starts gradually increases images added. running time algorithm depends hidden layer use. example block pool layer vgg- running time iteration power method batch image roughly seconds numpy libraries). since running time iteration linearly depends batch size total running time could estimated seconds iterations. ﬁxing obtain total running time generate universal perturbation approximately fooling rate whole dataset roughly minute hardware setup running time algorithm batch size approximately minutes fooling rate roughly achieved. according approximately images required obtain fooling rates order figure dependence fooling rate number images used constructing universal perturbation. singularfool denotes method proposed current paper denotes algorithm presented block pool layer vgg- used. many different methods proposed perform adversarial attacks deep neural networks white setting fully available attacker. works especially relevant present paper. goodfellow propose fast gradient sign method based computing gradient loss function image taking sign adversarial perturbation. approach allows construct rather efﬁcient adversarial perturbations individual images seen particular case method. indeed take batch size equal loss function hidden layer sign gradx exactly solution problem number problem depend second work moosavi universal adversarial perturbations proposed. based sequential solution nonlinear optimization problems followed projection onto sphere iteratively computes ’worst’ possible direction towards decision boundary. optimization problems proposed current work simpler nature well-studied homogeneous property adversarial perturbation arbitrary norm obtained simply rescaling computed perturbation contrast algorithm work explored algorithm generating universal adversarial perturbations analyzed main properties generalization across networks dependence fooling rate various hyperparameters cernock`y khudanpur. extensions recurrent neural network lanacoustics speech signal processing guage model. ieee international conference pages ieee russakovsky deng krause satheesh huang karpathy khosla bernstein imagenet large scale visual recognition challenge. international journal computer vision szegedy sermanet reed anguelov erhan vanhoucke rabinovich. proceedings going deeper convolutions. ieee conference computer vision pattern recognition pages certain visual properties. showed using images single perturbation fooling network roughly cases constructed previous known approach required several thousand images obtain fooling rates. future work plan address relation feature visualization adversarial perturbations well analyzing defense approach discussed section", "year": 2017}