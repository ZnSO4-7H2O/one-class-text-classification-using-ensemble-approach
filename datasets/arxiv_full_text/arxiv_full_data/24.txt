{"title": "Universal Adversarial Perturbations Against Semantic Image Segmentation", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "cs.NE"], "abstract": "While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification, this work proposes attacks against semantic image segmentation: we present an approach for generating (universal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the segmentation mostly unchanged otherwise.", "text": "deep learning remarkably successful perceptual tasks also shown vulnerable adversarial perturbations input. perturbations denote noise added input generated speciﬁcally fool system quasi-imperceptible humans. severely even exist universal perturbations input-agnostic fool network majority inputs. recent work focused image classiﬁcation work proposes attacks semantic image segmentation present approach generating adversarial perturbations make network yield desired target segmentation output. show empirically exist barely perceptible universal noise patterns result nearly predicted segmentation arbitrary inputs. furthermore also show existence universal noise removes target class segmentation leaving segmentation mostly unchanged otherwise. deep learning signiﬁcant performance increases numerous visual perceptual tasks relatively robust random noise several studies found vulnerable adversarial perturbations adversarial attacks involve generating slightly perturbed versions input data fool classiﬁer stay almost imperceptible human eye. adversarial perturbations transfer different network architectures networks trained disjoint subsets data furthermore papernot showed adversarial examples network unknown architecture constructed training auxiliary network similar data exploiting transferability adversarial examples. figure upper shows image validation cityscapes prediction. lower shows image perturbed universal adversarial noise resulting prediction. note prediction would look similar images perturbed noise prior work adversarial examples focuses task image classiﬁcation. paper investigate effect adversarial attacks tasks involving localization component speciﬁcally semantic image segmentation. semantic image segmentation important methodology scene understanding used example automated driving video surveillance robotics. wide-spread applicability domains comes risk confronted adversary trying fool system. thus studying adversarial attacks semantic segmentation systems deployed physical world becomes important problem. adversarial attacks systems grounded physical world physically realizable inconspicuous prerequisite physical realizability perturbations depend speciﬁc input since input known advance perturbations deter ieee. personal material permitted. permission ieee must obtained uses current future media including reprinting/republishing material advertising promotional purposes creating collective works resale redistribution servers lists reuse copyrighted component work works. mined. work proposes method generating imageagnostic universal perturbations. universal perturbations proposed moosavi-dezfooli however extend idea task semantic image segmentation. leave prerequisites physical realizability detailed sharif future work. attack inconspicuous raise suspicion humans monitoring system requires system inputs modiﬁed subtly semantic image segmentation task also requires system output looks mostly human would expect given scene. adversary’s objective remove occurrences speciﬁc class attack maximally inconspicuous leaves prediction classes unchanged hides target class. present adversarial attack explicitly targets dynamic target segmentation scenario. inconspicuous attacks require target scenes mostly match human expects also present attack yielding static target segmentations. attack generates universal perturbations system output always essentially segmentation regardless input even input completely different scene main motivation experiment show fragile current approaches semantic segmentation confronted adversary. practice attacks could used scenarios static camera monitors scene would allow attacker always output segmentation background scene blend activity like e.g. burglars robbing jewelry shop. summarize main contributions follows show existence universal perturbations semantic image segmentation models. existence clear priori receptive ﬁelds different output elements largely overlap. thus perturbations cannot chosen independently output target. makes space adversarial perturbations semantic image segmentation presumably smaller recognition tasks like image classiﬁcation existence universal perturbations even surprising. propose efﬁcient methods generating universal perturbations. methods optimize perturbations training set. objective ﬁrst methods network yield ﬁxed target segmentation output. second method’s objective leave segmentation unchanged except removing designated target class. generalizable fool unseen validation images high probability. controlling capacity universal perturbations important achieving generalization small training sets. function parameters moreover input output ytrue corresponding ground-truth target. speciﬁcally scenario studied work denotes deep neural network image conditional probability encoded class probability vector ytrue one-hot encoding class. furthermore jcls ytrue) basic classiﬁcation loss cross-entropy. assume jcls differentiable respect respect semantic image segmentation denotes dense prediction task addresses what image? question assigning class label pixel image. recently deep learning based approaches become dominant best performing class methods task work focus ﬁrst prominent architectures fully convolutional network architecture fcn-s introduced long model fcn-s architecture roughly divided parts encoder part transforms given image resolution semantic representation decoder part increases localization accuracy yields ﬁnal semantic segmentation resolution input image. encoder part based pretrained imagenet fully connected layers reinterpreted convolutions making network fully convolutional. output last encoder layer interpreted low-resolution semantic representation image input upsampling layers recover high spatial resolution image successive bilinear-interpolation fcn-s additionally parallel paths merge higher-resolution less abstract layers upsampling path convolutions element-wise summation. enables network utilize features higher spatial resolution. example. objective adversary perturbation changes output model desired way. instance perturbation either make true class less likely designated target class likely. time adversary typically tries keep quasiimperceptible e.g. bounding ∞-norm. ﬁrst method generating adversarial examples proposed szegedy method able generate adversarial examples successfully many inputs networks also relatively slow computationally since involved l-bfgs-based optimization. since then several methods generating adversarial examples proposed. methods either maximize predicted probability true class minimize probability true class. goodfellow proposed non-iterative hence fast method computing adversarial perturbations. fast gradient-sign method deﬁnes adversarial perturbation direction image space yields highest increase linearized cost function ∞-norm. achieved performing step gradient sign’s direction step-width ytrue)) here hyper-parameter governing distance between original image adversarial image. fgsm targeted method. means adversary solely trying make predicted probability true class smaller. however control classes becomes probable. kurakin proposed extension fgsm proposed leastiterative targeted. likely method makes least likely class miny prediction model probable. principle speciﬁc leastlikely class yll; rather used arbitrary target class ytarget. method tries xadv maximizes predictive probability class ytarget achieved following iterative procedure denotes step size entries clipped iteration absolute value remains smaller throughout experiments. concurrent work adversarial examples extended semantic image segmentation object detection moreover training adversarial examples applied mammographic mass segmentation reduce overﬁtting dezfooli proposed method generating universal image-agnostic perturbations that added arbitrary data points fool deep nets large fraction images. method generating adversarial perturbations based adversarial attack method deepfool deepfool applied images images presented sequentially roundrobin manner deepfool. ﬁrst image deepfool identiﬁes standard image-dependent perturbation. subsequent images checked whether adding previous adversarial perturbation already fools classiﬁer; algorithm continues next image otherwise updates perturbation using deepfool also current image becomes adversarial. algorithm stops perturbation adversarial large fraction train set. authors show impressive results imagenet show perturbations adversarial large fraction test images method generating perturbation. potential shortcoming approach attack targeted i.e. adversary cannot control class classiﬁer shall assign adversarial example. moreover high-resolution images small train perturbation might overﬁt train generalize unseen test data since number tunable parameters proportional number pixels. thus high-resolution images need large train large computational budget. paper propose method overcomes shortcomings. section describe input xadv ytarget) becomes minimal i.e. adversary quasi-imperceptible changes input achieves desired target segmentation ytarget. start describing adversary choose ytarget. adversarial target generation principle adversary choose ytarget arbitrarily. crucially however adversary choose ytarget based ytrue since ground-truth also unknown adversary. instead adversary ypred basis assume adversary access objects e.g. pedestrians. secondary objective adversary perform attacks inconspicuous i.e. call attention humans monitoring system thus input must modiﬁed subtly. semantic image segmentation task however also required output system looks mostly human would expect given scene. achieved instance keeping ytarget similar possible ypred primary objective apply. deﬁne different ways generating target segmentation static target segmentation scenario adversary deﬁnes ﬁxed segmentation system’s prediction time step target subsequent time target segmentation steps ytarget suited instance situations adversary wants attack system based static camera wants hide suspicious activity certain time span started time dynamic target segmentation situations involving ego-motion static target segmentation suited would account changes scene caused contrast dynamic tarmovement camera. segmentation aims keeping network’s segmentation unchanged exception removing certain target classes. class objects adversary wants hide assign ytarget ytarget −i)+−j). latter corresponds ij∈ibg ﬁlling gaps left target segmentation removing elements predicted using nearest-neighbor heuristic. illustration adversarial target generation shown figure turning image-agnostic universal perturbations ﬁrst deﬁne adversary might choose image-dependent perturbation. given ytarget formulate objective adversary follows constraint limits adversarial example ∞-distance clipε implement constraint |ξij| clipping entries absolute value based this deﬁne targeted iterative adversary analogously least-likely method alternative formulation takes consideration primary objective secondary objective necessarily equally important achieved modiﬁed version loss including weighting parameter potentially competition different target pixels i.e. gradient loss might point opposite direction loss gradient standard classiﬁcation losses cross entropy general encourage target predictions already correct become conﬁdent reduces loss. necessarily desirable face competition different targets. reason loss gradients making correct predictions conﬁdent might counteract loss gradients would make wrong predictions correct. note issue exist adversaries targeted image classiﬁcation essentially single target output. address issue loss target pixels predicted desired target conﬁdence throughout paper section propose method generating universal adversarial perturbations context semantic segmentation. general setting generate training inputs dtrain ytargetk)}m ytargetk generated either methods presented section interested generalization test inputs optimized target ytarget exists. generalization inputs target exists required generating ytarget would require evaluating might possible test time real-time constraints. propose following extension attack presented section consists publicly available labeled images resolution pixels different cities. used pixel-wise annotations covering frequent classes. computational reasons images labels downsampled resolution pixels images bilinear interpolation labels nearest-neighbor approach used down-sampling. trained fcn-s network architecture semantic image segmentation whole training data achieved class-wise intersection-over-union validation data generated universal perturbations training data evaluated unseen validation data. noted otherwise used experiments. value also used moosavidezfooli corresponds level noise perceptible humans closer inspection. moreover number iterations static target segmentation cityscapes involve static scenes evaluated even challenging scenario namely output static target scene segmentation nothing common actual input scene present image. this selected arbitrary ground-truth segmentation cityscapes target. number training images corresponds number images cityscapes train set. moreover used unweighted loss periodic tiles i.e. illustration setting unseen validation images shown figure adversary achieved desired target segmentation nearly perfectly adding universal perturbation generated training images. even striking human original scene nothing common target scene remains clearly dominant. figure shows illustration generated universal perturbation perturbation highly structured local structure depends strongly target class. comparing perturbation static ytargetk) loss gradient averaged entire training data. potential issue approach overﬁtting training data would reduce generalization unseen inputs. overﬁtting actually likely given dimensionality input image thus highdimensional. adopt relatively simple regularization approach enforcing periodic spatial dimensions. speciﬁcally enforce constraints ξi+hj ξij+w predeﬁned spatial periodicity achieved optimizing proto-perturbation size tile full results gradient averaged training data tiles denoting number tiles dimension h]∧w]}. show section quality generated universal perturbation depends crucially size train set. method generating universal perturbations require ground-truth labels principle arbitrary large unlabeled data sets. nevertheless also investigate well universal perturbations generated small since large requires considerable computational resources also queries might increase monetary costs risk identiﬁed. figure inﬂuence universal adversarial perturbation static targets first unmodiﬁed cityscapes image. network prediction second unmodiﬁed cityscapes image. network prediction universal adversarial perturbation static adversarial target. universal adversarial perturbation static adversarial target adversarial example network prediction adversarial example network prediction please refer supplementary material additional higher-resolution illustrations. table success rate static target segmentation different values generated perturbations achieve nearly success rate unseen validation data training data. target segmentation fairly easy recognize structure target perturbation. instance manmade structures buildings fences correspond mostly horizontal vertical edges. property indicates adversarial attack might exploit robustness deep networks contrast changes. allows contrast noise structures stronger impact high-contrast structures actual image. table shows quantitative analysis success rate different values here deﬁne success rate categorical accuracy static target segmentation predicted segmentation network adversarial example. success rate training validation data nearly shows overﬁtting issue even high-dimensional perturbations. probably large number training images consistent target. unsurprisingly larger leads higher success rates. value strikes good balance high success rate quasi-imperceptible. dynamic target segmentation experiment focused adversary tries hide pedestrians image leaving segmentation unchanged otherwise. noted otherwise number training images periodic tile size motivated empirically illustration setting unseen validation images shown figure note qualitatively adversary succeeds removing nearly pedestrian pixels leaving background mostly unchanged. however closer inspection human would probably raise suspicion predicted segmentation looks relatively inhomogeneous. quantifying well adversary achieves primary objective hiding target class measure percentage pixels predicted pedestrians original input assigned classes adversarial example measure categorical accuracy background pixels dynamic adversarial target segmentation segmentation predicted network adversarial example quantiﬁes secondary objective inconspicuous preserving background. note comparison involve ground-truth segmentation; solely figure illustration universal perturbation static target segmentation best seen color. network’s prediction applied perturbation input strongly resembles static target segmentation figure inﬂuence universal adversarial perturbation dynamic targets first unmodiﬁed cityscapes image. network prediction second unmodiﬁed cityscapes image. network prediction universal adversarial perturbation dynamic adversarial target note adversary tailor universal perturbation target validation data; image solely shows ideal output. universal adversarial perturbation dynamic adversarial target adversarial example network prediction adversarial example network prediction please refer supplementary material additional higher-resolution illustrations. figure shows periodic tile-size number training images affects results adversary. general training images smaller tile-sizes increase number hidden pedestrian pixels. indicates failures hiding pedestrian pixels validation data mostly overﬁtting training data; fact adversary succeeds hiding nearly pedestrian pixels train combination number training images tile-size number background pixels preserved typically decreases inprimary secondary objective investigated table larger pedestrian pixels hidden since number pedestrian pixels considerably smaller number background pixels setting close e.g. presents reasonable trade-off. contrast unweighted loss fails since focuses much preserving background. generalizability tested effect universal perturbation generated cityscapes camvid average pixels transformed adversarial target static target segmentation. dynamic target segmentation average pedestrian pixels hidden background pixels preserved. thus perturbations generalize similar dataset small decrease performance. moreover evaluated fcn’s static target universal perturbation pspnet adding universal perturbation reduced pspnet’s predictions ground truth cityscapes however prediction adversarial target also summary universal perturbation generalizes networks untargeted attack targeted attack. proposed method generating universal adversarial perturbations change semantic segmentation images close arbitrary ways adversary achieve desired static target segmentation arbitrary input images nothing common. moreover adversary blend certain classes almost completely leaving rest class nearly unchanged. results emphasize necessity future work address machine learning become robust perturbations adversarial attacks detected especially important safetysecurity-critical applications. hand presented method directly allow adversarial attack physical world since requires adversary able precisely control digital representation scene. ﬁrst works shown adversarial attacks might extended physical world deceive face recognition systems practical attack against e.g. automated driving surveillance system presented yet. investigating whether practical attacks feasible presents important direction future work. furthermore investigating whether architectures semantic image segmentation less vulnerable adversarial perturbations equally important. figure evaluation universal perturbations dynamic target segmentation different tile-sizes number train images validation data training images improve generalization validation data. smaller tile sizes increase percentage pedestrian pixels removed cost preserving background less well. comparison image-dependent non-periodic perturbations also shown nearly perfectly achieve objectives. creased score hiding pedestrians. also case training images likely underﬁtting optimization issue could improved future alternative regularization methods sophisticated adversarial attacks. presented method tile-size achieves good trade-off used remaining experiments. table illustrates inﬂuence maximum noise level values clearly correspond underﬁtting regime adversary capable hiding pedestrian pixels train data. failures adversary hiding pedestrian pixels validation data mostly overﬁtting additional capacity perturbation used adversary preserve background even better help reducing overﬁtting. inﬂuence parameter allows controlling trade-off", "year": 2017}