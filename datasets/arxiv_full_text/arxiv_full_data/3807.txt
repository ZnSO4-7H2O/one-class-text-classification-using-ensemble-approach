{"title": "Parsing using a grammar of word association vectors", "tag": ["cs.CL", "cs.NE"], "abstract": "This paper was was first drafted in 2001 as a formalization of the system described in U.S. patent U.S. 7,392,174. It describes a system for implementing a parser based on a kind of cross-product over vectors of contextually similar words. It is being published now in response to nascent interest in vector combination models of syntax and semantics. The method used aggressive substitution of contextually similar words and word groups to enable product vectors to stay in the same space as their operands and make entire sentences comparable syntactically, and potentially semantically. The vectors generated had sufficient representational strength to generate parse trees at least comparable with contemporary symbolic parsers.", "text": "paper ﬁrst drafted formalization system described u.s. patent u.s. describes system implementing parser based kind cross-product vectors contextually similar words. published response nascent interest vector combination models syntax semantics. method used aggressive substitution contextually similar words word groups enable product vectors stay space operands make entire sentences comparable syntactically potentially semantically. vectors generated suﬃcient representational strength generate parse trees least comparable contemporary symbolic parsers. basic intuition underlying formalisation word characterised enumerating words occur similar contexts e.g. menno zaanen attributes harris notion interchangeability intuition implicitly present almost linguistic classiﬁcations. e.g. deﬁnition part speech determiners based observation group words exhibit similar behaviour range contexts strong theme linguistics many years idiosyncrasy language defying classiﬁcation. example appears date halliday strong powerful used interchangeably many contexts context viz. strong tea/*powerful tea. pawley syder present long lists examples defy codiﬁcation rules title native-like selection characterize puzzle linguistics. fact applied linguistics literature full observations language appears rules without exceptions indeed senses consist exceptions only. nattinger weinert lewis provide sampling reaching extreme lewis’s lexical approach. traditionally assumed information words generalized using word classes... however never clearly shown unrestricted language indeed structured accordance assumption. lack proof noted dagan shown fact could characterize central dilemma linguistics last years. behind major divisions characterized subject time. strongly abstract language structure observation contradicts itself. shown long time ﬁrst category great success contrastive method phoneme. crisis catalysed polarizing ﬁgure century linguistic theory noam chomsky. chomsky brought fresh perspective many issues linguistics largely forgotten fair attack phoneme important time. e.g. hill contrastive methods along lines attributed harris standard method linguistic analysis. phoneme major success method. chomsky pointed contrastive methods contradictory results linguistics shattered. newmeyer says discussing chomsky’s logical basis linguistic theory part discussion phonology ’lblt’ directed towards showing conditions supposed deﬁne phonemic representation inconsistent incoherent cases absurd analyses others. abstracting language categories observation inconsistent incoherent analyses. equivalently elements language combine non-linearly. lamb says chomsky shown case phoneme seriously contested categories. contrastive method never beyond phoneme. instead linguistics moved beyond abstraction categories contrastive analysis fragmenting schools hand continued respect contrastive analysis rejected structure meaningful parameter like functionalism hand continued respect language structure rejected idea could learned observations generativism. generativism hypothesized contradictions observed tried abstract categories observations relevant system language all. still assumed structure relevant objective learnable categories observed contradict structure assumed innate. unfortunately split ﬁeld issue contradictions learnable categories largely forgotten unavailable outside linguistics seeking insights. rapid developments computing made large amounts data available non-linguists started trying learn non-linguists main reiterate theoretical linguistics already split schools assumed language structure unlearnable irrelevant making issue disappear. ironic reason non-linguists perhaps felt linguistics irrelevant ignored turn address issue interested viz. learning categories observable data. motivation seeking model grammar vector product word similarity vectors forcefully abstractions observations shown lead contradictions case lamb non-linearity. actually rare point agreement theoretical linguistics interpretations varied universal grammar functionalism cognitivism. unfortunately rare point agreement linguistics widely ignored. notably appears ignored ﬁeld machine learning. might seen render entire ﬁeld grammatical induction moot. learned observations must innate. functionalists concluded irrelevant. unlearnable need mean innate irrelevant. simply mean generalizations ad-hoc. better understanding complexity formal systems now. unlearnable random chaitin) even chaotic systems starting seem like norm. author aware kinds complexity issues ﬁrst suggested least context computational solutions earlier discussion motivations surrounding parser freeman possible similar ideas linguistics notably hopper’s emergent grammar notion emergence pregnant one. intended standard sense origins genealogy historical question ’how’ grammar came ’is’ instead takes adjective emergent seriously continual movement towards structure postponement ’deferral’ structure view structure always provisional always negotiable fact epiphenomenal least much eﬀect cause. possibly lamb’s non-linearity same. suggested paper ad-hoc character means regarded thing. rather hints greater richness structure available language. according method paper then order prevent relevant information lost task modelling natural language need concept overlapping classes constructed ad-hoc opposed pre-deﬁned. formally need deﬁne things actually context similar contexts means. simple initial approximation former question deﬁne context word word pairs least occurence triplet available corpus. deﬁnitions formulated above class word belongs deﬁned similarity values words below. convenience limit include words similar beyond threshold somewhat {somewhat significantly. slightly. considerably. substantially. far. a&lot. more&and. one&or. much. becoming. likely&to&be. a&little. considered. nothing. rather. so&much. relatively. something. certainly. quite. generally. getting. still. therefore. done. often. of&course. even. very. always. a&very. once. further. little. less. really....} extend formalization estimate representations potentially unobserved sequences words. potentially unobserved sequences fact observed however seek extend representation absence observation. essentially seek represent syntax specifying combinations words acceptable language. call potentially unobserved sequences trees formula non-associative. diﬀerent orders give diﬀerent products product structure like tree. seen advantage method predicts phrase structure. vector formulation vocabulary words word sequences. function pair elements another entry simply pair entry pair together observed occur together means entry exists i.e. observed occur together.) formally extension word class function binary trees deﬁned recursively word reduces binary tree consisting subtrees deﬁne components vector representing combination trees pair words components operand trees observed occur together components observed combination contribute component combination operand trees note common mutual information words occurring sequence used case scale signiﬁcance observed pairing. regard choice exhaustive exclusive. much similarity measure described section above many scalar association measures might considered. assumed stage association grammatical grouping would generate greatest number greatest concentration grammatically similar elements order association branching best phrase structure tree selected basis. broadly speaking order association words greatest system described section vector parser system originally formulated picked time although tests done comparing performance quantitatively task parsing chinese hope present results later paper. general parser performed level comparable contemporary symbolic parser greatly better. seems apparent classes abstracted runtime formulation important details lost word similarity assessed. class similarity words typically does vary contexts. needed vectors contexts similarity based context. fruitful direction investigation. however method generating product vectors aggressive substitution contextually similar words word groups above) quite successful. enables product vectors stay space operands makes entire sentences comparable syntactically potentially semantically. vectors generated suﬃcient representational strength generate parse trees least comparable contemporary symbolic parsers. context contemporary explorations vector combination semantic representation approach aggressive substitution contextually similar words word groups recommended possible solution many thanks wojciech skut formalism paper recent comments. formalization made ﬁrst realize ad-hoc arrangements examples model proposing could seen kind vector product.", "year": 2014}