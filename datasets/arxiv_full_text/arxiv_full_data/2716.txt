{"title": "Graph Classification via Deep Learning with Virtual Nodes", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Learning representation for graph classification turns a variable-size graph into a fixed-size vector (or matrix). Such a representation works nicely with algebraic manipulations. Here we introduce a simple method to augment an attributed graph with a virtual node that is bidirectionally connected to all existing nodes. The virtual node represents the latent aspects of the graph, which are not immediately available from the attributes and local connectivity structures. The expanded graph is then put through any node representation method. The representation of the virtual node is then the representation of the entire graph. In this paper, we use the recently introduced Column Network for the expanded graph, resulting in a new end-to-end graph classification model dubbed Virtual Column Network (VCN). The model is validated on two tasks: (i) predicting bio-activity of chemical compounds, and (ii) finding software vulnerability from source code. Results demonstrate that VCN is competitive against well-established rivals.", "text": "learning representation graph classiﬁcation turns variable-size graph ﬁxed-size vector representation works nicely algebraic manipulations. introduce simple method augment attributed graph virtual node bidirectionally connected existing nodes. virtual node represents latent aspects graph immediately available attributes local connectivity structures. expanded graph node representation method. representation virtual node representation entire graph. paper recently introduced column network expanded graph resulting end-to-end graph classiﬁcation model dubbed virtual column network model validated tasks predicting bioactivity chemical compounds ﬁnding software vulnerability source code. results demonstrate competitive wellestablished rivals. intro deep learning achieved record-breaking successes domains regular grid chain-like structures however many most real-world structured problems best modelled using graphs irregular connectivity. include example chemical compounds proteins rnas function calls software brain activity networks social networks. canonical task graph classiﬁcation assigning class labels graph instance study generic class known attributed graphs whose nodes attributes edges multi-typed directed. efﬁciently learn distributed representation graph turns variable-size graphs ﬁxed-size vectors matrices. representation would beneﬁt greatly powerful pool data manipulation tools. rules traditional approaches graph kernels graph feature engiseveral recent neural networks deﬁned graphs graph neural network diffusion-cnn column network start node representations taking account neighborhood structures typically convolution and/or recurrent operations. node representations aggregated graph representation. akin representing document ﬁrst embedding words vectors combining conjecture better learn graph representation directly simultaneously node representation. main idea augment original graph virtual node represent latent aspects graph. virtual node bidirectionally connected existing real nodes. virtual node assumes either empty attributes auxiliary information readily available original graph. augmented-graph passed existing graph neural networks computing node representation. graph representation vector representation virtual node. concreteness materialize idea virtual node using column network architecture node classiﬁcation. differentiable classiﬁer network end-to-end solution graph classiﬁcation name virtual column network validate applications predicting bio-activity chemical compounds assessing vulnerability software code results promising. section present virtual column network realization idea virtual node graph classiﬁcation using recent node representation method known column network applicable graphs multi-typed edges attributed nodes. plays role usual input step rnn. without this model becomes highway network parameters typing also suggests alternative reset gate neighbor type computing candidate becomes standard convolution operation neighbor indexed specifically virtual columns column networks compact effective integrating long-range dependencies nodes however designed node classiﬁcation. graph classiﬁcation need pool node states. simple taking average introduce integrating node states. particular augment virtual node original graph bidirectionally connecting real nodes corresponding virtual column hence performs functions integrating state information node columns distributing consensus graph-level information node columns. virtual column optionally take graph-level information input virtual column high-order implicit dependencies distributed much faster taking steps independent graph size. virtual column node columns computed follow figure virtual column network column network virtual node. graph nodes augmented virtual node connecting nodes. model graph vector graph descriptors node attributes graph label. boxes hidden layers. column network given attributed graph column network recurrent neural architecture deﬁned network multiple interacting recurrent nets responsible node. fig. illustrates model columns create column network. neural connections column another reﬂect graph edge corresponding nodes. concretely column node step updated using information neighbors step follows state virtual column step iterative estimation continue dimensional change requires projection onto different state space without input nodes. later part column essentially highway network parameters typing remark similar semi-restricted boltzmann machines real columns connections handle short-range dependencies virtual column enables long-range dependencies. recurrent structure akin mean-ﬁeld unrolled steps. candidate state nonlinear transformation parameters tied across steps. columns homogeneous remove sub-index smoothing gate parameterized typically sigmoid function. features atom degree number atoms attached. also make bond features bond type binary value indicating bond ring. results table reports results measured datasets. proposed virtual column network competitive best feature engineering techniques experiment settings experiments proposed highway network layers relu activation function. dropout applied after column layers. dataset divided training validation test sets. validation used early stopping tuning hyper-parameters. number hidden layers mini-batch search hidden dimensions virtual columns hidden dimensions node columns learning rate optimizers adam rmsprop. training starts learning rate divided model cannot better result validation set. learning terminated times halving epochs. best setting chosen validation result test reported. baselines random forest gradient boosting machine. method reads input vector representation graphs. sec. sec. describe feature extraction methods baselines. bioassay activity prediction neural fingerprint baseline. bioassay activity prediction datasets ﬁrst experiments uses largest bioassay activity tests collected pubchem website lung cancer leukemia yeast anticancer. bioassay test contains records activities chemical compounds. compound represented graph nodes atoms edges bonds them. chose common activities classiﬁcation active inactive. statistics data reported table datasets unbalanced therefore inactive compounds randomly removed lung cancer leukemia datasets graphs yeast anticancer dataset graphs. feature extraction rdkit toolkit molecular feature extraction rdkit computes ﬁxed-dimensional feature vectors molecules so-called circular ﬁngerprint. vectors used inputs baselines. dimension ﬁngerprint features code classiﬁcation dataset dataset contains java projects consists number source code ﬁles. source java class list attribute declarations number methods. class represented graph graph-level features attribute declarations nodes methods edges method call. task predict source vulnerable. dataset pre-processed removing replicated ﬁles different versions projects. remains samples total positive ones. application chemical compound classiﬁcation bears similarity work graph embedding also collected node embedding layer reﬁned iteratively bottom layers. however treatment principled widely applicable multi-typed edges. discussion proposed simple solution learning representation graph adding virtual node existing graph. expanded graph passed node representation method representation virtual node graph’s. virtual node coupled recent node representation method known column network results graph classiﬁcation method called virtual column network demonstrate power tasks classiﬁcation bio-activity chemical compounds given cancer; detecting software vulnerability source code. overall automatic representation learning powerful state-of-the feature engineering. rooms open investigations. first multiple virtual nodes instead one.the graph embedded matrix whose columns vector representation virtual nodes. beneﬁcial several ways. multitask learning virtual node used task tasks share node representations. graphs tight subgraph structures virtual node target subgraph. second node representation architectures beside column networks also applicable deriving graph representation including gated graph sequence neural network graph neural network diffusion-cnn acknowledgments paper partly supported samsung program titled predicting hazardous software components using deep learning telstra-deakin data machine learning. michael bronstein joan bruna yann lecun arthur szlam pierre vandergheynst. geometric deep learning going beyond euclidean data. arxiv preprint arxiv. choetkiertikul khanh truyen tran aditya ghose john grundy. predicting delivery capability iterative software development. ieee transactions software engineering feature extraction methods attribute declarations java classes considered sequences tokens representation learned language modeling using lstm noise contrastive estimation feature vector sequence mean hidden states outputted lstm. step sequence represented feature vector units. baselines feature vector java class mean feature vectors methods attribute declarations. model feature vector attribute declarations input virtual column. results fig. reports performance code classiﬁcation task f-score. outperforms baselines measures. related work sizable rise learning graph representation past years number works derive shallow embedding methods nodevec subgraphvec possibly inspired success embedding linear-chain text deep spectral methods introduced graphs given adjacency matrix whereas allow arbitrary graph structures graph. several methods extend convolutional operations irregular local neighborhoods recurrent nets also employed along random walk node david duvenaud dougal maclaurin jorge iparraguirre rafael bombarell timothy hirzel alán aspuru-guzik ryan adams. convolutional networks graphs learning molecular ﬁngerprints. advances neural information processing systems pages andriy mnih whye teh. fast simple algorithm training neural probabilistic language models. proceedings international conference machine learning. citeseer mathias niepert mohamed ahmed konstantin kutzkov. learning convolutional neural networks graphs. proceedings annual international conference machine learning. michael schlichtkrull thomas kipf peter bloem rianne berg ivan titov welling. modeling relational data graph convolutional networks. arxiv preprint arxiv. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research", "year": 2017}