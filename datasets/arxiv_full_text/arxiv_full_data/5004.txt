{"title": "Gigamachine: incremental machine learning on desktop computers", "tag": ["cs.AI", "cs.LG"], "abstract": "We present a concrete design for Solomonoff's incremental machine learning system suitable for desktop computers. We use R5RS Scheme and its standard library with a few omissions as the reference machine. We introduce a Levin Search variant based on a stochastic Context Free Grammar together with new update algorithms that use the same grammar as a guiding probability distribution for incremental machine learning. The updates include adjusting production probabilities, re-using previous solutions, learning programming idioms and discovery of frequent subprograms. The issues of extending the a priori probability distribution and bootstrapping are discussed. We have implemented a good portion of the proposed algorithms. Experiments with toy problems show that the update algorithms work as expected.", "text": "present concrete design solomonoff’s incremental machine learning system suitable desktop computers. scheme standard library omissions reference machine. introduce levin search variant based stochastic context free grammar together update algorithms grammar guiding probability distribution incremental machine learning. updates include adjusting production probabilities re-using previous solutions learning programming idioms discovery frequent subprograms. issues extending priori probability distribution bootstrapping discussed. implemented good portion proposed algorithms. experiments problems show update algorithms work expected. artiﬁcial general intelligence ﬁeld received considerable attention researchers last decade computing capacity marches towards human-scale. many interesting theoretical proposals forward practical general-purpose programs demonstrated currently understand requirements system much better used therefore believe time start constructing system least prototype based solid theoretical foundation exists today. anticipate tedious work writing general purpose programs solve various theoretical problems deal practical details. would best expose problems early gigamachine initial implementation system o’caml language goal building solomonoff calls phase machine plans basis quite powerful incremental machine learning system work remains implement full system present algorithms implementation demonstrate issues building realistic system. thus report ongoing research share experience designing system. space restrictions cannot give much background proceed directly contributions. argues choice reference machine introduces necessary bias learning system looking ultimate machine herring. program-size efﬁcient forth language employed great effect. publications solomonoff unimplemented reference machine called introduced. language functional programming language adopts preﬁx notation expressions. solomonoff also suggests adding primitives etc. speciﬁc application must choose universal computer many suitable primitives possible would take time system discover primitives training sequence would longer accommodate discovery primitives. general purpose machine learning system need general purpose programming system deal large variety data structures makes possible write sophisticated programs kind. forth yielded rather impressive results chosen scheme grounds simple general purpose high-level programming language. certain features also make desirable. scheme invented lewis steele gerald sussman improvement lisp statically scoped implementations required proper tail recursion; scheme deﬁned precisely standards document contains reasonably sized standard library. think scheme major handicaps compared small syntactic differences important language features are. scheme include functional language addition imperative features. highly orthogonal built around symbolic expressions. syntax-semantics mapping quite regular hence detecting patterns syntax helps detecting patterns semantics. efﬁcient interpreters scheme modiﬁed easily uses static scopes mean variable access fast uncomplicated. scheme quite high level basic data structures like lists vectors strings. work variety numbers like integers rationals think scheme ultimate reference machine formed good platform testing ideas incremental learning. implemented syntax omissions. elected exclude syntax quasi-quotations syntax transformation syntax advanced macro syntax would complicate grammar based guidance logic advanced feature used complex programs expect generate gigamachine. simpliﬁcations deemed necessary. parts syntax semantics expressed different ways instance semantics. case used quote. case number literals generate alternative radii base standard library implemented except input/output system interface forming adequate basis generating simple programs. special nonterminal called standard-procedure added grammar produces standard library procedure calls correct number arguments. standard-procedure added alternative production procedure-call head scheme standard grammar. useful libraries common across scheme implementations easily added present system. many systems variant extension levin search used ﬁnding solutions. solomonoff’s incremental machine learning also uses levin search basic search algorithm solutions system take advantage stochastic grammar based guiding probability mass function search procedure well. ﬁrst describe generalized version levin search build algorithm give generalized levin search algorithm similar described inputs follows. universal computer executes program duration returns output. grammar deﬁnes language valid programs language) priori programs testprog algorithm takes candidate program forms test program program coding trueval value true language start global time limit equal start inﬁnite loop. within iteration allocate time programs proportion priori probabilities. choose candidate programs among language allocated time program greater equal program construct test program using algorithm testprog. execute test program time limit thus total time running testing candidate programs exceed test successful search procedure returns otherwise candidate programs tested iteration time limit doubled search continues. extend levin search procedure work stochastic assigns probabilities sentence language. this need things ﬁrst generation logic individual sentences second search strategy enumerate sentences meet condition line algorithm present system leftmost derivation generate sentence intermediate steps thus left-sentential forms calculation priori probability sentence depends obvious fact derivation productions applied order start symbol probability sentence naturally q≤i≤n note productions derivation conditionally independent. makes much easier calculate probabilities sentential forms limits expressive power probability distribution. relevant optimization starting absolute start symbol arbitrary sentential form. helps ﬁxing known parts program searched done implementation. make depth-limit form probability horizon threshold impose corresponding smallest probability sentence willing generate. probability horizon calculated tq/t ensures waste time generating programs run. depthﬁrst search implemented expanding leftmost nonterminal sentential form pruning sentential forms priori probabilities smaller probability horizon sorting expanded sentential forms order decreasing priori probability recursively searching list sentences obtained fashion. recursion implemented stack plain recursion. stack implementation turned faster. problem depth-ﬁrst search order search procedure terminate program smaller priori probability best solution current amend shortcoming tried best-ﬁrst search strategy maintains global priority queue expansion leftmost non-terminal. since every nonterminal eventually expanded indeed maintain global order. however maintaining best-ﬁrst search high memory cost therefore implementation disabled strategy quickly exhausts available memory. solution tried hybrid search strategy. hybrid search ﬁrst proposed time sharing breadth-ﬁrst depth-ﬁrst search simpler program probability model. using stochastic cfg’s many non-terminals expand sentential form therefore strict breadth-ﬁrst search incur high memory cost. note lazily expanding nodes memory cost problem. hybrid search strategy however used either breadth-ﬁrst best-ﬁrst alongside depth-ﬁrst search. improved hybrid search strategy memory aware; runs best-ﬁrst breadth-ﬁrst search queue structure reaches certain size switches dynamically depth-ﬁrst search. riemann zeta function. used zeta distribution used pre-computed table generate ﬁxed integer zeta distribution empirical support preferred. upper bound present avoid many programs equal constants. expression syntax handles larger integers instance string literals generated sequence characters grammar default sequence rules seemed reasonable. need come specialized generation course zipf-distribution appropriate purpose. implement special distributions kinds rules deﬁned second kind non-terminal call non-terminal procedure. non-terminal procedure implementation function generates list sentential forms associated probabilities used sequential enumeration algorithm. below also used implement special kind context-sensitivity. maintain static environment leftmost derivation sentence passed along right possibly modiﬁcation. environment also passed along non-terminal procedures. initially environment includes input parameters function searched. variable deﬁned robotic variable name generated form varinteger non-terminal integer sampled zeta distribution. pre-computed zeta distribution upper bound integer literal generation. drastic limitation limits temporary variables within scope. variable reference generated environment present choose among available variable names uniform distribution. respect nesting scopes. expanding deﬁnition variables deﬁned deﬁnition must available enclosing scope. requires generator aware scope beginning ending. within scope deﬁnitions excluding mutually recursive deﬁnitions available point deﬁnition scope end. robotic variable deﬁnitions seen define lambda let* letrec blocks. mutually recursive definitions letrec blocks handled different ﬁrst generating robotic variables generating rest bindings. current implementation scope begin/end awareness since unlikely generate long programs. propagates environment modiﬁcations left right disregarding nesting scopes. make aware without rewriting everything might annotate rules deﬁne scopes special nonterminal symbols scope-begin scope-end. instead passing single environment partial derivation pass stack environments. scope-begin seen current stack pushed scope-end seen stack popped. critical part design updating stochastic discovered solutions training sequence probable searching subsequent problems. propose four update algorithms work tandem. mentions indeed used extrapolate programs however think practical incremental machine learning. fact adopted recent variants family compression programs purpose extrapolated programs mostly syntactically incorrect. devise variant useful extrapolating programs would make easier. update algorithms propose powerful. simplest kind update modifying probabilities solutions added solution corpus. this however search algorithm must supply derivation solution solution must parsed using grammar. then probability production solution corpus easily calculated ratio frequency productions solution corpus frequency productions corpus head non-terminal procedures naturally excluded update variant. however cannot simply write probabilities calculated initial probabilities initially solutions probabilities zero. exponential smoothing solve problem. initial probability probability corpus smoothing factor. used smoothing factor application smoothing similar problem. methods like laplace’s rule used avoid zero probabilities modifying production probabilities useful idea cannot much information guiding total amount information limited number bits probability multiplied number probabilities. arbitrary precision ﬂoating point numbers seem likely distinguishing ﬁnely among number alternative productions nonterminal result great improvements. then seems need augment grammar productions. idea thought tried convert occurrences non-terminal multiple non-terminals different probabilities result learning. collection non-terminals replicated well. however course replication also limited seem overcome structural limitation modifying probabilities. course training sequence solutions incorporated full adding solutions grammar. case scheme could many possible implementations. simplest found solutions library scheme interpreter hook non-terminal previous-solution grammar extend previous-procedure syntax call solution. assume syntax provided problem deﬁnition. solutions follows solution among previous solutions given probability hope solution re-used soon probabilities productions previous-solution normalized currently impossible difﬁcult solutions scheme interpreter case solutions added define blocks beginning program produced. scheme orthogonal language allow make deﬁnitions almost anywhere. however time penalty many solutions incorporated repeatedly parsed interpreter lsearch. solve problem make search scalable hook called solution-corpus grammar deﬁnition achieved similar previous-solution. however then probability deﬁning using previous solution greatly decrease. assume previous solution deﬁned probability called probability since grammar condition calling previous solution basis deﬁnition probability correct p.p; time logic generate semantically incorrect invocations past solutions. undesirable situation non-terminal procedure deﬁnition production particular solution stores solution environment already stores variable names deﬁnes solution name variable production calls previous solution selects among solutions environment uniform probability. solution present return production probability avoid generation redundant programs. since complex solution implementers preferable solutions interpreter format executed efﬁciently. many scheme interpreters compiled data structures interpreter ﬁrst converts parsing program evaluator designed work structures. programmers learn concrete solutions problems also learn abstract programs program schemas. formalize learn sentential forms. extract appropriate sentential forms grammar algorithm modiﬁes production probabilities. ately contain appropriate sentential forms. appropriate sentential forms obtained deriving relevant start symbol like body expression particular level border derivation tree solution body top-level expression. thus sub-expressions remain unexpanded. implement leftmost derivation scheme ours either parse found solutions change derivation logic derivation tree constructed search. also possible construct derivation tree leftmost derivation. hoped system able learn programming idioms like recursion patterns ways loops kind update. instance assume discovered kind integer recursion problem )))). then sentential forms intermediate levels derivation tree body solution would abstract sub-expressions resulting instance uinteger- pruned level leaf derivation tree determined hand made derivation tree come handy dealing similar recursions. several sentential forms learnt single solution fashion corresponding different syntactic abstractions algorithm re-using previous solutions invoked grammar. otherwise would teach kind recursion higherorder functions mining solution corpus would enhance guiding probability distribution. frequent sub-programs solution corpus i.e. sub-programs occur frequency given support threshold added alternative productions commonly occurring non-terminal expression scheme grammar. instance solution corpus contains several subprograms frequent sub-program mining would discover alternative expression scheme grammar. detailed update algorithm seems reasonable beneﬁt well-developed ﬁeld data mining. order encode useful information priori probability distribution must reﬂect human programmer knows writes program. richness scheme language requires solve problems like avoiding unbound references make even simplest program searches feasible therefore important encode much priori knowledge programming possible system. among things programmer knows following. syntax programming language sometimes imperfectly. system knows syntax perfectly make syntactic mistakes. semantics programming language imperfectly. system knows little writing semantically correct programs often generates incorrect programs. need semantic checks enhance that. system know referential semantics programs program. useful search procedure informed semantics. human programmers semantic information accelerate writing programs instance using proper types. running time space complexity program imperfectly. computer science programmer knows fewer mistakes make. argued programmers partial knowledge halting problem know many programs loop inﬁnitely avoid writing them. also learn programs seem loop indeﬁnitely. post conditions. sometimes programs wellspeciﬁed programmer understands conditions assumed prior running program after program ﬁnishes. incremental machine learning capabilities phase machine used calculate conditional distributions necessary phase machine take off. current implementation short programs despite future improvements processing speed difﬁculty ﬁnding required sort programs without huge training sequence approaches programs closely little chance rewriting unless current implementation further developed. currently made implementation work problems. initially solved problems inverting mathematical functions identity function division square-root functions test lsearch. developed simple training sequence composed series problems. problem sequence input output pairs incremental operator induction operator induction similar oops ﬁrst ﬁnds solution ﬁrst pair then ﬁrst second pairs ﬁrst three forth. partial complete solution problem stochastic update applied. ﬁrst training sequence identity function square function addition variables function test argument zero example pairs fourth power number example pairs boolean nand functions example pairs each factorial function pairs inputs factorial function took ﬁfth example interrupted feasible desktop machine partial solution ﬁrst four pairs. however observed update algorithms implemented work gracefully. search time later problems tend reduce compared ﬁrst. since extend grammar sometimes slight slowdown experienced would amortized later problems depending training sequence. instance problem took trials problem took solution logical functions took longer previous problems eventually time reductions them. nand solution took trials next problem took trials speedup scheme cycles. previous solutions re-used aggressively. problem solved ﬁrst example incorporating next example takes trials corresponding speedup. solution re-uses solution takes trials faster solving itself. update algorithms help machine learn something semantics. searching seventh iteration system reported error rate evaluation candidate programs. last problem error rate dropped iteration although consider semantics update algorithms. error reductions seen also code re-use disabled. described stochastic based incremental machine learning system targeting desktop computers detail. adapted scheme reference universal computer system. stochastic used sequential lsearch calculate priori probabilities generate programs efﬁciently avoiding syntactically incorrect programs. derive sentences using leftmost derivation. probability horizon limit depth depth-ﬁrst search also propose using best-ﬁrst search memory-aware hybrid search. specialized productions number literals variable bindings variable references; particular avoid unbound variables generated programs. proposed four update algorithms incremental machine learning. implemented. extent update algorithms work demonstrated training sequence. slowness searching factorial function made realize need major improvements search update algorithms would like continue using scheme. working realistic training sequence features recursive problems optimizing search implementing remaining update algoritms. that extend implementation work parallel hardware implement phase solomonoff’s system attempt incorporating features proposals hsearch g¨odel machine. would like", "year": 2017}