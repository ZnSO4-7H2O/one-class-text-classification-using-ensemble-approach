{"title": "KSR: A Semantic Representation of Knowledge Graph within a Novel  Unsupervised Paradigm", "tag": ["cs.LG", "cs.AI"], "abstract": "Knowledge representation is a long-history topic in AI, which is very important. A variety of models have been proposed for knowledge graph embedding, which projects symbolic entities and relations into continuous vector space. However, most related methods merely focus on the data-fitting of knowledge graph, and ignore the interpretable semantic expression. Thus, traditional embedding methods are not friendly for applications that require semantic analysis, such as question answering and entity retrieval. To this end, this paper proposes a semantic representation method for knowledge graph \\textbf{(KSR)}, which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple. Since both aspects and categories are semantics-relevant, the collection of categories in each aspect is treated as the semantic representation of this triple. Extensive experiments show that our model outperforms other state-of-the-art baselines substantially.", "text": "knowledge representation long-history topic important. variety models proposed knowledge graph embedding projects symbolic entities relations continuous vector space. however related methods merely focus dataﬁtting knowledge graph ignore interpretable semantic expression. thus traditional embedding methods friendly applications require semantic analysis question answering entity retrieval. paper proposes semantic representation method knowledge graph imposes two-level hierarchical generative process globally extracts many aspects locally assigns speciﬁc category aspect every triple. since aspects categories semantics-relevant collection categories aspect treated semantic representation triple. extensive experiments show model outperforms state-of-the-art baselines substantially. introduction facilitate application knowledge statistical learning methods continuous vectorial representations entities/relations necessary. thus knowledge graph embedding proposed fulﬁll motivation. speciﬁcally represents symbolic triple real-valued vector corresponds head entity relation tail entity respectively. currently variety embedding methods emerging including translation-based models transe many following variants neural network based models generative models transg etc. major methodology knowledge representation translation-based models adopt principle geometric translation formally intuitively corresponding objective aims ﬁtting translation-based principle knowledge representations minimizing data ﬁtting error. though models achieved great success representation models still semantically interpretable major harm potential applications. known fact traditional methods almost impossible exactly extract speciﬁc semantics geometric points. example transe representation table could hardly tell anything meaningful furniture daily tool animal etc. however withsemantics knowledge language remains limiting task knowledge applications natural language understanding instance freebase stanford university represented abstract symbol /m/pwq semantically interpretable representation entity preferred stanford university representation extremely useful many areas question answering knowledge base. example query which private university famous palo alto?. first linguistic features university private palo alto extracted query. then matching linguistic features query semantic knowledge representations answer entity stanford university retrieved. summary knowledge beneﬁts computational linguistics semantic representation. notably introduce term knowledge feature describe knowledge semantic aspects university geographical position etc. semantically represent knowledge model leverages two-level hierarchical generative process entity/relation representations. next paragraph introduce principal idea model novel unsupervised paradigm exempliﬁed figure applying paradigm knowledge representation propose method speciﬁcally presented figure referring figure exists question categorize objects?. addressed shape objects clusters grouped square star focused feature content three clusters generated hallow slash solid. summary shape content distinguished views clustering also called knowledge feature figure ﬁgure illustrates principal idea described introduction. simply leverage cluster ambiguity manner multi-view clustering construct semantic representation. gathering information clusters view/feature semantic representations formed. instance right-bottom ﬁgure indicates solid square ﬁrst/second dimension corresponds shape/content feature latent manner. similarly principal paradigm methodology leverages two-level hierarchical generative process semantic representation. figure illustratively exempliﬁes generative process model. first ﬁrst-level process generates many knowledge features/views different semantics university location. then second-level process groups entities/relations/triples according corresponding semantic features/views. last summarizing cluster identiﬁcation within feature/view constructs semantic representation knowledge elements. example tsinghua university category assigned university feature beijing category assigned location feature. exploiting multi-view clustering form knowledge semantically organized tsinghua university noted that cluster identiﬁcation could various forms model membership degree clusters rather linguistic lexicons besides knowledge features latent concepts ﬁrst dimension learned vectors. exempliﬁed dimension corresponds semantics university latent style rather linguistic description. however entity description easily latent features categories human-readable words. contributions proposed novel unsupervised paradigm semantic representation leverages ambiguity clustering multi-view methodology. inspired novel paradigm proposed figure ﬁgure demonstrates generative process clustering perspective. original knowledge semantically clustered multiple views knowledge features. speciﬁcally knowledge features location generated ﬁrst-level generative process denoting types clusters. category beijing knowledge feature generated second-level generative process. model represent knowledge graph good semantic proposed semantic method terpretation. entity retrieval knowledge representation provides potential application jointing knowledge language. related work transe pioneering work translation-based methods translates head entity tail relation vector formally geometric principle adopted many following works. following variants transform entities different subspaces almost based principle. manifolde opens classic branch translation applied based manifold. transh leverages relation-speciﬁc hyperplane embed entities. transr utilizes relationrelated matrix rotate embedding space. similar researches also contain transg transa transd transm researches incorporate additional structural information embedding. ptranse takes relation paths account simultaneously involving information conﬁdence level path knowledge graph. leverages rules concentrate embeddings complex relation types n-n. aims analyzing geometric structure embedding topologies based discoveries designs semantically smoothing score function. also involves gaussian analysis characterize uncertain concepts knowledge graph. aligns knowledge graph corpus jointly conducts knowledge word embedding. however necessity alignment information limits method performance practice. thus proposes joint method aligns freebase entity corresponding wiki-page. extends translation-based embedding methods triple-speciﬁc model textaware model encoding textual descriptions entities. also work hole rescal etc. process golden triples. parameters learned training procedure uniformly distributed indicating safely omitted simple mathematical manipulation. head-speciﬁc category tail-speciﬁc category discriminated active passive forms respectively subjectobject-relevant expressions. example shakespeare write macbeth written semantically differentiated subjectobjectspeciﬁc. thus better sample category head tail entities fact triples respectively. however single entity subjectobjectcategory consistent mathematically because matter entity subject object semantics identical. also noteworthy terms involved relations distinguished subject objective stated last paragraph. regarding since triple short imply facts headtail-speciﬁc semantics distributions categories proximal enough represent exact triple fact. constrain category generation impose laplace prior category distributions. figure probabilistic graph generative process. outer navy plate corresponds ﬁrst-level inner corresponds second-level. speciﬁc form factor introduced methodology. firstly enforce correspond forbidden category. thus case model δziyi example location feature triple assume i-th feature location. situation locationchina locationamerica allowed model triple short could talk exact thing usual. thus case locationchina accepted. secondly argued generative process sample category subjectobject-speciﬁc positions different probabilities. formally though guaranteed also discuss corresponding sampling probabilities rather sampled category point paragraph. difference sampling probabilities sampled items illustrated example last paragraph head entity suggests subject-speciﬁc location feature samples category china probability america tail supposed suggest object-speciﬁc feature sampled china category much higher probability america expect head tail could tell exact story guarantee coherence sampling distributions thus laplace prior imposed approximate distributions mathematically exp−p| )δziyi exp−p| )δziyi hyper-parameter laplace distribution presented generative process. second-level could analyzed clustering perspective. second-level generative process clusters knowledge elements according knowledge feature/view. features/views stem ﬁrst-level process mathematically according probabilistic terms involved furthermore ﬁrst-level generative process adjusts different knowledge feature spaces feedback second-level. mathematically feed-back corresponds essence knowledge semantically organized multi-view clustering form thus modeling multi-view clustering nature semantically interpretable. according figure knowledge features generated ﬁrst-level process means discovering different features/views clustering. then knowledge feature speciﬁc category assigned every entity/relation/triple second-level process. process equivalently explained calculating cluster identiﬁcations grouping entities/relations different features/views. last summarizing cluster identiﬁcation within feature/view constructs semantic representation knowledge elements. example tsinghua university belongs cluster university feature/view belong beijing cluster location feature/view. summary model represents entity semantically tsinghua university experiments experimental settings datasets. experiments conducted public benchmark datasets subsets freebase. statistics datasets refer readers entity descriptions dkrl small part corresponding wiki-page. implementation. implemented transe transh transr transg manifolde comparison directly reproduce claimed results reported optimal parameters. optimal settings learning factor margin laplace hyper-parameter fair comparison within parameter quantity adopt three settings dimensions denotes number knowledge features indicates number semantic categories. train model convergence stop rounds. however regarding transg manifolde trained formulation presented equations total knowledge feature number category number feature. notably generative probability triple score function. possible category speciﬁc knowledge features semantic representation. suggested probabilistic graph exactly inferred representation entity relation maxd objective training maximum data likelihood principle employed training. maximize ratio likelihood true triples false ones. objective follows♥ golden triples false triples generating negative sampling. speciﬁc formula presented previous subsection unknown distribution parameters learned sgd. training procedure similar efﬁciency theoretically time complexity training algorithm feature number category number feature. embedding dimension transe method comparative transe terms efﬁciency condition practically satisﬁed. real-word dataset regarding training time transe costs costs almost same. also comparison setting transr needs costs note transe almost fastest embedding method demonstrates method nearly efﬁcient. entity classiﬁcation motivation. testify semantics-speciﬁc performance conduct entity classiﬁcation prediction. since entity type human language artist book author represents semantics-relevant sense thus task could justify indeed addressed semantic representation. regarding evaluation protocol please refer results. evaluation results reported table noting means different settings knowledge features semantic categories. could observe that outperforms baselines large margin demonstrating effectiveness ksr. entity types represent level semantics thus better results illustrate method indeed semantics-speciﬁc. knowledge graph completion motivation. task benchmark task a.k.a link prediction concerns identiﬁcation ability triples. many tasks could beneﬁt link prediction relation extraction regarding evaluation protocol please refer results. evaluation results reported table could observe that outperforms baselines substantially justifying effectiveness model. theoretically effectiveness originates semanticsspeciﬁc modeling ksr. within parameter scale compared transe improves relatively compared transr improves comparison illustrates beneﬁts high-dimensional settings knowledge features categories. semantic analysis case study conduct case study analyze semantics model. brevity explore datasets employs knowledge features feature assigns categories. fact complex approach setting thus many minor features categories suppressed. consideration setting facilitate visualization presentation. first analyze speciﬁc semantics feature. leverage entity descriptions calculate joint probability corresponding occurrence number word textual descriptions entity inferred feature-category entity. there] fore δwj∈de sei=c words description entity regarding reader could refer subsection model description. then list signiﬁcant words category feature. semantics features categories could explicitly interpreted. directly list results table signiﬁcant features presented categories signiﬁcant words evidence. result strongly justiﬁes motivation ksr. notably four features vague recognized because latent space method similar lda. star trek television series produced american. thus semantic representations quite coherent semantics entity. textual description football club illichivets mariupol ukrainian professional football club based mariupol accordant semantic representation. note that football club team composed multiple persons reason multiple persons related. person producer could search nationality information person semantic representation could still interpretable. film highly correlated person accordant common knowledge. location indicates geographical position outside u.s. thus loosely related american. entity retrieval subsection motivate application language processing semantic knowledge representation. simply given sentence regarded entity task retrieve speciﬁc entity. according section obtained joint distribution speciﬁc sentence employ naive bayesian take semantic representation sentence. last retrieve entity cosine similarity semantic representations sentence entities. totally three sub-tasks single factoid multiple factoid inferential factoid. single factoid task deals descriptions recall entity. list cases. proposes relativity theory? albert einstein. company best multi-media? adobe systems incorporated. chinese province capital taiyuan? shan-xi. multiple factoid task processes sentences involve several entities. list cases. provinces neighbor beijing? tianjin hebei. countries neighbor china? burma vietnam india. inferential factoid task requires strong reasoning ability means answer direct snip current corpus. lists case. people chinese province richest? macao su-zhou hong kong jiangsu zhe-jiang. results entity retrieval demonstrates potentials semantic representations founded effectiveness ksr. method provides potential jointing knowledge language. conclusion propose model knowledge semantic representation two-level hierarchical generative process semantically represent knowledge. model able produce interpretable representations. also evaluate method extensive studies. experimental results justify effectiveness capability semantic expressiveness. americanunrelated sportsunrelated artunrelated personunrelated locationrelated). common sense capital location sports thus semantic representations reasonable. firstly randomly select entities manually check correctness semantic representations common knowledge. entities semantic representations totally correct also entities representations incorrect feature. entities corresponding representations incorrect feature. thus result proves strong semantic expressive ability ksr. secondly features co-occur semantic representation entity/relation knowledge element contributes correlation features. make statistics correlation draw heatmap figure darker color corresponds higher correlation. looking details sportsrelated entities would distribute world almost americanunrelated. result shows correlation features loose. richard socher danqi chen christopher manning andrew reasoning neural adtensor networks knowledge base completion. vances neural information processing systems pages zhen wang jianwen zhang jianlin feng zheng chen. knowledge graph embedding translating hyperplanes. proceedings twentyeighth aaai conference artiﬁcial intelligence pages xiao minlie huang xiaoyan zhu. point manifold knowledge graph embedding precise link prediction. proceedings international joint conference artiﬁcial intelligence xiao minlie huang xiaoyan zhu. transg generative model knowledge graph proceedings international embedding. conference computational linguistics. association computational linguistics xiao minlie huang lian meng xiaoyan zhu. semantic space projection knowledge graph embedding text description. proceedings thirty-first aaai conference artiﬁcial intelligence references antoine bordes jason weston ronan collobert yoshua bengio learning structured proceedings embeddings knowledge bases. twenty-ﬁfth aaai conference artiﬁcial intelligence antoine bordes nicolas usunier oksana alberto garcia-duran yakhnenko. translating embeddings modeling multi-relational data. advances neural information processing systems pages miao qiang zhou emily chang thomas fang zheng. transition-based knowledge graph proembedding relational mapping properties. ceedings paciﬁc asia conference language information computation pages shizhu kang guoliang zhao. learning represent knowledge graphs gaussian embedding. proceedings international conference information knowledge management pages raphael hoffmann congle zhang xiao ling luke zettlemoyer daniel weld. knowledge-based weak supervision information exproceedings traction overlapping relations. annual meeting association computational linguistics human language technologies-volume pages association computational linguistics yankai zhiyuan maosong sun. modeling relation paths representation learning knowledge bases. proceedings conference empirical methods natural language processing association computational linguistics yankai zhiyuan maosong yang xuan zhu. learning entity relation embeddings knowledge graph completion. proceedings twenty-ninth aaai conference artiﬁcial intelligence maximilian nickel volker tresp hans-peter kriegel. factorizing yago scalable machine learning linked data. proceedings international conference world wide pages", "year": 2016}