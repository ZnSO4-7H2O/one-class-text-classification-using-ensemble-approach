{"title": "Generative Models and Model Criticism via Optimized Maximum Mean  Discrepancy", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.NE", "stat.ME"], "abstract": "We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy (MMD). This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the MMD may be used in two roles: first, as a discriminator, either directly on the samples, or on features of the samples. Second, the MMD can be used to evaluate the performance of a generative model, by testing the model's samples against a reference data set. In the latter role, the optimized MMD is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classifier.", "text": "dougal sutherland∗† heiko strathmann∗ aaditya ramdas‡ ∗gatsby computational neuroscience unit university college london school computer science carnegie mellon university departments eecs statistics university california berkeley dougalgmail.com htungcs.cmu.edu propose method optimize representation distinguishability samples probability distributions maximizing estimated power statistical test based maximum mean discrepancy optimized applied setting unsupervised learning generative adversarial networks model attempts generate realistic samples discriminator attempts tell apart data samples. context used roles ﬁrst discriminator either directly samples features samples. second used evaluate performance generative model testing model’s samples reference data set. latter role optimized particularly helpful gives interpretable indication model data distributions differ even cases individual model samples easily distinguished either classiﬁer. many problems testing learning require evaluating distribution similarity high dimensions structured data images audio. complex generative model learned necessary provide feedback quality samples produced. generative adversarial network popular method training generative models rival discriminator attempts distinguish model samples reference data. training generator discriminator interleaved saddle point eventually reached joint loss. useful insight behavior gans note discriminator properly trained generator tasked minimizing jensen-shannon divergence measure model data distributions. model insufﬁciently powerful perfectly simulate test data nontrivial settings choice divergence measure especially crucial determines compromises made. range adversarial divergences proposed huszar using weight interpolate inverse jensen-shannon. weight interpreted prior probability observing samples model real world greater probability model samples approach reverse model seeks modes data distribution. greater probability drawing data distribution model approaches divergence tries cover full support data expense producing samples probability regions. insight developed nowozin showed much broader range f-divergences learned discriminator adversarial models based variational formulation f-divergences nguyen given f-divergence model learns composition density ratio derivative comparing generator data samples. provides lower bound true divergence would obtained density ratio perfectly known. event model smaller class true data distribution broader family divergences implements variety different approximations focus individual modes true sample density others cover support. straightforward visualize properties dimensions higher dimensions becomes difﬁcult anticipate visualize behavior various divergences. alternative family divergences integral probability metrics witness function distinguish samples popular class witness functions gans maximum mean discrepancy simultaneously proposed dziugaite architecture used approaches actually quite different dziugaite discriminator directly level generated test images whereas apply input features learned autoencoder share decoding layers autoencoder generator network generated samples better visual quality latter method becomes difﬁcult analyze interpret algorithm given interplay generator discriminator networks. related approach salimans propose feature matching generator tasked minimizing squared distance expected discriminator features model data distributions thus retaining adversarial setting. light varied approaches discriminator training important able evaluate quality samples generator reference data. approach used several studies obtain parzen window estimate density compute log-likelhiood unfortunately density estimates high dimensions known unreliable theory practice instead humans evaluate generated images evaluators able distinguish cases samples over-dispersed difﬁcult under-dispersed samples imbalances proportions different shapes since samples plausible images. recall different divergence measures result different degrees mode-seeking rely human evaluation tend towards always using divergences under-dispersed samples. propose distinguish generator reference data features kernels chosen maximize test power quadratic-time gretton optimizing test power requires sophisticated treatment different form null alternative distributions also develop efﬁcient approach obtaining quantiles distribution null demonstrate simple artiﬁcial data simply maximizing provides less powerful test approach explicitly maximizing test power. procedure applies even deﬁnition computed features inputs since also trained power maximization. designing optimized test choose kernel family allows visualize probability mass samples differs most. experiments performance evaluation automatic relevance determination kernel output dimensions learn coordinates differ meaningfully ﬁnding kernels retain signiﬁcant bandwidth test power optimized. apply method lloyd ghahramani visualize witness function associated ﬁnding model data samples occurring maxima minima witness function optimized witness function gives test greater power standard kernel suggesting associated witness function peaks improved representation distributions differ. also propose novel generative model based feature matching idea salimans using rather minibatch discrimination heuristic principled stable enforcement sample diversity without requiring labeled data. begin reviewing maximum mean discrepancy two-sample tests. kernel reproducing kernel hilbert space functions assume measurable bounded supx∈x distributions many kernels including popular gaussian characteristic implies metric particular mmdk tests characteristic kernel consistent. said different characteristic kernels yield different test powers ﬁnite sample sizes wish choose kernel maximize test power. below usually suppress explicit dependence estimator unbiased nearly minimal variance among unbiased estimators following gretton conduct hypothesis test null hypothesis given allowable probability alternative using test statistic false rejection choose test threshold reject converges asymptotically distribution depends unknown distribution thus cannot evaluate test threshold closed form. instead estimate data-dependent threshold permutation randomly partition data many times evaluate split estimate quantile samples. section discusses efﬁcient computation process. maximize test power. here propose maximize using efﬁcient computation section grows however asymptotically maximize power test choosing kernel maximizes t-statistic practice maximize estimator given maintain validity hypothesis test need divide observed data training sample used choose kernel testing sample used perform ﬁnal hypothesis test learned kernel. kernels. case however function kernel composition also kernel thus choose function extract meaningful features inputs standard kernel compare features. select function performing kernel selection family kernels merely need maximize ˆtκ◦z standard optimization techniques based gradient ˆtκ◦z respect parameterizations test power. estimate similar given bounliphone incorporates second-order terms corrects small sources bias. though expression somewhat unwieldy deﬁned various sums kernel matrices differentiable respect kernel given terms expectations appendix replace expectations ﬁnite-sample averages giving required estimator. deﬁne matrices similarly ˜kxx. vector ones. unbiased estimator common practice performing two-sample tests gaussian kernel bandwidth median pairwise distance among joint data. heuristic often works well fails scale vary differs scale overall variation ramdas study power median heuristic high-dimensional problems justify case means differ. early heuristic improving test power simply maximize sriperumbudur proved that certain classes kernels yields consistent test. shown sriperumbudur however maximizing amounts minimizing training classiﬁcation error linear loss. comparing plainly optimal approach maximizing test power since variance ignored. also consider maximizing criteria based cross validation injective characteristic characteristic. whether ﬁxed consistent approach differentiable thus difﬁcult maximize among ﬁxed candidate kernels. moreover cross-validation used maximize validation amounts maximizing classiﬁcation performance rather test performance suboptimal latter setting finally gretton previously studied direct optimization power test streaming estimator optimizing ratio empirical statistic variance also optimizes test power. streaming estimator uses data inefﬁciently however often requiring samples achieve power comparable tests based practical implementations tests based require efﬁcient estimates test threshold known test threshold estimates lead consistent test permutation test mentioned above sophisticated null distribution estimate based approximating eigenspectrum kernel previously reported faster permutation test fact relatively slow reported performance permutation approach naive matlab implementation permutation test code accompanying gretton creates copy kernel matrix every permutation. show that careful design permutation thresholds computed substantially faster even compared parallelized state-of-the-art spectral solvers first observe avoid copying kernel matrix simply generating permutation indices null sample accessing precomputed kernel matrix permuted order. practice however give much performance gain random nature memoryaccess conﬂicts modern cpus implement caching. second rather maintain inverse permutation indices easily traverse matrix sequential fashion. approach exploits hardware prefetchers reduces number cache misses almost less furthermore sequential access pattern kernel matrix enables invoke multiple threads computing null samples traversing matrix sequentially without compromising locality reference cache. consider example problem computing test using null distribution samples two-dimensional samples comparing gaussian laplace distribution matched moments. compare optimized permutation test spectral test using highlyoptimized state-of-the-art spectral solver intel’s library results averaged runs; variance across runs negligible. figure shows obtained speedups number computing threads grow implementation faster single thread also saturates slowly number threads increases. figure shows timings increasing problem sizes using available system threads larger problems permutation implementation order magnitude faster spectral test smaller problems still signiﬁcant performance increase. reference also report timings available non-parallelized implementations compared version’s figure open-sourced spectral test shogun using eigen reference matlab spectral implementation naive python permutation test partly avoids copying masking. classiﬁers sample test boils testing differences means. setting classiﬁer chosen fisher’s using classiﬁer accuracy held-out data test statistic turns minimax optimal rate constants meaning exist tests achieve power fewer samples. result proved statistic setting however generalization statistics settings open question. figure runtime comparison sampling null distribution. compare optimized permutation approach spectral method using intel’s spectral solver. time spent precomputing kernel matrix included. left increasing number threads ﬁxed problem size single-threaded times implementations matlab reference spectral python permutation shogun spectral right increasing problem sizes using maximum number system threads. synthetic data consider problem bandwidth selection gaussian kernels blobs dataset gretton grid two-dimensional standard normals spacing centers. laid identically covariance coordinates figure shows samples note take samples distribution compute bandwidths. repeat process times. figure shows median heuristic always chooses large bandwidth. maximizing alone bimodal distribution bandwidths signiﬁcant number samples falling region test power. variance much higher region however hence optimizing ratio never returns bandwidths. figure shows maximizing outperforms maximizing across variety problem parameters performs near-optimally. model criticism example real-world two-sample testing problem consider distinguishing output generative model reference distribution attempts reproduce. semi-supervised model salimans trained mnist dataset handwritten images. true samples dataset shown figure samples learned model figure salimans called results completely indistinguishable dataset images reported annotators mechanical turk able distinguish samples cases. comparing results however several pixel-level artifacts make distinguishing datasets trivial; methods pick quickly. relevance determination -type kernel notation section scales pixel learned value gaussian kernel learned global bandwidth. optimized samples batches size using adam optimizer learned weights visualized figure network essentially perfect discriminative power testing different samples permutations test cases obtained p-values twice contrast using kernel bandwidth optimized maximizing statistic gave less powerful test worst p-value repetitions power threshold. kernel based median heuristic found bandwidth times size t-statistic-optimized bandwidth performed worse still three repetitions found p-value exactly power threshold learned weights show model differs true dataset along outsides images well along vertical line center. investigate results detail using approach lloyd ghahramani considering witness function associated largest amplitude probability mass samples different. thus samples falling maxima minima witness function best represent difference distributions. value witness function sample plotted figure along images different values witness function. apparently slightly overproducing images resembling /-like digits left underproducing vertical case simply underproducing general p-values contingency test outputs digit classiﬁers distributions uniform. subtle difference proportions among types digits would quite difﬁcult human observers detect. testing framework allows model developer differences decide whether them. could complex representation function detect even subtle differences distributions. criterion demonstrate training criterion gans. consider basic approaches train mnist. first generative moment matching network approach uses statistic computed kernel directly images discriminator model. t-gmmn generator minimize statistic ﬁxed kernel. compared standard gmmns t-gmmn directly attempts make distributions indistinguishable kernel function; avoids situation like figure although value quite small distributions perfectly distinguishable small variance. used code minibatch discrimination labels chose best several runs. implementation details used architecture generator consists fully connected layers sizes relu activations except last uses sigmoids. kernel function gmmns gaussian kernels ﬁxed bandwidths feature matching discriminator fully connected layers size sigmoid activation. concatenate image layer features input mixture kernels gmmns. optimize sgd. initialization parameters gaussian standard deviation gmmns feature matching. learning rates respectively. learning rate feature matching discriminator experiments iterations momentum optimizer momentum could additionally update kernel adversarially maximizing statistic based generator samples difﬁculty optimizing model interplay generator discriminator adds difﬁculty task. witness function ex∼p ey∼q evaluated test set. images shown weights highlighted. vertical lines show distribution means; distance small highly consistent. figure mnist digits various models. part shows runs minibatch discrimination model salimans trained without labels model that labels generated figure next feature matching gans train discriminator classiﬁer like normal train generator minimize generator samples reference samples kernel computed intermediate features discriminator. salimans proposed feature matching using mean features discriminator instead mixture kernels ensuring full feature distributions match rather means. helps avoid common failure mode gans generator collapses outputting small number samples considered highly realistic discriminator. using mmd-based approach however single point approximate feature distribution. minibatch discrimination approach salimans attempts solve problem introducing features measuring similarity sample selection samples unable work without labels force discriminator reasonable direction; figure demonstrates failures showing samples representative runs model. references alba fernández jiménez-gamero muñoz garcia. test two-sample problem based empirical characteristic functions. computational statistics data analysis wacha bounliphone eugene belilovsky matthew blaschko ioannis antonoglou arthur gretton. test relative similarity model selection generative models. international conference learning representations arxiv.. emily denton soumith chintala arthur szlam fergus. deep generative image models using laplacian pyramid adversarial networks. advances neural information processing systems gintare karolina dziugaite daniel zoubin ghahramani. training generative neural networks maximum mean discrepancy optimization. uncertainty artiﬁcial intelligence arxiv.. arthur gretton kenji fukumizu zaid harchaoui bharath sriperumbudur. fast consistent kernel two-sample test. advances neural information processing systems arthur gretton bharath sriperumbudur dino sejdinovic heiko strathmann massimiliano pontil. optimal kernel choice large-scale two-sample tests. advances neural information processing systems xuanlong nguyen martin wainwright michael jordan. optimal quantization rules problems sequential decentralized detection. ieee transactions information theory sebastian nowozin botond cseke ryota tomioka. f-gan training generative neural samplers using variational divergence minimization. advances neural information processing systems aaaditya ramdas sashank reddi barnabás póczos aarti singh larry wasserman. adaptivity computation-statistics tradeoffs kernel distance based high dimensional sample testing arxiv.. aaditya ramdas sashank reddi barnabás póczos aarti singh larry wasserman. decreasing power kernel distance based nonparametric hypothesis tests high dimensions. aaai conference artiﬁcial intelligence arxiv.. salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems arxiv.. soeren sonnenburg heiko strathmann sergey lisitsyn viktor fernando iglesias garcía chiyuan zhang soumyajit tklein evgeniy andreev jonasbehr sploving parijat mazumdar christian widmer abhijeet kislay kevin hughes roman votyakov khalednasr saurabh mahindre alesis novik abinash panda evangelos anagnostopoulos liang pang serialhex alex binder sanuj sharma michal uˇriˇcáˇr björn esser daniel pyrathon. shogun shogun tajinohi agatamori bharath sriperumbudur kenji fukumizu arthur gretton gert lanckriet bernhard schölkopf. kernel choice classiﬁability rkhs embeddings probability distributions. advances neural information processing systems bharath sriperumbudur arthur gretton kenji fukumizu gert lanckriet bernhard schölkopf. hilbert space embeddings metrics probability measures. journal machine learning research bharath sriperumbudur kenji fukumizu arthur gretton bernhard schölkopf gert lanckriet. empirical estimation integral probability metrics. electronic journal statistics derive variance derivation similar appendix bounliphone except drop second-order term since gives little computational advantage remove several small biases estimator effect moderate large sample sizes discernible effect sample size small. recall kernel corresponds reproducing kernel hilbert space exists function deﬁne ex∼p ey∼q] kernel mean embeddings distributions µqh. below supress explicit dependence brevity.", "year": 2016}