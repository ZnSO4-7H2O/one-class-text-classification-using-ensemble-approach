{"title": "Componentwise Least Squares Support Vector Machines", "tag": ["cs.LG", "cs.AI", "I.2.6"], "abstract": "This chapter describes componentwise Least Squares Support Vector Machines (LS-SVMs) for the estimation of additive models consisting of a sum of nonlinear components. The primal-dual derivations characterizing LS-SVMs for the estimation of the additive model result in a single set of linear equations with size growing in the number of data-points. The derivation is elaborated for the classification as well as the regression case. Furthermore, different techniques are proposed to discover structure in the data by looking for sparse components in the model based on dedicated regularization schemes on the one hand and fusion of the componentwise LS-SVMs training with a validation criterion on the other hand. (keywords: LS-SVMs, additive models, regularization, structure detection)", "text": "summary. chapter describes componentwise least squares support vector machines estimation additive models consisting nonlinear components. primal-dual derivations characterizing ls-svms estimation additive model result single linear equations size growing number data-points. derivation elaborated classiﬁcation well regression case. furthermore different techniques proposed discover structure data looking sparse components model based dedicated regularization schemes hand fusion componentwise ls-svms training validation criterion hand. non-linear classiﬁcation function approximation important topic interest continuously growing research areas. estimation techniques based regularization kernel methods play important role. mention context smoothing splines regularization networks gaussian processes support vector machines many more e.g. svms related methods introduced within context statistical learning theory structural risk minimization. methods solves convex optimization problems typically quadratic programs. least squares support vector machines reformulations standard svms lead solving linear systems classiﬁcation tasks well regression. ls-svms proposed class kernel machines primaldual formulations relation kernel fisher discriminant analysis ridge regression partial least squares principal component analysis canonical correlation analysis recurrent networks control. dual problems static regression without bias term closely related gaussian processes regularization networks kriging ls-svms rather take optimization approach primal-dual formulations exploited towards large scale problems developing robust versions. direct estimation high dimensional nonlinear functions using non-parametric technique without imposing restrictions faces problem curse dimensionality. several attempts made overcome obstacle including projection pursuit regression kernel methods dimensionality reduction additive models useful approximating high dimensional nonlinear functions methods extensions become widely used nonparametric techniques offer compromise somewhat conﬂicting requirements ﬂexibility dimensionality interpretability. traditionally splines common modeling technique additive models e.g. mars combination anova additive models brought attention machine learning community e.g. estimation nonlinear components additive model usually performed iterative backﬁtting algorithm two-stage marginal integration based estimator although consistency shown certain conditions important practical problems theoretical problems still left. chapter show primal-dual derivations characterizing ls-svms employed formulate straightforward solution estimation problem additive models using convex optimization techniques classiﬁcation well regression problems. apart one-shot optimal training algorithm chapter approaches problem structure detection additive models considering appropriate regularization scheme leading sparse components. additive regularization framework adopted emulate effectively schemes based -norms -norms specialized penalization terms furthermore validation criterion considered select relevant components. classically exhaustive search methods used written combinatorial optimization problem. chapter proposes convex relaxation component selection problem. chapter organized follows. section presents componentwise ls-svm regressors classiﬁers efﬁcient estimation additive models relates result anova kernels classical estimation procedures. section introduces additive regularization context shows emulate dedicated regularization schemes order obtain sparse components. section considers giving training deﬁned yk}n size drawn i.i.d. unknown distribution according unknown real-valued smooth function uncorrelated random errors data points validation denoted following vector notations used throughj text rd×n estimator regression function difﬁcult dimension large. quantify optimal minimax rate convergence estimation times differentiable regression function converges zero slowly large compared possibility overcome curse dimensionality impose additional structure regression function. although needed derivation optimal solution input variables assumed uncorrelated applications. superscript denote d-th component input vector instance component correspond different dimension input observations. assume function approximated arbitrarily well model following structure components second term known becomes easy estimate lefthandside. large class linear smoothers so-called backﬁtting algorithms equivalent gauss-seidel algorithm solving linear equations backﬁtting algorithm theoretically practically well motivated. two-stages marginalization approaches construct ﬁrst stage general black-box pilot estimator ﬁnally estimate additive components marginalizing component variation remaining components. individual components additive model based ls-svms written primal space rnϕd denotes potentially inﬁnite dimensional feature map. regularized least squares cost function given taking conditions optimality ∂lγ/∂αk ∂lγ/∂b ∂lγ/∂ek ∂lγ/∂wd application kernel trick positive deﬁnite kernel gets following conditions optimality figure shows modiﬁed kernel case dimensional radial basis function kernel used components. observation implies componentwise ls-svms inherit results obtained classical ls-svms kernel methods general. practical point view previous kernels result fig. thetwodimensionalcomponentwiseradialbasisfunctionkernelforcompol asdisplayed.the nentwisels-svmstakestheformk primal space rnϕd denotes potentially inﬁnite dimensional feature map. regularized least squares cost function given regularization method ﬁxes priori answer ill-conditioned nature inverse problem. classical tikhonov regularization scheme states answer terms norm solution. formulation additive regularization framework made possible impose alternative answers ill-conditioning problem hand. refer areg level substrate ls-svms. appropriate regularization scheme additive models favor solutions using smallest number components explain data much possible. paper somewhat relaxed condition sparse components select appropriate components instead general problem input selection. fig. graphical representation additive regularization framework used emulatingotherlossfunctionsandregularizationschemes.conceptuallyonedifferentiatesbetween thenewlyspeciﬁedcostfunctionandthels-svmsubstrate whilecomputationally bothare computedsimultanously. however scheme general practical implementation limited appropriate imposing example constraints corresponding certain model assumptions speciﬁed cost function. consider moment conditions optimality componentwise ls-svm regressor using regularization term ridge regression equation corresponds given appropriate found satisﬁes constraints plugged ls-svm substrate turns omit conceptual second stage computations elimination variable constrained optimization problem alternatively measure corresponding cost function used fulﬁlls role model selection broad sense. variety explicit implicit limitations emulated based different criteria fig. thelevelcostfunctionsoffigureontheconceptuallevelcantakedifferentforms based validation performance trainings error. result convex tuning proceduresothermayloosethispropertydependingonthechosencostfunctiononthesecond level. study obtain sparse components considering dedicated regularization scheme. ls-svm substrate technique used emulate proposed scheme primal-dual derivations straightforward anymore. denote estimated training outputs d-th submodel component based regularization scheme translated following constrained optimization problem conditions optimality summarized satisﬁed exactly convex constrained optimization problem solved quadratic programming problem. consequence norm often sparse components obtained similar sparse variables lasso sparse datapoints important difference estimated outputs used regularization purposes instead solution vector. good practice omit sparse components training dataset simulation instead leads much simpler optimization problem additional assumptions needed distribution elements moreover component selection resort signiﬁcance test instead sparsity resulting practical algorithm proposed subsection uses iteration norm based optimizations order calculate optimum proposed regularized cost function. subsection considers extensions classical formulations towards dedicated regularization schemes sparsifying components. consider componentwise regularized least squares cost function deﬁned leads bridge regression known penalty function results ridge regression. penalty function solution soft thresholding rule lasso proposed penalized least squares estimate using penalty function hard thresholding penalty functions simultaneously satisfy mathematical conditions unbiasedness sparsity continuity hard thresholding discontinuous cost surface. continuous cost surface thresholding rule lp-family penalty function resulting estimator shifted constant avoid drawbacks suggests penalty function deﬁned penalty function behaves quite similarly smoothly clipped absolute deviation penalty function suggested smoothly thresholding penalty function improves properties penalty function hard thresholding penalty function unknowns regularization parameters. plausible value derived transformed penalty function satisﬁes oracle inequalities plugin described semi-norm improve component based regularization scheme again additive regularization scheme used emulation scheme section investigates tune componentwise ls-svms respect validation criterion order improve generalization performance ﬁnal model. proposed fusion training validation levels investigated optimization point view conceptually considered different levels. purpose fusion argument introduced brieﬂy revised relation regularization parameter tuning. estimator ls-svm regressor training data ﬁxed value given results solving linear equations substitution lagrange multipliers tuning regularization parameter using validation criterion gives following estimator referred fusion. resulting optimization problem noted non-convex optimal solutions corresponding non-convex. overcome problem re-parameterization tradeproposed leading additive regularization scheme. cost overparameterizing trade-off convexity obtained. circumvent drawback different ways restrict explicitly implicitly degrees freedom regularization scheme proposed retaining convexity convex problem resulting additive regularization possible relaxed version component selection problem goes follows investigate whether plausible drive components validation zero without large modiﬁcations global training solution. translated following cost function much spirit dek) equality constraints consist conditions optimality evaluation validation individual components. again convex problem solved quadratic programming problem. note regularization vector appears similar tikhonov regularization scheme component regularized individually. lagrangian constrained optimization problem multipliers becomes embedding problem additive regularization framework lead suitable representation allowing dedicated algorithms. relating conditions view latter within additive regularization framework imposing extra constraints bias term omitted remainder subsection notational convenience. ﬁrst constraints reﬂect training conditions schemes. solutions meaning appropriate determined enforcing estimation training data. summary second equations obtained eliminating last equation righthand side represents constraints values possible values product denotes tikhonov case readily seen solution space respect non-convex however constraint recognized bilinear form. fusion problem written practical applications following iterative approach used solving nonconvex cost-functions also used efﬁcient solution convex optimization problems become computational heavy case large number datapoints e.g. number classiﬁcation well regression problems employed illustrate capabilities described approach. experiments hyper-parameters kernel parameter regularization trade-off parameter tuned using -fold cross-validation. iterative scheme developed based graduated non-convexity algorithm proposed optimization non-convex cost functions. instead using local gradient step quite involved adaptive weighting scheme proposed every step relaxed cost function optimized using weighted -norm weighting terms chosen based initial guess global solution. every symmetric loss function monotonically increasing exists bijective transformation every proposed algorithm computing solution semi-norms employs iteratively convex relaxations prescribed non-convex norm. somewhat inspired simulated annealing optimization technique optimizing global optimization problems. weighted version based following derivation residuals corresponding solutions minθ equal solution convex optimization problem minθ satisfying stable results gradient penalty function quadratic approximation takne equal follows using intercept parameter denotes derivative evaluated minimum also minimizes weighted equivalent note constant intercepts relevant weighted optimization problem. assumption consecutive relaxations different global solutions following algorithm plausible practical tool fig. weighted l-normapproximation ofthe l-norm whichfollowsfromthelinearsetofequationsoncetheoptimalek areknown; theweightingterms forasequenceofek andk suchthat |ek| andν algorithm optimization semi-norms practical approach based deforming gradually -norm speciﬁc loss function interest. strictly decreasing series plausible choice initial convex cost function least squares cost function iterating scheme smaller least squares cost function penalizes higher residuals however number residuals increasing weight least squares loss function much lower small residuals. fig. example dataset consisting four input components whereonlytheﬁrstoneisrelevanttopredicttheoutput sinc.acomponentwise ls-svmregressorhasgoodpredictionperformancewhilethel penalizedcost functionofsubsectionalsorecoversthestructureinthedataastheestimatedcomponents correspndingwithx andx aresparse. input data randomly chosen interval gaussian nature noise model results least squares methods reported. described techniques applied training dataset tested independent test generated using saem rules. table reports whether algorithm recovered structure data experiment using smoothly tresholding penalized cost function designed follows every components version provided algorithm linear kernel another kernel regularization scheme able select components appropriate kernel except spurious component table resultsontestdataofnumericalexperimentsonthevapnikregressiondataset.the sparsenessisexpressedintherateofcomponentswhichisselectedonlyiftheinputisrelevant consists word frequencies email messages study screen email spam. test size drawn randomly data leaving training purposes. inputs preprocessed using following transformation standardized unit variance. figure gives indicator functions found using regularization based technique detect structure described subsection structure detection algorithm selected provided indicators. moreover componentwise approach describes form contribution indicator resulting highly interpretable model. chapter describes nonlinear additive models based ls-svms capable handling higher dimensional data regression well classiﬁcation tasks. estimation stage results solving linear equations size approximatively equal number training datapoints. furthermore additive regularization framework employed formulating dedicated regularization schemes leading structure detection. finally fusion argument component selection structure detection based training componentwise ls-svms validation performance introduced improve generalization abilities method. advantages using componentwise ls-svms include efﬁcient estimation additive models respect classical practice interpretability estimated model opportunities towards structure detection connection existing statistical techniques. fig. results spam dataset. non-sparse components found application subsection shown suggesting number usefull indicator variables classiﬁng mail message spam non-spam. ﬁnal classiﬁer takes form relevant components wereselectedoutoftheprovidedindicators. several phd/postdoc fellow grants; flemish government phd/postdoc grants projects research communities bil. int. collaboration hungary/ poland; grantsgbou belgian federal science policy ofﬁce iuap podo-ii fp-quprodis; ernsi; eureka -impact; eureka -flite; contract research/agreements ismc/ipcos datas elia mastercard supported grants several funding agencies sources. goa-ambiorics iuap project project project project associate professor full professor k.u.leuven belgium respectively.", "year": 2005}