{"title": "Semantic Parsing with Semi-Supervised Sequential Autoencoders", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms.", "text": "present novel semi-supervised approach sequence transduction apply semantic parsing. unsupervised component based generative model latent sentences generate unpaired logical forms. apply method number semantic parsing tasks focusing domains limited access labelled training data extend datasets synthetically generated logical forms. neural approaches particular attention-based sequence-to-sequence models shown great promise obtained state-of-the-art performance sequence transduction tasks including machine translation syntactic constituency parsing semantic role labelling requirement effectively training models abundance supervised data. paper focus learning mappings input sequences output sequences domains latter easily obtained annotation form pairs sparse expensive produce propose novel architecture accommodates semi-supervised training sequence transduction tasks. augment transduction objective autoencoding objective input sequence treated latent variable enabling training labelled pairs unpaired output sequences. autoencoder could principle constructed stacking sequence transducers modelling latent variable series discrete symbols drawn multinomial distributions creates serious computational challenges requires marginalising space latent sequences avoid intractable marginalisation introduce novel differentiable alternative draws softmax used reparametrisation trick kingma welling rather drawing discrete symbol softmax draw distribution symbols logistic-normal distribution time step. serve continuous relaxations discrete samples providing differentiable estimator expected reconstruction likelihood. demonstrate effectiveness proposed model three semantic parsing tasks geoquery benchmark sail maze navigation task natural language querying corpus openstreetmap. part evaluation introduce simple mechanisms generating large amounts unsupervised training data tasks. table examples natural language logical form three corpora tasks used paper. note sail corpus requires additional information order instruction action sequence. subsequently wish predict predicting discrete sequence symbols draws multinomial distributions vocabulary option would able backpropagate discrete choice. marginalising possible latent strings estimating gradient na¨ıve monte carlo methods would prohibitively high variance process number strings exponential maximum length vocabulary size base. allow backpropagation instead predict sequence distributions symbols attending sequential autoencoder shown figure high level seen sequenceto-sequence models attention chained together. precisely model consists four lstms hence name seq. ﬁrst bidirectional lstm encodes sequence next lstm stochastic output described below draws sequence distributions words vocabulary third lstm encodes distributions last attend reconstruct give details parts. ﬁrst lstm encoder half model reads sequence represented sequence one-hot vectors vocabulary using bidirectional sequence vectors sequence length loss function complete model described section gives reconstruction function deﬁne loss reconstruction accommodates unsupervised case observed training data supervised case pairs available. together allow train model semi-supervised setting experiments show provides beneﬁts purely supervised training regime. unsupervised case isn’t observed loss minimise training reconstruction loss expressed negative loglikelihood true labels relative predictions this regularising vector distribution vocabulary drawn logistic-normal distribution parameters which logt r|σx| predicted attending lstm attending outputs encoder |σx| size vocabulary logistic normal distribution serves regularise model semi-supervised learning regime described section. formally process depicted figure follows function lstm linear transformation r|σx|. reparametrisation trick kingma welling draw logistic normal allowing backpropagate sampling process. term divergence klp] effectively penalises mean variance diverging prior model diagonal gaussian effect smoothing logistic normal distribution draw distributions symbols guarding overﬁtting latent distributions symbols seen supervised case discussed below. unsupervised loss therefore formalised lunsup αklp] supervised case observed additionally minimise prediction loss expressed negative log-likelihood true labels relative predictions impose loss. supervised loss thus supervised unsupervised case because continuous relaxation generating reparameterisation trick gradient losses regard model parameters well deﬁned throughout seq. semi-supervised training inference train weighted combination supervised unsupervised losses described above. trained simply decoder segment model predict sequences symbols represented one-hot vectors. decoder trained without encoder fully supervised manner serves supervised sequenceto-sequence baseline model name geoquery ﬁrst task consider prediction query corpus frequently used benchmark semantic parsing. corpus contains questions geography together executable queries representing questions. follow approach established zettlemoyer collins split corpus training test cases. following common practice augment dataset referring database during training test time. particular database identify anonymise variables following method described dong lapata prior work corpus relies standard semantic parsing methods together custom heuristics pipelines corpus. recent paper dong lapata note uses sequence-to-sequence model training unidirectional equivalent also decoder part network. open street maps second task tackle model nlmaps dataset haas riezler dataset contains training testing instances natural language questions corresponding machine readable queries geographical openstreetmap database. dataset contains natural language question english german focus single language semantic parsing similar ﬁrst task haas riezler data pre-processing step tokenization natural language query form. navigational instructions actions sail corpus task developed train agents follow free-form navigational route instructions maze environment consists small number mazes containing features objects wall ﬂoor types. mazes come together large number human instructions paired required actions reach goal sentence-aligned version sail route instruction dataset containing sentences following previous work accept action sequence correct ﬁnal position orientation exactly match gold data. perform pre-processing dataset. data generation argued earlier focusing tasks aligned data sparse expensive obtain cheap unsupervised monomodal data. albeit reasonable assumption real world data datasets considered component thus approach taken generate random database queries maze paths i.e. machine readable side data train semi-supervised model. alternative explored would generate natural language questions instructions instead difﬁcult achieve without human intervention. reason generate machine readable side data geoquery sail tasks. geoquery -gram kneser-ney model queries training sample million queries ensure sampled queries different training queries enforce validity. intentionally simplistic approach demonstrate applicability model. sail dataset three mazes. added fourth random paths including duplicates. maze larger existing ones seeks approximately replicate statistics three mazes paths within maze created randomly sampling start positions. finally perform ablation study discard training data compare seq. trained solely reduced data supervised manner trained semi-supervised reduced data plus machine readable part discarded data extra generated data training train model using standard gradient descent methods. none datasets used contain development sets tune hyperparameters cross-validating training data. case sail corpus train three folds report weighted results across folds following prior work geoquery evaluation metric geoquery accuracy exactly predicting machine readable query. results table show supervised baseline model performs slightly better comparable model dong lapata semi-supervised model additional generated queries improves further. open street maps report results nlmaps corpus table comparing supervised model results posted haas riezler model used semantic parsing pipeline including alignment stemming language modelling inference strong performance model demonstrates strength fairly vanilla attentionbased sequence-to-sequence models. pointed previous work reports number correct answers queries executed dataset evaluate strict accuracy generated queries. expect numbers nearly equivalent evaluation strictly harder allow reordering query arguments similar relaxations. investigate model ablation study table little gain semi-supervised objective. attempt cheaply generating unsupervised data task successful likely complexity underlying database. action. simple instruction ‘turn left’ easily translated action sequence left-stop complex instructions ‘walk forward lamp’ require knowledge agent’s position maze. accomplish modify model follows. first encoding action sequences concatenate action representation maze given position representing mazestate akin bag-of-features vector. second decoding action sequences outputs action used update agent’s position representation position next input. training regime cross-validate three mazes dataset report overall results weighted test size supervised semi-supervised model perform worse state-of-the-art latter enjoys comfortable margin former. model broadly reimplements work discrepancy performance particular design choices follow order keep model general possible comparable across tasks. ablation studies show little gain semi-supervised approach using data original training substantial improvement additional unsupervised data. supervised training prediction accuracies supervised baseline model mixed respect prior results respective tasks. geoquery performs signiﬁcantly better similar model literature mostly fact table positive negative examples latent language together randomly generated logical form unsupervised part geoquery training. note natural language occur anywhere training data form. sail corpus performs worse state art. models broadly equivalent attribute difference number taskspeciﬁc choices optimisations made reimplement sake using common model across three tasks. semi-supervised training case geoquery sail task found semisupervised model convincingly outperform fully supervised model. effect particularly notable case sail corpus performance increased accuracy worth remembering supervised training regime consists three folds tuning maps subsequent testing third carries risk overﬁtting training maps. introduction fourth unsupervised clearly mitigates effect. table shows examples unsupervised logical forms transformed natural language demonstrate model learn sensibly ground unsupervised data. ablation performance experiments additional unsupervised data prove feasibility approach clearly demonstrate usefulness model general class sequence-to-sequence tasks supervised data hard come analyse model further also look performance reducing amount supervised training data available model. compare three settings supervised model reduced training data sequses removed training data unsupervised fashion seq+ uses randomly generated unsupervised data described section model behaves expected three tasks performance dropping size training data. performance seqseq+ requires analysis. enough clearly seen original dataset used supervised training remaining used unsupervised training. shrinks amount supervised data increased expected. hand using large amount extra generated data approximating distribution help much initially compared unsupervised data true distribution. however size unsupervised dataset seqbecomes bottleneck closes eventually model trained extra data achieves higher accuracy. sail task semi-supervised models better supervised results throughout model trained randomly generated additional data consistently outperforming model trained original data. gives credence risk overﬁtting training mazes already mentioned above. finally case nlmaps corpus semi-supervised approach appear help much point ablation. indistinguishable results likely task’s complexity causing ablation experiments either little supervised data sufﬁciently ground latent space make unsupervised data higher percentages little unsupervised data meaningfully improve model. semantic parsing tasks paper broadly belong domain semantic parsing describes process mapping natural language formal representation meaning. extended sail navigation task formal representation function language instruction given environment. semantic parsing well-studied problem numerous approaches including inductive logic programming stringto-tree string-to-graph transducers grammar induction machine translation cuses deﬁning grammar logical forms models learn purely aligned pairs text logical form weakly supervised signals question-answer pairs together database recent work liang induces synchronous context-free grammar generates additional training examples address data scarcity issues. semi-supervised setup proposed offers alternative solution issue. discrete autoencoders recently related work discrete autoencoders natural language processing work marcheggiani titov presents ﬁrst approach using effectively discretised sequential information latent representation without resorting draconian assumptions make marginalisation tractable. model exactly marginalisable either continuous relaxation makes training tractable. related idea recently presented g¨ulc¸ehre monolingual data improve machine translation fusing sequence-to-sequence model language model. described method augmenting supervised sequence transduction objective autoencoding objective thereby enabling semi-supervised training previously scarcity aligned data might held back model performance. across multiple semantic parsing tasks demonstrated effectiveness approach improving model performance training randomly generated unsupervised data addition original data. going forward would interesting further analyse effects sampling logisticnormal distribution opposed softmax order better understand impacts distribution latent space. focused tasks little supervised data additional unsupervised data would straightforward reverse model train additional labelled data i.e. natural language side. natural extension would also formulation semisupervised training performed", "year": 2016}