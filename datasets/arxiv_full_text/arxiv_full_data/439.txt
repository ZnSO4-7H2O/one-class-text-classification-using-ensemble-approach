{"title": "Kernel Nonnegative Matrix Factorization Without the Curse of the  Pre-image - Application to Unmixing Hyperspectral Images", "tag": ["cs.CV", "cs.IT", "cs.LG", "cs.NE", "math.IT", "stat.ML"], "abstract": "The nonnegative matrix factorization (NMF) is widely used in signal and image processing, including bio-informatics, blind source separation and hyperspectral image analysis in remote sensing. A great challenge arises when dealing with a nonlinear formulation of the NMF. Within the framework of kernel machines, the models suggested in the literature do not allow the representation of the factorization matrices, which is a fallout of the curse of the pre-image. In this paper, we propose a novel kernel-based model for the NMF that does not suffer from the pre-image problem, by investigating the estimation of the factorization matrices directly in the input space. For different kernel functions, we describe two schemes for iterative algorithms: an additive update rule based on a gradient descent scheme and a multiplicative update rule in the same spirit as in the Lee and Seung algorithm. Within the proposed framework, we develop several extensions to incorporate constraints, including sparseness, smoothness, and spatial regularization with a total-variation-like penalty. The effectiveness of the proposed method is demonstrated with the problem of unmixing hyperspectral images, using well-known real images and results with state-of-the-art techniques.", "text": "widely used signal image processing including bioinformatics blind source separation hyperspectral image analysis remote sensing. great challenge arises dealing nonlinear formulation nmf. within framework kernel machines models suggested literature allow representation factorization matrices fallout curse pre-image. paper propose novel kernel-based model suffer pre-image problem investigating estimation factorization matrices directly input space. different kernel functions describe schemes iterative algorithms additive update rule based gradient descent scheme multiplicative update rule spirit seung algorithm. within proposed framework develop several extensions incorporate constraints including sparseness smoothness spatial regularization total-variation-like penalty. effectiveness proposed method demonstrated problem unmixing hyperspectral images using well-known real images results state-of-the-art techniques. index terms—kernel machines nonnegative matrix factorization reproducing kernel hilbert space pre-image problem hyperspectral image unmixing problem become prominent analysis technique many ﬁelds owing power extract sparse tractable interpretable representations given data matrix. scope application spans feature extraction compression visualization within pattern recognition machine learning signal image processing popularized since seung discovered that applied image able learn parts objects since then successfully applied image classiﬁcation face expression recognition audio analysis objet recognition computational biology gene expression data clustering moreover tightly connected spectral clustering also review. consists approximating nonnegative matrix low-rank nonnegative ones. allows sparse representation nonnegativity constraints often provides physical interpretation factorization thanks resulting part-based representation opposed conventional subtractive models. typically idea described issue spectral unmixing hyperspectral imagery illustrated next. hyperspectral image details scene scrutiny spectral observations electromagnetic waves emitted/reﬂected typically corresponds acquisition ground scene sunlight reﬂected. hyperspectral image consists three-dimensional data cube dimensions spatial third reﬂectance. words spectral characteristic available pixel. example aviris sensors contiguous spectral bands covering ground resolution varies spatial resolution acquired spectrum superposition spectra several underlying materials. unmixing given hyperspectral image aims extract spectra single pure materials called endmembers estimate abundance endmember every pixel i.e. every position area scrutiny. obvious abundances spectra endmembers nonnegative. provides decomposition suitable physical interpretation. physical interpretation however free. illustrate this consider well-known singular-valuedecomposition allows solve efﬁciently unconstrained matrix factorization problem orthogonality constraints risk losing physical meaning providing unique solution. known polynomial-time complexity. opposed unfortunately np-hard ill-posed problem general. fact proven nphard; also ill-posed illustrated fact decomposition unique; references therein. practice non-uniqueness issue alleviated including priors nonnegativity known sparseness smoothness constraints. first studied problem reinvented several times scilicet work paatero tapper gained popularity thanks work seung published nature many optimization algorithms proposed fig. illustration straightforward application feature space studied belong feature space elements spanned images access elements pre-images input space novel model deﬁned mapping columns matrices columns lying input space. corresponding optimization problem efﬁciently tackled directly input space thanks nature underlying kernel function. derive iterative algorithms additive update rule based gradient descent scheme multiplicative update rule spirit investigate expressions associated polynomial gaussian kernels well linear yields conventional linear nmf. based proposed framework describe several extensions incorporate constraints including sparseness smoothness well tv-like spatial regularization. relevance proposed approach extensions shown well-known hyperspectral images. also provide theoretical analysis convexity studied optimization problem connections previous work given appendix. rest paper organized follows first presented ubiquitous form demonstrating difﬁculty applying feature space. section describes proposed framework kernel-nmf. several extensions kernel-nmf developed section incorporating constraints. section illustrates relevance proposed techniques unmixing real hyperspectral images cuprite moffett. section concludes paper future work. ultiplicative update rules nonnegative least squares sparseness allows uniqueness enhances interpretation often imposed either projections ℓ-norm regularization smoothness also reduces degrees freedom typically spectral unmixing problem either using piecewise smoothness estimated endmembers favoring spatial coherence regularization similar total-variation penalty additional constraints orthogonality minimum-volume sum-to-one constraint often imposed abundances illustrated developments unmixing problem hyperspectral imagery variants based linear mixing assumption. providing nonlinear models challenging issue kernel machines offering elegant framework derive nonlinear techniques based linear ones mapping data using nonlinear function feature space applying linear algorithm mapped data idea kernel trick kernel function allows evaluate inner product transformed data without need explicit knowledge mapping function. trick allows easily extend mapping functional spaces i.e. reproducing kernel hilbert space inﬁnite dimensional spaces namely using prominent gaussian kernel. kernel machines widely used decisional tasks initially so-called support vector machines classiﬁcation regression unsupervised learning tackled kernel principal component analysis recently kernel entropy component analysis. worth noting attractive property kernel machines linear inner product kernel leads underlying conventional linear technique. recently attempts made derive kernelnmf sake nonlinear variant conventional linear model latter deﬁned writing column matrix scrutiny linear combination columns ﬁrst matrix determined second matrix deﬁned weights linear combination. deﬁning input space columns studied matrix columns mapped nonlinear transformation feature space linear model applied. unfortunately obtained results cannot exploited since columns ﬁrst unknown matrix feature space. needs back feature space input space. curse pre-image problem major drawback inherited kernel machines ﬁrst revealed denoising kpca denoised feature mapped back input space ill-posed problem yields even difﬁcult problem dealing nonnegativity result paper propose original kernel-based framework nonlinear suffer curse pre-image problem opposed techniques derived within kernel machines explore fig. illustration kernel-nmf proposed paper. opposed shown fig. proposed approach estimates elements directly input space. strategy allows overcome curse preimage problem estimating directly spectra. written matrix form here elements feature space since belongs span essentially kernel-based proposed considering model unfortunately model suffers important weakness inherited kernel machines access elements feature space inner products kernel function. fact elements feature space leads several drawbacks shown next. here left-hand-side equivalent unfortun cannot evaluated nately inner product using kernel function. circumvent difﬁculty restrict form investigated authors write terms linear combination rearranging coefﬁcients linear combination matrix problem takes form simpliﬁes optimization problem however quiet different conventional problem another downside model cannot impose nonnegativity elements feature space particular dropped. coefﬁcients nonnegative values. case longer tackle problem relaxed semi-nmf problem constraint imposed subject figure notations. former nonnegativity constraint relaxed socalled semi-nmf. optimization problem written terms nonnegative least squares optimization frobenius minae≥ norm. nonnegativity constraints estimation entries matrices convex. luckily estimation matrix separately convex optimization problem. algorithms take advantage property iterative technique alternates optimization matrix keeping ﬁxed. commonly used algorithms gradient descent rule multiplicative update rule recent survey several standard algorithms. also references therein. without loss generality illustrate problem unmixing hyperspectral imagery. case following notation spectral image decomposed spectra denote respective abundances. physical problem allows incorporate additional constraints impose structural regularity solution detailed section here denotes gradient kernel respect argument easily derived valid kernels given problem different nmf. section iii-b case linear polynomial gaussian kernels. before derive iterative algorithms solving kernel-nmf alternating estimation additive update rule ﬁrst iterative algorithm additive update rule presented solve optimization problem. based gradient descent scheme alternating followed rectiﬁcation function impose nonnegativity. normalization step impose sum-to-one constraint also used. stepsize depend expression ∇enj given impose nonnegativity matrices negative values obtained update zero. done using rectiﬁcation function entries vectors abundance vectors normalized unit ℓ-norm substituting at/katk. multiplicative update rule additive update rule simple procedure however convergence generally slow directly related stepsize value used. order overcome issues propose multiplicative update rule spirit conventional determined. estimate matrix needs solve so-called pre-image problem. ill-posed problem consists estimating input vector whose image deﬁned nonlinear close possible given element feature space words determines column solving non-convex non-linear ill-posed problem. issue obvious previous work kernelbased nmf; instance including nonnegativity constraint pre-image problem challenging problem investigated recent work attempts conducted circumvent difﬁculties. homogeneous kernel considered restricting derivation kernel argued authors; appendix details. authors approximate kernel associated nonnegative requires solve another optimization problem prior processing associated nmf. moreover pre-image problem needs solved subsequently. reasons applying nonnegative matrix factorization feature space often limited preprocessing data solving classiﬁcation problem. still access bases resulting relevant representation. next propose framework matrices exhibited without suffering curse pre-image problem. core difference approaches illustrated fig. fig. section propose novel framework derive kernel-nmf underlying model deﬁned entries input space therefore without pain solving pre-image problem. explore characteristics investigated kernel. multiplicative algorithm split corresponding gradient subtraction terms nonnegative entries. possible since matrices nonnegative well kernel values. update rule work provides framework derive extensions kernel-nmf including constraints incorporating structural information. several extensions described following constraints imposed endmembers abundances typically motivated unmixing problem hyperspectral imagery deﬁned model different constraints imposed endmembers essentially improve smoothness estimates. turns derivatives respect abundances unconstrained cost function upcoming constrained cost functions identical. thus resulting update rules estimation abundances remain unchanged detailed additive scheme multiplicative scheme. normalization at/katk considered satisfy sum-to-one constraint. compared additive rule multiplicative rule several interesting properties absence tunable stepsize parameter nonexistence rectiﬁcation function. latter property multiplicative nature ensures elements cannot become negative initializes nonnegative right-hand-side similar procedure applied estimate elements trick expression gradient always decomposed nonnegative entries. called split gradient method obvious decomposition unique. still provide multiplicative update given kernel function shown next. back conventional linear property proposed kernel-nmf framework conventional special case linear kernel used z⊤en vector input space. gradient kernel ∇enκ case. substituting result expressions additive update rules latter expression updating element-wise operations used division multiplication latter hadamard product given expressions yield well-known classical nmf. worth noting case linear kernel namely identity operator optimization problem equivalent minimization frobenius norm matrices polynomial kernel polynomial kernel deﬁned here nonnegative constant balancing impact high-order low-order terms kernel. kernel’s gradient given adopting descent gradient scheme incorporating expression ∇enj given easily modiﬁed additive multiplicative update rules endmembers estimation. corresponding expressions omitted space limitation. smoothness weighted-average regularization ansmoothness regularization raised chen cichocki aims reduce difference weighted average eln. written matrix form smoothness -norm regularization estimation interested regular solutions namely less variations e.g. less spiky property exploited so-called smoothness constraint minimizing kenk input space. combining penalty gradient penalty term respect takes additive form update rule endmembers easy derive using descent gradient method. multiplicative update rule depends used kernel expressions similar adding term denominator. satisfy physical interpretation types constraints often imposed abundances sparseness spatial regularity. turns constraints inﬂuence update rules endmembers estimation given section iii. consequence shall study detail estimation abundances. sparseness regularization sparseness proved attractive many disciplines namely penalizing ℓ-norm weight coefﬁcients typically hyperspectral unmixing problem spectrum represented using endmembers namely abundances non-zero. since latter nonnegative ant. parameter controls tradeoff reconstruction accuracy sparseness level. considering derivative jsparse respect additive update rule obtained follows easy that dealing linear kernel corresponding update rules equivalent ones given constraint input space. moreover turns smoothing feature space associated gaussian kernel makes sense since smoothness ﬂuctuation regularization virtanen imposes smoothness every endmember sense ﬂuctuations neighboring values within small. cost function kernel-nmf constraint here control spatial effect ratios left right direction. particular denotes average allocation spatial effects. considering abundance maps cost function spatially-regularized kernel-nmf fig. schematic illustration spatial regularization. represents abundance n-th endmember pixel. four neighbors imposes spatial regularization effect center pixel. spatial regularization spatial regularization favors spatial coherence essential many image processing techniques often considered literature totalvariation penalty. penalty recently studied linear unmixing problem hyperspectral imagery. motivated work derive following tvlike penalty incorporating spatial regularity within proposed framework. worth noting derivations spatial regularization viewed application abundances method given section iv-a extending one-direction smoothness twodimensional spatial regularization transforming hyperspectral image size pixels matrix t-th column ﬁlled spectrum original image following denote matrix n-th abundance deﬁned entries inner element belonging n-th abundance shall spatial regularization four geographical neighboring directions; fig. additive nonlinear one. nonlinearity deﬁned using kernel-based formulation yielding linearmixture/nonlinear-ﬂuctuation model recently generalized bilinear model considered solved using semi-nonnegative matrix factorization techniques require complete knowledge endmembers identiﬁed either n-findr vca. also considered non-kernel techniques jointly extract endmembers estimate abundances. minimum dispersion constrained integrates dispersion regularity minimizing variance endmember imposing abundance fractions every pixel converge resulting problem solved alternate projected gradient scheme. terms convex optimization convex proposed restricts basic matrix nonnegative linear combinations samples thus facilitating interpretation. furthermore compared kernel-based approaches. kernel convex-nmf kernel semi-nmf based nonnegative least squares kernelized methods corresponding respectively convexnmf alternating nonnegativity constrained least squares active method proposed curse pre-image methods studied neither endmembers represented explicitly reconstruction error evaluated. opposed methods mercer-based introduced provides comparable results. based constructing mercer kernel kernel close gaussian kernel nonnegative constraint embedded data. conventional ﬁnally performed mapped data. noteworthy learning nonnegative embedding computationally expensive. provide comparable results estimated optimal values parameters conducting experiments unconstrained kernel-nmf multiplicative scheme since latter depend stepsize parameter case additive scheme order explore inﬂuence brought different regularizations unmixing performance used parameter values case constrained extensions kernel-nmf. note number iterations experiments. case polynomial kernel used quadratic kernel since related generalized bilinear model suggested inﬂuence additive constant illustrated fig. yielding cuprite moffett scene. similar process taken determine bandwidth parameter gaussian kernel employing candidate values images. reconstruction errors shown fig. ﬁxed cuprite moffett images respectively. concerning stepsize parameter additive scheme section relevance derived kernel-nmf extensions studied real hyperspectral images. studied images well-known acquired airborne visible/infrared imaging spectrometer images consists spectral bands wavelength ranging .µm. ﬁrst image sub-image pixels taken well-known cuprite image spectral bands interest. geographic composition area known dominated muscovite alunite cuprite investigated second image moffett field image studied sub-image pixels. scene known consist three materials vegetation soil water. analysis noisy water absorption bands removed yielding spectral bands recommended introduce criteria evaluate unmixing performance. reconstruction error input space measures mean distance spectrum reconstruction using estimated endmembers abundances state-of-the-art unmixing algorithms either extract endmembers estimate abundances case solving unmixing problem requires join algorithms endmember extraction abundance estimation. proposed kernel-nmf estimates simultaneously endmembers abundances spirit recently developed algorithms following succinctly present comparing algorithms. endmember extraction often operated separately abundance estimation. commonly used techniques n-findr vertex component analysis techniques rely linear unmixing model assume existence endmembers image. convex-geometry-based techniques inﬂate simplex formed spectra endmembers correspond vertices largest simplex englobing spectra. since provide comparable results used whenever needed abundance estimation techniques. known abundance estimation technique fully constrained least squares algorithm considering linear mixing model least square technique estimates abundances nonnegativity sumto-one constraints. nonlinear unmixing estimation abundances recently investigated model terms conventional linear mixing model reconstruction error input space outperform gaussian kernel feature space. reﬂected fig. inherent nonlinear correlation cuprite image revealed using gaussian kernel recognizes three regions abundance maps; whereas linear kernel capable distinguish regions. considering reconstruction error feature space unconstrained kernel-nmf gaussian kernel surpasses counterparts linear polynomial kernels also methods including kernel-based ones. also conducted analysis different extensions. results corresponding proposed regularizations detailed fig. fig. smoothness endmembers constraints abundance maps shown fig. sparseness regularization fig. spatial regularization. paper presented kernel-based matrices estimated input space. exploring nature used kernel functions approach circumvents curse pre-image problem. additive multiplicative update rules proposed several extensions derived order incorporate constraints sparseness smoothness spatial regularity. efﬁciency techniques illustrated well-known real hyperspectral images. future work extending approach dimensionality reduction principal component analysis. kernel functions investigated well choice parameters. experiments conducted linear polynomial gaussian kernels. corresponding abundance maps algorithms shown fig. cuprite image fig. moffett kernel-nmf compared aforementioned well-known unmixing techniques presented table conventional estimation matrix separately convex suboptimization problem. convergence multiplicative update rules linear studied monotone decreasing property proved constructing auxiliary function upper bound. similarly convergence rules studied convex semi-nmf projective practically efﬁcient relevant results rules extensively considered literature including several nonlinear kernel-based formulations nmf. unfortunately convergence analysis cannot investigated auxiliary functions. proposed early work restricts case polynomial kernel considers using auxiliary function approach limiting range input data basis however proof basis update relies inequality unfortunately relation hold general. counterexamples include cases orthogonal other. framework overcomes pre-image problem rules still guarantee monotone decrease cost function nonconvexity terms arbitrary nonlinear kernel. indeed show following corresponding suboptimization problem possibly nonconvex polynomial gaussian kernels proving corresponding hessian matrix longer guaranteed positive semideﬁnite. proceeding recall following well-established results literature. function continuous partial derivatives ﬁrst second order convex hessian evaluated guillamet bressan vitria weighted non-negative matrix factorization local representations computer vision pattern recognition cvpr proceedings ieee computer society conference vol. i––i– vol.. zhang cheng learning spatially localized parts-based representation computer vision pattern recognition cvpr proceedings ieee computer society conference vol. i––i– vol.. buciu pitas application non-negative local negative matrix factorization facial expression recognition international conference pattern recognition vol. cambridge smaragdis non-negative matrix factor deconvolution; extraction multiple sound sources monophonic inputs proceedings fifth international conference independent component analysis blind signal separation granada spain sep. f´evotte bertin j.-l. durrieu nonnegative matrix factorization itakura-saito divergence application music analysis neural computation vol. sep. zheng non-negative matrix factorization based methods object recognition pattern recognition letters vol. j.-p. brunet tamayo golub mesirov metagenes molecular pattern discovery using matrix factorization proceedings national academy sciences united states america vol. gong document clustering based nonnegative matrix factorization proceedings annual international sigir conference research development informaion retrieval ser. sigir york ding relationships among various nonnegative matrix factorization methods clustering proceedings sixth international conference data mining ser. icdm washington ieee computer society cichocki zdunek phan s.-i. amari nonnegative matrix tensor factorizations applications exploratory multi-way data analysis blind source separation. wiley publishing vavasis complexity nonnegative matrix factorization siam journal optimization vol. oct. gillis nonnegative matrix factorization complexity algorithms applications ph.d. dissertation universit´e catholique louvain feb. huang sidiropoulos swami non-negative matrix factorization revisited uniqueness algorithm symmetric decomposition ieee transactions signal processing vol. jan. park nonnegative matrix factorization based alternating nonnegativity constrained least squares active method siam journal matrix analysis applications vol. jul. park sparse non-negative matrix factorizations alternating non-negativity-constrained least squares microarray data analysis bioinformatics vol. jun. qian zhou robles-kelly hyperspectral unmixing sparsity-constrained nonnegative matrix factorization ieee transactions geoscience remote sensing vol. nov. m.-d. iordache bioucas-dias plaza total variation spatial regularization sparse hyperspectral unmixing ieee transactions geoscience remote sensing vol. nov. ding peng park orthogonal nonnegative matrix tfactorizations clustering proceedings sigkdd international conference knowledge discovery data mining ser. york adal wang emge cichocki cichocki non-negative matrix factorization orthogonality constraints application raman spectroscopy journal vlsi signal processing systems signal image video technology vol. zhou yang j.-m. yang minimum-volumeconstrained nonnegative matrix factorization enhanced ability learning parts ieee transactions neural networks vol. oct. masalmah vel´ez-reyes full algorithm compute constrained positive matrix factorization application unsupervised unmixing hyperspectral imagery spie defense security symposium. international society optics photonics ngom kernel non-negative matrix factorization application microarray data analysis ieee symposium computational intelligence bioinformatics computational biology diego may. mika sch¨olkopf smola k.-r. m¨uller scholz r¨atsch kernel de-noising feature spaces proc. conference advances neural information processing systems cambridge press kanagal sindhwani rank selection low-rank matrix approximations study cross-validation nmfs low-rank methods large-scale machine learning nips workshop whistler canada december j.-m. choi multiple kernel nonnegative matrix factorization acoustics speech signal processing ieee international conference ieee cichocki choi kernel nonnegative matrix factorization spectral {eeg} feature extraction neurocomputing vol. hybrid learning machines recent developments natural computation kallas honeine richard francis amoud nonnegative pre-image machine learning pattern recognition european signal processing conference barcelona spain aug. sep. lant´eri theys richard mary regularized split gradient method nonnegative matrix factorization ieee international conference acoustics speech signal processing piper pauca plemmons gifﬁn object characterization spectral data using nonnegative factorization information theory proceedings amos technical conference chen cichocki nonnegative matrix factorization temporal smoothness and/or spatial decorrelation constraints laboratory advanced brain signal processing riken tech. halimi altmann dobigeon tourneret nonlinear unmixing hyperspectral images using generalized bilinear model ieee transactions geoscience remote sensing vol. nov. dobigeon j.-y. tourneret c.-i. chang semi-supervised linear spectral unmixing using hierarchical bayesian model hyperspectral imagery ieee transactions signal processing vol. jul. winter n-findr algorithm fast autonomous spectral endmember determination hyperspectral data algorithm fast autonomous spectral end-member determination hyperspectral data proc. spie imaging spectrometry vol. nascimento bioucas dias vertex component analysis fast algorithm unmix hyperspectral data ieee transactions geoscience remote sensing vol. apr. heinz chang fully constrained least squares linear spectral mixture analysis method material quantiﬁcation hyperspectral imagery ieee transactions geoscience remote sensing vol. mar. chen richard honeine nonlinear unmixing hyperspectral data based linear-mixture/nonlinear-ﬂuctuation model ieee transactions signal processing vol. jan. yokoya chanussot iwasaki nonlinear unmixing hyperspectral data using semi-nonnegative matrix factorization ieee transactions geoscience remote sensing vol. feb. huck guillaume blanc-talon minimum dispersion constrained nonnegative matrix factorization unmix hyperspectral data ieee transactions geoscience remote sensing vol. jun.", "year": 2014}