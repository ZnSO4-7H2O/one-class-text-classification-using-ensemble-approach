{"title": "Deep Echo State Network (DeepESN): A Brief Survey", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The study of deep recurrent neural networks (RNNs) and, in particular, of deep Reservoir Computing (RC) is gaining an increasing research attention in the neural networks community. The recently introduced deep Echo State Network (deepESN) model opened the way to an extremely efficient approach for designing deep neural networks for temporal data. At the same time, the study of deepESNs allowed to shed light on the intrinsic properties of state dynamics developed by hierarchical compositions of recurrent layers, i.e. on the bias of depth in RNNs architectural design. In this paper, we summarize the advancements in the development, analysis and applications of deepESNs.", "text": "study deep recurrent neural networks particular deep reservoir computing gaining increasing research attention neural networks community. recently introduced deep echo state network model opened extremely efﬁcient approach designing deep neural networks temporal data. time study deepesns allowed shed light intrinsic properties state dynamics developed hierarchical compositions recurrent layers i.e. bias depth rnns architectural design. paper summarize advancements development analysis applications deepesns. keywords deep echo state network deepesn reservoir computing echo state networks recurrent neural networks deep learning deep neural networks last decade reservoir computing paradigm attested state-of-the-art approach design eﬃciently trained recurrent neural networks though diﬀerent instances methodology exist literature echo state network certainly represents widely known model strong theoretical ground plethora successful applications reported literature references therein well recent works e.g. essentially esns recurrent randomized neural networks state dynamics implemented untrained recurrent hidden layer whose activation used feed static output module trained part network. paper deal extension approach deep learning framework. study deep neural network architectures temporal data processing attractive area research neural networks community investigations ﬁeld hierarchically organized recurrent neural networks showed deep rnns able develop internal states multiple time-scales representation temporal information much desired feature e.g. approaching complex tasks area speech text processing recently introduction deep echo state network model allowed study properties layered architectures separately learning aspects. remarkably studies pointed structured state space organization multiple time-scales dynamics deep rnns intrinsic nature compositionality recurrent neural modules. interest study deepesn model hence twofold. hand allows shed light intrinsic properties state dynamics layered architectures. hand enables design extremely eﬃciently trained deep neural networks temporal data. previous explicit introduction deepesn model works hierarchical models targeted ad-hoc constructed architectures diﬀerent modules trained discovery temporal features diﬀerent scales synthetic data ad-hoc constructed modular networks made multiple modules also investigated speech processing area recently advantages multi-layered networks experimentally studied time-series benchmarks area diﬀerently mentioned works studies deepesn considered following address fundamental questions pertaining true nature layering factor architectural design. basic questions essentially summarized follows stacking layers recurrent units? inherent architectural eﬀect layering rnns extend advantages depth design using eﬃciently trained approaches? paper intended draw line recent developments response mentioned research questions provide up-to-date overview progress perspectives studies deepesns presented section that section recall major characteristics deepesn model. standard shallow model deepesn composed dynamical reservoir component embeds input history rich state representation feed-forward readout part wich exploits state econding provided reservoir compute output. crucially reservoir deepesn organized hierarchy stacked recurrent layers output layer acts input next illustrated figure case time step state computation proceeds following pipeline recurrent layers ﬁrst directly external input highest reservoir architecture. notation denote external input dimension indicate number reservoir layers assume sake simplicity reservoir layer recurrent units. moreover denote external input time step state reservoir layer time step general superscript indicate item related i-th reservoir stack. time step composition states reservoir layers i.e. gives global state network. computation carried stacked reservoir deepesn understood dynamical system viewpoint input-driven discretetime non-linear dynamical system evolution global state governed state transition function ruling state dynamics layer assuming leaky integrator reservoir units layer omitting bias terms ease notation reservoir dynamics deepesn mathematically described follows. equations rnr×nu input weight matrix rnr×nr weight matrix inter-layer connections layer layer rnr×nr recurrent weight matrix layer leaking rate layer denotes element-wise applied activation function recurrent reservoir units interestingly graphically illustrated figure observe reservoir architecture deepesn characterized respect shallow counterpart interpreting constrained version standard shallow esn/rnn total number recurrent units. particular following constraints applied order obtain layered architecture mentioned constraints graphically correspond layering explicitly extensively discussed previous work point view deepesn architecture seen simpliﬁcation corresponding single-layer leading reduction absolute number recurrent weights which assuming full-connected reservoirs layer quadratic number recurrent units layer total number layers detailed points however note peculiar architectural organization inﬂuences temporal information processed diﬀerent sub-parts hierarchical reservoir composed recurrent units progressively distant external input. furthermore diﬀerently case standard esn/rnn state information transmission consecutive layers deepesn presents temporal delays. respect make following considerations figure layered reservoir architecture deepesn constrained version shallow reservoir. compared shallow case total number recurrent units stacked deepesn architecture following connections removed input reservoir levels height higher lower reservoir levels reservoir level reservoirs levels aspect sequentiality layers operation already present actually stimulated investigation intrinsic role layering hierarchically organized recurrent network architectures; choice allows model process temporal information time step deep temporal fashion i.e. hierarchical composition multiple levels recurrent units; particular notice non-linearities applied individually layer state computation allow describe deepesn dynamics means equivalent shallow system. based observations major research question naturally arises drives motivation studies reported section i.e. extent described constraints rule layered construction hierarchical representation deep recurrent models inﬂuence dynamics. standard framework reservoir parameters i.e. weights matrices left untrained initialization stability constraints given analysis echo state property deep reservoirs provided regards output computation although diﬀerent choices possible pattern connectivity reservoir layers output module typical setting consists feeding time step state reservoir layers output layer illustrated figure note choice enables readout component give diﬀerent weights dynamics developed diﬀerent layers thereby allowing exploit potential variety state representations stacked reservoir. denoting size output space typical case linear readout output time step computed brieﬂy survey recent advances study deepesn model. works described following addressing questions summarized introduction provide general support signiﬁcance deepesn also critically discussing advantages drawbacks construction. deepesn model introduced extends preliminary work analysis provided papers revealed empirical investigations hierarchical structure temporal data representations developed layered reservoir architecture deepesn. speciﬁcally stacked composition recurrent reservoir layers shown enable multiple time-scales representation temporal information naturally ordered along network’s hierarchy. besides layering proved eﬀective also enhance eﬀect known factors network design including unsupervised reservoir adaptation means intrinsic plasticity resulting eﬀects analyzed also terms state entropy memory. hierarchically structured state representation deepesns investigated means frequency analysis speciﬁcally considered case recurrent units linear activation functions. results pointed intrinsic multiple frequency representation deepesn states where even simpliﬁed linear setting progressively higher layers focus progressively lower frequencies. potentiality deep approach also exploited predictive experiments showing deepesns outperform state-of-the-art results class multiple superimposed oscillator tasks several orders magnitude. fundamental conditions related echo state property generalized case deep networks speciﬁcally study stability contractivity nested dynamical systems theoretical analysis gives suﬃcient condition necessary condition echo state property hold case deep architectures. remarkably work provides relevant conceptual practical tool deﬁnition validity usage deepesn autonomous respect standard model. study deepesn dynamics dynamical system perspective pursued provide theoretical practical framework study stability layered recurrent dynamics terms local lyapunov exponents. study also provided interesting insights terms quality developed system dynamics showing layering eﬀect naturally pushing global dynamical regime recurrent network closer stable-unstable transition condition known edge chaos. work introduced application deepesn approach ﬁeld ambient assisted living results showed deepesns able achieve state-of-the-art results real-world benchmarks related human activity recognition tasks outperforming standard networks shallow architectures. outcomes work characterize domain potential candidate fruitful exploitation potentialities deepesn approach. survey provided brief overview extension approach towards deep learning framework describing salient features deepesn model. noticeably deepesns enable analysis intrinsic properties state dynamics deep architectures i.e. study bias layering design rnns. time deepesns allow transfer striking advantages methodology case deep recurrent architectures leading eﬃcient approach designing deep neural networks temporal data. analysis distinctive characteristics dynamical properties deepesn model carried ﬁrst empirically terms entropy state dynamics system memory. then conducted abstract theoretical investigations allowed derivation fundamental conditions deep networks well characterization developed dynamical regimes terms local lyapunov exponents. also current developments already include model variants ﬁrst applications overall ﬁnal paper sumarize successive advances development analysis applications deepesn model providing document intended contain constantly updated view research topic.", "year": 2017}