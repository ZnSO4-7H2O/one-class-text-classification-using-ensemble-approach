{"title": "What Is Working Memory and Mental Imagery? A Robot that Learns to  Perform Mental Computations", "tag": ["cs.AI", "cs.NE", "I.2.0"], "abstract": "This paper goes back to Turing (1936) and treats his machine as a cognitive model (W,D,B), where W is an \"external world\" represented by memory device (the tape divided into squares), and (D,B) is a simple robot that consists of the sensory-motor devices, D, and the brain, B. The robot's sensory-motor devices (the \"eye\", the \"hand\", and the \"organ of speech\") allow the robot to simulate the work of any Turing machine. The robot simulates the internal states of a Turing machine by \"talking to itself.\" At the stage of training, the teacher forces the robot (by acting directly on its motor centers) to perform several examples of an algorithm with different input data presented on tape. Two effects are achieved: 1) The robot learns to perform the shown algorithm with any input data using the tape. 2) The robot learns to perform the algorithm \"mentally\" using an \"imaginary tape.\" The model illustrates the simplest concept of a universal learning neurocomputer, demonstrates universality of associative learning as the mechanism of programming, and provides a simplified, but nontrivial neurobiologically plausible explanation of the phenomena of working memory and mental imagery. The model is implemented as a user-friendly program for Windows called EROBOT. The program is available at www.brain0.com/software.html.", "text": "introduction paper goes back turing treats machine cognitive model \"external world\" represented memory device simple robot consists sensory-motor devices brain robot's sensory-motor devices allow robot simulate work turing machine. robot simulates internal states turing machine \"talking itself.\" stage training teacher forces robot perform several examples algorithm different input data presented tape. effects achieved robot learns perform shown algorithm input data using tape; robot learns perform algorithm \"mentally\" using \"imaginary tape.\" model illustrates simplest concept universal learning neurocomputer demonstrates universality associative learning mechanism programming provides simplified nontrivial neurobiologically plausible explanation phenomena working memory mental imagery. model implemented user-friendly program windows called erobot. program available www.brain.com/software.html. detailed theory discussed model first described eliashberg later shown dynamics working memory model could connected statistical dynamics conformations channels. paper includes following sections ection associative neural networks programmable look-up tables illustrates levels formalism replacing \"neurobiological\" model section expressed terms differential equations understandable \"psychological\" model expressed terms \"elementary procedures.\" section experiments erobot discusses educational experiments program erobot simulates robot section section also serves user's manual describing user interface program. mentioned above program available www.brain.com/software.html. turing's machine system consider cognitive system shown figure diagram illustrates vision idea turing's machine original biological interpretation reader familiar concept turing machine read part turing's original paper describes thinking invention machine. good description turing's ideas found minsky interesting mention turing used term \"computer\" refer person performing computations. performance robot figure want make model neurobiologically consistent shall computational resources reasonably postulated biological neural networks. system-theoretical background proceeding problem synthesis formulated previous section need define basic system-theoretical concepts notation needed dealing problem. reader familiar concepts still read section make sure using definitions. combinatorial machine combinatorial machine abstract system input produces output pairs symbols describing function called commands instructions productions machine equivalent machines intuitively machines equivalent cannot distinguished observing input output signals. case combinatorial machines machines equivalent input output sets output functions. instead saying equivalent also machine simulates machine vice versa. machine universal respect class combinatorial machines machine universal respect class combinatorial machines system section objects called programs machine function called output function function also called interpretation decision making procedure procedure interprets program machine makes decisions based knowledge contained program. functions f={}; f={}; f={}; f={} g={ffff}. general case exist functions n=|y| number elements m=|x| number elements programmable machine universal respect class combinatorial machines machine section programmable machine universal respect class combinatorial machines states exists memory modification procedure allows machine state corresponding combinatorial machine learning machine universal respect class combinatorial machines programmable machine section called learning machine universal respect class combinatorial machines programming procedure satisfies intuitive notion learning. note. study learning treated \"physical\" rather \"mathematical\" problem. therefore shall attempt formally define general concept learning system. instead shall design examples programmable systems programmed intuitively similar process human associative learning. example programmable logic array programmable logic array example programmable machine universal respect class combinatorial machines. general architecture shown figure system programmable and-array programmable or-array store respectively input output parts commands combinatorial machine. binary vectors stored arrays represented conductivities fuses match detectors. unconnected inputs and-gate equal connected inputs equal corresponding components vector matrices describe conductivities fuses and-array or-array respectively. convenient think vectors data stored i-th locations input long-term memory output respectively. using terminology work described follows concept originally introduced concept read-only associative memory term coined texas instruments section show much similarity basic topology topology popular associative neural networks finite-state machine finite state machine system work machine described following expressions sν+=fs yν=fy xν∈x yν∈y sν∈s values input output state variables moment respectively. note. different equivalent formalizations concept finite-state machine. formalization described known mealy machine. another popular formalization moore machine. moore machine output described function next-state. details important current purpose. practical electronic designers usually term state machine instead term finite-state machine. finite-state machine combinatorial machine one-step delayed feedback finite-state machine implemented combinatorial machine one-step delayed feedback. result obvious diagram shown figure figure combinatorial machine finite-state machine. one-step delayed feedback x\"ν=y\"ν- makes state variable machine since specify output function machine implement desired output next-state functions finite-state machine result naturally extrapolated programmable machines universal respect class finite-state machines. one-step delayed feedback gives example programmable machine universal respect class finite-state machines. often used logic designers implement state machines also possible prom implement state machines large numbers commands relatively small width input vectors. back turing's robot section intended serve tutorial finite-state automata turing machines. many good books interested reader consult information. goal illustrate general concept abstract machine connect concept notion real machine. main point keep mind useful constraints real machines formulated rather general system-theoretical level without dealing specific implementations. general constraints exist silly overcome designing \"smart\" implementations. silly invent perpetual motion machine violation energy conservation law. return robot shown figure turing machine finite-state machine coupled infinite tape. therefore able simulate turing machine robot must learning system universal respect class finite state-machines. taking account said section assuming proprioceptive feedback utter_symbol→symbol_uttered provides one-step delay sufficient system learning system universal respect class combinatorial machines. system difficult design. example shown figure addition universal data storage procedure solves problem. solution however good enough purpose. want implement neurobiologically plausible neural network model. additional requirement makes design problem less trivial educational. solving problem right position attacking main problem problem working memory mental computations neurocomputing background section provides neurocomputing background needed understanding neural model described section anatomical structure typical neuron anatomical structure typical neuron shown figure diagram depicts three main parts neuron axon provides pathway neuron sends signals neurons. signals encoded trains electrical impulses spikes generated area axon adjacent cell body called axon hillock. duration spike order -msec. length axons exceed meter. typical axon branches several times. final branches terminal fibers reach tens thousands neurons. terminal fiber ends thickening called terminal button. point contact axon neuron surface another neuron called synapse. synapses axon terminal releases chemical transmitter affects protein molecules embedded postsynaptic membrane. fifty different neurotransmitters identified present time. single neuron secrete several different neurotransmitters. width typical synaptic order neurotransmitter crosses cleft small delay order millisecond. synapses divided categories excitatory synapses increase postsynaptic potential receiving neuron; inhibitory synapses decrease potential. typical resting membrane potential order -mv. potential swings somewhere generation spike. axons form synapses. serve \"garden sprinklers\" release neurotransmitters broader areas. non-local chemical messages play important role various phenomena activation. nicholls also find useful information tutorial http//psych.athabascau.ca/html/psych/biotutorials. reasonable postulate existence quite complex computational resources level single neuron follows need single-cell complexity. rather simple model neuron described sufficient current purpose. neuron linear threshold element simple concept neuron-like computing element shown figure parts figure illustrate different graphical representations model. graphical notation used section figure input signal k-th synapse gain synapse. according postsynaptic current inet equal scalar product vectors expression excitatory inhibitory synapses positive negative gains respectively. graphical notation shown figure excitatory inhibitory synapses represented small white black circles respectively. illustrate agreement figure shows inhibitory synapse located body neuron. dynamics postsynaptic potential described first-order differential equation output signal described linear threshold function sake simplicity threshold equal zero. using c-like language representation models models going study complex traditional scientific notation. therefore forced elements computer language represent models. want avoid verbal descriptions unsupported formalism. bear believe herald morowitz's proposition \"computers biology mathematics physics.\" trying avoid computer language stick traditional mathematical notation prolong one's suffering. c-like notation assuming language widely known. safe side follows explain notation. expressions means expressions enclosed braces computed i=...n-. post-increment operator \"++\" increments value cycle computations. expression; else expression; means boolean expression true compute expression else compute expression. designing neural brain turing's robot section presents model three-layer associative neural network work machine universal respect class combinatorial machines. network pla-like architecture fact functional possibilities pla. \"analog\" neurons gives network \"extras.\" model displays effect generalization similarity mechanism random choice simulate principle probabilistic combinatorial machine similar model described eliashberg model integrates following basic ideas topological structure model consider neural network schematically shown figure circles represent neurons. small white black circles denote excitatory inhibitory synapses respectively. incoming outgoing lines neuron represent dendrites axon respectively. dendrite inhibitory synapses layer intermediate neurons synapses neurons neurons every neuron inhibits every neuron except itself. parameters competition neurons produces \"winner-take-all\" effect. inhibitory synapses neuron neurons connections provide global inhibitory input layer corresponding index. description written n=...n) instead n=...n) etc. omission cause confusion.) notation input excitatory synapses neuron output excitatory synapses neuron. referred \"engineering\" notation. type notation used programmable logic devices notation connection represented line nodes referred \"connectionist\" notation. notation borrowed graph theory commonly used \"connectionist\" models. section demonstrate advantages \"engineering\" notation \"connectionist\" notation. functional model section presents functional model corresponding topological model figure topological model many different functional models associated models. notation x=..x) vector output signals neurons input vector model. y=..y) vector output signals neurons output vector model. gain synapse vector treated contents i-th location input model. transposed compared gain synapse vector treated contents i-th location output model. x_inh output signal \"nonspecific\" signal provides global inhibitory input model. beta absolute value gain synapse equal synapse inhibitory gain equal -beta. diagonal gains equal zero. input current neuron neurons output neuron postsynaptic potential neuron time constant neuron noise fluctuations postsynaptic current continuous time. output neuron equal scalar product output neuron linear threshold function saturation dependence postsynaptic potential postsynaptic current described model presented described c-function-like format braces indicating boundaries model. style comments \"//\" give model appearance computer program. implementations winner-take-all layer figures show possible implementations winner-take-all layer described expressions section topological model figure referred inhibit-everyone-but-itself implementation. topological model figure called inhibit-everyone-and-excite-itself implementation. alpha=beta functional models corresponding topological models mathematically equivalent. absolute value positive feedback gain alpha equal absolute value negative feedback gain beta model figure slightly richer properties model figure model figure studied eliashberg model figure studied eliashberg cases systems differential equations describing dynamics models explicit solutions number neurons properties model follows describe properties model rigorously proved. model include description learning procedure assume desired matrices preprogrammed. pair called program model ann. logic function inputs outputs exists program model program implements function. model work pla. input encoded vector done pla. example represent represent <noise f=sum finite normalized real positive n-vectors pair equal finite positive n-vectors. probabilistic combinatorial machine input alphabet output alphabet probability function pxxy→ probability output input respectively. assume rational values non-negative integer positive integer. apply periodic inhibition x_inh. global inhibitory input resets layer cycle random choice prepares next cycle. analytical solution equations section presented eliashberg solution allows understand layer works. \"connectionist\" notation \"engineering\" notation goal section show well known neural network models essentially pla-like topology network figure look similar network \"connectionist\" notation. switching \"engineering\" notation reveals similarity. \"top-down traces\" similar output synaptic matrix figure or-array figure blocks shown figure interest current discussion. main issue represented brain information stored accessed retrieved. \"local\" \"distributed\" dealing \"local\" representation data network figure \"one neuron memory location.\" feldman's terminology \"local\" approach referred \"grandmother cell\" approach. beta< layer longer works winner-take-all mechanism. instead produces effect contrasting selects several locations output superposition vectors sent output model ann. longer treat model \"local\" associative memory. assume output neuron sigmoid function postsynaptic potential also completely turn reciprocal inhibition setting beta=. model becomes traditional three-layer \"connectionist\" neural network shown \"connectionist\" notation figure current paper interested \"local\" case corresponding beta>. \"distributed\" case becomes important models hierarchical structure associative memory associative neural networks programmable look-up tables section discusses discrete-time counterpart continuous-time neural model described previous section. convenient view discrete-time system programmable look-up table transformed dynamical bias introducing states residual excitation leads concept primitive e-machine. model assume input vectors model changing step-wise time-step ∆t>>tau. beta> layer performs random winner-take-all choice. x_inh provide periodic inhibition needed reset layer step. exact values parameters important current discussion. step-wise mode operation network figure replaced programmable \"look-up table\" schematically shown figure functional model presented referred model model described composition following blocks data stored output ltm. simplest case ym=gy. output vector read location output selected block choice. model block implemented synaptic matrix neurons learning. block shown figure calculates next values model block described all. model described procedural terms without neural interpretation. possible neural implementations different learning algorithms discussed chapter ote. discouraged simplicity \"dumb\" learning algorithm described exp. theoretically universal powerful learning procedure possible practically size required memory grows linearly time. since memory addressed content decision making time increase much increase length recorded xy-sequence. easy improve \"dumb\" learning algorithm make less \"memory hungry.\" first obvious improvement \"selection novelty\". program erobot user select learning modes storing xy-sequence storing xypairs. correct decoding condition specify similarity function. combination input encoding similarity function work long combination satisfies following correct decoding condition. model work \"psychological\" model much easier understand work \"neurobiological\" model ann. nevertheless information processing possibilities model essentially model longer need talk neurons synapses treat model programmable look-up table effect \"generalization similarity\". heuristically important however keep mind relationship model model ann. respectively value input output vector model time value array postsynaptic potentials neurons time state model ann. state model ann. respectively output function next-state function expanding structure model introducing e-states simplicity model room development. important developments introduction \"psychological\" stm. term \"psychological\" means duration memory must longer psychological time step states memory referred study states \"residual excitation\" e-states. e-states model possibility connecting dynamics postulated phenomenological e-states statistical dynamics conformations protein molecules neural membranes discussed eliashberg effect imitation. sensory image sequence reactions \"pre-activates\" sequence. effect allows synthesis complex motor reactions presenting sensory images. start \"bubbling\" create complex sequences. explains complex reactions learned without teacher's acting directly learner's motor centers excitation. block. receives similarity front input produces front \"biased similarity\" output. \"bias\" associated e-states mentioned previous section. functional model described work block described procedures e-states different types dynamics. simple model sufficient current purpose. explained next section spite simplicity model produces effect read/write \"symbolic\" working memory allows robot section learn perform mental computations. main idea understood critically important effect produced many different ways. know \"nonspecific\" computational procedures naturally integrated models primitive e-machines. methodologically separate problem implement computational procedures neural models. model_af note. program erobot uses slightly complex data storage procedure described exp. allow user erase reuse parts robot's memory recording done first \"empty\" location. non-empty locations skipped. also exp. parameter allows user switch \"teacher\" mode \"memory\" mode. robot learns perform mental computations section enhances structure cognitive model shown figure give robot ability learn perform mental computations. general structure compare cognitive model shown figure model shown figure model figure following enhancements goal system simulate external system appears system interaction \"processor\" \"memory\" creates universal computing architecture perform principle computations. ote. primitive e-machine described section used system trivial primitive e-machine section used system effect read/write working memory buffer system achieved automatically implication e-state current model system need estates. robot's open output equal output eye. otherwise output equal output closed automatically gets input program erobot opening closing controlled user. complex model done system utter_symbol causes robot utter symbol representing internal state turing machine internal symbols. includes symbol causes turing machine halt. tape symbol i-th square tape i=.... i_scan {...} position scanned square symbol_uttered one-step-delayed input utter_symbol symbol_written one-step-delayed input write_symbol model describing coordinated work several blocks complete working functional model whole cognitive system shown figure needs connect blocks shown figure. simplicity formally describe connections assuming sufficiently clear figure simple case descriptions blocks included descriptions blocks complex cases convenient describe nuclei separate blocks. note signals play role signals teacher play point also explicitly describe timing details associated coordinated work blocks. easy solve timing problems computer simulation computations associated different blocks right order. timing details however become critically important addresses problem \"analog neural implementation\" complex e-machines composed several primitive e-machines nuclei including various feedback loops. interesting complex neurodynamical problem discussed paper. vast unexplored world sophisticated neurodynamical problems. experiments erobot program best understand robot figure works experiment program erobot. program available www.brain.com/software.html. following sections assume acquired program running computer. user interface program first time four windows shown screen. windows resizable rearrange liking. want preserve arrangement file menu select save default item. next time program start arrangement. leave windows displayed. windows following titles associative filed sensory nucleus figure associative field forms ms→s associations learns simulate external system long table right displays contents input output shorter table left displays input output signals. edit names signals clicking names. upper control from tape memory determines input signals coming from. lower control learn none switches learning mode. click desired mode activate color corresponds active mode. symbols entered right table table scrolled. click left mouse button square place yellow cursor square. enter desired character keyboard. empty square press space bar. backspace keys work. table store associations. scroll table toward higher addresses press move yellow cursor pressing key. scroll toward lower addresses press move yellow cursor pressing key. press table home return beginning table. note. keys work window selected clicking left button inside table. motor nucleus figure associative field forms sm→m associations learns simulate teacher. editing scrolling functions learn controls similar window. control teacher position user enter symbols lower three squares right column input/output table. click left mouse button squares position yellow cursor square. enter symbol keyboard. control memory position cannot enter symbols squares \"motor\" symbols read memory. force robot's motor reactions teacher mode. external system tape hand speech organ window corresponds external system figure displays current state tape tape history tape squares long. tape history stores previous states tape trace performance turing machine last steps. edit tape click left mouse button desired square. yellow cursor positioned square. enter symbol keyboard. green cursor represents scanned square. position cursor click right mouse button desired square. yellow cursor positioned history area current tape edited. scroll tape left right press keys respectively move yellow cursor pressing keys. scroll history table press pgup pgdn keys move yellow cursor pressing keys. home returns user beginning current tape. displays tape. leftmost column displays discrete time next columns display command executed block step. bottom right part window. displayed read locations displayed green displayed magenta. control displays time constant decay block value changed clicking number. step computations without affecting state tape without incrementing time. button used robot desired initial state. pressing produces effect. pressing step button performs complete cycle one-step computations. state tape time changed tape history scrolled. truth= truth=. blank character represents signals.\" denominator equal zero easy maximum value similarity many similarity functions would work well. coefficient bm=. \"additive\" biasing coefficient ba=. section ..). case bm=ba= accordingly value time constant needed block e-state front displayed block example computing external tape examples menu select example commands representing turing machine loaded block turing machine parentheses checker similar described minsky tape shows parentheses expression turing machine check. symbols sides parentheses expression serve delimiters indicating expression boundaries. green cursor indicating scanned square square note symbol_uttered='' showing turing machine initial \"state mind\" represented symbol symbol tape indicating checked parentheses expression correct left parenthesis matching right parenthesis. experiment program. enter parentheses expression. click left button desired square place yellow cursor square type parenthesis. forget place symbols sides expression. click right button square position green cursor square. yellow cursor also square square become blue. system initial state block click teacher. position yellow cursor square right name utter_symbol enter square. note memory mode cannot enter symbol. symbol entered press init button key. initial state symbol_uttered=''. return memory mode press step button another round computations. mode example robot performs mental computations. teaching robot parentheses checking write twelve commands parentheses checker clear block pressing button. following experiment teach robot entering output parts commands response input parts commands. input parts displayed upper squares xycolumn block block must teacher mode block tape mode. teach robot twelve commands parentheses checker sufficient following three training examples write first expression tape place green cursor square utter_symbol='' press init button. block learn mode start pushing step button key. commands recorded block repeat teaching experiment training examples. everything correctly twelve commands ltm. robot perform parentheses checker algorithm parentheses expression. example performing mental computations examples menu select example blocks programs ltm's. next section explain program block created result learning. sufficient mention program allows block simulate work external system tape containing squares external alphabet {'a''''x''t''f'}. read/write working memory limited duration depending time constant tau. bigger time constant longer memory lasts. qualitative theory effect described eliashberg study discussed chapter want show \"working memory\" works. interestingly enough block longer needs ltm. effect read/write working memory achieved without moving symbols simply changing levels \"residual excitation\" already stored associations. block programs. program locations parentheses checker used previous sections. program locations tape scanner. program starts state block memory mode program run. block tape mode robot tape. start pressing step button robot scans tape. first scans right reaches right boundary goes back. reaches left boundary moves square changes \"state mind\" transfers control parentheses checker moment close robot's clicking left button word memory block make sure word became red. continue pressing step button robot performs mental computations. note. block learn none mode. could learn mode. case would keep recording xysequence. performance would change. effect read/write working memory combined ability remember everything. highest level \"residual excitation among competing associations.) teaching block simulate external system examples menu select example block programs ltm. program locations program scans tape rewrites program starts state state rewrites symbol_read goes state state moves step right goes state program pressing step button program doing. note block learn mode records xy-sequence produced external system sufficient learn simulate system tape containing squares single external symbol {'a'}. prepare tape symbol external symbols {'a''''x''t''f'}. block state identical used example switch block learn none mode. write parentheses expression squares tape. forget enter delimiters sides expression. whole expression including delimiters must squares taught block simulate tape squares. green cursor representing scanned square square clicking right button square. test program locations need initial state state switch teacher mode enter utter_symbol='' press init button key. switch back memory mode program pressing step button key. repeating example trained block yourself. note. could train test block teacher mode done discussed programs. write training testing programs block memory mode. switching \"external world\" \"imaginary world\" select example examples menu. block four output channels. fourth output channel controls closing opening robot's eye. example pressing step button key. block switches memory mode scanning done parentheses checking starts. moment robot performs mental computations. interesting thing example robot decides switch \"external world\" \"imaginary world.\" switching affect robot's performance mental imagery predicts results robot's actions \"external world.\" make effect obvious select example robot switches \"external world\" \"imaginary world\" several times performing computations. computational universality human brain result interaction associative learning systems \"processor\" system responsible motor control \"memory\" system responsible working memory mental imagery. intentionally used terms \"processor\" \"memory\" emphasize similarity general \"processor-memory\" architecture traditional universal programmable computer. difference that discussed model systems arranged similar principle \"processor\" \"memory\" learning systems create software almost completely course learning. another difference \"processor\" switching back forth \"real\" world \"imaginary world\" serves internal \"memory.\" remark. identify \"processor\" part wondering infinite loop switching real imaginary worlds. notion infinite loop allows theory notion \"homunculus.\" need \"homunculus\" result lack universality. model learn principle anything cognitive theory associated model need \"homunculus.\" sufficient memorize sm→m ms→s associations produced performing several examples computations. universal learning architecture associations serve software allows system perform demonstrated algorithm input data. note. effect universal programming associative learning needed dedicate \"free\" motor channel representation internal states finite-state part simulated turing machine. erobot achieved \"speech\" motor channel. metaphor sheds light important role language. brain without language cannot achieve highest level computing power. another interesting implication erobot metaphor sufficiently expressive \"free\" motor channel serve language channel. similar mental computations? experiment discussed section provides answer question. \"processor\" interacts external system \"memory\" system learns ms→s associations allowing simulate external system trained erobot perform mental computations switching \"imaginary\" system simplest universal learning algorithm consistent questions memorizing xy-experience. erobot shows \"dumb\" algorithm combined associative memory data addressed parallel. time decoding depend size memory time choice encoding worse log. mentioned section many ways make algorithm less \"memory hungry\" efficient. note. \"smart\" learning algorithm universal. optimizes performance given context throws away information needed large number contexts. argue learning algorithm type cannot employed human brain. effect read/write symbolic working memory achieved erobot \"dynamical\" e-state without moving symbols \"symbolic\" ltm. working memory read/write memory buffer. many effects associated dynamic reconfiguration data stored ltm. metaphor brain e-machine suggests dynamic reconfiguration associated postulated phenomenological e-states. effect working memory produced system work \"imaginary\" external system explains nature functional importance mental imagery. neural network figure gives answer question. connection phenomenological e-states statistical conformational dynamics ensembles protein molecules discussed eliashberg whither several possibilities development concept universal learning neurocomputer illustrated erobot possibilities include introduction e-states motor control system enhancement produces several critically important effects mentioned section sequence. effect allows synthesis complex motor reactions presenting sensory images. start \"bubbling\" create complex motor sequences. explains complex reactions learned without teacher's acting directly learner's motor centers introduction \"centers emotion\" activating system besides sm→m ms→s associations employed erobot human brain forms associations modalities important \"emotional\" modality. remember recognize emotional states. means brain recognizable signals carry information states. letter denote \"emotional\" modality. assume besides associative learning systems responsible respectively motor control mental imagery also associative learning system call responsible motivation. system forms sh→h associations serve \"motivational software.\" approach produce much sophisticated effect \"reward\" \"punishment\" available traditional models reinforcement learning. latter models effect reinforcement limited modification sm→m associations. architecture sh→h associations interact sm→m associations activating system effect activation depending sensory inputs temporal context. creates complex interesting situation. introduction hierarchical structure primitive e-machines slightly complex model described section arranged hierarchical structures. corresponding complex e-machines produce various effects data compression statistical filtering generalization. effects type demonstrated eliashberg introduction \"bios\" biggest advantage \"whole-brain\" metaphor \"the brain e-machine\" compared traditional \"partial\" brain modeling metaphors former metaphor room arbitrarily complex \"brain software.\" traditional learning algorithms create \"neural firmware.\" firmware however compared firmware programmable logic devices rather software universal programmable computer. believe \"initial brain's firmware\" longest part description untrained human brain \"bios\" include initial \"motivational software\" determines direction learning initial \"motor software\" also initial software representing initial knowledge external world", "year": 2003}