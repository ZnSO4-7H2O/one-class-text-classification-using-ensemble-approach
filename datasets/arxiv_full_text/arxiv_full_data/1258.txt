{"title": "BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "Deep neural networks are state of the art methods for many learning tasks due to their ability to extract increasingly better features at each network layer. However, the improved performance of additional layers in a deep network comes at the cost of added latency and energy usage in feedforward inference. As networks continue to get deeper and larger, these costs become more prohibitive for real-time and energy-sensitive applications. To address this issue, we present BranchyNet, a novel deep network architecture that is augmented with additional side branch classifiers. The architecture allows prediction results for a large portion of test samples to exit the network early via these branches when samples can already be inferred with high confidence. BranchyNet exploits the observation that features learned at an early layer of a network may often be sufficient for the classification of many data points. For more difficult samples, which are expected less frequently, BranchyNet will use further or all network layers to provide the best likelihood of correct prediction. We study the BranchyNet architecture using several well-known networks (LeNet, AlexNet, ResNet) and datasets (MNIST, CIFAR10) and show that it can both improve accuracy and significantly reduce the inference time of the network.", "text": "fig. simple branchynet branches added baseline alexnet. ﬁrst branch convolutional layers second branch convolutional layer. exit boxes denote various exit points branchynet. ﬁgure shows general structure branchynet branch consists layers followed exit point. practice generally necessary multiple convolutional layers branch order achieve good performance. main branch original baseline neural network allow certain test samples exit early. novel architecture exploits observation often case features learned earlier stages deep network correctly infer large subset data population. exiting samples prediction earlier stages thus avoiding layer-by-layer processing layers branchynet signiﬁcantly reduces runtime energy inference majority samples. figure shows branchynet modiﬁes standard alexnet adding branches respective exit points. abstract—deep neural networks state methods many learning tasks ability extract increasingly better features network layer. however improved performance additional layers deep network comes cost added latency energy usage feedforward inference. networks continue deeper larger costs become prohibitive real-time energy-sensitive applications. address issue present branchynet novel deep network architecture augmented additional side branch classiﬁers. architecture allows prediction results large portion test samples exit network early branches samples already inferred high conﬁdence. branchynet exploits observation features learned early layer network often sufﬁcient classiﬁcation many data points. difﬁcult samples expected less frequently branchynet network layers provide best likelihood correct prediction. study branchynet architecture using several well-known networks datasets show improve accuracy signiﬁcantly reduce inference time network. reasons success deep networks ability learn higher level feature representations successive nonlinear layers. recent years advances hardware learning techniques emerged train even deeper networks improved classiﬁcation performance imagenet challenge exempliﬁes trend deeper networks state methods advanced layers layers layers span four years however progression towards deeper networks dramatically increased latency energy required feedforward inference. example experiments compare vggnet alexnet titan shown factor increase runtime power consumption reduction error rate around trade resource usage efﬁciency prediction accuracy even noticeable resnet current state method imagenet challenge order magnitude layers vggnet. rapid increase runtime power gains accuracy make deeper networks less tractable many real world scenarios real-time control radio resources next-generation mobile networking latency energy important factors. exit points. network trained branchynet utilizes exit points allow samples exit early thus reducing cost inference. exit point branchynet uses entropy classiﬁcation result measure conﬁdence prediction. entropy test sample learned threshold value meaning classiﬁer conﬁdent prediction sample exits network prediction result exit point processed higher network layers. entropy value threshold classiﬁer exit point deemed conﬁdent sample continues next exit point network. sample reaches last exit point last layer baseline neural network always performs classiﬁcation. fast inference early exit branches branchynet exits majority samples earlier exit points thus reducing layer-by-layer weight computation costs resulting runtime energy savings. regularization joint optimization branchynet jointly optimizes weighted loss exit points. exit point provides regularization others thus preventing overﬁtting improving test accuracy. mitigation vanishing gradients early exit points provide additional immediate gradient signal back propagation resulting discriminative features lower layers thus improving accuracy. lenet- introduced standard convolutional neural networks structure composed stacked convolutional layers optionally followed contrast normalization maxpooling ﬁnally followed fully-connected layers. structure performed well several image tasks image classiﬁcation. alexnet resnet others expanded structure innovative approaches make network deeper larger improved classiﬁcation accuracy. computational costs deep networks improving efﬁciency feedforward inference heavily studied. approaches network compression implementation optimization. network compression schemes reduce total number model parameters deep network thus reduce amount computation required perform inference. bucilua proposed method compressing deep network smaller network achieves slightly reduced level accuracy retraining smaller network synthetic data generated deep network recently proposed pruning approach removes network connections small contributions however pruning approaches signiﬁcantly reduce number model parameters layer converting reduction signiﬁcant speedup difﬁcult using standard implementations lack high degrees exploitable regularity computation intensity resulting sparse connection structure tucker decomposition extract shared information convolutional layers perform rank selection approach reduces number network parameters making network compact cost small amount accuracy loss. network compression methods orthogonal branchynet approach taken paper could potentially used conjunction improve inference efﬁciency further. implementation optimization approaches reduce runtime inference making computation algorithmically faster. vanhoucke explored code optimizations speed execution convolutional neural networks cpus mathieu showed convolution using used speed training inference cnns recently lavin introduced faster algorithms speciﬁcally convolutional ﬁlters contrast branchynet makes modiﬁcations network structure improve inference efﬁciency. deeper larger models complex tend overﬁt data. dropout regularization many techniques used regularize network prevent overﬁtting. additionally szegedy introduced concept adding softmax branches middle layers inception module within deep networks regularize main network also providing similar regularization functionalities branchynet goal allowing early exits test samples already classiﬁed high conﬁdence. main challenge deep neural networks vanishing gradient problem. several papers introduced ideas mitigate issue including normalized network initialization intermediate normalization layers recently approaches highway networks resnet deep networks stochastic depth studied. main idea skip connections layers. skip connection identity function helps propagate gradients backpropagation step neural network training. panda propose conditional deep learning iteratively adding linear classiﬁers convolutional layer starting ﬁrst layer monitoring output decide whether sample exited early. branchynet allows general branch network structures additional layers exit point uses cascade linear classiﬁers convolutional layer. addition jointly train classiﬁer original network. observed paper jointly training branch original network signiﬁcantly improve performance overall architecture compared cdl. branchynet modiﬁes standard deep network structure adding exit branches certain locations throughout network. early exit branches allow samples fig. branchynet fast inference algorithm. input sample vector n-th entry threshold determining whether exit sample n-th exit point number exit points network. perform fast inference given branchynet network follow procedure described figure procedure requires vector n-th entry threshold used determine input exit algorithm consists steps feedforward pass backward pass. feedforward pass training data passed network including main side branches output neural network exit points recorded error network calculated. backward propagation error passed back network weights updated using gradient descent. gradient descent adam algorithm though variants stochastic gradient descent also used. trained branchynet used fast inference classifying samples earlier stages network based algorithm figure classiﬁer exit point branch high conﬁdence correctly labeling test sample sample exited returns predicted label early computation performed higher branches network. entropy measure conﬁdent classiﬁer exit point sample. entropy deﬁned accurately classiﬁed early stages network exit stage. training classiﬁers exit branches also consider network regularization mitigation vanishing gradients backprogation. former branches provide regularization main branch vice versa. latter relatively shallower branch lower layer provide immediate gradient signal backpropagation resulting discriminative features lower layers main branch thus improving accuracy. designing branchynet architecture address number considerations including locations branch points structure branch well size depth classiﬁer exit point branch exit criteria branch associated test cost criteria training classiﬁers exit points branches. general branch notion recursively applied branch branches resulting tree structure. simplicity paper focus basic scenario one-level branches nested branches meaning tree branches. branchynet network consists entry point exit points. branch subset network containing contiguous layers overlap branches followed exit point. main branch considered baseline network side branches added. starting lowest branch moving highest branch number branch associated exit point increasing integers starting one. example shortest path entry point exit exit illustrated figure classiﬁcation task softmax cross entropy loss function commonly used optimization objective. describe branchynet uses loss function. one-hot ground-truth label vector input sample possible labels. objective function written n-th exit point. section discuss thresholds set. procedure begins lowest exit point iterates highest ﬁnal exit point network. exit point input sample corresponding branch. procedure calculates softmax entropy output checks entropy exit point threshold entropy less class label maximum score returned. otherwise sample continues next exit point. sample reaches last exit point label maximum score always returned. section demonstrate effectiveness branchynet adapting three widely studied convolutional neural networks image classiﬁcation task lenet alexnet resnet. evaluate branchy-lenet mnist dataset branchy-alexnet branchy-resnet cifar data set. present evaluation results gpu. .ghz cache nvidia geforce titan gpu. simplicity describe convolutional fullyconnected layers network. generally networks also contain pooling non-linear activation functions normalization dropout. lenet- consists convolutional layers fully-connected layers branch consisting convolutional layer fully-connected layer ﬁrst convolutional layer main network. alexnet consists convolutional layers fully-connected layers branches. branch consisting convolutional layers fully-connected layer added convolutional layer main network another branch consisting convolutional layer fully-connected layer added convolutional layer main network. resnet- consists convolutional layers fully-connected layer branches. branch consisting convolutional layers fully-connected layer added convolutional layer main network second branch consisting convolutional layers fully-connected layer added convolutional layer main network. initialize b-lenet b-alexnet b-resnet weights trained lenet alexnet resnet respectively. found initializing branchynet network weights trained baseline network improved classiﬁcation accuracy network several percent random initialization. train networks adam algorithm step size exponential decay rates ﬁrst second moment estimates respectively. performance results branchynet applied network. networks branchynet outperforms original baseline network. reported runtime average among test samples. blenet largest performance gain efﬁcient branch achieves almost level accuracy last exit branch. alexnet resnet performance gain still substantial since samples required exit last layer smaller b-lenet. knee point denoted green star represents optimal threshold point accuracy branchynet comparable main network inference performed signiﬁcantly faster. b-resnet accuracy slightly lower baseline. different threshold could chosen gives accuracy higher resnet much less savings inference time. performance characteristics branchynet running follow similar trend performance branchynet running gpu. table highlights selected knee threshold values exit gain speed branchynet network gpu. column denotes threshold values exit branch. since last exit branch must exit samples require exit threshold. therefore -branch network b-lenet single value -branch network b-alexnet b-resnet values. analysis sensitivity parameters discussed section exit column shows percentage samples exited branch point. networks branchynet able exit large percentage test samples last layer leading speedups inference time. b-lenet exits samples ﬁrst exit branch b-alexnet b-resnet exit respectively. exiting samples early translate cpu/gpu speedup gains ./.x lenet ./.x alexnet ./.x resnet. branch structure b-resnet mimics b-alexnet. important hyperparameters branchynet weights joint optimization exit thresholds fast inference algorithm described figure selecting weight branch observed giving weight early branches improves accuracy later branches added regularization. fig. performance results branchynet applied lenet alexnet resnet. original network accuracy runtime shown diamond. branchynet modiﬁcation network shown blue. point denotes different combinations entropy thresholds branch exit points star denotes knee point curve additional analysis shown table performance results similar characteristics also found table runtime measured milliseconds inference sample. evaluation batch size evaluated order target real-time streaming applications. larger batch size allows parallelism lessens beneﬁt early exit samples batch must exit batch processed. fig. overall classiﬁcation accuracy b-alexnet varying entropy threshold ﬁrst exit branch. experiment samples exited ﬁrst branch exited ﬁnal exit. entropy given value entropy samples point. simpliﬁed version branchyalexnet ﬁrst last branch weighting ﬁrst branch last branch provides increase classiﬁcation accuracy weighting branch equally. giving weight earlier exit branches encourages discriminative feature learning early layers network allows samples exit early high conﬁdence. figure shows choice affects number samples exited ﬁrst branch point b-alexnet. observe entropy value distinctive knee rapidly becomes less conﬁdent test samples. thus case relatively easy identify knee learn corresponding threshold. practice choice exit threshold exit point depends applications datasets. exit thresholds chosen satisﬁes inference latency requirement application maintaining required accuracy. additional hyperparameter mentioned explicitly location branch points network. practice location ﬁrst branch point depends difﬁculty dataset. simpler dataset mnist place branch directly ﬁrst layer immediately accurate classiﬁcation. challenging datasets branches placed higher order still achieve strong classiﬁcation performance. additional branches currently place equidistant points throughout network. future work derive algorithm optimal placement locations branches automatically. results shown figure provides accuracy runtime range values. values show branchynet trades accuracy faster runtime entropy thresholds increase. however practice want automatically speciﬁed runtime accuracy constraint. approach simply screen additionally possible meta-recognition algorithm estimate characteristics unseen test samples adjust automatically order maintain speciﬁed runtime accuracy goal. simple approach creating meta-recognition algorithm would train small multilayer perceptron corresponding exit point output softmax probability vectors exit. exit point would attempt predict given sample would correctly classiﬁed speciﬁc exit. generally approach closely related open world recognition problem interested quantifying uncertainty model particular unseen test samples. expand approach using different formulation softmax openmax attempts quantify uncertainty directly probability vector adding additional uncertain class. approaches could used tune automatically test estimating difﬁculty test data adapting accordingly meet runtime accuracy constraints. work outside scope paper provides groundwork branchynet architecture explored future work. figure shows impact accuracy last exit adding additional convolutional layers earlier side branch modiﬁed version b-alexnet ﬁrst side branch. optimal number layers improve accuracy main branch adding many layers actually harm overall accuracy. addition convolutional layers adding fully-connected layers convolutional layers branch also proves helpful since allows local global features combine form discriminative features. number layers branch size exit branch chosen overall size branch less amount computation needed exit later exit point. generally earlier branch points layers later branch points fewer layers. since majority samples exited early branch points later branches used rarely. allows weights early exit branches cached efﬁciently. figure shows effect cache based various values b-alexnet. aggressive values faster runtime also less cache miss rates. could insight select branch structure effectively cache potentially speeding inference further. proposed branchynet novel network architecture promotes faster inference early exits branches. proper branching structures exit criteria well joint optimization loss functions exit points architecture able leverage insight many test samples correctly classiﬁed early therefore need later network layers. evaluated approach several popular network architectures shown branchynet reduce inference cost deep neural networks provide speed gpu. branchynet toolbox researchers deep network models fast inference. branchynet used conjunction prior works network pruning network compression branchynet adapted solve types problems image segmentation limited classiﬁcation problems. future work plan explore meta-recognition algorithms openmax automatically adapt test samples. glorot bengio. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics pages zhang sun. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. proceedings ieee international conference computer vision pages panda sengupta roy. conditional deep learning design energy-efﬁcient enhanced pattern recognition. automation test europe conference exhibition pages ieee scheirer rocha micheals boult. metarecognition theory practice recognition score analysis. ieee transactions pattern analysis machine intelligence simonyan zisserman. deep convolutional networks large-scale image recognition. arxiv preprint arxiv. srivastava hinton krizhevsky sutskever salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research szegedy sermanet reed anguelov erhan vanhoucke rabinovich. going deeper proceedings ieee conference computer convolutions. vision pattern recognition pages", "year": 2017}