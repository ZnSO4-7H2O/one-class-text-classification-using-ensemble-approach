{"title": "Enhanced Image Classification With a Fast-Learning Shallow Convolutional  Neural Network", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "We present a neural network architecture and training method designed to enable very rapid training and low implementation complexity. Due to its training speed and very few tunable parameters, the method has strong potential for applications requiring frequent retraining or online training. The approach is characterized by (a) convolutional filters based on biologically inspired visual processing filters, (b) randomly-valued classifier-stage input weights, (c) use of least squares regression to train the classifier output weights in a single batch, and (d) linear classifier-stage output units. We demonstrate the efficacy of the method by applying it to image classification. Our results match existing state-of-the-art results on the MNIST (0.37% error) and NORB-small (2.2% error) image classification databases, but with very fast training times compared to standard deep network approaches. The network's performance on the Google Street View House Number (SVHN) (4% error) database is also competitive with state-of-the art methods.", "text": "abstract—we present neural network architecture training method designed enable rapid training implementation complexity. training speed tunable parameters method strong potential applications requiring frequent retraining online training. approach characterized convolutional ﬁlters based biologically inspired visual processing ﬁlters randomly-valued classiﬁer-stage input weights least squares regression train classiﬁer output weights single batch linear classiﬁer-stage output units. demonstrate efﬁcacy method applying image classiﬁcation. results match existing state-of-the-art results mnist norb-small image classiﬁcation databases fast training times compared standard deep network approaches. network’s performance google street view house number database also competitive state-of-the methods. state-of-the-art performance many image classiﬁcation databases achieved recently using multilayered neural networks performance generally relies convolutional feature extraction stage obtain invariance translations rotations scale training deep networks however often requires signiﬁcant resources terms time memory computing power tasks require online learning periodic replacement network weights based fresh data thus able beneﬁt deep learning techniques. desirable therefore seek rapid training methods even potentially expense small performance decrease. recent work shown good performance image classiﬁcation tasks achieved ‘shallow’ convolutional networks—neural architectures containing single training layer—provided sufﬁciently many features extracted perhaps surprisingly performance arises even entirely random convolutional ﬁlters ﬁlters based randomly selected patches training images although application relatively large numbers ﬁlters common good classiﬁcation performance also obtained sparse feature representation produces state-of-the-art results well known image classiﬁcation databases; trainable times order minutes standard desktop/laptop computers; sufﬁciently versatile hyper-parameter sets applied different datasets still produce results comparable dataset-speciﬁc optimisation hyper-parameters. training method developed independently several times gained increasing recognition recent years—see recent reviews different contexts applications. network architecture classiﬁcation stage three layer neural network comprised input layer hidden layer nonlinear units linear output layer. input weights randomly chosen untrained output weights trained single batch using least squares regression. convexity objective function method ensures output weights optimally chosen given random input weights. rapid speed training fact least squares optimisation problem solved using algorithm number hidden units number training points applied pixel-level features networks trained discriminative classiﬁers produce excellent results simple image databases poor performance difﬁcult ones. knowledge however method applied convolutional features. therefore devised network architecture consists three elements work together ensure fast learning good classiﬁcation performance namely convolutional feature extraction random-valued input weights classiﬁcation least squares training output weights feed linear output units. apply network several image classiﬁcation databases including mnist cifar- google street view house numbers norb network produces state-of-the-art classiﬁcation results mnist norb-small databases near state-of-the-art performance svhn. apply ﬁlter channel yikc apply termwise nonlinearity zikc apply lowpass ﬁlter wikc zikc apply termwise nonlinearity sikc downsample ˆsikc sikc concatenate channels normalize rik/) applied term term elements arguments. matrices wfilter wpool sparse toeplitz matrices. practice would form directly instead form pooling matrix ﬁltering matrix ﬁlter sequential apply ﬁlter entire data matrix intuitive explanation lp-pooling follows. first note hidden unit receives input linear combination patch input data i.e. hicjxij. hence squaring results contains terms proportional terms proportional products xij. thus squaring simple produce hidden layer responses depend product pairs input data elements i.e. interaction terms important discriminability. second promising results presented paper demonstrate potential beneﬁts method; clearly further innovations within method required competitive harder datasets like cifar- imagenet. expect likely avenues improving presented results cifar- whilst retaining method’s core attributes introduce limited training stage ﬁlters generalizing method introduction training data augmentation. pursuing directions future work. remainder paper organized follows. section contains generic description network architecture algorithms obtaining convolutional features classifying inputs based them. section describes generic architecture training algorithms speciﬁcally applied four well-known benchmark image classiﬁcation datasets. next section describes results obtained datasets ﬁnally paper concludes discussion remarks section overall network shown figure three hidden layers nonlinear units four layers weights. ﬁrst layer weights convolutional ﬁlter layer. second layer pooling downsampling layer. third layer random projection layer. fourth layer trained layer. output layer linear units. network conceptually divided stages algorithms knowledge previously combined. ﬁrst stage convolutional feature extraction stage largely follows existing approaches image classiﬁcation second stage classiﬁer stage largely follows approach describe stages detail. algorithm apply extract features images summarised algorithm note details ﬁlters described algorithm given section iii-b introduce size two-dimensional ﬁlters q×q. functions nonlinear transformations applied termwise matrix inputs produce matrix outputs size. symbol represents twodimensional convolution. sequence steps algorithm suggest looping images channels sequentially. however following mathematical formulation algorithm indicates standard layered neural network formulation algorithm applicable shown figure therefore computation features obtained shot k-column matrix containing batch training points. fig. overall network architecture. total three hidden layers plus input layer linear output layer. main stages convolutional ﬁltering pooling stage classiﬁcation stage. ﬁnal layer weights wout learnt achieved single batch using least squares regression. remaining weights matrices wfilter speciﬁed remains ﬁxed e.g. taken overfeat wpool describes standard average pooling downsampling; randomly using method speciﬁes weights sampling examples training distribution described text. variables shown follows number pixels image number features extracted image downsampling factor number hidden units classiﬁer stage number classes. paper employ form training ﬁlters pooling matrices. details ﬁlter weights form pooling used example classiﬁcation problems presented paper given section iii. training approach classiﬁer described e.g. default situation methods input weights generated randomly speciﬁc distribution e.g. standard gaussian uniform bipolar. however known setting weights non-randomly based training data leads superior performance paper method input weights also trained iteratively desired using singlebatch backpropagation train size moore-penrose pseudo inverse corresponding atrain. solution equivalent least squares regression applied overcomplete linear equations n-dimensional target. known often useful regularise problems instead solve following ridge regression problem hyper-parameter identity matrix. practice efﬁcient avoid explicit calculation inverse equation instead factorisation solve following linear equations unknown variables wout square root transforms distribution hidden-unit responses; observed practice result square root operation often distribution closer gaussian without helps regularise least squares regression method training output weights. however described shortly classiﬁer stage also square nonlinearity. using nonlinearity found classiﬁcation performance generally optimised taking square root input random projection layer. based observation strictly lp-pooling instead ylabel indicator matrix size numerically represents labels training vector classes—we column single corresponding label class training vector entries zero; norb-small downsample implementation efﬁciency reasons svhn convert channels adding conversion greyscale rgb. found local and/or global contrast enhancement diminished performance; cifar- convert channels adding conversion greyscale rgb; apply whitening channel image since objective train single layer network seek train network ﬁlters optimised training set. instead size twodimension ﬁlters considered following options simple rotated corner ﬁlters square uniform centre-surround ﬁlters; ﬁlters trained imagenet made available overfeat used stage- ‘accurate’ ﬁlters; patches obtained central region randomly selected training images training images class. ﬁlters overfeat ﬁlters. hence databases images applied channel ﬁlter corresponding channel image. applied greyscale channels converted overfeat ﬁlter greyscale. norb applied ﬁlter stereo channels. ﬁlters subtract mean value dimensions channel order ensure mean zero channel. implementing two-dimensional convolution operation required ﬁltering images using obtained central ‘valid’ region i.e. images size total dimension valid region consequently total number features image obtained prior pooling ﬁlters images channels implementing two-dimensional convolution operation required ﬁltering using obtained ‘full’ convolutional region images size given ‘valid’ convolution ﬁrst applied using described above. remaining part pooling step downsample image dimension factor resulting total features image. choosing experimented variety scales settling value mentioned algorithms algorithm simply form atrain solve eqn. followed optimisation using ridge regression. large runtime bottleneck method typically matrix multiplication required obtain gram matrix atraina thus since pseudo-inverse obtained equation equations constitute closedform solution entire test-data classiﬁcation output given speciﬁed matrices wﬁlter wpool hidden-unit activation functions examined method’s performance used classiﬁer images. table lists attributes four well known databases used. databases comprised images used channels namely channels conversion greyscale. approach shown effective svhn considered three kinds untuned ﬁlters described section iii-b well combinations them. exhaustively consider options settled overfeat ﬁlters marginally superior norb svhn cifar- hand-designed ﬁlters superior mnist marginally compared randomly selected patches training data. clearly investigated determine whether hand-designed ﬁlters match trained ﬁlters using method paper. mnist example indicate classiﬁcation performance scales number hidden units classiﬁer stage remain parameters included hand-designed ﬁlters comprised rotated bars rotated corners centred squares zero mean. rotations binary ﬁlters used standard pixel value interpolation. figure shows power law-like decrease error rate increases linear trend log-log axes. best error rate shown ﬁgure shown table attained best repeatable rate using ﬁlters combined overfeat ﬁlters hand-designed ﬁlters randomly selected patches training data obtained error mnist outlier since repeatedly obtained different samples win. implementation matlab cores mnist total time required generate features training images ﬁlter approximately seconds. largest number ﬁlters used date applied svhn total time feature extraction hours runtime achieve feature generation beneﬁts carrying convolutions using matrix multiplication applied large batches simultaneously; instead iterate training images individually still carry convolutions shown table note exists interesting tradeoff number ﬁlters downsampling factor example whereas found that point smaller enables smaller number ﬁlters comparable performance. construct matrix method proposed method matrix chosen normalized difference data vectors corresponding randomly chosen examples distinct classes training set. method previously shown superior setting weights values chosen random distributions nonlinearity classiﬁer stage hidden units typical choice work sigmoid. however found sufﬁcient quadratic nonlinearity. suggests good image classiﬁcation strongly dependent presence interaction terms—see discussion section ii-a. remains hyperparameters classiﬁer stage regression parameter number hidden-units experiments examined classiﬁcation error rates function varying optimize using cross-validation. however also found good generic heuristic setting examined performance network classifying test images four chosen databases function number ﬁlters downsampling rate number hidden units classiﬁer stage maximum number channels available dataset svhn best result within state-of-theart. highly likely using ﬁlters trained svhn database rather imagenet would reduce given structured nature digits opposed complex nature imagenet images. another avenue closing state-of-the-art using ﬁlters would increase decrease thus resulting features classiﬁer hidden units. although increased observe saturation error rate increased point. cifar- less clear lacking method comparison state-of-the-art methods. note cifar- relatively training points observed classiﬁcation performance actual training comparison test suggests designing enhanced methods regularisation necessary ensure method achieve good performance cifar. another possibility nonlinearity classiﬁer stage ensures hidden-layer responses reﬂect higher order correlations possible squaring function used. however expect training convolutional ﬁlters stage extract features discriminative speciﬁc dataset likely enhancement improving results cifar-. finally note exist iterative approaches training classiﬁer component stage using least squares regression without training input weights— e.g. methods easily adapted convolutional front-end example additional batches training data become available problem involves online learning. closing following acceptance paper became aware newly published paper combines convolutional feature extraction least squares regression training classiﬁer weights obtain good results norb dataset three main differences method current paper method follows. first used hidden layer classiﬁer stage whereas solves output weights using least squares regression applied output pooling stage. second used variety methods convolutional ﬁlter weights whereas uses orthogonalised random weights only. third downsample following pooling whereas mark mcdonnell’s contribution supported australian research fellowship australian research council gratefully acknowledge prof david kearney victor stamatescu university south australia sebastien wong dsto australia useful discussions provision computing resources. also acknowledge discussions prof philip chazal university sydney. using matrix multiplication time generating features approximately doubles. note also employ matlab’s sparse matrix data structure functionality represent wfilter wpool also provides speed boost multiplying matrices carry convolutions. matrix-multiplication method convolution instead apply two-dimensional convolutions individual image feature generation slowed even more. classiﬁer stage mnist runtime approximately seconds hence total time mnist ﬁlters order minutes achieve correct classiﬁcation rate fewer ﬁlters smaller simple achieve minute less. svhn cifar- scaled time bottleneck classiﬁer runtime complexity. found necessary case network trained hour cifar- svhn took hours. results within percent best however obtained less time. stated introduction purpose paper highlight potential beneﬁts method presented namely attain excellent results rapid training speed implementation complexity whilst suffering reduced performance relative state-of-the-art particularly hard problems. terms efﬁcacy classiﬁcation tasks shown table best result surpasses best ever reported performance classiﬁcation mnist test augmentation training done. also achieved knowledge best performance reported netzer wang coates bissacco reading digits natural images unsupervised feature learning nips workshop deep learning unsupervised feature learning. http//ufldl.stanford.edu/housenumbers. lecun huang bottou learning methods generic object recognition invariance pose lighting proceedings ieee computer society conference computer vision pattern recognition vol. sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks international conference learning representations cbls sermanet deep learning pipeline image understanding acoustic modeling ph.d. dissertation department computer science york university. mairal koniusz harchaoui schmid convolutional kernel networks advances neural information processing systems ghahramani welling cortes lawrence weinberger eds. vol. cires¸an meier masci gambardella schmidhuber flexible high performance convolutional neural networks image classiﬁcation proceedings twenty-second international joint conference artiﬁcial intelligence simard steinkraus platt best practices convolutional neural networks applied visual document analysis proceedings seventh international conference document analysis recognition cires¸an meier schmidhuber multi-column deep neural networks image classiﬁcation proc. cvpr tapson schaik learning pseudoinverse solution network weights neural networks vol. coates analysis single-layer networks unsupervised feature learning proc.th international conference artiﬁcial intelligence statistics fort lauderdale usa. volume jmlrw&cp ngiam chen chia tiled convolutional neural networks advances neural information processing systems lafferty williams shawe-taylor zemel culotta eds. schmidt kraaijveld duin feed forward neural networks random weights proc. iapr int. conf. pattern recognition volume conf. pattern recognition methodology systems ieee computer society press alamitos g.-b. huang q.-y. c.-k. siew extreme learning machine learning scheme feedforward neural networks proc. international joint conference neural networks july g.-b. huang zhou ding zhang extreme learning machine regression multiclass classiﬁcation ieee transactions systems cybernetics—part cybernetics vol. mcdonnell tissera vladusich schaik tapson fast simple accurate handwritten digit classiﬁcation training shallow neural network classiﬁers ‘extreme learning machine’ algorithm plos vol. schaik j.tapson online adaptive pseudoinverse solutions weights neurocomputing vol. j.tapson chazal schaik explicit computation input weights extreme learning machines proc. conference arxiv..", "year": 2015}