{"title": "Exploiting Unlabeled Data to Enhance Ensemble Diversity", "tag": ["cs.LG", "cs.AI"], "abstract": "Ensemble learning aims to improve generalization ability by using multiple base learners. It is well-known that to construct a good ensemble, the base learners should be accurate as well as diverse. In this paper, unlabeled data is exploited to facilitate ensemble learning by helping augment the diversity among the base learners. Specifically, a semi-supervised ensemble method named UDEED is proposed. Unlike existing semi-supervised ensemble methods where error-prone pseudo-labels are estimated for unlabeled data to enlarge the labeled data to improve accuracy, UDEED works by maximizing accuracies of base learners on labeled data while maximizing diversity among them on unlabeled data. Experiments show that UDEED can effectively utilize unlabeled data for ensemble learning and is highly competitive to well-established semi-supervised ensemble methods.", "text": "abstract—ensemble learning aims improve generalization ability using multiple base learners. well-known construct good ensemble base learners accurate well diverse. paper unlabeled data exploited facilitate ensemble learning helping augment diversity among base learners. speciﬁcally semisupervised ensemble method named udeed proposed. unlike existing semi-supervised ensemble methods error-prone pseudo-labels estimated unlabeled data enlarge labeled data improve accuracy udeed works maximizing accuracies base learners labeled data maximizing diversity among unlabeled data. experiments show udeed effectively utilize unlabeled data ensemble learning highly competitive well-established semisupervised ensemble methods. ensemble learning number base learners trained combined prediction achieve strong generalization ability. numerous effective ensemble methods proposed boosting bagging stacking etc. methods work supervised setting labels training examples known. many real-world tasks however unlabeled training examples readily available obtaining labels would fairly expensive. semi-supervised learning major paradigm exploit unlabeled data together labeled training data improve learning performance automatically without human intervention. paper deals semi-supervised ensembles ensemble learning labeled unlabeled data. contrast huge volume literatures ensemble learning semi-supervised learning work devoted study semi-supervised ensembles. indicated zhou caused different philosophies ensemble learning community semi-supervised learning community. ensemble learning community believes able boost performance weak learners strong learners using multiple learners need unlabeled data; semi-supervised learning community believes able boost performance weak learners strong learners exploiting unlabeled data need multiple learners. however zhou indicated several important reasons ensemble learning semisupervised learning actually mutually beneﬁcial among important considering unlabeled data possible help augment diversity among base learners explained following paragraph. well-known generalization error ensemble related average generalization error base learners diversity among base learners. generally lower average generalization error base learners higher diversity among base learners better ensemble previous ensemble methods work supervised setting trying achieve high average accuracy high diversity using labeled training set. noteworthy however pursuing high accuracy high diversity suffer dilemma. example classiﬁers perfect performance labeled training would diversity since difference predictions training examples. thus increase diversity needs sacriﬁce accuracy classiﬁer. however unlabeled data might classiﬁers actually make different predictions unlabeled data. would important ensemble design. example given pairs classiﬁers know accuracy labeled training data difference taking either ensemble consisting ensemble consisting however make predictions unlabeled data make different predictions unlabeled data know ensemble consisting better. contrast previous ensemble methods focus achieving high accuracy high diversity using labeled data unlabeled data would open promising direction designing ensemble methods. paper propose udeed approach. experiments show using unlabeled data diversity augmentation udeed achieves much better performance counterpart consider usefulness unlabeled data. moreover udeed also achieves highly comparable performance state-of-the-art semi-supervised ensemassigning pseudo-labels enlarge labeled training set. speciﬁcally pseudo-labels unlabeled instances estimated based ensemble trained speciﬁc form smoothness manifold regularization that regarding estimated labels ground-truth labels unlabeled instances used conjunction labeled examples update current ensemble iteratively. although various strategies employed make pseudo-labeling process reliable incorporating data editing estimated pseudo-labels still prone error especially initial training iterations ensemble moderately accurate. next section present udeed approach. rather working pseudo-labels enlarge labeled training udeed utilizes unlabeled data different i.e. help augment diversity among base learners. d-dimensional input space output space. suppose contains labeled training examples {xi|l contains unlabeled training examples addition {xi| denote unlabeled data derived assume classiﬁer ensemble composed base classiﬁers {fk| takes form here real value corresponds conﬁdence positive. accordingly regarded posteriori probability positive given i.e. basic idea udeed maximize classiﬁers labeled data maximizing diversity classiﬁers unlabeled data. therefore udeed generates classiﬁer ensemble minimizing following loss function here ﬁrst term vemp corresponds empirical loss labeled data second term vdiv corresponds diversity loss speciﬁed data furthermore cost parameter balancing importance terms. paper udeed calculates ﬁrst term vemp rest paper organized follows. section brieﬂy reviews related work semi-supervised ensembles. section presents udeed. section reports experimental results. finally section concludes. zhou proposed tri-training approach uses three classiﬁers round classiﬁers agree unlabeled instance third classiﬁer disagrees classiﬁers certain condition label unlabeled instance third classiﬁer; three classiﬁers voted make prediction. disagreement-based semi-supervised learning approach viewed variant famous co-training method later zhou extended tri-training co-forest including base classiﬁers round majority teach minority strategy still adopted. addition tri-training co-forest several semi-supervised boosting methods d’alch´e proposed ssmboost handle unlabeled data within margin cost functional optimization framework boosting margin ensemble unlabeled data deﬁned either |h|. furthermore ssmboost requires base learners semi-supervised algorithms themselves. later bennett developed assemble labels unlabeled data current ensemble sign iteratively puts newly labeled examples original labeled train base classiﬁer added following margin cost functional optimization framework chen wang added local smoothness regularizer objective function used assemble help induce base classiﬁer reliable self-labeling process. margin cost functional formalization mcssb semiboost estimate labels unlabeled instances optimizing objective function containing terms. ﬁrst term encodes manifold assumption unlabeled instances high similarities input space share similar labels term encodes clustering assumption unlabeled instances high similarities labeled example share given label. difference lies mcssb implemented objective terms based bregman divergence semiboost implemented traditional exponential loss. shown second term vdiv used characterize diversity among based learners. however well-known diversity measurement straightforward task since generally accepted formal deﬁnition paper udeed chooses calculate vdiv novel follows here returns cardinality data intuitively represents prediction difference pair base classiﬁers speciﬁed data addition prediction difference calculated based concrete output instead signed output sign]. prediction conﬁdence classiﬁer simple binary prediction fully utilized. initialize ensemble classiﬁer learned bootstrapped sample namely conventional maximum likelihood procedure. speciﬁcally corresponding model parameter obtained minimizing objective function ||wk|| here balances model complexity binomial likelihood note logistic regression implementation loss function generally non-convex target model returned gradient descent process would correspond local optimal solution. here standard logistic regression function weight vector bias value without loss generality rest paper absorbed appending input space extra dimension ﬁxed value reviewed existing diversity measures calculated based oracle outputs base learners i.e. ground-truth labels data assumed known. however considering examples contained speciﬁed data unlabeled infeasible calculate directly utilizing existing diversity measures. shown second term vdiv regarding ensemble diversity deﬁned speciﬁed data given labeled training unlabeled training consider three possibilities instantiating unlabeled training examples employed measure diversity among base learners ensemble optimized exploiting resulting implementation called lcud; ensemble initialized series gradient descent steps performed optimize model minimizing loss function deﬁned eq.. lcud however instead directly minimizing straightforward setting loss function ﬁrstly minimized series gradient descent steps that using learned model starting point series gradient descent steps conducted ﬁnely search model space purpose two-stage process distinguish priorities contribution labeled data unlabeled data. gradient descent-based optimization process terminated either loss function diversity term vdiv decrease anymore. implementation udeed label unseen example strategies adopted successful semisupervised ensemble methods objective terms involving labeled data given much higher weight involving unlabeled data. section comparative studies udeed semi-supervised ensemble methods ﬁrstly reported. importantly experimental analysis three different implementations udeed conducted show whether unlabeled data beneﬁt ensemble learning helping augment diversity among base learners. twenty-ﬁve publicly-available binary data sets used experiments whose characteristics summarized table fifteen machine learning repository archive four twenty regular-scale data sets well large-scale data sets included. data size varies dimensionality varies ratio positive examples negative examples varies various ensemble sizes considered experiments representing case small-scale ensemble; representing case medium-scale ensemble; representing case largescale ensemble. addition shown cost preliminary experiments show that ensemble size increases within interval performance udeed signiﬁcantly change within successive ensemble sizes tends converge ensemble size approaches data diabetes heart wdbc austra house vote vehicle hepatitis labor ethn ionosphere isolet sonar colic credit digit coil adult ijcnn cod-rna forest subsection udeed compared popular ensemble methods bagging adaboost successful semi-supervised ensemble methods assemble semiboost fair comparison logistic regression employed base learner compared method. udeed maximum number gradient descent steps learning rate compared methods default parameters suggested respective literatures adopted. tables report detailed experimental results small-scale medium-scale largescale ensemble sizes respectively. semiboost fails work large-scale data sets demanding storage complexity maintain data mean predictive accuracy well standard deviation algorithm recorded. furthermore statistically measure signiﬁcance performance difference pairwise t-tests signiﬁcance level conducted algorithms. speciﬁcally whenever udeed achieves signiﬁcantly better/worse performance compared algorithm data win/loss counted maker shown. otherwise counted marker given. resulting win/tie/loss counts udeed compared algorithms highlighted last line table. summary ensemble size small udeed statistically superior bagging adaboost assemble semiboost cases inferior much less cases; ensemble size medium udeed statistically superior bagging data diabetes heart wdbc austra house vote vehicle hepatitis labor ethn ionosphere isolet sonar colic credit digit coil adult ijcnn cod-rna forest adaboost assemble semiboost cases inferior much less cases; ensemble size large udeed statistically superior bagging adaboost assemble semiboost cases inferior much less cases. results indicate udeed highly competitive compared methods. roughly speaking time complexity udeed slightly higher bagging adaboost fairly comparable assemble semiboost. motivated section udeed aims exploit unlabeled data help ensemble learning particular augmenting diversity among base learners. therefore addition comparative experiments ensemble methods important show whether udeed achieve better pertable reports performance improvement lcud various ensemble sizes. data mean improved predictive accuracy well standard deviation recorded. addition statistically measure signiﬁcance performance difference pairwise t-tests signiﬁcance level conducted. speciﬁcally whenever lcud achieves signiﬁcantly superior/inferior performance data win/loss counted maker shown table. otherwise counted marker given. resulting win/tie/loss counts lcud highlighted last line table data diabetes heart wdbc austra house vote vehicle hepatitis labor ethn ionosphere isolet sonar colic credit digit coil adult ijcnn cod-rna forest cases inferior cases; ensemble size large lcud statistically superior cases inferior cases. results indicate that exploiting unlabeled data speciﬁc helping augment ensemble diversity udeed capable achieving better performance counterparts consider employing unlabeled ensemble generation. suppose denotes number base classiﬁers ensemble denotes number examples test addition oracle output matrix. here i-th base learner correctly classiﬁes j-th test example otherwise formal deﬁnitions four diversity measures follows clearly verify udeed increase diversity among base learners generating ensemble utilizing unlabeled data additional experiments analyzed subsection based several existing diversity measures. speciﬁcally four diversity measures summarized note although number cases accuracy difference algorithms looks rather marginal difference still statistically signiﬁcant according pairwise t-test. data diabetes heart wdbc austra house vote vehicle hepatitis labor ethn ionosphere isolet sonar colic credit digit coil adult ijcnn cod-rna forest table compares udeed’s initial diversity ensemble initialization ﬁnal diversity ensemble learning various ensemble sizes. data pairwise t-tests signiﬁcance level conducted initial ﬁnal ensemble diversities. whenever ﬁnal data diabetes heart wdbc austra house vote vehicle hepatitis labor ethn ionosphere isolet sonar colic credit digit coil adult ijcnn cod-rna forest ensemble achieves signiﬁcantly higher/lower diversity initial win/loss recorded. otherwise recorded. resulting win/tie/loss counts highlighted last line table previous ensemble methods obtain high accuracy base learners high diversity among base learners considering labeled data. studies using unlabeled data focusing using unlabeled data improve accuracy. major contribution work unlabeled data augment diversity suggests direction ensemble design. speciﬁcally novel semi-supervised ensemble method named udeed proposed works maximizing accuracy labeled experiments show that udeed achieves highly comparable performance successful semi-supervised ensemble methods; udeed beneﬁt unlabeled data using augment diversity among base learners. future interesting whether udeed works well base learners. would insightful analyze udeed achieve good performance theoretically. furthermore designing ensemble methods exploiting unlabeled data augment ensemble diversity gracefully direction worth studying. authors wish thank anonymous reviewers helpful comments improving paper. work supported national science foundation china national fundamental research program china ph.d. programs foundation ministry education china jiangsu science foundation jiangsu program. bennett demiriz maclin exploiting unlabeled data ensemble methods proceedings sigkdd international conference knowledge discovery data mining edmonton canada chen wang regularized boost semi-supervised learning advances neural information processing systems platt koller singer roweis eds. cambridge press d’alch´e grandvalet ambroise semisupervised marginboost advances neural information processing systems dietterich becker ghahramani eds. cambridge press freund schapire decision-theoretic generalization on-line learning application boosting lecture notes computer science vit´anyi berlin springer krogh vedelsby neural network ensembles cross validation active learning advances neural information processing systems tesauro touretzky leen eds. cambridge press improve computer-aided diagnosis machine learning techniques using undiagnosed samples ieee transactions systems cybernetics part systems humans vol. mason bartlett baxter frean functechniques combining hypotheses tional gradient advances large margin classiﬁers smola bartlett sch¨olkopf schuurmans eds. cambridge press z.-h. zhou when semi-supervised learning meets ensemble learning proceedings international workshop multiple classiﬁer systems reykjavik iceland", "year": 2009}