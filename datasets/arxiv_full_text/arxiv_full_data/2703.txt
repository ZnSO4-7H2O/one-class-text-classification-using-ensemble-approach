{"title": "GLSR-VAE: Geodesic Latent Space Regularization for Variational  AutoEncoder Architectures", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "VAEs (Variational AutoEncoders) have proved to be powerful in the context of density modeling and have been used in a variety of contexts for creative purposes. In many settings, the data we model possesses continuous attributes that we would like to take into account at generation time. We propose in this paper GLSR-VAE, a Geodesic Latent Space Regularization for the Variational AutoEncoder architecture and its generalizations which allows a fine control on the embedding of the data into the latent space. When augmenting the VAE loss with this regularization, changes in the learned latent space reflects changes of the attributes of the data. This deeper understanding of the VAE latent space structure offers the possibility to modulate the attributes of the generated data in a continuous way. We demonstrate its efficiency on a monophonic music generation task where we manage to generate variations of discrete sequences in an intended and playful way.", "text": "vaes proved powerful context density modeling used variety contexts creative purposes. many settings data model possesses continuous attributes would like take account generation time. propose paper glsr-vae geodesic latent space regularization variational autoencoder architecture generalizations allows control embedding data latent space. augmenting loss regularization changes learned latent space reﬂects changes attributes data. deeper understanding latent space structure offers possibility modulate attributes generated data continuous way. demonstrate efﬁciency monophonic music generation task manage generate variations discrete sequences intended playful way. autoencoders useful learning encode observable data latent space smaller dimensionality thus perform dimensionality reduction however latent variable space often lacks structure impossible construction sample data distribution. variational autoencoder framework addresses issues introducing regularization latent space together adapted training procedure. allows train complex generative models latent variables providing sample learned data distribution makes useful unsupervised density modeling. trained provides decoding function i.e. mapping low-dimensional latent space observation space deﬁnes usually called data manifold interesting that even observation space discrete latent variable space continuous allows deﬁne continuous paths observation space i.e. images continuous paths latent variable space. interpolation scheme successfully applied image generation text generation however continuous path latent space produce interpolation observation space prefer another priori; thus straight line points necessary produce best interpolation. dealing data contains information labels interpretive quantities interesting information encoded latent space understanding latent space structure great generation content provide manipulate high-level concepts creative way. control generating process annotated data conditioning model resulting conditional variational autoencoder architectures models used generate images speciﬁc attributes also allow generate interpolation images changing given attribute. approaches cvae latent spaces contain high-level information randomness produced images ﬁxed attributes. another approach decoupling attributes latent space representation consists ﬁnding attribute vectors vectors latent space could encode given high-level feature concept. translating encoded input vector attribute vector decoding ideally additional attribute data. successfully used image generation vaes even directly high-level feature representation spaces classiﬁer however approaches rely ﬁnding posteriori interpretations learned latent space theoretical reason simple vector arithmetic particular signiﬁcance setting. indeed original article mnist manifold obtained transforming linearly spaced coordinate grid unit square inverse normal distribution order obtain equally-probable spaces decoded image. advocates fact latent space coordinates data manifold need perceived geodesic normal coordinates. decoding straight line drawn latent space give rise elements whose attributes vary uniformly. work focus ﬁxing priori geometry latent space dataset elements possess continuous attributes. introducing geodesic latent space regularization show possible relate variations latent space variations attributes decoded elements. augmenting training procedure regularizing term recently explored context image generation introduction discriminative regularization aimed improving visual quality samples using pre-trained classiﬁer. approach differs fact glsr favors latent space representations ﬁxed attribute directions focuses latent space structure. show adding regularization grants latent space meaningful interpretation retaining possibility sample learned data distribution. demonstrate claim experiments musical data. experiments suggest regularization also helps learn latent variable spaces little correlation regularized regularized dimensions. adding possibility gradually alter generated sample according user-deﬁned criteria great many generative tasks. since decoding fast believe technique used interactive creative purposes many interesting novel ways. background variational autoencoders deﬁne variational autoencoder deep generative model observations depends latent variables writing joint distribution prior distribution conditional distribution parametrized neural network given i.i.d. dataset elements seek parameter maximizing dataset likelihood generally computationally intractable makes maximum likelihood estimation unfeasible. solution proposed consists preforming variational inference introducing parametric variational distribution approximate model’s posterior distribution lower-bound marginal log-likelihood observation results jointly optimizing parameters depending choice prior variational approximation kullback-leibler divergence dkl||p) either computed analytically approximated monte carlo integration. understood autoencoder stochastic units together regularization term given kullback-leibler divergence approximation posterior prior. analogy distribution plays role encoder network stands decoder network. geodesic latent space regularization suppose access additional information observation space namely possesses ordered quantities interest want take account modeling process. quantities interest given independent differentiable real attribute functions {gk} less dimension latent space. behave functions information geometry literature functions called moment parameters statistics contrary approaches attribute vectors attribute directions posteriori propose impose directions interest latent space linking changes latent space changes functions training time. indeed linking changes changes latent variable point steering generation. section report experiments training task modeling distribution chorale melodies style j.s. bach geodesic latent space regularization. learning good latent representations discrete sequences known challenging problem speciﬁc issues pinpointed sect. describes used framework context sequence generation sect. exposes dataset considered sect. presents experimental results inﬂuence geodesic latent space regularization tailored musical application. detailed account implementation deferred appendix focus paper generation discrete sequences given length using vaes. contrary recent approaches recurrent latent variable models encode entire sequence single latent variable. speciﬁc case sequence composed time steps elements number possible tokens variable vector choose prior standard gaussian distribution zero mean unit variance. approximated posterior encoder modeled using normal distribution modeling conditional distribution sequences suppose variables order take account sequential aspect data make model size independent sequence length implement using rnn. particularity implementation latent variable passed input decoder ﬁrst time step. enforce this introduce binary mask ﬁnally write multiplication scalar multiplication mi−} practice implemented using cell takes input previous hidden state hi−. takes also binary mask input model differentiates case case latent variable given. decoder returns fact probabilities order obtain sequence typically approach different proposed since latent variable passed ﬁrst time step decoder variables independent. believe weaken decoder recommended force decoder information latent variable discuss precisely parametrization used conditional distribution approximated posterior appendix extracted monophonic soprano parts j.s. bach chorales dataset given music python package. chose discretize time sixteenth notes used real name notes encoding. following extra symbol encodes note held replayed. every chorale melody transposed possible keys provided transposition lies within original voice ranges. dataset composed contiguous subsequences length latent variable space dimensions. observation space thus composed sequences element sequence chosen different tokens. adding regularization directly inﬂuences embedding latent space performed vae. experimentally check increase ﬁrst coordinate latent space variable leads increase fig. shows plots function restricted plane pzz. case geodesic latent space regularization applied visible fig. case regularization applied latent space dimension shown fig. clear distinction cases regularization applied function increasing function horizontal line predictable pattern structure non-regularized case. order geodesic latent space regularization effects regularized quantity also affects attribute functions plot fig. attribute functions figure show plots different attribute functions highest lowest midi pitch sequence presence sharps ﬂats. remark adding latent space regularization tends decorrelate regularized quantities non-regularized ones. reducing correlations features feature best accounts high-level attribute often desired property since lead better generalization non-redundant representations. kind orthogonal features particular highly suitable interactive music generation. indeed musical point view interesting able generate variations given sequence notes instance attributes sequence remain unchanged. problem sampling sequences ﬁxed number notes correct data distribution been example addressed context sequence generation markov chains. present case possibility progressively notes existing sequence simply moving equal steps regularized dimension. show fig. moving regularized dimension latent space gives rise variations initial starting sequence intended way. natural question arises adding geodesic regularization latent space deteriorates effect kullback-leibler regularization reconstruction accuracy? possibility sample data distribution simply drawing latent variable prior distribution drawing conditional distribution indeed constitutes great advantage architecture. figure plot plane latent variable space x-axis corresponds regularized dimension. different non-regularized quantities decoded sequences displayed highest pitch lowest pitch sequence contains accidental sharps ﬂats. figure image straight line data manifold obtained starting random increasing ﬁrst coordinate argmax decoding procedure used. generated sequences two-bar long separated double lines. generates variations initial motif adding notes. figure plot aggregated distribution projected plane latent space contains regularized dimension x-axis. point attributed color depending number notes contained decoded sequence. denotes data distribution. ideal setting perfectly matches posterior aggregated distribution match prior experimentally verify plotting aggregated distribution projected plane fig. assigning colors depending regularized quantity notice even global aggregated distribution normally distributed approach prior aggregated distribution cluster sequences depends regularized dimension. report slight drop reconstruction accuracy adding geodesic latent space regularization. fact adding regularization term reduces reconstruction accuracy also noted nonetheless report better visual quality regularized model. geodesic latent space regularization thus permits obtain meaningful posterior distributions maintaining possibility sample using prior distribution price small drop reconstruction accuracy. believe devising adaptive geodesic latent space regularizations could prevent slight deterioration model’s performance provide best worlds. possibility navigate latent space seems important desired feature generative models creative applications. paper introduced regularization function latent space vae. geodesic latent space regularization aims binding displacement directions latent space qualitative change attributes decoded sequences. demonstrated efﬁciency music generation task providing generate variations given melody prescribed way. experiments shows adding regularization allows interpolations latent space meaningful gives notion geodesic distance latent space provides latent space variables less correlation regularized non-regularized coordinates. future work generalizing regularization variational autoencoders multiple stochastic layers. could indeed tackle issue inactive units lower stochastic layers noted forcing lower layers account high-level attributes. regularization scheme general applied recent generalizations variational autoencoders introduce generative adversarial training order obtain better approximations posterior distributions order obtain better similarity metric", "year": 2017}