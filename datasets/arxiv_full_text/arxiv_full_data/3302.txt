{"title": "Manitest: Are classifiers really invariant?", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Invariance to geometric transformations is a highly desirable property of automatic classifiers in many image recognition tasks. Nevertheless, it is unclear to which extent state-of-the-art classifiers are invariant to basic transformations such as rotations and translations. This is mainly due to the lack of general methods that properly measure such an invariance. In this paper, we propose a rigorous and systematic approach for quantifying the invariance to geometric transformations of any classifier. Our key idea is to cast the problem of assessing a classifier's invariance as the computation of geodesics along the manifold of transformed images. We propose the Manitest method, built on the efficient Fast Marching algorithm to compute the invariance of classifiers. Our new method quantifies in particular the importance of data augmentation for learning invariance from data, and the increased invariance of convolutional neural networks with depth. We foresee that the proposed generic tool for measuring invariance to a large class of geometric transformations and arbitrary classifiers will have many applications for evaluating and comparing classifiers based on their invariance, and help improving the invariance of existing classifiers.", "text": "invariance geometric transformations highly desirable property automatic classiﬁers many image recognition tasks. nevertheless unclear extent state-of-the-art classiﬁers invariant basic transformations rotations translations. mainly lack general methods properly measure invariance. paper propose rigorous systematic approach quantifying invariance geometric transformations classiﬁer. idea cast problem assessing classiﬁer’s invariance computation geodesics along manifold transformed images. propose manitest method built efﬁcient fast marching algorithm compute invariance classiﬁers. method quantiﬁes particular importance data augmentation learning invariance data increased invariance convolutional neural networks depth. foresee proposed generic tool measuring invariance large class geometric transformations arbitrary classiﬁers many applications evaluating comparing classiﬁers based invariance help improving invariance existing classiﬁers. huge research efforts recently deployed computer vision machine learning state-of-the-art image classiﬁcation systems reaching performances close human visual system terms accuracy datasets questions emerge differences remain human visual system state-of-the-art classiﬁers. focus difference namely problem invariance geometric transformations. human visual system invariant extent geometric transformations unclear whether automatic classiﬁers enjoy invariance properties. importance invariance classiﬁers outlined recent works effective solutions transformation-invariant classiﬁcations proposed either adapting classiﬁcation rules proper distance metrics improving features used classiﬁcation validate design choices understand improve classiﬁers’ invariance becomes however primordial develop general methods properly measure robustness classiﬁers geometric transformations data samples. previous works proposed methods evaluate invariance classiﬁers either controlled changes simple images speciﬁc tests features popular neural network architectures copyright document resides authors. distributed unchanged freely print electronic forms. previous studies however limited restricted one-dimensional transformations particular types classiﬁers simple images based heuristically-driven quantities. another approach measuring invariance consists generating datasets transformed images measuring accuracy classiﬁers datasets however laborious involves building novel well-designed dataset compare classiﬁers common ground. paper propose principled systematic method measure robustness arbitrary image classiﬁers geometric transformations. particular design framework applied group classiﬁer regardless particular nature classiﬁer. given image deﬁne invariance measure minimal distance identity transformation transformation sufﬁcient change decision classiﬁer image. order deﬁne transformation metric novel idea represent transformed versions image manifold; transformation metric naturally captured geodesic distance manifold. hence given image invariance measure essentially corresponds minimal geodesic distance manifold leads point classiﬁer’s decision changed. global invariance measure derived averaging sufﬁciently large sample set. equipped generic deﬁnition invariance leverage techniques used analysis manifolds transformed visual patterns design manitest method built efﬁcient fast marching algorithm compute invariance classiﬁers. using manitest quantitatively show following results invariance convolutional neural networks scattering transforms largely outperform classiﬁers classiﬁers similar accuracy different invariance scores invariance convolutional neural networks improves network depth natural images classiﬁcation task baseline convolutional networks invariant slight combinations translations rotations dilations data augmentation dramatically increase invariance classiﬁer. latter result particularly surprising kernel trained augmented samples outperform invariance convolutional neural networks handwritten digits dataset. besides results showcase examples illustrating introduced invariance scores. providing systematic tool assess classiﬁers terms robustness geometric transformations bridge towards understanding invariance properties different families classiﬁers hopefully lead building classiﬁers perform closer human visual system. code manitest available project website. problem formulation deﬁnitions consider mathematical model images represented functions denote space square integrable images. group consisting geometric transformations denote dimension transformation belongs denote image transformed examples groups include rotation group similarity group minimal normalized distance identity transformation transformation changes classiﬁcation label i.e. identity element group metric formations namely larger values indicate larger invariance. worth noting deﬁnition related recent work deﬁned adversarial noise minimal perturbation required misclassify datapoint. however instead considering generic adversarial perturbations focus minimal geometric transformations metric borrowed group tions deﬁned quantity depends well distribution datapoints however simplify notations omitted dependence assuming distribution clear context. practical classiﬁcation tasks true underlying distribution generally unknown. case estimate global resilience taking empirical average training points transformation metric discuss introduce distance used invariance score noted possibly multi-dimensional group hence deﬁning trivial metric measures absolute distance transformation parameters limited interest combines parameters possibly different nature. instead relevant notion distance depends underlying image case quantiﬁes change appearance images rather absolute distance transformations. consider example image distance iτl. explicitly depends underlying image fails capture intrinsic geometry family transformed images. illustrate point consider simple example images fig. transformed versions reference image note transformed objects intersection reference object. however clear incurred large rotation translation underwent slight vertical translation. hence distance metric naturally satisfy case image distance. crucial setting classiﬁer recognizes similarity objects figure schematic representation problem encountered using metric metric. black pixels indicate pixels value iτiτ obtained applying combination rotation translation image taken certainly robust transformations classiﬁer merely recognizes similarity given higher score. example underlines well-known fundamental issue distance fails capture intrinsic distance curved manifold transformed images correctly capture intrinsic structure manifold deﬁne length shortest path belonging manifold illustration show fig. images along geodesic path geodesic distance essentially local distances transformed images geodesic path. formalize notions follows. family transformed images equipped metric deﬁnes metric space continuous submanifold following works considered similar manifolds different contexts call image appearance manifold follow approach. assuming curve differentiable respect deﬁne length note expressed terms metric image appearance manifold corresponds summing local distances transformed images path show expressed length associated riemannian metric derive. deﬁning finally problem therefore consists computing global invariance score equivalently deﬁned geodesic distance. words problem becomes computing minimal geodesic distance identity transformation transformation sufﬁcient change estimated label efﬁcient accurate approximation lies effective computation geodesics manifold address follows. geodesic measures geodesic distance identity element geodesic satisﬁes following eikonal equation moreover proved geodesic unique viscosity solution eikonal equation provided continuous. many numerical schemes rely eikonal equation characterization approximate geodesic map. popular fast marching method fast front propagation approach computes values discrete geodesic increasing order. provide brief description space constraints focus case manifold two-dimensional extension arbitrary dimensions straightforward refer complete explanations computations. assume manifold sampled using regular grid; sampling discrete vector approximates nodes. structure fast marching almost identical dijkstra’s algorithm computing shortest paths graphs main difference lies update step bypasses constraint propagation along edges. given node deﬁne neighbours algorithm grid point tagged either known unknown initially grid points unknown except zero. iteration unknown node τmin smallest selected tagged known. then unknown neighbour visited updated follows minimum itself τmingτ known forms triangle worth noting that unlike dijkstra seeks optimal point neighbourhood boundary minimizes estimated distance linear approximation assumption fortunately problem solved closed form corresponds minimization scalar quadratic equation manitest method applies algorithm compute given algorithm dimensional case. algorithm stopped whenever transformation changes classiﬁcation label found. nodes metrics generated on-theﬂy order avoid spending unnecessary ressources far-away nodes might farther minimal transformation satisﬁes therefore never visited. ensure termination algorithm limit number iterations however experiments limit never reached algorithm terminated successfully ﬁnding transformation satisﬁes complexity manitest number visited nodes min-heap structure used important note however complexity algorithm exponential dependence dimension since method involves enumeration simplices dimension however limitation main focus goes low-dimensional transformation groups finally note metric isotropic provides consistent scheme. discretization step tends zero solution computed algorithm tends towards viscosity solution eikonal equation. unfortunately arbitrary anisotropic metrics consistency however guaranteed exact computation geodesics becomes much difﬁcult computationally demanding however observed anisotropy considered metric generally large vicinity leads empirically accurate estimates geodesic distance using manitest discretization step sufﬁciently small. finally stress previous methods addressing metric anisotropy readily applied setting leave future work. experiments ttrans in-plane translations image tdil+rot dilations rotations around center image tsim similarity transformations describe combinations translations dilations experiments used discretization step pixels translations radians rotation dilation manitest. finally transformed images size original image zero-padding boundary condition. handwritten digits dataset linear kernel convolutional neural network employ baseline architecture hidden layers containing convolution operation rectiﬁed linear unit nonlinearity pooling windows followed subsampling. architecture trained stochastic gradient descent softmax loss. scattering transform followed generative classiﬁer. used table reports performance different classiﬁers study invariance scores using manitest. expected linear rbf-svm classiﬁers compare poorly classiﬁers terms invariance. construction scat. explicitly take account invariance pooling operations others not. moreover noted scat. outperforms terms robustness translations global similarity transformations even insight invariance classiﬁers focus two-dimensional group tdil+rot show fig. geodesic distance example image digit computed starting identity transformation moreover overlay minimally transformed images change labels classiﬁers along corresponding geodesic paths. example scat. classiﬁer robust large dilation accompanied rotation required change classiﬁcation label. contrast linear easily fooled slight dilation. fig. illustrate white region rotation-scale plane classiﬁer outputs correct label interestingly scat. classiﬁers largely invariant dilations moderately robust rotations. vision tasks common practice augment training data artiﬁcial examples obtained slightly distorting original examples achieve invariance. although practice known improve classiﬁcation performance classiﬁers many tasks effect invariance classiﬁer quantitatively understood. fig. illustrates manitest invariance scores l-svm rbf-svm classiﬁers trained augmented training sets obtained randomly generating transformations similarity group tsim mnist dataset. classiﬁers improve invariance score transformed samples added training set. result moreover element surprise rbf-svm succeeds improving invariance score around mere additions artiﬁcial examples training outperforms invariance moreover obtained score comparable scat. classiﬁer carefully designed satisfy invariance properties. experiment permits characterize actual power data augmentation learning invariance data. natural images second experimental section perform experiments cifar- dataset focus baseline classiﬁers learn architectures hidden layers. speciﬁcally layer consists successive combination convolutional rectiﬁed linear units pooling operations. convolutional layers consist ﬁlters respectively feature maps layer pooling operations done window size stride parameter build three architectures gradually successively stacking hidden layer previous architecture last hidden layer connected fully connected layer softmax loss used. moreover different architectures trained stochastic gradient descent. test error three architectures respectively figure invariance scores cnns ttrans tdil+rot tsim cifar- dataset. show fig. manitest invariance scores three architectures. approach captures increasing invariance number layers network three groups study. result agreement empirical studies previous known belief invariance increases depth network. however previous results measuring invariance respect dimensional transformation group manitest provides systematic principled verifying increased invariance cnns depth complex groups interestingly enough noted despite relatively small difference performance three layers architectures invariance score strongly increases. highlights invariance performance measures capture different properties classiﬁers. figure illustration images worst average invariance similarity transformations three-layer cnn. rows show original images even rows show minimally transformed images changing prediction cnn. manitest invariance score indicated transformed image. original images correctly classiﬁed -layer cnn. compared handwritten digits task note manitest scores obtained cifar task generally much smaller suggests harder achieve invariance task. visualize level invariance -layer cifar- dataset show fig. sorted example images. images average invariance score less note distinction transformed original images hardly perceptible. suggests robust combinations translations rotation dilation even achieves high accuracy. hand difference original minimally transformed images clearly perceptible top-scored images even though human observer likely correctly recognize class transformed images. conclusion paper proposed systematic rigorous approach measuring invariance classiﬁer low-dimensional transformation groups. using manifold perspective able convert problem assessing classiﬁer’s invariance computing geodesic distances. using manitest quantiﬁed increasing invariance cnns depth highlighted importance data augmentation learning invariance data. believe manitest used perform in-depth empirical analysis different classiﬁcation architectures order better understanding building blocks best preserve invariance potentially build robust classiﬁers. acknowledgments. grateful comments provided anonymous reviewers. also thank laurent jacques gabriel peyré insights fast marching. thank luca barofﬁo hamza fawzi comments paper draft.", "year": 2015}