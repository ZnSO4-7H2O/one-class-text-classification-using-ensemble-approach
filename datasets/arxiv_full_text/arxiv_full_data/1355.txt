{"title": "Deep Trans-layer Unsupervised Networks for Representation Learning", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "Learning features from massive unlabelled data is a vast prevalent topic for high-level tasks in many machine learning applications. The recent great improvements on benchmark data sets achieved by increasingly complex unsupervised learning methods and deep learning models with lots of parameters usually requires many tedious tricks and much expertise to tune. However, filters learned by these complex architectures are quite similar to standard hand-crafted features visually. In this paper, unsupervised learning methods, such as PCA or auto-encoder, are employed as the building block to learn filter banks at each layer. The lower layer responses are transferred to the last layer (trans-layer) to form a more complete representation retaining more information. In addition, some beneficial methods such as local contrast normalization and whitening are added to the proposed deep trans-layer networks to further boost performance. The trans-layer representations are followed by block histograms with binary encoder schema to learn translation and rotation invariant representations, which are utilized to do high-level tasks such as recognition and classification. Compared to traditional deep learning methods, the implemented feature learning method has much less parameters and is validated in several typical experiments, such as digit recognition on MNIST and MNIST variations, object recognition on Caltech 101 dataset and face verification on LFW dataset. The deep trans-layer unsupervised learning achieves 99.45% accuracy on MNIST dataset, 67.11% accuracy on 15 samples per class and 75.98% accuracy on 30 samples per class on Caltech 101 dataset, 87.10% on LFW dataset.", "text": "learning features massive unlabelled data vast prevalent topic highlevel tasks many machine learning applications. recent great improvements benchmark data sets achieved increasingly complex unsupervised learning methods deep learning models lots parameters usually requires many tedious tricks much expertise tune. however ﬁlters learned complex architectures quite similar standard hand-crafted features visually. paper unsupervised learning methods auto-encoder employed building block learn ﬁlter banks layer. lower layer responses transferred last layer form complete representation retaining information. addition beneﬁcial methods local contrast normalization whitening added proposed deep trans-layer networks boost performance. trans-layer representations followed block histograms binary encoder schema learn translation rotation invariant representations utilized high-level tasks recognition classiﬁcation. compared traditional deep learning methods implemented feature learning method much less parameters validated several typical experiments digit recognition mnist mnist variations object recognition caltech dataset face veriﬁcation dataset. deep trans-layer unsupervised learning achieves accuracy mnist dataset accuracy samples class accuracy samples class caltech dataset dataset. almost high-layer tasks classiﬁcation recognition veriﬁcation require design representations speciﬁc aims. classiﬁcation images taken wild numerous factors environment diﬀerent lighting conditions occlusions corruptions deformations lead large amount intra-class variability images. good representations reduce non-informative intra-class variability whilst preserving discriminative information across classes. however designing good feature representations quite tough diﬃcult procedure pattern recognition tasks topic machine learning ﬁeld. research feature representations mainly contains aspects handcrafted feature designing automatic feature learning. researchers engineers made enormous eﬀorts devise robust feature representations domains decade ago. many successful features proposed sift features computer vision domain. however hand-crafted features poor transfer ability domains. novel features need redesigned elaborately domain application changed. representation learning quite prevalent topic deep learning coming nevertheless fully learned representations multi-layer unsupervised learning followed ﬁne-tuning procedure many parameters tuned require much expertise knowledge sophisticated hardware support train long time. paper demonstrate novel trans-layer neural network quite simple classical unsupervised learning method autoencoder building block. diﬀerent pcanet one-by-one layer network responses previous layer model concatenated last layer form complete representation. trans-layer connections make rapid information loss cascade unsupervised learning eﬀectively. addition local contrast normalization whitening added trans-layer unsupervised network boost learning ability commonly used deep neural networks diﬀerence implemented deep trans-layer unsupervised network conventional networks deep trans-layer unsupervised network requires back propagation information ﬁne-tune feature banks. experimental results indicate implemented trans-layer connection scheme boosts deep trans-layer unsupervised network eﬀectively commonly used local contrast normalization whitening also contribute performances. demonstrated deep trans-layer unsupervised network validated digit recognition object recognition tasks. quite surprisingly stacked conventional unsupervised learning trans-layer representations achieves accuracy mnist dataset accuracy samples class accuracy samples class caltech dataset network including pre-processing trans-layer unsupervised learning illustrated detailedly section deep trans-layer unsupervised network extract features tackle applications also described section experimental results comparative analysis mnist mnist variations caltech datasets presented section finally discussions conclusions future work summarized section section much research conducted pursuit good representation manually designing elaborative low-level features lbph feature sift feature feature computer vision ﬁeld. however handcrafted features cannot easily adapted conditions tasks redesigning usually requires novel expertise knowledge tough studies. learning good feature representations probably promising handle required elaborative expertise problem devising hand-crafted features. much recent work machine learning focused learn good feature representations massive unlabelled data great progresses made methods main idea deep models learn multi-level features diﬀerent layers. high-level features generated upper-layer expected extract complex abstract semantics data invariance intra-class variability quite useful highlevel tasks. deep learning methods typically learn multi-level features greedily pre-training layer using speciﬁc unsupervised learning ﬁne-tuning pre-trained features stochastic gradient descent method supervised information however deep architectures numerous parameters number features learn parameters unsupervised learning layer. besides also various parameters momentum weight decay rate learning rate extra parameters including dropout rate dropconnet rate recently proposed convolution deep neural networks also work conventional unsupervised learning methods single layer main idea methods learn complete representations dense features. although methods made much progress benchmark datasets almost hyper parameters single layer unsupervised representational learning models require complete features dimensions high possible parameters need elaborately chosen order obtain satisfactory results major drawback deep learning methods ﬁne-tuning stacking representations consuming expensive computational resources high complexities models. intuition that since elaborately learned features quite similar conventional unsupervised features wavelets auto encoder jump tough timeconsuming parameter ﬁne-tuning procedure take features stacked representation directly. furthermore robust invariant features better devised various pooling strategies. wavelet scattering networks networks pre-ﬁxed wavelet ﬁlter banks deep convolution architectures scatnets quite solid mathematical analysis rotation translation invariants scale. surprisingly superior performance convnet dnns obtained scatnets ﬁne-tuning phase. however scatnet shown inferior performance large intra-class variability including great illumination changing corruption face related tasks non-propagation deep network pre-ﬁxed feature banks pcanet pcanet uses layer cascaded linear networks na¨ıve ﬁlter banks extract complex features. output layer cascaded network processed quantized histogram units. pcanet presents superior highly comparable performance methods scatnet convnet especially face recognition tasks large occlusion illumination expression pose changes. addition pcanet quite fewer parameters much faster learning speed much reliable currently widely researched convnets dnns much convenient practical applications however cascaded structure pcanet face great information loss corruption multi-layer transformation illustrated section current prevalent deep networks also probably facing problem lower layer’s discriminative information lost layers’ transformation. leads inferior results pcanet conventional object recognition tasks. paper tackle multi-layer information loss problem demonstrating novel trans-layer structure based multi-layer conventional unsupervised ﬁlter banks. local contrast normalization whitening operations applied ameliorate unsupervised learning deep trans-layer network. thus trans-layer unsupervised network forms complete eﬀective representation whilst retaining advantages fewer parameters faster learning speed reliable performance. also histogram operation adopted preserve translation rotation invariance binary quantization. experimental results conﬁrm deep trans-layer unsupervised network boosts performance conventional unsupervised learning learns eﬀective feature representations achieve accuracy mnist dataset accuracy samples class accuracy samples class caltech dataset. section present novel framework deep trans-layer unsupervised network feature learning representation. framework proposed deep trans-layer unsupervised network illustrated figure procedures deep trans-layer unsupervised network similar commonly used frameworks computer vision feature learning work figure framework deep trans-layer unsupervised network threelayer neural network including ﬁrst unsupervised learning layer second unsupervised learning layer trans-layer. well. diﬀerent traditional methods deep trans-layer unsupervised network utilizes unsupervised learning methods auto encoder learn local receptive ﬁlter banks needs ﬁne-tuning procedure adjust local ﬁlter banks. besides previous layer’s unsupervised feature maps concatenated last layer form much completed representation shown quite eﬀective following tasks. high level proposed deep trans-layer unsupervised performs apply unsupervised learning feature banks ﬁrst layer learned test image generate feature maps local contrast normalization whitening processing operations. apply unsupervised learning feature banks second layer learned ﬁrst layer feature maps local contrast normalization whitening processing operations. apply dimensionality reduction methods distance metrics learned deep trans-layer unsupervised feature representations directly train classiﬁer based representations tackle applications. structure deep trans-layer unsupervised network partially similar convolution neural network convolution operations done small patches. deep trans-layer unsupervised network local ﬁlters learned unsupervised learning auto encoder requires tuning process error back propagation. unsupervised layer system begins extracting large number random patches unlabelled input images. suppose images used gray images. patch receptive ﬁeld size dimension k-by-k. images color images channels patch dimension k-by-k-by-d. process channels following procedures step step independently. dataset patches constructed rk×k stands patch extracted input images ﬁrst layer feature maps second layer. sequentially apply preprocessing local contrast normalization whitening unsupervised learning ﬁrst second layer respectively. explicit explanations physics physiology. mean local patch stands local brightness standard deviation represents contrast normalization. illumination material optimal property eﬀects removed. hand eﬀect similar lateral inhibition found real neurons. operator inhabits responses within local patch whilst activating responses location patches. following whitening second preprocessing method unsupervised learning layer. whitening commonly used various application decorrelation operator reduces redundant representation images. whitening operator transforms patches formed patches stands covariance function size output data eigenvalue decomposition function eigenvalues eigenvectors respectively make operator robust. whitening also biological explanation proved eﬀectiveness work. ﬁrst unsupervised layer assuming number feature banks ﬁrst layer ﬂatten pre-processed patch extracted input images ﬂattened vectors together. extracted patch matrix ﬁrst layer obtained auto encoder transformation weights encoder bias tradeoﬀ errors model complexity used activation function hyperbolic tangent function decoder bias column vector size full elements obtained randomly turning elements encoder weights bias calculated stochastic gradient decent method. solution de-noising auto encoders ﬁrst de-noising auto encoders unsupervised layer’s feature maps generated applying encoder layer convolute input images. input image feature maps obtained unsupervised learning method applied patch matrix ﬁrst layer. assuming number second layer’s ﬁlters solution obtained solving second layer’s feature maps based ﬁrst layer’s feature calculated third layer network illustrated figure third layer quite similar lbph input training image ﬁrst step encoder real valued feature maps binary values operation converts feature maps binary images. second step compress binary feature maps quantizing binary feature maps. number second layer ﬁlters number ﬁrst layer ﬁlters also compress binary feature maps feature compressed feature maps pixel values compressed feature maps training image third step construct block-wise histogram illustrated third procedure figure first partition compressed feature map. assuming size compressed feature size block strides compressed feature partitioned blocks. compressed feature maps input image blocks stands block. next step build histograms blocks. deep trans-layer network number bins histogram means integer pixel values sparse vector representing histogram constructed. concatenate ×⌊/s+⌋×⌊/s+ histograms form complete representation input image figure illustration showing output layer deep trans-layer unsupervised network. ﬁrst encoder output trans-layer unsupervised learning. compress binarized feature maps feature pixels integer range thus compressed feature maps obtained image. receptive ﬁeld stride generate blocks compressed feature map. block histogram bins calculated. finally concatenate histograms form feature representation deep trans-layer unsupervised network. hist stands histogram operators. deep trans-layer unsupervised network representation training image learn dimensional reduction weight train classiﬁer tackle next applications directly. given test image zero-pad image sake keeping size feature maps input image. pre-processing whitening applied zero-padded image. feature maps calculated ﬁrst unsupervised layer ﬁlters size second unsupervised layer procedure ﬁrst layer. first zero-pad feature map. then pre-process feature maps. next feature maps unsupervised learning feature banks size finally merge ﬁrst layer’s feature maps second layer’s feature maps form complete trans-layer representation. last phase block-wise histogram. binary encoding used tackle real valued feature maps. form compact representation quantizing binary feature maps feature pixel values partition compact feature maps blocks construct histogram representation within block. concatenate histograms form deep trans-layer unsupervised representation translation rotation invariance. real applications next experiments classiﬁers dimension reduction methods following deep trans-layer unsupervised representation. relative high dimensions representation whitening used reduce representation object recognition tasks. wpca conducted conventional weights weighted inverse corresponding squared root energies. also deep trans-layer unsupervised representation directly used train classiﬁer tackle recognition tasks. experiments digit recognition object recognition simple linear classiﬁer parameter tuned used following deep translayer unsupervised network. parameter cost factor used linear software liblinear default experiment validate performance deep trans-layer network deep trans-layer auto encoder network proposed deep trans-layer unsupervised network phases pre-processing trans-layer concatenation. validate phases mnist variations data using deep tran-layer network. also parameters block size stride size chosen cross validation validation set. benchmark experiments conducted digit recognition mnist mnist data contains training samples test samples gray images size pixels. data subset nist contains hand-written digits real world. recognition targets size-normalized centered images mnist variations data sets created applying simple controllable factor variations mnist digits data sets eﬀective ways study invariance ability representation learning methods. details recognition tasks numbers classes samples included table parameters block size stride size patch size determined validation experimentally. validation sets typically partitioned training consistence related work digit recognition tasks ﬁlter size pixels number ﬁlters l=l= size strides half size block. mnist mnist basic mnist-rotation rectangles-image data sets block size pixels. mnist-back-rand mnist-back-image mnist-rot-backimage data sets block size pixels. rectangles data block size pixels. convex data block size pixels. validated tiny parameters ﬁxed following digit recognition tasks. simple linear default parameters connected deep trans-layer representation recognition task groups experiments conducted mnist basic mnist-rotation mnist-back-rand mnist-back-image-rotation data sets validate eﬀect lcn. performance deep trans-layer network without reported table table observe model achieves better performance data sets except mnist-back-rand data set. results prove helps conventional performance explained section main reason performs worse mnist-back-rand data that mean standard deviation data corrupted noise information large area random noise background degrades performance. boosts performance natural images validated improvements mnist basic mnist-rotation mnist-back-image mnist-back-image-rotation data sets. eﬀect trans-layer connection validated mnist basic mnistrotation mnist-back-rand mnist-back-image-rotation data sets second experiment. parameters section performance deep trans-layer network without trans-layer connection recorded table table observe deep trans-layer networks translayer connection consistently better performance without trans-layer connection. trans-layer connection boosts almost percentage performance even hardest task mnist basic data performance mnist-back-image-rotation data set. trans-layer connection deep trans-layer network provides complete representation always helpful recognition. report performance implemented model mnist mnist variations data sets compared methods convolution network scatnet- performance methods recorded table mnist data table mnist variations data sets respectively. fair comparison following results include results using augmented samples. table results show deep trans-layer unsupervised network inferior scatnet- enhanced convolution network related methods. worthy mention performance scatnet- achieved connected non-linear kernels tuned parameters model connected linear default parameters liblinear software model’s performance highly comparable convolution network mnist data mnist data contains many training samples small intra-class variability methods work well data tiny diﬀerence much meaningful statistically. despite this deep trans-layer network boosts almost performance higher related method table observe deep trans-layer unsupervised network achieves best performance data sets simple linear classiﬁer. model highly superior performance methods. suﬃcient prove proposed structure pre-processing trans-layer connection work well convolution structure unsupervised ﬁlters. also evaluate model object recognition task caltech data set. caltech data contains color images belonging categories including background class. number class’s images varies pre-processing data convert images gray scale adjust longer side image preserved aspect ratio. typical tasks conducted. training samples class. training samples class. training sets randomly sampled caltech rest test set. five rounds experiments recorded performance recorded average rounds results. parameters follows. ﬁlter size pixels number ﬁlters block size quarter image size size strides half size blocks. wpca used reduce dimension block’s trans-layer representation linear default parameters liblinear used tackle recognition task. comparison results gray images recorded table table shows deep trans-layer unsupervised network gets impressive performance trained samples class samples class tasks respectively simply using unsupervised methods. surprisingly face veriﬁcation task conducted model lfw-a data data contains faces diﬀerent individuals images collected unconstrained conditions. data unsupervised setting used deep trans-layer network suﬃciently validating representation eﬀectiveness. lfw-a data alignment used cropped face images pixels. standard evaluation protocol followed performance evaluation. histogram block size non-overlapping. parameters before. wpca used reduce dimension trans-layer representation additional square-root operation data set. classiﬁer cosine distance tackle veriﬁcation task. performance recorded table unsupervised setting single descriptor. table shows deep trans-layer network achieves best performance unsupervised setting lfw-a data set. also boosted performance model contrast pcanet dimensions reveals that trans-layer architecture provides uncorrelated information increased dimensions beneﬁcial tasks. typical cases illustrate trans-layer representation works local information naevi faces vanish upper layer’s unsupervised representation size receptive ﬁelds increased local discriminative information neglected. however naevi highly discriminative features recognize person. conventional deep neural networks discriminative details also hard preserve trans-layer representation scheme quite successful convolution network auto encoder ﬁlters number ﬁlters relatively small preserve enough information tasks. information lost rapidly increase layer numbers illustrated fig. model uses second layer’s feature maps would much noise lose much useful information. therefore second layer’s feature maps enough classiﬁcation trans-layer connection quite helpful. deep trans-layer network also problems improved despite trans-layer representation added main problem high dimensions trans-layer representation. although good representation tasks needs high dimensions make dimensions representation possible providing good representational ability. future work could include translation rotation invariance preserving methods reduce dimensions trans-layer representations pooling operations paper novel feature learning representation model deep trans-layer unsupervised network demonstrated. several elements added boost deep unsupervised learning operation trans-layer representation. several experiments digit recognition object recognition tasks shown that proposed method based layer-wise unsupervised learning local receptive features also obtains impressive results trans-layer representation. deep trans-layer unsupervised network quite simple structure much parameters cascaded local receptive ﬁlters need learned unsupervised methods. structure trans-layer network also provides novel direction mine.", "year": 2015}