{"title": "A Machine Learning Perspective on Predictive Coding with PAQ", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.IR", "stat.ML"], "abstract": "PAQ8 is an open source lossless data compression algorithm that currently achieves the best compression rates on many benchmarks. This report presents a detailed description of PAQ8 from a statistical machine learning perspective. It shows that it is possible to understand some of the modules of PAQ8 and use this understanding to improve the method. However, intuitive statistical explanations of the behavior of other modules remain elusive. We hope the description in this report will be a starting point for discussions that will increase our understanding, lead to improvements to PAQ8, and facilitate a transfer of knowledge from PAQ8 to other machine learning methods, such a recurrent neural networks and stochastic memoizers. Finally, the report presents a broad range of new applications of PAQ to machine learning tasks including language modeling and adaptive text prediction, adaptive game playing, classification, and compression using features from the field of deep learning.", "text": "open source lossless data compression algorithm currently achieves best compression rates many benchmarks. report presents detailed description statistical machine learning perspective. shows possible understand modules understanding improve method. however intuitive statistical explanations behavior modules remain elusive. hope description report starting point discussions increase understanding lead improvements facilitate transfer knowledge machine learning methods recurrent neural networks stochastic memoizers. finally report presents broad range applications machine learning tasks including language modeling adaptive text prediction adaptive game playing classiﬁcation compression using features ﬁeld deep learning. detecting temporal patterns predicting future fundamental problem machine learning. gained great interest recently areas nonparametric bayesian statistics deep learning applications several domains including language modeling unsupervised learning audio video sequences. researchers argued sequence prediction understanding human intelligence close connections sequence prediction data compression perhaps underappreciated within machine learning community. goal report describe state-of-the-art compression method called perspective machine learning. show makes several simple well known machine learning models algorithms improved exchanging components sophisticated models algorithms. family open-source compression algorithms closely related better known prediction partial matching algorithm ppm-based data compression methods dominated many compression benchmarks since eclipsed paq-based methods. compression algorithms typically need make trade-oﬀ compression ratio speed memory usage. version achieves record breaking compression ratios expense increased time memory usage. example winning submissions hutter prize contest losslessly compress ﬁrst wikipedia specialized versions paq. dozens variations basic method prize website this compression contest motivated fact able compress well closely related acting intelligently thus reducing slippery concept intelligence hard size numbers. stochastic sequence memoizer language modeling technique recently developed ﬁeld bayesian nonparametrics. table shows comparison several compression algorithms calgary corpus widely-used compression benchmark. summary calgary corpus ﬁles appears table ppm-test implementation used testing diﬀerent compression techniques. ppm*c implementation state implementations stochastic sequence memoizer cppmii- currently among best implementations. paql outperforms compression algorithms considered large margin benchmark. despite huge success rarely mentioned compared machine learning papers. reasons this. core diﬃculty lack scientiﬁc publications inner-workings paq. best knowledge exist incomplete high-level descriptions source code although available close machine language underlying algorithms diﬃcult extract. many architectural details report understood examining source code presented ﬁrst time. ascii text unix refer format bibliographic references. unformatted ascii text madding crowd ascii text unix troff format principles computer speech numbers floating point format seismic data. ascii text usenet batch file variety topics. macintosh executable program knowledge support system. provide detailed explanation works. believe contribution great value machine learning community. understanding could lead design better algorithms. stated inspired research neural networks schmidhuber heil developed experimental neural network data compressor. used layer network trained back propagation predict characters character alphabet text. used separate training prediction phases. compressing text required several days computation workstation. mahoney made several improvements made neural network compression practical. algorithm times faster. uses techniques could lead advances machine learning. second contribution demonstrate understanding enables deploy machine learning techniques achieve better compression rates. speciﬁcally show second order adaptation scheme extended kalman ﬁlter results improvements paq’s ﬁrst order adaptation scheme. third contribution present several novel applications paq. first demonstrate applied adaptive text prediction game playing. tasks tackled using compression algorithms. second show ﬁrst time adapted classiﬁcation. previous works explored using compression algorithms classiﬁcation show proposed classiﬁer paqclass outperform techniques text classiﬁcation task. also show paqclass achieves near state-of-the-art results shape recognition task. finally develop lossy image compression algorithm combining recently developed unsupervised feature learning techniques. section provide general background information problem lossless data compression including description arithmetic coding ppm. section present detailed explanation works. section also includes description improvement compression rate using ekf. present several novel applications section section contains conclusions possible future work. appendix contains information access demonstration programs created using paq. arbitrary data considered sequence characters alphabet. characters could bits bytes characters lossless data compression usually involves stages. ﬁrst creating probability distribution prediction every character sequence given previous characters. second encode probability distributions using coding scheme arithmetic coding huﬀman coding arithmetic coding usually preferable huﬀman coding arithmetic coders produce near-optimal encodings symbols probabilities since problem encoding predicted probability distributions solved performance diﬀerence compression algorithms assign probabilities predictions. next section give overview arithmetic coding works. arithmetic coders perform operations encoding decoding. encoder takes input sequence characters sequence predicted probability distributions. outputs compressed representation sequence characters. decoder takes input compressed representation sequence predicted probability distributions. outputs original sequence characters sequence predicted probability distributions needs exactly encoder decoder order decoder reproduce original character sequence. perspective component compression program creates predicted probability distributions compression decompression equivalent. order generate predicted probability distribution speciﬁc character takes input previous characters sequence. case compression access characters directly trying compress. case decompression access characters output arithmetic decoder. since predictor output exact probability distributions compression decompression randomized components algorithm need initialized seed. process used encode sequence characters arithmetic encoder essentially equivalent storing single number present process works small example. suppose alphabet three characters given goal compress string using arithmetic encoding. ﬁrst character prediction assume predictor gives uniform probability distribution. visualize number line seen figure second character assume predictor assigns probability since ﬁrst character sequence arithmetic encoder expands region assigns second predicted probability distribution according expanded region. visualized middle layer figure ﬁnal character sequence assume predictor assigns probability visualized bottom figure arithmetic coder needs store single number values number eﬃciently encoded using binary search. binary search ranges would ﬁnally represents number encode decision lower half encode decision upper half sequence represented binary consider task decoding ﬁle. input arithmetic decoder number sequence predicted probability distributions. ﬁrst character predictor gives uniform probability distribution. number falls sector arithmetic decoder tells predictor ﬁrst character ‘a’. similarly next characters arithmetic decoder knows characters must ‘a’. point arithmetic decoder needs know reached sequence. typically techniques used communicate length sequence decoder. ﬁrst encode special sequence character decoder reaches character knows reached string. second technique store additional integer along compressed represents length sequence although arithmetic coding achieve optimal compression theory practice factors prevent this. ﬁrst fact ﬁles stored disk using sequence bytes requires overhead comparison storing optimal number bits. second fact precision limitations ﬂoating point numbers prevent optimal encodings. practice factors result relatively small overhead arithmetic coding still produces near-optimal encodings. well text compression benchmarks. creates predicted probability distributions based history characters sequence using technique called context matching. consider alphabet lower case english characters input sequence abracadabra. character string needs create probability distribution representing likely character occur. ﬁrst character sequence prior information character likely occur assigning uniform distribution optimal strategy. second character sequence assigned slightly higher probability observed input history. consider task predicting character entire sequence. prediction longest match input history matches recent input. case longest match abra occurs ﬁrst eighth positions. based longest match good prediction next character sequence simply character immediately match input history. string abra character ﬁfth position. therefore good prediction next character. longer context matches result better predictions shorter ones. longer matches less likely occur chance noise data. essentially creates probability distributions according method described above. instead generating probability distribution entirely based longest context match blends predictions multiple context lengths assigns higher weight longer matches. various techniques blending diﬀerent context lengths. strategy used combining diﬀerent context lengths partially responsible performance diﬀerences various implementations. example technique used generate predicted probabilities shown table table shows state model string abracadabra processed. order context match occurence count context computed probability. ‘esc’ refers event unexpected character causes algorithm lower order model note lowest order model escape event since matches possible character alphabet nonparametric model adaptively changes based data compressing. surprising similar methods discovered ﬁeld bayesian nonparametrics. stochastic memoizer nonparametric model based unbounded-depth hierarchical pitman-yor process. stochastic memoizer shares several similarities implementations. compression performance stochastic memoizer currently comparable best implementations. measuring compression performance size compressed data. however size dependent particular type coding scheme common metrics used measure performance directly based predicted probability distributions cross entropy perplexity. cross entropy used estimate average number bits needed code byte original data. sequence characters probability assigned character prediction algorithm gives expected number bits needed code character string. another common metric used compare text prediction algorithms perplexity deﬁned power cross entropy. compressing images represents signiﬁcantly diﬀerent problem compressing text. lossless compression algorithms tend work best sequence characters contain relatively little noise. well suited natural language characters highly redundant contain less noise individual pixels. problem noise reduces maximum context lengths algorithm like identify. shown variant called prediction partial approximate matching shows competitive compression rates compared lossless image compression algorithms ppam uses approximate matches order longer context lengths improve compression ratio images. another fundamental diﬀerence text images text single dimensional sequence characters images dimensional. applying image compression requires mapping pixels single sequential dimension. trivial performing keogh demonstrate compression algorithms used distance metric time series data. distance metric used solve several interesting problems clustering anomaly detection classiﬁcation. example distance metric used cluster variety types ﬁles music text documents images genome sequences. propose following metric measure distance strings program computes given auxiliary input program. length shortest program outputs concatenated kolmogorov complexity represents best possible compression achieved. since kolmogorov complexity directly computed compression algorithm used approximate algorithms without modifying source code. fortunately open source modiﬁcations easily implemented purposes classiﬁcation investigated deﬁning distance metrics. using cross entropy computationally eﬃcient distance metric deﬁned requires pass data section report compression-based distance metric perform classiﬁcation. keogh compression algorithm distance metric perform experiments clustering anomaly detection classiﬁcation. since achieves better compression theoretically result better distance metric. although perform experiments report would make interesting future work compare experiments keogh uses weighted combination predictions large number models. models based context matching. unlike models allow noncontiguous context matches. noncontiguous context matches improve noise robustness comparison ppm. also enables capture longer-term dependencies. models specialized particular types data images spreadsheets. implementations make predictions byte-level however models used make predictions bit-level. architectural details depend version used. even particular version algorithm changes based type data detected. example fewer prediction models used image data detected. provide high-level overview architecture used paql general case type recognized. paql stable version released matt mahoney march versions submitted hutter prize include additional language modeling components present paql dictionary preprocessing word-level modeling. overview paql architecture shown figure prediction models used. model mixer combines output predictors single prediction. prediction passed adaptive probability used arithmetic coder. practice apms typically reduce prediction error apms also known secondary symbol estimation apms originally developed serge osnach paq. dimensional table takes model mixer prediction order context inputs outputs prediction nonlinear scale table entries adjusted according prediction error coded. paql model mixer architecture shown figure architecture closely resembles neural network hidden layer. however subtle diﬀerences distinguish standard neural network. ﬁrst major diﬀerence weights ﬁrst second layers learned online independently node. unlike back propagation multi-layer network node trained separately minimize predictive cross-entropy error outlined section sense type ensemble method unlike typical ensembles parameters converge ﬁxed values unless data stationary. designed stationary non-stationary data. second major diﬀerence model mixer standard neural network fact hidden nodes partitioned seven sets. every data node selected set. sizes shown rectangles figure refer leftmost rectangle rightmost rectangle edges connected refer non-stationary data data statistics change time. example would consider novel non-stationary text document repeating string stationary. uses diﬀerent selection mechanism choose node. sets number choose node index based single byte input history. example byte value ﬁfth node would selected. uses second recent byte input history uses recent byte uses third recent byte uses fourth recent byte. chooses node based length longest context matched recent input. sets combination several bytes input history order choose node index. selection mechanism used paql shown algorithm history returns i’th recent byte loworderm atches number loworder contexts observed least lastf ourbytes four recent bytes longestm atch length longest context match bitm bitwise operation bitp osition index current byte previous section compared model mixer multilayer neural network. model mixer also compared technique known mixtures experts although standard mixtures experts architecture share similarities. jacobs state backpropagation used train single multilayer network perform diﬀerent subtasks diﬀerent occasions generally strong interference eﬀects lead slow learning poor generalization. know advance training cases naturally divided subsets correspond distinct subtasks interference reduced using system composed several diﬀerent ‘expert’ networks plus gating network decides experts used training case. architecture shown figure mixtures experts architecture gating mechanism choose expert models. problem-speciﬁc properties lead development mixtures experts also applies compression data naturally divided subsets separate ‘experts’ figure mixtures experts architecture. ﬁgure recreation ﬁgure experts feedforward networks input. gating network acts switch select single expert. output selected expert becomes output system. weights selected expert trained. trained subset. using gating mechanism additional computational beneﬁt expert needs trained time instead training experts simultaneously. increasing number expert networks increase time complexity algorithm. diﬀerence mixtures experts model model mixture gating mechanism. jacobs feedforward network learn gating mechanism uses deterministic algorithm perform adaptive learning. gating algorithm used contains problem-speciﬁc knowledge speciﬁed priori. interesting area future work would investigate eﬀect adaptive learning gating mechanism. adaptive learning could potentially lead better distribution data expert. ideally data uniformly partitioned across experts. however using deterministic gating mechanism runs risk particular expert selected often. gating mechanism governed values input data. idea gating units network according value input also used recurrent neural network architectures. example long-short-term-memory used maintain hidden units switched-on hence avoid problem vanishing gradients back-propagation e.g. graves however deterministic gating mechanism intended improving prediction performance avoiding vanishing gradients. rather objective reduce computation vastly. recommend researchers working rnns take days train investigate ways incorporating ideas speed training rnns. adaptive training instead training ﬁxed value another important aspect keep mind. direct mapping mixtures experts architecture model mixer architecture. hidden layer figure corresponds separate mixture experts model. seven mixtures experts combined using additional feedforward layer. number experts mixtures experts model corresponds number nodes mixtures experts architecture weights expert chosen gating mechanism trained data. another diﬀerence standard mixtures experts model fact mixtures experts models typically optimized converge towards stationary objective function designed adaptively train stationary non-stationary data. vector weights vector predictors time next data compressed sigm sigmoid logistic function. number predictors equal ﬁrst layer neural network second layer network. sigm. negative log-likelihood t-th given improve compression rate paql applied extended kalman ﬁlter adapt weights. assume dynamic state-space model consisting gaussian transition prior based local linearization observation model second order adaptive method worthy investigation. earliest implementations train multilayer perceptrons time complexity would singhal since makes tradeoﬀ speed memory usage compression performance. integer value zero eight. lower values level faster less memory achieve worse compression performance. level slowest setting uses memory achieves best compression performance. level memory limit. paq--tuned customized version paql changed value weight initialization second layer neural network. found changing initialization value improved compression performance. finally paq--ekf refers modiﬁed version paql used update weights second layer neural network. using slightly outperforms ﬁrst order updates. improvement order magnitude improvement level level however changing level signiﬁcant cost memory usage using signiﬁcant computational cost. initialization values paq--tuned paq--ekf determined using manual parameter tuning ﬁrst calgary corpus performance diﬀerence paql- paq--tuned similar diﬀerence paq--tuned paq--ekf. natural language applications. example many speech recognition systems composed acoustic modeling component language modeling component. could used directly replace language modeling component existing speech recognition system achieve accurate word predictions. text prediction used minimize number keystrokes required type particular string predictions used improve communication rate people disabilities people using slow input devices modiﬁed source code paql create program predicts next characters user typing string. prediction created input character typed. uses fork input character create process generates likely next characters. fork system call unix-like operating systems creates exact copy existing process. program also given ﬁles train preliminary observational studies text prediction system shown figure note continuously online learning even making prediction character predictions capture syntactic structures even semantic information implied training text. sutskever recurrent neural networks perform text prediction. also compare rnns sequence memoizer terms compression rate. conclude rnns achieve better compression sequence memoizer worse paq. perform several text prediction tasks using rnns diﬀerent training sets diﬀerence method fact continuously online learning test data. feature could beneﬁcial text prediction applications allows system adapt users data appear training set. found text prediction program could modiﬁed rock-paper-scissors usually beats human players. given sequence opponent’s rock-paper-scissors moves predicts likely next move opponent. next round would play move beats prediction. reason strategy usually beats human players humans typically predictable patterns large number rockpaper-scissors rounds. text prediction program rock-paper-scissors available downloaded m—ay contemplation many wonders extinguish spirit ofvengeance companions decided escape soon vessel cameclose enough heard n—erves calmed little brain arousedi swift review whole existence name ears enormous baleen whales name b—ay bengal seas east indies seasof china name byr—on insane idea. lounge.i stared ship bearing name byron k—eeling island disappeared horizon name byron kn—ow skiﬀ escaped maelstrom’sfearsome eddies name byron knoll.— insane idea. fortunately controlled myselfand stretched name byron knoll. name b—yron knoll. name byron knoll. name byron knoll. figure ou—r conclusions convex combination prior mean constraints figure bayesian theory must. jo—rdan conjugate prior figure bayesian theory must. jos—h tenenbaum point posterior mean mode figure examples interactive text prediction sessions. user typed text boldface generated prediction symbol. shortened predictions presentation purposes. example trained twenty thousand leagues seas bottom example trained latex source machine learning book kevin murphy. many classiﬁcation settings practical interest data appears sequences text categorization particular relevance classiﬁcation tasks domain even data appear obviously sequential nature sometimes ingenious ways mapping data sequences. compression-based classiﬁcation discovered independently several researchers main beneﬁts compression-based methods easy apply usually require data preprocessing parameter tuning. several standard procedures performing compression-based classiﬁcation. procedures take advantage fact compressing concatenation pieces data compression programs tend achieve better compression rates data share common patterns. data point test compresses well particular class training likely belongs class. distance metrics deﬁned section directly used classiﬁcation developed classiﬁcation algorithm using show used achieve competitive classiﬁcation rates disparate domains text categorization shape recognition. marton describe three common compression-based classiﬁcation procedures standard minimum description length approximate minimum description length best-compression neighbor suppose data point training test sets stored separate ﬁles. training belongs classes concatenation training ﬁles class smdl runs compression algorithm obtain model test compressed using assigned class whose model results best compression table number times data gets compressed using diﬀerent compression-based classiﬁcation methods. number training ﬁles number test ﬁles number classes. size compressed version also concatenated amdl assigned class minimizes diﬀerence training set. checks every pair assigns class minimizes diﬀerence speed comparison methods made considering many times data gets compressed shown table noted primary diﬀerence smdl amdl fact smdl processes training data once amdl reprocesses training data every test set. many datasets number training test ﬁles much larger number classes means smdl orders magnitude faster amdl although achieves state compression rates also extremely slow compared majority compression algorithms. using classiﬁcation large datasets would unfeasibly slow using amdl bcn. amdl work oﬀ-the-shelf compression programs. however implementing smdl usually requires access compression program’s source code. since open source modiﬁed source code paql implement smdl. call classiﬁer paqclass. best knowledge never modiﬁed implement smdl before. changed source code call fork ﬁnishes processing data training forked process created every test set. essentially copies state compressor training allows test compressed independently. note procedure slightly diﬀerent smdl model continues adaptively modiﬁed processing test however still time complexity smdl. paql parameter compression level. used default parameter setting classiﬁcation experiments. compression performance amdl measured using size. size fundamentally limited ways. ﬁrst accurate within byte second reliant non-optimal arithmetic coding process encode ﬁles disk. cross entropy better measurement compression performance subject neither limitations. since access paql source code used cross entropy measure compression performance instead size. alt.atheism comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x misc.forsale rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space soc.religion.christian talk.politics.guns talk.politics.mideast talk.politics.misc talk.religion.misc total text categorization problem assigning documents categories based content. evaluated paqclass newsgroup dataset. dataset contains newsgroup documents partitioned evenly across categories. number documents category shown table used rennie’s version corpus available http//people.csail.mit.edu/people/jrennie/newsgroups/news-.tar.gz. version corpus duplicate postings newsgroup removed. message headers also removed subject from ﬁelds retained. evaluated paqclass using randomized train-test splits. split seems common evaluation protocol used dataset. document preprocessing performed. results shown table result correct competitive best results published dataset. table shows comparative results. noted several versions news dataset many publications diﬀerent evaluation protocols. published results directly compared. example zhang oles report ﬁgure correct version dataset newsgroup headers removed messages. four best results table seem version news. paqclass outperforms classiﬁcation using compression algorithm dataset. shape recognition problem assigning images categories based shape contour object within image. evaluated paqclass chicken dataset available http//algoval.essex.ac.uk/data/sequence/chicken. dataset contains binary images chicken parts categories example images dataset shown figure chicken pieces images standard orientation. discussed section compressing images poses signiﬁcantly diﬀerent problem compared compressing text. seem large body research using compression-based methods image classiﬁcation fact compression-based methods tend slow infeasible large datasets often used object recognition tasks. several options creating one-dimensional lossy representations images. example watanabe demonstrate method converting images text. show system eﬀective image classiﬁcation tasks. describe method converting shape contours time series data. representation achieve successful classiﬁcation results chicken dataset. based results decided combine representation classiﬁcation. figure demonstrates convert images chicken dataset one-dimensional time series data. ﬁrst step calculate centroid shape. project centroid point measure distance edge shape. intersects shape edge multiple points take furthest intersection rotate around entire shape take measurements uniform intervals. number measurements taken along shape contour tunable parameter. euclidean distance measured particular angle converted single byte rounding result figure example converting shape one-dimensional time series data. original image shown time series data shown bottom. points along contour labeled corresponding points time series shown. table leave-one-out classiﬁcation results chicken dataset diﬀerent settings number measurements parameter. total images. best classiﬁcation results boldface. classiﬁcation results diﬀerent settings number measurements parameter shown table used leave-one-out cross-validation since seems common evaluation protocol used dataset. number measurements parameter property adjusting parameter classiﬁcation accuracy convex function means ﬁnding optimal value parameter would require exhaustive search. time constraints perform exhaustive search table shows confusion matrix best parameter setting. classiﬁcation procedure used rotationally invariant. since chicken pieces dataset arbitrary orientations could lead decrease classiﬁcation accuracy. dimensional image representation rotationally invariant classiﬁcation procedure. -nearest-neighbor classiﬁer combined euclidean distance metric. comparing distance images possible rotations angle results lowest euclidean distance time series representations. procedure trying possible orientations could used make paqclass classiﬁcation procedure rotationally invariant alternatively could single rotationally invariant representation always setting smallest sampled edge distance angle eﬀect rotational invariance classiﬁcation accuracy would make interesting future work. also used performing lossy compression. lossy representation potentially passed achieve additional compression. example paql losslessly compress jpeg images paql contains model speciﬁcally designed jpeg images. essentially undoes lossless compression steps performed jpeg performs lossless compression eﬃciently. create lossy order create lossy image representation calculated closest ﬁlter match image patch original image. ﬁlter selections encoded performing raster scan image using byte patch store ﬁlter index. ﬁlter selections losslessly compressed using paql. example images compressed using method shown figures maximum jpeg compression rate jpeg images still larger images created using method. even larger size jpeg images appeared lower visual quality compared images compressed using method. also compared advanced lossy compression algorithm jpeg. jpeg designed exploit limitations human visual perception less sensitive color variation high spatial frequencies diﬀerent degrees sensitivity brightness variation depending spatial frequency method designed exploit limitations simply uses ﬁlters learned data. based test images jpeg appears outperform method terms visual quality hope technical exposition make method accessible stir research area temporal pattern learning prediction. casting weight updates statistical setting already enabled make modest improvements technique. tried several techniques ﬁelds stochastic approximation nonlinear ﬁltering including unscented kalman ﬁlter observe signiﬁcant improvements implementation. promising technique ﬁeld nonlinear ﬁltering implemented rao-blackwellized particle ﬁltering online logistic regression leave future work. adaptively combines predictions multiple models using context matching diﬀerent typically done mixtures experts ensemble methods boosting random forests. statistical perspective this allows generalization technique focus future eﬀorts. bridging online learning framework potentially fruitful research direction. recent developments rnns seem synergistic still requires methodical exploration. particular relevance adoption paq’s deterministic gating application front found remarkable single algorithm could used tackle broad range tasks. fact many tasks could tackled including clustering compression-based distance metrics anomaly detection speech recognition interactive interfaces. equally remarkable method achieves comparable results state-of-the-art text classiﬁcation image compression. challenges deploying beyond point. ﬁrst challenge models non-parameteric hence require enormous storing capacity. better memory architecture forgetting needed. second challenge fact applies univariate sequences. computationally eﬃcient extension multiple sequences seem trivial. sense rnns advantage stochastic memoizers. would like thank matt mahoney enthusiastically helped understand important details provided many insights predictive data compression. would also like thank marlin ilya sutskever discussions helped improve manuscript.", "year": 2011}