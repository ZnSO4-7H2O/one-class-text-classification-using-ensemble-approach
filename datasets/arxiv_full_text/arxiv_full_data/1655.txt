{"title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue  Responses", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model's predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.", "text": "automatically evaluating quality dialogue responses unstructured domains challenging problem. unfortunately existing automatic evaluation metrics biased correlate poorly human judgements response quality. accurate automatic evaluation procedure crucial dialogue research allows rapid prototyping testing models fewer expensive human evaluations. response challenge formulate automatic dialogue evaluation learning problem. present evaluation model learns predict human-like scores input responses using dataset human response scores. show adem model’s predictions correlate signiﬁcantly level much higher word-overlap metrics bleu human judgements utterance systemlevel. also show adem generalize evaluating dialogue models unseen training important step automatic dialogue evaluation. building systems naturally meaningfully converse humans central goal artiﬁcial intelligence since formulation turing test research type systems sometimes referred non-task-oriented dialogue systems goes back mid-s weizenbaum’s famous program context conversation speaker want tonight? speaker don’t movie? model response let’s something active. reference response yeah turing looks great figure example word-overlap scores fail dialogue evaluation; although model response reasonable words common reference response thus would given scores metrics bleu. eliza rule-based system mimicking rogerian psychotherapist persistently either rephrasing statements asking questions recently surge interest towards building large-scale non-task-oriented dialogue systems using neural networks models trained end-to-end manner optimize single objective usually likelihood generating responses ﬁxed corpus. models already substantial impact industry including google’s smart reply system microsoft’s xiaoice chatbot million users. challenges developing systems good measuring progress case performance chatbot. turing test provides solution evaluation dialogue systems limitations original formulation. test requires live human interactions expensive difﬁcult scale furthermore test requires carefully designing instructions human interlocutors order balance behaviour expectations different systems ranked accurately performance. although unavoidable instructions introduce bias evaluation measure. common approach humans evaluate quality dialogue system responses rather distinguish human responses induces similar drawbacks terms time expense lack scalability. case chatbots designed speciﬁc conversation domains also difﬁcult sufﬁcient human evaluators appropriate background topic despite advances neural network-based models evaluating quality dialogue responses automatically remains challenging understudied problem non-task-oriented setting. widely used metric evaluating dialogue systems bleu metric measuring word overlaps originally developed machine translation. however shown bleu word-overlap metrics biased correlate poorly human judgements response quality many obvious cases metrics fail often incapable considering semantic similarity responses despite this many researchers still bleu evaluate dialogue models alternatives available correlate human judgements. human evaluation always used evaluate dialogue models often expensive time-consuming every model speciﬁcation therefore accurate model evaluate dialogue response quality automatically could considered automatic turing test critical quest building human-like dialogue agents. make progress towards goal make simplifying assumption ‘good’ chatbot whose responses scored highly appropriateness human evaluators. believe sufﬁcient making progress current dialogue systems often generate inappropriate responses. also empirically asking evaluators metrics results either inter-annotator agreement scores highly correlated appropriateness thus collect dataset appropriateness scores various dialogue responses dataset train automatic dialogue evaluation model model trained semi-supervised manner using hierarchical recurrent neural network predict human scores. show adem scores correlate signiﬁcantly human judgement utterance-level system-level. also show adem often generalize evaluating models whose responses unseen training making adem strong ﬁrst step towards effective automatic dialogue response evaluation. train model predict human scores dialogue responses ﬁrst collect dataset human judgements twitter responses using crowdsourcing platform amazon mechanical turk accurate human scores variety conversational responses conditioned dialogue contexts span full range response qualities. example responses include relevant irrelevant responses coherent non-coherent responses achieve variety candidate responses several different models. following following sources candidate responses response selected tf-idf retrieval-based model response selected dual encoder response generated using hierarchical recurrent encoder-decoder model human-generated responses. noted humangenerated candidate responses reference responses ﬁxed corpus novel human responses different reference. addition increasing response variety necessary want evaluation model learn compare reference responses candidate responses. provide details experiments supplemental material including additional experiments suggesting several metrics currently unlikely useful building evaluation models. note that order maximize number responses obtained ﬁxed budget obtain evaluation score dialogue response dataset. train evaluation models human judgements crucial obtain scores responses near distribution produced advanced models. twitter corpus models pre-trained readily available. further topics discussed quite broad opposed speciﬁc ubuntu dialogue corpus therefore model also suited chit-chat domains. finally since require domain speciﬁc knowledge easy workers annotate. recurrent neural networks recurrent neural networks type neural network time-delayed connections between internal units. leads formation hidden state updated every input parameter matrices non-linear activation function tanh input time hidden state allows rnns better model sequential data language. paper consider rnns augmented long-short term memory units lstms gates allow learn much update hidden state. lstms well-established methods dealing vanishing gradient problem recurrent networks word-overlap metrics popular approaches automatically evaluating quality dialogue responses computing word overlap reference response. particular popular metrics bleu meteor scores used machine translation rouge score used automatic summarization. metrics tend correlate human judgements target domains recently shown highly biased correlate poorly human judgements dialogue response evaluation brieﬂy describe bleu here provide detailed summary word-overlap metrics supplemental material. bleu bleu analyzes co-occurrences n-grams reference proposed responses. computes n-gram precision whole dataset multiplied brevity penalty penalize short translations. bleu-n denotes largest value ngrams considered drawbacks major drawbacks word-overlap metrics failure capturing semantic similarity between model reference responses common words. problem less critical machine translation; since reasonable translations given sentence document rather small reasonably infer quality translated sentence measuring word-overlap reference translations. however dialogue appropriate responses given context much larger words high response diversity unlikely captured word-overlap comparison single response. further word-overlap scores computed directly model reference responses. such consider context conversation. reasonable assumption machine translation case dialogue; whether model response adequate substitute reference response clearly context-dependent. example responses figure equally appropriate given context. however simply change context have heard good movies recently? model response longer relevant reference response remains valid. adem learns distributed representations context model response reference response using hierarchical encoder. given dialogue context reference response model response adem ﬁrst encodes vectors using encoder. then adem computes score using dot-product vector representations linearly transformed space matrices interpreted linear projections model response space contexts reference responses respectively. model gives high scores responses similar vector representations context reference response projection. model end-to-end differentiable; parameters learned backpropagation. implementation parameters model trained minimize squared error model predictions human score l-regularization sordoni lower-level utterance-level encoder takes input words dialogue produces vector output utterance. context-level encoder takes representation utterance input outputs vector representation context. hierarchical structure useful incorporating information early utterances context following previous work take last hidden state context-level encoder vector representation input utterance context. parameters encoder pretrained learned human scores. important point adem procedure dialogue retrieval model fundamental difference adem access reference response. thus adem compare model’s response known good response signiﬁcantly easier inferring response quality solely context. pre-training vhred would like evaluation model make accurate predictions labeled examples since examples expensive obtain. therefore employ semi-supervised learning pre-training procedure learn parameters encoder. particular train encoder part neural dialogue model; attach third decoder takes output encoder input train predict next utterance dialogue conditioned context. dialogue model employ pre-training latent variable hierarchical recurrent encoderdecoder model shown figure vhred model extension original hierarchical recurrent encoderdecoder model turn-level stochastic latent variable. dialogue figure vhred model used pre-training. hierarchical structure encoder shown around bottom half ﬁgure. training using vhred procedure last hidden state context-level encoder used vector representation input text. context encoded vector using hierarchical encoder vhred samples gaussian variable used condition decoder after training vhred last hidden state context-level encoder input vector representations respectively. representations vhred model produces diverse coherent responses compared hred. order reduce effective vocabulary size byte pair encoding splits word sub-words characters. also layer normalization hierarchical encoder found worked better task dialogue generation related recurrent batch normalization train vhred model employed several techniques found drop words decoder ﬁxed rate anneal kl-divergence term linearly ﬁrst batches. adam optimizer training adem also employ subsampling procedure based model response length. particular divide training examples bins based number words response score response. over-sample bins across score ensure adem response length predict score. humans tendency give higher rating shorter responses longer responses shorter responses often generic thus likely suitable context. indeed test pearson correlation response length human score training vhred context embedding size however found adem model learned effectively embedding size reduced. thus training vhred principal component analysis reduce dimensionality context model response reference response embeddings found experimentally provided best performance. figure scatter plot showing model human scores bleu- rouge full dataset adem test set. gaussian noise drawn integer human scores better visualize density points expense appearing less correlated. table correlation metrics human judgements p-values shown brackets. ‘adem indicates adem tweetvec embeddings ‘vhred’ indicates product vhred embeddings cr-adem represent adem model trained compare model response context reference response respectively. compute baseline metric scores full dataset provide accurate estimate scores results utterance-level correlations ﬁrst present utterance-level correlation results existing word-overlap metrics addition results embedding baselines adem table baseline metrics evaluated entire dataset responses provide accurate estimate score. measure correlation adem validation test sets constitute responses each. note word-overlap correlation results table also lower presented galley measure corpus-level correlation i.e. correlation averaged across different subsets data pre-ﬁlter high-quality reference responses. kens beginning utterance. results detailed supplemental material. observe data data table correlations word-overlap metrics even lower estimated previous studies particular case bleu- frequently used dialogue response evaluation table adem correlates better human judgement wordoverlap baselines. illustrated scatterplots figure also compare adem using tweetvec embeddings case instead using vhred pre-training method presented section off-the-shelf embeddings ﬁnetune dataset. tweetvec embeddings computed character-level figure scatterplots depicting system-level correlation results adem bleu- bleu-and rouge test set. point represents average scores responses dialogue model human scores shown horizontal axis normalized metric scores vertical axis. ideal metric perfectly linear relationship. system-level correlations show systemlevel correlations various metrics table present visually figure point scatterplots represents dialogue model; humans give scores tfidf responses higher scores hred highest scores human responses. clear existing word-overlap metrics incapable capturing relationship even models. renders completely deﬁcient dialogue evaluation. however adem produces almost model ranking humans achieving signiﬁcant pearson correlation thus adem correlates well humans response system level. generalization previously unseen models adem used practice take input responses model seen training. thus crucial adem correlates human judgements models. test adem’s generalization ability performing leave-one-out evaluation. dialogue model source response data training adem conduct experiment train model responses except chosen model test model unseen training. results given table observe adem model able generalize models except dual encoder. particularly surprising hred model; case adem trained responses written humans able generalize responses produced generative neural network model. testing entire test model achieves comparable correlations adem model trained less data selected random. qualitative analysis strengths weaknesses adem show human adem scores responses various contexts table several instances adem predicts accurately particular adem often good assigning scores poor responses. seen ﬁrst contexts responses given score humans given scores less adem. single exception response second context seems somewhat appropriate perhaps scored higher human evaluator. also several instances model assigns high scores suitable responses ﬁrst contexts. drawback observed adem tends conservative predicting response scores. case third context model assigns scores responses human rated highly. behaviour likely squared error loss used train adem; since model receives large penalty incorrectly predicting extreme value learns predict scores closer average human score. table correlation adem various model responses removed training set. left columns show performance entire test right columns show performance responses dialogue model seen training. last corresponds adem model trained model responses amount training data model going mall. sure going able good another user leadership ranks awww poor baby hope feeling better soon. maybe many work days piedmont tweet much? photo television debut some. <url> hehe <url> really you? thought recognizing someone looked like oysters worth wait? beat call duty want cookie? come provide many experiments including investigation evaluation speed learning curves data efﬁciency failure analysis primary source improvement word-overlap metrics supplemental material. related approach literature novel methods evaluation machine translation systems especially evaluation task particular proposed evaluate machine translation systems using regression tree-lstms respectively. approach differs dialogue domain must additionally condition score context conversation necessary translation. also related work estimating quality responses chat-oriented dialogue systems. train automatic dialogue policy evaluation metric structured role-playing sessions enriched paraphrases external referee annotations. propose semi-automatic evaluation metric dialogue coherence similar bleu rouge based ‘wizard type data. propose framework predict utterance-level problematic situations dataset chinese dialogues using intent sentiment factors. finally train classiﬁer distinguish user utterances system-generated utterances using various dialogue features dialogue acts question types predicate-argument structures. several recent approaches hand-crafted reward features train dialogue models using reinforcement learning example features related ease answering information metrics related turn-level appropriateness conversational depth. metrics based hand-crafted features capture small relevant aspects; inevitably leads suboptimal performance unclear whether objectives preferable retrieval-based crossentropy word-level maximum log-likelihood objectives. furthermore many metrics computed conversation-level available evaluating single dialogue responses. metrics computed responselevel could incorporated framework example adding term equation consisting product features vector learned parameters. signiﬁcant work evaluation methods task-oriented dialogue systems attempt solve user’s task ﬁnding restaurant. methods include paradise framework memo consider task completion signal. paradise particular perhaps ﬁrst work learning automatic evaluation function dialogue accomplished linear regression. however paradise requires measure task completion task complexity available setting. twitter corpus train models contains broad range non-task-oriented conversations used train many state-ofthe-art models. however model could easily extended general-purpose datasets reddit similar pre-trained models become publicly available. models necessary even creating test domain help determine adem generalizes related dialogue domains. leave investigating domain transfer ability adem future work. evaluation model proposed paper favours dialogue models generate responses rated highly appropriate humans. likely property fully capture desired end-goal chatbot systems. example issue building models approximate human judgements response quality problem generic responses. since humans often provide high scores generic responses appropriateness many given contexts model trained predict scores exhibit behaviour. important direction future work modifying adem subject bias. could done example censoring adem’s representations contain information length. alternatively combine adversarial evaluation model assigns score based easy distinguish dialogue model responses human responses. case model generates generic responses easily distinguishable obtain score. important direction future research building models evaluate capability dialogue system engaging meaningful interaction human. compared evaluating single response evaluation arguably closer end-goal chatbots. however evaluation extremely challenging completely automatic way. view evaluation procedure presented paper important step towards goal; current dialogue systems incapable generating responses rated highly appropriate humans believe evaluation model useful measuring facilitating progress direction. acknowledgements we’d like thank casper help correlation code laurent charlin helpful discussions data collection jason weston suggesting improvements experiments jean harb emmanuel bengio debugging aid. gratefully acknowledge support samsung institute advanced technology national science engineering research council calcul quebec. we’d also like thank developers theano", "year": 2017}