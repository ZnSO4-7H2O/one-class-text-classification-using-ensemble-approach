{"title": "Modeling reverse thinking for machine learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Human inertial thinking schemes can be formed through learning, which are then applied to quickly solve similar problems later. However, when problems are significantly different, inertial thinking generally presents the solutions that are definitely imperfect. In such cases, people will apply creative thinking, such as reverse thinking, to solve problems. Similarly, machine learning methods also form inertial thinking schemes through learning the knowledge from a large amount of data. However, when the testing data are vastly difference, the formed inertial thinking schemes will inevitably generate errors. This kind of inertial thinking is called illusion inertial thinking. Because all machine learning methods do not consider illusion inertial thinking, in this paper we propose a new method that uses reverse thinking to correct illusion inertial thinking, which increases the generalization ability of machine learning methods. Experimental results on benchmark datasets are used to validate the proposed method.", "text": "abstract human inertial thinking schemes formed learning applied quickly solve similar problems later. however problems signiﬁcantly diﬀerent inertial thinking generally presents solutions deﬁnitely imperfect. cases people apply creative thinking reverse thinking solve problems. similarly machine learning methods also form inertial thinking schemes learning knowledge large amount data. however testing data vastly diﬀerence formed inertial thinking schemes inevitably generate errors. kind inertial thinking called illusion inertial thinking. machine learning methods consider illusion inertial thinking paper propose method uses reverse thinking correct illusion inertial thinking increases generalization ability machine learning methods. experimental results benchmark datasets used validate proposed method. human learning automatically induces general rules large number experiences. rules taken elements form inertial thinking schemes used solve similar problems later. reasons inertial thinking exists humans fact solve large number daily problems insuﬃcient information solve quickly however accuracy solutions problems generally reduced problems diﬀerent learning. like human learning machine learning automatically learns concepts large number data. learning process inertial thinking schemes formed including illusion inertial thinking leading generalization ability machine learning methods. according hoeﬀding accuracy machine learning methods depend heavily large number training samples advanced machine learning methods require large number training samples e.g. deep learning methods however diﬃcult obtain large number training samples real applications disease diagnosis training samples easily leading formation illusion inertial thinking machine learning methods. humans solve kinds problem reverse thinking eﬀective method creative thinking however machine learning methods reverse thinking totally considered. therefore paper present method applies reverse thinking correct illusion inertial thinking machine learning methods generalization ability enhanced. method suitable machine learning method long illusion inertial thinking facilitated. ability learned probability model correctly classify samples measured generalization ability. lower generalization ability model result training samples insuﬃcient correctly data distribution problem. order formally illustrate generalization ability model data taken balls diﬀerent colors jar. expected proportion diﬀerent color balls computed portion balls partially taken jar. assumes ratio orange balls proportion green balls balls taken ratio orange balls proportion green balls relationship cases follows hoeﬀding inequality illusion inertial thinking appears people reverse thinking deal process begins designing inertial thinking discrimination model judge whether current testing data line inertial thinking illusion inertial thinking. second reverse thinking model created correct illusion inertial thinking. overall procedure shown fig. classiﬁers trained training database form suitable inertial thinking model illusion inertial thinking model inertial thinking discrimination model. suitable inertial thinking model works well simple samples easily classiﬁed illusion inertial thinking easily occurs classifying complicated samples. many measures applied rank samples simplicity hence rank samples training data according simplicity samples using k-fold crossvalidation method. samples fold removed training data remaining ones used train classiﬁer applied classify training data. case simplicity sample computed. subsequently simple training database constructed samples greater simplicity given threshold similarly complicated training database also constructed shows large right-hand side inequality small indicating probability diﬀerence larger given small. case distribution overall samples inferred distribution partially taken samples. however real applications usually diﬃcult suﬃciently large error inevitable. current machine learning methods consider suitable inertial thinking ignoring illusion inertial thinking. case size training samples large enough testing samples could high probability diﬀering training samples inevitably leading error. many machine learning methods perform well experimental data even accuracy behave badly practice. inertial thinking model suitable inertial thinking model discrimination model denotes samples classiﬁed easily denotes samples classiﬁed diﬃcultly. owing illusion inertial thinking learned inertial thinking model misclassify coming samples. example classiﬁes test sample true class belonging class results obtained illusion inertial thinking absolute probabilistic model like bayesian theorem applied. conditions complete statistical knowledge model presented classiﬁer designed according bayesian theorem obtain minimum classiﬁcation error terms probability means classiﬁer based reverse thinking rule optimal. refers probability belongs class denotes probability class misclassiﬁed belonging right-hand side equation involves prior probability applied compute posterior probability left-hand side equation. equation applied implement reverse thinking rule recomputing probability. posterior probabilities computed confusion matrix. avoid confusion rewritten thus reverse thinking rule enhance probability classiﬁer classify belonging classify belonging proves reverse thinking rule ability correct classiﬁcation samples misclassiﬁed. classiﬁer classiﬁed samples depending wrong also replaced prior probability computed confusion matrix. subsequently another alternative reverse thinking rule noteworthy reverse thinking rule applied classiﬁcation complicated samples reverse direction considered deal misclassiﬁed samples. therefore following context necessary condition reverse thinking rule simpliﬁed follows design machine learning approach necessary bayesian theorem calculate probabilities reverse thinking rules calculated computing confusion matrix speciﬁed classiﬁer given training data. example training data samples categories category composed samples confusion matrix given classiﬁer computed -fold cross-validation. shown table column table represents predicted category total number column represents number samples predicted category. represents true category sample total number samples represents number samples class. example number located cross-point ﬁrst ﬁrst column indicates samples true class correctly predicted number located cross-point ﬁrst column second indicates samples true class incorrectly predicted class reverse thinking machine learning algorithm described table including training testing stages. inertial thinking model rtml algorithm obtained training classiﬁer training data. illusion inertial thinking formed original environment. second testing sample discrimination model used decide whether testing sample easy classify. testing sample easy classify classiﬁed using suitable inertial thinking model trained easy training data. testing sample hard classify inertial thinking model applied reverse thinking model applied modify result. ﬁnal category determined modiﬁed results. algorithm classes considered datasets real tasks binary class. second binary-class datasets reverse thinking rules needs consider bayesian theorem without considering bayesian network simplify problem. finally multi-class tasks classiﬁed classiﬁcation methods binary-class tasks support vector machine complexity rtml algorithm training stage mainly distributed computing confusion matrix training three classiﬁers. therefore complexity rtml algorithm equal maximum complexity three classiﬁers. complexity rtml algorithm testing stage also maximum complexity three classiﬁers testing sample. calculation reverse thinking rules related number categories regarded constant. rtml algorithm additional parameter simplicity threshold determines boundary illusion inertial thinking model suitable inertial thinking model. eﬀectiveness proposed method demonstrated experiments standard datasets. principle proposed method applicable existing classiﬁcation methods improve classiﬁcation performance. experiments highly representative machine learning method selected namely softmax widely used deep learning expected rtml algorithm applied deep learning. discrimination model svm-knn selected inherits advantages able elegantly deal large datasets. using artiﬁcial data control number available samples noise according experimental purpose. experiments spirals data selected synthetic diﬃcult many machine learning methods. data widely used many classiﬁers standard data here applied observe eﬀect reverse thinking noisy data diﬀerent sizes. data examples shown fig. points noise disruption. obviously data linearly separable nearly evenly distributed classes. addition artiﬁcial data benchmark real datasets classes selected libsvm keel table reason selecting data classes real datasets binary class. second binary-class datasets reverse thinking rules needs consider bayesian theorem without considering bayesian network simplify problem. finally multi-class data classiﬁed classiﬁcation methods binary-class data respectively diﬀerent datasets formed observe eﬀects noise size data performance rtml algorithm. order observe sensitivity rtml algorithm parameters time classiﬁcation results corresponding parameter computed plotted ﬁgure. seen fig. noise negative eﬀects rtml algorithm; intense noise lower accuracy. noise random disrupting formation illusion inertial thinking. however case noise rtml algorithm still obviously increases accuracy softmax. time rtml algorithm sensitive parameter still achieves remarkable results many values indicating choice parameter easier. finally data size becomes larger rtml algorithm increases eﬀect obviously less sensitive parameter. number misclassiﬁed samples increase larger dataset leading formation stable illusion inertial thinking. data conducted results presented fig. misclassiﬁed testing data denoted red. seen number points misclassiﬁed softmax larger misclassiﬁed rtml algorithm. samples misclassiﬁed softmax corrected rtml algorithm showing reverse thinking model eﬀective. however easy points misclassiﬁed rtml algorithm cannot modify them. depends ability original classiﬁer i.e. softmax. this turn means original classiﬁer selected best possible. misclassiﬁed samples cannot modiﬁed since cases illusion inertial thinking formed. rtml algorithm simplicity threshold parameter decide whether illusion inertia thinking model formed database composed samples simplicity larger given threshold. experiments conducted real datasets illustrate inﬂuence parameter rtml algorithm. seen fig. rtml algorithm sensitive threshold. threshold fig. rtml algorithm worse eﬀect since case many samples mistaken easy ones. indicates threshold parameter values need determined accurately possible. however fig. parameter varies terms indicating optimal value determined easily using -fold cross-validation method. simplicity samples plays important role rtml algorithm needs measured precisely possible. context many models applied deﬁne simplicity. models created using -fold cross-validation method training database classiﬁcation method. seen fig. number models larger accuracy tends optimal threshold easily selected. means number models used measure simplicity samples large possible make measurement stable. generally machine learning methods easily inﬂuenced noisy data. experiments therefore conducted assess inﬂuence noisy data rtml algorithm results shown fig. here datasets points added noises whose variances artiﬁcially constructed examples correspond situations likely occur practice experiments real datasets conducted. advantages real data generated without knowledge classiﬁcation procedures used testing. here real datasets shown table applied conduct experiments using -fold cross-validation method determine training data testing data average classiﬁcation accuracy obtained. training data -fold cross-validation method used select optimal parameters given classiﬁer. easily seen table rtml algorithm much eﬀective datasets. rtml algorithm’s performance especially prominent fourclass svmguide waveform glass phoneme data outperforming softmax terms accuracy respectively indicating illusion inertial thinking deﬁnitely exists reverse thinking model eﬀectively correct illusions. conﬁrmed confusion matrices. example confusion matrix svmguide data performed softmax signiﬁcant illusion inertial thinking established often classiﬁes samples class belonging class notvice versa however rtml algorithm fails data madelon splice performing poorly data. reason illusion inertial thinking obvious. seen confusion matrix illusion inertial thinking schemes classes similar. thinking falls chaos cannot form stable illusion inertial thinking direction leading failure reverse thinking model. case good measures applied maintain suitable inertial thinking enhance illusion inertial thinking improve overall performance. strengthening inertial thinking also signiﬁcantly strengthen illusion inertial thinking. consistent conﬂict other similar human learning machine learning easily form illusion inertial thinking machine learning methods consider paper reverse thinking overcome illusion inertial thinking improve generalization ability machine learning methods. experimental results indicate method eﬀective. proposed method universal applicable machine learning methods especially performing badly data unbalanced data machine learning methods easily form illusion inertial thinking. planned future work select machine learning methods combine rtml algorithm solve concrete tasks like emotion recognition training data emotion recognition generally unbalanced easily inducing machine learning methods form illusion inertial thinking. second proposed method uses bayesian theorem bayesian network. matter fact reverse inertial thinking categories linked dialectical unity investigated framework bayesian network future additionally datasets madelon mammographic softmax performs better taking easy samples training database rather taking samples training database. indicates reasonable rank samples training database simplicity. supported china national science foundation science technology planning project guangdong province guangzhou science technology planning project.", "year": 2018}