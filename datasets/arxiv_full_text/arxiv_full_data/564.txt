{"title": "Deep Speech: Scaling up end-to-end speech recognition", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional speech systems, which rely on laboriously engineered processing pipelines; these traditional systems also tend to perform poorly when used in noisy environments. In contrast, our system does not need hand-designed components to model background noise, reverberation, or speaker variation, but instead directly learns a function that is robust to such effects. We do not need a phoneme dictionary, nor even the concept of a \"phoneme.\" Key to our approach is a well-optimized RNN training system that uses multiple GPUs, as well as a set of novel data synthesis techniques that allow us to efficiently obtain a large amount of varied data for training. Our system, called Deep Speech, outperforms previously published results on the widely studied Switchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speech also handles challenging noisy environments better than widely used, state-of-the-art commercial speech systems.", "text": "present state-of-the-art speech recognition system developed using end-toend deep learning. architecture signiﬁcantly simpler traditional speech systems rely laboriously engineered processing pipelines; traditional systems also tend perform poorly used noisy environments. contrast system need hand-designed components model background noise reverberation speaker variation instead directly learns function robust effects. need phoneme dictionary even concept phoneme. approach well-optimized training system uses multiple gpus well novel data synthesis techniques allow efﬁciently obtain large amount varied data training. system called deep speech outperforms previously published results widely studied switchboard hub’ achieving error full test set. deep speech also handles challenging noisy environments better widely used state-of-the-art commercial speech systems. speech recognition systems rely sophisticated pipelines composed multiple algorithms hand-engineered processing stages. paper describe end-to-end speech system called deep speech deep learning supersedes processing stages. combined language model approach achieves higher performance traditional methods hard speech recognition tasks also much simpler. results made possible training large recurrent neural network using multiple gpus thousands hours data. system learns directly data require specialized components speaker adaptation noise ﬁltering. fact settings robustness speaker variation noise critical system excels deep speech outperforms previously published methods switchboard hub’ corpus achieving error performs better commercial systems noisy speech recognition tests. traditional speech systems many heavily engineered processing stages including specialized input features acoustic models hidden markov models improve pipelines domain experts must invest great deal effort tuning features models. introduction deep learning algorithms improved speech system performance usually improving acoustic models. improvement signiﬁcant deep learning still plays limited role traditional speech pipelines. result improve performance task recognizing speech noisy environment must laboriously engineer rest system robustness. contrast system applies deep learning end-to-end using recurrent neural networks. take advantage capacity provided deep learning systems learn large datasets improve overall performance. model trained end-to-end produce tapping beneﬁts end-to-end deep learning however poses several challenges must innovative ways build large labeled training sets must able train networks large enough effectively utilize data. challenge handling labeled data speech systems ﬁnding alignment text transcripts input speech. problem addressed graves fern´andez gomez schmidhuber thus enabling neural networks easily consume unaligned transcribed audio training. meanwhile rapid training large neural networks tackled coates demonstrating speed advantages multi-gpu computation. leverage insights fulﬁll vision generic learning system based large speech datasets scalable training surpass complicated traditional methods. vision inspired partly work applied early unsupervised feature learning techniques replace hand-built speech features. chosen model speciﬁcally well gpus novel model partition scheme improve parallelization. additionally propose process assembling large quantities labeled speech data exhibiting distortions system learn handle. using combination collected synthesized data system learns robustness realistic noise speaker variation taken together ideas sufﬁce build end-to-end speech system simpler traditional pipelines also performs better difﬁcult speech tasks. deep speech achieves error rate full switchboard hub’ test set—the best published result. further noisy speech recognition dataset construction system achieves word error rate best commercial systems achieve error. remainder paper introduce ideas behind speech recognition system. begin describing basic recurrent neural network model training framework section followed discussion optimizations data capture synthesis strategy conclude experimental results demonstrating state-of-the-art performance deep speech followed discussion related work conclusions. core system recurrent neural network trained ingest speech spectrograms generate english text transcriptions. single utterance label sampled training utterance time-series length every time-slice vector audio features spectrograms features denotes power p’th frequency audio frame time goal convert input sequence sequence character probabilities transcription {abc space apostrophe blank}. model composed layers hidden units. input hidden units layer denoted convention input. ﬁrst three layers recurrent. ﬁrst layer time output depends spectrogram frame along context frames side. remaining non-recurrent layers operate independent data time step. thus time ﬁrst layers computed min{max{ clipped rectiﬁed-linear activation function weight matrix bias parameters layer fourth layer bi-directional recurrent layer layer includes sets hidden units forward recurrence ﬁfth layer takes forward backward units inputs output layer standard softmax function yields predicted character probabilities time slice character alphabet computed prediction compute loss measure error prediction. training evaluate gradient ∇ˆyl respect network outputs given ground-truth character sequence point computing gradient respect model parameters done back-propagation rest network. nesterov’s accelerated gradient method training complete model illustrated figure note structure considerably simpler related models literature limited single recurrent layer long-short-term-memory circuits. disadvantage lstm cells require computing storing multiple gating neuron responses step. since forward backward recurrences sequential small additional cost become computational bottleneck. using homogeneous model made computation recurrent activations efﬁcient possible computing relu outputs involves highly optimized blas operations single point-wise nonlinearity. gone signiﬁcant lengths expand datasets recurrent networks still adept ﬁtting training data. order reduce variance further several techniques. commonly employed technique computer vision network evaluation randomly jitter inputs translations reﬂections feed jittered version network vote average results jittering common however found beneﬁcial translate audio ﬁles left right forward propagate recomputed features average output probabilities. test time also ensemble several rnns averaging outputs way. trained large quantities labeled speech data model learn produce readable character-level transcriptions. indeed many transcriptions likely character sequence predicted exactly correct without external language constraints. errors made case tend phonetically plausible renderings english words— table shows examples. many errors occur words rarely never appear training set. practice hard avoid training enough speech data hear words language constructions might need know impractical. therefore integrate system n-gram language model since models easily trained huge unlabeled text corpora. comparison speech datasets typically include million utterances n-gram language model used experiments section trained corpus million phrases supporting vocabulary words. table examples transcriptions directly errors ﬁxed addition language model given output perform search sequence characters probable according output language model speciﬁcally sequence maximizes combined objective tunable parameters control trade-off language model constraint length sentence. term denotes probability sequence according n-gram model. maximize objective using highly optimized beam search algorithm typical beam size range -—similar approach described hannun noted above made several design decisions make networks amenable highspeed execution example opted homogeneous rectiﬁedlinear networks simple implement depend highly-optimized blas calls. fully unrolled networks include almost billion connections typical utterance thus efﬁcient computation critical make experiments feasible. multi-gpu training accelerate experiments effectively requires additional work explain. order process data efﬁciently levels data parallelism. first processes many examples parallel. done usual concatenating many examples single matrix. instance rather performing single matrix-vector multiplication wrht recurrent layer prefer many parallel computing wrht corresponds i’th example time efﬁcient relatively wide thus prefer process many examples possible wish larger minibatches single support data parallelism across multiple gpus processing separate minibatch examples combining computed gradient peers iteration. typically data parallelism across gpus. data parallelism easily implemented however utterances different lengths since cannot combined single matrix multiplication. resolve problem sorting training examples length combining similarly-sized utterances minibatches padding silence necessary utterances batch length. solution inspired itpack/ellpack sparse matrix format similar solution used sutskever accelerate rnns text. data parallelism yields training speedups modest multiples minibatch size faces diminishing returns batching examples single gradient update fails improve training convergence rate. processing many examples many gpus fails yield speedup training. also inefﬁcient total minibatch size spread examples many gpus minibatch within shrinks operations become memory-bandwidth limited. scale further parallelize partitioning model model challenging parallelize sequential nature recurrent layers. since bidirectional layer comprised forward computation backward computation independent perform computations parallel. unfortunately naively splitting place separate gpus commits signiﬁcant data transfers compute thus chosen different partitioning work requires less communication models divide model half along time dimension. layers except recurrent layer trivially decomposed along time dimension ﬁrst half time-series assigned second half another gpu. computing recurrent layer activations ﬁrst begins computing forward activations second begins computing backward activations mid-point gpus exchange intermediate activations swap roles. ﬁrst ﬁnishes backward computation second ﬁnishes forward computation worked minimize running time recurrent layers since hardest parallelize. ﬁnal optimization shorten recurrent layers taking steps size original input unrolled half many steps. similar convolutional network step-size ﬁrst layer. cudnn library implement ﬁrst layer convolution efﬁciently. large-scale deep learning systems require abundance labeled data. system need many recorded utterances corresponding english transcriptions public datasets sufﬁcient scale. train largest models thus collected extensive dataset consisting hours read speech speakers. comparison summarized labeled datasets available table expand potential training data even data synthesis successfully applied contexts amplify effective number training samples work goal primarily improve performance noisy environments existing systems break down. capturing labeled data noisy environments practical however thus must ways generate data. ﬁrst order audio signals generated process superposition source signals. fact synthesize noisy training data. example speech audio track noise audio track form noisy speech track simulate audio captured noisy environment. necessary reverberations echoes forms damping power spectrum simply together make fairly realistic audio scenes. however risks approach. example order take hours clean speech create hours noisy speech need unique noise tracks spanning roughly hours. cannot settle hours repeating noise since become possible recurrent network memorize noise track subtract synthesized data. thus instead using single noise source length hours large number shorter clips treat separate sources noise superimposing them superimposing many signals collected video clips noise sounds different kinds noise recorded real environments. ensure good match synthetic data real data rejected candidate noise clips average power frequency band differed signiﬁcantly average power observed real noisy recordings. challenging effect encountered speech recognition systems noisy environments lombard effect speakers actively change pitch inﬂections voice overcome noise around them. effect show recorded speech datasets since collected quiet environments. ensure effect represented training data induce lombard effect intentionally data collection playing loud background noise performed sets experiments evaluate system. cases model described section trained selection datasets table predict character-level transcriptions. predicted probability vectors language model decoder yield word-level transcription compared ground truth transcription yield word error rate compare system prior research accepted highly challenging test hub’ researchers split easy hard instances often reporting results easier portion alone. full challenging case report overall word error rate. evaluate system trained hour switchboard conversational telephone speech dataset trained switchboard fisher hour corpus collected similar manner switchboard. many researchers evaluate models trained hours switchboard conversational telephone speech testing hub’. part training full hour fisher corpus computationally difﬁcult. using techniques mentioned section system able perform full pass hours data hours. since switchboard fisher corpora distributed sample rate compute spectrograms linearly spaced ﬁlter banks energy term. ﬁlter banks computed windows strided evaluate sophisticated features mel-scale ﬁlter banks mel-frequency cepstral coefﬁcients. speaker adaptation critical success current systems particularly trained hour switchboard. models test hub’ apply simple form speaker adaptation normalizing spectral features speaker basis. this modify input features way. decoding -gram language model word vocabulary trained fisher switchboard transcriptions. again hyperparameters decoding objective chosen cross-validation held-out development set. deep speech model network hidden layers neurons trained hour switchboard. deep speech model ensemble rnns hidden layers neurons trained full hour combined corpus. networks trained inputs frames context. report results table model vesely uses sequence based loss function using typical hybrid dnn-hmm system realign training set. performance model combined hub’ test best previously published result. trained combined hours data deep speech system improves upon baseline absolute relative. model maas achieves trained fisher hour corpus. system built using kaldi state-of-the-art open source speech recognition software. include result demonstrate deep speech trained comparable amount data competitive best existing systems. experimented noise played headphones well computer speakers. using headphones advantage obtain clean recordings without background noise included synthetic noise afterward. standards exist testing noisy speech performance constructed evaluation noisy noise-free utterances speakers. noise environments included background radio washing dishes sink; crowded cafeteria; restaurant; inside driving rain. utterance text came primarily search queries text messages well news clippings phone conversations internet comments public speeches movie scripts. precise control signal-to-noise ratio noisy samples aimed following experiments train rnns datasets listed table since train epochs newly synthesized noise pass model learns hours novel data. ensemble networks hidden layers neurons. form speaker adaptation applied training evaluation sets. normalize training examples utterance basis order make total power example consistent. features linearly spaced ﬁlter banks computed windows strided energy term. audio ﬁles resampled prior featurization. finally frequency remove global mean training divide global standard deviation primarily inputs well scaled early stages training. described section -gram language model decoding. train language model million phrases common crawl selected least characters phrase alphabet. common words kept rest remapped unknown token. compared deep speech system several commercial speech systems wit.ai google speech bing speech apple dictation. test designed benchmark performance noisy environments. situation creates challenges evaluating speech apis systems give result cases utterance long. therefore restrict comparison subset utterances systems returned non-empty result. results evaluating system test ﬁles appear table evaluate efﬁcacy noise synthesis techniques described section trained rnns hours data trained hours plus noise. clean utterances models perform same commoncrawl.org wit.ai google speech http-based apis. test apple dictation bing speech used kernel extension loop audio output back audio input conjunction dictation service windows bing speech recognition api. clean trained model noise trained model respectively. however noisy utterances noisy model achieves clean model’s absolute relative improvement. table results systems evaluated original audio. scores reported utterances predictions given systems. number parentheses next dataset e.g. clean number utterances scored. several parts work inspired previous results. neural network acoustic models connectionist approaches ﬁrst introduced speech pipelines early systems similar acoustic models replace stage speech recognition pipeline. mechanically system similar efforts build end-to-end speech systems deep learning algorithms. example graves previously introduced connectionist temporal classiﬁcation loss function scoring transcriptions produced rnns lstm networks previously applied approach speech similarly adopt loss part training procedure much simpler recurrent networks rectiﬁed-linear activations recurrent network similar bidirectional used hannun multiple changes enhance scalability. focusing scalability shown simpler networks effective even without complex lstm machinery. work certainly ﬁrst exploit scalability improve performance algorithms. value scalability deep learning well-studied parallel processors instrumental recent large-scale results early ports algorithms gpus revealed signiﬁcant speed gains researchers also begun choosing designs well hardware gain even efﬁciency including convolutional locally connected networks especially optimized libraries like cudnn blas available. indeed using high-performance computing infrastructure possible today train neural networks billion connections using clusters gpus. results inspired focus ﬁrst making scalable design choices efﬁciently utilize many gpus trying engineer algorithms models themselves. potential train large models need large training sets well. ﬁelds computer vision large labeled training sets enabled signiﬁcant leaps performance used feed larger larger systems speech recognition however large training sets less common typical benchmarks training sets ranging tens hours several hundreds hours larger benchmark datasets fisher corpus hours transcribed speech rare recently studied. fully utilize expressive power recurrent networks available rely large sets labeled utterances also synthesis techniques generate novel examples. approach well known computer vision found especially convenient effective speech done properly. presented end-to-end deep learning-based speech system capable outperforming existing state-of-the-art recognition pipelines challenging scenarios clear conversational speech speech noisy environments. approach enabled particularly multi-gpu training data collection synthesis strategies build large training sets exhibiting distortions system must handle combined solutions enable build data-driven speech system better performing existing methods longer relying complex processing stages stymied progress. believe approach continue improve capitalize increased computing power dataset sizes future. grateful whose work speech baidu spurred forward advice support throughout project. also thank lane povey jurafsky dario amodei andrew maas calisa cole helpful conversations.", "year": 2014}