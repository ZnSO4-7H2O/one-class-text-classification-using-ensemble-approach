{"title": "Generative Bridging Network in Neural Sequence Prediction", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In order to alleviate data sparsity and overfitting problems in maximum likelihood estimation (MLE) for sequence prediction tasks, we propose the Generative Bridging Network (GBN), in which a novel bridge module is introduced to assist the training of the sequence prediction model (the generator network). Unlike MLE directly maximizing the conditional likelihood, the bridge extends the point-wise ground truth to a bridge distribution conditioned on it, and the generator is optimized to minimize their KL-divergence. Three different GBNs, namely uniform GBN, language-model GBN and coaching GBN, are proposed to penalize confidence, enhance language smoothness and relieve learning burden. Experiments conducted on two recognized sequence prediction tasks (machine translation and abstractive text summarization) show that our proposed GBNs can yield significant improvements over strong baselines. Furthermore, by analyzing samples drawn from different bridges, expected influences on the generator are verified.", "text": "figure overall architecture novel generative bridging network main components namely generator network bridge module connected samples bridge module training time. generator implemented attentive encoder-decoder ﬁgure represents attention module. despite popularity teacher forcing neural sequence prediction tasks general issues always haunting data sparsity tendency overﬁtting harm model generalization. combat data sparsity different strategies proposed. take advantage monolingual data others modify ground truth target based derived rules similar examples training alleviate overﬁtting regularization techniques order alleviate data sparsity overﬁtting problems maximum likelihood estimation sequence prediction tasks propose generative bridging network novel bridge module introduced assist training sequence prediction model unlike directly maximizing conditional likelihood bridge extends point-wise ground truth bridge distribution conditioned generator optimized minimize kl-divergence. three different gbns namely uniform language-model coaching proposed penalize conﬁdence enhance language smoothness relieve learning burden. experiments conducted recognized sequence prediction tasks show proposed gbns yield signiﬁcant improvements strong baselines. furthermore analyzing samples drawn different bridges expected inﬂuences generator veriﬁed. sequence prediction widely used tasks outputs sequentially structured mutually dependent. recently massive explorations area made solve practical problems machine translation syntactic parsing spelling correction image captioning speech recognition armed modern computation power deep lstm based neural sequence prediction models achieved state-of-the-art performance. shown figure propose novel learning architecture titled generative bridging network combine beneﬁts synthetic data regularization. within architecture bridge module ﬁrst transforms point-wise ground truth bridge distribution viewed target proposer target examples drawn train generator. introducing different constraints bridge trained possess speciﬁc property drawn samples augment target-side data regularizing training generator network paper introduce three different constraints build three bridge modules. together generator network three systems constructed uniform instantiating constraint uniform distribution penalize conﬁdence; language-model instantiating constraint pre-trained neural language model increase language smoothness; coaching instantiating constraint generator’s output distribution seek closeto-generator distribution enables bridge draw easy-to-learn samples generator learn. without constraint degrades mle. uniform proved minimize kl-divergence so-called payoff distribution reward augmented maximum likelihood raml experiments conducted sequence prediction tasks namely machine translation abstractive text summarization. them proposed gbns signiﬁcantly improve task performance compared strong baselines. among them coaching achieves best. samples three different bridges demonstrated conﬁrm expected impacts training generator. summary contributions novel architecture proposed sequence prediction alleviate data sparsity overﬁtting problems bridge module generator network integrated jointly trained. proposed variants outperform baselines machine translation abstractive text summarization. similar relative improvements achieved compared recent state-of-the-art methods translation task. also demonstrate advantage gbns qualitatively comparing ground truth samples bridges. section ﬁrst give conceptual interpretation novel learning architecture sketched figure since data augmentation regularization golden solutions tackling data sparsity overﬁtting issues. willing design architecture integrate beneﬁts. basic idea so-called bridge transforms easyto-sample distribution distribution train meanwhile regularize sequence prediction model bridge viewed conditional distribution target given construct training pairs meantime could inject prior knowledge bridge optimization objective inspired design payoff distribution raml. formulate optimization objective parts equation expected similarity score computed similarity score function interpolated knowledge injection constraint controls minimizing empowers bridge distribution concentrate mass around ground truth also adopt certain hope property constructed bridge distribution optimize generator network match output distribution towards bridge distribution minimizing kl-divergence practice kl-divergence approximated sampling process detailed sec. matter fact bridge crux integration synthesizes targets alleviate data sparsity uses synthetic data regularization overcome overﬁtting. thus regularization-by-synthetic-example approach similar prior-incorporationby-virtual-example method generator network generator network parameterized commonly used encoder-decoder architecture encoder used encode input sequence sequence hidden states based attention mechanism leveraged compute context vectors decoding stage. context vector together previous decoder’s hidden state previously predicted label used time step compute next hidden state predict output label. claimed equation generator network trained maximize likelihood ground truth tries best match bridge distribution delegate ground truth. gradient descent optimize kldivergence respect generator optimization process viewed generator maximizing likelihood samples tribution however believe mathematical form restricted could motivate development. bridge module bridge module designed transform single target example bridge distribution design optimization target equation consist terms namely concentration requirement constraint. constraint instantiated kldivergence bridge contraint distribution transform equation follows convenient mathematical manipulation later kl||pc) predeﬁned score function measures similarity peaks reshapes bridge distribution. speciﬁcally ﬁrst term ensures bridge concentrate around ground truth second introduces willing property help regularize generator. hyperparameter interpreted temperature scales score function. following bridge speciﬁcations score function instantiated according sec. delta bridge delta bridge seen simplest case constraint imposed. bridge seeks minimize optimal solution bridge samples thus dirac delta distribution described follows although name bridge module explicitly learn generator closed-form static solution exists terms equation otherwise adopt encoder-decoder construct dynamic bridge network. language-model bridge bridge utilizes pretrained neural language model constraint motivates propose target examples conforming language ﬂuency. coaching bridge coaching bridge utilizes generator’s output distribution constraint motivates generate training samples easy understood generator relieve learning burden. coaching bridge follows spirit coach proposed imitation-via-coaching which reinforcement learning vocabulary advocates guide policy easy-to-learn action trajectories gradually approach oracle optimal action hard achieve. since constraint moving target generator updated coaching bridge remain static. therefore perform iterative optimization train bridge generator jointly. formally derivatives coaching bridge written follows ﬁrst term corresponds policy gradient algorithm described reinforce coefﬁcient −s/τ corresponds reward function. mutual dependence bridge module generator network design iterative training strategy i.e. networks take turns update parameters treating ﬁxed. training training three variants illustrated figure since proposed bridges divided static ones require pretraining dynamic ones require continual training generator describe training process details respectively. stratiﬁed-sampled training since closed-formed optimal distributions found uniform/lm gbns need draw samples static bridge distributions train sequence generator. unfortunately intractability bridge distributions direct sampling infeasible. therefore follow norouzi adopt stratiﬁed sampling approximate direct sampling process. given sentence ﬁrst sample edit distance randomly select positions replace original tokens. difference uniform bridge lies uniform bridge replaces labels drawing substitutions uniform distribution bridge takes history condition draws substitutions step-wise distribution. since kl-constraint moving target coaching bridge iterative training strategy designed alternately update generator bridge ﬁrst pre-train generator bridge start alternately update parameters. figure intuitively demonstrates intertwined optimization effects coaching bridge generator. hypothesize iterative training easyto-learn guidance could beneﬁt gradient update thus result better local minimum. figure four iterative updates coaching bridge generator. early stage pre-trained generator mass ground truth target points within output space shown coaching bridge ﬁrst updated equation locate dirac delta distribution generator’s output distribution. then sampling coaching bridge approximating equation target samples demonstrate easy-to-learn sequence segments facilitate generator optimized achieve closeness coaching bridge. process repeats generator converges. rogate n-gram matching reward follows .∗n+.∗n+.∗n+.∗n represents n-gram matching score order alleviate reward sparsity sequence level decompose global reward series local rewards every time step. formally write step-wise reward follows machine translation dataset follow ranzato bahdanau select german-english machine translation track iwslt evaluation campaign. corpus contains sentencewise aligned subtitles tedx talks. moses toolkit remove sentences longer words well lowercasing. evaluation metric bleu computed multi-bleu.perl. system setting uniﬁed gru-based generator coaching bridge. order compare existing papers similar system setting hidden units embedding size. attentive encoder-decoder build system during training apply adadelta optimize parameters generator coaching bridge. decoding beam size used approximate full search space. important hyper-parameter experiments temperature uniform/lm bridge follow norouzi adopt optimal temperature coaching bridge test hyper-parameters besides comparing ﬁne-tuned baseline systems comparison relative bleu improvement mixer softmax-q competing systems proposed yield improvement. also observe coaching achieved better performance uniform conﬁrms better regularization effects achieved generators become robust generalize better. draw learning curve bridge generator figure demonstrate cooperate training. easily observe interaction them generator makes progress coaching bridge also improves propose harsher targets generator learn. abstractive text summarization dataset follow previous works rush zhou corpus annotated english gigaword dataset order comparable script released rush pre-process extract training validation sets. test english gigaword released rush evaluate system rouge following previous works employ rouge- rouge- rouge-l evaluation metrics reported experimental results. system setting follow zhou rush input output vocabularies respectively also word embedding size hidden state size adopt dropout probability strategy output layer. attention-based sequence-tosequence model baseline reproduce results baseline reported zhou stated attentive encoder-decode architecture already outperform existing abs/abs+ systems coaching fact input abstractive summarization contains information summary target directly training bridge understand generator infeasible. therefore re-design coaching bridge receive source target input enlarge vocabulary size encompass information source side. uniform/lm experiments also hyper-parameter optimal setting. results experimental results summarized table observe signiﬁcant improvement systems. similarly coaching system achieves strongest performance among reﬂects assumption sophisticated regularization beneﬁt generator’s training. draw learning curve coaching figure demonstrate bridge generator promote other. introducing different constraints bridge module bridge distribution propose different training samples generator learn. table observe samples still reserve original meaning. uniform bridge simply performs random replacement withconsidering linguistic constraint. bridge strives smooth reference sentence high-frequent words. coaching bridge simpliﬁes difﬁcult expressions relieve generator’s learning burden. experimental results rational aggressive diversiﬁcation coaching clearly beneﬁts generator helps generator generalize unseen scenarios. related literature data augmentation self-training order resolve data sparsity problem neural machine translation many works conducted augment dataset. popular strategy self-learning incorporates self-generated data directly training. zhang zong sennrich self-learning leverage massive monolingual data training. bridge take advantage parallel training data only instead external monolingual ones synthesize training data. reward augmented maximum likelihood raml proposes integrate task-level reward training using exponentiated payoff distribution. divergence payoff distribution generator’s output distribution minimized achieve optimal task-level reward. following work introduces softmax q-distribution interpret raml reveals relation bayesian decision theory. works alleviate data sparsity problem augmenting target examples based ground truth. method draws inspiration seeks propose general generative bridging network transform ground truth different bridge distributions samples drawn account different interpretable factors. coaching system inspired imitation learning coaching instead directly behavior cloning oracle advocate learning hope actions targets coach interpolated learner’s policy environment loss. learner makes progress targets provided coach become harsher gradually improve learner. similarly proposed coaching motivated construct easy-to-learn bridge distribution lies ground truth generator. experimental results conﬁrm effectiveness relieve learning burden. paper present generative bridging network overcome data sparsity overﬁtting issues maximum likelihood estimation neural sequence prediction. implemented systems prove signiﬁcantly improve performance compared strong baselines. believe concept bridge distribution applicable wide range distribution matching tasks probabilistic learning. future intend explore gbn’s applications well provable computational statistical guarantees.", "year": 2017}