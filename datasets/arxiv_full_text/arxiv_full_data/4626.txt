{"title": "The Offset Tree for Learning with Partial Labels", "tag": ["cs.LG", "cs.AI"], "abstract": "We present an algorithm, called the Offset Tree, for learning to make decisions in situations where the payoff of only one choice is observed, rather than all choices. The algorithm reduces this setting to binary classification, allowing one to reuse of any existing, fully supervised binary classification algorithm in this partial information setting. We show that the Offset Tree is an optimal reduction to binary classification. In particular, it has regret at most $(k-1)$ times the regret of the binary classifier it uses (where $k$ is the number of choices), and no reduction to binary classification can do better. This reduction is also computationally optimal, both at training and test time, requiring just $O(\\log_2 k)$ work to train on an example or make a prediction.  Experiments with the Offset Tree show that it generally performs better than several alternative approaches.", "text": "present algorithm called oﬀset tree learning make decisions situations payoﬀ choice observed rather choices. algorithm reduces setting binary classiﬁcation allowing reuse existing fully supervised binary classiﬁcation algorithm partial information setting. show oﬀset tree optimal reduction binary classiﬁcation. particular regret times regret binary classiﬁer uses reduction binary classiﬁcation better. reduction also computationally optimal training test time requiring instead absolute loss bounds apply inherently noisy problems. turns simple approach regret scales square root regressor’s regret recalling latter upper bounded undesirable. another natural approach technique given distribution actions given idea transform partial label example importance weighted multiclass example ra/p cost predicting label input examples importance weighted multiclass classiﬁcation algorithm output classiﬁer used make future predictions. section shows uniform resulting regret original partial label problem bounded times importance weighted multiclass regret number choices. importance weighted multiclass classiﬁcation problem turn reduced binary classiﬁcation known conversions yield worse bounds approach presented paper. propose oﬀset tree algorithm reducing partial label problem binary classiﬁcation allowing reuse existing fully supervised binary classiﬁcation algorithm partial label problem. oﬀset tree uses following trick easiest understand case choices observed reward choice essentially pretend choice chosen diﬀerent reward observed. precisely done driven regret analysis. basic trick composable binary tree structure described section oﬀset tree achieves computational eﬃciency ways first improves dependence also oracle algorithm implies implicit optimization existing learning algorithms rather brute-force enumeration policies algorithm prove oﬀset tree policy regret bounded times regret binary classiﬁer solving induced binary problems. section shows reduction provide better guarantee giving ﬁrst nontrivial lower bound learning reductions. since bound tight dependence shows partial label problem inherently diﬀerent standard fully supervised learning problems like k-class classiﬁcation. problem considered non-interactive version contextual bandit problem background bandit problem). interactive version analyzed various additional assumptions including payoﬀs linear function side information algorithm nice assumption-free analysis. however intractable number policies want compete large. also relies careful control action choosing distribution thus cannot applied historical data i.e. non-interactively. sample complexity results policy evaluation reinforcement learning contextual bandits show empirical risk minimization type algorithms good policy non-interactive setting. results mostly orthogonal results although show section constant factor improvement sample complexity possible using oﬀset trick. banditron algorithm deals similar setting address several concerns oﬀset tree addresses banditron requires interactive setting; deals specialization setting reward choice choices; analysis specialized case linear separators small hinge loss exist; requires exponentially computation; banditron oracle algorithm unclear example compose decision tree bias. transformations partial label problems fully supervised problems thought learning methods dealing sample selection bias heavily studied economics statistics. importance weighted classiﬁcation generalization errors costly others. formally importance weighted classiﬁcation problem deﬁned distribution given training examples form cost associated mislabeling goal learn classiﬁer minimizing importance weighted loss y)]. costing method used resample training drawn using rejection sampling importance weights resampled eﬀectively drawn then binary classiﬁcation algorithm resampled optimize importance weighted loss costing runs base classiﬁcation algorithm multiple draws resampled averages learned classiﬁers making importance weighted predictions details). simplify analysis actually consider separate classiﬁers. simply augment feature space index resampled learn single classiﬁer union resampled data. implication observation view costing machine maps importance weighted examples unweighted examples. method algorithms below. binary oﬀset algorithm reduction -class partial label problem binary classiﬁcation. reduction operates example implying used either online oﬄine. state oﬄine case. algorithm reduces original problem binary importance weighted classiﬁcation reduced binary classiﬁcation using costing method described above. base binary classiﬁcation algorithm learner used subroutine. trick appears inside loop algorithm importance weighted binary examples formed. oﬀset changes range importances eﬀectively reducing variance induced problem. trick driven regret analysis section section proves regret transform theorem binary oﬀset reduction. informally regret measures well predictor performs compared best possible predictor problem. regret transform shows regret base classiﬁer induced problem controls regret resulting policy original problem. thus regret transform bounds excess loss suboptimal prediction. binary oﬀset transforms partial label examples binary examples. process implicitly transforms distribution deﬁning partial label problem distribution binary examples distribution importance weighted binary examples. note even though latter distribution depends action-choosing distribution induced binary distribution depends indeed unnormalized probability label given according theorem states policy regret bounded binary regret. surprising strictly less information available binary classiﬁcation. note lower bound section implies reduction better. redoing proof oﬀset rather also reveals rege bounds policy regret implying oﬀset trick gives factor improvement bound. condition follows ﬁxed taking expectation end. assume without loss generality e~r∼d|x e~r∼d|x. regrets sides claim holds trivially. otherwise makes mistake thus importance weighted regret binary classiﬁer policy regret. folk theorem section says importance weighted regret bounded binary regret times expected importance. latter technique previous section applied repeatedly using tree structure give algorithm general consider maximally balanced binary tree choices conditioned given observation every internal node tree associated classiﬁcation problem predicting inputs larger expected reward. node oﬀsetting technique used binary case described section internal node denote leaves subtree rooted every input node either leaf winning choice another internal node closer leaves. training algorithm oﬀset tree given algorithm testing algorithm deﬁning theorem gives extension theorem general analysis simple trick allows consider single induced binary problem thus single binary classiﬁer trick node index additional feature importance section reduction transforms partial label distribution distribution binary examples. draw draw action action-choosing distribution apply algorithm transform binary examples draw uniformly random. note independent explained beginning section note section shows reduction give better regret transform theorem. little side information however better oﬀset minimizing regret bound turns median value reward given thus generally best pair choices tend similar rewards. note algorithm need know well performs proof reworked oﬀset resulting regret bound proof ﬁrst step induction nodes tree. want show importance weighted regrets nodes subtree bounds regret output choice subtree. hypothesis trivially holds one-node trees. consider node making importance weighted decision choices without loss generality larger expected reward. shown proof theorem importance weighted binary regret wregu classiﬁer’s decision either predicts e~r∼d|x predicts costing theorem discussed section importance weighted regret bounded unweighted regret resampled distribution times expected importance. expected importance deciding actions setting akin boosting round booster creates input distribution calls oracle learning algorithm obtain classiﬁer error distribution depends classiﬁers returned oracle previous rounds. accuracy ﬁnal classiﬁer analyzed terms ǫt’s. binary problems induced internal nodes oﬀset tree depend similarly classiﬁers closer leaves. performance resulting partial label policy analyzed terms oracle’s performance problems. analysis simple trick beginning subsection consider single binary classiﬁer. theorem quantiﬁed classiﬁers thus holds classiﬁer returned algorithm. practice either call oracle multiple times learn separate classiﬁer node iterative techniques dealing fact classiﬁers dependent classiﬁers closer leaves. section shows method reducing partial label setting binary classiﬁcation better. first formalize learning reduction relies upon binary classiﬁcation oracle. lower bound prove holds learning reductions. advice. advice query consists single example feature vector binary label. advice query equivalent presenting oracle training example return value. algorithm takes unlabeled example binary classiﬁcation oracle input. asks sequence predict queries makes prediction dependent oracle’s predictions. oracle’s predictions adversarial proof proof construction. choose uniform examples example i-th component reward vector zeros elsewhere. corresponding feature vector consists binary representation index reward action-choosing distribution uniform. reduction produces simulatable sequence advice calls observed reward oracle ignores advice calls chooses answer queries zero error rate according sequence. inverse reduction access unlabeled example oracle since oracle’s answers independent draw output action reward probability reward probability implying regret respect best policy. factor greater regret oracle proving lower bound. section analyzes simple approaches reducing partial label problems basic supervised learning problems. approaches discussed previously analysis new. obvious approach regress value choice algorithm argmax classiﬁer algorithm instead learning single regressor learn separate regressor choice. proof choose action true value e∼d. action larger expected reward squared error regret suﬀered similarly regret order chosen must convexity regrets implies minima reached va+va arms. thus average regret noted partial label problem could reduced importance weighted multiclass classiﬁcation. algorithm creates importance weighted multiclass examples weights stripped using costing resulting multiclass distribution converted binary distribution using example all-pairs reduction last step done comparable analysis. all-pairs-train uses given binary learning algorithm learn distinguish pair classes multiclass distribution created costing. learned classiﬁer predicts given distinct pair classes whether class likely given test time make choice using all-pairs-test takes unlabeled example returns class wins pairwise comparisons according proof proof ﬁrst bounds policy regret terms importance weighted multiclass regret. then apply known results reductions relate policy regret binary classiﬁcation regret. relative oﬀset tree theorem undesirable extra factor regret bound. factor all-pairs reduction weak regret transform aware alternative approach reducing multiclass binary classiﬁcation composition yield regret transform oﬀset tree. ideally comparison would data source partial label setting. unfortunately data sort rarely available publicly used number publicly available multiclass datasets allowed queries reward value example. datasets report average result random splits dataset used training testing. figure shows error rates oﬀset tree plotted error rates regression importance weighting decision trees used base binary learning algorithm oﬀset tree importance weighting. regression approach learned separate regressor choices. reptree available weka used base regression algorithms. oﬀset tree clearly outperforms regression cases considerably. advantage importance weighting moderate often performance similar occasionally substantially better. figure error rates oﬀset tree versus regression approach using diﬀerent base regression algorithms oﬀset tree versus importance sampling several diﬀerent datasets using decision trees base classiﬁer learner. included. note although error rates appear large choosing among alternatives thus error rate less gives advantage random guessing. dataset-speciﬁc test error rates reported table banditron algorithm special case problem rewards rest sample complexity guarantees provided particularly good correct choice separated multiclass margin classes. chose binary perceptron base classiﬁcation algorithm since closest fully supervised learning algorithm banditron. exploration done according epoch-greedy instead epsilon-greedy motivated observation optimal rate exploration decay time. banditron tested dataset -class specialization reuters dataset consisting examples. precisely dataset made available authors since banditron analysis suggests realizable case dataset tested nearly perfectly separable also specialized oﬀset tree realizable case. particular realizable case freely learn every observation implying unnecessary importance weight also specialize epoch-greedy case using realizable bound resulting probability exploration decays rather /t/. algorithms compared according error rate. banditron error rate pass dataset realizable oﬀset tree method above error rate fully agnostic version oﬀset tree error rate results suggest tradeoﬀ optimal arbitrary noise performance little noise. no-noise situation realizable oﬀset tree performs substantially superior banditron. analyzed tractability learning outcome alternatives known reductions setting. oﬀset tree approach worst-case dependence reduction approach provide better guarantee furthermore computation oﬀset tree qualitatively eﬃcient known algorithms best experimental results suggest approach empirically promising. algorithms presented show learn step exploration. aggregating information multiple steps learn good policies using binary classiﬁcation methods. straightforward extension method deeper time horizons compelling replaced regret bounds. lower bound proved here appears progress multi-step problem framework must come additional assumptions. would like thank tong zhang alex strehl sham kakade helpful discussions. would also like thank shai shalev-shwartz providing data helping setup clean comparison banditron.", "year": 2008}