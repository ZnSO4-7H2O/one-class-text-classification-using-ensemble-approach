{"title": "Joint Extraction of Entities and Relations Based on a Novel Tagging  Scheme", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What's more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.", "text": "joint learning framework extract entities together relations using single model. effectively integrate information entities relations shown achieve better results task. however existing joint methods feature-based structured systems need complicated feature engineering heavily rely toolkits might also lead error propagation. order reduce manual work feature extraction recently presents neural network-based method end-to-end entities relations extraction. although joint models represent entities relations shared parameters single model also extract entities relations separately produce redundant information. instance sentence figure contains three entities united states trump apple inc. united states trump hold relation country-president. entity apple obvious relationship entities sentence. hence extracted joint extraction entities relations important task information extraction. tackle problem ﬁrstly propose novel tagging scheme convert joint extraction task tagging problem. then based tagging scheme study different end-toend models extract entities relations directly without identifying entities relations separately. conduct experiments public dataset produced distant supervision method experimental results show tagging based methods better existing pipelined joint learning methods. what’s more end-to-end model proposed paper achieves best results public dataset. joint extraction entities relations detect entity mentions recognize semantic relations simultaneously unstructured text figure shows. different open information extraction whose relation words extracted given sentence task relation words extracted predeﬁned relation appear given sentence. important issue knowledge extraction automatic construction knowledge base. traditional methods handle task pipelined manner i.e. extracting entities ﬁrst recognizing relations separated framework makes task easy deal with component ﬂexible. neglects relevance sub-tasks paper focus extraction triplets composed entities relation entities. therefore model triplets directly rather extracting entities relations separately. based motivations propose tagging scheme accompanied end-to-end model settle problem. design kind novel tags contain information entities relationships hold. based tagging scheme joint extraction entities relations transformed tagging problem. also easily neural networks model task without complicated feature engineering. recently end-to-end models based lstm successfully applied various tagging tasks named entity recognition supertagging chunking lstm capable learning long-term dependencies beneﬁcial sequence modeling tasks. therefore based tagging scheme investigate different kinds lstm-based end-to-end models jointly extract entities relations. also modify decoding method adding biased loss make suitable special tags. method proposed supervised learning algorithm. reality however process manually labeling training large number entity relation expensive error-prone. therefore conduct experiments public dataset produced distant supervision method validate approach. experimental results show tagging scheme effective task. addition end-to-end model achieve best results public dataset. major contributions paper novel tagging scheme proposed jointly extract entities relations easily transform extraction problem tagging task. based tagging scheme study different kinds end-to-end models settle problem. tagging-based methods better existing pipelined joint learning methods. furthermore also develop end-toentities relations extraction important step construct knowledge base beneﬁt many tasks. main frameworks widely used solve problem extracting entity relationships. pipelined method joint learning method. pipelined method treats task separated tasks i.e. named entity recognition relation classiﬁcation classical models linear statistical models hidden markov models conditional random fields recently several neural network architectures successfully applied regarded sequential token tagging task. existing methods relation classiﬁcation also divided handcrafted feature based methods neural network based methods joint models extract entities relations using single model. joint methods feature-based structured systems recently uses lstmbased model extract entities relations reduce manual work. different methods method proposed paper based special tagging manner easily end-toend model extract results without end-to-end method input sentence meaningful vectors back produce sequence. widely used machine translation sequence tagging tasks methods apply bidirectional lstm encode input sentences decoding methods always different. examples layers decode sequence propose novel tagging scheme end-toend model biased objective function jointly extract entities relations. section ﬁrstly introduce change extraction problem tagging problem based tagging method. detail model used extract results. figure example results tagged. word assigned label contributes extract results. represents other means corresponding word independent extracted results. addition tags consist three parts word position entity relation type relation role. bies signs represent position information word entity. relation type information obtained predeﬁned relations relation role information represented numbers extracted result represented triplet means word belongs ﬁrst entity triplet belongs second entity behind relation type. thus total number tags size predeﬁned relation set. figure example illustrating tagging method. input sentence contains {united states country-president triplets trump} {apple company-founder steven paul jobs} country-president company-founder predeﬁned relation types. words unitedstates trumpappleinc steven paul jobs related ﬁnal extracted results. thus tagged based special tags. example word united ﬁrst word entity united states related relation country-president b-cp-. entity trump corresponding united states labeled s-cp-. besides words irrelevant ﬁnal result labeled sequence figure know trump united states share relation type country-president apple steven paul jobs share relation type company-founder. combine entities relation type triplet ﬁnal result. accordingly trump united states combined triplet whose relation type country-president. because relation role trump united states ﬁnal result {united states countrypresident trump}. applies {apple company-founder steven paul jobs}. besides sentence contains triplets relation type combine every entities triplet based nearest principle. example relation type country-president figure company-founder four entities given sentence relation type. united states closest entity trump apple closest jobs results {united states companyfounder trump} {apple companyfounder steven paul jobs}. recent years end-to-end model based neural network widely used sequence tagging task. paper investigate end-toend model produce tags sequence figure shows. contains bi-directional long short term memory layer encode input sentence lstm-based decoding layer biased loss. biased loss enhance relevance entity tags. bi-lstm encoding layer. sequence tagging problems bi-lstm encoding layer shown effectiveness capture semantic information word. contains forward lstm layer backward lstm layer concatenate layer. word embedding layer converts word -hot representation embedding vector. hence sequence words represented ...wt wt+...wn} d-dimensional word vector input gate forget gate output gate respectively bias term cell memory parameters. word forward lstm layer encode considering contextual informa− tion word marked similar backward lstm layer code based contextual information finally conwn marked catenate represent word encoding information denoted former hidden vector decoding layer structure diagram memory block lstmd shown figure detail operations deﬁned follows corresponding t-th word sentence length given sentence. word embedding layer parallel lstm layers forward lstm layer backward lstm layer. lstm architecture consists recurrently connected subnets known memory blocks. time-step lstm memory block. lstm memory block bi-lstm encoding layer used compute current hidden vector based previous hidden vector previous cell vector current input word embedding structure diagram shown figure detail operations deﬁned follows softmax matrix total number tags. similar embedding lstm capable learning long-term dependencies decoding manner model interactions. bias objective function. train model maximize log-likelihood data optimization method used rmsprop proposed hinton objective function deﬁned size training length sentence label word sentence normalized probabilities tags deﬁned formula besides switching function distinguish loss relational tags indicate results. deﬁned follows experimental setting dataset evaluate performance methods public dataset produced distant supervision method large amount training data obtained means distant supervision methods without manually labeling. test manually labeled ensure quality. total training data contains triplets test contains triplets. besides size relation evaluation adopt standard precision recall score evaluate results. different classical methods method extract triplets without knowing information entity types. words label entity types train model therefore need consider entity types evaluation. triplet regarded correct relation type head offsets corresponding entities correct. besides groundtruth relation mentions given none label excluded did. create validation randomly sampling data test remaining data evaluation based suggestion. times experiment report average results standard deviation table shows. hyperparameters model consists bilstm encoding layer lstm decoding layer bias objective function. word embeddings used encoding part initialed running wordvec training corpus. dimension word embeddings regularize network using dropout embedding layer dropout ratio number lstm units encoding layer number decoding layer bias parameter corresponding results table three https//github.com/shanzhenren/cotype. data sets public resource dataset. data bioinfer overlapping relations beyond scope paper. dataset wiki-kbp number relation type test train also suitable supervised training method. details data found ren’s paper. https//code.google.com/archive/p/wordvec/ table predicted results different methods extracting entities relations. ﬁrst part pipelined methods second part jointly extracting methods. tagging methods shown part three part report results precision recall also compute standard deviation. baselines compare method several classical triplet extraction methods divided following categories pipelined methods jointly extracting methods end-to-end methods based tagging scheme. pipelined methods follow settings results obtained cotype several classical relation classiﬁcation methods applied detect relations. methods ds-logistic distant supervised feature based method combines advantages supervised unsupervised features; line network embedding method suitable arbitrary types information networks; compositional model combines lexicalized linguistic context word embeddings relation extraction. jointly extracting methods used paper listed follows ds-joint supervised method jointly extracts entities relations using structured perceptron human-annotated dataset; multir typical distant supervised method based multi-instance learning algorithms combat noisy training data; cotype domain independent framework jointly embedding entity mentions relation mentions text features type labels meaningful representations. lstm-crf proposed entity recognition using bidirectional lstm encode input sentence conditional random ﬁelds predict entity sequence. different lstm-crf lstm-lstm uses lstm layer decode sequence instead crf. used ﬁrst time jointly extract entities relations based tagging scheme. report results different methods shown table seen method lstm-lstm-bias outperforms methods score achieves improvement best method cotype shows effectiveness proposed method. furthermore table also jointly extracting methods better pipelined methods tagging methods better jointly extracting methods. also validates validity tagging scheme task jointly extracting entities relations. compared traditional methods precisions end-to-end models signiﬁcantly improved. lstm-lstm-bias better balance precision recall. reason end-to-end models bi-lstm encoding input sentence different neural networks decode results. methods based neural networks well data. therefore learn common features training well lead lower expansibility. also lstm-lstm model better lstm-crf model based tagging scheme. because lstm capable learning long-term dependencies good capturing joint probability entire sequence labels. related tags long distance other. hence lstm decoding manner little better crf. lstm-lstm-bias adds bias weight enhance effect entity tags weaken effect invalid tag. therefore tagging scheme method better common lstm-decoding methods. error analysis paper focus extracting triplets composed entities relation. table shown predict results task. treats triplet correct relation type head offsets corresponding entities correct. order factors affect results end-to-end models analyze performance predicting element triplet table shows. represent performance predicting entity respectively. head offset ﬁrst entity correct instance correct regardless relation type head offsets corresponding entities correct instance correct. shown table higher precision compared recall result lower means predicted entities form pair. obtain corresponding obtain corresponding thus leads prediction single less pairs. therefore entity pair higher precision lower recall single besides predicted results table improvement compared predicted results table means test data predifferent lstm-crf lstm-lstm approach biased towards relational labels enhance links entities. order analyze effect bias objective function visualize ratio predicted single entities end-to-end method figure single entities refer cannot corresponding entities. figure shows whether method relatively ratio single entities. means method effectively associate entities compared lstm-crf lstm-lstm little attention relational tags. besides also change bias parameter predicted results shown figure large affect accuracy prediction small recall decline. lstm-lstmbias balance precision recall achieve best scores. econtain condos area econtain sales rose march compared year earlier. panama city beach condos area econtain sales rose march compared year earlier. econtain condos area econtain sales rose march compared year earlier. came econtain econtain center brass production since middle ages. came nuremberg econtain center brass production since econtain. came nuremberg econtain center brass production since econtain. co-founder process going public made million last year. co-founder process going public made million last year. co-founder process going public made million last year. other difﬁcult detect relationships. compared lstmlstm lstm-lstm-bias uses bias objective function enhance relevance entities. therefore example lstm-lstmbias extract related entities lstmlstm extract entity florida detect entity panama city beach. negative example shows methods mistakenly predict entity. indicative words entities uremberg germany. besides patten germany iddleages easy mislead models exists relation contains them. problem solved adding samples kind expression patterns training data. case models predict entities’ head offset right relational role wrong. lstm-lstm treats stephen schwarzman blackstone group entity corresponding although lstm-lsmt–bias entities pair reverses roles stephen schwarzman blackstone group. shows lstm-lstm-bias able better pre. case study section observe prediction results end-to-end methods select several representative examples illustrate advantages disadvantages methods table shows. example contains three ﬁrst gold standard second third rows extracted results model lstm-lstm lstm-lstm-bias respectively. paper propose novel tagging scheme investigate end-to-end models jointly extract entities relations. experimental results show effectiveness proposed method. still shortcoming identiﬁcation overlapping relations. future work replace softmax function output layer multiple classiﬁer word multiple tags. word appear multiple triplet results solve problem overlapping relations. although model enhance effect entity tags association corresponding entities still requires reﬁnement next works. thank xiang dataset details helpful discussions. work also supported national high technology research development program china national natural science foundation china nsfc project", "year": 2017}