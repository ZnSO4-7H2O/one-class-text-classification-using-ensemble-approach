{"title": "Transferring Knowledge from a RNN to a DNN", "tag": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "abstract": "Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art results in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent Neural Network (RNN) models have been shown to outperform DNNs counterparts. However, state-of-the-art DNN and RNN models tend to be impractical to deploy on embedded systems with limited computational capacity. Traditionally, the approach for embedded platforms is to either train a small DNN directly, or to train a small DNN that learns the output distribution of a large DNN. In this paper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We use the RNN model to generate soft alignments and minimize the Kullback-Leibler divergence against the small DNN. The small DNN trained on the soft RNN alignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task compared to a baseline 4.54 WER or more than 13% relative improvement.", "text": "teries) latency requirements unfortunately many state-of-the-art models simply expensive impractical embedded platforms. traditionally approach simply small reducing number layers number neurons layer; however approaches often suffer word error rate performance degradations paper seek improve small models applied embedded platforms. dnns rnns typically trained forced alignments generated gmm-hmm system. refer hard alignment posterior distribution concentrated single acoustic state acoustic context. evidence alignment labels optimal training labels seen alignments make various assumptions data independence acoustic frames given states paper show soft distribution labels generated expert potentially informative hard alignments leading improvements. effects poor alignment quality hidden away large deep networks sufﬁcient model capacity. however narrow shallow networks training alignments often hurts performance approach change training criteria rather trying match alignments instead match distribution expert model small trained match output distribution large dnn. training data labels generated passing labelled unlabelled data large training small match output distribution. results promising achieved reduction baseline systems. another approach train model match softmax logits expert model. ensemble experts trained used teach dnn. motivation inference however principle model compression applies also generalized framework showed train models match logits softmax rather directly modelling distributions could yield knowledge transfer. paper want maximize small model performance targeted embedded platforms. transfer knowledge expert small dnn. ﬁrst build large acoustic model small model learn distribution soft alignment large model. show technique yield improvements deep neural network acoustic models yielded many state-of-the-art results automatic speech recognition tasks. recently recurrent neural network models shown outperform dnns counterparts. however state-of-the-art models tend impractical deploy embedded systems limited computational capacity. traditionally approach embedded platforms either train small directly train small learns output distribution large dnn. paper utilize state-of-the-art transfer knowledge small dnn. model generate soft alignments minimize kullback-leibler divergence small dnn. small trained soft alignments achieved wall street journal eval task compared baseline relative improvement. index terms deep neural networks recurrent neural networks automatic speech recognition model compression embedded platforms deep neural networks combined hidden markov models shown perform well across many automatic speech recognition tasks dnns accept acoustic context inputs models posterior distribution acoustic model. deep critical state-of-the-art models often contain multiple layers non-linearities giving powerful modelling capabilities recently recurrent neural networks demonstrated even potential counterparts models neural network models contain recurrent connections cycles connectivity graph. models unrolled actually seen special case dnn. recurrent nature allows model temporal dependencies often case speech sequences. particular recurrent structure model allows store temporal information within model. rnns shown outperform dnns large commercial systems. rnns shown provide better performance dnns robust asr. currently much industry interest embedded platforms example mobile phones tablets smart watches. however platforms tend limited computational capacity limited power availability divergence distributions. namely given posterior distribution posterior distribution want minimize divergence acoustic states cross entropy term entropy term. safely ignore entropy term since gradient zero respect small parameters. thus minimizing divergence equivalent minimizing cross entropy error distributions scenarios dnns rnns typically trained forced alignments generated gmm-hmm models model posterior distribution. refer alignment hard alignment probability concentrated single state. furthermore alignment labels generated gmm-hmm model always optimal training dnns gmm-hmm makes various assumptions true possible solution labels alignments another expert model example ensemble experts used teach model. paper generate labels expert provide better training targets compared alignments. possibility generate hard alignments expert. done ﬁrst training hard alignments gmm-hmm model. trained realign data taking hard alignments trained rnn. alignment hard takes probable phoneme state acoustic context probability concentrated single phoneme state. hand could utilize full distribution soft alignment associated acoustic frame. precisely acoustic context take full distribution phonetic states probabilities. however suffers several problems. first training need either parallel pre-cache distribution disk. running parallel expensive operation undesirable. alternative caching distribution disk would require obscene amounts storage example would take store full distribution dataset. also bandwidth issues loading training samples disk cache. finally entire distribution useful many states paper structured follows. section begins introduction state-of-the-art acoustic model. section describe methodology used transfer knowledge large model small model. section gives experiments results analysis. ﬁnish section conclusion future work discussions. exist many implementations rnns lstm particular implementation easy train suffer vanishing exploding gradient problem backpropagation time follow lstm implementation particular lstm implementation omits bias peephole connections. also apply cell clipping ease optimization avoid exploding gradients. lstms also extended bidirectional lstm capture temporal dependencies directions rnns also extended deep architectures evidence deep models perform better shallow models additional layers nonlinearities give network additional model capacity similar multiple layers nonlinearities dnn. follow building deep rnn; exact particular model actually termed tc-dnn-blstmdnn model. architecture begins time convolution input features followed signal processor project features higher dimensional space. projected features consumed blstm modelling acoustic context sequence. finally softmax layer used model posterior distribution. model gave relative improvement previous state-of-the-art dnns wall street journal eval task. paper tc-dnn-blstm-dnn model deep generate training alignments small learn from. goal transfer knowledge expert small dnn. follow approach similar transfer knowledge training match rnn’s output distribution. note train soft distribution rather top- state paper show distribution generated informative alignments. also show soft distribution informative taking top- state generated rnn. solution sits inbetween extremes taking top- state taking full distribution. posterior distributions typically concentrated states. therefore make almost full distribution storing small portion states probability distribution. take states contains probability distribution. note different taking top-k states take least states capture least distribution vary frame. re-normalize probability frame ensure distribution sums lossy compression method losses original probability mass. experiment dataset; approximately hours speech training development eval test set. observe development every epoch stop training development longer improves. report converged corresponding eval wers. fmllr features generated kaldi recipe decoding setup exactly recipe trib alignments hard forced alignment training targets total acoustic states. trib baseline achieved test respectively. optimization procedure initialized networks randomly used stochastic gradient descent minibatch size apply gradient clipping gradient projection lstm. experimented constant learning rates geometric decayed learning rates initial values decay factor report best wers learning rate hyperparameter optimizations. ﬁrst built several baseline systems. large networks suitable deployment mobile platforms. followed kaldi recipe built layer neurons hidden layer pretraining achieves eval also followed built layer relu neurons hidden layer achieves eval model follows consists neurons layer layers bidirectional cells blstm. model achieves eval signiﬁcantly better want build small easily computable embedded device. decided layer network wherein hidden layer relu neurons ﬁnal softmax acoustic states matching gmm. since matrix-matrix multiplication operation effect approximately times reduction number computations hidden layers allow perform fast interference embedded platforms limited cpu/gpu capacity. ﬁrst trained small relu using hard alignments. achieved compared relu model eval task. small model large model; suggests model able optimize substantially better. large model significantly model capacity thus yielding better results small dnn. next experimented hard alignment. take top- state model train towards alignment. improvement improves eval degrades suggests hard alignments worse labels original alignments. information provided looking state informative hard alignments. hypothesis model overﬁts towards hard alignments since able improve model unable generalize performance eval test set. experiment soft alignment wherein soft distribution characteristics small dnn. take percentile probabilities distribution renormalize minimize divergence soft alignments small dnn. signiﬁcant improvement wer. achieve eval eval scenario improves relative compared baseline hard alignment. almost able match despite many layers neurons. soft alignment adds considerable information training labels hard alignments hard alignments. also experimented training soft alignments. model relu model mentioned table wherein achieved eval again generate soft alignments train small minimize divergence. achieved eval several things note ﬁrst improve baseline relative. next close soft alignment however widens look eval suggests model overﬁts soft alignments soft alignments provide generalization. quality soft alignments much better soft alignments. table summarizes wers small model using different training alignments. compute various models alignment dataset. measure since stopping criteria optimization loss. give better indication optimization procedure models overﬁtting. table summarizes measurements. several observations ﬁrst able achieve lower compared dnn. model able optimize better seen better wers model provides. expected since model achieves best wer. next observation small dnns trained soft alignment large achieved lower compared small trained hard alignment. suggests soft alignment labels indeed better training labels optimizing model. extra information contained soft alignment helps optimize better towards dataset. small trained soft alignments soft alignments give interesting results. models achieved lower compared large large models trained alignments. however wers worse large large models. suggests small model trained soft distribution overﬁtting unclear overﬁtting occurs smaller model generalize well large model overﬁtting occurs quality soft alignment labels. motivation application work extend onto embedded platforms limited computational capacity. paper introduced method transfer knowledge small dnn. minimize divergence distributions match dnn’s output rnn’s output. improve trained forced alignments soft alignments generated rnn. method resulted relative improvement additional inference cost. question answer paper whether small dnn’s model capacity rnn’s soft alignment bottleneck performance. measure effect small dnn’s model capacity would similar wers increased decreased small dnn’s size? bottleneck quality soft alignments princple could reduce small dnn’s size without impacting however model capacity issue smaller networks. similar question investigate impact probability selection alignment. threshold probabilities convenience however would selecting less probabilities affect quality alignments. extreme case wherein selected top- probability found model perform much worse compared soft alignments even worse alignments evidence deﬁnitely shows importance information contained soft alignment. could also extend work similar utilize vast amounts unlabelled data improve small dnn. applied unlabelled data large expert generate vast quantities soft alignment labels small learn from. principle could extend inﬁnite amount training data synthetic data generation shown improve performance finally experiment sequence training sequence training almost always shown help would interesting effects sequence training small models whether improve performance. chan lane deep convolutional neural networks acoustic modeling resource languages ieee international conference acoustics speech signal processing povey ghoshal boulianne burget glembek goel hannenmann motlicek qian schwarz silovsky stemmer vesely kaldi speech recognition toolkit automatic speech recognition understanding workshop vinyals heigold senior mcdermott monga sequence discriminative distributed training long short-term memory recurrent neural networks interspeech dahl deng acero context-dependent pre-trained deep neural networks large-vocabulary speech recognition ieee transactions audio speech language processing vol. january soltau saon sainath joint training convoutional non-convoutional neural networks ieee international conference acoustics speech signal processing zeiler ranzato monga yang nguyen senior vanhoucke dean hinton rectiﬁed linear units speech processing ieee international conference acoustics speech signal processing dahl sainath hinton improving deep neural networks lvcsr using rectiﬁed linear units dropout ieee international conference acoustics speech signal processing graves rahman mohamed hinton speech recognition deep recurrent neural networks ieee international conference acoustics speech signal processing hinton vinyals dean distilling knowledge neural network neural information processing systems workshop deep learning representation learning workshop chung gulcehre bengio empirical evaluation gated recurrent neural networks sequence modeling neural information processing systems workshop deep learning representation learning workshop", "year": 2015}