{"title": "Group-Sparse Signal Denoising: Non-Convex Regularization, Convex  Optimization", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Convex optimization with sparsity-promoting convex regularization is a standard approach for estimating sparse signals in noise. In order to promote sparsity more strongly than convex regularization, it is also standard practice to employ non-convex optimization. In this paper, we take a third approach. We utilize a non-convex regularization term chosen such that the total cost function (consisting of data consistency and regularization terms) is convex. Therefore, sparsity is more strongly promoted than in the standard convex formulation, but without sacrificing the attractive aspects of convex optimization (unique minimum, robust algorithms, etc.). We use this idea to improve the recently developed 'overlapping group shrinkage' (OGS) algorithm for the denoising of group-sparse signals. The algorithm is applied to the problem of speech enhancement with favorable results in terms of both SNR and perceptual quality.", "text": "given residual energy. however non-convex formulations generally difﬁcult solve also solutions produced non-convex formulations generally discontinuous functions input data generally convex approaches based sparsitypromoting convex penalty functions non-convex approaches based non-convex penalty functions re-weighted non-convex algorithms seek sparse solutions directly iterative hard thresholding greedy work take different approach proposed blake zimmerman nikolova namely non-convex non-smooth penalty function chosen total cost function strictly convex. possible balancing positive second derivatives negative second derivatives terms idea extended nikolova contribution work relates formulation group-sparse denoising problem convex optimization problem albeit deﬁned terms non-convex penalty function derivation computationally efﬁcient iterative algorithm monotonically reduces cost function value. utilize non-convex penalty functions parametric forms; identify interval parameter ensures strict convexity total cost function total cost function strictly convex minimizer unique obtained reliably using convex optimization techniques. algorithm present derived according principle majorization-minimization proposed approach abstract—convex optimization sparsity-promoting convex regularization standard approach estimating sparse signals noise. order promote sparsity strongly convex regularization also standard practice employ nonconvex optimization. paper take third approach. utilize non-convex regularization term chosen total cost function convex. therefore sparsity strongly promoted standard convex formulation without sacriﬁcing attractive aspects convex optimization idea improve recently developed ‘overlapping group shrinkage’ algorithm denoising group-sparse signals. algorithm applied problem speech enhancement favorable results terms perceptual quality. additive white gaussian noise assume group-sparse vector. group-sparse mean large magnitude values tend isolated. rather large magnitude values tend form clusters furthermore assume group locations known group boundaries known. fact assume groups well deﬁned boundaries. example vector spectrogram speech waveform. spectrogram speech waveform exhibits areas ridges large magnitude isolated large values. method proposed work demonstrated problem speech ﬁltering. regularization term convex formulations advantageous wealth convex optimization theory leveraged robust algorithms guaranteed convergence available hand non-convex approaches advantageous usually yield sparser solutions authors department electrical computer engineering polytechnic institute york university metrotech center brooklyn email poyupaulchengmail.com selesipoly.edu phone denote i-th group vector size consistently denote group size. boundaries indices fall outside take values zero; i.e. take denote non-negative real line positive real line given function left-sided rightsided derivatives denoted respectively. notation denotes difference; i.e. estimation reconstruction signals group sparsity properties addressed numerous authors. make distinction cases non-overlapping groups overlapping groups nonoverlapping case easier case groups non-overlapping decoupling variables simpliﬁes optimization problem. groups overlapping variables coupled. case common deﬁne auxiliary variables apply methods alternating direction method multipliers approach increases number variables hence increases memory usage data indexing. previous work describe ‘overlapping group shrinkage’ algorithm overlapping-group case auxiliary variables. algorithm exhibits favorable asymptotic convergence comparison algorithms auxiliary variables comparison previous work convex optimization overlapping group sparsity including current work promotes sparsity strongly. current work extends algorithm case non-convex regularization remains within convex optimization framework. noted above balancing data consistency term penalty term formulate convex problem non-convex penalty term described refs. extended approach used initialize scheme named ‘graduated non-convexity’ goal minimize non-convex function minimizing sequence functions ﬁrst convex approximation subsequent ones non-convex progressively similar order initial approximation convex penalty function must satisfy eigenvalue condition looser condition promotes sparsity strongly expressed semideﬁnite program incurs higher computational cost method described here balancing idea gnc; however goal minimize convex function non-convex gnc. particular balancing idea construct convex function maximally promotes sparsity seek subsequently solve convex problem. note primary goal capture group sparsity behavior considered work. also note computationally demanding arising ref. arise current work. algorithm developed computationally simple. sparsity promoting penalty function satisfying assumptions sec. ii-b group size selected based size groups arising data. constitutes one’s ‘prior knowledge’ regarding group sparsity behavior data need obtained trialand-error. order leverage convex optimization principles avoid non-convex optimization issues seek restrict strictly convex. note minimization straight forward. first variables coupled overlapping group structure regularization term. component depends every data sample secondly differentiable. particular generally differentiable minimizer sparsity induced regularizer. reasons desirable strictly convex. efﬁcient sparse signal processing scalar case proximity operators thresholding/shrinkage function derived using convex penalty function. work utilize non-convex penalty functions; however still deﬁne threshold function similar deﬁnition proximity operator. following proposition closely related lemma theorem analyze behavior non-smooth necessarily convex proposition deﬁne multivariate threshold function figure illustrates threshold functions corresponding several penalty functions. threshold function corresponding absolute value penalty function called soft threshold function notice that except soft threshold function threshold functions approach identity function. atan threshold function approaches identity fastest. fact soft threshold function reduces large values constant amount considered deﬁciency. estimation sparse signals awgn behavior results systematic underestimation large magnitude signal values hence threshold functions asymptotically unbiased often preferred soft threshold function penalty functions derived promote sparsity strongly norm atan penalty function derived speciﬁcally favorable behavior regard order determine convexity ﬁrst consider simpler cost function consists single group. values ensure strictly convex? proposition consider functions deﬁned strictly convex. suppose strictly convex. increasing based convexity proposition ref. strictly convex hence strictly convex. suppose strictly convex. given deﬁne note symmetric. satisfying prove second part proposition according assumptions sec. ii-b continuous twice differentiable r\\{} symmetric. hence corollary sufﬁcient positive r\\{} hence condition absolute value function minimizer given point-wise soft thresholding current work addresses case non-convex regularizer promote group sparsity strongly comparison convex regularization. enhanced sparsity illustrated example sect. iv-a. adjustable. based multivariate laplace probability density function shape parameter analogous current work.) furthermore overlapping group sparsity considered using results above condition ensure strictly convex. result permits non-convex regularization strongly promote group sparsity preserving strict convexity total cost function theorem consider deﬁned give practical comments using parameters suggest chosen ﬁrst based structural properties signal denoised. suggest ﬁxed fraction maximal value; i.e. consider function according noise variance. sec. iii-e describe approaches selection numerical experiments speech enhancement found setting maximal value generally yields best results; i.e. hence examples sec. maximal value. equation suggest proposed method becomes ineffective large noted large small achieve desired degree noise suppression implies small. small turn limits nonconvexity regularizer. hence appears beneﬁt proposed non-convex regularization method diminished large however considerations offset reasoning. first larger smaller value needed achieve ﬁxed level noise suppression condition used determine values ensure strict convexity atan rational penalty functions obtain following intervals ensuring strict convexity based strictly convex function deﬁne multivariate threshold/shrinkage function scalar case informative note threshold multivariate thresholding function. since minimizer deduce following. suppose choose satisfying implies minimizer suppose satisfying hence minimizer arguments above conclude deﬁnes threshold absolute value function induced multivariate threshold function expressed closed form generalization case data consistency term also addressed form ky−axk note neither consider either non-convex regularization overlapping group sparsity. penalty function strictly concave positive real line induced multivariate threshold function results less bias large magnitude components; i.e. approaches identity function large exploration along lines given non-convexity quite mild however note components uncoupled. furthermore quadratic hence minimizer respect easily obtained. quantities readily computed; essentially double k-point convolution nonlinearity convolutions. derive algorithm minimizing strictly convex function majorization-minimization procedure procedure replaces single minimization problem sequence ones. speciﬁcally based iteration specify majorizer cost function ﬁrst specify majorizer penalty function simplify notation suppress dependence lemma assume satisﬁes assumptions sec. ii-b. deﬁned majorizer except i.e. note algorithm summarized table penalty function appears place computation therefore observed role penalty encapsulated function φ′/u. table lists function penalty functions given sec. ii-b. function φ′/u similar functional forms. similarity functions reveal close relationship among listed penalty functions. results algorithm described preceding sections extended multidimensional case straightforwardly. numerical experiments below twodimensional version algorithm order denoise time-frequency spectrogram noisy speech waveform. respectively. algorithm table essentially two-dimensional case. summations become double summations etc. extensions higher dimensional signals similarly straight forward. note undeﬁned singularity issue often arises quadratic function used majorize non-smooth function issue manifest algorithm whenever k-point group equal k-point zero vector; i.e. index iteration event occurrence algorithm would encounter ‘divideby-zero’ error. however occurrence guaranteed occur suitable initialization described example sufﬁcient initialize non-zero values i.e. initialization readily observed denominator strictly positive ﬁnite iterations components solution zero values approach zero limit; i.e. propose initializing i.e. exclude iteration table serves exclude components iterative update. case iterations justiﬁed part lemma consequence lemma initializing zero optimal. therefore algorithm excludes values update procedure already optimal. initialization readily observed above denominator strictly positive ﬁnite iterations assuming inﬁnite precision sufﬁcient deﬁne prior loop only; last line table indicated omitted. guaranteed division zero never occur discussed above. algorithm proceeds gradually attenuating toward optimal values attenuation multiplicative value never equals zero even though converge zero. many values reach ‘machine epsilon’ divide zero subsequently occur implementation. hence avoid possible divideby-zero errors ﬁnite precision arithmetic algorithm updates loop table small number ‘machine epsilon’ single precision ﬂoating point value usually considered zero. prove convergence algorithm minimizer complication singularity issue. however derivation based majorizationminimization principle guaranteed decrease cost function iteration. moreover practice observed extensive numerical investigation algorithm rapid convergence behavior convex regularized error problem denoising signal awgn unknown practice noise-free signal unknown. estimated using stein’s unbiased risk estimator estimate sure requires observation noise variance divergence estimator. however computation divergence intractable many estimators including ogs. overcome issue proposed ref. monte-carlo methods used. applied approach i.e. ‘monte-carlo sure’ estimate complex-valued speech spectrogram denoising using ogs. since spectrogram complex calculate ms-sure averaging real imaginary divergences figure illustrates calculated mcsure true functions estimated quite accurate mse-optimal value however disadvantage mc-sure high computational complexity. requires optimizations emulate divergence. noted ref. non-smooth estimators calculated mc-sure tends deviate randomly true calculated mc-sure closely follows true illustrated shows close continuous bounded. regularization parameter selection noise level suppression. regularization parameter selected using existing generic techniques lcurve method. however described approach based directly standard deviation awgn assume known. approach seeks preserve concepts scalar thresholding namely processing signal values based relative magnitude. consider problem estimating sparse signal awgn. many nonzero values sparse signal exceed noise ﬂoor suitable threshold value exceed noise ﬂoor. large else non-zero values sparse signal annihilated. hence reasonable value threshold noise zero. simplicity ‘three-sigma’ rule leveraged easily proposed algorithm. however still implement concept setting reduce noise speciﬁed fraction original power. purpose effect algorithm pure zero-mean gaussian noise measured computation. particular standard deviation output function found empirically recorded. example table records value several group sizes table used atan penalty function maximum value i.e. φatan). value also depends number iterations algorithm. computing table used ﬁxed number iterations. clarify table regularization parameter suppose one-dimensional signal denoising seeks algorithm reduces uses group size atan penalty function iterations according table group size table records discrete pairs linear interpolation α-logarithmic scale used estimate example seeks algorithm reduces according interpolation illustrated fig. approach penalty functions values complex data necessary compute additional tables. precomputed tables available supplementary material. using precomputed tables interpolation suitable value found quickly. tables assume noise awgn; noise models tables need precomputed. approach also effective two-dimensional denoising monte-carlo sure. another approach select regularization parameter based minimizing mean square figure shows synthetic group-sparse signal noisy signal shown fig. obtained adding white gaussian noise soft hard thresholding used threshold maximizes snr. values summarized table result obtained using prior version shown equivalent setting absolute value function; i.e. |x|. denote ogs. result using proposed non-convex regularized shown fig. arctangent penalty function maximum value preserves convexity i.e. φatan). denote ogs. also used logarithmic penalty version used group size maximize snr. non-convex regularization) substantially superior substantially higher almost residual noise visible denoised signal. comparing hard thresholding observed non-convex regularized algorithm also yields higher hard thresholding. example demonstrates effectiveness non-convex regularization promoting group sparsity. clearly compare result results shown together fig. fig. output value shown versus input value compared algorithm better preserves amplitude non-zero values original signal better thresholding small values. figure shows denoising error methods. observed denoised signal produced much less error ogs. error essentially zero signal values.) second experiment selected method reduce noise standard deviation described sec. iii-e. resulting snrs given second table much lower. cases attenuation large magnitude values. however observed especially non-convex regularization signiﬁcantly outperforms scalar thresholding. example evaluates proposed algorithm problem speech enhancement compare algorithm several algorithms. evaluation female male speakers multiple sentences noise levels sampling rates. sentence evaluation spoken male female speaker. sentences sampled sentences sampled khz. signals obtained ref. carnegie mellon university website respectively. simulate noisy speech added white gaussian noise. downloaded http//www.speech.cs.cmu.edu/cmu arctic/cmu arctic/wav http//www.speech.cs.cmu.edu/cmu arctic/cmu arctic/wav. evaluation used ﬁles arctic_a arctic_a. figure compares proposed algorithm prior version i.e. ogs. ﬁgure shows single frame denoised spectrograms corresponding seconds. prior proposed algorithms illustrated parts respectively. samples noise-free spectrogram recovered indicated dots. comparing observed estimates noise-free spectrum accurately ogs. regularization parameter. found empirically setting maximize yields speech noticeable undesirable perceptual artifacts known phenomenon residual noise stft domain. therefore instead regularization parameter using noise suppression approach described sec. iii-e. particular reduce noise standard deviation selected value optimize perceptual quality denoised speech quality. investigate effect group size denoised spectrograms using groups size illustrated fig. fig. shows noisy spectrogram highlight areas spectrogram. low-frequency area denoted exhibits high level temporal correlation. hand high-frequency area denoted exhibits high level spectral correlation. figs. show areas spectrogram obtained using group size figs. show areas spectrogram obtained using group size observed area group size suppresses inter-formant noise completely group size conversely area group size recovers original spectrogram accurately group size since area representative spectrogram area snr-optimal group size whole spectrogram however distortion high frequencies area group size yields perceptually inferior result. moreover lower inter-formant noise suppression group size appears negligible adverse impact perceptual quality. therefore even though group size yields higher female speaker group size evaluation superior perceptual quality. points potential value allowing groups sized adaptively ref. however explore extension work. conducted equivalent evaluations sampling rate order determine appropriate group size case. found group sizes optimal terms male female speaker respectively. above selected group size genders better perceptual quality. algorithm comparisons. table compare algorithm several speech enhancement algorithms. table summarizes output according informal listening tests. particular value effective suppressing ‘musical noise’ artifact. also note approach leads greater regularization snr-optimization group size. perceptual quality speech denoised using depends speciﬁed group size. apply time-frequency spectrogram size group respect temporal spectral dimensions must speciﬁed. denote number spectral temporal samples respectively. approach select pair parameters maximize denoising experiments. performed denoising noisy speech signals using pairs experiment used speech sampled selected case according preceding note found male speaker group size maximized frequently. conforms informal listening tests different group sizes. denoised spectrum figure obtained using group size highest output achieves second highest terms perceptual quality clearly audible artifacts; slight audible artifacts; least audible artifacts. however high computational complexity eigenvalue factorization. compared better preserves perceptual quality high frequencies. similar results observed different noise levels female speaker. empirical wiener post-processing improves methods noise levels least ogs. effective increasing effectively rescales large stft coefﬁcients unnecessarily attenuated algorithms fact yields least improvement demonstrates algorithm inherently induces less bias algorithms. according informal listening tests effect audible artifacts depends algorithm. although improves denoising artifacts still clearly perceptible. improves perceptual quality slightly. also improves perceptual quality already good perceptual quality. effect almost imperceptible; good perceptual quality maintained. figure illustrates individual snrs sentences denoised using utilized algorithms observed improves algorithm except ogs. however shown fig. outperforms algorithms terms irrespective ewp. sampling rates male female speakers input levels. value averaged sentences depending sampling rate. observed proposed algorithm achieves highest case. algorithms used comparison spectral subtraction log-mmse algorithm subspace algorithm block thresholding persistent shrinkage used matlab software provided ref. algorithms used software provided authors pages. furthermore additionally evaluated method empirical wiener post-processing technique based mean square error minimization effectiveness well demonstrated table values obtained using shown parenthesis algorithm scenario. positive real line promotes sparsity strongly convex regularizer can. several non-convex penalty functions parameterized variable shown constrain ensure optimization problem strictly convex. numerical experiments demonstrate effectiveness proposed method speech enhancement. note strictly convex either monotone increasing right-derivative monotone increasing leftderivative corollary suppose continuous second derivative exists satisfying r\\{}. strictly convex proof based proposition sufﬁcient prove right derivative monotone increasing since monotone increasing. also monotone increasing follows monotone increasing hence strictly convex. algorithm refer reader ref. particular remarks ref. regarding convergence behavior implementation issues computational complexity relationship focuss apply also version presented here. proximal framework proven effective convex optimization problems arising sparse signal estimation reconstruction proposed non-convex regularized algorithm resembles proximity operator; however proximity operator deﬁned terms convex penalty function hence proposed approach appears fall outside proximal framework. effectiveness proximal framework solving inverse problems much general denoising interest future work explore extent proposed method used general inverse problems using proximal-like techniques. paper formulates group-sparse signal denoising convex optimization problem non-convex regularization term. regularizer based overlapping groups promote group-sparsity. regularizer concave berouti schwartz makhoul. enhancement speech corrupted acoustic noise. proc. ieee int. conf. acoust. speech signal processing volume pages april boyd parikh peleato eckstein. distributed optimization statistical learning alternating direction method multipliers. foundations trends machine learning chartrand. exact reconstruction sparse signals nonconvex minimization. ieee signal processing letters chartrand wohlberg. nonconvex admm algorithm proc. ieee int. conf. acoust. combettes j.-c. pesquet. proximal splitting methods signal processing. bauschke editors fixed-point algorithms inverse problems science engineering pages springerverlag foucart m.-j. lai. sparsest solutions underdetermined linear systems ℓq-minimization appl. comp. harm. analysis march gasso rakotomamonjy canu. recovering sparse signals certain family nonconvex penalties programming. ieee trans. signal process. december mosci villa verri rosasco. primal-dual algorithm group sparse regularization overlapping groups. advances neural information processing systems pages moulin liu. analysis multiresolution image denoising ieee ramani unser. monte-carlo sure blackbox optimization regularization parameters general denoising algorithms. ieee trans. image process. september sendur selesnick. bivariate shrinkage functions waveletbased denoising exploiting interscale dependency. ieee trans. signal process. november mallat bacry. audio denoising time-frequency block thresholding. ieee trans. signal process. yuan efﬁcient methods overlapping group lasso. advances neural information processing systems pages", "year": 2013}