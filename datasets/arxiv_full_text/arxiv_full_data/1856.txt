{"title": "Unsupervised Document Embedding With CNNs", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "We propose a new model for unsupervised document embedding. Leading existing approaches either require complex inference or use recurrent neural networks (RNN) that are difficult to parallelize. We take a different route and develop a convolutional neural network (CNN) embedding model. Our CNN architecture is fully parallelizable resulting in over 10x speedup in inference time over RNN models. Parallelizable architecture enables to train deeper models where each successive layer has increasingly larger receptive field and models longer range semantic structure within the document. We additionally propose a fully unsupervised learning algorithm to train this model based on stochastic forward prediction. Empirical results on two public benchmarks show that our approach produces comparable to state-of-the-art accuracy at a fraction of computational cost.", "text": "propose model unsupervised document embedding. leading existing approaches either require complex inference recurrent neural networks difﬁcult parallelize. take different route develop convolutional neural network embedding model. architecture fully parallelizable resulting speedup inference time models. parallelizable architecture enables train deeper models successive layer increasingly larger receptive ﬁeld models longer range semantic structure within document. additionally propose fully unsupervised learning algorithm train model based stochastic forward prediction. empirical results public benchmarks show approach produces comparable state-of-the-art accuracy fraction computational cost. document representation machine reasoning fundamental problem natural language processing typical approach develop document embedding model produces ﬁxed length vector representations accurately preserve semantic information within document models supervised unsupervised work focus unsupervised category models trained using unlabeled text. unsupervised approach particularly attractive since large amount unlabeled text freely available internet virtually major languages used directly without expensive labeling annotation. moreover since embeddings utilized variety tasks within pipeline even labeling resources available difﬁcult determine target labels common tasks include sentiment topic analysis personalization information retrieval would require different labels embeddings trained individually supervised fashion. despite signiﬁcant research effort area bag-of-words bag-ofngrams approaches remain popular still achieve highly competitive results however representations fail capture similarities words phrases result suffer sparsity dimensionality explosion. moreover treating words independent tokens temporal information lost making impossible model long range semantic dependencies. recently signiﬁcant attention devoted embedding approaches distributed representations words models within category trained produce document embeddings word representations either jointly learn word representations training pretrained word model. main advantage approaches directly exploit semantic similarities words produce highly compact embeddings state-of-the-art accuracy. recent work shown embeddings several hundred dimensions achieve leading accuracy tasks topic/sentiment classiﬁcation information retrieval within category popular approaches include weighted word combination models docvec recurrent neural network models word combination models directly aggregate word representations given document averaging another related function. similarly approaches straightforward implement achieve highly competitive performance. unlike resulting embeddings order magnitude smaller size don’t suffer sparsity dimensionality explosion problems. however averaging together word representations temporal information lost applying word weights partially addresses problem doesn’t eliminate easily come examples documents contain nearly words different meaning docvec another popular unsupervised model builds wordvec approach incorporating document vectors capture document speciﬁc semantic information. during training word document vectors learned jointly word vectors held ﬁxed inference. accurate model requires iterative optimization conducted inference. involves computing multiple gradient updates applying document embedding optimizer choice sgd. high volume production environments running optimization document prohibitively expensive. moreover documents vary signiﬁcantly length word composition difﬁcult control over/under-ﬁtting without running diagnostics additional complexity. finally models address inference problem training parametrized neural network model requires deterministic forward pass conducted inference. embedding models ingest document word time hidden activations entire document processed taken embedding. approach naturally addresses variable length problem provides principled model temporal aspects word sequence. however sequential nature makes difﬁcult leverage full beneﬁts modern hardware gpus offer highly scalable parallel execution. signiﬁcantly affect training inference speed. consequently embedding models relatively shallow hidden layers. moreover many commonly used achitectures lstm gate information already seen input recurrence step. repeated gating effect weight placed later words network forget earlier parts document ideal language modeling important information occur anywhere within document. work propose unsupervised embedding model based convolutional neural network addresses aforementioned problems. cnns utilized supervised tasks considerable success little research done unsupervised problem. work aims address gap. speciﬁcally show convolutional architecture effectively utilized learn accurate document embeddings fully unsupervised fashion. convolutions naturally parallelizable suffer memory problem rnns. allows signiﬁcantly faster training inference leading order magnitude inference speed-up embedding models. faster training enable explore deeper architectures model longer range semantic dependencies within document. show architectures variable length input problem effectively dealt using aggregating layer convolution fully connected layers. layer selects salient information convolutional layers combined fully connected layers generate embedding. finally propose learning algorithm based stochastic multiple word forward prediction. algorithm hyper parameters straightforward implement. summary contributions follows propose architecture unsupervised document embedding. architecture fully parallelizable applied variable length input. develop novel learning algorithm train model fully unsupervised fashion. learning algorithm based stochastic multiple word forward prediction requires virtually input pre-processing tunable parameters. conduct extensive empirical evaluation public benchmarks. evaluation show embeddings generated model produce comparable state-of-the-art accuracy fraction computational cost. section describe model architecture outline learning inference procedures. typical unsupervised document embedding problem given document corpus document contains sequence words w|d|. goal learn embedding function outputs p-dimensional vector every document. embedding dimension typically kept small highly competitive performance demonstrated generated embeddings need accurately summarize semantic/syntactic structure used primary document representation subsequent pipelines. common cases include efﬁcient information storage retrieval various supervised tasks sentiment analysis topic classiﬁcation. note besides documents assume additional information available training done fully unsupervised fashion. figure embedding model diagram. input document looked bird. training document partitioned word mat. words used input passed multiple layers convolutions activations output last layer passed aggregating function pool maxk pool. operation converts variable length activation matrix ﬁxed length one. fixed length activations passed fully connected layers last outputs embedding. example model trained predict words words looked bird taken positive targets randomly sampled words taken negative targets. products output embedding target word vectors converted probability using logistic function model updated binary cross-entropy objective. word sequences can’t used input directly common approach ﬁrst transform numeric format. recently distributed representation become increasingly popular allows preserve temporal information doesn’t suffer sparsity dimensionality explosion problems. given dictionary word represented ﬁxed length vector concatenating together word vectors within document provides input representation input matrix. unlike bagof-words representation fully preserves temporal order dense captures semantic similarity words word vectors either learned together model initialized using approaches wordvec glove format default input representation learn embedding function maps ﬁxed length vector free parameters learned. given operates variable length input word sequence subsequence words within also forms valid input. denote subsequence i’th j’th word matrix. passing subsequence yields subsequence embedding intuitively learned good representation function embedding accurately summarize semantic properties subsequence dij. extensively utilize notion training procedure allows signiﬁcantly expand training considering multiple subsequences within document. models recently shown perform well supervised tasks distributed representations considerably efﬁcient rnns. inspired results propose model given input matrix apply multiple layers convolutions convolutions computed left right along word sequence ﬁrst layer composed kernels operate words time. stacking layers together allows successive layer model increasingly longer range dependencies receptive ﬁelds span larger sections output l’th layer convolution weights element-wise product between matrices sigmoid function. linear component ensures gradient doesn’t vanish layers sigmoid gating selectively chooses information passed next layer. analogous forget gate lstm selectively choses information passed next recurrence step. however unlike lstm convolutions executed parallel along entire sequence layers memory bias allowing model focus part input. empirically found learning glus converged quicker consistently produced better accuracy relu activations. output last layer activation matrix corresponds convolutional kernel number columns varies input length. fully connected layers can’t applied variable length activations investigate several ways address problem. ﬁrst approach apply zero padding convert input ﬁxed length zero vector target length. documents shorter left padded zero vectors longer truncated words. analogous approach used supervised models conceptually simple easy implement approach drawback. imbalanced datasets document length varies signiﬁcantly difﬁcult select small leads information loss long documents large results wasted computation short documents. address problem note activation matrix converted ﬁxed length applying aggregating function along rows. common back-propagatable aggregating functions include mean median. convert arbitrary length input ﬁxed length output. work focus corresponds pooling operation commonly used deep learning models computer vision domains. operation applied along rows produces output vector length number convolutional kernels layer generalization procedure involves storing multiple values using operator maxk outputs top-k values instead top-. order maximum values occur preserved operation allowing fully connected layers capture additional temporal information another advantage using operation tracing selected activations back network gain insight parts input word sequence model focusing generate embedding. used interpret model show example analysis experiments section. popular alternative pooling attention layer particular self attention rows ﬁrst passed softmax functions self gated. however beyond scope paper leave future work. aggregating variable length input eliminates need document padding/truncation saves computation makes model ﬂexible. aggregating layer ﬁxed length activations passed fully connected layers last outputs p-dimensional document embedding. full architecture diagram pooling layer shown figure hypothesize good embedding sequence words accurate predictor words follow. accurately predict next words model must able extract sufﬁcient sufﬁcient semantic information input sequence improvement prediction accuracy would indicate better semantic representation. forms basis learning approach. embedding length word vector length deﬁne probability given word occurs word sub-sequence using sigmoid function product word vector embedding dij. probability thus raised word vector similar embedding lowered otherwise. optimizing word vectors embeddings models learns joint semantic space product determines semantic relatedness. training optimize model perform well prediction task. goal generate embeddings accurately predict next words large product them. formally given input document w|d| prediction point subsequence predict next words wi+h. framing multi-instance binary classiﬁcation problem optimize cross entropy objective minimizing raise probability words follow lower words. expanding forward prediction multiple words makes problem challenging requires deeper understanding input sequence. turn improves embedding quality found predicting words forward signiﬁcantly improves accuracy. practice expensive compute second term equation term involves summation entire word vocabulary document prohibitively large. address problem sampling randomly sample small subset words approximate sum. empirically found using word samples document produced good results fraction computational cost. analogous approach taken prediction point instead ﬁxing document also sampling. sampling interval prediction point sampled uniformly within interval. lower bound ensures model sufﬁcient context forward prediction. upper bound ensures least words addition simplicity sampling prediction point another advantage forces model learn accurate embeddings short long documents acting regularizer improving generalization. sampling procedures lead learning algorithm outlined algorithm algorithm straightforward implement three hyper parameters tune. accelerate learning train document mini-batches prediction point negative samples used every document minibatch re-sampled across mini-batches. fixing prediction point within mini-batch allows represent input ﬁxed length tensor improves parallelization. moreover ensure training examples contribute equally optimization sample documents without replacement count epoch documents exhausted. example learning procedure single document shown figure here input document {the looked bird} prediction point forward window subsequence {the mat} passed generate embedding products embedding word vectors target positive words {and looked bird} randomly sampled target negative words converted probabilities equation model updated using cross entropy objective equation proposed learning algorithm different existing embedding models docvec skip-thought. unlike approaches entire subsequence predict words follow gives model available information enabling capture richer semantic structure. contrast docvec uses ﬁxed context words skip-thought trained sentences. training larger context possible model layers fully parallelizable allow efﬁcient forward backward passes. furthermore instead using ﬁxed context tokenizing input sentences sample prediction point. eliminates need input pre-processing also keeping context size dynamic. found procedure produce robust models even training data relatively small. finally train forward word prediction many existing models also backward prediction. backward prediction complicates training feed-forward models goes natural language ﬂow. found forward prediction training sufﬁcient achieve highly competitive performance. validate proposed approach conducted extensive experiments publicly available datasets imdb amazon fine food reviews implemented model using tensorflow library experiments conducted server -core intel .ghz nvidia geforce ram. found initializing word vectors wordvec updating training resulted faster learning produced better performance. pre-trained vectors taken wordvec project page thus input word vector output embedding dimensions models. address variable length input problem experiment padding pooling approaches proposed section pooling additionally experiment maxk pooling largest values retained convolutional kernel. cross validation found setting resulted optimal trade-off accuracy complexity value cnn-pool-k models. embeddings evaluated training shallow classiﬁer using labeled training instances dataset report test classiﬁcation accuracy. classiﬁer hidden layer hidden units tanh activations. classiﬁer training done using mini-batches size optimizer momentum make comparison fair classiﬁcation setembedding models. evaluation labeled instances analogous previous work area aimed validating whether unsupervised models capture sufﬁcient semantic information supervised tasks sentiment classiﬁcation. compare approach leading unsupervised embedding models including docvec docvecc skip-thought described section baseline code respective authors extensively tune model using parameter sweeps. skip-thought code provides pre-trained model optimized large book corpus compare model well skip-thought models tuned speciﬁcally imdb amazon datasets. imdb dataset largest publicly available sentiment analysis datasets collected imdb database. dataset consists movie reviews split evenly training test sets. reviews movie prevent model learning movie speciﬁc review patterns. target sentiment labels binarized review scores treated negative scores treated positive. addition labeled reviews dataset also contains unlabeled reviews used unsupervised training. note remove test reviews unsupervised training phase train embedding models using unlabeled reviews training reviews. removing test reviews simulates production environment inference documents typically seen training phase. reported accuracy thus differ reported previous work. understand effects architecture parameters performance conduct extensive grid search record classiﬁcation accuracy various parameter settings. experiments conducted cnn-pad require many training runs cnn-pad faster train cnn-pool; results cnn-pool exhibit similar patterns. results important parameters shown figure figure shows classiﬁcation accuracy number convolutional layers cnn. ﬁgure exception four layers accuracy steadily improves layers. indicates depth useful task parallel execution cnns enables explore deeper architectures would possible recurrent models. also found layers model would start overﬁt aggressive regularization dropout weight norm hurt accuracy. future work explore larger datasets well additional regularization methods address problem. figure shows accuracy forward prediction window ﬁgure seen accuracy signiﬁcantly improves model trained predict word forparticular gain inward. creased supporting conclusion difﬁcult task predicting multiple words leads better embeddings. finally figure shows difference performance relu layers. across epochs glus consistently outperform relus relative improvement figure imdb performance analysis various architecture settings. figure shows accuracy number layers. figure shows accuracy length forward prediction window figure compares relu layers. using ﬁndings select following architecture layers kernels batch normalization layer residual connections every layer accelerate learning. layers apply pooling single fully connected layer outputs -dimensional embedding. model trained predict words forward negative word samples prediction point offset training done using mini-batch gradient descent batch size adam optimizer table shows inference speed tokens second architecture well docvec uni/bi-directional skip-thought models. results generated inference batch size remove effects batch cpu/gpu parallelization. table architecture faster uni-directional skip-thought faster bidirectional version. despite fact hidden layers skip-thought one. similar results reported related language modeling task clearly demonstrate advantage using convolutional architecture. model cnn-pool-k outperforms baselines passes difﬁcult accuracy level. also cnnpool generally performs better cnn-pad suggesting pooling effective padding truncation. cnn-pool models signiﬁcantly outperform rnn-based skip-thought approach. results indicate convolution pooling good alternative recurrence unsupervised learning variable length textual input. amazon fine food reviews dataset collection reviews amazon food products left users october example contains full text review short summary rating labels. dataset come train-test split highly unbalanced. address this perform split removing duplicate reviews randomly sample reviews class these randomly select training testing. produces training test sets documents respectively. train model using architecture training method imdb experiments compare baseline approaches. models evaluated using binary -class classiﬁcation tasks. binary classiﬁcation documents ratings treated negative positive discard documents labeled however training documents including labeled used unsupervised training phase. results classiﬁcation task shown table table cnn-pool-k performs comparably best baselines classiﬁcation tasks. pooling produces better performance padding leading conclusion pooling used default method deal variable length input. strong performance -class classiﬁcation task suggest found little lobster also found could purchase product local publix market less cost. decent herring although favorite found herring little soft side still enjoy them. you’ll love plan seafood pasta sauce don’t pasta sauce. plain boring read abundance lobster maine paying times amount lobster tails shipping drink horrible coconut water tastes like really watered milk would recommend anyone. really excited coconut water came ﬂavors strong tastes terrible save money stuff bad. drink brands time it’s awful throwing whole case away drink this. ﬁrst coffee tried keurig. disappointed ﬂavor; tasted like plastic would recommend comes running. although expensive really good. can’t wait take pills... kitty can’t enough ’em. loves much anything every time open meows like crazy kind food know likes. keeps healthy. table retrieval results amazon dataset. query review shown bold retrieve top- similar reviews using cosine distance embeddings produced model. cosine distance score shown left retrieved result. table amazon results class sentiment classiﬁcation tasks. model capable successfully learning ﬁne-grained differences sentiment directly unlabeled text. overall together imdb results indicate convolutional architecture well suited unsupervised used learn robust embedding models. common application document embedding information retrieval embedding vectors indexed used quickly retrieve relevant results given query. approach asses quality embeddings model generates. using amazon dataset select several reviews queries retrieve top- similar results using embedding cosine distance similarity measure. results shown table table retrieved reviews highly relevant query content sentiment. ﬁrst group complains seafood products second group unhappy drink product last group owners like particular food product. interestingly product retried reviews varies topic sentiment stay consistent. instance ﬁrst group three retrieved reviews herring seafood pasta lobster. however similar visibility embeddings applied tsne embeddings inferred imdb test set. t-sne compresses embedding vectors dimensions plot corresponding dimensional points coloring according sentiment label. plot shown figure ﬁgure distinct separation sentiment classes negative reviews near positive reviews bottom. validates model able capture encode sentiment information making classes near linearly separable. presented model unsupervised document embedding. approach successive layers convolutions applied distributed word representations model longer range semantic structure within document. proposed learning algorithm based stochastic forward prediction. learning procedure hyper parameters tune straightforward implement. model able take full advantage parallel execution making signiﬁcantly faster leading models. experiments public benchmarks references abadi mart´ın agarwal ashish barham paul brevdo eugene chen zhifeng citro craig corrado greg davis andy dean jeffrey devin matthieu tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. bahdanau dzmitry kyunghyun bengio yoshua. neural machine translation jointly learning align translate. international conference learning representations ioffe sergey szegedy christian. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning kiros ryan yukun salakhutdinov ruslan zemel richard urtasun raquel torralba antonio fidler sanja. skip-thought vectors. neural information processing systems chung junyoung gulcehre caglar kyunghyun bengio yoshua. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv. conneau alexis schwenk holger barrault lo¨ıc lecun yann. deep convolutional networks text classiﬁcation. european chapter association computational linguistics zhouhan feng minwei santos cicero nogueira xiang bing zhou bowen bengio yoshua. structured self-attentive sentence embedding. international conference learning representations maas andrew daly raymond pham peter huang andrew potts christopher. learning word vectors sentiment analysis. association computational linguistics mikolov tomas sutskever ilya chen corrado greg dean jeff. distributed representations words phrases compositionality. neural information processing systems jeffrey socher richard manning christopher glove global vectors word representation. empirical methods natural language processing http//www. aclweb.org/anthology/d-.", "year": 2017}