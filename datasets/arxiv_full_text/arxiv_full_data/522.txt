{"title": "Learning to Transduce with Unbounded Memory", "tag": ["cs.NE", "cs.CL", "cs.LG", "68T05", "I.5.1; I.2.6; I.2.7"], "abstract": "Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.", "text": "recently strong results demonstrated deep recurrent neural networks natural language transduction problems. paper explore representational power models using synthetic grammars designed exhibit phenomena similar found real transduction problems machine translation. experiments lead propose memory-based recurrent networks implement continuously differentiable analogues traditional data structures stacks queues deques. show architectures exhibit superior generalisation performance deep rnns often able learn underlying generating algorithms transduction experiments. recurrent neural networks offer compelling tool processing natural language input straightforward sequential manner. many natural language processing tasks viewed transduction problems learning convert string another. machine translation prototypical example transduction recent results indicate deep rnns ability encode long source strings produce coherent translations elegant application rnns transduction tasks requires hidden layers large enough store representations longest strings likely encountered implying wastage shorter strings strong dependency number parameters model memory. paper number linguistically-inspired synthetic transduction tasks explore ability rnns learn long-range reorderings substitutions. further inspired prior work neural network implementations stack data structures propose evaluate transduction models based neural stacks queues deques stack algorithms well-suited processing hierarchical structures observed natural language hypothesise neural analogues provide effective learnable transduction tool. models provide middle ground simple rnns recently proposed neural turing machine implements powerful random access memory read write operations. neural stacks queues deques also provide logically unbounded memory permitting efﬁcient constant time push operations. version paper identical version found proceedings advances neural information processing systems addition missing references. figures made larger increased legibility. short-term memory cells learn transductions tested inputs length seen training fail consistently generalise longer strings. contrast sequential memory-based algorithms able learn reproduce generating transduction algorithms often generalising perfectly inputs well beyond encountered training. string transduction central many applications name transliteration spelling correction inﬂectional morphology machine translation. common approach leverages symbolic ﬁnite state transducers approaches based context free representations also popular rnns offer attractive alternative symbolic transducers simple algorithms expressive representations however show work models limited ability generalise beyond training data memory capacity scales number trainable parameters. previous work touched topic rendering discrete data structures stacks continuous especially within context modelling pushdown automata neural networks inspired continuous push operations architectures idea controlling data structure developing models. difference work adapts operations work within recurrent continuous stack/queue/deque-like structure dynamics fully decoupled controlling models backwards dynamics easily analysable order obtain exact partial derivatives error propagation rather approximate done previous work. parallel effort ours researchers exploring addition memory recurrent networks. memory networks provide powerful random access memory operations whereas focus efﬁcient restricted class models believe sufﬁcient natural language transduction tasks. closely related work sought develop continuous stack controlled rnn. note model—unlike work proposed here—renders discrete push operations continuous mixing information across levels stack time step according scalar push/pop action values. means model ends compressing information stack thereby limiting effectively loses unbounded memory nature traditional symbolic models. section present extensible memory enhancement recurrent layers continuous version classical stack queue deque begin describing operations dynamics neural stack showing modify queue extend deque. neural stack differentiable structure onto continuous vectors pushed popped. inspired neural pushdown automaton render traditionally discrete operations continuous letting push operations real values interval intuitively interpret values degree certainty controller wishes push vector onto stack stack. formally neural stack fully parametrised embedding size described timestep value matrix strength vector form core recurrent layer acted upon controller receiving controller value signal push signal outputs read vector recurrence layer comes fact receive previous state stack pair produce next state pair following dynamics described below. here represents represents value equation shows update value component recurrent layer state represented matrix number rows grows time maintaining record values pushed stack timestep values appended bottom matrix never changed. equation shows effect push signal updating strength vector produce first operation removes objects stack. think value initial deletion quantity operation. traverse strength vector highest index lowest. next strength scalar less remaining deletion quantity subtracted remaining quantity value remaining deletion quantity less next strength scalar remaining deletion quantity subtracted scalar deletion stops. next push value strength value added current timestep. equation shows dynamics read operation similar operation. ﬁxed initial read quantity temporary copy strength vector traversed highest index lowest. next strength scalar smaller remaining read quantity value preserved operation subtracted remaining read quantity. temporarily remaining read quantity strength scalars lower indices temporarily output read operation weighted rows scaled temporary scalar values created traversal. example stack read calculations across three timesteps pushes pops described above illustrated figure third step shows setting strength logically removes stack ignored read. completes description forward dynamics neural stack cast recurrent layer illustrated figure operations described section differentiable. equations describing backwards dynamics provided appendix supplementary materials. neural queue operates neural stack exception operation reads lowest index strength vector rather highest. represents popping reading front queue rather stack. operations described equations neural deque operates likes neural stack except takes push value input ends structure outputs read ends. write utop instead state m-dimensional matrix t-dimensional vector respectively. timestep followed bottom deque followed pushes reads. dynamics deque unlike neural stack queue grows directions described equations below. equations decompose strength vector update three steps purely notational clarity. three memory modules described seen recurrent layers operations used produce next state output input previous state fully differentiable contain tunable parameters optimise training. such need attached controller order used practical purposes. exchange offer extensible memory logical size unbounded decoupled nature parameters controller size problem applied here describe controller enhanced neural stack queue deque. begin giving case memory neural stack illustrated figure wish replicate overall ‘interface’ recurrent layer—as seen outside dotted lines—which takes previous recurrent state input vector transforms return next recurrent state output vector setup previous state recurrent layer tuple previous state previous stack read previous state stack described above. exception initialised randomly optimised training initial states -valued vectors/matrices updated training. overall input concatenated previous read passed controller input along previous controller state ht−. controller outputs next state controller output obtain push scalars value vector passed stack well network output vector-to-scalar projection matrices scalar biases; vector-to-vector projections vector biases randomly intialised tuned training. along previous stack state stack operations value passed neural stack obtain next read next stack state packed tuple controller state form next state overall recurrent layer. output vector serves overall output recurrent layer. structure described adapted control neural queue instead stack substituting memory module other. additional trainable parameters either conﬁguration relative non-enhanced projections input concatenated previous read controller projections controller output various stack/queue inputs described above. case deque read rtop bottom read rbot must preserved overall state. concatenated input form input controller. output controller must additional projections output push/pop operations values bottom deque. roughly doubles number additional tunable parameters wrapping controller compared stack/queue case. every experiment integer-encoded source target sequence pairs presented candidate model batch single joint sequences. joint sequence starts start-of-sequence symbol ends end-of-sequence symbol separator symbol separating source target sequences. integer-encoded symbols converted -dimensional embeddings embedding matrix randomly initialised tuned training. separate wordto-index mappings used source target vocabularies. separate embedding matrices used encode input output embeddings. following tasks read input sequence generate target sequence transformed version source sequence followed symbol. source sequences randomly generated vocabulary meaningless symbols. length training source sequence uniformly sampled unif symbol sequence drawn replacement uniform distribution source vocabulary deterministic task-speciﬁc transformation described task below applied source sequence yield target sequence. training sequences entirely determined source sequence close training sequences task training examples sampled space random generation source sequences. following steps followed training test sequence presented models symbol prepended source sequence concatenated separator symbol target sequences symbol appended. following tasks examine well models approach sequence transduction problems source target sequence jointly generated inversion transduction grammars subclass synchronous context-free grammars often used machine translation present simple itg-based datasets interesting linguistic properties underlying grammars. show grammars table appendix supplementary materials. synchronised non-terminal expansion chosen according probability distribution speciﬁed rule probability beginning rule. grammar always root tree. tuned generative probabilities recursive rules hand grammars generate left right sequences lengths relatively uniform distribution. generate training data rejecting samples outside range testing data rejecting samples outside range terminal symbol-generating rules balance classes terminal-generating symbols grammar terminal-generating non-terminal generates vocabulary approximately vocabulary word class equiprobable. design choices made maximise similarity experimental settings tasks described synthetic tasks described above. subj–verb–obj subj–obj–verb persistent challenge machine translation learn faithfully reproduce high-level syntactic divergences languages. instance translating english sentence non-ﬁnite verb german transducer must locate move verb object ﬁnal position. simulate phenomena synchronous grammar generates strings exhibiting verb movements. extra challenge also simulate simple relative clause embeddings test models’ ability transduce presence unbounded recursive structures. sample output grammar presented here spaces words included stylistic purposes indicate subject object verb terminals respectively mark input output indicates relative pronoun genderless gendered grammar design small grammar simulate translations language gender-free articles gender-speciﬁc deﬁnite indeﬁnite articles. real world example translation would english german grammar simulates sentences form every noun phrase become inﬁnite sequence nouns joined conjunction. noun source language neutral deﬁnite indeﬁnite article. matching word target language needs preceeded appropriate article. sample output grammar presented here spaces words included stylistic purposes task test data generated procedure training data difference length source sequence sampled unif result change assured models cannot observe test sequences training also measuring well sequence transduction capabilities evaluated models generalise beyond sequence lengths observed training. control generalisation ability also report accuracy scores sequences separately sampled training given size sample space unlikely ever observed actual model training. round testing sample sequences appropriate test set. sequence model reads source sequence separator symbol begins generating next symbol taking maximally likely symbol softmax distribution target symbols produced model step. based process give model coarse accuracy score corresponding proportion test sequences correctly predicted beginning without error well accuracy score corresponding average proportion sequence correctly generated ﬁrst error. formally have correct seqs number correctly predicted sequences total number sequences test batch respectively; correcti number correctly predicted symbols ﬁrst error sequence test batch |targeti| length target segment sequence task benchmarks deep lstms described layers. benchmarks evaluate neural stack- queue- deque-enhanced lstms. running experiments trained tested version model lstms model hidden layer size hidden layer size stack/queue/deque embedding size arbitrarily half maximum hidden size. number parameters model reported architecture table appendix. concretely neural stack- queue- deque-enhanced lstms number trainable parameters two-layer deep lstm. come extra connections memory module trainable parameters regardless logical size. models trained minibatch rmsprop batch size grid-searched learning rates across used gradient clipping clipping gradients average training perplexity calculated every batches. training test accuracies recorded every batches. impossibility overﬁtting datasets models train unbounded number steps report results convergence. present figure coarseﬁne-grained accuracies task best model architecture described paper alongside best performing deep lstm benchmark. best models automatically selected based average training perplexity. lstm benchmarks performed similarly across range random initialisations effect procedure primarily select better performing stack/queue/deque-enhanced lstm. cases procedure yield actual bestperforming model practice sophisticated procedure ensembling produce better results. experiments neural stack queue outperforms deep lstm benchmarks often signiﬁcant margin. experiments neural stackqueue-enhanced lstm learns partially consistently solve problem neural deque. experiments enhanced lstms solve problem completely training accuracy persists longer sequences test whereas benchmark accuracies drop experiments except gender conjugation transduction tasks. across tasks enhanced lstms solve convergence accuracy happens orders magnitude earlier enhanced lstms benchmark lstms exempliﬁed figure results sequence inversion copying tasks serve unit tests models controller mainly needs learn push appropriate number times continuously. nonetheless failure deep lstms learn regular pattern generalise indicative limitations benchmarks presented here relative expressive power models. ability generalise perfectly sequences twice long attested training also notable also attested experiments. finally pair -layer lstm stack-lstm queue-lstm deque-lstm -layer lstm stack-lstm queue-lstm deque-lstm -layer lstm stack-lstm queue-lstm deque-lstm -layer lstm stack-lstm queue-lstm deque-lstm -layer lstm stack-lstm queue-lstm deque-lstm experiments illustrates neural queue solves copying stack solves reversal simple lstm controller learn operate deque either structure solve tasks. results bigram flipping task models consistent failure consistently correctly generate last symbols sequence. hypothesise deep lstms models economically learn pairwise sequence tokens attempt half time reaching token. tasks success deep lstm benchmarks relative performance tasks explained ability exploit short local dependencies dominating longer dependencies particular grammars. overall rapid convergence possible general solution transduction problem manner propagates longer sequences without loss accuracy indicative unbounded memory-enhanced controller learn solve problems procedurally rather memorising underlying distribution data. experiments performed paper demonstrate single-layer lstms enhanced unbounded differentiable memory capable acting limit like classical stack queue deque capable solving sequence-to-sequence transduction tasks deep lstms falter. even tasks benchmarks obtain high accuracies memory-enhanced lstms converge earlier higher accuracies requiring considerably fewer parameters simplest deep lstms. therefore believe constitute crucial addition neural network toolbox complex linguistic transduction tasks machine translation parsing rendered tractable inclusion. acknowledgements thank alex graves demis hassabis tom´aˇs koˇcisk´y rockt¨aschel ritter geoff hinton ilya sutskever chris dyer many others helpful comments.", "year": 2015}