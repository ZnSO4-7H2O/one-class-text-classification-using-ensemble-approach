{"title": "Deep Learning with Dynamic Computation Graphs", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.", "text": "neural networks compute graph structures natural problems variety domains including natural language cheminformatics however since computation graph different shape size every input networks directly support batched training inference. also difﬁcult implement popular deep learning libraries based static data-ﬂow graphs. introduce technique called dynamic batching batches together operations different input graphs dissimilar shape also different nodes within single input graph. technique allows create static graphs using popular libraries emulate dynamic computation graphs arbitrary shape size. present high-level library compositional blocks simpliﬁes creation dynamic graph models. using library demonstrate concise batch-wise parallel implementations variety models literature. training deep neural networks directly minimally pre-processed corpora many recent performance breakthroughs mainly problems domains vision natural language inputs cast dense n-dimensional arrays sequences tensors. successes exploit effectiveness training gradient descent mini-batches tens hundreds inputs implemented using parallel simd capabilities modern gpus multi-core cpus this turn proliferation libraries making easier train deploy models expressing terms differentiable data-ﬂow graphs tensors however also long history neural networks compute structures parse trees logical terms molecular graphs models distinct input different computation graph structure; dynamic computation graphs models continue developed recently yielded superior results problems sentiment classiﬁcation semantic relatedness question-answering screening chemical compounds despite successes practitioners avoid dcgs implementation reasons. example bowman assert because treernns different model structure sentence efﬁcient batching impossible standard implementations. moreover even efﬁcient batching possible principle current libraries tensorflow assume data-ﬂow graph static impose signiﬁcant cost graph construction makes infeasible build graph input. section introduces dynamic batching enables efﬁcient batching training inference dcgs. dynamic batching runs dcgs efﬁciently existing libraries support static data-ﬂow graphs; e.g. static graph treernn parse tree. present empirical results implementation tensorflow. section presents combinator library concisely implementing models dcgs using dynamic batching. section concludes. deep learning libraries like tensorflow computations manually batched. computation expressed static graph mathematical operations polymorphic batch size; input dimensions yield output dimensions batch size. dcgs graph operations static assumed different every input multiple inputs longer naturally batch together way. dynamic batching algorithm overcomes difﬁculty. given computation graphs input different size topology rewrite graphs batching together instances operation occur depth graph. rewriting process inserts additional concat gather operations move data batched operations; indices gather encode topology original input graphs. distinguish individual operations appearing nodes underlying data-ﬂow graph addition matrix-multiply small sub-graphs conceptually functions tensors feed-forward layer lstm cell. refer former latter operations. operations form building-blocks neural networks dcgs composed; dynamic batching schedules operations ops. algorithm requires operations might used speciﬁed advance enumerates scheduling purposes. example binary treernn parse trees operations embedding table lookups words leaves tree cells non-terminals. inputs outputs operations tensor types. input output different type types must ﬁxed fully speciﬁed advance. tensor type consists shape together scalar data type inputs operation shall tensors dimension batch size shape corresponding input tensor type. outputs must tensors dimension shape corresponding output tensor type. operations must polymorphic respect batch size batch size change time operation invoked depending topologies input graphs. however tensor types ﬁxed possible assign known tensor type edge input computation graph. dynamic batching algorithm takes directed acyclic computation graph input. batch multiple input graphs treated single disconnected graph. source nodes constant tensors non-source nodes operations. edges connect outputs node inputs another node. scheduling performed using greedy algorithm assign depth node graph. nodes dependencies assigned depth zero. nodes dependencies depth zero assigned depth nodes whose dependencies maximum depth assigned depth etc. batch together nodes invoking operation depth single node. concatenate outputs depth tensor type. order concatenation corresponds order dynamic batching operations enumerated. assign label edge original graph depth tensor type integer index edge outputs schedule graph consists indices edges grouped together depth operation. tensorflow implementation dynamic operation instantiated static data-ﬂow graph. inputs operation tf.gather outputs tf.concat described above. tensorflow placed within tf.while_loop. iteration loop evaluate operations particular depth. loop maintains state variables tensor type feeds output concat tensor type iteration input gathers tensor type iteration indices gather iteration drawn edge labels depth schedule. initial values state variables iteration/depth constants input graph. dynamic batching allows construct static tensorflow graph contains single instance operation emulate input graphs arbitrary size topology operations appear arbitrary number times. tensorflow concat gather while_loop differentiable gradients calculations back-propagation require additional code. example binary treernn described yields tensorflow data-ﬂow graph tf.while_loop whose body shown left figure gather additional input picks elements operations called with. long downward arrows pass-throughs. algorithm consumes tree shown right figure turns inputs gather operations depth implemented dynamic batching part library tensorflow fold designed synthetic speed benchmark compare manual batching native tensorflow. benchmark uses underlying kernels execution engine cases. native tensorflow cannot batch together trees different shapes testing purposes batch random binary trees shape. test results thus represent best-case scenario operations batched together perfectly. manual batching tests construct static data-ﬂow graph operations corresponding shape tree. dynamic batching tests traverse tree construct schedule described above. leaves tree lookups embedding table non-terminals implement variant tree-lstm equations. tree size state size lstm. tests dell workstation dual -core intel xeon processors tests done using consumer nvidia geforce gtx- card. compare manual batching dynamic batching trees shape dynamic batching tree different shape measurable penalty dealing trees different shapes. test results shown table emphasize importance batching especially gpus. tensorflow launch kernel every node tree ﬁxed overhead proportional size tree dominates execution small batch sizes. tensorflow begin saturate relatively large batch sizes higher. difference speed fully-batched unbatched dynamic batching less kernel invocation overhead data-ﬂow graph smaller. dynamic batching instantiates operation once invokes depth number kernel invocations rather tree size. dynamic batching thus achieves substantial speedups even batch size batches operations depth within single tree. however extra concat gather dynamic batching inserts cost. cost ratio column shows ratio dynamic manual batching case trees batch shape. cost inference gpus batch-size rises training backpropagation. cost mainly visible large batch sizes balanced beneﬁt within-tree batching smaller sizes. even cost dynamic batching yields speedup using batch size cpu. speedup ratio column shows ratio per-tree time dynamic batching random shapes versus manual batching batch size note using batch size actually feasible tensorflow tensorflow large graph construction overhead included measurements apply libraries lack overhead. addition dynamic batching tensorflow fold library provides combinators simplify task constructing neural networks dcgs. goal show dynamic batching enables implementing deep learning models higher level abstraction manual batching. turn facilitates rapid feedback loop trying novel model variants thus obtaining superior results. design library inspired functional programming techniques parser combinators arrows combinator library computations structured compositionally plugging together simpler computations various ways. basic unit computation tensorflow fold block essentially function input output. typical model input graph tree kind output vector attached loss training. example consider model inputs sequences words varying lengths output sentence vector. library provide several different ways handling sequences. given simpler block operates elements sequence pairs elements deﬁne following combinators input denotes objects host language trees dictionaries. tensor dtypeshape denotes tensors particular dtype shape. tuple denotes tuple values types sequence denotes sequence elements type length. blocks composed hierarchically; block expression always tree. non-terminals tree combinators fold take simpler blocks arguments. leaves tree atomic blocks include following assume pairs input wish predict label text. text consists words want array pretrained word embeddings corresponding dictionary mapping words indices call word_idx.get obtain index word word_matrix none word unknown. block uses inputtransform index word passed optional block converts scalar index tensor turn gets passed embedding operation performs lookup embedding table. inputtransform split string words. words vectors wordvec combine word vectors simple uses single fully connected layer hidden units. zeros block deﬁnes initial state rnn. finally create compiler validates block performs type-checking sets dynamic batching tensorflow. outputs compiled block available tensorflow tensors training proceeds would tensorflow model model block architecture simple pipeline instead forms directed acyclic graph illustrated figure composition block allows blocks composed dags. model code details found appendix n-ary tree-lstms generalize lstms previous states. applied classify sentences stanford sentiment treebank. corpus consists binarized constituency parse trees one-sentence movie reviews every node sentiment label. leaves tree words mapped word-embedding vectors serve input binary tree-lstm previous states. internal nodes lstm takes input previous states children. formally reelst learnable function corresponding eqs. since tree recursive data type model processes trees must recursively deﬁned illustrated cycle figure forwarddeclaration allows creation recursive models forward declaration like expr block called syntax) create references i.e. blocks refer declaration. subsequent call resolve_to updates references refer expr_def. brieﬂy report experiments implementation n-ary tree-lstms sentiment analysis. state-of-the-art really point here. models particularly original could certainly implemented without using tensorflow fold. fold enable simpler concise deﬁnitions along faster execution thus making easier rapidly explore novel model variants. used constituency tree-lstms tuned glove vectors word embedding achieved best results sentiment models presented addition speciﬁc model explored several novel variants. particular employed nonrecurrent dropout weight regularization. eliminated weight regularization favor recurrent dropout scheme introduced semeniuta increased lstm state size leaving hyperparameters unchanged. results shown table including best previously reported results. fine-grained accuracy measured trees calculated based possible labels. binary accuracy measured trees non-neutral sentiment based negative positive classiﬁcation. numbers parentheses standard deviations. report independent runs results based thirty independent runs. noting small size dataset evaluated ensemble consisting thirty independently trained models; variant sets state-of-the-art subtasks. ﬁnal example used fold library implement graph convolution model introduced kearnes molecules represented undirected graphs atoms. code complex previous examples involves nested composition blocks given appendix neural architectures dynamic computation graphs suffer inefﬁcient batching poor tooling. dynamic batching solves former problem full generality believe ﬁrst time. spinn architecture alternative stack-based approach also enables efﬁcient batching dcgs limited binary trees requires padding/truncation handle trees different sizes. fold library addresses tooling problem providing high-level combinator library intended make easy practitioners rapidly develop iterate architectures dcgs. experimental results presented section quantify impact dynamic batching. impact combinator library harder demonstrate quantitatively. approach comparing lines code table original author’s sources. appendix details comparison protocol. course short implementation suboptimal comes cost ﬂexibility. results section show models literature reimplemented fold extended achieve superior performance. suspect models dcgs quite head room well simply less work done tuning compared mainstream architectures. references martın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorflow large-scale machine learning heterogeneous systems arxiv samuel bowman gauthier abhinav rastogi raghav gupta christopher manning christopher potts. fast uniﬁed model parsing sentence understanding. naacl steven kearnes kevin mccloskey marc berndl vijay pande patrick riley. molecular graph convolutions moving beyond ﬁngerprints. journal computer-aided molecular design within composition scope blocks wired together reads provided directed cycles formed. input output properties used deﬁne overall inputs outputs composition block. example introduces several additional block types section implements graph convolution model introduced kearnes molecules represented undirected graphs atoms. real-valued feature vectors atom distinct pair atoms. molecule atoms index atom feature vectors index pair feature vectors core graph convolution model weave module combines atom-level pair-level features using learnable functions weave module stacked arbitrarily create deep graph convolution models. denoting inputs outputs superscripts respectively weave module weave.input weave.input a_to_a map.reads p_to_a sum).reads zipwith f_a).reads a_to_p zipwith.reads.reads) p_to_p map).reads zipwith f_p)).reads weave.output.reads deﬁne functional unit comparison input-output mapping. prepare single implements functionality nothing else. remove import statements abstract base classes logging validation logic. count lines code ignoring blank lines comments.. functional unit comparison creating model variable-length experiment described raffel ellis includes loss accuracy calculations include training loop creation training data. original implementation python uses theano lasagne. tensorflow fold implementation concise partly differences tensorflow lasagne. fold reduces implementation complexity eliminating need manual batching e.g. x.sum batching explicit axis implicitly batched. functional unit comparison creating constituency tree-lstm running epoch training ﬁne-grained sentiment classiﬁcation task described include loading word embeddings dataset provided inputs. original implementation uses torch. terminates blocks keyword; count lines. here python tensorflow leads substantially concise code torch. unlike previous example manual batching plays role here original implementation computes gradients losses tree time. fold reduces complexity using oneof block distinguish leaves internal nodes rather recursive function explicitly traverses tree. functional unit comparison creating single weave module described kearnes original implementation python uses tensorflow. here implementations language deep learning library. fold helps eliminating need manual batching ﬁrst example. particularly apparent atoms-to-pairs calculation requires making copies matrix tensor. native tensorflow ﬁrst dimension batch copying explicit reshape) fold broadcast sufﬁces number copies needed determined lazily subsequent computations.", "year": 2017}