{"title": "CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble  Behavior", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We introduce a new deep convolutional neural network, CrescendoNet, by stacking simple building blocks without residual connections. Each Crescendo block contains independent convolution paths with increased depths. The numbers of convolution layers and parameters are only increased linearly in Crescendo blocks. In experiments, CrescendoNet with only 15 layers outperforms almost all networks without residual connections on benchmark datasets, CIFAR10, CIFAR100, and SVHN. Given sufficient amount of data as in SVHN dataset, CrescendoNet with 15 layers and 4.1M parameters can match the performance of DenseNet-BC with 250 layers and 15.3M parameters. CrescendoNet provides a new way to construct high performance deep convolutional neural networks without residual connections. Moreover, through investigating the behavior and performance of subnetworks in CrescendoNet, we note that the high performance of CrescendoNet may come from its implicit ensemble behavior, which differs from the FractalNet that is also a deep convolutional neural network without residual connections. Furthermore, the independence between paths in CrescendoNet allows us to introduce a new path-wise training procedure, which can reduce the memory needed for training.", "text": "introduce deep convolutional neural network crescendonet stacking simple building blocks without residual connections. crescendo block contains independent convolution paths increased depths. numbers convolution layers parameters increased linearly crescendo blocks. experiments crescendonet layers outperforms almost networks without residual connections benchmark datasets cifar cifar svhn. given sufﬁcient amount data svhn dataset crescendonet layers parameters match performance densenet-bc layers parameters. crescendonet provides construct high performance deep convolutional neural networks simple network architecture. moreover investigating behavior performance subnetworks crescendonet note high performance crescendonet come implicit ensemble behavior. furthermore independence paths crescendonet allows introduce path-wise training procedure reduce memory needed training. deep convolutional neural networks signiﬁcantly improved performance image classiﬁcation however training also becomes increasingly difﬁcult network deepening. important research efforts overcome difﬁculty develop neural network architectures recently residual network variants used residual connections among layers train deep cnn. residual connections promote feature reuse help gradient reduce need massive parameters. resnet densenet achieved state-of-the-art accuracy benchmark datasets. alternatively fractalnet expanded convolutional layers fractal form generate deep cnns. without residual connections manual deep supervision fractalnet achieved high performance image classiﬁcation based network structural design only. many studies tried understand reasons behind representation view deep cnns. veit showed residual network seen ensemble relatively shallow effective paths. however greff argued ensembles shallow networks cannot explain experimental results lesioning layer dropout layer reshufﬂing resnet. proposed residual connections unrolled iterative estimation resnet. meanwhile larsson speculated high performance fractalnet unrolled iterative estimation features longest path using features shorter paths. although unrolled iterative estimation model explain many experimental results unclear helps improve classiﬁcation performance resnet fractalnet. hand ensemble model explain performance improvement easily. blocks called crescendo blocks crescendo block comprises independent feed-forward paths increased number convolution batch-norm layers identical size convolutional ﬁlters entire network. despite simplicity crescendonet shows competitive performance benchmark cifar cifar svhn datasets. similar fractalnet crescendonet include residual connections. high performance crescendonet also comes completely network structural design. unlike fractalnet numbers convolutional layers associated parameters increased exponentially numbers convolutional layers parameters crescendo blocks increased linearly. crescendonet shows clear ensemble behavior crescendonet although longer paths better performances shorter paths combination different length paths even better performance. paths generally outperform subsets. different fractalnet longest path alone achieves similar performance entire network does better paths furthermore independence paths crescendonet allows introduce pathwise training procedure paths building block trained independently sequentially. path-wise procedure reduce memory needed training. especially reduce amortized memory used training crescendonet fourth. propose crescendo block linearly increased convolutional batch-norm layers. crescendonet generated stacking crescendo blocks demonstrates high performance deep cnns achieved without explicit residual learning. analysis experiments discovered emergent behavior signiﬁcantly different fractalnet. entire crescendonet outperforms subset provide insight improving model performance increasing number paths pattern. crescendo block crescendo block built layers convolution layer activation function following batch normalization layer convolutional layers identical size conv-activation-batchnorm unit deﬁned base branch crescendo block. relu activation function avoid problem vanishing gradients variable denotes input feature maps. hyper-parameters scale interval deﬁne structure crescendo block interval speciﬁes depth difference every adjacent branches scale sets number branches block. structure branch deﬁned following equation denotes element-wise averaging operation. note feature maps path averaged element-wise leaving width channel unchanged. crescendo block shown figure structure crescendo block designed exploiting feature expressiveness. different depths parallel paths lead different receptive ﬁelds therefore generate features different abstract levels. addition incremental parallel form explicitly supports ensemble effects shows excellent characteristics efﬁcient training anytime classiﬁcation. explain demonstrate following sections. crescendonet architecture main body crescendonet composed stacked crescendo blocks max-pooling layers adjacent blocks following main body like deep cnns fully connected layers soft-max layer classiﬁer. experiments fully connected layers hidden units hidden units respectively. overall structure crescendonet simple need tune crescendo block modify entire network. reduce memory consumption training crescendonet propose path-wise training procedure leveraging independent multi-path structure model. denote stacked conv-batchnorm layers crescendo block path. train path individually shortest longest repetitively. training path freeze parameters paths. words frozen layers provide learned features support training. figure illustrates procedure path-wise training within crescendonet block containing four paths. advantages path-wise training. first path-wise training procedure signiﬁcantly reduces memory requirements convolutional layers constitutes major memory cost training cnns. example higher bound memory required computation storage gradients using momentum stochastic gradient descent algorithms reduced crescendo block paths interval second path-wise training works well various optimizers regularizations. even dropout drop-path applied model training. dropout drop-connect randomly selected subset activations weights zero respectively effective regularization techniques deep neural networks. variant drop-path shows performance improvement dropping paths training fractalnet. dropout drop-path regularizing crescendo block. drop branches block predeﬁned probability. example given drop-path rate expectation number dropped branches crescendo block four branches. fully connected layers norm weights additional term loss. evaluate models three benchmark datasets cifar cifar street view house numbers cifar cifar training images test images belonging classes respectively. images format size -pixel. svhn color images size -pixel containing images training testing respectively. note digits cropped series numbers. thus digit image center used label. data augmentation widely adopted scheme ﬁrst images zero pixels side crop padded images -pixel randomly horizontally ﬂipping probability. preprocess image three datasets subtracting mean dividing variance pixels. mini-batch gradient descent train models. implement models using tensorflow distributed computation framework nvidia gpu. also optimize models adaptive momentum estimation optimization nesterov momentum optimization respectively. adam optimization learning rate hyper-parameter adam adaptively tune learning rate training. choose momentum decay hyper-parameter smoothing term conﬁguration default setting adamoptimizer class tensorflow. nesterov momentum optimization hyper-parameter momentum decay learning rate epochs cifar epochs epochs respectively svhn. truncated normal distribution parameter initialization. standard deviation hyper-parameters convolutional weights fully connected layer weights. datasets batch size training replica. whole training epochs cifar epochs svhn. path-wise training epochs cifar epochs svhn. using crescendonet model three blocks contains four branches illustrated figure investigate following preliminary aspects model performance different block widths ensemble effect path-wise training performance. study crescendo block three different width conﬁgurations equal width globally equal width within block increasing width. three conﬁgurations fully connected layers. ﬁrst number feature maps convolutional layers. second numbers feature maps convolutional layers block. last gradually increase feature maps branch three blocks correspondingly. example number feature maps second fourth branches second block exact number maps layer deﬁned following equation nmaps denotes number feature maps layer ninmaps noutmaps number input output maps respectively nlayers number layers block ilayer index layer branch starting inspect ensemble behavior crescendonet compare performance models without drop-path technique subnets composed different combinations branches block. simplicity denote branch combination containing index branch. example means blocks subnet contains ﬁrst third branches. notation used table figure table gives comparison among crescendonet representative models cifar svhn benchmark datasets. datasets crescendonet layers outperforms alnetworks without residual connections plus original resnet resnet stochastic depth. cifar cifar without data augmentation crescendonet also performs better given models except densenet bottleneck layers compression layers. however crescendonet’s error rate matches error rate given densenet-bc svhn dataset plentiful data class. comparing fractalnet another outstanding model without residual connection crescendonet simpler structure fewer parameters higher accuracies. lower rows table compare performance model given different conﬁguration. three different widths performance simultaneously grows number feature maps. words over-ﬁtting increase capacity crescendonet appropriate scope. thus crescendonet demonstrates potential improve performance scaling addition drop-path technique shows beneﬁts models datasets fractalnet. another interesting result table performance comparison adam nesterov momentum optimization methods. comparing nesterov momentum method adam performs similarly cifar svhn worse cifar. note roughly training images class svhn cifar cifar respectively. implies adam better option training crescendonet training data abundant convenience adaptive learning rate scheduling. last table gives result path-wise training. training model less memory requirement achieved cost performance degradation. however pathwise trained crescendonet still outperform many networks without residual connections given datasets. table provides performance comparison among different path combinations crescendonet trained adam optimization block-wise width results show ensemble behavior model. speciﬁcally paths contained network better table whole classiﬁcation error cifar/cifar/svhn. highlight three accuracies column bold font. three numbers parentheses denote number output feature maps block. plus sign denotes data augmentation. sign means feature maps layers branch increase explained model conﬁguration section. compared models include network network all-cnn deeply supervised highway network fractalnet resnet resnet stochastic depth wide resnet densenet performance. whole outperforms single path network large margin. example whole based longest path show inference error rate respectively cifar without data augmentation. implicit ensemble behavior differentiates crescendonet fractalnet shows student-teacher effect. speciﬁcally longest path fractalnet achieve similar even lower error rate compared whole net. investigate dynamic behavior subnets test error rate changes subnets training. adam train crescendonet structure shown figure cifar epochs. figure illustrates behavior different path combinations training. shows inference accuracy whole grows simultaneously subnets demonstrates ensemble effect. second single path network performance grows depth. behavior anytime classiﬁer also shown fractalnet. words could short path network give rough quick inference paths gradually increase accuracy. useful time-critical applications like integrated recognition system autonomous driving tasks. conventional deep cnns alexnet vgg- directly stacked convolutional layers. however vanishing gradient problem makes difﬁcult train tune deep conventional structures. recently stacking small convolutional blocks become important method build deep cnns. introducing building blocks becomes improve performance deep cnn. ﬁrst introduced networkinnetwork module micro neural network using multiple layer perceptron local modeling. then piled micro neural networks deep macro neural network. szegedy introduced building block called inception based built googlenet. inception block four branches shallow cnns building convolutional kernels size max-pooling kernel size multiple-branch scheme used extract diversiﬁed features reducing need tuning convolutional sizes. main body googlenet inception blocks stacked other. stacking multiplebranch blocks create exponential combination feed-forward paths. structure combined dropout technique show implicit ensemble effect googlenet improved blocks powerful models xception inception-v improve scalability googlenet szegedy used convolution factorization label-smoothing regularization inception-v. addition chollet explicitly deﬁned depth-wise separable convolution module replacing inception module. recently larsson introduced fractalnet built stacked fractal blocks combination identical convolutional layers fractal expansion fashion. fractalnet showed possible train deep neural network network architecture design. fractalnet implicitly also achieved deep supervision student-teacher learning fractal architecture. however fractal expansion form increases number convolution layers associated parameters exponentially. example original fractalnet model layers million parameters resnet depth similar accuracy million parameters thus exponential expansion reduced scalability fractalnet. another successful idea network architecture design skip-connections resnet used identity mapping short connect stacked convolutional layers allows data pass layer subsequent layers. identity mapping possible train -layer convolutional neural network. huang recently proposed densenet extremely residual connections. connected layer dense block every subsequent layer. densenet achieved best performance benchmark datasets far. hand highway networks used skip-connections adaptively infuse input output traditional stacked neural network layers. highway networks helped achieve high performance language modeling translation. shown excellent performance image recognition tasks. however still challenging tune modify design cnn. propose crescendonet simple convolutional neural network architecture without residual connections crescendo block uses convolutional layers size joins feature maps branch averaging operation. number convolutional layers grows linearly crescendonet exponentially fractalnet leads signiﬁcant reduction computational complexity. even much fewer layers simpler structure crescendonet matches performance original variants resnet cifar cifar classiﬁcation tasks. like fractalnet dropout drop-path regularization mechanisms train crescendonet anytime classiﬁer namely crescendonet perform inference combination branches according latency requirements. experiments also demonstrated crescendonet synergized well adam optimization especially training data sufﬁcient. words avoid scheduling learning rate usually performed empirically training existing architectures. crescendonet shows different behavior fractalnet experiments cifar/ svhn. fractalnet longest path alone achieves similar performance entire network better paths shows student-teacher effect. whole fractalnet except longest path acts scaffold training becomes dispensable later. hand crescendonet shows whole network signiﬁcantly outperforms fact sheds light exploring mechanism improve performance deep cnns increasing number paths. mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov. improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems vinod nair geoffrey hinton. rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning yuval netzer wang adam coates alessandro bissacco andrew reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning volume nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition christian szegedy vincent vanhoucke sergey ioffe shlens zbigniew wojna. rethinking inception architecture computer vision. proceedings ieee conference computer vision pattern recognition andreas veit michael wilber serge belongie. residual networks behave like ensembles relatively shallow networks. advances neural information processing systems", "year": 2017}