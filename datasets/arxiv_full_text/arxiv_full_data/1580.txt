{"title": "Similarity-Based Models of Word Cooccurrence Probabilities", "tag": ["cs.CL", "cs.AI", "cs.LG", "I.2.7;I.2.6"], "abstract": "In many applications of natural language processing (NLP) it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations ``eat a peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in any given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on ``most similar'' words.  We describe probabilistic word association models based on distributional word similarity, and apply them to two tasks, language modeling and pseudo-word disambiguation. In the language modeling task, a similarity-based model is used to improve probability estimates for unseen bigrams in a back-off language model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.  We also compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency to avoid giving too much weight to easy-to-disambiguate high-frequency configurations. The similarity-based methods perform up to 40% better on this particular task.", "text": "many applications natural language processing necessary determine likelihood given word combination. example speech recognizer need determine word combinations peach beach likely. statistical methods determine likelihood word combination frequency training corpus. however nature language many word combinations infrequent occur given corpus. work propose method estimating probability previously unseen word combinations using available information most similar words. describe probabilistic word association models based distributional word similarity apply tasks language modeling pseudo-word disambiguation. language modeling task similarity-based model used improve probability estimates unseen bigrams back-oﬀ language model. similarity-based method yields perplexity improvement prediction unseen bigrams statistically signiﬁcant reductions speech-recognition error. also compare four similarity-based estimation methods back-oﬀ maximumlikelihood estimation methods pseudo-word sense disambiguation task controlled unigram bigram frequency avoid giving much weight easy-todisambiguate high-frequency conﬁgurations. similarity-based methods perform better particular task. data sparseness inherent problem statistical methods natural language processing. methods statistics relative frequencies conﬁgurations elements training corpus learn evaluate alternative analyses interpretations samples text speech. likely analysis taken contains frequent conﬁgurations. problem data sparseness also known zero-frequency problem arises analyses contain conﬁgurations never occurred training corpus. possible estimate probabilities observed frequencies estimation scheme generalize training data used. language processing applications sparse data problem occurs even large data sets. example essen steinbiss report split million-word corpus bigrams test partition occur training portion. trigrams sparse data problem even severe instance researchers examined training corpus consisting almost million english words discovered expect word triples english text absent training sample. thus estimating probability unseen conﬁgurations crucial accurate language modeling since aggregate probability unseen events signiﬁcant. focus particular kind conﬁguration word cooccurrence. examples cooccurrences include relationships head words syntactic constructions word sequences commonly used models probability estimate previously unseen cooccurrence function probability estimates words cooccurrence. example word bigram models probability conditioned word never occurred training following conditioning word typically calculated probability estimated frequency corpus method makes independence assumption cooccurrence frequent higher estimate regardless class-based similarity-based models provide alternative independence assumption. models relationship given words modeled analogy words sense similar given ones. instance brown suggest class-based n-gram model words similar cooccurrence distributions clustered word classes. cooccurrence probability given pair words estimated according averaged cooccurrence probability corresponding classes. pereira tishby propose soft distributional clustering scheme certain grammatical cooccurrences membership word class probabilistic. cooccurrence probabilities words modeled averaged cooccurrence probabilities word clusters. dagan marcus markovitch present similarity-based model avoids building clusters. instead word modeled speciﬁc class words similar using scheme predict unobserved cooccurrences likely others. model however provide probability estimates cannot used component larger probabilistic model would required speech recognition. class-based similarity-based methods cooccurrence modeling ﬁrst sight seem special cases clustering weighted nearest-neighbor approaches used widely machine learning pattern recognition important diﬀerences methods ours. clustering nearest-neighbor techniques often rely representing objects points multidimensional space coordinates determined values intrinsic object features. however language-modeling settings know word frequencies cooccurrences words certain conﬁgurations. since purpose modeling estimate probabilities cooccurrences cooccurrence statistics basis similarity measure model predictions. means measuring word similarity predictions words make words cooccur with whereas typical instance clustering learning methods word similarity deﬁned intrinsic features independently predictions main contributions general scheme using word similarity improve probability estimates back-oﬀ models comparative analysis several similarity measures parameter settings important language processing tasks language modeling disambiguation showing similarity-based estimates indeed useful. initial study language-model evaluation used similarity-based model estimate unseen bigram probabilities wall street journal text compared standard back-oﬀ model testing held-out sample similarity model achieved perplexity reduction back-oﬀ unseen bigrams. constituted test sample leading overall reduction test-set perplexity similarity-based model also tested speech-recognition task yielded statistically signiﬁcant reduction recognition error. disambiguation evaluation compared several variants initial method cooccurrence smoothing method essen steinbiss estimation method katz decision task involving unseen pairs direct objects verbs. found similaritybased models performed almost better back-oﬀ yielded accuracy experimental setting. furthermore scheme based jensen-shannon divergence yielded statistically signiﬁcant improvement error rate cooccurrence smoothing. also investigated eﬀect removing extremely low-frequency events training set. found that contrast back-oﬀ smoothing events often discarded training little discernible eﬀect similarity-based smoothing methods suﬀer noticeable performance degradation singletons omitted. paper organized follows. section describes general similarity-based framework; particular section presents functions measures similarity. section details initial language modeling experiments. section describes comparison experiments pseudo-word disambiguation task. section discusses related work. finally section summarizes contributions outlines future directions. wish model conditional probability distributions arising cooccurrence linguistic objects typically words certain conﬁgurations. thus consider pairs appropriate sets necessarily disjoint. follows subscript element pair; thus conditional probability pair second element given ﬁrst element denotes probability estimate according base language model ﬁrst word pair given second word denotes base estimate unigram probability word best knowledge ﬁrst particular distribution dissimilarity function statistical language processing. function implicit earlier work distributional clustering used tishby distributional similarity work. finch discusses word clustering provide experimental evaluation actual data. similarity-based language model consists three parts scheme deciding word pairs require similarity-based estimate method combining information similar words course function measuring similarity words. give details three parts following three sections. concerned similarity words conditioning events probabilities want estimate. data sparseness makes maximum likelihood estimate word pair probabilities unreliable. probability word pair conditional appearance word simply frequency training corpus frequency however zero unseen word pair pair would predicted impossible. generally unreliable events small nonzero counts well zero counts. language modeling literature term smoothing used refer methods adjusting probability estimates small-count events away alleviate unreliability. proposals address zero-count problem exclusively rely existing techniques smooth small counts. previous proposals zero-count problem adjust total probability seen word pairs less leaving probability mass redistributed among unseen pairs. general adjustment involves either interpolation used linear combination estimator guaranteed nonzero unseen word pairs discounting reduced used seen word pairs probability mass left reduction used model unseen pairs. represents good-turing discounted estimate seen word pairs denotes model probability redistribution among unseen word pairs. normalization factor. since extensive comparison study chen goodman indicated back-oﬀ better interpolation estimating bigram probabilities consider interpolation methods here; however could easily incorporate similarity-based estimates interpolation framework well. original back-oﬀ model katz used model predicting unseen word pairs model backed unigram model unseen bigrams. however conceivable backing detailed model unigrams would advantageous. therefore generalize katz’s formulation writing instead enabling similarity-based estimates unseen word pairs instead unigram frequency. observe similarity estimates used unseen word pairs only. similarity-based models make following assumption word similar word yield information probability unseen word pairs involving weighted average evidence provided similar words neighbors weight given particular word considerable latitude allowed deﬁning evidenced previous work form. essen steinbiss karov edelman however desirable restrict fashion eﬃciency reasons especially large. instance language modeling application section closest fewer words less threshold value tuned experimentally. represents eﬀect linear combination similarity estimate back-oﬀ estimate exactly katz’s back-oﬀ scheme. language modeling task experimentally; simplify comparison diﬀerent similarity models sense disambiguation would possible make depend contribution similarity estimate could vary among words. dependences often used interpolated models indeed advantageous. however since introduce hidden variables require complex training algorithm pursue direction present work. consider several word similarity measures derived automatically statistics training corpus opposed derived manually-constructed word classes sections discuss related information-theoretic functions divergence jensen-shannon divergence. section describes norm geometric distance function. section examines confusion probability previously employed language modeling tasks. course many possible functions; opted restrict attention reasonably diverse set. given. choice weight function extent arbitrary; requirement increasing similarity extremely constraining. clearly performance depends using good weight function would impossible conceivable therefore section describe experiments evaluating similarity-based models without weight functions. similarity functions describe depend base language model katz discounted model section above. discuss complexity computing similarity function noted current implementation one-time cost construct matrix word-to-word similarities parameter training takes place. kullback-leibler divergence standard information-theoretic measure dissimilarity probability mass functions apply conditional distributions induced words words whenever unfortunately generally hold mles based samples; must smoothed estimates redistribute probability mass zero-frequency events. forces makes calculation expensive large vocabularies. role free parameter control relative inﬂuence neighbors closest high also form probability distribution arose sample drawn distribution however reasons heuristic rather theoretical since rigorous probabilistic justiﬁcation similarity-based methods. extending work sugawara nishimura toshioka okochi kaneko essen steinbiss used confusion probability estimate word cooccurrence probabilities. report improvement test-set perplexity small corpus. confusion probability also used grishman sterling estimate likelihood selectional patterns. confusion probability computed empirical estimates provided unigram estimates nonzero fact smoothed estimates provided katz’s back-oﬀ scheme problematic estimates typically preserve consistency respect marginal estimates bayes’s rule associated similarity function depends parameter needs tuned experimentally. goal ﬁrst experiments described section provide proof concept showing similarity-based models achieve better language modeling performance backoﬀ. therefore used similarity measure. success experiments convinced similarity-based methods worth examining closely; results second experiments comparing several similarity functions pseudo-word disambiguation task described next section. language modeling experiments used similarity-based model divergence similarity measure alternative unigram frequency backing bigram model. used bigram language model deﬁned entire vocabulary. noted earlier estimates must smoothed avoid division zero computing employed standard katz bigram back-oﬀ model purpose. since application considered small fraction computing psim using tunable thresholds described section purpose. represents average number alternatives presented model test word. thus better model lower perplexity. task lower perplexity indicate better prediction unseen bigrams. evaluated model comparing test-set perplexity eﬀect speechrecognition accuracy baseline bigram back-oﬀ model developed lincoln laboratories wall street journal text dictation corpora provided arpa’s program baseline back-oﬀ model follows katz design except that sake compactness frequency bigrams ignored. counts used model obtained million words text years test set. best parameter values found values improvement perplexity unseen bigrams held-out thousand word sample since unseen bigrams comprise sample improvement unseen bigrams corresponds overall test perplexity improvement table shows reductions training test perplexity sorted training reduction diﬀerent choices number closest neighbors used. values best ones found equation clear computational cost applying similarity model unseen bigram therefore lower values computationally preferable. table reducing incurs penalty less perplexity improvement relatively values appear suﬃcient achieve beneﬁt similarity model. table also shows best value increases decreases; lower greater weight given conditioned word’s frequency. suggests predictive power neighbors beyond closest modeled fairly well overall frequency conditioned word. bigram similarity model also tested language model speech recognition. test data experiment pruned word lattices closed-vocabulary test sentences. scores lattices sums acoustic score languagemodel score case negative probability provided baseline bigram model. given lattices constructed lattices scores modiﬁed similarity model instead baseline model. compared best sentence hypothesis original lattice best hypothesis modiﬁed counted word disagreements hypotheses correct. total disagreements; similarity model correct cases back-oﬀ model advantage similarity model statistically signiﬁcant level. overall reduction error rate small number disagreements small compared overall number errors recognition setup employed experiments. table shows examples speech recognition disagreements models. hypotheses labeled back-oﬀ similarity bold-face words errors. similarity model seems better modeling regularities semantic parallelism lists avoiding past tense form hand similarity model makes several mistakes function word inserted place punctuation would found written text. since experiments described previous section demonstrated promising results similaritybased estimation second experiments designed help compare analyze somewhat diverse similarity measures given table unfortunately divergence confusion probability diﬀerent requirements base language model could direct four-way comparison. explained below elected omit divergence consideration. chose evaluate three remaining measures word sense disambiguation task method presented noun verbs asked verb likely noun direct object. thus measure absolute quality assignment probabilities would case perplexity evaluation rather relative quality. could therefore ignore constant factors normalize similarity measures. usual word sense disambiguation problem method tested presented ambiguous word context asked identify correct sense word context. example test instance might sentence fragment robbed bank; question whether bank refers river bank savings bank perhaps alternative meaning. sense disambiguation clearly important problem language processing applications evaluation task presents numerous experimental diﬃculties. first notion sense clearly deﬁned; instance dictionaries provide sense distinctions coarse data hand. also needs training data correct senses assigned; acquiring correct senses generally requires considerable human eﬀort. furthermore words many possible senses whereas others essentially monosemous; means test cases uniformly hard. circumvent diﬃculties pseudo-word disambiguation experiment format follows. first list pseudo-words constructed combination diﬀerent words word contributes exactly pseudo-word. then every test replaced corresponding pseudo-word. example pseudo-word created words make take data altered follows advantages using pseudo-words two-fold. first alternative senses control experimenter. test instance presents exactly alternatives disambiguation method alternatives chosen frequency part speech secondly pre-transformation data yields correct answer hand-tagging word senses necessary. advantages make pseudo-word experiments elegant simple means test eﬃcacy diﬀerent language models; course provide completely accurate picture models would perform real disambiguation tasks although could create realistic settings making pseudo-words words varying frequencies alternative pseudo-senses ease comparison consider interpolation unigram probabilities. thus model used experiments diﬀers slightly used language modeling tests; summarized follows used statistical part-of-speech tagger pattern matching concordancing tools identify transitive main verbs head nouns corresponding direct objects million words associated press newswire. selected noun-verb pairs frequent nouns corpus. pairs undoubtedly somewhat noisy given errors inherent part-of-speech tagging pattern matching. used pairs derived building models reserving testing purposes. some similarity measures require smoothed models calculated katz back-oﬀ model maximumlikelihood model furthermore wished evaluate hypothesis compact language model built without aﬀecting model quality deleting singletons word pairs occur once training set. claim made particular language modeling therefore built four base models summarized table since wished test eﬀectiveness using similarity unseen word cooccurrences removed test data verb-object pairs occurred training set; resulted unseen pairs unseen pairs divided equal-sized parts formed basis ﬁvefold cross-validation runs used performance test four combined used tuning parameters simple grid search evaluated error tuning regularly spaced points parameter space. finally test pseudo-words created pairs verbs similar frequencies control word frequency decision task. method simply rank verbs frequency create pseudo-words adjacent pairs table lists randomly chosen pseudowords frequencies corresponding verbs. performances four base language models shown table mle- mle-o error rates exactly test sets consist unseen bigrams assigned probability maximum-likelihood estimates thus ties method. back-oﬀ models bo-o also perform similarly. since back-oﬀ models consistently performed worse models chose models subsequent experiments. therefore comparisons measures could utilize unsmoothed data namely norm jensen-shannon divergence diﬀerent order emerges play likely verb cooccur role ranked higher likely verb cooccur thus indicating role higher confusion probability respect does. shows closest words order base language model mle-o. relative order four closest words remains same; however next words quite diﬀerent mle-. data suggests eﬀect singletons calculations similarity quite strong borne experimental evaluations described section conjecture eﬀect fact many low-frequency verbs data omitting singletons involving verbs well drastically alter number verbs cooccur given nouns since similarity functions consider experiments depend words surprising eﬀect deleting singletons rather dramatic. contrast back-oﬀ language model sensitive missing singletons good-turing discounting small counts inﬂation zero counts. figure shows results experiments test sets using mle- base language model. parameter always optimal value corresponding training set. rand shown comparison purposes simply chooses weights randomly. equal cases. similarity-based methods consistently outperformed katz’s back-oﬀ method large margin indicating information word pairs useful unseen pairs unigram frequency informative. similarity-based methods also much better rand indicates enough simply combine information words arbitrarily word similarity taken account. cases edged methods. average improvement using instead diﬀerence signiﬁcant level according paired t-test. results mle-o case depicted figure again similarity-based methods achieving lower error rates back-oﬀ rand methods again always performed best. however omitting singletons ampliﬁed disparity average diﬀerence signiﬁcant level treated diﬀerently unseen bigrams even seen bigrams extremely rare. thus conclude cannot create compressed similarity-based language model omitting singletons without hurting performance least task. figure shows value aﬀects disambiguation performance. four curves shown corresponding choice similarity function base language model. error bars depict average range error rates disjoint test sets. immediately clear good performance results must much higher jensen-shannon divergence norm. phenomenon results fact range possible values much smaller compression values requires large scale diﬀerences distances correctly. also observe setting causes substantially worse error rates; however curves level rather moving upwards again. long suﬃciently large value chosen setting suboptimally greatly impact performance. furthermore shape curves base language models suggesting relation test-set performance relatively insensitive variations training data. fact higher values seem lead better error rates suggests role ﬁlter distant neighbors. test hypothesis experimented using similar neighbors. figure shows error rate depends diﬀerent ﬁxed values lowest curves depict performance jensen-shannon divergence norm optimal value respect average test performance; appears distant neighbors essentially eﬀect error rate contribution negligible. contrast value chosen distant neighbors weighted heavily. case including distant neighbors causes serious degradation performance. figure error rates test base language model mle-. methods going left right rand performances shown settings optimal corresponding training set. ranged neighbors actually improves error rate. seems indicate confusion probability correctly ranking similar words order informativeness. however alternative explanation disadvantage employed context tunable weighting scheme. distinguish possibilities experiment dispensed weights altogether. instead took vote similar neighbors alternative chosen likely preferred majority similar neighbors results shown figure similar neighbors according always informative chosen according confusion probability largest performance gaps occurring graph provides clear evidence confusion probability good measure informativeness words. large body work notions work similarity word clustering applications. impossible compare methods directly since assumptions experimental settings applications methods vary widely. therefore discussion mainly descriptive highlighting main similarities diﬀerences methods. work instance growing body research using word similarity improve performance language-processing problems. similarity-based algorithms either similarity early attempt automatically classify words semantic classes carried linguistic string project semantic classes derived similar cooccurrence patterns words within syntactic relations. cooccurrence statistics considered class level used alleviate data sparseness syntactic disambiguation. sch¨utze captures contextual word similarity ﬁrst reducing dimensionality context representation using singular value decomposition using reduceddimensionality representation characterize possible contexts word. information used word sense disambiguation. occurrences ambiguous word clustered cluster mapped manually senses word. context vector occurrence ambiguous word mapped nearest cluster determines sense occurrence. sch¨utze emphasizes method avoids clustering words pre-deﬁned classes claiming clustering likely introduce artiﬁcial boundaries words part semantic neighborhood. karov edelman also addressed data sparseness problem word sense disambiguation using word similarity. circular deﬁnition word similarity measure context similarity measure. circularity resolved iterative process system learns typical usages senses ambiguous word. given occurrence ambiguous word system selects sense whose typical context similar current context applying procedure resembles sense selection process sh¨utze. scheme employing word similarity disambiguation inﬂuenced work dagan method computes word similarity measure directly word cooccurrence data. word modeled similar words plausibility unseen cooccurrence judged cooccurrence statistics words set. similarity measure weighted tanimoto measure version also used grefenstette word association measured mutual information following earlier work word method dagan provide probabilistic models. disambiguation decisions based comparing scores diﬀerent alternatives produce explicit probability estimates therefore cannot integrated directly within larger probabilistic framework. cooccurrence smoothing model essen steinbiss like model produces explicit estimates word cooccurrence probabilities based cooccurrence statistics similar words. similarity-based estimates interpolated direct estimates n-gram probabilities form smoothed n-gram language model. word similarity model computed confusion probability measure described evaluated earlier. several language modeling methods produce similarity-based probability estimates class-based models. methods direct measure similarity word words instead cluster words classes using global optimization criterion. brown present class-based n-gram model records probabilities sequences word classes instead sequences individual words. probability estimate bigram contains particular word aﬀected bigram statistics words class words class considered similar cooccurrence behavior. word classes formed bottom-up hard-clustering algorithm whose objective function average mutual information class cooccurrence. ushioda introduces several improvements mutual-information clustering. method applied part-of-speech tagging records classes contained particular word bottom-up merging process. word represented mixture classes rather single class. algorithms kneser ueberla similar brown although diﬀerent optimization criterion used number clusters remains constant throughout membership assignment process. pereira formalism statistical mechanics derive top-down soft-clustering algorithm probabilistic class membership. word cooccurrence probability modeled weighted average class cooccurrence probabilities weights correspond membership probabilities words within classes. figure average range test-set error rates varied. base language model mle-. similarity function indicated point style; dashed dotted lines indicate suboptimal choice approaches described previous section induce word similarity relationships word clusters cooccurrence statistics corpus. researchers developed methods quantify similarity relationships based information manually crafted wordnet thesaurus resnik proposes node-based approach measuring similarity pair words thesaurus applies various disambiguation tasks. similarity function information-theoretic measure informativeness least general common ancestor words thesaurus classiﬁcation. jiang conrath combine node-based approach edge-based approach similarity nodes thesaurus inﬂuenced path connects them. similarity method tested data word pair similarity ratings derived human judgments. derives general concept-similarity measure assumptions desired properties similarity. measure function number bits required describe concepts well commonality. describes instantiation measure hierarchical thesaurus applies wordnet part word sense disambiguation algorithm. query expansion information retrieval provides additional motivation automatic identiﬁcation word similarity. line work literature considers words similar occur often documents. another line work considers type word similarity concerned with similarity measured derived word-cooccurrence statistics. grefenstette argues cooccurrence within document yields similarity judgements sharp enough query expansion. instead extracts coarse syntactic relationships texts represents word word-cooccurrences within relation. figure average range voting-scheme test-set error rates varied. similarity function indicated point style; base language model indicated line style. word similarity deﬁned weighted version tanimoto measure compares cooccurrence statistics words. similarity method evaluated measuring impact retrieval performance. ruge also extracted word cooccurrences within syntactic relationships evaluated several similarity measures data focusing versions cosine measure. similarity rankings obtained measures compared produced human judges. similarity-based language models provide appealing approach dealing data sparseness. work proposed general method using similarity-based models improve estimates existing language models evaluated range similarity-based models parameter settings important language-processing tasks. pilot study compared language modeling performance similarity-based model standard back-oﬀ model. improvement achieved bigram back-oﬀ model statistically signiﬁcant relatively modest overall eﬀect small proportion unseen events. second detailed study compared several similarity-based models parameter settings smaller manageable word-sense disambiguation task. observed similarity-based methods perform much better unseen word pairs measure based jensen-shannon divergence best overall. experiments restricted bigram probability estimation reasons simplicity computational cost. however relatively small proportion unseen bigrams test data makes eﬀect similarity-based methods necessarily modest overall tasks. believe beneﬁts similarity-based methods would substantial tasks larger proportion unseen events instance language modeling longer contexts. obstacle principle this trigram case example would still determining probability pairs would consist word pairs instead single words. however number possible similar events given element much larger bigram case. direct tabulation events similar event would thus practical compact approximate representations would investigated. would also worth investigating beneﬁt similarity-based methods improve estimates low-frequency seen events. however would need replace back-oﬀ model another combines multiple estimates event example interpolated model context-dependent interpolation parameters. another area investigation relationship similarity-based classbased approaches. mentioned introduction rely common intuition namely events modeled extent similar events. class-based methods computationally expensive training time nearest neighbor methods require searching best model structure estimation hidden parameters hand class-based methods reduce dimensionality thus smaller eﬃcient test time. dimensionality reduction also claimed improve generalization test data evidence mixed. furthermore class-based models theoretically satisfying probabilistic interpretations whereas justiﬁcation similarity-based models heuristic empirical present. given variety class-based language modeling algorithms described section related work above beyond scope paper compare performance approaches. however comparison especially would bring approaches common probabilistic interpretation would well worth pursuing. thank hiyan alshawi joshua goodman rebecca slava katz doug mcilroy stuart shieber yoram singer many helpful discussions doug paul help bigram back-oﬀ model andrej ljolje michael riley providing word lattices speech recognition evaluation. also thank reviewers paper constructive criticisms editors present issue claire cardie mooney help suggestions. portions work appeared previously thank reviewers papers comments. part work done ﬁrst author member technical staﬀ visitor at&t labs second author graduate student harvard university summer visitor at&t labs. second author received partial support national science foundation grant iri- national science foundation graduate fellowship at&t grpw/alfp grant.", "year": 1998}