{"title": "Bayesian Sparsification of Recurrent Neural Networks", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Recurrent neural networks show state-of-the-art results in many text analysis tasks but often require a lot of memory to store their weights. Recently proposed Sparse Variational Dropout eliminates the majority of the weights in a feed-forward neural network without significant loss of quality. We apply this technique to sparsify recurrent neural networks. To account for recurrent specifics we also rely on Binary Variational Dropout for RNN. We report 99.5% sparsity level on sentiment analysis task without a quality drop and up to 87% sparsity level on language modeling task with slight loss of accuracy.", "text": "recently molchanov proposed principled method based variational dropout sparsiﬁcation fully connected convolutional networks. probabilistic model described parameters controlling sparsity tuned automatically neural network training. model called sparse variational dropout leads extremely sparse solutions without signiﬁcant quality drop. however technique previously investigated rnns. paper apply sparse recurrent neural networks. take account speciﬁcs rnns rely insights underlined paper ghahramani explain proper binary dropout rnns bayesian point view. experiments show lstms sparse yield high sparsity level slight drop quality. achieved sparsity level sentiment analysis task character level language modeling experiment. consider neural network weights modeling dependency target variables corresponding input objects bayesian neural network weights treated random variables. prior distribution search posterior distribution help expected target value inference. case neural networks true posterior usually intractable approximated parametric distribution quality approximation measured kldivergence kl||p). optimal parameter found maximization variational lower bound w.r.t. recurrent neural networks show state-of-theart results many text analysis tasks often require memory store weights. recently proposed sparse variational dropout eliminates majority weights feed-forward neural network without signiﬁcant loss quality. apply technique sparsify recurrent neural networks. account recurrent speciﬁcs also rely binary variational dropout report sparsity level sentiment analysis task without quality drop sparsity level language modeling task slight loss accuracy. recurrent neural networks among powerful models natural language processing speech recognition question-answering systems problems sequential data complex tasks machine translation speech recognition modern architectures incorporate huge number parameters. models portable devices limited memory instance smartphones model compression desired. high compression level also lead acceleration rnns. addition compression regularizes rnns helps avoid overﬁtting. *equal contribution national research university higher school economics moscow russia kaspersky moscow russia lomonosov moscow state university moscow russia yandex moscow russia. correspondence ekaterina lobacheva <elobachevahse.ru> nadezhda chirkova <nchirkovahse.ru>. expected log-likelihood term usually approximated monte-carlo sampling. make estimation unbiased weights parametrized deterministic function sampled non-parametric distribution kl-divergence term acts regularizer usually computed approximated analytically. dropout standard technique regularization neural networks. implies inputs layer multiplied randomly generated noise vector. elements vector usually sampled bernoulli gaussian distribution parameters tuned using cross-validation. kingma interpreted gaussian dropout bayesian perspective allowed tune dropout rate automatically model training. later model extended sparsify fully connected convolutional neural networks resulting model called sparse variational dropout consider dense layer feed-forward neural network input size output size weight matrix following kingma sparse prior weights fully factorized logrepresentation called additive reparameterization reduces variance gradients mij. moreover since normal distributions normal distribution computable parameters noise applied preactivation instead trick called local reparameterization trick reduces variance gradients even makes training efﬁcient. kl-divergence term encourages large values αij. weight posterior weight high-variance normal distribution beneﬁcial model well αijm avoid inaccurate predictions. result posterior approaches zero-centered δ-function weight affect network’s output ignored. ghahramani considered rnns bayesian networks. prior recurrent layer weights fully factorized standard normal distribution. posterior factorized rows weights factor searched mixture normal distributions k|mx using stochastic mini-batch methods. recurrence expected log-likelihood term unfolded approximated using integral estimated single sample input sequence. reparameterization trick additive reparameterization sample input-to-hidden hidden-to-hidden ﬁnal framework works follows sample gaussian additive noise input-to-hidden preactivations hidden-to-hidden weight matrix optimize variational lower integral ﬁrst part estimated integration single sample make estimation unbiased weights sample remain time steps ﬁxed object; dropout rates ﬁxed distribution sampling depends them. kl-divergence term approximately equivalent regularization variational parameters finally probabilistic model leads following dropout application rnns sample binary mask input hidden neurons mask object moments time optimize l-regularized log-likelihood dropout rates weight l-regularization chosen using cross-validation. also dropout technique applied forward connections rnns example embedding dense layers rnns proposed ghahradropout mani sensitive choice dropout rates. hand sparse allows automatic tuning gaussian dropout parameters individually weight results model sparsiﬁcation. combine techniques sparsify regularize rnns. lstm prior-posterior pair consisered input-to-hidden hidden-to-hidden matrices computations stay same. noise matrices inputto-hidden hidden-to-hidden connections generated individually gates input modulation perform experiments lstm popular recurrent architecture nowadays. theano lasagne implementation. source code available soon https//github.com/tiptp/ sparsebayesianrnn. demonstrate effectiveness approach diverse problems character level language modeling sentiment analysis. results show sparse variational dropout leads high level sparsity recurrent models without signiﬁcant quality drop. according molchanov training neural networks sparse variational dropout random initialization troublesome weights become pruned away could possibly learn something useful data. observe effect experiments lstms especially complex models. lstm trained random initialization high sparsity level also noticeable quality drop. overcome issue start pre-trained models obtain training networks without sparse variational dropout several epochs. weights models sparse variational dropout cannot converge exactly zero stochastic nature training procedure. obtain sparse networks explicitly weights high corresponding dropout rates testing molchanov value threshold. table results sentiment regression task. prediction quality reported sparsity levels reported separately percents zero weights. sparse methods initialization types reported brackets. sentiment analysis data. following ghahramani evaluated approach sentiment analysis regression task. dataset constructed based cornell reviews corpus collected pang consists approximately thousands non-overlapping segments words reviews. task predict corresponding scores provided train test partitions. setup. networks embedding layer units lstm layer hidden units ﬁnally fully connected layer applied last output lstm weights initialized ghahramani train networks using batches size learning rate epochs. also clip gradients threshold layers dropout rate weight decay start training network sparse random initialization different pretrained models. ﬁrst pre-trained model obtained epochs training network without dropout. second obtained epochs training network layers. choose number pretraining epochs using models quality crossvalidation. results shown table task approach achieves extremely high sparsity level random initialization pre-trained models. sparse networks trained pre-trained models achieve even better quality baselines. note models already sparsity level approximately epochs. character level language modeling data. following mikolov penn treebank corpus train language model dataset contains approximately million characters vocabulary characters. provided train validation test partitions. setup. networks lstm layer hidden units solve character level task. weight matrices networks initialized orthogonally biases initialized zeros. initial values hidden cell elements trainable also initialized zeros. train networks non-overlapping sequences characters batches using learning rate epochs clip gradients threshold layers dropout rate weight decay results. baseline train network without dropout recurrent weights semeniuta showed particular task applying dropout feed-forward connections additionally recurrent ones improve network quality. observe effect experiments. experiment sparsify lstm dense layers therefore apply sparse layers. start training network sparse random initialization different pre-trained models. ﬁrst pre-trained model obtained epochs training network without dropout. second obtained epochs training network recurrent connections. choose number pretraining epochs using models quality validation set. results shown table achieve extreme sparsity level previous experiment. effect consequence higher complexity task. also problem several outputs input sequence instead output sentiment regression. result log-likelihood part loss function much stronger task regularizer sparsify network effectively. balance likelihood regularizer varies different tasks table results character level language modelling task. prediction quality reported bits-per-character sparsity levels reported separately percents zero weights. sparse methods initialization types reported brackets. fig. show progress test quality network sparsity level training process. sparse network trained random initialization underﬁts therefore slight quality drop comparison baseline network without regularization. sparse networks trained pre-trained models achieve much higher quality lower sparsity levels trained random initialization. better pretrained models harder sparsify. quality model pretrained drops ﬁrst epoches sparsity grows model fully recover later. deep neural networks often suffer overﬁtting different regularization techniques used improve generalization ability. dropout popular method neural networks regularization. ﬁrst successful implementations method rnns applied dropout feed-forward connections recurrent ones. introducing dropout recurrent connections lead better regularization technique straightforward implementation results underﬁtting memory loss time several ways dropout application recurrent connections lstm proposed recently methods inject binary noise different parts lstm units. semeniuta shows proper implementation dropout recurrent connections important effective regularization also avoid vanishing gradients. figure results character level language modelling task prediction quality test bits-per-character left training random initialization right training pretrained initialization. stars correspond pretrained models. sparse methods initialization types reported brackets. applying sparse rnns rely dropout rnns proposed ghahramani reason dropout technique rnns closest sparse approach. however several dropout methods recurrent networks outperform baseline comparison future work. combining sparse latest dropout recipes also interesting research direction. challenge noise neurons gates instead weights model. however several recent papers group sparsity methods proposed fully connected convolutional networks. methods used solve underlined problem. comparison approach sparsiﬁcation techniques still work-in-progress. would interesting perform comparison larger networks example speech recognition task. curious direction research sparsify recurrent layer embedding layer too. parameters tasks large dictionary word based language modeling. acknowledgements would like thank dmitry molchanov arsenii ashukha valuable feedback. nadezhda chirkova supported russian academic excellence project ekaterina lobacheva supported russian science foundation grant would also like thank department algorithms theory programming faculty innovation high technology moscow institute physics technology provided computational resources. reducing size important rapidly developing area research. possible concept represent large weight matrix approximation smaller size. example tjandra tensor train decomposition weight matrices approximate matrix kronecker product. hubara limit weights activations binary values proposing compute gradients them. another concept start large network reduce size training. popular approach pruning weights threshold. narang choose threshold using several hyperparameters control frequency references amodei dario ananthanarayanan sundaram anubhai rishita deep speech end-to-end speech recognition english mandarin. proceedings international conference machine learning chan william jaitly navdeep quoc vinyals oriol. listen attend spell neural network large vocabulary conversational speech recognition. icassp yarin ghahramani zoubin. dropout bayesian approximation representing model uncertainty deep learning. proceedings international conference international conference machine learning yarin ghahramani zoubin. theoretically grounded application dropout recurrent neural netadvances neural information processing works. systems hubara itay courbariaux matthieu soudry daniel elyaniv bengio yoshua. quantized neural networks training neural networks precision weights activations. arxiv e-prints abs/. september mikolov kombrink burget cernocky j.h. khudanpur sanjeev. extensions recurrent neural network language model. proceedings ieee international conference acoustics speech signal processing molchanov dmitry ashukha arsenii vetrov dmitry. variational dropout sparsiﬁes deep neural networks. proceedings international conference machine learning icml pang lillian. seeing stars exploiting class relationships sentiment categorization respect rating scales. association computational linguistics mengye kiros ryan zemel richard exploring models data image question answering. advances neural information processing systems annual conference neural information processing systems semeniuta stanislau severyn aliaksei barth erhardt. recurrent dropout without memory loss. coling international conference computational linguistics proceedings conference technical papers", "year": 2017}