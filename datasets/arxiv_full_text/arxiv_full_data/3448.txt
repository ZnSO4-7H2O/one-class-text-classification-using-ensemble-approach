{"title": "Real-Time Adaptive Image Compression", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "We present a machine learning-based approach to lossy image compression which outperforms all existing codecs, while running in real-time.  Our algorithm typically produces files 2.5 times smaller than JPEG and JPEG 2000, 2 times smaller than WebP, and 1.7 times smaller than BPG on datasets of generic images across all quality levels. At the same time, our codec is designed to be lightweight and deployable: for example, it can encode or decode the Kodak dataset in around 10ms per image on GPU.  Our architecture is an autoencoder featuring pyramidal analysis, an adaptive coding module, and regularization of the expected codelength. We also supplement our approach with adversarial training specialized towards use in a compression setting: this enables us to produce visually pleasing reconstructions for very low bitrates.", "text": "present machine learning-based approach lossy image compression outperforms existing codecs running real-time. algorithm typically produces ﬁles times smaller jpeg jpeg times smaller webp times smaller datasets generic images across quality levels. time codec designed lightweight deployable example encode decode kodak dataset around image gpu. architecture autoencoder featuring pyramidal analysis adaptive coding module regularization expected codelength. also supplement approach adversarial training specialized towards compression setting enables produce visually pleasing reconstructions bitrates. streaming digital media makes internet trafﬁc projected reach however challenging existing commercial compression algorithms adapt growing demand changing landscape requirements applications. digital media transmitted wide variety settings available codecs one-size-ﬁts-all hard-coded cannot customized particular cases beyond high-level hyperparameter tuning. last years deep learning revolutionized many tasks machine translation speech recognition face recognition photo-realistic image generation. even though world compression seems natural domain machine learning approaches beneﬁted advancements main reasons. first deep learning primitives forms well-suited construct representations sufﬁciently compact. recently number important efforts toderici theis ball´e johnston towards alleviating this section second difﬁcult develop deep learning compression approach sufﬁciently efﬁcient deployment environments constrained computation power memory footprint battery life. algorithm outperforms existing image compression approaches traditional ml-based typically produces ﬁles times smaller jpeg jpeg times smaller webp times smaller kodak photocd raise-k datasets across quality levels. time designed approach lightweight efﬁciently deployable. takes around encode decode image datasets jpeg encode/decode times ms/ms ms/ms webp ms/ms. results representative quality level presented table additionally supplement algorithm adversarial training specialized towards compression setting. enables produce convincing reconstructions bitrates. table performance different codecs raise-k dataset representative ms-ssim value ycbcr color spaces. comprehensive results found section emphasize codec gpu. figure examples reconstructions different codecs bits pixel values. uncompressed size examples represent compression around times. reduce bitrates codecs header lengths fair comparison. codec search bitrates present reconstruction smallest ours. webp jpeg able produce reconstructions reconstructions presented smallest bitrate offer. examples found appendix. background related work traditional compression techniques compression general closely related pattern recognition. able discover structure input eliminate redundancy represent succinctly. traditional codecs jpeg achieved pipeline roughly breaks modules transformation quantization encoding rabbani joshi provide great overviews jpeg standards). traditional codecs since components hard-coded heavily engineered together. example coding scheme custom-tailored match distribution outputs preceding transformation. jpeg instance employs block transforms followed run-length encoding exploits sparsity pattern resultant frequency coefﬁcients. employs adaptive arithmetic coder capture distribution coefﬁcient magnitudes produced preceding multiresolution wavelet transform. pipelines still remains signiﬁcant room improvement compression efﬁciency. example transformation ﬁxed place irrespective distribution inputs adapted statistics way. addition hard-coded approaches often compartmentalize loss information within quantization step. such transformation module chosen bijective however limits ability reduce redundancy prior coding. moreover encode-decode pipeline cannot optimized particular metric beyond manual tweaking even perfect metric image quality assessment traditional approaches cannot directly optimize reconstructions ﬁrst efforts bottou example introduced djvu format document image compression employs techniques segmentation k-means clustering separate foreground background analyze document’s contents. figure overall model architecture. feature extractor described section discovers structure reduces redundancy pyramidal decomposition interscale alignment modules. lossless coding scheme described section compresses quantized tensor bitplane decomposition adaptive arithmetic coding. adaptive codelength regularization modulates expected code length prescribed target bitrate. distortions target reconstruction penalized reconstruction loss. discriminator loss described section encourages visually pleasing reconstructions penalizing discrepancies distributions targets’. high level natural approach implement encoder-decoder image compression pipeline autoencoder target bitrate bottleneck train model minimize loss function penalizing reconstruction. requires carefully constructing feature extractor synthesizer encoder decoder selecting appropriate objective possibly introducing coding scheme compress ﬁxedsize representation attain variable-length codes. many existing ml-based image compression approaches follow general strategy. toderici explored various transformations binary feature extraction based different types recurrent neural networks; binary representations entropy-coded. johnston enabled another considerable leap performance introducing loss weighted ssim spatiallyadaptive allocation. theis ball´e quantize rather binarize propose strategies approximate entropy quantized representation provides proxy penalize finally pied piper recently claimed employ techniques middle-out algorithm although nature shrouded mystery. generative adversarial networks exciting innovations machine learning last years idea generative adversarial networks idea construct generator network whose goal synthesize outputs according target distribution ptrue discriminator network whose goal distinguish examples sampled ground truth distribution ones produced generator. expressed concretely terms minimax problem ex∼ptrue ez∼pz adversarial training framework particularly relevant compression world. traditional codecs distortions often take form blurriness pixelation artifacts unappealing increasingly noticeable bitrate lowered. propose multiscale adversarial training model encourage reconstructions match statistics ground truth counterparts resulting sharp visually pleasing results even bitrates. know ﬁrst propose using gans image compression. model model architecture shown figure comprises number components brieﬂy outline below. section limit focus operations performed encoder since decoder simply performs counterpart inverse operations address exceptions require particular attention. feature extraction. images feature number different types structure across input channels within individual scales across scales. design feature extraction architecture recognize these. consists pyramidal decomposition analyzes individual scales followed interscale alignment procedure exploits structure shared across scales. code computation regularization. module responsible compressing extracted features. quantizes features encodes adaptive arithmetic coding scheme applied binary expansions. adaptive codelength regularization introduced penalize entropy features coding scheme exploits achieve better compression. pyramidal decomposition encoder loosely inspired wavelets multiresolution analysis input analyzed recursively feature extraction downsampling operators jpeg standard example employs discrete wavelet transforms daubechies kernels transform fact linear operator entirely expressed compositions convolutions hard-coded separable ﬁlters applied irrespective scale independently channel. idea pyramidal decomposition employed machine learning instance mathieu uses pyramidal composition next frame prediction denton uses image generation. spectral representations activations also investigated rippel enable processing across spectrum scales approach enable processing wavelet analysis. generalize wavelet decomposition idea learn optimal nonlinear extractors individually scale. assume input model total scales. perform recursive analysis denote input scale input ﬁrst scale input model. scale perform operations ﬁrst extract coefﬁcients rcm×hm×wm parametrized function output channels height width second compute input next scale downsampling operator illustrated figure coefﬁcient extraction pipeline scales. pyramidal decomposition module discovers structure within individual scales. extracted coefﬁcient maps aligned discover joint structure across different scales. interscale alignment designed leverage information shared across different scales beneﬁt offered classic wavelet analysis. takes input coefﬁcients extracted different scales {cm}m rcm×hm×wm produces single tensor target output dimensionality this ﬁrst input tensor target dimensionality parametrized function involves ensuring function spatially resamples appropriate output size outputs appropriate number channels apply another parametrized non-linear transformation joint processing. interscale alignment module seen figure denote output practice choose convolution deconvolution appropriate stride produce target spatial size section detailed discussion. choose simply sequence convolutions. code computation regularization given extracted tensor rc×h×w proceed quantize encode pipeline involves number components overview describe detail throughout section. bitplane decomposition. quantized tensor transformed binary tensor suitable encoding lossless bitplane decomposition bitplanedecomposeb }b×c×h×w adaptive arithmetic coding. adaptive arithmetic coder trained leverage structure remaining data. encodes ﬁnal variable-length binary sequence length problem classic autoencoder architectures bottleneck ﬁxed capacity. bottleneck small represent complex patterns well affects quality large simple patterns results inefﬁcient compression. need model capable generating long representations complex patterns short simple ones maintaining expected codelength target large number examples. achieve this necessary sufﬁcient. extend architecture increasing dimensionality time controlling information content thereby resulting shorter compressed code aacencode speciﬁcally introduce adaptive codelength regularization enables regulate expected codelength target value target. penalty designed encourage structure exactly able exploit namely regularize quantized tensor train model continuously modulate scalar coefﬁcient pursue target codelength. feedback loop. monitor mean number effective bits. high increase decrease practice model reaches equilibrium hundred iterations able maintain throughout training. hence knob tune ratio total bits namely bchw bits available target number effective bits target. allows exploring trade-off increasing number channels spatial size figure spatial maps rh×w decomposed bitplanes element ˆychw expressed binary expansion. bitplanes adaptive arithmetic coder variable-length encoding. adaptive codelength regularization enables compact codes higher bitplanes encouraging feature higher sparsity. special case reduces exactly binary quantization scheme. ml-based approaches compression employ thresholding found better performance smoother quantization described. quantize models paper. decompose bitplanes. transformation maps value ˆychw binary expansion bits. hence spatial maps rh×w expands binary bitplanes. illustrate transformation figure denote output }b×c×h×w transformation lossless. described section decomposition enable entropy coder exploit structure distribution activations achieve compact representation. section introduce strategy encourage exploitable structure featured. output bitplane decomposition binary tensor contains signiﬁcant structure example higher bitplanes sparser spatially neighboring bits often value exploit entropy lossless compression adaptive arithmetic coding. namely associate location context comprises features indicative value. based position well values neighboring bits. train classiﬁer predict value context features probabilities compress arithmetic coding. decoding decompress code performing inverse operation. namely interleave computing context particular using values previously decoded bits using context retrieve activation probability decode note constrains context include features composed bits already decoded. figure compression results raise-k dataset measured domain ycbcr domain compare commercial codecs jpeg jpeg webp plots left present average reconstruction quality function number bits pixel ﬁxed image. plots right show average compressed sizes relative different target ms-ssim values image. section discuss curve generation procedures detail. many approaches featuring reconstruction discrimination loss target reconstruction treated independently separately assigned label indicating whether real fake. formulation consider target reconstruction jointly single example compare asking images real one. this ﬁrst swap target reconstruction input pair discriminator uniform probability. following random swap propagate examples network. however instead producing output classiﬁcation last layer pipeline accumulate scalar outputs along branches constructed along different depths. average attain ﬁnal value provided terminal sigmoid function. multiscale architecture allows aggregating information across different scales motivated observation undesirable artifacts vary function scale exhibited. example high-frequency artifacts noise blurriness discovered earlier scales whereas abstract discrepancies found deeper scales. apply discriminator aggregate across scales proceed formulate objectives described section complete discriminator architecture illustrated figure training system tricky optimization instability. case able address designing training scheme adaptive ways. first reconstructor trained confusion signal gradient well reconstruction loss gradient balance function gradient magnitudes. second point training either train discriminator propagate confusion signal reconstructor function prediction accuracy discriminator. figure performance kodak photocd dataset measured domain ycbcr domain compare commercial codecs jpeg jpeg webp well recent mlbased compression work toderici theis ball´e johnston settings results exist. plots left present average reconstruction quality function number bits pixel ﬁxed image. plots right show average compressed sizes relative different target ms-ssim values image. similarity metric. trained tested models multi-scale structural similarity index metric metric specifically designed match human visual system established signiﬁcantly representative losses family variants psnr. color space. since human visual system much sensitive variations brightness color codecs represent colors ycbcr color space devote bandwidth towards encoding luma rather chroma. common assign components weights many ml-based compression papers evaluate similarity space equal color weights allow fair comparison standard codecs jpeg jpeg webp since designed perform optimally domain. work provide comparisons traditional ml-based codecs present results domain equal color weights well ycbcr weights above. reported performance metrics. present compression performance algorithm also runtime. requirement running approach real-time severely constrains capacity model must enable feasible deployment real-life applications. training deployment procedure. trained tested models geforce custom codebase. trained models patches sampled random yahoo flickr creative comoptimized models adam used initial learning rate reduced twice factor training. chose batch size trained model total iterations. initialized coefﬁcient runtime deployed model arbitrarily-sized images fully-convolutional way. attain ratedistortion curves presented section trained models range target bitrates target. test sets. enable comparison approaches ﬁrst present performance kodak photocd dataset. kodak dataset popular testing compression performance contains images hence susceptible overﬁtting necessarily fully capture broader statistics natural images. such additionally present performance raise-k dataset contains images. resized image size intend release preparation code enable reproduction dataset used. reconstructions ball´e images kodak dataset found http//www.cns.nyu.edu/ ˜lcv/iclr/ ycbcr across spectrum bpps. compute curves procedure described section. compressing image introduces artifacts bias particular codec used results favorable curve compressed codec. figure plot demonstrating effect. codecs. compare commercial compression techniques jpeg jpeg webp well recent mlbased compression work toderici theis ball´e johnston settings results available. also compare which widely used surpassed codecs past. best-performing conﬁguration jpeg jpeg webp reduce bitrates respective header lengths fair comparison. performance evaluation. image test compression approach color space selection available compression rates recorded ms-ssim computation times encoding decoding. important take great care design performance evaluation procedure. image separate curve computed available compression rates given codec ball´e discusses detail different summaries curves lead disparate results. evaluations compute given curve sweep across values independent variable interpolate individual curve independent variable value average results. ensure accurate interpolation sample densely across rates codec. acknowledgements grateful trevor darrell sven strohband michael gelbart robert nishihara albert azout vinod khosla meaningful discussions input. dang-nguyen duc-tien pasquini cecilia conotter valentina boato giulia. raise images dataset digital image forensics. proceedings multimedia systems conference goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. nips johnston nick vincent damien minnen david covell michele singh saurabh chinen troy hwang sung shor joel toderici george. improved lossy image compression priming spatially adaptive rates recurrent networks. arxiv preprint arxiv. ledig christian theis lucas husz´ar ferenc caballero jose cunningham andrew acosta alejandro aitken andrew tejani alykhan totz johannes wang zehan photo-realistic single image super-resolution using generative adversarial network. arxiv preprint arxiv. radford alec metz luke chintala soumith. unsupervised representation learning deep convolutional generative adversarial networks. arxiv preprint arxiv. thomee bart shamma david friedland gerald elizalde benjamin karl poland douglas borth damian li-jia. yfccm data multimedia research. communications toderici george o’malley sean hwang sung vincent damien minnen david baluja shumeet covell michele sukthankar rahul. variable rate image compression recurrent neural networks. arxiv preprint arxiv. johnston nick hwang sung minnen david shor joel covell michele. full resolution image compresarxiv preprint sion recurrent neural networks. arxiv. wang zhou bovik alan sheikh hamid simoncelli eero image quality assessment error visibility structural similarity. ieee transactions image processing figure used jpeg compress kodak dataset various quality levels. each jpeg recompress images plot resultant rate-distortion curve. evident image previously compressed jpeg better jpeg able recompress figure architecture discriminator used adversarial training procedure. ﬁrst module randomly swaps targets reconstructions. goal discriminator infer inputs real target reconstruction. accumulate scalar outputs along branches constructed along processing pipeline branched different depths. average attain ﬁnal value provided objective sigmoid function. multiscale architecture allows aggregating information across different scales. section main text discuss motivation architectural choices detail.", "year": 2017}