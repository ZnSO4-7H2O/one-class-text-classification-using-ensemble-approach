{"title": "Two-Stage Metric Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric with unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM.", "text": "alexandros kalousis department business informaticsuniversity applied scienceswestern switzerland department computer science university geneva switzerland paper present novel two-stage metric learning algorithm. ﬁrst learning instance probability distribution computing similarities ﬁxed anchor points. then deﬁne distance input data space fisher information distance associated statistical manifold. induces input data space family distance metric unique properties. unlike kernelized metric learning require similarity measure positive semi-deﬁnite. moreover also interpreted local metric learning algorithm well deﬁned distance approximation. evaluate performance number datasets. outperforms signiﬁcantly metric learning methods svm. distance measures play crucial role many machine learning tasks algorithms. standard distance metrics e.g. euclidean cannot address satisfactory manner multitude learning problems fact development metric learning methods learn problemspeciﬁc distance measure directly data learning distance metric global linear transformation called single metric learning approach distance computation equivalent applying learning instances learned linear transformation followed standard distance metric computation projected space. since discriminatory power input features might vary locally approach often ﬂexible enough well distance different regions. local metric learning addresses limitation learning neighborhood local metric local metrics vary smoothly feature space learning local metrics equivalent learning riemannian metric data manifold main challenge geodesic distance endowed riemannian metric often computationally expensive. practice approximated assuming geodesic curves formed straight lines local metric change along lines unfortunately approximation satisfy symmetric property therefore result nonmetric distance. propose general two-stage metric learning algorithm learn ﬂexible distance different types data manifolds e.g. euclidean probability simplex hypersphere etc. concretely ﬁrst instances onto statistical manifold similarity-based differential computes non-negative similarities number predeﬁned anchor points. deﬁne fisher information distance distance chosen since induces family riemannian distance metric enjoys interesting properties riemannian metric robust density variations original data space produced example different intrinsic variabilities learning instances different categories. distance learning metric hence robust density variation. riemannian distance metric largest distance discrimination manifold anchor points distance directions orthogonal manifold. distance metric remove effect locally irrelevant dimensions anchor point manifold figure detials. remainder section brieﬂy introduce necessary terminology concepts. details found monographs statistical manifold. denote n-dimensional smooth manifold. point exists least smooth coordinate chart deﬁnes coordinate system points open subset containing smooth coordinate coordinate deﬁned statistical manifold smooth manifold whose points probability distributions. given n-dimensional statistical manifold denote probability distribution coordinate coordinate random variable distribution taking values note that probability distributions share paper particularly interested ndimensional statistical manifold whose points ﬁnite discrete distributions denoted function global mahalanobis metric learned rkhs space. deﬁning distance input feature space mahalanobis distance rkhs space equivalent learning ﬂexible non-linear distance input space. however main limitation kernel matrix induced kernel function must positive semi-deﬁnite although non-psd kernel could transformed kernel kernel nevertheless cannot keep original similarity information. paper propose novel two-stage metric learning algorithm similarity-based fisher information metric learning ﬁrst maps instances data manifold ﬁnite discrete distributions computing similarities number predeﬁned anchor points data space. then fisher information distance statistical manifold used distance input feature space. induces family riemannian distance metric input data space important properties. first riemannian metric robust density variation original data space. without robustness objective function easily biased towards data regions density thus dominates learning objective function. second riemannian metric largest distance discrimination manifold anchor points distance directions orthogonal manifold. effect locally irrelevant dimensions anchor points removed. best knowledge ﬁrst metric learning algorithm important properties. sbfiml ﬂexible general; applied different types data spaces various non-negative similarity functions. comparing sbfiml require similarity measure form matrix. moreover sbfiml interpreted local metric learning algorithm. compared previous local metric learning algorithms produce non-metric distance distance approximation sbfiml well deﬁned distance function closed form expression. evaluate sbfiml number datasets. experimental results show outperforms statistically signiﬁcant manner metric learning methods svm. preliminaries given number learning instances d-dimensional vector instance vector associated class labels assume input feature space smooth manifold. different learning problems different types data manifolds possibly different dimensionality. commonly used manifold coordinate jacobian matrix function point since riemannian metric pullback metric general least metric. following lemma gives relation geodesic distances lemma pullback metric riemannian metric induced differential geodesic distance endowed geodesic distance endowed then holds limp→p proof lemma provided appendix. addition approximating directly assuming geodesic curve formed straight lines previous local metric learning algorithms lemma allows also approximate note that approximations asymptotic convergence result. present two-stage metric learning algorithm sbfiml. following ﬁrst present deﬁne similarity-based differential learn fisher information distance. similarity-based differential given number anchor points denote differentiable similarity function. component differentiable function output nonnegative similarity input instance anchor point based similarity function deﬁne similarity-based differential ﬁnite discrete distribution manifold simplicity denote probability mass outcome given order valid differential similarity function must family differential maps general applied space fisher information metric. fisher information metric riemannian metric deﬁned statistical manifolds endows distance probability distributions explicit form fisher information metric positive deﬁnite symmetric matrix element deﬁned integral replaced discrete. following lemma gives explicit form fisher information metric lemma statistical manifold fisher information metric coordinate properties fisher information metric. fisher information metric enjoys number interesting properties. first fisher information metric unique riemannian metric induced f-divergence measures kullback-leibler divergence divergence divergences converge fisher information distance probability distributions approaching other. another important property fisher information metric metric learning perspective distance endows approximated hellinger distance cosine distance f-divergence measures importantly statistical manifold ﬁnite discrete distributions e.g. cosine distance exactly equivalent fisher information distance pullback metric. smooth manifolds tpmn tangent space given differential riemannian metric differential induces pullback metric point deﬁned tpmn differential point maps tangent vectors tpmn tangent vectors given coordinate systems respectively deﬁned smooth coordinate maps respectively explicit form compare similarity-based equation similarity-based equation aspects namely representation robustness pullback metric analysis. representation robustness. compared representation induced similarity-based equation representation induced similarity-based equation robust density variations original data space i.e. density learning instances varies signiﬁcantly different regions. explained fact ﬁnite discrete distribution essentially representation neighborhood structure learning instance normalized scaling factor similarities learning instance anchor points. hence distance implied ﬁnite discrete distribution representation less sensitive density variations different data regions. important property. without robustness objective function based distances easily biased towards data regions density thus dominates learning objective function. example kind objective lmnn also later sbfiml learn fisher information distance. pullback metric analysis. also show approaches differ comparing pullback metrics induced similarity-based maps ﬁrst need specify riemannian metrics proximity space statistical manifold following work similarity-based learning euclidean metric proximity space statistical manifold fisher information metric deﬁned equation simplify analysis assume however note analysis generalized manifolds e.g. standard cartesian coordinate system points m-afﬁne coordinate system equation points pullback metric induced differential maps given following lemma. lemma cartesian coordinate form pullback metric euclidean metric induced differential equation non-negative differentiable similarity function deﬁned. ﬁnite discrete distribution representation learning instance intuitively seen encoding neighborhood structure deﬁned similarity function note that idea mapping instances onto statistical manifold previously studied manifold learning e.g. t-sne akin appropriate choice kernel function kernel-based method choice appropriate similarity function also crucial sbfiml. principle appropriate similarity function good match geometrical structure data manifold. example data lying probability simplex space i.e. similarity functions deﬁned either used. however similarity function appropriate exploits geometrical structure which contrast ignored similarity function anchor points deﬁned various ways. ideally anchor points similar given learning instances i.e. anchor points follow distribution learning instances. empirically directly training instances cluster centers latter established clustering algorithms. similar current practice kernel methods sbfiml anchors points training instances. similarity functions deﬁne similarity various ways. paper investigate types differentiable similarity functions. ﬁrst based gaussian function deﬁned norm. controls size neighborhood anchor point large values producing large neighborhoods. note different could different values; equal similarity function exactly gaussian kernel. second type similarity function look measures normalized angular similarity similarity function explained ﬁrst projecting points hypersphere applying angular similarity points hypersphere. result similarity function useful data approximately hypersphere. note similarity function also valid kernel function without centralization. difference intuitively compared difference local without centralization. therefore closer principle directions local anchor points. second since also easy show distance orthogonal directions afﬁne subspace spanned weighted anchor points ¯sizi. removes effect locally irrelevant dimensions anchor point manifold. show differences pullback metrics intuitively visualize equi-distance curves figure guassian similarity function euqation used deﬁne similarity maps equations shown figure pullback metric emphasizes distance along principle direction local anchor points pullback metric furthermore figure zero distance direction orthogonal manifold anchor points straight line anchor points therefore discriminative manifold anchor points. explore effect differences also experimentally compare approaches section results show learning fisher information distance outperforms signiﬁcant manner learning mahalanobis distance proximity space applying learning instances differential equation statistical manifold ready learn fisher information distance data. distance parametrization. discussed section fisher information distance exactly computed cosine distance gaussian similarity function. form pullback metrics depends explicit form similarity function study differences using gaussian similarity function kernel width equation ﬁrst show difference comparing largest eigenvectors directions metrics largest distance discrimination. largest eigenvectors complexity note that learning distance metric previously studied riemannian metric learning χ-lmnn χ-lmnn symmetric distance learned large margin idea similar problem sbfiml differs χlmnn uses cosine distance measure distance described section cosine distance exactly equivalent fisher information distance distance approximation. contrast sbfiml χ-lmnn work focuses unsupervised fisher information metric learning. importantly χ-lmnn applied problems input data sbfiml applied general data manifolds similaritybased differential map. finally note sbfiml also applied problems access pairwise instance similarity matrix since needs probability mass ﬁnite discrete distributions input. local metric learning view sbfiml. sbfiml also interpreted local metric learning algorithm. sbfiml deﬁnes local metric pullback metric fisher information metric induced following similarity-based parametric differential probability mass vector ﬁnite discrete distribution deﬁned equation sbfiml learns local metric learning parameters explicit form pullback metric computed according equation given pullback metric approximate geodesic distance assuming geodesic curves formed straight lines local metric learning methods would result nonmetric distance. however lemma allows approximate geodesic distance fisher information distance sbfiml follows latter approach. compared non-metric distance approximation distance well deﬁned distance function closed form expression. furthermore distance approximation asymptotic convergence result non-metric distance approximation. evaluate performance sbfiml datasets machine learning mldata repositories. details datasets reported ﬁrst column table datasets preprocessed standardizing input features. compare distribution parametrize fisher information distance apply probability mass vector linear transformation intuition that effect optimal linear transformation equivalent locating hidden anchor points data’s similarity representation transformed representation. thus parametric fisher information distance deﬁned size number hidden anchor points. speedup learning process practice often learn rank linear transformation matrix small added ensure still ﬁnite discrete distribution manifold learning. follow large margin metric learning approach deﬁne optimization problem learning parameter balances importance terms. unlike lmnn margin parameter added large margin triplet constraints following work since cosine distance linear ltl. large margin triplet constraints instance generated using same-class nearest neighbors different-class nearest neighbors space constraining distance instance different class neighbors larger class neighbors margin. objective function matrix learned minimizing hinge losses pairwise distances instance same-class nearest neighbors. optimization. since cosine distance deﬁned equation convex optimization problem convex. however constraints matrix linear solve problem using projected subgradient method. iteration main computation sub-gradient computation complexity number large margin triplet constraints. dimensions matrix. simplex projection operator matrix efﬁciently computed table mean standard deviation times -fold accuracy results datasets. superscripts next accuracies sbfiml indicate result student’s t-test sbmmlχ lmnn lmnn glml plml svm. denote respectively signiﬁcant loss difference sbfiml. bold entries dataset signiﬁcant difference best accuracy dataset. number parenthesis indicates score respective algorithm given dataset based pairwise comparisons student’s t-test. sbfiml three metric learning baseline methods lmnn glml plml former learn global mahalanobis metric input feature space rkhs space respectively last learn smooth local metrics addition also compare sbfiml similaritybased mahalanobis metric learning difference pullback metrics equation equation sbmml learns global mahalanobis metric proximity space similar sbfiml metric learned optimizing problem cosine distance replaced mahalanobis distance. constraints problem also removed. difference cosine distance used sbfiml distance used lmnn compare sbfiml lmnn. note that methods solve exactly optimization problem different distance computations. finally also compare sbfiml binary classiﬁcation problems multi-class svms multiclass classiﬁcation problems. multi-class svms oneagainst-all strategy determine class label. sbmml lmnn learn matrix thus computationally expensive datasets large number instances. speedup learning process similar sbfiml learn rank transformation matrix size methods sbmml lmnn sbfmil experiments. matrix sbmml initialized clipping identity matrix size similar manner lmnn sbfiml matrix initialized applying initialization matrix simplex projector ensures constraints problem satisﬁed. saul default value glml uses gaussian distribution model learning instances given class. hyper-parameters plml following sbfiml hyper-parameters following lmnn parameter select margin parameter using -fold inner cross validation selection appropriate similarity function crucial sbfiml. choose similarity function -fold inner angular similarity equation gaussian similarity equation examine types gaussian similarity. ﬁrst selected average pairwise distances. second anchor point separately; making entropy conditional distribution equal number training instances selected since lmnn sbfiml apply different distance parametrizations solve optimization problem parameters lmnn exactly sbfiml except margin parameter lmnn selected lmnn uses squared distance best similarity lmnn also selected using -fold inner similarity function sbfiml. akin sbfiml performance depends heavily selection kernel. select automatically best kernel -fold inner kernels chosen linear polynomial angular similarity equation gaussian kernels widths sbfiml average pairwise distances. addition also select margin parameter parameter selected sbmml constraints similarity function thus select similarity function -fold inner includes kernel similarity functions used sbfiml kml. select margin parameter sbmml methods except glml involve triplet constraints triplet constraints constructed using three same-class different-class nearest neighbors learning instance. finally rule evaluate performance different metric learning methods. estimate classiﬁcation accuracy used times fold statistical signiﬁcance differences tested using student’s t-test p-value order better understanding relative performance different algorithms given dataset used simple ranking schema algorithm assigned point found statistically signiﬁcantly better accuracy another algorithm points algorithms signiﬁcant difference zero points found signiﬁcantly worse results. table report accuracy results. sbfiml outperforms statistical signiﬁcant manner single metric learning method lmnn local metric learning methods glml plml seven eight datasets respectively. compare sbmml learn mahalanobis metric rkhs proximity space respectively signiﬁcantly better sbmml four datasets signiﬁcantly worse dataset. compared lmnn sbfiml outperforms χ-lmnn eight datasets statistically signiﬁcant better three never loses statistical signiﬁcant manner. finally compared sbfiml signiﬁcantly better datasets signiﬁcantly worse dataset. terms total score sbfiml achieves best predictive performance point followed which scores point χ-lmnn point. local metric learning method glml performs worst. potential explanation poor performance glml could gaussian distribution assumption appropriate datasets experimented with. formance difference sbfiml sbmml lmnn applied large datasets. speedup learning process anchor points randomly selected training instances. moreover parameter rank transformation matrix reduced number anchor points. kernel function similarity selected using -fold inner classiﬁcation accuracy isolet pendigits estimated default train test split three datasets used -fold cross-validation. statistical signiﬁcance difference tested mcnemar’s test p-value accuracy results reported table sbfiml achieves statistical signiﬁcant better accuracy sbmml datasets splice pendigits. compare lmnn statistical signiﬁcant better dataset pendigits. terms total score sbfiml achieves best score points followed lmnn. paper present two-stage metric learning algorithm sbfiml. ﬁrst maps learning instances onto statistical manifold similarity-based differential deﬁnes distance input data space fisher information distance statistical manifold. induces family distance metrics input data space important properties. first induced metrics robust density variations original data space second largest distance discrimination manifold anchor points. furthermore learning metric statistical manifold sbfiml learn distances different types input feature spaces. similarity-based used sbfiml natural ﬂexible; unlike need psd. addition sbfiml interpreted local metric learning method well deﬁned distance approximation. experimental results show outperforms statistical signiﬁcant manner metric learning methods svm. proof. coordinate under smooth coordinate coordinate smooth coordinate since approaches coordinate coordinate inﬁnitesimal small change approaching furthermore since differential function denote also differentiable. according taylor expansion coordinate under coordinate jacobian matrix function point remainder term linear approximation. finally according deﬁnidgγ) tion pullback metric limdθ→ hauberg sren freifeld oren black michael. geometric take metric learning. bartlett pereira f.c.n. burges c.j.c. bottou weinberger k.q. advances neural information processing systems honeine richard angular kernel machine learning hyperspectral data classiﬁcation. hyperspectral image signal processing evolution remote sensing workshop ieee kedem tyree stephen weinberger kilian lanckriet gert. non-linear metric learning. bartlett pereira f.c.n. burges c.j.c. bottou weinberger k.q. advances neural information processing systems s.m. abbott a.l. araman p.a. dimensionality reduction clustering statistical manicomputer vision pattern recognition folds. cvpr’. ieee conference ieee wang kalousis alexandros woznica adam. parametric local metric learning nearest neighbor classiﬁcation. bartlett pereira f.c.n. burges c.j.c. bottou weinberger k.q. advances neural information processing systems", "year": 2014}