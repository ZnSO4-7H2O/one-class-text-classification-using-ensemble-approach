{"title": "The Limitations of Deep Learning in Adversarial Settings", "tag": ["cs.CR", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.", "text": "adversarial sample input crafted cause deep learning algorithms misclassify. note adversarial samples created test time trained defender require alteration training process. figure shows examples adversarial samples taken validation experiments. shows image originally showing digit altered force classify another digit. adversarial samples created benign samples adding distortions exploiting imperfect generalization learned dnns ﬁnite training sets underlying linearity components used build dnns previous work explored properties could used craft adversarial samples simply techniques exploit gradients computed network training algorithms instead using gradients update network parameters would normally done gradients used update original input itself subsequently misclassiﬁed dnns. abstract—deep learning takes advantage large datasets computationally efﬁcient training algorithms outperform approaches various machine learning tasks. however imperfections training phase deep neural networks make vulnerable adversarial samples inputs crafted adversaries intent causing deep neural networks misclassify. work formalize space adversaries deep neural networks introduce novel class algorithms craft adversarial samples based precise understanding mapping inputs outputs dnns. application computer vision show algorithms reliably produce samples correctly classiﬁed human subjects misclassiﬁed speciﬁc targets adversarial success rate modifying average input features sample. evaluate vulnerability different sample classes adversarial perturbations deﬁning hardness measure. finally describe preliminary work outlining defenses adversarial samples deﬁning predictive measure distance benign input target classiﬁcation. large neural networks recast deep neural networks altered machine learning landscape outperforming approaches many tasks. made possible advances reduced computational complexity training instance deep learning take advantage large datasets achieve accuracy rates higher previous classiﬁcation techniques. short transforming computational processing complex data many domains vision speech recognition language processing ﬁnancial fraud detection recently malware detection increasing deep learning creating incentives adversaries manipulate dnns force misclassiﬁcation inputs. instance applications deep learning image classiﬁers distinguish inappropriate appropriate content text image classiﬁers differentiate spam non-spam email. adversary able craft misclassiﬁed inputs would proﬁt evading detection–indeed attacks occur today non-dl classiﬁcation systems physical domain consider driverless system uses identify trafﬁc signs slightly altering stop signs causes dnns misclassify them would stop thus subverting car’s safety. paper describe class algorithms adversarial sample creation feedforward formalize threat model space deep learning respect integrity output classiﬁcation. unlike previous approaches mentioned above compute direct mapping input output achieve explicit adversarial goal. furthermore approach alters fraction input features leading reduced perturbation source inputs. also enables adversaries apply heuristic searches perturbations leading input targeted misclassiﬁcations formally models multidimensional function feature vector output vector. construct adversarial sample benign sample adding perturbation vector solving following optimization problem adversarial sample desired adversarial output norm appropriate compare inputs. solving problem non-trivial properties dnns make non-linear non-convex thus craft adversarial samples constructing mapping input perturbations output variations. note research mentioned took opposite approach used output variations corresponding input perturbations. understanding changes made inputs affect dnn’s output stems evaluation forward derivative matrix introduce deﬁne jacobian function learned dnn. forward derivative used construct adversarial saliency maps indicating input features include perturbation order produce adversarial samples inducing certain behavior dnn. forward derivatives approaches much powerful gradient descent techniques used prior systems. applicable supervised unsupervised architectures allow adversaries generate information broad families adversarial samples. indeed adversarial saliency maps versatile tools based forward derivative designed adversarial goals mind giving greater control adversaries respect choice perturbations. work consider following questions formalize security adversarial settings what minimal knowledge required perform attacks vulnerable resistant samples identiﬁed? adversarial samples perceived humans?. adversarial sample generation algorithms validated using widely studied lenet architecture mnist dataset show input sample perturbed misclassiﬁed target class success perturbing average input features sample. computational costs sample generation modest; samples generated less second formalize space adversaries classiﬁcation dnns respect adversarial goal capabilities. here provide better understanding attacker capabilities constrain attack strategies goals. introduce class algorithms crafting adversarial samples solely using knowledge architecture. algorithms exploit forward derivatives inform learned behavior dnns build adversarial saliency maps enabling efﬁcient exploration adversarial-samples search space. validate algorithms using widely used computer vision dnn. deﬁne measure sample distortion source-to-target hardness explore defenses adversarial samples. conclude studying human perception distorted samples. taxonomy threat models deep learning classical threat models enumerate goals capabilities adversaries target domain section taxonimizes threat models deep learning systems positions several previous works respect strength modeled adversary. begin providing overview deep neural networks highlighting inputs outputs function. consider taxonomy presented figure deep neural networks large neural networks organized layers neurons corresponding successive representations input data. neuron individual computing unit transmitting neurons result application activation function input. neurons connected links different weights biases characterizing strength neuron pairs. weights biases viewed parameters used information storage. deﬁne network architecture include knowledge network topology neuron activation functions well weight bias values. weights biases determined training ﬁnding values minimize cost function evaluated training data network training traditionally done gradient descent using backpropagation deep learning partitioned categories depending whether dnns trained supervised unsupervised manner supervised training leads models unseen samples using function inferred labeled training data. contrary unsupervised training learns representations unlabeled training data resulting dnns used generate samples automate feature engineering acting pre-processing layer larger dnns. restrict problem learning multi-class classiﬁers supervised settings. dnns given input output class probability vector note work remains valid unsupervised-trained dnns leaves detailed study issue future work. targeted misclassiﬁcation produce inputs force output classiﬁcation speciﬁc target class. continuing example illustrated figure adversary would create speckles classiﬁed digit. source/target misclassiﬁcation force output classiﬁcation speciﬁc input speciﬁc target class. continuing example figure adversaries take existing image digit small number speckles classify resulting image another digit. scientiﬁc community recently started exploring adversarial deep learning. previous work machine learning techniques referenced later section vii. szegedy introduced system generates adversarial samples perturbing inputs creates source/target misclassiﬁcations perturbations made work focused computer vision application distinguishable humans example small carefully-crafted perturbations image vehicle resulted classifying ostrich. authors named modiﬁed input adversarial image generalized part broader deﬁnition adversarial samples. producing adversarial samples adversary’s goal generate inputs correctly classiﬁed humans classiﬁers misclassiﬁed targeted dnn. another example nguyen presented method producing images unrecognizable humans nonetheless labeled recognizable objects dnns instance demonstrated classify noise-ﬁlled image constructed using technique television high conﬁdence. named images produced method fooling images. here fooling image source class crafted solely perform targeted misclassiﬁcation attack. figure illustrates example shallow feedforward neural network. network input neurons hidden layer neurons single output neuron words simple multi-layer perceptron. input neurons take real values correspond network input feature vector hidden layer neurons logistic sigmoid function +e−x activation function. function frequently used neural networks continuous demonstrates linear-like behavior around saturates input goes neurons hidden layers apply sigmoid weighted input layer instance neuron computes wx+wx+b weights bias. similarly output neuron applies sigmoid function weighted output hidden layer weight bias values determined training. thus overall behavior network learned training modeled function threats deﬁned speciﬁc function protected/defended. case deep learning systems integrity classiﬁcation paramount importance. speciﬁcally adversary deep learning system seeks provide input results incorrect output classiﬁcation. nature incorrectness represents adversaries deﬁned information capabilities disposal. following describes range adversaries loosely organized decreasing adversarial strength note considers attack conducted test time tampering training procedure outside scope paper. training data network architecture adversary perfect knowledge neural network used classiﬁcation. attacker access training data functions algorithms used network training able extract knowledge dnn’s architecture includes number type layers activation functions neurons well weight bias matrices. also knows algorithm used train network including associated loss function strongest adversary analyze training data simulate deep neural network toto. network architecture adversary knowledge network architecture parameter values. instance corresponds adversary collect information layers activation functions used design neural network weights biases resulting training phase. gives adversary enough information simulate network. algorithms assume threat model show class algorithms generate adversarial samples supervised unsupervised feedforward dnns. training data adversary able collect surrogate dataset sampled distribution original dataset used train dnn. however attacker aware architecture used design neural network. thus typical attacks conducted model would likely include training commonly deployed deep learning architectures using surrogate dataset approximate model learned legitimate classiﬁer. oracle adversary ability neural network oracle. adversary obtain output classiﬁcations supplied inputs enables differential attacks adversary observe relationship changes inputs outputs adaptively craft adversarial samples. adversary parameterized number absolute rate-limited input/output trials perform. samples adversary ability collect pairs input output related neural network classiﬁer. however cannot modify inputs observe difference output. continue cryptanalysis analogy threat model would correspond known plaintext attack. pairs largely labeled output data intuition states would likely useful large quantities. section present general algorithm modifying samples yields adversarial output. later validate algorithm classiﬁer misclassify samples chosen target class. algorithm captures adversaries crafting samples setting corresponding upper right-hand corner figure show knowledge architecture weight parameters sufﬁcient derive adversarial samples acyclic feedforward dnns. requires evaluating dnn’s forward derivative order construct adversarial saliency identiﬁes input features relevant adversary’s goal. perturbing features identiﬁed quickly leads desired adversarial output instance misclassiﬁcation. although describe approach supervised neural networks used classiﬁers also applies unsupervised architectures. simple architecture introduced previously section illustrated figure dimensionality allows better understand underlying concepts behind algorithms. indeed show small input perturbations found using forward derivative induce large variations neural network output. assuming input biases null train network learn function desired output note non-integer inputs rounded closest integer thus instance using backpropagation samples corresponding case function train epochs using learning rate overall function learned neural network plotted figure input values horizontal axes represent input dimensions vertical axis represents network output corresponding recalling round inputs outputs network agrees boolean function adversarial sample rounding importantly forward derivative tells input regions unlikely yield adversarial samples thus immune adversarial manipulations. notice figure either input close forward derivative small. aligns intuition difﬁcult adversarial samples close tells adversary focus features corresponding larger forward derivative values given input constructing sample making search efﬁcient ultimately leading smaller overall distortions. takeaways example thereby small input variations lead extreme variations output neural network regions input domain conducive adversarial samples forward derivative reduces adversarial-sample search space. generalize approach feedforward using assumptions adversary model section iii-a. assumptions make architecture neurons form acyclic differentiable activation function. note last assumption limiting back-propagation algorithm imposes requirement. figure give example feedforward deep neural network architecture deﬁne notations used throughout remainder paper. importantly n-dimensional function learned training assigns output given m-dimensional input write number hidden layers. layers indexed index input layer corresponds hidden layers indexes output layer. algorithm shows process constructing adversarial samples. input algorithm takes benign sample target output acyclic feedforward maximum distortion parameter feature variation parameter returns adversarial sample proceeds three basic steps compute forward derivative construct saliency based derivative modify input feature imax process repeated network outputs maximum distortion reached. detail step. forward derivative deep neural network ﬁrst step compute forward derivative given sample introduced previously given essentially jacobian function corresponding neural network learned training. forward derivative computes gradients similar computed backpropagation important distinctions take derivative network directly rather adversarial sample desired adversarial output norm appropriate compare points input domain. informally adversary searching small perturbations input incur modiﬁcation output finding perturbations done using optimization techniques simple heuristics even brute force. however solutions hard implement deep neural networks non-convexity non-linearity instead propose systematic approach stemming forward derivative. deﬁne forward derivative jacobian matrix function learned neural network training. example output dimensional matrix therefore reduced vector components vector computable using adversary’s knowledge later show compute term efﬁciently. forward derivative example network illustrated figure plots gradient second component vertical axis plot horizontal axes. omit approximately symmetric inputs making ﬁrst component redundant purposes. plot makes easy visualize divide network’s possible outputs terms values assigned input feature left spike left. notice aligns figure gives information needed achieve adversarial goal input perturbations drive output closer desired value. consulting figure alongside example network conﬁrm intuition looking sample points. consider located near spike figure although differ small amount cause signiﬁcant change network’s output algorithm crafting adversarial samples benign sample target network output function learned network training maximum distortion change made features. algorithm applied speciﬁc algorithm input .|x|} ||δx|| cost function differentiate respect input features rather network parameters. consequence instead propagating gradients backwards choose approach propagate forward allows input components lead signiﬁcant changes network outputs. goal express terms constant values only. simplify expressions consider element forward derivative matrix deﬁned equation derivative output neuron according input dimension course results true matrix element. start ﬁrst hidden layer neural network. differentiate output ﬁrst hidden layer terms input components. recursively differentiate hidden layer terms previous neuron hidden output layer indexed connected previous layer using weights deﬁned vector wkp. deﬁning weight matrix accordingly deﬁne fully sparsely connected interlayers thus modeling variety architectures. similarly write bias neuron layer applying chain rule write series formulae ∂hk− formula according threat model terms precisely term computed known recursively. plugging results successive layers back equation expression component dnn’s forward derivative. hence forward derivative network computed input successively differentiating layers starting input layer output layer reached. later discuss methodology evaluation computability stateof-the-art architectures. notably forward derivative computed using symbolic differentiation. adversarial saliency maps extend saliency maps previously introduced visualization tools construct adversarial saliency maps. maps indicate input features adversary perturb order effect desired changes network output efﬁciently adversarial saliency maps deﬁned suit problemspeciﬁc adversarial goals. instance later study network used classiﬁer output probability vector across classes ﬁnal predicted class value corresponds component highest probability case saliency therefore based forward derivative gives adversary information needed cause neural network misclassify given sample. precisely adversary wants misclassify sample assigned target class label. probability target class given must increased probabilities classes decrease maxj adversary accomplish increasing input features using following saliency input feature. condition speciﬁed ﬁrst line rejects input components negative target derivative overall positive derivative classes. indeed positive order increase needs negative decrease stay constant feature increased. product second line allows consider forward derivative components together easily compare input features. summary high values correspond input features either increase target class decrease classes signiﬁcantly both. increasing features adversary eventually misclassiﬁes sample target class. saliency example shown figure possible deﬁne adversarial saliency maps using forward derivative quality large impact amount distortion algorithm introduces; study detail later. moving introduce additional acts counterpart given equation ﬁnding features adversary decrease achieve misclassiﬁcation. difference lies constraints placed forward derivative values location absolute value second line fig. saliency -dimensional input lenet architecture input dimensions arranged correspond image pixel alignment. large absolute values correspond features signiﬁcant impact output perturbed. iteration algorithm amount selected feature perturbed also problem-speciﬁc. discuss section parameter application computer vision. lastly maximum number iterations equivalent maximum distortion allowed sample speciﬁed parameter limits number features changed craft adversarial sample take positive integer value smaller number features. finding right value requires considering impact distortion humans’ perception adversarial samples much distortion might cause adversarial samples easily identiﬁed humans. formally described class algorithms crafting adversarial samples misclassiﬁed feedforward dnns using three tools forward derivative adversarial saliency maps crafting algorithm. apply tools used computer vision classiﬁcation task handwritten digit recognition. show algorithms successfully craft adversarial samples source class given target class application means digit perturbed misclassiﬁed digit. investigate based well-studied lenet architecture proven excellent classiﬁer handwritten digits recent architectures like alexnet googlenet heavily reliant convolutional layers introduced lenet architecture thus making lenet relevant validate approach. reason believe method perform well larger architectures. network input black white images handwritten digits ﬂattened vectors features feature corresponds pixel intensity taking normalized values input processed succession convolutional layer pooling layer repeated twice fully connected hidden layer output softmax layer output class probability vector class corresponds digit shown figure network labels input image class assigned maximum probability shown equation train network using mnist training dataset samples determine whether using theoretical framework introduced previous sections effectively craft adversarial samples misclassiﬁed dnn. instance image handwritten digit classiﬁed network label adversary wishes craft adversarial sample based image classiﬁed label source class target class ideally crafting process must smallest perturbation required construct adversarial sample perturbation pixel intensities input feature variations added order craft note perturbations introduced craft adversarial samples must remain indistinguishable humans. algorithm shows crafting algorithm used experiments implemented python based algorithm several details changed accommodate handwritten digit recognition problem. given network algorithm iteratively modiﬁes sample perturbing input features selected saliency_map. saliency constructed updated iteration algorithm using dnn’s forward derivative algorithm halts following conditions adversarial sample classiﬁed target class maximum number iterations max_iter reached feature search domain empty. crafting algorithm ﬁne-tuned three parameters versarial target class. maximum distortion expressed percentage corresponds maximum number pixels modiﬁed crafting adversarial sample thus sets maximum number iterations max_iter follows number pixels sample. saliency subroutine saliency_map generates deﬁning input features modiﬁed iteration. policies used generate saliency maps vary nature data handled considered well adversarial goals. provide subroutine example later algorithm feature variation iteration input features selected using saliency must modiﬁed. variation introduced features another parameter accordance saliency maps uses. problem ﬁnding good values parameters goal current evaluation discussed later section note human perception limiting factor limits acceptable maximum distortion feature variation introduced. show application framework different adversarial strategies. algorithm crafting adversarial samples lenet- benign image target network output function learned network training maximum distortion change made pixels. input .|x|} maxj maxj iter max_iter compute forward derivative saliency_map modify remove remove maxj iter ﬁrst strategy craft adversarial samples based increasing intensity pixels. achieve purpose consider samples handwritten digits mnist test digit class small subset samples illustrate techniques. scale evaluation entire dataset section fig. adversarial samples generated feeding crafting algorithm empty input. sample produced corresponds target class interestingly classes clearly recognize target digit. computed layers ensure probabilities leading extreme derivative values. reduces quality information neurons activated different inputs causes forward derivative loose accuracy generating saliency maps. better results achieved working last hidden layer also made neurons corresponding digit class justiﬁes enforcing constraints forward derivative. indeed output layer used computing forward derivative increasing algorithm able craft successful adversarial samples source-target class pairs. figure shows adversarial samples obtained well original samples used craft them. original samples found diagonal. sample column sample crafted image originally classiﬁed source class misclassiﬁed target class verify validity algorithms speciﬁcally adversarial saliency maps simple experiment. crafting algorithm empty input craft adversarial sample class different samples shown figure demonstrate adversarial saliency maps able identify input features relevant classiﬁcation class. instead increasing pixel intensities achieve adversarial targets second adversarial strategy decreases pixel intensities implementation identical exception adversarial saliency map. formula goal report whether reach adversarial target class given source class. instance given handwritten increase pixel intensities produce adversarial samples respectively classiﬁed classes pixel intensities changed increased discuss choice parameter section allow unlimited maximum distortion simply measure source-target class pairs whether adversarial sample produced not. adversarial saliency used crafting algorithm select pixel pairs increased application introduced general case classiﬁcation equation aims pairs pixels using following heuristic index target class left operand multiplication operation constrained positive right operand multiplication operation constrained negative. heuristic introduced previous section manuscript searches pairs pixels producing increase target class output reducing output classes simultaneously increased. pseudocode corresponding subroutine saliency_map given algorithm saliency considers pairs pixels individual pixels selecting pixels time strict pixels would meet heuristic search criteria described equation searching pairs pixels likely match condition pixels compensate minor pixel. let’s consider simple example target derivative classes derivatives equal target derivative equal classes derivatives equal pixels match saliency map’s criteria stated equation combined pair match saliency criteria deﬁned equation would also envision considering larger groups input features deﬁne saliency maps. however comes greater computational cost combinations need considered time group size increased. implementation algorithms compute forward derivative network using last hidden layer instead output probability layer. justiﬁed extreme variations introduced logistic regression previously showed feasibility crafting adversarial samples source-target class pairs seek measure whether crafting algorithm successfully handle large quantities distinct samples hand-written digits. design experiments evaluate whether legitimate samples mnist dataset exploited adversary produce adversarial samples. crafting algorithm three sets samples extracted three mnist training validation test subsets. samples craft adversarial samples classiﬁed target classes distinct original legitimate class. thus generate samples leading total adversarial samples. maximum distortion pixel intensities increased maximum distortion ﬁxed studying effect increasing success rate found adversarial samples could crafted distortion less observed success rate increase signiﬁcantly larger maximum distortions. parameter observing decreasing giving negative values increased number features modiﬁed whereas interested reducing number features altered crafting. also notice features normalized introduce variation always pixels maximum value justiﬁes algorithm remove modiﬁed pixels search space iteration. impact performance beneﬁcial reduce size feature search space iteration. words algorithm performs best-ﬁrst heuristic search without backtracking. measure success rate distortion adversarial samples three sets samples. success rate deﬁned percentage adversarial samples successfully classiﬁed adversarial target class. distortion deﬁned percentage pixels modiﬁed legitimate sample obtain adversarial sample. words percentage input features modiﬁed order obtain adversarial samples. compute average distortion values taking account samples second taking account successful samples write figure presents results three sets original samples extracted. results consistent across sets. average success rate average distortion adversarial samples average distortion successful adversarial samples means average number pixels modiﬁed craft successful adversarial sample pixels. ﬁrst distortion ﬁgure higher includes unsuccessful samples crafting algorithm used maximum distortion unable induce misclassiﬁcation. fig. adversarial samples obtained decreasing pixel intensities. original samples mnist dataset found diagonal whereas adversarial samples non-diagonal elements. samples organized columns corresponding class previously written equation constraints different left operand multiplication operation constrained negative right operand positive. heuristic also introduced previous section paper searches pairs pixels producing increase target class output reducing output classes simultaneously decreased. algorithm able craft successful adversarial samples source-target class pairs. figure shows adversarial samples obtained well original samples used craft them. observation made distortion introduced reducing pixel intensities seems harder detect human eye. address human perception aspect study later section experimental setup answer following questions exploit sample? identify samples vulnerable others? humans perceive adversarial samples compared dnns?. primary result adversarial samples crafted reliably validation problem success rate modifying samples average deﬁne hardness measure identify sample classes easier exploit others. measure necessary designing robust defenses. also found humans cannot perceive perturbation introduced craft adversarial samples misclassiﬁed still correctly classify adversarial samples crafted distortion smaller also studied crafting adversarial samples using decreasing saliency map. found success rate lower average distortion slightly lower. again decreasing pixel intensities less successful producing desired adversarial behavior increasing pixel intensities. intuitively understood removing pixels reduces information entropy thus making harder dnns extract information required classify sample. greater absolute values intensity variations conﬁdently misclassiﬁed dnn. looking previous experiment adversarial samples successfully crafted. suggests samples harder exploit others. furthermore distortion ﬁgures reported averaged adversarial samples produced samples require distortion misclassiﬁed. thus study hardness different samples order quantify phenomena. identify source-target class pairs easiest exploit well similarities distinct source-target class pairs. class pair pair source class target class hardness metric allows ground defense mechanisms. experiment construct deeper understanding crafting algorithm’ success rate average distortion different source-target class pairs. adversarial samples crafted previous experiments samples mnist test set. break success rate reported figure source-target class pairs. allows know given source class many samples class successfully misclassiﬁed target classes. figure draw success rate matrix indicating pairs successful. darker shades correspond higher success rates. rows correspond success rate source class columns correspond success rate target class. reads matrix row-wise perceived classes hard start with classes easy start with. similarly reading matrix column-wise observe classes hard make classes easy make. figure report average distortion successful samples source-target class pair thus identifying class pairs requiring distortion successfully craft adversarial samples. interestingly classes requiring lower distortions correspond classes higher success rates previous matrix. instance column corresponding class associated highest distortions column least success rates previous matrix. indeed higher average distortion class pair likely samples class pair reach maximum distortion thus produce unsuccessful adversarial samples. better understand class pairs harder exploit tracked evolution class probabilities crafting process. observed distortion required leave source class higher class pairs high distortions whereas distortion required reach target class source class left remained similar. correlates fact source classes conﬁdently classiﬁed others. hardness measure results indicating sourcetarget class pairs easy others lead question existence measure quantifying distance classes. relevant defender seeking identify classes vulnerable adversaries. name measure hardness target class relatively given source class. normalizes average distortion class pair relatively success rate average distortion samples corresponding success rate practice quantities computed ﬁnite number samples ﬁxing maximum distortion parameter values crafting algorithm ..k. maximum distortions gives series pairs ..k. thus practical formula used compute hardness sourcedestination class pair derived trapezoidal rule computed hardness values classes using maximum distortion values algorithm. average distortions success rates averaged adversarial samples maximum distortion value figure shows hardness values pairs {..}. reader observe matrix shape similar average distortion matrix plotted figure however hardness measure accurate plotted using series maximum distortions. adversarial distance measure introduced lays ground towards ﬁnding defenses adversarial samples. indeed hardness measure predictive instead computed adversarial crafting defender could identify vulnerable inputs. furthermore predictive measure applicable single sample would allow defender evaluate vulnerability speciﬁc samples well class pairs. investigated several complex estimators including convolutional transformations forward derivative hessian matrices. however found simply using formulae derived intuition behind adversarial saliency maps gave enough accuracy predicting hardness samples experimental setup. indicator function event nutshell normalized number non-zero elements adversarial saliency computed ﬁrst crafting iteration algorithm closer adversarial distance likely sample going harder misclassify target class figure conﬁrms formulae empirically well-founded. illustrates value adversarial distance averaged source-destination class pairs making easy compare average value hardness matrix computed previously crafting samples. compute slightly altered equation pairs features reﬂecting observations made validation process. samples considered sufﬁciently large represent input domain network. good approximation robustness computed training dataset. note operator used replaced relevant operators like statistical expectation. study various operators left future work. recall adversarial samples must misclassiﬁed target class deep neural networks also visually appear source class humans. evaluate property experiment using human participants mechanical turk online service. presented three original adversarially altered samples mnist dataset human participants. paraphrase participants asked sample sample numeric digit?’ digit it?’. questions designed determine distortion intensity rates effected human perception samples. ﬁrst experiment designed identify baseline perception rate input data. participants presented unaltered samples randomly picked original mnist data set. respondents identiﬁed digits classiﬁed digits correctly samples. shown figure second experiments attempted evaluate amount distortion impacts human perception. here participants presented total samples varying levels distortion experiments showed threshold participants able identify samples digits correctly classify slightly less accurately unaltered samples. classiﬁcation rate dropped dramatically distortion rates threshold. ﬁnal experiments evaluate impact intensity variations perception shown figure participants accurate identifying samples digits classifying correctly higher absolute intensities speciﬁc digit classiﬁcation decreased slightly identiﬁcation digits largely unchanged. preliminary experiments conﬁrm overwhelming number generated samples retain human recognizability. note generate samples less distortion threshold almost input data produce adversarial samples humans mis-interpret—thus meeting adversarial goal. furthermore altering feature distortion intensity provides even better results humans classiﬁed sample data essentially rates original sample data. introduced class algorithms systematically craft adversarial samples misclassiﬁed adversary possesses knowledge architecture. although focused work techniques used context classiﬁcation trained supervised methods approach also applicable unsupervised architectures. instead achieving given target class adversary achieves target output output space complex might harder impossible match case equation would need relaxed acceptable distance network output adversarial target thus remaining assumption made paper dnns feedforward. words consider recurrent neural networks cycles architecture forward derivative must adapted accommodate networks. results reducing distortion—the number features altered—to craft adversarial samples compared previous work. believe makes adversarial crafting much easier input domains like malware executables easy perturb images distortion reduction comes performance cost. indeed elaborate accurate saliency formulae expensive compute attacker. would like emphasize method’s high success rate improved adversaries interested crafting limited number samples. indeed lower distortion particular sample adversary adversarial saliency maps ﬁne-tune perturbation introduced. hand adversary wants craft large amounts adversarial samples performance important. evaluation balanced factors craft adversarial samples less second. algorithm implementation concerned computationally expensive steps matrix manipulations required construct adversarial saliency maps forward derivative matrix. complexity dependent number input features. matrix operations made efﬁcient notably making better gpu-accelerated computations. efforts represent ﬁrst meaningful step towards mitigating adversarial samples hardness adversarial distance metrics bases defense mechanisms. although designing defenses outside scope paper outline classes defenses adversarial sample detection improvements robustness. developing techniques adversarial sample detection reactive solution. experimental process noticed adversarial samples instance detected evaluating regularity samples. speciﬁcally application example squared difference pair neighboring pixels always higher adversarial samples benign samples. however priori reason assume technique reliably detect adversarial samples different settings extending approach avenue future work. another approach proposed unsuccessful stacking denoising auto-encoder used detection original adversary produce adversarial samples. second class solutions seeks improve training return increase robustness dnns. interestingly problem adversarial samples closely linked training. work generative adversarial networks showed player game dnns lead generation samples training help augment training datasets. furthermore adding adversarial samples training like regularizer also observed experiments training adversarial samples makes crafting additional adversarial samples harder. indeed adding adversarial samples original mnist training dataset trained instance dnn. algorithms newly trained network crafted adversarial samples. preliminary analysis adversarial samples crafted showed success rate reduced average distortion increased suggesting training adversarial samples make dnns robust. security machine learning active research topic within security machine learning communities. broad taxonomy attacks required adversarial capabilties discussed along considerations building defense mechanisms. biggio studied classiﬁers adversarial settings outlined framework securing however work consider dnns rather techniques used binary classiﬁcation like logistic regression support vector machines. generally speaking attacks machine learning separated categories depending whether executed training test time prior work adversarial sample crafting dnns derived simple technique corresponding architecture training tools threat model based backpropagation procedure used network training approach creates adversarial samples deﬁning optimization problem based dnn’s cost function. words instead computing gradients update weights computes gradients update input misclassiﬁed target class dnn. alternative approach proposed paper identify input regions relevant classiﬁcation dnn. accomplished computing saliency given input described simonyan case dnns handling images extended concept create adversarial saliency maps highlighting regions input need perturbed order accomplish adversarial goal. previous work yosinki investigated features transferable deep neural networks szegedy showed adversarial samples indeed misclassiﬁed across models report adversarial sample generated given neural network architecture also likely misclassiﬁed neural networks designed differently explains attack successful. however effectiveness kind attack depends quality size surrogate dataset collected adversary adequateness adversarial network used craft adversarial samples. broadly speaking paper explored adversarial behavior deep learning systems. addition exploring goals capabilities adversaries introduced class algorithms craft adversarial samples based computing forward derivatives. technique allows adversary knowledge network architecture construct adversarial saliency maps identify features input signiﬁcantly impact output classiﬁcation. algorithms reliably produce samples correctly classiﬁed human subjects misclassiﬁed speciﬁc targets adversarial success rate modifying average input features sample. improving training phase. detection adversarial samples remains open problem. interestingly universal approximation theorem formulated hornik states hidden layer sufﬁcient represent arbitrarily accurately function thus intuitively conceive improving training phase resisting adversarial samples. future work plan address limitations trained unsupervised manner well cyclical recurrent neural networks also models taxonomy researched leaves room investigation various adversarial settings. authors would like warmly thank damien octeau aline papernot insightful discussions work. research sponsored army research laboratory accomplished cooperative agreement number wnf--- views conclusions contained document authors interpreted representing ofﬁcial policies either expressed implied army research laboratory u.s. government. u.s. government authorized reproduce distribute reprints government purposes notwithstanding copyright notation bergstra breuleux bastien lamblin pascanu desjardins turian warde-farley bengio. theano math expression compiler. proceedings python scientiﬁc computing conference volume page austin biggio corona maiorca nelson ˇsrndi´c laskov giacinto roli. evasion attacks machine learning test time. machine learning knowledge discovery databases pages springer biggio fumera roli. pattern recognition systems attack design issues research challenges. international journal pattern recognition artiﬁcial intelligence biggio fumera roli. security evaluation pattern classiﬁers attack. knowledge data engineering ieee transactions biggio rieck ariu wressnegger corona giacinto roli. poisoning behavioral malware clustering. proceedings workshop artiﬁcial intelligent security workshop pages collobert weston. uniﬁed architecture natural language processing deep neural networks task learning. proceedings international conference machine learning pages dahl stokes deng large-scale malware classiﬁcation using random projections neural networks. acoustics speech signal processing ieee international conference pages ieee fogla lee. evading network anomaly detection systems formal reasoning practical techniques. proceedings conference computer communications security pages goodfellow shlens szegedy. explaining harnessproceedings international adversarial examples. conference learning representations. computational biological learning society rigazio. towards deep neural network architectures robust adversarial examples. proceedings international conference learning representations. computational biological learning society knorr. paypal beats guys machine learning. http//www.infoworld.com/article//machine-learning/howpaypal-reduces-fraud-with-machine-learning.html lecun cortes. mnist database handwritten digits lisa lab. http//deeplearning.net/tutorial/lenet.html murphy. machine learning probabilistic perspective. nguyen yosinski clune. deep neural networks easily senior beaufays. long short-term memory recurrent neural network architectures large scale acoustic modeling. proceedings annual conference international speech communication association szegedy zaremba sutskever bruna erhan goodfellow fergus. intriguing properties neural networks. proceedings international conference learning representations. computational biological learning society taigman yang deepface closing human-level ieee conference computer performance face veriﬁcation. vision pattern recognition pages ieee yosinski clune bengio lipson. transferable features deep neural networks? advances neural information processing systems pages train deep neural network theano python package designed simplify largescale scientiﬁc computing. theano allows efﬁciently implement network architecture training backpropagation forward derivative computation. conﬁgure theano make computations ﬂoat precision accelerated using graphics processors. indeed experiments facilitated using acceleration machine equipped xeon processor nvidia tesla graphics processor. deep neural network makes simpliﬁcations suggested theano documentation original lenet- architecture. nevertheless trained batches samples taken mnist dataset learning parameter epochs learned network parameters exhibits accuracy rate mnist training accuracy rate mnist test comparable state-of-the-art accuracies.", "year": 2015}