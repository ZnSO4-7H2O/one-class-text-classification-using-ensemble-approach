{"title": "Group-sparse Matrix Recovery", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We apply the OSCAR (octagonal selection and clustering algorithms for regression) in recovering group-sparse matrices (two-dimensional---2D---arrays) from compressive measurements. We propose a 2D version of OSCAR (2OSCAR) consisting of the $\\ell_1$ norm and the pair-wise $\\ell_{\\infty}$ norm, which is convex but non-differentiable. We show that the proximity operator of 2OSCAR can be computed based on that of OSCAR. The 2OSCAR problem can thus be efficiently solved by state-of-the-art proximal splitting algorithms. Experiments on group-sparse 2D array recovery show that 2OSCAR regularization solved by the SpaRSA algorithm is the fastest choice, while the PADMM algorithm (with debiasing) yields the most accurate results.", "text": "rm×n also known sensing matrix observed vector rm×d additive noise. comparing matrix interest always assumed sparse also particular sparse structure. instance multiple measurement vector model unknown source matrix sparse; group lasso coefﬁcient matrix also enforced sparse; multi-task learning task parameter matrix usually assumed or/and column sparse. paper pursue general sparsity patterns arrangement group nonzeros limited rows and/or columns include row/columns segments blocks groups connected non-zero elements. addressing question whether available regularizers able promote kind group sparsity ﬁrst brieﬂy review existing group-sparsityinducing regularizers. recent years much attention devoted sparsity solutions also structure sparsity words number non-zeros solutions also non-zeros located interest. research direction lead concept group/block sparsity general structured sparsity patterns classical model group sparsity group lasso which making information original lasso able simultaneously encourage sparsity group sparsity. addition sparse group lasso approach proposed regularizer consists term plus group lasso regularizer thus unlike group lasso selects groups also individual variables within group. also observed real-world problems makes sense encourage solution sparse also several components sharing similar values. formalize goal several generic models proposed elastic fused lasso octagonal shrinkage clustering algorithm regression apply oscar recovering group-sparse matrices compressive measurements. propose version oscar consisting norm pair-wise norm convex non-differentiable. show proximity operator oscar computed based oscar. oscar problem thus efﬁciently solved state-of-the-art proximal splitting algorithms. experiments group-sparse array recovery show oscar regularization solved sparsa algorithm fastest choice padmm algorithm yields accurate results. rm×n known sensing matrix rn×d original unknown matrix/d-array rm×d observed data rm×d denotes additive noise. many cases interest making ill-posed problem addressed using form regularization injects prior knowledge unknown classical regularization formulations seek solutions problems form proximal splitting algorithms fista twist sparsa admm padmm investigated. naturely build relationship oscar oscar address oscar regularization problems brieﬂy review elements convex analysis used below. real hilbert space inner product norm function class lower semi-continuous convex proper functions proximity operator deﬁned term seeks data-ﬁdelity regularizer φoscar consists term pair-wise term pairs total) encouraging equality pair elements |xi| |xj|. thus φoscar promotes sparsity grouping. parameters nonnegative constants controlling relative weights terms. becomes lasso φoscar becomes pair-wise regularizer. note that choice φoscar convex ball octagonal case. case vertices octagon divided categories four sparsity-inducing vertices four equality-inducing vertices fig. depicts data-ﬁdelity term φoscar illustrating possible effects. level curves several regularizers mentioned previous paragraph shown fig. ﬁgure illustrates models promote variable grouping firstly regularizer elastic consists term term thus simultaneously promoting sparsity group-sparsity former comes sparsity-inducing corners latter strictly convex edges creates grouping effect similar quadratic/ridge regularizer. secondly regularizer fused lasso composed term total variation term encourages successive variables similar making able promote sparsity smoothness. thirdly oscar regularizer constituted term pair-wise term promotes equality pair variables. recent variants group-sparsity regularizers weighted fused lasso presented pair-wise fused lasso uses pair-wise term oscar extends fused lasso cases variables natural ordering. novel graphguided fused lasso proposed grouping structure modeled graph. bayesian version elastic developed finally adaptive grouping pursuit method proposed underlying regularizer neither sparsity-promoting convex. fused lasso elastic oscar regularizers ability promote sparsity variable grouping. however pointed oscar outperforms models terms grouping. moreover fused lasso suitable group according magnitude grouping ability convex edges elastic inferior oscar. thus paper focus oscar regularizer solve problems group-sparse matrix recovery. paper propose two-dimensional version oscar group-sparse matrix recovery. solving oscar regularization problems addressed previous work which state-of-thesolve investigate state-of-the-art proximal splitting algorithms fista twist sparsa admm padmm limitation space next detail sparsa since experimentally shown fastest one. however report experimental results aforementioned algorithms. well known solutions obtained oscar attenuated/biased magnitude. thus common practice apply debiasing postprocessing step; i.e. solutions obtained sparsa algorithm provides structure/support estimate debiasing step recovers magnitudes solutions. debiasing method used sparsa also adopted paper. speciﬁcally debiasing phase solves fig. illustration term φoscar case. example least square solution high correlation contour likely equality-inducing vertex whereas correlation contour prefers sparsity-inducing vertex. certain order variables; compared elastic stronger equality-inducing ability. features make oscar convenient regularizer many applications. fundamental building block using oscar proximity operator proxφoscar obtained exactly approximately grouping proximity operator approximate proximity operator proposed respectively. address matrix inverse problem propose elapsed time consider experiments recovery matrix different styles groups blocks lines curved groups consisting positive negative elements. observed matrix generated variance noise sensing matrix matrix components sampled standard normal distribution. nonzeros original matrix values arbitrarily chosen {−−− algorithms mentioned without debiasing. stopping condition /xk+ represents estimate thek-th iteration. parameters hand-tuned case best improvement mae. recovered matrices shown fig. quantitative results reported table conclude fig. table oscar criterion solved proximal splitting algorithms debiasing able accurately recover group-sparse matrices. among algorithms sparsa fastest padmm obtains accurate solutions. proposed solved state-of-the-art proximal spiting algorithms fista twist sparsa admm padmm without debaising. experiments group-sparse matrix recovery show oscar regularizer solved sparsa algorithm fastest convergence padmm leads accurate estimates. s.f. cotter b.d. engan kreutzdelgado sparse solutions linear inverse problems multiple measurement vectors ieee trans. signal processing vol. boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers foundations trends machine learning vol.", "year": 2014}