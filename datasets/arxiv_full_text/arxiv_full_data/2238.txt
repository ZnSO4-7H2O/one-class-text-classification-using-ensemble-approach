{"title": "Mixed Precision Training", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.", "text": "paulius micikevicius∗ jonah alben david garcia boris ginsburg michael houston oleksii kuchaiev ganesh venkatesh nvidia {pauliusm alben dagarcia bginsburg mhouston okuchaiev gavenkatesh skyw}nvidia.com increasing size neural network typically improves accuracy also increases memory compute requirements training model. introduce methodology training deep neural networks using half-precision ﬂoating point numbers without losing model accuracy modify hyperparameters. nearly halves memory requirements recent gpus speeds arithmetic. weights activations gradients stored ieee halfprecision format. since format narrower range single-precision propose three techniques preventing loss critical information. firstly recommend maintaining single-precision copy weights accumulates gradients optimizer step secondly propose loss-scaling preserve gradient values small magnitudes. thirdly half-precision arithmetic accumulates single-precision outputs converted halfprecision storing memory. demonstrate proposed methodology works across wide variety tasks modern large scale model architectures trained large datasets. deep learning enabled progress many different applications ranging image recognition language modeling machine translation speech recognition trends critical results increasingly large training data sets increasingly complex models. example neural network used hannun million parameters grew approximately million bidirectional rnns million latest forward gated recurrent unit models amodei larger models usually require compute memory resources train. requirements lowered using reduced precision representation arithmetic. performance program including neural network training inference limited three factors arithmetic bandwidth memory bandwidth latency. reduced precision addresses limiters. memory bandwidth pressure lowered using fewer bits store number values. arithmetic time also lowered processors offer higher throughput reduced precision math. example half-precision math throughput recent gpus higher single-precision. addition speed improvements reduced precision formats also reduce amount memory required training. modern deep learning training systems single-precision format. paper address training reduced precision maintaining model accuracy. speciﬁcally train various neural networks using ieee half-precision format since format narrower dynamic range introduce three techniques prevent model accuracy loss maintaining master copy weights loss-scaling minimizes gradient values becoming zeros arithmetic accumulation using techniques demonstrate wide variety network architectures applications trained match accuracy training. experimental results include convolutional recurrent network architectures trained classiﬁcation regression generative tasks. applications include image classiﬁcation image generation object detection language modeling machine translation speech recognition. proposed methodology requires changes models training hyper-parameters. number publications training convolutional neural networks reduced precision. courbariaux proposed training binary weights tensors arithmetic full precision. hubara extended work also binarize activations gradients stored computed single precision. hubara considered quantization weights activations bits gradients real numbers. rastegari binarize tensors including gradients. however approaches lead non-trivial loss accuracy larger models trained ilsvrc classiﬁcation task zhou quantize weights activations gradients different counts improve result accuracy. still incurs accuracy loss requires search width conﬁgurations network impractical larger models. mishra improve top- accuracy achieved prior weight activation quantizations doubling tripling width layers popular cnns. however gradients still computed stored single precision quantized model accuracy lower widened baseline. gupta demonstrate ﬁxed point representation used train cnns mnist cifar- datasets without accuracy loss. clear approach would work larger cnns trained large datasets whether would work recurrent neural networks also several proposals quantize training. train quantized variants long short term memory cells fewer bits weights activations albeit small loss accuracy. clear whether results hold larger networks needed larger datasets hubara propose another approach quantize rnns without altering structure. another approach quantize rnns proposed evaluate binary ternary exponential quantization weights various different models trained language modelling speech recognition. approaches leave gradients unmodiﬁed singleprecision therefore computation cost back propagation unchanged. techniques proposed paper different approaches three aspects. first tensors arithmetic forward backward passes reduced precision case. second hyper-parameters adjusted. lastly models trained techniques incur accuracy loss compared single-precision baselines. demonstrate technique works across variety applications using state-of-the-art models trained large scale datasets. introduce techniques training still matching model accuracy training session single-precision master weights updates loss-scaling accumulating products results training techniques presented section mixed precision training weights activations gradients stored order match accuracy networks master copy weights maintained updated weight gradient optimizer step. iteration copy master weights need master weights universal possible reasons number networks require explanation updates become small represented value whose magnitude smaller becomes zero figure approximately weight gradient values exponents smaller small valued gradients would become zero optimizer multiplied learning rate adversely affect model accuracy. using single-precision copy updates allows overcome problem recover accuracy. another explanation ratio weight value weight update large. case even though weight update representable could still become zero addition operation right-shifts align binary point weight. happen magnitude normalized weight value least times larger weight update. since bits mantissa implicit must right-shifted positions potentially create zero cases ratio larger implicit would right-shifted positions. cause weight update become zero cannot recovered. even larger ratio result effect de-normalized numbers. again effect counteracted computing update illustrate need master copy weights mandarin speech model trained dataset comprising approximately hours speech data epochs. shown match training results updating master copy weights forward backward passes updating weights results relative accuracy loss. even though maintaining additional copy weights increases memory requirements weights compared single precision training impact overall memory usage much smaller. training memory consumption dominated activations larger batch sizes activations layer saved reuse back-propagation pass. since activations also stored half-precision format overall memory consumption training deep neural networks roughly halved. loss scaling exponent bias centers range normalized value exponents gradient values practice tend dominated small magnitudes example consider figure showing histogram activation gradient values collected across layers training multibox detector network note much representable range left unused many values minimum representable range became zeros. scaling gradients shift occupy representable range preserve values otherwise lost zeros. particular network diverges gradients scaled scaling factor sufﬁcient match accuracy achieved training. suggests activation figure figure shows results three experiemnts; baseline pseudo master copy pseudo without master copy. figure shows histogram exponents weight gradients mandarin speech recognition training weights. gradients sampled every iterations training layers model. figure histogram activation gradient values training multibox network. note bins x-axis cover varying ranges there’s separate zeros. example values range values range values zero. gradient values magnitude irrelevant training model values range important preserve. efﬁcient shift gradient values fp-representable range scale loss value computed forward pass prior starting back-propagation. chain rule back-propagation ensures gradient values scaled amount. requires extra operations back-propagation keeps relevant gradient values becoming zeros. weight gradients must unscaled weight update maintain update magnitudes training. simplest perform unscaling right backward pass gradient clipping gradient-related computations ensuring hyper-parameters adjusted. several options choose loss scaling factor. simplest pick constant scaling factor. trained variety networks scaling factors ranging constant scaling factor chosen empirically gradient statistics available directly choosing factor product maximum absolute gradient value downside choosing large scaling factor long cause overﬂow back-propagation overﬂows result inﬁnities nans weight gradients irreversibly damage weights update. note overﬂows efﬁciently detected inspecting computed weight gradients example weight gradient values unscaled. option skip weight update overﬂow detected simply move next iteration. large neural network arithmetic falls three categories vector dot-products reductions point-wise operations. categories beneﬁt different treatment comes reduced precision arithmetic. maintain model accuracy found networks require vector dot-product accumulates partial products value converted writing memory. without accumulation models match accuracy baseline models. whereas previous gpus supported multiplyadd operation nvidia volta gpus introduce tensor cores multiply input matrices accumulate products either outputs large reductions carried reductions mostly come batch-normalization layers accumulating statistics softmax layers. layer types implementations still read write tensors memory performing arithmetic slow training process since layers memory-bandwidth limited sensitive arithmetic speed. point-wise operations non-linearities element-wise matrix products memorybandwidth limited. since arithmetic precision impact speed operations either math used. mixed precision used storage arithmetic. weights activations gradients stored using master copy weights used updates. loss-scaling used applications. experiments arithmetic used tensor core operations accumulation convolutions fully-connected layers matrix multiplies recurrent layers. baseline experiments conducted nvidia’s maxwell pascal gpu. mixed precision experiments conducted volta accumulates products mixed precision speech recognition experiments conducted using maxwell gpus using storage only. setup allows emulate tensorcore operations non-volta hardware. number networks trained mode conﬁrm resulting model accuracies equivalent training volta gpus. intuitive since arithmetic accumulating products converting result memory write. trained several cnns ilsvrc classiﬁcation task using mixed precision alexnet vgg-d googlenet inception inception pre-activation resnet-. cases able match top- accuracy baseline training session using identical hyper-parameters. networks trained using caffe framework modiﬁed volta tensorops except resnet used pytorch training schedules used public repositories available top- accuracy ilsvrc validation shown table baseline accuracy cases different published results single-crop testing simpler data augmentation. data augmentation caffe included random horizontal ﬂipping random cropping images resnet training pytorch used full augmentation training script pytorch vision repository. loss-scaling technique required successful mixed precision training networks. tensors forward backward passes master copy weights updated outlined section object detection regression task bounding coordinate values predicted network object detectors also classiﬁcation component probabilities object type predicted bounding box. trained popular detection approaches faster-rcnn multibox-ssd detectors used vgg- network backbone. models training scripts public repositories mean average precision computed pascal test set. fasterrcnn trained training whereas trained union data reason behind baseline difference table seen table detector failed train without loss-scaling. losing small gradient values zeros described section poor weights learned training diverges. described section loss-scaling factor recovers relevant gradient values mixed-precision training matches map. explore mixed precision training speech data using deepspeech model english mandarin datasets. model used training english dataset consists convolution layers three recurrent layers cells convolution layer connectionist temporal classiﬁcation cost layer approximately million parameters. model trained internal dataset consisting hours english speech. mandarin model similar architecture total million parameters. mandarin model trained hours internal training set. models baseline pseudo experiments. models trained epochs using nesterov stochastic gradient descent hyper-parameters learning rate annealing schedule momentum baseline pseudo experiments. table shows results experiments independent test sets. table character error rate using mixed precision training speech recognition. english results reported test set. mandarin results reported internal test set. similar classiﬁcation detection networks mixed precision training works well recurrent neural networks trained large scale speech datasets. speech models largest models trained using technique. also number time-steps involved training speech model unusually large compared applications using recurrent layers. shown table pseudo results roughly better baseline. suggests half-precision storage format regularizer training. language translation trained several variants model tensorflow tutorial english french translation model used word-vocabularies entries english french respectively. networks trained layers encoder decoder each. cases layer consisted lstm cells. optimizer used train dataset. noticeable variation accuracy different training sessions settings. example three curves figure shows -layer model. mixed-precision loss-scaling matched results loss-scaling resulted slight degradation results. -layer model exhibited training behavior. trained english language model designated biglstm billion word dataset. model consists layers lstm cells projection -dimensional embedding. model trained epochs using adagrad optimizer. vocabulary size words. training sampled softmax layer negative samples. batch size aggregated gpus match perplexity training network requires loss-scaling shown figure without loss scaling training perplexity curve training diverges compared training iterations. scaling factor recovers relevant gradient values accuracy training matches baseline run. generative adversarial networks combine regression discrimination tasks training. image tasks generator network regresses pixel colors. case generator predicts three channels -bit color values each. network trained generate pixel images faces using dcgan methodology celebfaces dataset generator layers fractionally-strided convolutions leaky relu activations tanh. discriminator convolutions fully-connected layers. used leaky relu activations except last layer used sigmoid. batch normalization applied layers except last fully-connected layer discriminator. adam optimizer used train iterations. output images figure note show randomly selected output images whereas publications typically show curated outputs excluding poor examples. unlike networks covered paper gans widelyaccepted quantiﬁcation result quality. qualitatively outputs mixed-precision training appear comparable. network require loss-scaling match results. mixed precision training important technique allows reduce memory consumption well time spent memory arithmetic operations deep neural networks. demonstrated many different deep learning models trained using technique loss accuracy without hyper-parameter tuning. certain models large number small gradient values introduce gradient scaling method help converge accuracy baseline models. operations benchmarked deepbench volta speedups compared implementations limited memory arithmetic bandwidth. speedups lower operations latency-limited. full network training inference speedups depend library would also like extend work include generative models like text-to-speech systems deep reinforcement learning applications. furthermore automating loss-scaling factor selection would simplify training mixed precision. loss-scaling factor could dynamically increased decreased inspecting weight gradients overﬂow skipping weight updates overﬂow detected. references amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos deep speech end-to-end speech recognition english proceedings international conference machine learning pages mandarin. merri¨enboer gulcehre bahdanau bougares schwenk bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. courbariaux bengio j.-p. david. binaryconnect training deep neural networks binary weights propagations. cortes lawrence sugiyama garnett editors advances neural information processing systems pages curran associates inc. http//papers.nips.cc/paper/ -binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations. pdf. graves fern´andez gomez schmidhuber. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. proceedings international conference machine learning pages gupta agrawal gopalakrishnan narayanan. deep learning limited numerical precision. proceedings international conference machine learning pages hannun case casper catanzaro diamos elsen prenger satheesh sengupta coates deep speech scaling end-to-end speech recognition. arxiv preprint arxiv. hubara courbariaux soudry el-yaniv bengio. quantized neural networks training neural networks precision weights activations. arxiv preprint arxiv. ioffe szegedy. batch normalization accelerating deep network training reducf. bach blei editors icml volume internal covariate shift. jmlr workshop conference proceedings pages jmlr.org http //dblp.uni-trier.de/db/conf/icml/icml.htmlioffes. lutional neural networks. berger editors advances neural curran associates -imagenet-classification-with-deep-convolutional-neural-networks. pdf. radford metz chintala. unsupervised representation learning deep convolutional generative adversarial networks. corr abs/. http//dblp. uni-trier.de/db/journals/corr/corr.htmlradfordmc. rastegari ordonez redmon farhadi. xnor-net imagenet classiﬁcation using binary convolutional neural networks pages springer international publishing cham isbn ----. ./---- https//doi. org/./----_. russakovsky deng krause satheesh huang karpathy khosla bernstein berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision s---y. szegedy sermanet reed anguelov erhan vanhoucke rabinovich. going deeper convolutions. computer vision pattern recognition http//arxiv.org/abs/.. szegedy vanhoucke ioffe shlens wojna. rethinking inception architecture computer vision. ieee conference computer vision pattern recognition june schuster chen norouzi macherey krikun macherey google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv.", "year": 2017}