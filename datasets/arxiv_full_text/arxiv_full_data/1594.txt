{"title": "Semantic Composition and Decomposition: From Recognition to Generation", "tag": ["cs.CL", "cs.AI", "cs.LG", "H.3.1; I.2.6; I.2.7"], "abstract": "Semantic composition is the task of understanding the meaning of text by composing the meanings of the individual words in the text. Semantic decomposition is the task of understanding the meaning of an individual word by decomposing it into various aspects (factors, constituents, components) that are latent in the meaning of the word. We take a distributional approach to semantics, in which a word is represented by a context vector. Much recent work has considered the problem of recognizing compositions and decompositions, but we tackle the more difficult generation problem. For simplicity, we focus on noun-modifier bigrams and noun unigrams. A test for semantic composition is, given context vectors for the noun and modifier in a noun-modifier bigram (\"red salmon\"), generate a noun unigram that is synonymous with the given bigram (\"sockeye\"). A test for semantic decomposition is, given a context vector for a noun unigram (\"snifter\"), generate a noun-modifier bigram that is synonymous with the given unigram (\"brandy glass\"). With a vocabulary of about 73,000 unigrams from WordNet, there are 73,000 candidate unigram compositions for a bigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a unigram. We generate ranked lists of potential solutions in two passes. A fast unsupervised learning algorithm generates an initial list of candidates and then a slower supervised learning algorithm refines the list. We evaluate the candidate solutions by comparing them to WordNet synonym sets. For decomposition (unigram to bigram), the top 100 most highly ranked bigrams include a WordNet synonym of the given unigram 50.7% of the time. For composition (bigram to unigram), the top 100 most highly ranked unigrams include a WordNet synonym of the given bigram 77.8% of the time.", "text": "semantic composition task understanding meaning text composing meanings individual words text. semantic decomposition task understanding meaning individual word decomposing various aspects latent meaning word. take distributional approach semantics word represented context vector. much recent work considered problem recognizing compositions decompositions tackle diﬃcult generation problem. simplicity focus noun-modiﬁer bigrams noun unigrams. test semantic composition given context vectors noun modiﬁer noun-modiﬁer bigram generate noun unigram synonymous given bigram test semantic decomposition given context vector noun unigram generate noun-modiﬁer bigram synonymous given unigram vocabulary unigrams wordnet candidate unigram compositions bigram candidate bigram decompositions unigram. generate ranked lists potential solutions passes. fast unsupervised learning algorithm generates initial list candidates slower supervised learning algorithm reﬁnes list. evaluate candidate solutions comparing wordnet synonym sets. decomposition highly ranked bigrams include wordnet synonym given unigram time. composition highly ranked unigrams include wordnet synonym given bigram time. distributional semantics based hypothesis words occur similar contexts tend similar meanings hypothesis leads naturally vector space models words represented context vectors much recent work concerned problem extending vector space models beyond words phrases sentences approach worked words scale phrases sentences data sparsity growth model size paper focus noun-modiﬁer bigrams relatively simple testbed exploring problem modeling phrases. noun-modiﬁer bigram consists head noun noun adjective modiﬁes meaning head noun. given context vectors noun modiﬁer would like model meaning noun-modiﬁer bigram. natural test model whether recognize noun unigram synonymous bigram given target bigram list candidate unigrams model recognize synonymous unigram among candidates given target unigram list candidate bigrams model recognize synonymous bigram among candidates many noun unigrams synonymous noun-modiﬁer bigrams. example wordnet synonym contrabass bass ﬁddle bass viol bull ﬁddle double bass contrabass string bass. bigrams viewed decomposition unigram contrabass parts example bigram bass ﬁddle expresses contrabass type ﬁddle covers bass range; head noun ﬁddle expresses type aspect contrabass modiﬁer bass expresses range aspect contrabass. decomposition maps word deﬁnition whereas composition maps deﬁnition word past work achieved promising results recognizing noun-modiﬁer compositions decompositions given relatively short lists candidates. example turney achieves accuracy noun-modiﬁer composition recognition given seven candidate unigrams target bigram kartsaklis achieve accuracy noun decomposition recognition given seventy-two choices. however degree challenge recognition task depends nature lists candidates. distractors diﬀerent correct choice task easy. generation task avoids criticism predetermined list choices. choices limited vocabulary. work main resources wordnet lexicon waterloo corpus. corpus consists pages gathered university websites webcrawler charles clarke university waterloo. corpus contains words. stored plain text without html markup corpus size gigabytes. terms wordnet extracted approximately n-grams occur least times waterloo corpus. n-grams include unigrams bigrams n-grams majority unigrams nouns majority bigrams noun-modiﬁer phrases. composition given target bigram fast unsupervised algorithm comp score unigrams wordnet output highest scoring candidates. slower supervised algorithm super rescore candidates comp. synonymous unigram somewhere rescored candidates super. achieve target bigrams test dataset. decomposition given target unigram fast unsupervised algorithm decomp score unigrams wordnet. first unigrams scored candidate modiﬁers scored candidate heads resulting list potential modiﬁers another list potential heads. combining lists concatenation every modiﬁer wordnet available http//wordnet.princeton.edu/. experiments wordnet linux. attempt ﬁlter unigrams nouns bigrams nounmodiﬁers although would easy using information wordnet. intent algorithms robust enough handle automatically using corpus-based information. every head form list candidate bigrams. bigrams scored output highest scoring candidates. slower supervised algorithm super rescore candidates decomp. time synonymous bigram among rescored candidates target unigrams test dataset. comp decomp variations unsupervised learning algorithm turney super based supervised algorithm turney algorithms originally designed recognition task. main contribution paper show that working together algorithms scale recognition generation. section review related work recognizing compositions decompositions. datasets evaluating algorithms presented section three algorithms share features describe section algorithms presented section experiments composition given section section covers experiments decomposition. summarize results section section discusses limitations work conclude section mapping nouns noun-modiﬁers form paraphrasing. madnani dorr androutsopoulos malakasiotis present thorough surveys datadriven approaches paraphrasing. paraphrases also generated using knowledgebased techniques focus corpus-based methods since generally require much less human eﬀort knowledge-based techniques. general corpus-based approaches paraphrase extended distributional hypothesis words phrases. extended distributional hypothesis phrases occur similar contexts tend similar meanings example consider following fragments text shared context infer degree semantic similarity phrases withdrew pulled call holistic approach paraphrase phrases treated opaque wholes. holistic approach model individual words phrases. creative power language comes combining words create meanings. vocabulary unigrams possible bigrams possible trigrams. give meaning n-grams composing meanings component words. holistic approach lacks ability compose meanings cannot scale phrases sentences. holistic approaches paraphrase address creative power language holistic approach often achieves excellent results. especially suited idiomatic phrases eventually consider longer diverse phrases encounter problems data sparsity model size. paper concentrate compositional models include holistic models baselines experiments. noun-modiﬁer phrase assume context vectors represent component words earliest proposals semantic composition represent bigram vector measure similarity noun-modiﬁer phrase noun calculate cosine angle context vector simple proposal actually works relatively well although lacks order sensitivity. since animal farm farm animal representation although type farm type animal. landauer estimates meaning english text comes word choice remaining comes word order thus vector addition misses least meaning bigram. neighbours context vectors words given vocabulary. mitchell lapata found simple additive model peformed better additive model included neighbours. mitchell lapata suggest element-wise multiplication composition operation since element-wise multiplication sensitive word order. however experimental evaluation seven compositional models noncompositional models element-wise multiplication best performance holistic approach treated individual word. context vector constructed corpus manner would constructed unigram. approach scale work well predetermined small high frequency n-grams guevara baroni zamparelli point small bigrams holistic context vectors used train regression model. example regression model trained context vectors holistic context vector given bigram context vectors regression model predict holistic context vector many ideas proposed extending distributional semantics phrases sentences. recently several overviews topic proposed extensions distributional semantics involve operations linear algebra tensor products another proposal operate similarities instead working directly context vectors example sims measure semantic similarity words phrases sentences. simv measure similarity vectors matrices tensors cosine euclidean distance inner product frobenius norm. given bigram unigram context vectors suppose seeking good measure semantic similarity sims. semantic similarity must somehow compose order recognize similarity turney took second approach recognizing compositions using hand-built functions turney used supervised learning call general approach similarity composition based arguments socher combined context composition similarity composition sentence paraphrase recognition. unsupervised recursive autoencoders used compose context vectors supervised softmax classiﬁer used compose similarity matrix. turney introduced dataset semantic composition questions split questions training testing. table shows questions. stem target noun-modiﬁer bigram seven candidate unigrams. questions generated automatically wordnet. stem solution always belong wordnet synonym set. intention evaluate proposed similarity measure sims accuracy testing questions. using unsupervised learning algorithm hand-built function turney achieved accuracy testing questions. vector addition reached element-wise multiplication attained turney used supervised learning dinu pham baroni created dataset adjective-noun phrases paired noun unigram wordnet synonym set. table shows ﬁrst four phrases dataset. given vocabulary noun unigrams task distributional model rank nouns semantic similarity adjective-noun phrase. dinu evaluated seven diﬀerent models adjective-noun dataset. performance models measured medians ranks solution nouns ranked lists candidates. experiments general approach evaluation models. noun-modiﬁer phrase modiﬁer either noun adjective; therefore adjective-noun phrases subset noun-modiﬁer phrases. dinu hypothesize adjectives functions nouns onto modiﬁed nouns thus believe noun-noun phrases adjective-noun phrases different kinds models. models present treat noun-modiﬁers hence datasets contain noun-noun phrases adjective-noun phrases. comparison also evaluate models dinu al.’s adjective-noun dataset collins-thompson callan describe deﬁnition production task human subjects asked generate short deﬁnition given target word evaluate understanding word. propose algorithm automatically scoring narrow street small road tiny passage waterproof jacket rainproof coat nonabsorbent cover male child masculine youngster manlike young child infant person answers human subjects. algorithm measures semantic similarity gold standard reference deﬁnition given human expert human subject’s deﬁnition. semantic similarity calculated using markov chain. kartsaklis created dataset target noun unigrams target verb unigrams. target unigram three gold standard deﬁnitions deﬁnitions contain words average table shows ﬁrst four target terms corresponding deﬁnitions. kartsaklis treat target term class label evaluate model accuracy classifying deﬁnitions. noun unigrams deﬁnitions deﬁnition must assigned classes verb unigrams deﬁnitions classes. model based matrix multiplication combined element-wise multiplication achieves accuracy nouns verbs. element-wise multiplication attains accuracy nouns verbs. recently dinu baroni partially addressed task decomposing noun unigram adjective-noun bigram. given noun generated ranked list candidate adjectives ranked list candidate nouns attempt rank combined adjective-noun bigrams. section introduce four datasets used experiments. standard datasets based wordnet synonym sets. holistic datasets based holistic approach bigrams. four datasets created wordnetquerydata perl package. standard datasets derived seven-choice noun-modiﬁer questions used previous work test composition recognition original dataset included compositional bigrams idiomatic bigrams given diﬃculty generation problem relative recognition problem decided make dataset easier avoiding idiomatic bigrams. used wordnet glosses heuristic clues ﬁnding compositional bigrams. bigram considered highly compositional least gloss contained head noun least gloss contained modiﬁer. example stool pigeon glosses someone acting informer decoy police dummy pigeon used decoy others. head noun pigeon occurs second gloss modiﬁer stool appears neither gloss. therefore stool pigeon fails test; considered highly compositional. hand magnetic force gloss attraction iron; associated electric currents well magnets; characterized ﬁelds force. head noun force occurs gloss ﬁrst characters modiﬁer magnetic match ﬁrst characters word magnets gloss. therefore magnetic force deemed highly compositional. standard composition dataset generated extracting stem bigram seven-choice noun-modiﬁer questions checking glosses bigram whether bigram highly compositional. bigrams passed test selected target bigrams standard composition dataset. target bigram unigrams wordnet synonym bigram selected solutions bigram. standard composition dataset divided training testing subsets based whether target bigram came training testing subset original seven-choice noun-modiﬁer questions. standard decomposition dataset generated extracting solution unigram seven-choice noun-modiﬁer questions. unigram bigrams wordnet synonym unigram selected solutions unigram. solution bigrams checked whether least solution bigrams highly compositional. unigram highly compositional solution unigram selected target unigram standard decomposition dataset bigrams wordnet synonym unigram selected solutions unigram. standard decomposition dataset divided training testing subsets based whether target unigram came training testing subset original seven-choice noun-modiﬁer questions. standard datasets exploit human expertise eﬀort went construction wordnet synonym sets. idea holistic datasets reduce need human expertise using holistic vectors instead synonym sets. idea inspired guevara baroni zamparelli used holistic vectors train regression models. let’s represent bigram represent holistic vector. although bigram pretend unigram construct context vector manner would unigram. call pseudounigram. holistic composition dataset consists true bigram targets pseudounigram solutions. holistic decomposition dataset consists pseudo-unigram targets true bigram solutions. idea mapping salmon sockeye analogous mapping salmon salmon mapping sockeye salmon analogous mapping salmon salmon although mapping salmon salmon trivial holistic datasets based bigrams selected bigrams wordnet. calculated frequency bigram corpus used frequent bigrams construct targets solutions. sizes holistic datasets matched sizes corresponding standard datasets. four datasets table shows ﬁrst four targets solutions. table gives sizes datasets. holistic target solution standard targets solution. several possible solutions algorithm considered successful guess matches possible soutions. comp decomp super types features ranking candidates. features functions take unigrams pseudo-unigrams arguments return real values. tables list functions arguments. features introduced turney features supplemented ppmi turney feature addition group. frequency unigram waterloo corpus. deﬁne frequency undeﬁned. waterloo corpus zero thus also zero. pseudo-unigram frequency bigram waterloo corpus. google -gram dataset frequency bigram webt dataset. deﬁne pseudo-unigram frequency trigram webt dataset frequency trigram cab. experiments never need compute arguments pseudo-unigrams need work -grams. precompute bigrams trigrams component unigrams wordnet bigrams trigrams webt dataset. much data store data berkeley database order rapidly lbf. pointwise mutual information measure strength association words corpus probabilities observing words given corpus. probability observing together. deﬁned follows ranges negative inﬁnity positive inﬁnity. positive values indicate association negative values indicate lack association. research suggests negative values useful vector space models semantics common positive pointwise mutual information deﬁned follows experiments training datasets found accuracy improved ppmi forced range zero applying logistic function. logistic function sigmoid function following equation positive pointwise mutual information features stored sparse matrix. general procedure creating ppmi matrix described detail turney pantel following experiments ppmi matrix turney neuman assaf cohen word-context matrix rows correspond n-grams wordnet columns correspond unigrams wordnet marked left right. approximately rows matrix columns. matrix density n-gram wordnet unigram wordnet either left right. suppose corresponds i-th matrix marked handedness corresponds j-th column matrix. deﬁne ppmi value i-th j-th column matrix. value normalized positive pointwise mutual information observing side waterloo corpus either immediately adjacent separated stop words. word wordnet treated stop word. correspond matrix correspond column ppmi assigned value zero. turney estimated ppmi sampling waterloo corpus phrases containing looking side sampled phrases. sampling process ppmi necessarily equal ppmi. suppose rare word common. given ppmi sample phrases containing relatively likely phrases. given ppmi sample phrases containing less likely phrases containing although theory ppmi equal ppmi likely unequal given limited sample. given pseudo-unigram ppmi found looking corresponds bigram column corresponds unigram marked however ppmi problem columns ppmi matrix unigrams bigram never correspond column matrix. case approximate ppmi ppmi opposite handedness never need compute ppmi arguments pseudo-unigrams ppmi. domain similarity designed capture topic word. following experiments domain matrix turney make domain matrix turney ﬁrst constructed frequency matrix rows correspond n-grams wordnet columns correspond nouns observed near n-grams waterloo corpus. hypothesis nouns near term characterize topics associated term. given n-gram waterloo corpus sampled phrases containing phrases processed partof-speech tagger identify nouns. noun closest noun left right word-context frequency matrix domain space rows columns density frequency matrix converted ppmi matrix processed singular value decomposition yields three matrices term domain space represented vector ukσp parameter speciﬁes number singular values truncated singular value decomposition; number latent factors low-dimensional representation term generate deleting columns corresponding smallest singular values. parameter raises singular values power zero factors equal weight. factor weighted corresponding singular value decreasing eﬀect making similarity measure discriminating similarity words domain space computed extracting vectors ukσp correspond n-grams calculating cosine. optimal performance requires tuning parameters task comp decomp parameter settings given turney super generate features wide range parameter settings supervised learning algorithm decide features. zero. since rows domain matrix correspond n-grams wordnet pseudo-unigrams present problem. given pseudo-unigram compute extracting vectors ukσp correspond bigram unigram calculating cosine vectors. function similarity designed capture function word following experiments function matrix turney similar domain matrix except context based verbal patterns instead nearby nouns. hypothesis functional role word characterized patterns relate word nearby verbs. word-context frequency matrix function space rows columns density frequency matrix converted ppmi matrix smoothed svd. similarity words function space computed extracting vectors ukσp correspond n-grams calculating cosine. section presents algorithms comp decomp super. comp decomp fast unsupervised algorithms generate initial lists candidates super uses slower supervised algorithm reﬁne initial lists comp decomp super handles composition decomposition using algorithm requires diﬀerent training datasets builds diﬀerent models diﬀerent tasks. composition comp takes bigram input generates ranked list maxu unigrams output. super takes bigram list maxu unigrams input generates ranking list output. decomposition decomp takes unigram input generates ranked list maxb bigrams output. super takes unigram list maxb bigrams input generates ranking list output. bigram unigram wordnet unigrams. comp uses scoring function scoreu estimate quality unigram considered semantic composition bigram score based geometric mean geometric mean suitable positive numbers negative. suppress negative values comp decomp allow super since supervised learning algorithm able make them. zero otherwise. deﬁne scoreu follows heuristic score based idea that noun-modiﬁer bigram noun general domain modiﬁer general function head noun example suppose salmon sockeye. seems reasonable sockeye domain things sockeye functional role salmon. given bigram comp calculates scoreu every unigram wordnet unigrams. candidate unigrams sorted order decreasing score maxu highest scoring candidates output comp. perl data langague package rapidly calculate scoreu. scores calculated quickly processing candidate unigrams together matrix instead working individual candidate single vector. table shows parameter settings comp following experiments. values ﬁrst four parameters copied turney last parameter maxu based small number trials using standard composition training dataset. larger values maxu tend yield improved accuracy applying super comes cost increased execution time super. number latent factors domain space exponent singular values domain space number latent factors function space exponent singular values function space maximum candidate unigrams target bigram unigram bigram wordnet unigrams. decomp uses scoring function scorem estimate quality unigram considered modiﬁer bigram semantic decomposition deﬁne scorem follows like comp decomp uses modiﬁers heads. ppmi terms equations designed give weight candidate modiﬁers heads observed corpus near target unigram example suppose sockeye salmon. large corpus sample phrases contain sockeye would expect phrases contain either salmon scoring functions scorem scoreh preference whether appear right left appear sides better appearing side. decomp scores every unigram wordnet unigrams scorem. unigrams sorted order decreasing score maxm highest scoring unigrams considered candidate modiﬁers. every unigram scored scoreh maxh highest scoring unigrams taken candidate heads. decomp generates maxm maxh bigrams combining candidate modiﬁer candidate head finally maxm ·maxh bigrams scored scoreb deﬁned follows number latent factors domain space exponent singular values domain space number latent factors function space exponent singular values function space maximum candidate modiﬁers target unigram maximum candidate heads target unigram maximum candidate bigrams target unigram table shows parameter settings decomp following experiments. ﬁrst four parameters comp table remaining three maxm maxh maxb based small number trials using standard decomposition training dataset. larger values tend improve accuracy cost increased execution time super. equations decomp complex equation comp decomp exploring larger candidates. given target bigram comp considers possible unigram compositions. given target unigram decomp considers candidate bigram decompositions. algorithms share core functions decomp requires additional functions ppmi order handle larger set. maxm maxh maxm maxh thus scoreb applied million candidate bigrams input target unigram seem large candidates considerably smaller possible bigrams also smaller bigrams google -gram dataset decomp splits task steps first select subsets candidate modiﬁers heads independently evaluate bigrams formed relatively small subsets. super uses supervised learning algorithm reﬁne lists gets comp decomp. comp takes target bigram input generates maxu unigrams output. super views maxu triples form ranges maxu. decomp takes target unigram input generates maxb bigrams output. super views maxb triples form ranges maxb. cases task super learn rank triples. super uses feature vectors represent triples regardless whether triples come comp decomp. ﬁrst step super represent triple feature vector. triple unigrams. super generates three features unigram features possible pair unigrams ppmi possible pairs unigrams possible values hence twelve features. order pairs matter cosine symmetric cos) three pairs consider however explore values eleven values features. gives total features summarized table features turney except features. since super supervised requires training dataset. example standard composition training dataset contains target bigrams given maxu running comp training dataset triples thus training feature vectors. feature vectors train model. first drop target bigrams solutions. example table target bigram foot lever solutions pedal treadle. neither unigrams appear among candidates output comp remove triples containing foot lever training dataset. standard composition training dataset step leaves target bigrams total training triples. second adjust class ratio. table shows possible solutions target bigram standard composition training dataset solutions appear candidates target generated comp. target bigrams survive ﬁrst ﬁltering step above solutions target bigram actually occur candidates target comp. gives triples label class remaining triples labeled class class imbalance makes learning diﬃcult therefore target bigrams select triples class randomly sample triples class ratio triples class every triple class following experiments ratio triples class triples class total triples. super uses sequential minimal optimization support vector machine implemented weka kernel normalized third-order polynomial. weka provides probability estimates classes ﬁtting outputs logistic regression models. training apply super testing dataset. testing target rank candidates descending order probability belong class estimated svm. table shows parameter settings super following experiments. super parameter values copied turney except ratio based small number trials using standard composition training dataset. parameter settings used four datasets. supervised learning able construct accurate scoring functions equations super requires training data executes slowly. super uses features whereas comp uses features decomp uses eight features. computation time super spent calculating extra features. combining comp decomp super able speed advantage unsupervised heuristics accuracy advantage supervised learning. table uses target bigram salmon illustrate process comp super. solution unigram sockeye marked asterisk. among candidates output comp sockeye ranked score according equation candidates comp rescored super sockeye rises rank third score according model. table summarizes performance comp super standard composition testing dataset containing target bigrams. labeled mean rank candidates mean rank ﬁrst correct answer guesses. labeled median rank candidates median rank ﬁrst correct answer guesses. rows labeled percent give percentage targets correct solution guesses. solutions guesses rank ﬁrst solution. percent candidates percentage target bigrams correct answer anywhere maxu candidates generated comp. ﬁnal number candidates considered shows number possibilities considered comp super. calculating mean median rank include targets correct answer among guesses since rank deﬁned super correct answer among guesses. thus mean median cover targets. makes mean median slightly misleading algorithm could improve mean median ranks refusing guess unless conﬁdent. four evaluation metrics cover targets. include metrics order provide broad perspective performance algorithms. preferred evaluation metric percent percentage targets correct answer guesses. metric comp super working together achieve score compare comp super three baselines vector addition element-wise multiplication holistic approach. bigram unigram. corresponding context vectors. vector addition score triple using cosine angle element-wise multiplication triple scored holistic approach treat pseudo-unigram. context vector pseudo-unigram score triple using cos. vector addition used domain matrix since matrix best performance addition turney used training dataset optimize parameters smoothed matrix ukσp best results obtained element-wise multiplication used ppmi matrix since matrix negative elements. turney pointed element-wise multiplication suitable vectors contain negative elements. matrices contain negative elements truncated singular value decomposition. turney suggested modiﬁed form element-wise multiplication address issue found scale number vectors testing dataset ppmi matrix parameters tune. holistic approach used mono matrix turney since matrix best performance holistic approach turney mono matrix formed merging domain function matrices. turney details. used training dataset optimize parameters smoothed matrix ukσp since turney showed geometric mean domain similarity function similarity performed better vector addition element-wise multiplication noun-modiﬁer composition recognition task decided apply addition multiplication output comp instead applying full unigrams. intention give vector addition element-wise multiplication beneﬁt preprocessing comp. hand turney found holistic approach accurate approaches; therefore applied holistic approach whole unigrams preproessing comp. table shows results. look percent percent table addition multiplication seem impair ranking done comp. percent seems beneﬁt addition multiplication. however super signiﬁcantly better addition multiplication according fisher’s exact test conﬁdence level. diﬀerence super holistic approach signﬁcant. percent diﬀerence super holistic approach also signﬁcant. percent holistic approach signiﬁcantly better super note holistic approach handle target bigrams included bigrams wordnet whereas super handle target bigram contains unigrams unigrams wordnet chose base vocabulary wordnet target bigrams standard composition dataset restricted bigrams appear wordnet. since datasets features based vocabulary wordnet holistic approach seems useful would appear testing dataset derived wordnet. holistic approach cannot scale bigrams. comp super scale attain accuracy near level holistic approach although testing dataset favours holistic approach. given standard composition dataset super learns triples derived wordnet synonym sets super learns expert knowledge embedded wordnet. would like able train super without using kind expertise. past work adjective-noun bigrams shown holistic bigram vectors train supervised regression model turney adapted approach supervised classiﬁcation applied composition recognition. purpose holistic datasets investigate whether apply holistic training composition decomposition generation. holistic datasets generated without using wordnet synomym sets. construct holistic datasets used wordnet source bigrams ignored rich information wordnet provides bigrams synonyms hypernyms hyponyms meronyms glosses. table shows performance comp super holistic compostion dataset. ease comparison table also shows performance standard composition dataset. training testing subsets holistic composition dataset sizes corresponding training testing subsets standard composition dataset however comp super achieve much better results holistic dataset standard dataset. suggests holistic dataset nearly challenging dataset based wordnet synonym sets. table looks performance various combinations testing training datasets. although comp super work together table shows results super order make table easier read. columns labeled daum´e iii’s domain adaptation algorithm training. algorithm allows train super standard training dataset holistic training dataset. daum´e iii’s algorithm general strategy merging datasets somewhat diﬀerent statistical distributions class labels standard holistic datasets. table shows model learned standard training carries well holistic testing. focusing percent standard training achieves holistic testing signiﬁcantly diﬀerent obtained holistic training hand model learned holistic training carry well standard training. holistic training achieves standard testing whereas standard training achieves signiﬁcantly higher. table suggests much beneﬁt merging holistic standard training datasets daum´e iii’s domain adaptation algorithm. training standard training dataset alone achieves results good merging datasets. dataset. training merged standard training testing composition datasets removed target bigrams appeared adjectivenoun dataset testing used adjective-noun phrases. slight decrease performance adjective-noun phrases compared noun-modiﬁer dataset hypothesize deliberately sought highly compositional phrases noun-modiﬁer dataset using wordnet glosses heuristic clues seems phrases adjectivenoun dataset less compositional phrases noun-modiﬁer dataset. adjective-noun phrases introduced dinu used recently better results baroni dinu table copy results evaluated seven diﬀerent models adjective-noun dataset. reduction column indicates algorithm used reduce dimensionality matrix nonnegative matrix factorization singular value decomposition column indicates number factors dimensionality reductions. rank column gives median rank solution ranked list candidates. table vector addition weights dilation model introduced mitchell lapata mult element-wise multiplication powers weights fulladd multiplies vector weight matrix adds resulting weighted vectors lexfunc represents adjectives matrices nouns vectors adjective matrix kind lexical function modiﬁes noun vector. fulllex represents every component unigram whether noun adjective models table weights parameters tuned learned training data. training takes holistic approach. given bigram models trained predict vector pseudo-unigram order compare results dataset results need make adjustments experimental setup. first consider vocabulary unigrams consider nouns. fair comparison restrict comp nouns. filtercomp modiﬁed version comp ﬁlters output comp remove unigrams among nouns second calculate mean median rank targets solutions among candidates super rank candidates. since single pass approach rank candidates. make median comparable median give rank candidates. target solution among candidates assume rank solution worst possible rank; rank last candidate table shows performance filtercomp super adjustments. filtering makes task easier removing irrelevant candidates tends improve results adjustment mean median calculation negative impact evaluation metrics. fortunately median robust statistic slightly aﬀected adjustment. mean hand greatly changed worse. medians filtercomp super performing much better best model table enetlex median rank recall discussion section context composition similarity composition. models table involve context composition whereas filtercomp super similarity composition. believe main reason better performance filtercomp super. super attempting learn function maps feature vectors scalar probabilities. majority features various kinds similarities models table attempting learn function maps vectors. models trained pseudo-unigram context vectors. believe pseudo-unigram context vectors approximation surrogate really want learn. really want learn salmon sockeye synonymous instead learn futhermore lexfunc enetlex must learn separate model adjective vocabulary. diﬀerent adjectives adjectivenoun phrases; thus lexfunc enetlex require least diﬀerent trained models order solution nouns targets. filtercomp super model noun-modiﬁer expressions adjective-noun noun-noun. readily handle adjectives never appeared training data unlike lexfunc enetlex. decomp ﬁrst builds list maxm modiﬁers using scorem list maxh heads using scoreh. table illustrates ﬁrst step using target unigram sockeye example. table shows sockeye several possible solution bigrams blueback salmon salmon sockeye salmon oncorhynchus nerka. looking ahead solution eventually found sockeye salmon. table sockeye ranked ﬁrst list candidate modiﬁers salmon ranked sixth list candidate heads. next decomp builds list maxb bigrams using scoreb rank combination maxm modiﬁers maxh heads. super rescores list bigrams. table continues sockeye example. decomp ranks sockeye salmon ﬁrst. unfortunately super moves rank sockeye salmon twenty-ﬁrst choice. second guess super table pink salmon seems like good decomposition sockeye member wordnet synonym sockeye. pink salmon salmon distinct species. coho salmon chinook salmon table summarizes performance decomp super standard decomposition testing dataset containing target unigrams. according preferred evaluation metric percent decomp super working together achieve score dataset. comp super reach standard composition testing dataset comp super consider candidates whereas decomp super consider decomposition diﬃcult task composition. composition compare decomp super three baselines vector addition element-wise multiplication holistic approach. addition multiplication take output decomp rescore holistic approach scores bigrams wordnet treating bigrams pseudo-unigrams. table shows results. addition multiplication perform signiﬁcantly worse decomp super according percent holistic approach performs signiﬁcantly better decomp super according percent note holistic approach decompose unigram bigram bigram bigrams wordnet whereas decomp super decompose unigram bigram contains unigrams unigrams wordnet although holistic approach performs better table decomp super considering much larger candidates. fair comparison restrict decomp bigrams holistic approach. filterdecomp modiﬁed version decomp simply ﬁlters output decomp remove bigrams wordnet. table compares filterdecomp super holistic approach. filterdecomp super approach performance holistic approach. diﬀerence percent super versus holistic approach statistically signiﬁcant diﬀerences signiﬁcant also performance super decomposition dataset near performance composition dataset suggests main diﬀerence composition decomposition tasks simply number candidates must considered. point table show decomp super competitive performance holistic approach contrary appearances table would actually want filterdecomp realistic application. problem bigrams enough bigrams serve miniature deﬁnitions unigrams. table shows performance decomp super holistic decomposition dataset. previous results standard decomposition dataset copied make comparison easier. composition task results show holistic dataset nearly challenging dataset based wordnet synonym sets. table gives results various combinations testing training datasets. beneﬁt merging training datasets using daum´e iii’s domain adaptation algorithm however focus percent diﬀerences training standard dataset merged dataset signiﬁcant. training standard training dataset alone achieves results good merging datasets. still signiﬁcant beneﬁt using holistic dataset. section evaluted comp super comparing output wordnet synonym sets. bigrams standard compositon dataset highly ranked unigrams included wordnet synonym time section evaluated decomp super wordnet. standard decomposition dataset bigrams included wordnet synonym time together comp super explore candidate unigram compositions bigram. decomp super explore candidate bigram decompositions unigram. given decomp super explore much larger space comp super encouraging decomp super able achieve sections compared comp decomp super three baselines vector addition element-wise multiplication holistic approach. given output comp addition multiplication able improve percentage solutions found candidates super able achieve signiﬁcantly larger improvement given output decomp addition multiplication impair ranking decomp whereas super improves ranking decomp however holistic approach seems achieve better results comp decomp super. comp super working together achieve whereas holistic approach achieves diﬀerences scores statistically signiﬁcant. decomp super ﬁrst seem performing substantially holistic approach take account decomp super explore candidate bigram decompositions whereas holistic approach scaling problems explore decompositions. correct diﬀerences statistically signiﬁcant. given good performance holistic baseline natural consider using holistic datasets relatively inexpensive train supervised system strength super uses compositional approach scale weakness requires training. strength holistic approach works well without training weakness scale hope training super holistic datasets give strengths compositional approach relatively inexpensive holistic training. experiments training testing datasets observed holistic datasets much easier master standard wordnetbased datasets understanding experiments training testing datasets diﬀerent takes eﬀort. present model give insight cross-domain results particularly interested training holistic dataset testing standard dataset suppose super given target generates list candidate compositions decompositions probability solution among candidates generated super subscript indicates holistic training holistic testing. probability solution target among candidates super subscript indicates holistic training standard testing. probability solution target among candidates holistic approach indicates standard training standard testing. holistic approach require training usual sense standard training tune parameters model qss. probability super transfer learned holistic domain targets sampled standard domain estimated multiplying probability super successfully emulate holistic approach probability holistic approach successfully handle standard domain qss. view results holistic training holistic testing providing estimates probability super successfully emulate holistic approach phh. likewise view results holistic approach given standard training testing providing estimates probability holistic approach solutions wordnet-based targets qss. super trained holistic datasets evaluated standard datasets estimate probability success qss. column gives performance holistic approach trained tested using standard composition dataset. interpret percentages column estimated probabilities varying values numbers column product corresponding values columns based interpreting percentages probabilities. example interpret percentages column predictions model varying values column shows performance super trained holistic composition dataset tested standard composition dataset. column gives observed values compare calculated values column cannot apply model immediately performance decomp super decomposition datasets performance holistic approach table based partial search. holistic approach considers possible candidates need base full search considering candidates. problem holistic approach scale size. would need build termcontext matrix rows considerably beyond current state art. therefore approximate expected performance holistic approach full search extrapolating results partial full search decomp super. table gives estimated probabilities holistic approach composition evaluation metric percent percent percent percent candidates training dataset testing dataset source percentages observed calculated? decomposition evaluation metric percent percent percent percent candidates training dataset testing dataset source percentages observed calculated? column shows performance super trained holistic decomposition dataset tested standard decomposition dataset. column gives observed values compare calculated values column holistic approach unsupervised. although training data useful parameter tuning larger training dataset unlikely much impact qss. training data improve best hope thus model predicts cannot greater likely improvements qss. hand training super wordnet already achieving performance near qss. however model take account might possible domain adaptation summary gain insight cross-domain results sections viewing observed performance super product things ability super emulate holistic approach ability holistic approach handle targets solutions based wordnet qss. main limitation work described focused attention noun-modiﬁer bigrams noun unigrams. natural next step would consider subject-verb bigrams verb-object bigrams subject-verb-object trigrams. plan extend approach recognizing sentence paraphrase another. achieve goal propose adapt dynamic pooling approach introduced socher eventually hope able move beyond sentence paraphrase recognition sentence paraphrase generation. turney demonstrated features used work well recognizing semantic relations; example recognize semantic relation mason stone analogous semantic relation carpenter wood. analogy generation another possibility work. believe improved performance obtained adding features. features based third-order tensors might useful recognizing generating paraphrases right features possible surpass accuracy holistic approach. comp decomp rely hand-crafted score functions advantage hand-crafted functions eight features makes much faster using features super. avoid hand-crafting would aggressive feature selection super. instead using comp decomp ﬁrst pass candidates could highly pruned version super. expect pruning reduce accuracy still beneﬁt second pass super using full feature set. another area future work experimentation domain adaptation holistic training. although model suggests limits value holistic training possible overcome limits domain adaptation main weakness recognition task degree challenge task depends given list candidates. critic always claim given list easy; distractors unlike solution. generation task avoids criticism. furthermore ability generate solutions opens opportunities applications beyond applications possible ability recognize solutions. following example dinu extended distributional models composition task recognition task generation. results adjective-noun dataset suggest similarity composition better approach generating compositions context composition experiments holistic training support hypothesis models dinu limited reliance holistic pseudo-unigram training stringent criterion allows guess achieved accuracy composition generation decomposition generation relaxed criterion allows guesses accuracy composition accuracy decomposition know accuracy average human would tasks generating solution agrees wordnet diﬃcult. many words wordnet unfamiliar also developers wordnet likely missed many reasonable possibilities wordnet synonym sets. introduced simple cross-domain model holistic training wordnet-based testing gives insight limitations holistic training. standard training models achieving performance near level holistic approach features expect super able surpass performance holistic approach. however simple cross-domain model suggest ways improve holistic training. results daum´e iii’s domain adaptation algorithm hint holistic training able serve supplement standard training. main contribution extend similarity composition approach beyond recognition generation composition decomposition. combining unsupervised ﬁrst pass supervised second pass scale generation task. thanks charles clarke egidio terra sharing waterloo corpus. thanks colleagues provided many helpful comments presentation research. thanks georgiana dinu nghia pham marco baroni sharing noun-adjective dataset vocabulary nouns.", "year": 2014}