{"title": "Adversarially Regularized Autoencoders", "tag": ["cs.LG", "cs.CL", "cs.NE"], "abstract": "While autoencoders are a key technique in representation learning for continuous structures, such as images or wave forms, developing general-purpose autoencoders for discrete structures, such as text sequence or discretized images, has proven to be more challenging. In particular, discrete inputs make it more difficult to learn a smooth encoder that preserves the complex local relationships in the input space. In this work, we propose an adversarially regularized autoencoder (ARAE) with the goal of learning more robust discrete-space representations. ARAE jointly trains both a rich discrete-space encoder, such as an RNN, and a simpler continuous space generator function, while using generative adversarial network (GAN) training to constrain the distributions to be similar. This method yields a smoother contracted code space that maps similar inputs to nearby codes, and also an implicit latent variable GAN model for generation. Experiments on text and discretized images demonstrate that the GAN model produces clean interpolations and captures the multimodality of the original space, and that the autoencoder produces improve- ments in semi-supervised learning as well as state-of-the-art results in unaligned text style transfer task using only a shared continuous-space representation.", "text": "junbo zhao yoon kelly zhang alexander rush yann lecun department computer science york university school engineering applied sciences harvard university facebook research {jakezhaokzyann}cs.nyu.edu {yoonkimsrush}seas.harvard.edu autoencoders technique representation learning continuous structures images wave forms developing general-purpose autoencoders discrete structures text sequence discretized images proven challenging. particular discrete inputs make difﬁcult learn smooth encoder preserves complex local relationships input space. work propose adversarially regularized autoencoder goal learning robust discrete-space representations. arae jointly trains rich discrete-space encoder simpler continuous space generator function using generative adversarial network training constrain distributions similar. method yields smoother contracted code space maps similar inputs nearby codes also implicit latent variable model generation. experiments text discretized images demonstrate model produces clean interpolations captures multimodality original space autoencoder produces improvements semi-supervised learning well state-of-the-art results unaligned text style transfer task using shared continuous-space representation. recent work regularized autoencoders variational denoising variants shown signiﬁcant progress learning smooth representations complex high-dimensional continuous data images. codespace representations facilitate ability apply smoother transformations latent space order produce complex modiﬁcations generated outputs still remaining data manifold. unfortunately learning similar latent representations discrete structures text sequences discretized images remains challenging problem. initial work vaes text shown optimization difﬁcult decoder easily degenerate unconditional language model recent work generative adversarial networks text mostly focused getting around discrete structures either policy gradient methods gumbel-softmax distribution however neither approach produce robust representations directly. major difﬁculty discrete autoencoders mapping discrete structure continuous code vector also smoothly capturing complex local relationships input space. inspired recent work combining pretrained autoencoders deep latent variable models propose target issue adversarially regularized autoencoder speciﬁcally jointly train discrete structure encoder continuous space generator constraining models discriminator agree distribution. approach allows utilize complex encoder model still constrain ﬂexible limited generator distribution. full model used smoother discrete structure autoencoder latent variable model sample decoded decoder discrete output. since system produces single continuous coded representation—in contrast methods state—it easily regularized problem-speciﬁc invariants instance learn ignore style sentiment attributes transfer tasks. experiments apply arae discretized images sentences demonstrate properties model. using latent variable model model able generate varied samples quantitatively shown cover input spaces generate consistent image sentence manipulations moving around latent space interpolation offset vector arithmetic. using discrete encoder model used semi-supervised setting give improvement sentence inference task. arae model trained task-speciﬁc adversarial regularization model improves current best results sentiment transfer reported shen produces compelling outputs topic transfer task using single shared code space. outputs listed appendix code available related work practice unregularized autoencoders often learn degenerate identity mapping latent code space free structure necessary apply method regularization. popular approach regularize explicit prior code space variational approximation posterior leading family models called variational autoencoders unfortunately vaes discrete text sequences challenging train—for example training procedure carefully tuned techniques like word dropout annealing decoder simply becomes language model ignores latent code possible reason difﬁculty training vaes strictness prior and/or parameterization posterior. work making prior/posterior ﬂexible explicit parameterization notable technique adversarial autoencoders attempt imbue model ﬂexible prior implicitly adversarial training. framework discriminator trained distinguish samples ﬁxed prior distribution input encoding thereby pushing code distribution match prior. adds ﬂexibility similar issues modeling text sequences suffers mode-collapse experiments. approach similar motivation notably sample ﬁxed prior distribution—our ‘prior’ instead parameterized ﬂexible generator. nonetheless view provides interesting connection vaes gans. success gans images many researchers consider applying gans discrete data text. policy gradient methods natural deal resulting non-differentiable generator objective training directly discrete space trained text data however methods often require pre-training/co-training maximum likelihood objective precludes latent encoding sentence also potential disadvantage existing language models another direction work reparameterizing categorical distribution gumbel-softmax trick —while initial experiments encouraging synthetic task scaling work natural language challenging open problem. also ﬂurry recent related approaches work directly soft outputs generator example shen exploits adversarial loss unaligned style transfer text discriminator hidden states using soft outputs step input generator utilizing professor-forcing framework approach instead works entirely code space require utilizing hidden states directly. background discrete structure autoencoders deﬁne discrete structures vocabulary symbols distribution space. instance binarized images number pixels sentences vocabulary sentence length. discrete autoencoder consists parameterized functions deterministic encoder function encφ parameters maps input code space conditional decoder distribution structures parameters parameters choice encoder decoder parameterization speciﬁc structure interest example rnns sequences. notation maxx decoder mode. autoencoder said perfectly reconstruct generative adversarial networks gans class parameterized implicit generative models method approximates drawing samples true distribution instead employing latent variable parameterized deterministic generator function produce samples initial work gans minimizes jensen-shannon divergence distributions. recent work wasserstein replaces earth-mover distance. training utilizes separate models generator maps latent vector easy-to-sample source distribution sample critic/discriminator aims distinguish real data generated samples informally generator trained fool critic critic tell real generated. wgan training uses following min-max optimization generator parameters critic parameters denotes critic function obtained generator real generated distributions. critic parameters restricted -lipschitz function term correspond minimizing wasserstein- distance naive approximation enforce property weight-clipping i.e. model adversarially regularized autoencoder ideally discrete autoencoder able reconstruct also smoothly assign similar codes similar continuous autoencoders property enforced directly explicit regularization. instance contractive autoencoders regularize loss functional smoothness encφ. however criteria apply inputs discrete lack even metric input space. enforce similar discrete structures nearby codes? adversarially regularized autoencoders target issue learning parallel continuous-space generator restricted functional form smoother reference encoding. joint objective regularizes autoencoder constrain discrete encoder agree distribution continuous counterpart wasserstein- distance distribution codes discrete encoder model distribution codes continuous generator model e.g. approximate wasserstein- term function includes embedded critic function optimized adversarially encoder generator described background. full model shown figure train model block coordinate descent alternate optimizing different parts model encoder decoder minimize reconstruction loss wgan critic function approximate term encoder generator adversarially fool critic minimize figure arae architecture. model used autoencoder structure encoded decoded produce sample passed though generator produce code vector similarly decoded critic function used training help approximate sample {x}m backpropagate reconstruction loss lrec sample {x}m compute code-vectors encφ) gθ). backpropagate loss sample {x}m compute code-vectors encφ) gθ). backpropagate adversarial loss extension code space transfer beneﬁt arae framework compresses input single code vector. framework makes ideal manipulating discrete objects continuous code space. example consider problem unaligned transfer want change attribute discrete input without supervised examples e.g. change topic sentiment sentence. first extend decoder condition transfer variable denoting attribute known training learn next train code space invariant attribute force learned fully decoder. speciﬁcally regularize code space similar different attribute labels near enough fool code space attribute classiﬁer i.e. lclass loss classiﬁer code space labels incorporate additional regularization simply gradient update steps training classiﬁer discriminate codes adversarially training encoder fool classiﬁer. algorithm shown algorithm note similar technique introduced domains notably images video modeling methods architectures experiment three different arae models autoencoder discretized images trained binarized version mnist autoencoder text sequences trained using stanford natural language inference corpus autoencoder trained text transfer based yelp yahoo datasets unaligned sentiment topic transfer. three models utilize generator architecture generator architecture uses dimensional gaussian prior maps critic generator parameterized feed-forward mlps. image model uses fully-connected autoencode binarized images. image size. encoder used feed-forward network mapping encφ decoder predicts pixel parameterized logistic text model uses recurrent neural network encoder decoder. sentence length vocabulary underlying language. deﬁne parameterized recurrent function maps discrete input structure hidden vectors encoder deﬁne encφ decoding feed additional input decoder time step i.e. calculate distribution softmaxxj parameters finding likely sequence distribution intractable possible approximate using greedy search beam search. experiments lstm architecture encoder/decoder decode using greedy search. text transfer model uses architecture text model extends code space classiﬁer modeled using trained minimize cross-entropy. baselines utilize standard autoencoder cross-aligned autoencoder transfer. note arae standard experiments encoded code encoder normalized unit sphere generated code bounded tanh function output layer. additionally experimented sequence introduced bowman adversarial autoencoder model snli dataset. however despite extensive parameter tuning found neither model able learn meaningful latent representations—the simply ignored latent code experienced mode-collapse repeatedly generated samples. appendix includes detailed descriptions hyperparameters model architecture training regimes. experiments consider three aspects model. first measure empirical impact regularization autoencoder. next apply discrete autoencoder applications unaligned style transfer semi-supervised learning. finally employ learned generator network implicit latent variable model discrete sequences. main goal arae regularize model produce smoother encoder requiring distribution encoder match distribution continuous generator simple latent variable. examine claim consider basic statistical properties code space training text model snli shown figure left norm code converge quickly arae training. encoder code always restricted unit sphere generated code quickly learns match middle plot shows convergence trace covariance matrix generator encoder training figure left norm encoder code generator code arae training. encoder normalized model whereas generator learns match training progresses. middle dimension-wise variances encoder codes generator codes compared standard right average cosine similarity nearby sentences arae table left. reconstruction error original sentence corrupted sentence. number swaps performed original sentence. right. samples generated arae input noised swapping words. progresses. variance encoder generator match several epochs. check smoothness model arae/ae take sentence calculate average cosine similarity randomly-selected sentences edit-distance original sentence. sentences calculate mean average cosine similarity. figure shows cosine similarity nearby sentences quite high arae case edit-distance ideal proxy similarity sentences often sufﬁcient condition. finally ideal representation robust small changes input around training examples code space test property feeding noised input encoder calculating score given original input checking reconstructions. table shows experiment text noise permuting words sentence. observe arae able noised sentence natural sentence table shows empirical results experiments. obtain reconstruction error original sentence decoder utilizing noised code. regular better reconstructs input expected. however increase number swaps push input away data manifold arae likely produce original sentence. note unlike denoising autoencoders require domain-speciﬁc noising function arae explicitly trained denoise input learns byproduct adversarial regularization. unaligned text transfer smooth autoencoder combined reconstruction error make possible robustly manipulate discrete objects code space without dropping data manifold. test hypothesis experimented unaligned text transfer tasks. tasks attempt change attribute sentence without aligned examples change. perform transfer learn code space represent input agnostic attribute decoder incorporate attribute experiment unaligned transfer sentiment yelp corpus topic yahoo corpus sentiment follow setup shen split yelp corpus sets unaligned positive negative reviews. train arae autoencoder separate decoders positive negative sentiment incorporate adversarial training encoder remove sentiment information code space. test encoding sentences class decoding greedily opposite decoder. evaluation based four automatic metrics shown table transfer measuring successful model transferring sentiment based automatic classiﬁer bleu measuring consistency transferred text original. expect model maintain much information possible transfer style; perplexity measuring ﬂuency generated text; reverse perplexity measuring extent generations representative underlying data distribution. perplexity numbers obtained training language model. additionally perform human evaluations cross-aligned best arae model. randomly select sentences obtain corresponding transfers models amazon mechanical turkers evaluate sentiment naturalness transferred sentences. create separate task show turkers original transferred sentences evaluate similarity based sentence structure explicitly turkers disregard sentiment similarity assessment. addition comparing cross-aligned shen also compare vanilla trained without adversarial regularization. arae experimented different weighting adversarial loss generally experimentally adversarial regularization enhances transfer perplexity tends make transferred text less similar original compared randomly selected sentences shown ﬁgure samples shown available appendix method applied style transfer tasks instance challenging yahoo data yahoo chose relatively distinct topic classes transfer science math entertainment music politics government. dataset contains this reverse perplexity calculated training language model generated data measuring perplexity held-out real data also found metric helpful early-stopping based validation data. three fabulous artists incredible talent three genetically bonded water many substances capable producing special case three competing government semi-supervised training utilize arae standard setup semi-supervised training. experiment natural language inference task shown table original labeled training data rest training unlabeled training. labeled randomly picked. full snli training contains sentence pairs supervised sets sentence pairs respectively three settings. baseline trained additional data similar setting explored arae subset unsupervised data length roughly includes single sentences observed training unlabeled data objective improves upon model trained labeled data. training adversarial regularization provides gains. latent variable model discrete structures training arae also used implicit latent variable model controlled generator refer arae-gan. models form widely used generation modalities less effective discrete structures. section attempt measure effectiveness induced discrete gan. common test gans ability mimic true distribution train simple model generated samples pitfalls evaluation provides starting point text modeling. generate samples arae-gan trained data real training test respectively using supervised labels full snli training right. perplexity language models trained synthetic samples gan/ae/lm evaluated real data corner sport area corner road lady outside racetrack lady outside racetrack people outdoors urban setting people outdoors urban setting people outdoors urban setting figure sample interpolations arae-gan. constructed linearly interpolating latent space decoding output space. word changes highlighted black. results arae. block shows output generation decoder taking fake hidden codes generated gan; bottom block shows sample interpolation results. ⇒walking sleeping clapping balloons ⇒walking person standing beneath criminal ⇒man jewish trying stay skateboard ⇒man people works uniform studio ⇒two child head playing plastic drink ⇒two baby workers watching steak water ⇒dog people shine looks area ⇒dog babies wearing huge factory ⇒standing women walking outside near ⇒standing dogs sleeping front dinner side child listening piece steps playing table ⇒several ⇒several children working shirt cold ﬁeld clapping walking dogs person walking beneath pickup jewish trying stay horse works studio uniform children playing head plastic drink workers watching baby steak grass arrives looks area babies wearing huge ears three women standing near walking dogs standing front dinner several child playing guitar side table several children working shirt cold ﬁeld figure left. quantitative evaluation transformations. match refers samples least decoder samples desired transformation output prec. measures average precision output original sentence. right. examples offset vectors produced successful transformations original sentence. appendix full methodology. shown appendix models size allow fair comparison. train language model generated samples evaluate held-out data calculate reverse perplexity. seen table training real data outperforms training generated data large margin. surprisingly however language model trained arae-gan data performs slightly better trained lm-generated/ae-generated data. found reverse quite high mode-collapse. another property gans gaussian form induces ability smoothly interpolate outputs exploiting structure latent space. language models provide better estimate underlying probability space constructing style interpolation would require combinatorial search makes useful feature text gans. experiment property sampling points constructing intermediary points generate argmax output ˜xλ. samples shown figure text figure discretized mnist arae-gan. ﬁnal intriguing property image gans ability move latent space offset vectors example radford observe mean latent vector glasses subtracted mean latent vector without glasses applied image woman without glasses resulting image woman glasses. experiment property generate million sentences arae-gan compute vector transforms space attempt change main verbs subjects modiﬁer examples successful transformations shown figure quantitative evaluation success vector transformations given figure conclusion present adversarially regularized autoencoders simple approach training discrete structure autoencoder jointly code-space generative adversarial network. model learns improved autoencoder demonstrated semi-supervised experiments improvements text transfer experiments. also learns useful generative model text exhibits robust latent space demonstrated natural interpolations vector arithmetic. note model seemed quite sensitive hyperparameters. finally many useful models text generation already exist text gans provide qualitatively different approach inﬂuenced underlying latent variable structure. envision framework could extended conditional setting combined existing decoding schemes used provide interpretable model language. tong yanran ruixiang zhang devon hjelm wenjie yangqui song yoshua bengio. maximum-likelihood augment discrete generative adversarial networks. arxiv. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. proceedings nips guillaume lample neil zeghidour nicolas usuniera antoine bordes ludovic denoyer marc’aurelio ranzato. fader networks manipulating images sliding attributes. proceedings nips interpret arae framework dual pathway network mapping distinct distributions similar one; encφ output code vectors kept similar terms wasserstein distance measured critic. provide following proposition showing parameterization encoder generator wasserstein distance converges encoder distribution converges generator distribution further moments converge. ideal since setting generated distribution simpler encoded distribution input generator simple distribution generator possesses less capacity encoder. however simple overly restrictive empirically observe ﬁrst second moments indeed converge training progresses proposition distribution compact sequence distributions suppose following statements hold people quote entire poems song lyrics ever actually chosen best answer think scientists learn human anatomy physiology life people knows anything recent issue <unk> leadership three fabulous artists incredible talent three genetically bonded water many substances capable producing special case three competing government offer terriﬁc information cast characters offer insight characteristics earth composed many stars offer legitimate information invasion iraq u.s. aspects history woman preparing three woman seeing river passes woman near birds people sitting ofﬁce stolen young dinner monks running court boys glasses girl small sitting tell children children eating balloon animal woman trying microscope dogs sleeping three woman cart tearing tree hugging fancy skier starting drag <unk> standing boys swimming surfer couple waiting show couple kids barbecue motorcycles ocean loading bike empty actor walking small area young mother walking outside dirt road sitting dock large group people taking photo christmas night someone avoiding soccer game woman dressed movie person empty stadium pointing mountain children little <unk> blue shirt rides bicycle girl running another forest indian women figure text samples generated arae-gan simple baseline trained data. generate multivariate gaussian learned code space generate code vectors gaussian. generate million sentences arae-gan parse sentences obtain main verb subject modiﬁer. given sentence change main verb subtract mean latent vector sentences main verb mean latent vector sentences desired transformation transform subject modiﬁer. decode back sentence space transformed latent vector sampling pψ). examples successful transformations shown figure quantitative evaluation success vector transformations given figure original vector sample sentences transformed latent vector consider match sentences demonstrate desired transformation. match proportion original vectors yield match post transformation. ideally want generated samples differ speciﬁed transformation also calculate average word precision original sentence match. encoder three-layer ---. additive gaussian noise added decoder. standard decoder four-layer ---- autoencoder optimized adam learning rate generator ---- using batch normalization relu critic ---- weight clipping critic trained components optimized adam learning rate generator weighing factor encoder one-layer lstm hidden units. gaussian noise feeding decoder. standard deviation noise initialized exponentially decayed every iterations factor decoder one-layer lstm hidden units. decoding process time step takes layer lstm hidden state concatenates hidden codes feeding output softmax layer. word embedding size adopt grad clipping encoder/decoder grad_norm encoder/decoder optimized vanilla learning rate generator using batch normalization relu non-linearity. critic weight clipping critic trained components optimized adam learning rate increment number training loop respectively encoder decoder size increased hidden units. style adversarial classiﬁer structure learning rate employ larger generator discriminator architectures generator weighing factor critic gradient loop scheduling employed here.", "year": 2017}