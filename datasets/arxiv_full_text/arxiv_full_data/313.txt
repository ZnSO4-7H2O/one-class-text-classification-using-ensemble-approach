{"title": "Tensor network language model", "tag": ["cs.CL", "cond-mat.dis-nn", "cs.LG", "cs.NE", "stat.ML"], "abstract": "We propose a new statistical model suitable for machine learning of systems with long distance correlations such as natural languages. The model is based on directed acyclic graph decorated by multi-linear tensor maps in the vertices and vector spaces in the edges, called tensor network. Such tensor networks have been previously employed for effective numerical computation of the renormalization group flow on the space of effective quantum field theories and lattice models of statistical mechanics. We provide explicit algebro-geometric analysis of the parameter moduli space for tree graphs, discuss model properties and applications such as statistical translation.", "text": "abstract. propose statistical model suitable machine learning systems long distance correlations natural languages. model based directed acylic graph decorated multi-linear tensor maps vertices vector spaces edges called tensor network. tensor networks previously employed eﬀective numerical computation renormalization group space eﬀective quantum ﬁeld theories lattice models statistical mechanics. provide explicit algebro-geometric analysis parameter moduli space tree graphs discuss model properties applications statistical translation. previous work acknowledgements quantum statistical models quantum states pure state statistical model learning model multi-linear isometric maps tensor vertices directed multigraphs directed acyclic graphs props slicing layers geometry moduli space tree variety learning network sampling learning sampling discussion supervised model classiﬁcation tasks translation natural languages network architecture testing model references natural language processing unsupervised statistical learning language aims construct eﬃcient approximation probability measure expressions language learned sampling data set. impressive results constructing function also called continuous vector representation achieved earlier construction neural network language model continuous space word representations). model constructs range training sample language. curiously function constructed found satisfy interesting semantic syntactic linear relations english language correlation structures beyond sentence level. proposed explain long range correlations text hierarchy levels language reminds hierachical structure/renormalization group physical theories. analysis conﬁrmed long range correlations sequence integers constructed sequence words text word mapped positive integer equal rank word sorted list individual word frequencies. criticality language surprising abundance critical phenomena biology inverse correlation length mass physics terminology. formal probabilistic regular grammar almost equivalent probabilistic ﬁnite automaton hidden markov model chain matrix product state called also tensor train decomposition machine learning precise statements). models thought tensor networks based graph topology -dimensional chain constructed three-valent vertices whose output edges correspond observables. context language matrix product state model considered absence mass observed natural language implies -dimensional tensor chain models fail reproduce correctly basic statistics natural language models mass-gapped exponential decay natural language criticality power decay observation explains hidden markov models similar describe well natural languages. tensor network based graph topology tree leafs correspond observable also hierarchy tree structure example case binary tree means state vectors words construct state vector sentence words form. state vectors sentences words construct state vector sentence four words form physics corresponds idea iteratively coarse graining system many locally interacting variables resulting renormalization group space theories developed kadanoﬀ wilson fischer many others density matrix renormalization algorithm suggested turned quite eﬃcient numerical solutions quantum lattice systems. algorithms implementing kadanoﬀ-wilson-white real space renormalization group tensor tree networks developed recent survey tensor network models. however critical systems performance bare tree tensor networks models limited eﬀects remaining long range entanglement. handle entanglement dimensions hilbert spaces layers grow substantially moving higher layers nodes represent compound objects. vidal modiﬁed bare tensor tree network introducing disentangling operators neighbor blocks applying step kadanoﬀ-wilson-white density matrix renormalization group projection operation fig. tensor tree interlaced disentaglers called multiscale entanglement renormalization ansatz successfully applied study numerically many critical systems impressive precision e.g. review intuitively p-valent tree tensor networks could thought discrete models ads/cft correspondence side discrete gravity theory discrete hyperbolic geometry represented p-valent trees figure left bare tree tensor network. right mera decorated tensor tree network obtained composition tensors vertices vector space associated single incoming arrow vector space associated squares. direction arrows opposite renormalization group ﬂow. picture displays base layer length binary tree. conformal limit length base layer goes inﬁnity number layers scales paper propose quantum mera-like tensor networks statistical model language data sets observed critical phenomena long range power type correlation functions. previous work. recurrent neural networks shown long range correlation connection deep learning architectures renormalization group pointed several recent works moreover shown arithmetic circuit deep convolutional network particular case tree tensor network. linear matrix product states density matrix renormalization group context image recognition analyzed hand deep neural networks used compute correlation functions ising model deep neural networks used learn wave-function quantum system. suggest represent topological states long-range quantum entanglement neural network suggested accelerat monte carlo statistical simulations deep neural network. analysis equivalence certain restricted boltzmann machines tensor network states. acknowledgements. would like thank maxim kontsevich john terilla useful discussions. research v.p. project received funding european research council european union’s horizon research innovation program research y.v. received funding simons foundation award pure state statistical model. statistical model family probability distributions ﬁbered base space parameters parameter positive real valued function indeed µpsq real non-negative number normalization follows remark quantum physics hilbert space states u-family normed states hxψu|ψuy often called wave-function ansatz. typical problem posed deﬁnition distance probability distributions satisﬁes certain natural axioms information theory also lead standard deﬁnition entropy distribution. think morphism projection image unitary transformation deﬁned isometry unitary transformations forms group called unitary group upvq. sense isometry generalization notion unitary transformation. hermitian metric pwiqiprps pvjqjprqs induces hermitian metric tensor called isometry isometry respect induced hermitian metric. orthonormal basis means identity formally directed multigraph pvert edge vert vertices edge edges edge vert source associates edge source vertex edge vert target associates notice unlike theory quiver representations vertices decorated vector spaces edges maps vector spaces tensor network edges quiver decorated vector spaces vertices decorated multi-linear maps tensor product vector spaces incoming edges tensor product vector space outgoing edges. vertex decorated linear called tensor vertex. boundary pvert edge vert vertices edge internal edges incoming edges outgoing edges edge vert source associates edge source vertex edge vert open tensor network pγpupiqqipvertq quiver possibly boundary edge edge decorated vector space vertex vert decorated multi-linear remark tensor network could recognized feynman diagram directed graph -dimensional ﬁeld theory pair ﬁelds ˜φeq edge edge kinetic term interaction tensor vertices also graphical representation contraction tensor indices corresponding composition multi-linear maps vertices known penrose graphical notation. directed acyclic graphs props. assume directed multigraph acyclic i.e. contain directed cycles mathematical structure associates tensor network called colored prop prop generalization notion operad. operad takes several inputs returns single output prop takes element tensor product several inputs returns element tensor product several outputs. tensor network acyclic directed graph object endomorphism prop objects symmetric monoidal category remark directed multigraph contains directed cycles corresponding tensor network involves trace operation. formally structure encoded notion colored called ‘wheels’ context ‘wheeled prop’. category ﬁnite-dimensional vector spaces linear morphisms trace thus suitable build tensor network arbitrary directed graph. vector spaces. abstractly isometric tensor network thought object endomorphism prop objects category hermitian vector spaces isometric morphisms. concretely means directed acyclic multigraph tensor vertices isometries evaluation also isometry. isometric pure state tensor network model. given hilbert space whose basis length sequences symbols pure state isometric tensor network model state notice general directed acyclic graph underlying tensor network tree i.e. could multiple directed paths node node particular mera-like graph directed acyclic tree. form tensor product states intermediate edges. however observed criticality language dimension vector spaces layers needs grow sensible model. morphism layer evaluation isometric tensor network sequential composition maps starting source layer input ﬁnishing target layer output uγrls url´ls preserve inner product. consequently analogy renormalization group expect orthogonal commuting operators projections basis states bottom layer projection operators words context free choice layer expressions |largeyb|hilly |smallyb|mountainy irrelevant higher level operates within hilbert space hrls ators words deﬁne state |ψyl intermediate level tree variety. directed tree graph moduli space particular simple algebro-geometric description. first notice constraint isometry hermitian spaces figure small tree tensor network moduli space isomorphic tree variety given ﬁbration grassmanian ﬁbers grpvv base grpv grpv tautological vector grvpwq denotes grassmanian v-dimensional planes vector space dimc grvpwq c.f. directed tree single input vertex upiq single incoming input edge. therefore tree tensor network isometric constraints maps upiq vertex level symplectic moment action full automorphism group consequently generic directed tree moduli space thought generalization case moduli space ﬁbration base grpv grpv ﬁbers grpvv denote tautological vector bundles grpv grpv general tree isometric tensor network moduli space projective algebraic learning. isometric tensor network hilbert space based length sequences training sample strings eﬀective free energy function fpuq needs minimized moduli space parameters equivalent divergence observed probability standard hermitian metric basis element labelled sequence summation takes element training multiset multiplicity. since objective function fpuq additive training sample particular eﬀective approximate algorithm minimize fpuq large sample term-wise local moduli space step limited time might turn also eﬀective. recursive composition leaf tree deleting leaf. local term gradient also eﬃciently evaluated product structure evaluation morphism namely gradient components moduli parameters vertex computed pulling tangent bundle local variation upiq along composition remaining vertices supervised model classiﬁcation tasks. ‘supervised’ version algorithm also possible ‘supervised’ label relevant operator survives higher levels network general topic input text general feature relevant operators. simply another leaf input network higher level decorated vector space whose basis higher level labels. guages critical universality class suﬃciently high level network translation engine language language constructed connecting unitary transformation suﬃciently high layers isometric tensor networks describing language language projecting diﬀerent phrases equivalent meaning state scale meaning base layer language ordinary isometry vector spaces however general expansion image state basis contains many phrases weighted probability amplitudes. phrase possible translation corresponding probability |ψs|. network architecture. note simplicity assumed certain ﬁxed topology isometric tensor network. however expect natural generalization construction topology underlying graph model ﬁxed arbitrary amplitudes computed spirit feynman diagrams diﬀerent graph topologies appear. testing model. presented construction theoretical. would interesting implement suggested models study performance various types languages display critical properties", "year": 2017}