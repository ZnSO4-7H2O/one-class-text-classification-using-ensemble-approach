{"title": "Recurrent Models of Visual Attention", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.", "text": "applying convolutional neural networks large images computationally expensive amount computation scales linearly number image pixels. present novel recurrent neural network model capable extracting information image video adaptively selecting sequence regions locations processing selected regions high resolution. like convolutional neural networks proposed model degree translation invariance built-in amount computation performs controlled independently input image size. model non-differentiable trained using reinforcement learning methods learn task-speciﬁc policies. evaluate model several image classiﬁcation tasks signiﬁcantly outperforms convolutional neural network baseline cluttered images dynamic visual control problem learns track simple object without explicit training signal neural network-based architectures recently great success signiﬁcantly advancing state challenging image classiﬁcation object detection datasets excellent recognition accuracy however comes high computational cost training testing time. large convolutional neural networks typically used currently take days train multiple gpus even though input images downsampled reduce computation case object detection processing single image test time currently takes seconds running single approaches effectively follow classical sliding window paradigm computer vision literature classiﬁer trained detect object tightly cropped bounding applied independently thousands candidate windows test image different positions scales. although computations shared main computational expense models comes convolving ﬁlter maps entire input image therefore computational complexity least linear number pixels. important property human perception tend process whole scene entirety once. instead humans focus attention selectively parts visual space acquire information needed combine information different ﬁxations time build internal representation scene guiding future movements decision making. focusing computational resources parts scene saves bandwidth fewer pixels need processed. also substantially reduces task complexity object interest placed center ﬁxation irrelevant features visual environment outside ﬁxated region naturally ignored. line fundamental role guidance human movements extensively studied neuroscience cognitive science literature. low-level scene properties bottom processes play important role locations humans ﬁxate also shown strongly task speciﬁc review also e.g. paper take inspiration results develop novel framework attention-based task-driven visual processing neural networks. model considers attention-based processing model recurrent neural network processes inputs sequentially attending different locations within images time incrementally combines information ﬁxations build dynamic internal representation scene environment. instead processing entire image even bounding once step model selects next location attend based past information demands task. number parameters model amount computation performs controlled independently size input image contrast convolutional networks whose computational demands scale linearly number image pixels. describe end-to-end optimization procedure allows model trained directly respect given task maximize performance measure depend entire sequence decisions made model. procedure uses backpropagation train neural-network components policy gradient address non-differentiabilities control problem. show model learn effective task-speciﬁc strategies look several image classiﬁcation tasks well dynamic visual control problem. results also suggest attention-based model better convolutional neural network dealing clutter scaling large input images. previous work computational limitations received much attention computer vision literature. instance object detection much work dedicated reducing cost widespread sliding window paradigm focusing primarily reducing number windows full classiﬁer evaluated e.g. classiﬁer cascades removing image regions consideration branch bound approach classiﬁer output proposing candidate windows likely contain objects even though substantial speedups obtained approaches combined used add-on classiﬁers remain ﬁrmly rooted window classiﬁer design object detection exploit past information inform future processing image limited way. second class approaches long history computer vision strongly motivated human perception saliency detectors approaches prioritize processing potentially interesting image regions typically identiﬁed based measure local low-level feature contrast. saliency detectors indeed capture properties human movements typically integrate information across ﬁxations saliency computations mostly hardwired based low-level image properties only usually ignoring factors semantic content scene task demands works computer vision literature elsewhere e.g. embraced vision sequential decision task here. there work information image gathered sequentially decision attend next based previous ﬁxations image. employs learned bayesian observer model task object detection. learning framework related also employ policy gradient formulation overall setup considerably restrictive parts system learned. work perhaps similar attempts implement attentional processing deep learning framework formulation employs integrate visual information time decide however general learning procedure allows end-to-end optimization sequential decision process instead relying greedy action selection. demonstrate general architecture used efﬁcient object recognition still images well interact dynamic visual environment task-driven way. recurrent attention model paper consider attention problem sequential decision process goal-directed agent interacting visual environment. point time agent observes environment bandwidth-limited sensor i.e. never senses environment full. extract figure glimpse sensor given coordinates glimpse input image sensor extracts retina-like representation centered contains multiple resolution patches. glimpse network given location input image uses glimpse sensor extract retina representation retina representation glimpse location mapped hidden space using independent linear layers parameterized respecg combine information tively using rectiﬁed units followed another linear layer deﬁnes trainable bandwidth limited sensor components. glimpse network model architecture overall model rnn. core network model takes glimpse representation input combining internal representation previous time step produces internal state model location network action network internal state model produce next location attend action/classiﬁcation respectively. basic iteration repeated variable number steps. information local region narrow frequency band. agent however actively control deploy sensor resources agent also affect true state environment executing actions. since environment partially observed agent needs integrate information time order determine deploy sensor effectively. step agent receives scalar reward goal agent maximize total rewards. formulation encompasses tasks diverse object detection static images control problems like playing computer game image stream visible screen. game environment state would true state game engine agent’s sensor would operate video frame shown screen. environment actions would correspond joystick controls reward would reﬂect points scored. object detection static images state environment would ﬁxed correspond true contents image. environmental action would correspond classiﬁcation decision reward would reﬂect decision correct. model agent built around recurrent neural network shown fig. time step processes sensor data integrates information time chooses deploy sensor next time step sensor step agent receives observation environment form image agent full access image rather extract information bandwidth limited sensor e.g. focusing sensor region frequency band interest. paper assume bandwidth-limited sensor extracts retina-like representation around location image encodes region around high-resolution uses progressively lower resolution pixels resulting vector much lower dimensionality original image refer low-resolution representation glimpse glimpse sensor used inside call glimpse network produce glimpse feature vector internal state agent maintains interal state summarizes information extracted history past observations; encodes agent’s knowledge environment instrumental deciding deploy sensor. internal state formed hidden units recurrent neural network updated time core network external input network glimpse feature vector actions step agent performs actions decides deploy sensor sensor control environment action might affect state environment. nature environment action depends task. work location actions chosen stochastically distribution parameterized location network time environment action similarly drawn distribution conditioned second network output classiﬁcation formulated using softmax output dynamic environments exact formulation depends action deﬁned particular environment reward executing action agent receives visual observation environment reward signal rt+. goal agent maximize reward signal case object recognition example object classiﬁed correctly steps otherwise. setup special instance known community partially observable markov decision process true state environment unobserved. view agent needs learn policy π|st; parameters that step maps history past interactions environment distribution actions current time step subject constraint sensor. case policy deﬁned outlined above history summarized state hidden units describe speciﬁc choices components section training parameters agent given parameters glimpse network core network action network learn maximize total reward agent expect interacting environment. formally policy agent possibly combination dynamics environment induces distribution possible interaction sequences maximize reward distribution depends policy maximizing exactly non-trivial since involves expectation high-dimensional interaction sequences turn involve unknown environment dynamics. viewing problem pomdp however allows bring techniques literature bear shown williams sample approximation gradient given learning rule also known reinforce rule involves running agent current policy obtain samples interaction sequences adjusting parameters agent log-probability chosen actions high cumulative reward increased actions produced reward decreased. requires compute variance reduction equation provides unbiased estimate gradient high variance. therefore common consider gradient estimate form baseline depend itself. estimate equal expectation lower variance. natural select form baseline known value function reinforcement learning literature. resulting algorithm increases log-probability action followed larger expected cumulative reward decreases probability obtained cumulative reward smaller. type baseline learn reducing squared error using hybrid supervised loss algorithm described allows train agent best actions unknown learning signal provided reward. instance know priori sequence ﬁxations provides information unknown image total reward episode give indication whether tried sequence good bad. however situations know correct action take instance object detection task agent output label object ﬁnal action. training images label known directly optimize policy output correct label associated training image observation sequence. achieved common supervised learning maximizing conditional probability true label given observations image i.e. maximizing associated image observations obtained. follow approach classiﬁcation problems optimize cross entropy loss train action network backpropagate gradients core glimpse networks. location network always trained reinforce. experiments evaluated approach several image classiﬁcation tasks well simple game. ﬁrst describe design choices common experiments retina location encodings retina encoding extracts square patches centered location ﬁrst patch pixels size successive patch twice width previous. patches resized concatenated. glimpse locations encoded real-valued coordinates center image left corner glimpse network glimpse network fully connected layers. linear denote linear transformation vector i.e. linear weight matrix bias vector rect rectiﬁer nonlinearity. output glimpse network deﬁned rect linear) rect)) rect). dimensionality dimensionality attention models trained paper. location network policy locations deﬁned two-component gaussian ﬁxed variance. location network outputs mean location policy time deﬁned linear state core network/rnn. core network classiﬁcation experiments follow core network rectiﬁer units deﬁned rect linear). experiment done dynamic environment used core lstm units table classiﬁcation results mnist translated mnist datasets. denotes fullyconnected network layers rectiﬁer units. convolutional network layer ﬁlters stride followed fully connected layer units rectiﬁers layer. instances attention model labeled number glimpses number scales retina size retina. attention network used following classiﬁcation experiments made classiﬁcation decision last timestep action network simply linear softmax classiﬁer deﬁned normalizing constant. state vector dimensionality methods trained using stochastic gradient descent momentum hyperparameters learning rate variance location policy selected using random search reward last time step agent classiﬁed correctly otherwise. rewards timesteps centered digits ﬁrst tested ability training method learn successful glimpse policies using train models glimpses mnist digits dataset. retina experiment simply patch enough capture part digit hence experiment also tested ability combine information multiple glimpses. note since ﬁrst glimpse always random single glimpse model effectively classiﬁer gets single random patch input. also trained standard feedforward neural network hidden layers rectiﬁed linear units baseline. error rates achieved different models test shown table additional glimpse improves performance reaches minimum glimpses matches performance fully connected model training full centered digits. demonstrates model successfully learn combine information multiple glimpses. non-centered digits second problem considered classifying non-centered digits. created task called translated mnist data generated placing mnist digit random location larger blank patch. training cases generated effective training size multiplied possible number locations. figure contains random sample test cases translated mnist task. table shows results several different models trained translated mnist task patches. addition fully-connected networks also trained network convolutional layer ﬁlters stride followed rectiﬁer nonlinearity fully-connected layer rectiﬁer units. convolutional network networks smaller fully connected model roughly number parameters. since convolutional network degree translation invariance built table classiﬁcation cluttered translated mnist dataset. denotes fully-connected network layers rectiﬁer units. convolutional network layer ﬁlters stride followed fully connected layer units case units case rectiﬁers layer. instances attention model labeled number glimpses size retina number scales retina. models except fully connected network roughly number parameters. figure examples learned policy cluttered-translated mnist task. column input image glimpse path overlaid green. columns glimpses network chooses. center image shows full resolution glimpse outer resolution areas obtained upscaling resolution glimpses back full image size. glimpse paths clearly show learned policy avoids computation empty noisy parts input space directly explores area around object interest. attains signiﬁcantly lower error rate fully connected networks. however glimpses gets roughly performance convolutional network outperforms glimpses reaching roughly error. possible attention model focus retina digit hence learn translation invariant policy. experiment also shows attention model able successfully search object image object centered. cluttered non-centered digits challenging aspects classifying real-world images presence wide range clutter. systems operate entire image full resolution particularly susceptible clutter must learn invariant possible advantage attention mechanism make easier learn presence clutter focusing relevant part image ignoring irrelevant part. test hypothesis several experiments task call cluttered translated mnist. data task generated ﬁrst placing mnist digit random location larger blank image adding random subpatches random mnist digits random locations image. goal classify complete digit present image. figure shows random sample test cases cluttered translated mnist task. table shows classiﬁcation results models trained cluttered translated mnist pieces clutter. presence clutter makes task much difﬁcult performance attention model affected less performance models. glimpses reaches error outperforms fully-connected models wide margin convolutional neural network trained glimpses achieves even lower error. since achieves larger relative error improvements convolutional network presence clutter results suggest attention-based models better dealing clutter convolutional networks simply ignore looking samples learned policy shown figure included supplementary materials. ﬁrst column shows original data point glimpse path overlaid. location ﬁrst glimpse marked ﬁlled circle location ﬁnal glimpse marked empty circle. intermediate points path traced solid straight lines. consecutive image right shows representation glimpse network sees. seen learned policy reliably explore around object interest avoiding clutter time. test hypothesis also performed experiments cluttered translated mnist pieces clutter. test errors achieved models compared shown table results show similar improvements convolutional network. noted overall capacity amount computation model change images whereas hidden layer convolutional network connected linear layer grows linearly number pixels input. dynamic environments appealing property recurrent attention model applied videos interactive problems visual input easily static image tasks. test ability approach learn control policy dynamic visual environment perceiving environment bandwidth-limited retina training play simple game. game played screen binary pixels involves objects single pixel represents ball falling screen bouncing sides screen two-pixel paddle positioned bottom screen agent controls catching ball. falling pixel reaches bottom screen agent either gets reward paddle overlaps ball reward otherwise. game restarts beginning. trained recurrent attention model play game catch using ﬁnal reward input. network retina three scales input means agent capture ball highest resolution region order know precise position. addition location actions attention model three game actions action network used linear softmax model distribution game actions. used core network lstm units. performed random search suitable hyper-parameters trained agent million frames. video best agent catches ball roughly time downloaded http//www.cs.toronto.edu/˜vmnih/docs/attention.mov. video shows recurrent attention model learned play game tracking ball near bottom screen. since agent told track ball rewarded catching result demonstrates ability model learn effective task-speciﬁc attention policies. discussion paper introduced novel visual attention model formulated single recurrent neural network takes glimpse window input uses internal state network select next location focus well generate control signals dynamic environment. although model differentiable proposed uniﬁed architecture trained end-to-end pixel inputs actions using policy gradient method. model several appealing properties. first number parameters amount computation performs controlled independently size input images. second model able ignore clutter present image centering retina relevant regions. experiments show signiﬁcantly outperforms convolutional architecture comparable number parameters cluttered object classiﬁcation task. additionally ﬂexibility approach allows number interesting extensions. example network augmented another action allows terminate time point make ﬁnal classiﬁcation decision. preliminary experiments show allows network learn stop taking glimpses enough information make conﬁdent classiﬁcation. network also allowed control scale retina samples image allowing objects different size ﬁxed size retina. cases extra actions simply added action network trained using policy gradient procedure described. given encouraging results achieved applying model large scale object recognition video classiﬁcation natural direction future work. figure examples learned policy cluttered-translated mnist task. column input image mnist test glimpse path overlaid green columns glimpses network chooses. center image shows full resolution glimpse outer resolution areas obtained upscaling resolution glimpses back full image size. glimpse paths clearly show learned policy avoids computation empty noisy parts input space directly explores area around object interest. figure examples learned policy cluttered-translated mnist task. column input image mnist test glimpse path overlaid green columns glimpses network chooses. center image shows full resolution glimpse outer resolution areas obtained upscaling resolution glimpses back full image size. glimpse paths clearly show learned policy avoids computation empty noisy parts input space directly explores area around object interest. figure examples learned policy cluttered-translated mnist task. column input image mnist test glimpse path overlaid green columns glimpses network chooses. center image shows full resolution glimpse outer resolution areas obtained upscaling resolution glimpses back full image size. glimpse paths clearly show learned policy avoids computation empty noisy parts input space directly explores area around object interest. references bogdan alexe thomas deselaers vittorio ferrari. object? cvpr bogdan alexe nicolas heess whye vittorio ferrari. searching objects driven nicholas butko javier movellan. optimal scanning faster object detection. cvpr n.j. butko j.r. movellan. i-pomdp infomax model movement. proceedings ranzato. learning look. arxiv e-prints ronald rensink. dynamic representation scenes. visual cognition pierre sermanet david eigen xiang zhang micha¨el mathieu fergus yann lecun. overfeat integrated recognition localization detection using convolutional networks. corr abs/. kenneth stanley risto miikkulainen. evolving roving gecco richard sutton david mcallester satinder singh yishay mansour. policy gradient methods antonio torralba aude oliva monica castelhano john henderson. contextual guidance movements attention real-world scenes role global features object search. psychol pages", "year": 2014}