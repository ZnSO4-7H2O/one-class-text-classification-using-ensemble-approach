{"title": "Advances in Very Deep Convolutional Neural Networks for LVCSR", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Very deep CNNs with small 3x3 kernels have recently been shown to achieve very strong performance as acoustic models in hybrid NN-HMM speech recognition systems. In this paper we investigate how to efficiently scale these models to larger datasets. Specifically, we address the design choice of pooling and padding along the time dimension which renders convolutional evaluation of sequences highly inefficient. We propose a new CNN design without timepadding and without timepooling, which is slightly suboptimal for accuracy, but has two significant advantages: it enables sequence training and deployment by allowing efficient convolutional evaluation of full utterances, and, it allows for batch normalization to be straightforwardly adopted to CNNs on sequence data. Through batch normalization, we recover the lost peformance from removing the time-pooling, while keeping the benefit of efficient convolutional evaluation. We demonstrate the performance of our models both on larger scale data than before, and after sequence training. Our very deep CNN model sequence trained on the 2000h switchboard dataset obtains 9.4 word error rate on the Hub5 test-set, matching with a single model the performance of the 2015 IBM system combination, which was the previous best published result.", "text": "along time dimension). networks pool time stride higher layers network applied time-padding throughout. allowed elegant design analogous networks computer vision. however leads inefﬁcient convolution full sequences making sequence training infesible large data sets. paper explore alternatives design choice. importantly focus designs allow efﬁcient convolutional evaluation full utterances essential sequence training test time contributions paper demonstrate convolutional models without timepooling time-padding slightly suboptimal recognition accuracy allow fast convolutional evaluation sequences enable sequence training large data sets demonstrate merit batch normalization acoustic models technique accelerate training improve generalization normalizing internal representations inside network. show order batch normalization cnns sequence training important train several utterances time. since feasible efﬁcient architectures only batch normalization gives essentially compensate lost performance following time-padding design principle. pooling stride essential element cnns computer vision downsampling reduces grid size building invariance local geometric transformations. acoustic cnns pooling readily applied along frequency dimension help build invariance small spectral variation. deepest -layer reduce frequency-dimension size pooling second fourth seventh tenth convolutional layer convolutions zero-padded order preserve size frequency dimension. along time dimension application pooling less straightforward. argued downsampling time avoided rather pooling stride used downsample time. however pool stride pooling layers. design cnns zero-padding applied along time frequency directions pooling operation reduces context window original size ﬁnal size design directly analogous deep cnns small kernels recently shown achieve strong performance acoustic models hybrid nn-hmm speech recognition systems. paper investigate efﬁciently scale models larger datasets. speciﬁcally address design choice pooling padding along time dimension renders convolutional evaluation sequences highly inefﬁcient. propose design without timepadding without timepooling slightly suboptimal accuracy signiﬁcant advantages enables sequence training deployment allowing efﬁcient convolutional evaluation full utterances allows batch normalization straightforwardly adopted cnns sequence data. batch normalization recover lost peformance removing time-pooling keeping beneﬁt efﬁcient convolutional evaluation. demonstrate performance models larger scale data before sequence training. deep model sequence trained switchboard dataset obtains word error rate test-set matching single model performance system combination previous best published result. index terms convolutional networks acoustic modeling speech recognition neural networks present advances results using deep convolutional networks acoustic model large vocabulary continuous speech recognition extending earlier work introduced deep convolutional network architectures lvcsr hybrid nn-hmm framework input window frames frame logmel feature vector output produce vector probabilities states center frame window. presented strong results babel low-resource task -hour switchboard- dataset cross-entropy training only. deep convolutional networks inspired architecture introduced imagenet classiﬁcation challenge. central idea networks replace layers large convolutional kernels stack layers small kernels. receptive ﬁeld created less parameters nonlinearity. furthermore applying zero-padding throughout reducing spatial resolution strided pooling networks simple elegant. followed design acoustic model cnns however important design choice remained unexplored accuracy computational efﬁciency impact time-pooling time-padding time-dimension shown horizontally depth shown vertically frequency dimension number feature maps indicated color shades. pooling frequency implicitly understood transitions color shades. original architecture starting -frame window. pool time rather leave time-padding layers. reduces size along time-direction size using highest convolutional layers. want remove time-padding pooling altogether need start larger context window architecture allows efﬁcient convolutional evaluation full utterances batch normalization table part hub’ testset architectures column titles show training dataset method cross-entropy training initialize networks cross-entropy trained swb- *new results refers annealed dropout. apart practical reason justify design choice hypothesize downsampling time advantage disadvantage. advantage higher layers network able access context learn useful invariants time. argued feature detected lower layers exact location matter much anymore blurred long approximate relative position conserved. disadvantage resolution reduced neighboring different states distinguished could possibly hurt performance. section empirically investigate whether pooling time justiﬁed. figure summarizes three variations -layer architecture original version figure shows alternative pooling time. reduce context original size size want absorb fully connected layer simply omit time-padding layers needed table compare results withtimepooling. architecture time-pooling outperforms architecture sequence training training hours. result hours sequence training matches system combination classical state system combination result. also note number better baselines gains less. explained fact stochastic rather sequence training leaves opportunity improvement. efﬁcient convolution full utterances sequence training test time desirable process utterance convolutional originally described foundational papers classical cnns never problem. however best performing deep introduced padding pooling along time dimension different prior architectures lvcsr. destroys desirable property able process full utterance once outputting dense prediction i.e. state probability frame utterance pool time stride number frames output reduced factor pooling times results reduction factor ﬁrst network means utterance length uttlen number output frames uttlen obviously avoid need splice utterance sequence training propose design ﬁgure timepadding pooling therefore takes larger temporal context window. sequence training model efﬁciently convolve full utterance. increased context lower layers gives slightly increased computational cost training however architecture architecture allows efﬁcient sequence training deployment results network bottom table results signiﬁcantly different h-st discuss next section next computational efﬁciency another advantage architecture fact architecture allows modiﬁed version batch normalization sequence training time. batch normalization technique accelerate training improve generalization gained traction deep learning community. idea standardize internal representations inside network helps network converge faster generalize better inspired whitening network input improves performance. implemented standardizing output layer applying nonlinearity using local mean variance computed minibatch correcting learned variance bias term mean variance cheap simple-to-implement stochastic approximation data statistics speciﬁc layer current network weights. since test time want able inference without presence minibatch prescribed method accumulate running average mean variance training used test time. cnns mean variance computed samples spatial location. standard formulation cnns readily applied cross-entropy training minibatch contains samples different utterances different targets different speakers. however sequence training spliced evaluation ﬁgure problematic. construct minibatch consecutive windows utterance minibatch mean variance reasons drastically improve mean variance architecture allows efﬁcient convolutional processing full utterance like ﬁgure case issues solved ﬁrstly duplication splicing. secondly several utterances processed minibatch since memory. figure ways evaluating full utterance splicing different samples minibatch efﬁcient convolutional evaluation treating full utterance single sample. spliced duplicates amount input factor context size. efﬁcient convolutional evaluation duplicate input computation rather takes full utterance single input sample produces correspondingly large feature maps intermediate convolutional layers. table training speed deep cnns frames second numbers average experiments cudnnv-based torch implementation executed single nvidia tesla including overhead data loading host-device transfer. architecture omitted because computational cost similar architecture problem time padding subtle. consider ﬁrst convolutional layer zero padding time makes edges dependent location window. shifting frame predict next timestep values edges truncated zero padding recomputed. second convolutional layer zero padding changes outer edge values third layer three outer edge values modiﬁed etc. illustrated ﬁgure light squares edges zero-padding time. dashed line indicates output values modiﬁed timepadding compared network without timepadding. modiﬁcation travels inwards deeper network. everything outside dashed lines modiﬁed timepadding speciﬁc window location. obvious still obtain output frame architecture pooling padding time call splicing utterance uttlen different samples minibatch sample window want output. depicted ﬁgure regular full efﬁcient convolution ﬁgure amount computation multiplied factor sequence training testing network. maximize number frames processed minibatch order mean variance become better estimate. achieve matching number utterances minibatch utterance length algorithm batch assembly expressed pseudo-code table shows results architectural variants without expected architecture batch normalization obtain good performance sequence training since resort spliced evaluation. performance worse withbn. contrast using efﬁcient convolutional evaluation architecture using batch normalization improves performance swb- matching performance superior architecture adding brings almost matching result conclude model recovers lost performance model follow hybrid setup states logmel features window size described ﬁgure work done using torch environment sequence training sgdbased batch size described section weight penalty training found work best networks without nesterov accelerated gradient learning rate momentum networks ﬁxed learning rate decay scheme divides learning rate frames. deal class imbalance adopt balanced sampling sampling context depeni dent state probability keep throughout experiments. cnns become dominant approach solving largescale machine learning problems natural data example computer vision speech recognition recently also character-level text classiﬁcation language modeling deep nets small ﬁlters excel imagenet classiﬁcation shown transfer well different tasks like neural image captioning object detection semantic segmentation etc. pooling time applied early work time delay prior work cnns lvcsr explored non-strided pooling time contrast strided pooling subsample time obtains superior performance. cnns strided pooling time succesfully used small-footprint keyword spotting learning ﬁlterbanks signal endto-end ctc-based model sequence training introduced neural network training performed stochastic sequence training opposed hessian-free sequence training used baselines mentioned section smoothed loss loss used nesterov accelerated gradient optimization method batch normalization introduced closely related prior work aimed whitening activations inside network shown improve imagenet classiﬁcation performance googlenet architecture residual networks submissions imagenet classiﬁcation competition. applying batch normalization sequence data stacking multiple utterances batch computing mean variance stastistics identical applied recurrent neural networks paper demonstrated strength deep convolutional networks applied speech recognition hybrid nn-hmm framework. obtain sequence training hour switchboard dataset single model matches performance state model combination dnn+rnn+cnn model combined state acoustic model better language models obtains signiﬁcantly better performance published model naturally raises question whether combine best both pool time like architecture efﬁcient convolutional evaluation like architecture possible architecture pool time compensates lost resolution either applying offsets proposed using non-strided pooling sparse kernels along time dimension elegant technique independently proposed leave future work. waibel hanazawa hinton shikano lang phoneme recognition using time-delay neural networks acoustics speech signal processing ieee transactions vol. palaz collobert doss estimating phoneme class conditional probabilities speech signal using convolutional neural networks arxiv preprint arxiv. amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos deep speech end-to-end speech recognition english mandarin corr arxiv. kingsbury sainath soltau scalable minimum bayes risk training deep neural network acoustic models using distributed hessian-free optimization thirteenth annual conference international speech communication association sainath a.-r. mohamed kingsbury ramabhadran deep convolutional neural networks lvcsr acoustics speech signal processing ieee international conference seide error back propagation sequence training context-dependent deep networks conversational speech transcription acoustics speech signal processing ieee international conference ieee sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional networks arxiv preprint arxiv. abdel-hamid a.-r. mohamed jiang penn applying convolutional neural networks concepts hybrid nn-hmm model speech recognition acoustics speech signal processing ieee international conference ieee jernite sontag rush character-aware neural language models arxiv preprint arxiv. kiros courville salakhutdinov zemel bengio show attend tell neural image caption generation visual attention arxiv preprint arxiv.", "year": 2016}