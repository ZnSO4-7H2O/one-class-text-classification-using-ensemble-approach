{"title": "Structural Attention Neural Networks for improved sentiment analysis", "tag": ["cs.CL", "cs.NE"], "abstract": "We introduce a tree-structured attention neural network for sentences and small phrases and apply it to the problem of sentiment classification. Our model expands the current recursive models by incorporating structural information around a node of a syntactic tree using both bottom-up and top-down information propagation. Also, the model utilizes structural attention to identify the most salient representations during the construction of the syntactic tree. To our knowledge, the proposed models achieve state of the art performance on the Stanford Sentiment Treebank dataset.", "text": "introduce tree-structured attention neural network sentences small phrases apply problem sentiment classiﬁcation. model expands current recursive models incorporating structural information around node syntactic tree using bottomtop-down information propagation. also model utilizes structural attention identify salient representations construction syntactic tree. knowledge proposed models achieve state performance stanford sentiment treebank dataset. sentiment analysis deals assessment opinions speculations emotions text relatively recent research area attracted great interest demonstrated series shared evaluation tasks e.g. analysis tweets affective ratings unknown words predicted utilizing affective ratings small words semantic relatedness between unknown seed words. example sentence-level analysis proposed application areas include detection public opinion prediction election results correlation mood states stock market indices applied various natural language processing tasks. tree structured neural networks found literature recursive neural networks hold linguistic interest close relation syntactic structures sentences able capture distributed information structure logical terms. syntactic structures n-ary trees represent either underlying structure sentence known constituency trees relations between words known dependency trees. paper focuses sentence-level sentiment classiﬁcation movie reviews using syntactic parse trees input proposed networks. order solve task sentiment analysis sentences work upon variant recursive neural networks recursively create representation following syntactic structure. proposed computation model exploits information subnodes well parent nodes node examination. neural network referred bidirectional recursive network model enhanced memory units proposed structural attention mechanism. observed different nodes tree structure hold information variable saliency. nodes tree equally informative proposed model selectively weights contribution node regarding sentence level representation using structural attention model. evaluate approach sentence-level sentiment classiﬁcation task using standard movie review dataset experimental results show proposed model outperforms state-of-the methods. bidirectional treegru natural extension tree-structure addition bidirectional approach. treegrus calculate activation node previously computed activations lying lower tree structure. bidirectional approach tree structure uses information lower nodes tree particular node manner newly calculated activation incorporates content children parent particular node. bidirectional neural network trained seperate phases upward phase downward phase. upward phase network topology similar topology treegru every activation calculated based previously calculated activations found lower structure bottom fashion. every activation computed leaves root root activation used input downward phase. downward phase calculates activations every child node using content parent fashion. process computing internal representations phases separated ﬁrst pass network compute upward activation completed downward representations computed. upward activation similarly treegru node interpolation previous calculated activation child recursive grus upon tree structures extension sequential grus allow information propagate network topologies. similar recursive lstm network tree structures every node tree tree-gru gating mechanisms modulate information inside unit without need separate memory cell. activation tree-gru node interpolation previous calculated activation child total children canvector dimensionality. sigmoid function non-linear tanh function.the matrices rdxd used trainable weight parameters connect children node representation node representation input vector proposed attention model applied structural content since node representations contain syntactic structural information training because recursive nature network topology. evaluate performance aforementioned models task sentiment classiﬁcation sentences sampled movie reviews. stanford sentiment treebank dataset contains sentiment labels every syntactically plausible phrase train/dev/test sentences. phrase labeled respect -class sentiment value i.e. negative negative neutral positive positive. dataset also used binary classiﬁcation subtask excluding neutral phrases original splits. binary classiﬁcation subtask evaluated train/dev/test splits. sentiment classiﬁcation aforementioned architectures node softmax classiﬁer predict sentiment label ˆyj. example predicted label corresponds sentiment class spanned phrase produced node classiﬁer unidirectional treegru architectures uses hidden state produced recursive computations till node using input nodes predict label follows classiﬁer bidirectional treebigru architectures uses hidden state produced recursive computations till node upward downward phase using input nodes predict label follows tmax extracts informative nodes syntactic tree aggregates representation nodes order form sentence vector. feed representation node one-layer multilayer perceptron rdxd weight matrix hidden representation probability input layer softmax layer. word embeddings initialized using public available glove vectors dimensionality. glove vectors provide coverage dataset. initialized word vectors ﬁnetuned training process along every parameter. every matrix initialized identity matrix multiplied except matrices softmax layer attention layer randomly initialized normal gaussian distribution. every bias vectors initialized zeros. training process lasts epochs. training evaluate network times every epoch keep parameters give best root accuracy development dataset. short paper propose extension recursive neural networks incorporates bidirectional approach gated memory units well attention model structure level. proposed models evaluated ﬁnegrained binary sentiment classiﬁcation tasks sentence level. results indicate direction computation attention structural level enhance performance neural networks sentiment analysis task. structural attention models ﬁnal sentence representation predict sentiment label corresponding root node sentence. cost function used negative log-likelihood ground-truth label node results evaluation results presented table terms accuracy several state-of-the-art models proposed literature well treegru treebigru models proposed work. among approaches reported literature highest accuracy yielded drnn binary scheme ﬁne-grained scheme observe best performance achieved treebigru attention binary ﬁne-grained evaluation metrics exceeding previously reported results. addition attentional mechanism employed proposed treegru treebigru models improve performance evaluation metrics. evaluated models trained using adagrad algorithm using learning rate minibatch size sentences. l-regularization performed model parameters value dropout richard socher jeffrey pennington eric huang andrew christopher manning. semi-supervised recursive autoencoders predictproceedings sentiment distributions. conference empirical methods natural language processing emnlp pages stroudsburg usa. association computational linguistics. richard socher brody huval christopher manning andrew semantic compositionality recursive matrix-vector spaces. proceedings joint conference empirical methods natural language processing computational natural language learning emnlp-conll pages association computational linguistics. richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment proceedings conference treebank. empirical methods natural language processing john duchi elad hazan yoram singer. adaptive subgradient methods online learning stochastic optimization. technical report eecs department university california berkeley. ankit kumar ozan irsoy jonathan james bradbury robert english brian pierce peter ondruska ishaan gulrajani richard socher. anything dynamic memory networks natural language processing. corr abs/.. nikolaos malandrakis kazemzadeh alexandros potamianos shrikanth narayanan. sail prohybrid approach sentiment analysis. ceedings semeval pages tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionaladvances neural information processing ity. systems pages preslav nakov alan ritter sara rosenthal fabrizio sebastiani veselin stoyanov. semeval task sentiment analysis twitter. proceedings international workshop semantic evaluation kartik singhal basant agrawal namita mittal. modeling indian general elections sentiment information analysis political twitter data. systems design intelligent applications pages", "year": 2017}