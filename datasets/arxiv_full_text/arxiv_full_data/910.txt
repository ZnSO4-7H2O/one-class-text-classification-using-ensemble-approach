{"title": "Stochastic Pooling for Regularization of Deep Convolutional Neural  Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.", "text": "introduce simple effective method regularizing large convolutional neural networks. replace conventional deterministic pooling operations stochastic procedure randomly picking activation within pooling region according multinomial distribution given activities within pooling region. approach hyper-parameter free combined regularization approaches dropout data augmentation. achieve state-of-the-art performance four image datasets relative approaches utilize data augmentation. neural network models prone over-ﬁtting high capacity. range regularization techniques used prevent this weight decay weight tying augmentation training transformed copies allow training larger capacity models would otherwise possible yield superior test performance compared smaller unregularized models. dropout recently proposed hinton another regularization approach stochastically sets half activations within layer zero training sample training. shown deliver signiﬁcant gains performance across wide range problems although reasons efﬁcacy fully understood. drawback dropout seem beneﬁts convolutional layers common many networks designed vision tasks. paper propose novel type regularization convolutional layers enables training larger models without over-ﬁtting produces superior performance recognition tasks. idea make pooling occurs convolutional layer stochastic process. conventional forms pooling average deterministic latter selecting largest activation pooling region. stochastic pooling selected activation drawn multinomial distribution formed activations within pooling region. alternate view stochastic pooling equivalent standard pooling many copies input image small local deformations. similar explicit elastic deformations input images delivers excellent mnist performance. types data augmentation ﬂipping cropping differ global image transformations. furthermore using stochastic pooling multi-layer model gives exponential number deformations since selections higher layers independent below. stochastic pooling scheme designed standard convolutional neural network architecture. ﬁrst review model along conventional pooling schemes introducing novel stochastic pooling approach. classical convolutional network composed alternating layers convolution pooling ﬁrst convolutional layer extract patterns found within local regions input images common throughout dataset. done convolving template ﬁlter input image pixels computing inner product template every location image outputting feature ﬁlter layer. output measure well template matches portion image. non-linear function applied element-wise feature resulting activations passed pooling layer. aggregates information within small local regions producing pooled feature output. denoting aggregation function pool feature have pooling region feature index element within motivation behind pooling activations pooled less sensitive precise locations structures within image original feature multi-layer model convolutional layers take pooled maps input thus extract features increasingly invariant local transformations input image. important classiﬁcation tasks since transformations obfuscate object identity. range functions used tanh logistic functions popular choices. paper linear rectiﬁcation function non-linearity. general shown signiﬁcant beneﬁts tanh logistic functions. however especially suited pooling mechanism since formulation involves non-negativity elements pooling regions clipping negative responses introduces zeros pooling regions ensuring stochastic sampling selecting speciﬁc locations rather possible locations region. types pooling drawbacks training deep convolutional networks. average pooling elements pooling region considered even many magnitude. combined linear rectiﬁcation non-linearities effect down-weighting strong activations since many zero elements included average. even worse tanh non-linearities strong positive negative activations cancel leading small pooled responses. pooling suffer drawbacks easily overﬁts training practice making hard generalize well test examples. proposed pooling scheme advantages pooling stochastic nature helps prevent over-ﬁtting. stochastic pooling select pooled response sampling multinomial distribution formed activations pooling region. precisely ﬁrst compute probabilities region normalizing activations within region procedure illustrated fig. samples pooling region layer training example drawn independently another. back-propagating network selected location used direct gradient back pooling region analogous back-propagation pooling. pooling captures strongest activation ﬁlter template input region. however additional activations pooling region taken account passing information network stochastic pooling ensures non-maximal activations also utilized. figure example illustrating stochastic pooling. input image. convolutional ﬁlter. rectiﬁed linear function. resulting activations within given pooling region. probabilities based activations. sampled activation. note selected element pooling region largest element. stochastic pooling thus represent multi-modal distributions activations within region. using stochastic pooling test time introduces noise network’s predictions found degrade performance instead probabilistic form averaging. this activations region weighted probability summed differs standard average pooling element potentially different weightai rather pooling region size |rj|. practice using conventional average pooling results huge performance drop probabilistic weighting viewed form model averaging setting locations pooling regions deﬁnes model. training time sampling locations produces model since connection structure throughout network modiﬁed. test time using probabilities instead sampling effectively estimate averaging possible models without instantiate them. given network architecture different pooling regions size number possible models range typically example signiﬁcantly larger number model averaging occurs dropout always section conﬁrm using probability weighting achieves similar performance compared using large number model instantiations requiring pass network. using probabilities sampling training time weighting activations test time leads state-of-the-art performance many common benchmarks demonstrate. figure selection images datasets evaluated. shows images bottom preprocessed versions images used training. cifar datasets show slight changes subtracting pixel mean whereas svhn almost indistinguishable original images. prompted local contrast normalization normalize extreme brightness variations color changes svhn. experiments compare method average pooling variety image classiﬁcation tasks. experiments mini-batch gradient descent momentum optimize cross entropy network’s prediction class ground truth labels. given parameter time weight updates added parameters .∆xt− gradient cost function respect parameter time averaged batch learning rate hand. experiments conducted using extremely efﬁcient convolution library wrapped matlab using gpumat allowed rapid development experimentation. begin network layout hinton dropout work convolutional layers ﬁlters feature maps layer rectiﬁed linear units outputs. model train epochs experiments aside additional model section feature maps layer trained epochs. unless otherwise speciﬁed pooling stride pooling layers. additionally pooling layer response normalization layer normalizes pooling outputs location subset neighboring feature maps. typically helps training suppressing extremely large outputs allowed rectiﬁed linear units well helps neighboring features communicate. finally single fully-connected layer soft-max outputs produce network’s class predictions. applied model four different datasets mnist cifar- cifar- street view house numbers fig. examples images. begin experiments cifar- dataset convolutional networks methods dropout known work well dataset composed classes natural images training examples total class. image image size taken tiny images dataset labeled hand. dataset scale follow hinton approach subtracting per-pixel mean computed dataset image shown fig. figure cifar- train test error rates throughout training average stochastic pooling. average pooling test errors plateau methods overﬁt. stochastic pooling training error remains higher test errors continue decrease. cross-validating cifar- training images found good value learning rate convolutional layers ﬁnal softmax output layer. rates annealed linearly throughout training original values. additionally found small weight decay optimal applied layers. hyperparameter settings found cross-validation used datasets experiments. using network architecture described above trained three models using average stochastic pooling respectively compare performance. fig. shows progression train test errors training epochs. stochastic pooling avoids over-ﬁtting unlike average pooling produces less test errors. table compares test performance three pooling approaches current state-of-the-art result cifar- uses data augmentation adds dropout additional locally connected layer stochastic pooling surpasses result using architecture without requiring locally connected layer. determine effect pooling region size behavior system stochastic pooling compare cifar- train test performance pooling sizes throughout network fig. optimal size appears smaller regions overﬁtting larger regions possibly noisy training. sizes stochastic pooling outperforms average pooling. mnist digit classiﬁcation task composed images handwritten digits training images test images benchmark. images scaled perform pre-processing. training error using stochastic pooling pooling dropped quickly latter completely overﬁt training data. weight decay prevented average pooling over-ﬁtting inferior performance methods. table compares three pooling approaches state-of-the-art methods mnist also utilize convolutional networks. stochastic pooling outperforms methods data augmentation methods jittering elastic distortions current state-of-the-art single model approach ciresan uses elastic distortions augment original training set. stochastic pooling different type regularization could combined data augmentation improve performance. train error test error cifar- dataset another subset tiny images dataset classes training examples total test examples. cifar scale subtract per-pixel mean image shown fig. limited number training examples class typical pooling methods used convolutional networks perform well shown table stochastic pooling outperforms methods preventing over-ﬁtting surpasses believe state-of-the-art method table cifar- classiﬁcation performance various pooling methods compared stateof-the-art method based receptive ﬁeld learning. street view house numbers street view house numbers dataset composed images test images goal task classify digit center cropped color image. difﬁcult real world problem since multiple digits visible within image. practical application classify house numbers throughout google’s street view database images. siﬁcation difﬁcult. instead utilized local contrast normalization three channels pre-process images fig. normalized brightness color variations helped training proceed quickly relatively large dataset. despite signiﬁcant amounts training data large convolutional network still overﬁt. dataset train additional model epochs feature maps layers respectively. stochastic pooling helps prevent overﬁtting even large model despite training long time. existing state-of-theart dataset multi-stage convolutional network sermanet stochastic pooling beats multi-stage conv. -layer classiﬁer multi-stage conv. -layer classifer padding pooling pooling stochastic pooling pooling pooling stochastic pooling table svhn classiﬁcation performance various pooling methods model layer feature maps compared state-of-the-art results without data augmentation. illustrate ability stochastic pooling prevent over-ﬁtting reduced training size minst cifar- datasets. fig. shows test performance training random selection half full training set. cases stochastic pooling overﬁts less pooling approaches. analyze importance stochastic sampling training time probability weighting test time different methods pooling training testing cifar- choosing locations stochastically test time degrades performance slightly could expected however still outperforms models average pooling used test time. conﬁrm probability weighting valid approximation averaging many models draw samples pooling locations throughout network average output probabilities models increases results approach probability weighting method obvious downside n-fold increase computations. using model trained average pooling using stochastic pooling test time performs poorly. suggests training stochastic pooling incorporates non-maximal elements sampling noise makes model robust test time. furthermore nonmaximal elements utilized correctly scale produced pooling function correct average pooling used test time drastic performance seen. using probability weighting training network easily over-ﬁts performs suboptimally test time using pooling methods. however beneﬁts probability weighting test time seen model speciﬁcally trained utilize either probability weighting stochastic pooling training time. train method stochastic pooling stochastic pooling stochastic pooling stochastic pooling stochastic pooling stochastic pooling probability weighting probability weighting probability weighting probability weighting pooling pooling pooling pooling pooling pooling table cifar- classiﬁcation performance various train test combinations pooling methods. best performance obtained using stochastic pooling training using probability weighting test time. visualizations insight mechanism stochastic pooling gained using deconvolutional network zeiler provide novel visualization trained convolutional network. deconvolutional network components convolutional network inverted top-down decoder maps top-layer feature maps back input pixels. unpooling operation uses stochastically chosen locations selected forward pass. deconvolution network ﬁlters transpose feed-forward ﬁlters auto-encoder tied encoder/decoder weights. repeat top-down process input pixel level reached producing visualizations fig. pooling many input image edges present average pooling produces reconstruction discernible structure. fig. shows examples pixel-space reconstructions different location samples throughout network. reconstructions similar pooling case pooling locations change result small local deformations visualized image. despite stochastic nature model multinomial distributions effectively capture regularities data. demonstrate this compare outputs produced deconvolutional network sampling using feedforward proabilities versus sampling uniform distributions. contrast fig. uses feedforward proabilities fig. replace pooling layers’ distributions uniform distributions. feed forward probabilities encode signiﬁcant structural information especially lower layers model. additional visualizations videos sampling process provided supplementary material www.matthewzeiler.com/pubs/iclr/. discussion propose simple effective stochastic pooling strategy combined forms regularization weight decay dropout data augmentation etc. prevent overﬁtting training deep convolutional networks. method also intuitive selecting information network already providing opposed methods dropout throw information away. show state-of-the-art performance numerous datasets comparing approaches employ data augmentation. furthermore method negligible computational overhead hyper-parameters tune thus swapped existing convolutional network architecture. figure visualizations third layer feature activations horse image average pooling visualizations also shown left. image block instantiation pooling locations using stochastic pooling. sampling locations layer either multinomial distribution pooling region derived feed-forward activations eqn. uniform distribution. feed-forward probabilities encode much structure image almost lost uniform sampling used especially lower layers. krizhevsky. cuda-convnet. http//code.google.com/p/cuda-convnet/ lecun. mnist database. http//yann.lecun.com/exdb/mnist/ lecun bottou bengio haffner. gradient-based learning applied document", "year": 2013}