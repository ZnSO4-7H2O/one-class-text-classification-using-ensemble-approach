{"title": "Compressing Deep Convolutional Networks using Vector Quantization", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1% loss of classification accuracy using the state-of-the-art CNN.", "text": "deep convolutional neural networks become promising method object recognition repeatedly demonstrating record breaking results image classiﬁcation object detection recent years. however deep generally involves many layers millions parameters making storage network model extremely large. prohibits usage deep cnns resource limited hardware especially cell phones embedded devices. paper tackle model storage issue investigating information theoretical vector quantization methods compressing parameters cnns. particular found terms compressing storage demanding dense connected layers vector quantization methods clear gain existing matrix factorization methods. simply applying k-means clustering weights conducting product quantization lead good balance model size recognition accuracy. -category classiﬁcation task imagenet challenge able achieve times compression network loss classiﬁcation accuracy using state-of-the-art cnn. deep convolutional neural networks recently achieved signiﬁcant progress become gold standard object recognition image classiﬁcation retrieval. almost recent successful recognition systems built architecture. importing onto embedded platforms recent trend toward mobile computing wide range application impacts. especially useful bandwidth limited photos allowed sent servers. however size models typically large limits applicability models embedded platform. example almost impossible users download iphone application thus order apply neural network methods embedded platforms important research problem compress parameters reduce storage requirements. article mainly consider compressing cnns computer vision tasks. example typical works well object recognition contains eight layers huge number parameters order produce state-of-the-art results. widely known parameters heavily over-parameterized interesting investigate whether compress parameters exploring structure. here mainly interested compressing parameters reduce storage instead speeding testing time typical network described storage taken dense connected layers; running time taken convolutional layers. therefore shall focus upon compress dense connected layers reduce storage neural networks. early works compressing cnns published; however focus different ours. closely related denton explored matrix factorization methods speeding testing time. researchers showed exploring linear structure parameters testing time sped much keeping accuracy within original model. another similar work speeding authors described several reconstruction methods approximating ﬁlers convolutional layers. goal works complimentary ours focus compressing convolutional layers speeding cnn. focus however compressing dense connected layers order reduce size model. work instead traditional matrix factorization methods considered mainly consider series information theoretical vector quantization methods compressing dense connected layers. example consider binarizing parameters scalar quantization using kmeans structured quantization using product quantization residual quantization. surprisingly found simply applying kmeans-based scalar quantization achieves impressive results approach better matrix factorization methods. structured quantization methods give additional gain exploring redundancy parameters. knowledge ﬁrst work systematically study different vector quantization methods compressing parameters. paper makes following contributions among ﬁrst systematically explore vector quantization methods compressing dense connected layers deep cnns reduce storage; performed comprehensive evaluation different vector quantization methods shown particular structured quantization product quantization works signiﬁcantly better methods; performed experiments tasks image retrieval verify generalization ability compressed model. deep convolutional neural network achieved great successes image classiﬁcation object detection image retrieval great progress area state-of-the-art image classiﬁer achieve accuracy ilsvrc dataset object classes already close human performance. great success ignites interest adopting cnns real world applications. example already applied object classiﬁcation scene classiﬁcation indoor scene classiﬁcation. also applied image retrieval impressive results. discussed section state-of-the-art usually involves hundreds millions parameters require huge storage model difﬁcult achieve. bottleneck comes model storage testing speed. several works published speeding prediction speed. vanhoucke explored properties speed execution particularly focused aligning memory simd operations boost matrix operations. mathieu showed convolutional operation efﬁciently carried fourier domain leads speed-up recent works denton jaderberg explored linear matrix factorization methods speeding convolutional layers showed speed-up little compromise classiﬁcation performance. almost mentioned works focus making prediction speed faster; little work speciﬁcally devoted making models smaller. vector quantization methods compress parameters mainly inspired work denil demonstrate redundancies neural network parameters. show weights within layer accurately predicted small subset parameters indicates neural network over-parameterized. results motivate apply vector quantization methods explore redundancy parameter space. particular paper viewed compression realization parameter prediction results reported denil somewhat surprisingly found similar results denil section consider classes methods compressing parameters dense connected layers. ﬁrst consider matrix factorization methods introduce vector quantization methods. ﬁrst consider matrix factorization methods widely used speed well compressing parameters linear models particular consider using singular-value decomposition factorize parameter matrix. given parameter rm×n dense connected layer factorize rm×m rn×n dense orthogonal matrices rm×n diagonal matrix. order approximate using much smaller matrices pick singular vectors corresponding eigenvalue reconstruct rm×k rn×k submatrices correspond leading singular vectors diagonal elements rk×k correspond largest singular values. approximation controlled decay along eigenvalues method optimal sense frobenius norm minimizes error approximated matrix original low-rank matrices well eigenvalues must stored. compression rate given computed mn/k. method mainly inspired dropconnect randomly sets part parameters training. here taking aggressive approach turning neuron positive turning negative. geometric point view assuming dense connected layers hyperplanes actually rounding hyperplane nearest coordinate. method compress data times since neuron represented bit. scalar quantization using kmeans another simple method perform scalar quantization parameters. rm×n collect scalar values r×mn perform kmeans clustering values approach need store indexes codebook parameters. given centers need bits encode centers. example centers need bits needed cluster index. thus compression rate assuming ﬂoating numbers original assuming codebook negligible. despite simplicity approach experiments showed approach gives surprisingly good performance compressing parameters. next consider structured vector quantization methods compressing parameters. particular consider product quantization explores redundancy structures vector space. basic idea partition vector space many disjoint subspaces perform quantization subspace. assumption vectors subspace heavily redundant performing quantization subspace able better explore redundancy structure. speciﬁcally given matrix partition colum-wise several submatrices approach need store cluster indexes codebooks subvector. particular contrast scalar quantization case codebook negligible. compression rate method /ms). third quantization method consider residual quantization another form structured quantization. basic idea ﬁrst quantize vectors centers recursively quantize residuals. example given vectors ﬁrst stage begin quantizing different vectors using kmeans clustering abovementioned three different kinds vector quantization methods compressing matrices. captures redundancy neuron explores local redundancy structure; tries explore global redundancy structure weight vectors. interesting investigate kinds redundancies present behavior learned parameters. many learning-based binarization product quantization methods available spectral hashing iterative quantization catesian kmeans among others. however suitable particular task need store learned parameter matrix large. reason consider methods paper. also interesting explore structure performing particular learned outputs different ﬁlters grouping together output speciﬁc ﬁlters grouping together speciﬁc dimensions different ﬁlters might interesting. however preliminary investigation improvement exploring structure therefore grouped dimensions default order. evaluated different methods ilsvrc benchmark image classiﬁcation dataset. dataset contains million training images object categories. also validation images categories contain images each. trained standard training performed compression parameters tested validation set. convolutional neural network used zeiler fergus contains convolutional layers dense connected layers. input images ﬁrst resized minimal dimensions performed random cropping patches. images different convolutional layers respective ﬁlter sizes ﬁrst convolutional layers followed local response normalization layer pooling layer. point obtained three fully connected layers sizes nonlinear function used relu. network trained days epochs. learning rate started halved every epochs; weight decay momentum evaluate different methods used classiﬁcation accuracy validation evaluation protocol. used accuracy accuracy evaluate different parameter compression methods. goal either achieve higher compression rate accuracy higher accuracy compression rate. ﬁrst performed analysis pq-based compression parameters several different parameters. used different number clusters segment. ﬁxed show results different segment dimension sizes changed compression rate lower higher. mentioned section able perform either x-axis y-axis therefore show results cases. shall compare different methods align segment size also compare aligned compression rate. results accuracy reported figure figure different axis alignments. results figure using centers smaller segment size able obtain smaller classiﬁcation error. example figure curve always much smaller classiﬁcation error methods. result consistent previous observations however took size codebook account measured accuracy compression rate figure found using centers always helpful figure comparison compression aligned compression rate accuracy. clearly taking codebook size account using centers necessarily lead better accuracy compression rate. text detailed discussion. aggressively increase codebook size. example used centers classiﬁcation error clearly lower used fewer number clusters difference codebook using much storage space makes compression rate low. compare results compressing x-axis y-axis results x-axis slightly better. improvement probably dimensions x-axis makes codebook size larger thereby reduces loss information. next experiments ﬁxed number centers approach achieves good balance compression rate accuracy. section contains comparison quantization methods introduce herein. similarly sections present classiﬁcation errors respect compression rate. binary quantization parameter tune compression rate used centers segment varied dimension segment achieve different compression rates. kmeans varied number clusters achieve compression rate vary output dimensionality achieve different compression rates. performance unsatisfactory; here report results centers iterations. also experimented lower numbers centers found performance even worse. accuracy accuracy shown figure trend consistent ﬁgures. particular achieves impressive results compressing speeding convolutional layers work well compressing dense connected layers. difference mainly factorized matrices still need stored optimized saving storage. somewhat surprisingly kmeans despite simplicity works well task. able achieve times compression rate keeping accuracy loss within applying structured quantization methods improve performance beyond results found able achieve high compression rate mainly codebook size big. given compression rate accuracy also much worse methods. finally simplest binarization method worked reasonably well. able vary compression rate given compression rate performance comparable addition storage quite simple. therefore method also good choice goal compress data aggressively. comparison among suggests interesting insights. first works reasonably well achieve descent compression rate without sacriﬁcing performance. results suggests considerable redundancy single neuron. applying works even better means meaningful sub-vector local structures weight matrices. works extremely poorly task probably means global structures weight vectors. also conducted additional analysis classiﬁcation error rate compressing single layer ﬁxing layers uncompressed. results reported figure found compressing eighth ninth hidden layers usually lead signiﬁcant decrease performance compressing tenth ﬁnal classiﬁcation layer much larger decrease accuracy. compressing three layers together usually larger error especially compression rate high. finally sample predictions results shown figure section presents application compressed image retrieval order verify generalization ability compressed networks. real industry applications many situations would allow uploading photo server uploading large numbers photos server would unaffordable. fact bandwidth limits able upload processed data server. given compressed above able process images using compressed cellphone side perform retrieval database side uploading processed feature. performed experiments holidays dataset widely used standard benchmark dataset image retrieval contains images different instances; instance contains images. number query images ﬁxed rest used populate database. used mean average precision evaluate different methods. used compressed model generate -dimensional activation features last hidden layer used features image retrieval. used cosine distance measure similarities among image features. according results table trend different methods similar classiﬁcation results; i.e. consistently worked better methods. surprising ﬁnding kmeans centers gave high results–even higher original feature special case however probably came application near-duplicate image retrieval quantizing values binary robust small image transformations. however goal best reconstruct original weight matrix improvement binary kmeans case indeed showed accurate approximation. reporting results here performance poor. addressed storage problem applying vector quantization compress deep convolutional neural networks embedded systems. work systematically studied compress parameters deep convolutional neural network order save storage models. unlike previous approaches considered using matrix factorization methods proposed study series vector quantization methods compressing parameters. somewhat surprisingly found simply performing scalar quantization parameter values using kmeans able obtain compression rate parameters without sacriﬁcing top-ﬁve accuracy compressions. addition using structured quantization methods able compress parameters times keeping loss top-ﬁve accuracy within compressing parameters times addressed problem applying state-of-the cnns embedded devices. given state-of-the-art model parameters able reduce less enabled easily deploy models. another interesting implication paper empirical results conﬁrmed ﬁnding denil useful parameters future interesting explore hardware efﬁcient operations compressed models embedded devices speed computation. also interesting apply ﬁne-tuning compressed layers improve performance. whereas paper mainly focused compressing dense connected layers interesting investigate apply vector quantization methods compress convolutional layers. culurciello eugenio dundar aysegul jonghoon gokhale vinayak martini berin. accelerating deep neural networks mobile processor embedded programmable logic. nips. denil misha shakibi babak dinh laurent ranzato marc’aurelio freitas nando predicting parameters deep learning. burges c.j.c. bottou welling ghahramani weinberger k.q. advances neural information processing systems donahue jeff yangqing vinyals oriol hoffman judy zhang ning tzeng eric darrell trevor. decaf deep convolutional activation feature generic visual recognition. corr abs/. sermanet pierre eigen david zhang xiang mathieu michael fergus lecun yann. overfeat integrated recognition localization detection using convolutional networks. arxiv preprint arxiv. simonyan karen vedaldi andrea zisserman andrew. deep ﬁsher networks large-scale image classiﬁcation. advances neural information processing systems szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. corr abs/.", "year": 2014}