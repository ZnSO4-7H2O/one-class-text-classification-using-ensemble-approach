{"title": "Learning Network of Multivariate Hawkes Processes: A Time Series  Approach", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Learning the influence structure of multiple time series data is of great interest to many disciplines. This paper studies the problem of recovering the causal structure in network of multivariate linear Hawkes processes. In such processes, the occurrence of an event in one process affects the probability of occurrence of new events in some other processes. Thus, a natural notion of causality exists between such processes captured by the support of the excitation matrix. We show that the resulting causal influence network is equivalent to the Directed Information graph (DIG) of the processes, which encodes the causal factorization of the joint distribution of the processes. Furthermore, we present an algorithm for learning the support of excitation matrix (or equivalently the DIG). The performance of the algorithm is evaluated on synthesized multivariate Hawkes networks as well as a stock market and MemeTracker real-world dataset.", "text": "learning inﬂuence structure multiple time series data great interest many disciplines. paper studies problem recovering causal structure network multivariate linear hawkes processes. processes occurrence event process affects probability occurrence events processes. thus natural notion causality exists processes captured support excitation matrix. show resulting causal inﬂuence network equivalent directed information graph processes encodes causal factorization joint distribution processes. furthermore present algorithm learning support excitation matrix performance algorithm evaluated synthesized multivariate hawkes networks well stock market memetracker real-world dataset. many disciplines including biology economics social sciences computer science important learn structure interacting networks stochastic processes. particular succinct representation causal interactions network interest. studies causality ﬁelds focus causal discovery time series. causal relations time series vector autoregressive models time series generally evaluate causal inﬂuences transfer entropy directed information paper considers learning causal structure speciﬁc type time series multivariate linear hawkes process hawkes processes originally motivated quest good statistical models earthquake occurrences. since then successfully applied seismology biology criminology computational ﬁnance etc. desirable develop speciﬁc causal discovery methods processes study properties existing methods particular scenario. multivariate mutually exciting point processes occurrence event process affects conditional probability occurrences i.e. intensity function processes network. interdependencies intensity functions linear hawkes process modeled follows intensity function processes assumed linear combination different terms term captures effects process therefore natural notion functional dependence exists among processes sense linear mutually exciting processes coefﬁcient pertaining effects process non-zero intensity function process know process inﬂuencing process dependency captured support excitation matrix network. result estimation excitation matrix multivariate processes crucial learning structure causal network inference tasks focus research. instance maximum likelihood estimators proposed estimating parameters excitation matrices exponential laguerre decay estimators depend existence i.i.d. samples. however often access i.i.d. samples analyzing time series. second-order statistics multivariate hawkes processes used estimate kernel matrix subclass multivariate hawkes processes called symmetric hawkes processes utilizing branching property hawkes processes expectation maximization algorithm proposed estimate excitation matrix investigate efﬁcient approaches estimation excitation matrix hawkes processes time series require i.i.d. samples investigate concept causality processes related established approaches analyze causal effects time series. contribution paper fold. first prove linear multivariate hawkes processes causal relationships implied excitation matrix equivalent speciﬁc factorization joint distribution system called minimal generative model. minimal generative models encode causal dependencies based generalized notion granger causality measured causally conditioned directed information signiﬁcance result provides surrogate directed information measure capturing causal inﬂuences hawkes processes. thus instead estimating directed information often requires estimating high dimensional joint distribution sufﬁces learn support excitation matrix. second contribution indeed providing estimation method learning support excitation matrices exponential form using second-order statistics hawkes processes. proposed learning approach contrast previous work limited symmetric hawkes processes. symmetric hawkes process assumed laplace transform excitation matrix factored product diagonal matrix constant unitary matrix. moreover assumed expected values intensities same. numerical method approximate excitation matrix coupled integral equations recently proposed approach based exact analytical solution excitation matrix. interestingly exact approach turns robust less expensive terms complexity compared numerical method rest paper organized follows. background material deﬁnitions notation presented section speciﬁcally therein formally introduce multivariate hawkes processes directed information graphs. section establish connection excitation matrix corresponding dig. section propose algorithm learning excitation matrix equivalently class stationary multivariate linear hawkes processes. section illustrates performance proposed algorithm inferring causal structure network synthesized mutually exciting linear hawkes processes stock market. finally conclude work section section review basic deﬁnitions notation. denote random processes capital letters collection random processes denote random process time random process time subset random process time laplace time multivariate hawkes processes complete probability space denotes counting process representing cumulative number events time t}t≥ increasing σ-algebras non-negative t-measurable process called intensity denotes matrix entries γij; arrays entries respectively. matrix called excitation matrix. figure illustrates intensities multivariate hawkes process comprised processes following parameters causal model allows factorization joint distribution speciﬁc ways. generative model graphs type graphical model similar bayesian networks represent causal factorization joint precisely shown assumption −{i} minimal processes causes process i.e. parent node corresponding minimal generative model graph. factorization joint distribution called minimal generative model. equation extending deﬁnition generative model graphs continuous-time systems requires technicalities necessary purpose paper. hence illustrate general idea example. following example demonstrates minimal generative model graph simple continuoustime system. example consider dynamical system processes evolve time horizon following coupled differential equations linear multivariate hawkes processes natural notion causation exists following sense occurrence event process affect likelihood arrivals process. next establish relationship excitation matrix multivariate hawkes processes generative model graph. ﬁrst discuss equivalence directed information graphs generative models graphs established alternative graphical model encode statistical interdependencies stochastic causal dynamical systems directed information graphs graphs deﬁned based information-theoretic quantity directed information shown mild assumptions equivalent minimal generative model graphs. hence digs also represent minimal factorization joint distribution. determine whether causes time horizon network random processes conditional probabilities compared kl-divergence sense conditional probability given full past i.e. conditional probability given full past except past i.e. t−{j} t−{j}}. declared inﬂuence conditional probabilities same. precisely inﬂuence following directed information measure positive mentioned earlier corresponding minimal generative model graph causal dynamical system equivalent. thus characterize corresponding minimal generative model graphs multivariate hawkes system study properties corresponding dig. proposition consider mutually exciting processes excitation matrix assumption time interval proof section proposition signiﬁes support excitation matrix determines adjacency matrix vice versa. therefore learning mutually exciting hawkes processes satisfying assumption equivalent learning excitation matrix given samples processes. word presence side information processes hawkes efﬁcient learn causal structure learning excitation matrix rather directed information needed learning general. section present approach learning causal structure stationary hawkes network exponential exciting functions learning excitation matrix. method based second order statistic hawkes processes suitable case i.i.d. samples available. note i.i.d. samples available non-parametric methods learning excitation matrix mmel algorithm exist. approach exciting functions expressed linear combination base kernels penalized likelihood used estimate parameters model. mentioned earlier focus learning excitation matrix multivariate hawkes processes exponential exciting functions. class hawkes processes widely applied many areas seismology criminology ﬁnance deﬁnition excitation matrix multivariate hawkes processes exponential exciting functions deﬁned follows describing algorithm need derive useful properties moments process. multivariate hawkes process excitation matrix stationary increments i.e. intensity processes stationary following assumption holds order learn excitation matrix exponential exciting functions need learn exciting modes {βd} number components coefﬁcient matrices {ad}. next results establishes relationship exciting modes number components normalized covariance matrix process. corollary consider network stationary multivariate hawkes processes excitation matrix belonging exp. exciting modes absolute values zeros rf−. proof section next need coefﬁcient matrices {ad}. covariance density processes. covariance density stationary multivariate hawkes process deﬁned shown equation admit unique solution next proposition provides system linear equations allows learn coefﬁcient matrices. proposition consider network stationary multivariate hawkes processes excitation matrix exciting modes βd}. {ad} solution linear system equations hm×m block matrix block given ...l]. proof section combining results corollary proposition allows learn excitation matrix exponential multivariate hawkes processes second order moments. consequently applying proposition causal structure network learned drawing arrow node section discusses estimators second order moments namely normalized covariance matrix covariance density stationary multivariate hawkes processes data. estimators available approach previous section maybe used learn network. intuitive estimator deﬁned equation turns estimator converges almost surely goes inﬁnity furthermore proposes empirical estimator normalized covariance matrix follows paper shown assumption t→∞σz. notice normalized covariance matrix covariance density related σdt/dt therefore estimate covariance density matrix using equation choosing small algorithm summarizes steps proposed approach learning excitation matrix consequently causal structure exponential multivariate hawkes process. figure recovered network example excitation matrix given algorithm numerical method mmel i.i.d. samples length approach learns graph approaches fail sample size. gnnnnnnnnnnnnnnnnnnnn nnnnnnnnnnnnnnnnnnnn ukkkkkkkkkkk ];;;;;;;;;;;;;;;;;;;;;;;;;;;; ;;;;;;;;;;;;;;;;;;;;;;;;;;;; fffffffffffffffffffffff rrrrrrrrrrrrrrrr dddddddddddddddddddd lyyyyyyyyyyyyyyyyyyyy juuuuuuuuuuuuuuuuuuuuuuuuuu \u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016 jjjjjjjjjjjjjjjjjjjjjjjj tjjjjjjjjjjjjjjjjjjjjjjjj gnnnnnnnnnnnnnnnnnnnnnnnnnnnnn ffff bfffffffffffffffffffffffffffffff &&&&&&&&&&&&&&&&&&&&&&&&&&&&&& ysssssssssssssssssssssssssss xqqq ~||||||||||||||||||||||||||||| }zzzzzzzzzu zzzzzzzzz \u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016 \u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016\u0016 \u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013\u0013 ^^^^^^^^^^^^^^^^^^^^^^^^^ lzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz jttttttttttttttttttttttttttttttttt fnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn beeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee ............................ qccccccccccccccccccccccccccccc ccccccccccccccccccccccccccccc ujjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj jjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj \u0006\u0006\u0006\u0006 t)))))))))))))))  lzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz jtttttttttttttttttttttttttttttttttttt tttttttttttttttttttttttttttttttttttt fmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm beeeeeeeeeeeeeeeeeeeeeeeeeeeeee w................... ukkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk ztttttttttttttttttttttttttttttt \u0006\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e \u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e vmmmmmmmmmmmmmmmmmmmmmmmmmmmm juuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu uuuuuuuuuuuuuuuuuuuuuuuuuuuuuuu gnnnnnnnnnnnnnnnnnnnnnnnnnn wwwwwwwwwwwwwwwwwwwwwww \u0002\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005 \u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005\u0005 network satisﬁes assumption since exciting modes observed arrivals processes time period figure depicts outputs algorithms observation lengths illustrated figure increasing length observation output graph converges true shown figure comparison applied mmel algorithm proposed learn excitation matrix example numerical method based nystrom method proposed number quadrature since mmel requires i.i.d. samples generate i.i.d. samples length obtain figure proposed algorithm outperforms mmel numerical method furthermore conducted another experiment network processes edges illustrated figure sample length algorithm able recover edges correctly identiﬁed false arrows. mmel could recover arrows correctly detecting another false arrows. input mmel sequences length w////////////////////// cffffffffffff ffffffffffff hqqqqqqqqqqqqqqqqq cggggggggggggggggggggg r%%%%%%%%%%%%%%%%%%%%% *********** \u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018 qqqqqqqqqqqqqqqqqqqqq ................. tiiiiiiiiiiiiiiiiiiii pppppppppppppppppppp \u001b\u001b\u001b\u001b\u001b\u001b\u001b\u001b\u001b\u001b hqqqqqqqqqqqqqqqqqqqqq gggggggggggggggggg   cffffffffffff w////////////////////// hqqqqqqqqqqqqqqqqq r%%%%%%%%%%%%%%%%%%%%% cggggggggggggggggggggg \u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e\u000e \u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018 qqqqqqqqqqqqqqqqqqqqq ................. tiiiiiiiiiiiiiiiiiiii pppppppppppppppppppp \u001b\u001b\u001b\u001b\u001b\u001b\u001b\u001b\u001b\u001b hqqqqqqqqqqqqqqqqqqqqq gggggggggggggggggg ekkkkkkkkkkkkk  w////////////////////// cffffffffffff hqqqqqqqqqqqqqqqqq r%%%%%%%%%%%%%%%%%%%%% \u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018 qqqqqqqqqqqqqqqqqqqqq ................. pppppppppppppppppppp yyyyyyyyyyyyyyyyyyyyy \u001b\u001b\u001b\u001b\u001b\u001b\u001b\u001b\u001b\u001b gggggggggggggggggg ekkkkkkkkkkkkk   exchange sourced google finance. prices sampled every minutes twenty market days every time stock price changed current price event logged stock’s process. order prevent substantial changes stock’s prices opening closing market ignored samples beginning working day. part assumed jumps occurring stock’s prices correlated multivariate hawkes process. model class advocated figure illustrate causal graph resulting algorithm minutes. compare learning approach approaches applied mmel algorithm learn corresponding causal graph. scenario assumed data collected generated i.i.d. hence total i.i.d. samples used. figure illustrates resulting graph. figures convey pretty much similar causal interactions dataset. instance graphs suggest inﬂuential companies period time hewlett-packard looking global market share indeed case. another modality derive corresponding network applying equation part used market based black-scholes model stock’s prices modeled coupled stochastic pdes. assumed logarithm stock’s prices jointly gaussian therefore corresponding estimated using equation resulting shown figure note derived logarithm prices jump processes used earlier. still shares similarities graphs. instance also identiﬁes inﬂuential companies microsoft inﬂuenced companies time period. table shows number edges approaches recovers number edges jointly recover. demonstrates power exponential kernels even data come model class. also studied causal inﬂuences blogosphere. causal information media sites captured studying hyperlinks provided media site others. speciﬁcally time linking modeled using linear multivariate hawkes processes exponential exciting functions model also intuitive sense emerging topic ﬁrst several days blogs websites likely feature topics also likely topic would trigger discussions create hyperlinks. thus exponential exciting functions well suited capture phenomenon exiting functions relatively large values ﬁrst decay fast time elapses. experiment used memetracker dataset. data contains time-stamped phrase hyperlink information news media articles blog posts million different websites. extracted times hyperlinks well-known websites listed table created august april hyperlink website created certain time arrival events recorded time. precisely experiment picked different phrases appeared different websites different times. website published phrases time also contained hyperlink listed websites arrival event recorded time website list. figure illustrates resulting causal structure learned algorithm hours hour. graph arrow node another node means creating hyperlink yelp.com triggers creation hyperlinks youtube.com. also applied mmel algorithm exponential kernel function learn excitation matrix. experiment data corresponding phrase treated i.i.d. realization system. resulting causal structure depicted figure figure illustrates nodes clustered main groups wi}. ﬁrst group consists mainly merchandise reviewing websites second group contains broadcasting websites. however clear figure mmel requires i.i.d. samples able identify correct arrows. note increase number phrases figure graphs become similar clearly visible main clusters. learning causal structure stochastic network processes requires estimation conditional directed information estimating quantity general high complexity requires large number samples. however complexity learning task could signiﬁcantly reduced side information underlying structure system dynamics available. proved multivariate hawkes processes estimating support excitation matrix sufﬁces learn associated dig. therefore approaches learning excitation matrix multivariate hawkes processes estimation algorithm nonparametric estimation techniques proposed proposed method paper used learn causal interactions networks. previous estimation approaches either \u0002\u0004\u0004\u0004\u0004\u0004\u0004\u0004\u0004\u0004\u0004\u0004\u0004 )))))))))))))))) t−{j}) λidt t−{j}-measurable. since assumed γi.j obtain t−{j}-measurable words process determined processes contradicts assumption states deterministic relationships processes. nominator equation straightforward check −jβk quantity non-zero fact distinct since polynomial real coefﬁcients complex conjugate root theorem therefore {±jβd} contains poles", "year": 2016}