{"title": "Spectrally-normalized margin bounds for neural networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized \"spectral complexity\": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnist and cifar10 datasets, with both original and random labels; the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity.", "text": "paper presents margin-based multiclass generalization bound neural networks scales margin-normalized spectral complexity lipschitz constant meaning product spectral norms weight matrices times certain correction factor. bound empirically investigated standard alexnet network trained mnist cifar datasets original random labels; bound lipschitz constants excess risks direct correlation suggesting selects predictors whose complexity scales diﬃculty learning task secondly presented bound sensitive complexity. neural networks astonishing success ability data also generalize well meaning provide close unseen data. classical statistical adage models capable ﬁtting much generalize poorly; what’s going here? let’s navigate many possible explanations provided statistical theory. ﬁrst observation analysis based solely number possible labellings ﬁnite training case dimension doomed function class possible labels analysis distinguish collection possible functions figure analysis alexnet trained cifar original random labels. triangle-marked curves track excess risk across training epochs marking earliest epoch zero training error. circle-marked curves track lipschitz constants normalized curves random labels meet. lipschitz constants tightly correlate excess risk moreover normalizing margins neutralizes growth across epochs. ∗<peterberkeley.edu>; university california berkeley queensland university technology; work performed †<djfcornell.edu>; cornell university; work performed visiting simons institute. ‡<mjtillinois.edu>; university illinois urbana-champaign; work performed visiting simons institute. next let’s consider scale-sensitive measures complexity rademacher complexity covering numbers work directly real-valued function classes moreover sensitive magnitudes. figure plots excess risk across training epochs candidate scale-sensitive complexity measure lipschitz constant network demonstrates tightly correlated data considered figure standard cifar dataset original random labels used sanity check investigating neural network generalization still issue basing complexity measure purely lipschitz constant depicted figure measure grows time despite excess risk plateauing. fortunately standard resolution issue investigating margins outputs network. tool used study behavior -layer networks boosting methods svms many others boosting instance similar growth complexity time whereas margin bounds correctly stay even decrease. behavior recovered here depicted figure even though standard networks exhibit growing lipschitz constants normalizing lipschitz constants margin instead gives decaying curve. contributions work investigates complexity measure neural networks based lipschitz constant normalized margin predictor. central contributions follows. theorem give rigorous statement generalization bound basis work. contrast prior work bound scales lipschitz constant divided margin; dependence combinatorial parameters outside factors; multiclass measures complexity reference network reference network identity mappings layer). bound stated below general form analysis summary appearing section full details relegated appendix. empirical investigation section neural network generalization standard datasets cifar cifar mnist using preceding bound. rather using bound provide single number used form margin distribution figure margin distributions illuminate following intuitive observations cifar harder mnist; random labels make cifar mnist much diﬃcult; margin distributions converge training even though weight matrices continue grow; regularization signiﬁcantly impact margins generalization. detailed description margin distributions follows. suppose neural network computes function number classes; natural convert classiﬁer select output coordinate largest magnitude meaning maxj margin then measures output correct label labels meaning maxj=y unfortunately margins alone seem much; instance figure collections margins data points unnormalized margin distribution similar cifar without random labels. missing appropriate normalization figure normalization provided theorem explained detail. state bound little notation necessary. networks ﬁxed nonlinearities rdi− ρi-lipschitz occasionally also hold given weight matrices figure margin distributions training alexnet cifar without random labels. proper normalization random labels demonstrably correspond harder problem. network output converted class label taking components arbitrary rule breaking ties. whenever input data given collect rows matrix rn×d. occasionally notation overloaded discuss matrix whose column denote maximum dl}. norm always computed entry-wise; thus matrix corresponds frobenius norm. next deﬁne collection reference matrices dimensions instance obtain good bound resnet sensible identity bound worsen network moves farther identity map; alexnet simple choice suﬃces. finally denote spectral ﬁxed whose weight matrices bounded spectral complexity theorem nonlinearities reference matrices given drawn probability distribution probability least every margin network weight matrices satisfy full proof generalization beyond spectral norms relegated appendix sketch provided section along lower bound. section also gives discussion related work brieﬂy it’s essential note margin lipschitz-sensitive bounds long history neural networks literature distinction proceeding plots it’s good time give reﬁned description margin distribution suitable comparisons across datasets. given pattern/label pairs patterns rows matrix rn×d given predictor margin distribution univariate empirical distribution labeled data points transformed single scalar according taken margin distributions datasets interpreted follows. considering ﬁxed point horizontal axis cumulative distribution density lower other corresponds lower right hand side theorem reason visual interpretability plots instead depict density estimate margin distribution. little detail experimental setup follows. experiments implemented keras order minimize conﬂating eﬀects optimization regularization optimization method vanilla step size regularization disabled. cifar general refers cifar however cifar also explicitly mentioned. network architecture essentially alexnet normalization/regularization removed adjustments kind across diﬀerent experiments. comparing datasets. ﬁrst comparison cifar standard mnist digit data. mnist considered easy since variety methods achieve roughly test error. easiness corroborated figure margin distribution mnist places mass right mass cifar. interestingly randomizing labels mnist figure results margin distribution left cifar also slightly left cifar randomized labels. next figure compares cifar cifar cifar uses input images cifar; indeed cifar obtained cifar collapsing original categories groups. interestingly cifar perspective margin bounds diﬃcult cifar random labels. consistent large observed test error cifar lastly figure replaces cifar input images random images sampled gaussians matching ﬁrstsecond-order image statistics similar experiments). convergence margins. pointed section weights neural networks seem converge usual sense training however depicted figure sequence margin distributions converging. regularization. remarked regularization seems bring minor beneﬁts test error observation certainly consistent margin distributions figure improve visible regularization. open question discussed section design regularization improves margins. multiclass margin bound starting point analysis margin-based bound multiclass prediction. state bound ﬁrst recall margin operator deﬁned deﬁne ramp loss respectively upper bound probability fraction errors source distribution training set. lastly given real-valued functions deﬁne rademacher complexity expectation rademacher random variables bound direct consequence standard tools rademacher complexity. order instantiate bound covering numbers used directly upper bound rademacher complexity term r|s). interestingly choice directly working terms covering numbers seems essential providing bound explicit dependence contrast prior work primarily handles multiclass rademacher complexity analysis coordinate k-tuple functions pays factor covering number complexity upper bounds subsection proves theorem lemma controlling covering numbers rademacher complexity r|s) networks bounded spectral complexity. notation covering numbers follows. denote least cardinality subset covers scale norm meaning full proof following steps. matrix covering bound aﬃne transformation layer provided lemma handling whole layers allows ﬂexible norms. induction layers gives covering number bound entire networks; analysis sketched special case norms used theorem full proof appendix culminates bound general norms preceding whole-network covering number leads theorem lemma standard techniques. step matrix covering handled following lemma. covering number considers matrix product instantiated weight matrix layer data passed layers prior present layer. proof relies upon maurey sparsiﬁcation lemma stated terms sparsifying convex hulls inspired covering number bounds linear predictors prove theorem matrix covering bound instantiated case possible instead scale even case identity matrix incurs extra dimension factor. thus helps theorem avoid appearance outside terms; indeed goal covering whole matrix time allow greater sensitivity avoid combinatorial parameters. images examples columns inductively suppose exists cover element depends covering matrices ai−) chosen cover weight matrices earlier layers. thanks lemma also exists aixi aixi desired cover element thus σiaixi) nonlinearity layer indeed supposing ρi-lipschitz preceding proof sensitivity particular choice norms; merely required operator norm well norm allows matrix covering. analysis presented full generality appendix specializing particular case spectral norms group norms leads following full-network covering bound. theorem ﬁxed nonlinearities reference matrices given ρi-lipschitz spectral norm bounds matrix norm bounds given. data matrix rn×d given rows correspond data points. denote family matrices obtained evaluating choices network rademacher complexity lower bounds reduction linear case easy provide lower bound rademacher complexity networks studied here. unfortunately bound scales product spectral norms terms theorem consider setting theorem nonlinearities relu max{ output dimension non-output dimensions least data collected data matrix rn×d. scalar algorithmic idea large margin classiﬁers introduced linear case vapnik vapnik gave intuitive explanation performance methods based sample-dependent vc-dimension calculation without generalization bounds. ﬁrst rigorous generalization bounds large margin linear classiﬁers required scale-sensitive complexity analysis real-valued function classes. time large margin analysis developed two-layer networks indeed proof technique inspired layer-wise induction used prove theorem present work. margin theory quickly extended many settings major success explanation generalization ability boosting methods exhibit explicit growth size function class time stable excess risk contribution present work provide margin bound adapted various operator norms layer. additionally present work operates multiclass setting avoids explicit dependence number classes seems appear prior work numerous generalization bounds neural networks including vc-dimension fatshattering bounds scale-sensitive analysis neural networks started interpreted present setting utilizing data norm operator norm weight matrix ai). analysis adapted give rademacher complexity analysis adapted norms although setting appears necessary avoid extra combinatorial factors. work still needed develop complexity analyses matching upper lower bounds also determine norms well-adapted neural networks used practice. present analysis utilizes covering numbers closely connected earlier covering number bounds based earlier fat-shattering analysis however technique pushing empirical cover layers akin dimension proofs neural networks maurey’s sparsiﬁcation lemma inspired linear predictor covering number bounds since spectrally-normalized margin bounds ﬁrst proposed preprint subsequent works re-derived similar spectrally-normalized bound using pac-bayes framework. speciﬁcally works showed replaced appearing i=)/ appearing using adversarial examples. adversarial examples phenomenon neural network predictions altered adding seemingly imperceptible noise input phenomenon connected margins follows. margin nothing distance input must traverse label ﬂipped; consequently margin points susceptible adversarial noise high margin points. concretely taking lowest margin inputs cifar adding uniform noise scale yielded ﬂipped labels images whereas level noise high margin points yielded ﬂipped labels. bounds suggest defend adversarial examples? observed explicit regularization contributes little regularization. generalization performance neural networks. margin framework standard weight decay regularization seemed little impact margin distributions section hand boosting literature special types regularization developed maximize margins perhaps similar development performed here? sgd. present analysis applies predictors large margins; missing analysis verifying applied standard neural networks returns large margin predictors indeed perhaps returns simply large margin predictors predictors well-behaved variety ways directly translated reﬁned generalization bounds. improvements theorem several directions theorem might improved. better choice layer geometries yield better bounds practical networks? nonlinearities’ worst-case lipschitz constant replaced averaged quantity? alternatively better lower bounds rule directions? acknowledgements authors thank srinadh bhojanapalli ryan jian behnam neyshabur maxim raginsky andrew risteski belinda tzen useful conversations feedback. authors thank recht giving provocative lecture simons institute stressing need understanding generalization optimization neural networks. m.t. d.f. acknowledge machine provided karthik sridharan made possible nvidia grant. d.f. acknowledges support ndseg fellowship. p.b. gratefully acknowledges support grant iis- australian research council australian laureate fellowship centre excellence mathematical statistical frontiers. authors thank simons institute theory computing spring program foundations machine learning. lastly authors grateful burrita upholding glorious tradition california burrito.", "year": 2017}