{"title": "Deep Reinforcement Learning in Large Discrete Action Spaces", "tag": ["cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm's abilities on a series of tasks having up to one million actions.", "text": "gabriel dulac-arnold* richard evans* hado hasselt peter sunehag timothy lillicrap jonathan hunt timothy mann theophane weber thomas degris coppin google deepmind able reason environment large number discrete actions essential bringing reinforcement learning larger class problems. recommender systems industrial plants language models many real-world tasks involving large numbers discrete actions current methods difﬁcult even often impossible apply. ability generalize actions well sub-linear complexity relative size necessary handle tasks. current approaches able provide these motivates work paper. proposed approach leverages prior information actions embed continuous space upon generalize. additionally approximate nearest-neighbor methods allow logarithmic-time lookup complexity relative number actions necessary time-wise tractable training. combined approach allows reinforcement learning methods applied large-scale learning problems previously intractable current methods. demonstrate algorithm’s abilities series tasks million actions. paper present policy architecture operates efﬁciently large number actions. achieve leveraging prior information actions embed continuous space upon actor generalize. embedding also allows policy’s complexity decoupled cardinality action set. policy produces continuous action within space uses approximate nearestneighbor search closest discrete actions logarithmic time. either apply closest action directly environment ﬁne-tune selection selecting highest valued action relative cost function. approach allows generalization action logarithmic time necessary making learning acting tractable time. begin describing problem space detail policy architecture demonstrating train using policy gradient methods actor-critic framework. demonstrate effectiveness policy various tasks million actions intent approach could scale well beyond millions actions. advanced systems likely need reason large number possible actions every step. recommender systems used large systems youtube amazon must reason hundreds millions items every second control systems large industrial processes millions possible actions applied every time step. systems fundamentally consider markov decision process discrete actions discrete states ×a×s transition probability distribution reward function discount factor future rewards. action corresponds n-dimensional vector vector provides information related action. manner state vector return episode discounted rewards received agent episode γi−tr. goal learn policy maximizes expected return episodes state-action value function expected recommon case value function parameterized function takes state action input evaluations necessary choose action. quickly becomes intractable especially parameterized function costly evaluate case deep neural networks. approach does however desirable property capable generalizing actions using smooth function approximator. similar learning also inform make learning efﬁcient also allows value-based policies action features reason previously unseen actions. unfortunately execution complexity grows linearly renders approach intractable number actions grows signiﬁcantly. standard actor-critic approach policy explicitly deﬁned parameterized actor function practice often classiﬁer-like function approximator scale linearly relation number actions. however actor-based architectures avoid computational cost evaluating likely costly q-function every action equation nevertheless actor-based approaches generalize action space naturally value-based approaches cannot extend previously unseen actions. sub-linear complexity relative action space ability generalize actions necessary handle tasks interest with. current approaches able provide these motivates approach described paper. propose policy architecture call wolpertinger architecture. architecture avoids heavy cost evaluating actions retaining generalization actions. policy builds upon actor-critic framework. deﬁne efﬁcient action-generating actor utilize critic reﬁne actor’s choices full policy. multi-layer neural networks function approximators actor critic functions. train policy using deep deterministic policy gradient state previously received environment. {receive proto-action actor.} {retrieve approximately closest actions.} maxaj∈ak apply environment; receive function parametrized mapping state representation space action representation space function provides proto-action given state likely valid action i.e. likely therefore need able element with k-nearest-neighbor mapping continuous space discrete set. returns actions exact case closest distance. lookup complexity value-function derived policies described section step evaluation distance instead full value-function evaluation. task extensively studied approximate nearest neighbor literature lookup performed approximate manner logarithmic time step described bottom half figure actor network producing proto-action k-nearest neighbors chosen action embedding. depending well action representation structured actions q-value occasionally closest even part space actions high q-value. additionally certain actions near action embedding space certain states must distinguished particularly long-term value relative neighbors. cases simply selecting closest element actions generated previously ideal. avoid picking outlier actions generally improve ﬁnally emitted action second phase algorithm described part figure reﬁnes choice action selecting highest-scoring action according equation described explicitly algorithm introduces full wolpertinger policy. parameter represents parameters action generation element critic demonstrate section second pass makes algorithm signiﬁcantly robust imperfections choice action representation essential making system learn certain domains. size generated action task speciﬁc allows explicit trade-off policy quality speed. although architecture policy fully differentiable argue nevertheless train policy following policy gradient fθπ. ﬁrst consider training simpler policy deﬁned fθπ. initial case consider policy effects deterministic aspect environment. allows maintain standard policy gradient approach train output effectively interpreting effects environmenˆ dynamics. similarly operation equation seen introducing non-stationary aspect environmental dynamics. training algorithm’s goal parameterized policy maximizes expected return episode’s length. this parametrization policy maximizes expected return episode maxθ perform optimization using deep deterministic policy gradient train qθq. ddpg draws stabilityinducing aspects deep q-networks extend deterministic policy gradient neural network function approximators introducing replay buffer target networks. similar work introduced nfqca leverages gradient-update originally introduced adhdp goal algorithms perform policy iteration alternatively performing policy evaluation current policy q-learning improving upon current policy following policy gradient. critic trained samples stored replay buffer actions stored replay buffer generated policy gradient ∇aqθq taken allows learning algorithm leverage otherwise ignored information action actually executed training critic taking policy gradient actual output fθπ. target action q-update generated full policy simply fθπ. time-complexity algorithm scales linearly number selected actions practice though increasing beyond certain limit provide increased performance. diminishing returns aspect approach provides signiﬁcant performance gains initial increases quickly renders additional performance gains marginal. consider following simpliﬁed scenario. random proto-action nearby action probability broken action value q−c. values remaining actions uniformly drawn interval probability distribution value chosen action therefore mixture distributions. lemma denote closest actions integers scenario described above expected value maximum closest actions highest value action best action within k-sized thus expectation ﬁrst term decreases exponentially second term terms decrease relatively large amount additional action small marginal returns quickly diminish grows larger. property also observable experiments section notably figures using maximal number actions performance similar full action used. using remaining actions would result relatively small performance beneﬁts increasing computational time order magnitude. limited attention literature regards large discrete action spaces within prior work concentrated factorizing action space binary subspaces. generalized value functions proposed form h-value functions allow policy evaluate binary decisions act. learns factorized value function greedy policy derived subspace. amounts performing binary operations action-selection step. similar approach proposed leverages errorcorrecting output code classiﬁers factorize policy’s action space allow parallel training sub-policy action subspace ecoc-based approach case policy learned rollouts classiﬁcation policy iteration policy deﬁned multi-class ecoc classiﬁer. thus policy directly predicts binary action code nearest-neighbor lookup performed according hamming distance. approaches effectively factorize action space binary subspaces reason subspaces independently. approaches scale large action spaces however require binary code representation action difﬁcult design properly. additionally generalized value-function approach uses linear program explicitly stores value function state prevents generalizing continuous state space. ecoc-based approach deﬁnes action producing policy allow reﬁnement q-function. approaches cannot naturally deal discrete actions associated continuous representations. closest approach literature uses continuous-action policy gradient method learn policy continuous action space apply nearest discrete action principle similar approach tested small problems uni-dimensional continuous action space low-dimensional observation space. small discrete action spaces selecting nearest discrete action sufﬁcient show section complex action-selection scheme necessary scale larger domains. recent work extends deep q-networks ‘unbounded’ action spaces effectively generating action representations action environment provides picking action provides highest however setup environment ever provides small number actions need evaluated hence explicitly pick action large set. evaluate agent’s performance learning speed relate number discrete actions mujoco physics simulator simulate classic continuous control tasks cart-pole dimension original continuous control action space discretized equally spaced values yielding discrete action space actions. cart-pole swing-up agent must balance pole attached cart applying force cart. pole cart start random downward position reward received pole within degrees vertical cart middle track otherwise reward zero received. current state position velocity cart pole well length pole. environment reset steps. environment demonstration agent able reason small large number actions efﬁciently especially action representation well-formed. tasks actions represented force applied dimension. cart-pole case along single dimension actions represented single number. choosing amongst possible n-step plans general large action problem. example environment actions available time step agent needs plan time steps future number actions quickly intractable max-based approaches. implement version task puddle world environment grid world four cell types empty puddle start goal. agent consistently starts start square reward given visiting empty square reward given visiting puddle square reward given episode ends goal cell. agent observes ﬁxed-size square window surrounding current position. goal agent shortest path goal trades cost puddles distance traveled. goal always placed bottom right hand corner environment base actions restricted moving right guarantee goal discovery random exploration. action possible n-length action sequences. base actions {down right}. means environments plan length actions total actions. environment demonstrates agent’s abilities large number actions difﬁcult discern representation less obvious continuity regards effect environment compared mujoco tasks. represent action concatenation step plan. possible steps represent either means full plan vector concatenated steps total length representation chosen arbitrarily show algorithm nevertheless able reason well demonstrate agent would perform realworld large action space problem constructed simulated recommendation system utilizing data live large-scale recommendation engine. environment characterized items recommend correspond action transition probability matrix deﬁnes probability user accept recommendation given last item accepted item item also reward associated accepted user. current state item user currently consuming previously recommended items affect current transition. time-step agent presents item user action recommended item either accepted user user selects random item instead. presented item accepted episode ends probability item accepted episode ends probability effect simulating user patience user likely ﬁnish session search item rather selecting recommendation. episode environment reset selecting random item initial environment state. environment vary number nearest neighbors effectively ignores reranking step described section effectively ignores action generation step described section demonstrate performance nearest-neighbor element policy fθπ. fastest policy conﬁguration section always sufﬁciently expressive. demonstrate performance policy greedy relative always choosing true maximizing action gives upper bound performance soon approach often computationally intractable. intermediate values evaluated demonstrate performance gains partial re-ranking. also evaluate performance terms training time average reward full nearest-neighbor search three approximate nearest neighbor conﬁgurations. flann three settings refer ‘slow’ ‘medium’ ‘fast’. ‘slow’ uses hierarchical k-means tree branching factor corresponds retrieval accuracy recommender task. ‘medium’ corresponds randomized tree nearest neighbors leaf nodes checked. corresponds retrieval accuracy recommender task. ‘fast’ corresponds randomized tree nearest neighbor leaf node checked. corresponds retrieval accuracy recommender task. settings obtained flann’s autotune mechanism. cart-pole task generated discretization million actions. task algorithm able optimal policies. video available ﬁnal policy million actions ‘fast’ flann lookup here http//goo.gl/yfyae. visualize performance agent million action cart-pole task figure using exact lookup. relatively simple cart-pole task agent able converge good policy. however equates actions training failed attain steps amount time. figure shows performance function wall-time cart-pole task. presents performance agents varying neighbor sizes flann settings number seconds training. agents able achieve convergence seconds whereas trains much slowly. table display median steps second training algorithm. flann helpful lookups. computation time spent evaluating instead ﬁnding nearest neighbors. flann performance impacts nearest-neighbor lookup negatively settings except ‘fast’ looking nearest neighbor single dimension. next section action dimensions longer true. system ﬁxed puddle world size setup system dynamics deterministic main goal show agent able appropriate actions amongst large begin note simple case actions figure difﬁcult stable policy. believe large number states producing observation makes high-frequency policy difﬁcult learn. plans longer policies signiﬁcantly better. best possible score without puddles experiments different recommender tasks involving elements elements elements. tasks’ dynamics quite irregular certain actions good many states certain states requiring speciﬁc action rarely used elsewhere. effect rendering agents quite poor task. additionally although initial exploration methods purely uniform random epsilon probability better simulate reality running system state transitions also heavily guided user choice restricted epsilon exploration likely subset good actions provided simulator. subset used guide exploration; step policy must still choose amongst full actions exploring. learning uniform exploration converges larger tasks performance typically guided exploration. figure shows performance -element task using exact lookup varying values percentage total number actions. clear progression performance increased task. although displayed plot smaller action sizes much less signiﬁcant speedups taking twice long results element task visualized figures varying values figure varying flann settings. figure shows performance exact nearestneighbor lookup varying values note agent using actions train many steps slow training speed. training approximately times slower wall-time agent. figure agent performance various lengths plan plan corresponds actions. agent able learn faster longer plan lengths. ‘slow’ flann settings used. puddle world number neighbors actions. ﬁgure absent failed arrive ﬁrst evaluation step. task ﬁnding near optimal policy never using pass policy. even lossy flann setting re-ranking converges optimal policy task. large number actions equivalent value surprising even lossy approximate nearest neighbor search returns sufﬁciently pertinent actions task. experiments recommender system section show always case. figure recommender task actions using dimensional action representation -dimensional action representations varying values ﬁxed flann setting ‘slow’. ﬁgure intends show general behavior detailed values. dimensional -dimensional representation presented figure using ﬁxed ‘slow’ setting flann varying values observe using small number actions compact representation action space beneﬁcial stabilizing convergence. results series tasks suggests approach scale real-world mdps large number actions exploration remain issue agent needs learn scratch. fortunately generally case either domain-speciﬁc system provides good starting state action distribution system’s dynamics constrain transitions reasonable subset actions given states. paper introduce policy architecture able efﬁciently learn large discrete action spaces. describe architecture trained using ddpg demonstrate good performance series tasks range tens million discrete actions. architectures type give policy ability generalize actions sub-linear complexity relative number actions. demonstrate considering subset full actions sufﬁcient many tasks provides signiﬁcant speedups. additionally demonstrate approximate approach nearest-neighbor lookup achieved often impacting performance slightly. future work direction would allow action representations learned training thus allowing actions poorly placed embedding space moved appropriate parts space. also intend investigate application methods wider range real-world control problems.", "year": 2015}