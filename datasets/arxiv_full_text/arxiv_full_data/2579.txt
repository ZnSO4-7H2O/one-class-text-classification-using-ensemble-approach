{"title": "Estimating individual treatment effect: generalization bounds and  algorithms", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a \"balanced\" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.", "text": "intense interest applying machine learning problems causal inference ﬁelds healthcare economics education. particular individual-level causal inference important applications precision medicine. give theoretical analysis family algorithms predicting individual treatment effect observational data assumption known strong ignorability. algorithms learn balanced representation induced treated control distributions look similar. give novel simple intuitive generalization-error bound showing expected estimation error representation bounded standard generalization-error representation distance treated control distributions induced representation. integral probability metrics measure distances distributions deriving explicit bounds wasserstein maximum mean discrepancy distances. experiments real simulated data show algorithms match outperform state-of-the-art. making predictions causal effects actions central problem many domains. example doctor deciding medication cause better outcomes patient; government deciding would beneﬁt subsidized training; teacher deciding study program would beneﬁt speciﬁc student. paper focus problem making predictions based observational data. observational data data contains past actions outcomes possibly context without direct access mechanism gave rise action. example might access records patients medications outcomes complete knowledge speciﬁc action applied patient. hallmark learning observational data actions observed data depend variables might also affect outcome resulting confounding example richer patients might better afford certain medications training might given motivated enough seek challenge untangle confounding factors make valid predictions. speciﬁcally work common simplifying assumption no-hidden confounding assuming factors determining actions taken observed. examples above would mean measured patient’s wealth employee’s motivation. learning problem estimating causal effects observational data different classic learning training data never individual-level effect. unit response possible actions actually received. close known machine learning literature learning logged bandit feedback distinction access model generating action. work differs much work causal inference focus individual-level causal effect pearl rather average population level. main contribution give best knowledge ﬁrst generalization-error bound estimating individual-level causal effect individual identiﬁed features bound leads naturally family representation-learning based algorithms show match outperform state-of-the-art methods several causal effect inference tasks. frame results using rubin-neyman potential outcomes framework follows. assume unit features action potential outcomes data unit potential outcomes depending treatment assignment observe observe known consistency assumption. example denote tests demographic factors diabetic patient denote standard medication controlling blood sugar denotes medication indicate patient’s blood sugar level given medications respectively. denote interested learning function expected treatment effect relative individual unit characteristics individual treatment effect example patient features predict treatments better outcome. fundamental problem causal inference data observe never both. mentioned above make important no-hidden confounders assumption order make conditional causal effect identiﬁable. formalize assumption using standard strong ignorability condition strong ignorability sufﬁcient condition function identiﬁable proof supplement. validity strong ignorability cannot assessed data must determined domain knowledge understanding causal relationships variables. approach problem estimating function learning functions using samples similar standard machine learning problem learning ﬁnite samples. however additional source variance work here example mostly rich patients received treatment mostly poor patients received treatment might unreliable estimation poor patients. paper upper bound additional source variance using integral probability metric measure distance distributions also known control treated distributions. practice speciﬁc ipms maximum mean discrepancy wasserstein distance show expected error learning individual treatment effect function upper bounded error learning plus term. randomized controlled trial setting term bound naturally reduces standard learning problem learning functions. bound derive points family algorithms based idea representation learning jointly learn hypotheses treated control representation minimizes weighted factual loss distance control treated distributions induced representation. viewed learning functions constraint encourages better generalization across treated control populations. experiments section apply algorithms based multilayer neural nets representations hypotheses along wasserstein distributional distances representation layer; figure basic architecture. foundational text causality pearl writes whereas traditional learning tasks attempt generalize instances another causal modeling task generalize behavior conditions behavior another set. causal models therefore chosen criterion challenges stability changing conditions... believe work points stability criterion causal inference strongly ignorable case. underlying causal graph causal direction data focus case causal graph simple known form hidden confounders. causal model assume common goal causal effect inference used applied sciences obtain average treatment effect ex∼p brieﬂy discuss standard statistical causal effect inference methods relate proposed method. note approaches assume form ignorability. widely used approaches estimating covariate adjustment also known back-door adjustment g-computation formula basic version covariate adjustment amounts estimating functions therefore covariate adjustment methods natural candidates estimating well using estimates however previous work subject focused asymptotic consistency much work generalization-error procedure. view results point previously unaccounted source variance using covariate adjustment estimate ite. suggest type regularization learning representations reduced distance treated control enabling type bias-variance trade-off. another widely used family statistical methods used causal effect inference weighting methods. methods propensity score weighting reweight units observational data make treated control populations comparable. methods yield immediately estimating individual level effect adapting purpose interesting research question. doubly robust methods combine re-weighting samples covariate adjustment clever ways reduce model bias again believe ﬁnding adapt concept double robustness problem effectively estimating interesting open question. adapting machine learning methods causal effect inference particular individual level treatment effect gained much interest recently. example wager athey athey imbens discuss treebased methods adapted obtain consistent estimator semi-parametric asymptotic convergence rate. recent work also looked machine learning method help detect heterogeneous treatment effects data randomized experiments available neural nets also used purpose exempliﬁed early work beck recently hartford work deep instrumental variables. work differs focusing generalization-error aspects estimating individual treatment effect opposed asymptotic consistency focusing solely observational study case randomized components instrumental variables. another line work causal inference community relates bounding estimate average treatment effect given instrumental variable hidden confounding example ignorability assumption hold work differs deal ignorable case bound different quantity generalization-error estimating individual level treatment effect. work strong connections work domain adaptation. particular estimating requires prediction outcomes different distribution observed one. error upper bound similarities generalization bounds domain adaptation given bendavid mansour ben-david cortes mohri bounds employ distribution distance metrics a-distance discrepancy metric related distance use. algorithm similar recent algorithm domain adaptation ganin principle domain adaptation methods could adapted estimation presented here. finally paper builds work johansson authors show connection covariate shift task estimating counterfactual outcome causal inference scenario. proposed learning representation data makes treated control distributions similar ﬁtting linear ridge-regression model bounded relative error ﬁtting ridge-regression using distribution reverse treatment assignment versus ﬁtting ridge-regression using factual distribution. unfortunately relative error bound informative regarding absolute quality representation. paper focus related substantive task estimating individual treatment effect building counterfactual error term. provide informative bound absolute quality representation. also derive much ﬂexible family algorithms including non-linear hypotheses much powerful distribution metrics form ipms wasserstein distances. finally conduct signiﬁcantly thorough experiments including realsection prove bound expected error estimating individual treatment effect given representation hypothesis deﬁned representation. bound expressed terms expected loss model learning observed outcomes function denoted standing factual; integral probability metric distance distribution treated control units. term classic machine learning generalization-error turn upper bounded using empirical error model complexity terms applying standard machine learning theory employ following assumptions notations. important notations notation supplement. space covariates bounded subset outcome space treatment binary variable. assume exists joint distribution treated control distributions distribution features conditioned treatment respectively. throughout paper discuss representation functions form representation space. make following assumption assumption representation twicedifferentiable one-to-one function. without loss generality assume image inverse representation pushes forward treated control distributions space denote induced distribution deﬁnition deﬁne treated control distributions induced one-to-one distributions obtained standard change variables formula using determinant jacobian representation function hypothesis deﬁned representation space loss function. deﬁne complimentary loss functions standard machine learning loss call factual loss denote expected loss respect distribution treatment assignment ﬂipped call counterfactual loss deﬁnition unit t))pdyt. expected factual denotes patients’ features treatment potential outcome mortality think measuring well predict mortality patients doctors’ actions sampled distribution data sample. measures well prediction would topsy-turvy world patients doctors inclined prescribe exactly opposite treatment real-world doctors would prescribe. deﬁnition expected factual treated control losses integral probability metrics always symmetric obey triangle inequality trivially satisfy ipmg rich enough function families also ipmg ipmg true metric corresponding probabilities. examples function families ipmg true metric family bounded continuous functions family -lipschitz functions unit-ball functions universal reproducing hilbert kernel space deﬁnition recall expected variance respect distribution ﬁrst state lemma bounding counterfactual loss step obtaining bound error estimating individual treatment effect. give main thoerem. proofs details supplement. marginal probability treatment. strong ignorability assumption lemma one-to-one representation function inverse hypothesis. family functions assume exists constant ﬁxed per-unit expected loss functions obey main idea proof showing \u0001pehe upper bounded expected factual loss expected counterfactual loss however cannot estimate since samples relevant therefore bound difference using ipm. choosing small function family make bound tighter. however choosing small family could result incomputable bound. example minimal choice evaluate expectation term cannot general evaluate pectations since assumption observe addition function families known efﬁciently compute distance gradients. paper function families available optimization tools. ﬁrst family lipschitz functions leads wasserstein distance denoted wass. second family norm- reproducing kernel hilbert space functions leading metric denoted mmd. wasserstein metrics consistent estimators efﬁciently computed ﬁnite sample case used various machine learning tasks recent years order explicitly evaluate constant theorem make assumptions elements problem. wasserstein case loss lipschitz constants condition number jacobian case make assumptions rkhs representability rkhs norms standard deviation yt|x. full details given supplement major results stated theorems cases obtain making smaller increases constant precluding trivial solutions making arbitrarily small. empirical sample family representations hypotheses upper bound respective empirical losses model complexity term using standard arguments ipms consistently estimated ﬁnite samples arises fact that negative variance term following hill athey imbens deﬁne error \u0001pehe terms conditional mean functions opposed ﬁtting random variables note deﬁnition simply proportion treated units population. weights compensate difference treatment group size sample theorem ipmg integral probability metric deﬁned function family ipms cannot compute factor equation treat part hyperparameter makes objective sensitive scaling even constant therefore normalize either projection batch-normalization ﬁxed scale. refer model minimizing counterfactual regression variant without balance regularization treatment-agnostic representation network train models minimizing using stochastic gradient descent backpropagate error hypothesis representation networks described algorithm prediction loss penalty term ipmg computed minibatch time. details obtain gradient respect empirical ipms supplement. evaluating causal inference algorithms difﬁcult many machine learning tasks since real-world data rarely access ground truth treatment effect. existing literature mostly deals ways. using synthetic semi-synthetic datasets outcome treatment assignment fully known; semi-synthetic ihdp dataset hill using real-world data randomized controlled trials problem using data rcts imbalance treated control distributions making method redundant. partially overcome problem using jobs dataset lalonde includes randomized non-randomized component. training randomized component evaluation. alleviates solve issue completely balanced dataset unsuited method. propose general framework called estimation based theoretical results above. algorithm end-to-end regularized minimization procedure simultaneously balanced representation data hypothesis outcome. draws intuition approach proposed johansson overcomes following limitations method theory requires two-step optimization procedure speciﬁc linear hypotheses learned representation treatment indicator might lost learned representation high-dimensional assume exists distribution strong ignorability holds. assume sample distribution standard assumption means treatment assignment determines potential outcome see. goal representation hypothesis minimize \u0001pehe work parameterized deep neural networks trained jointly end-to-end fashion figure model allows learning complex non-linear representations hypotheses large ﬂexibility. johansson parameterized single network using concatenation input. dimension high risks losing inﬂuence training. combat this ﬁrst contribution parameterize separate heads joint network former used estimate outcome treatment latter under control. means statistical power shared representation layers network effect treatment retained separate heads. note sample used update head corresponding observed treatment; example observation used update second contribution excplicitly account adbias induced treatment group imbalance. seek representation hypothesis minimizes trade-off predictive accuracy imbalance representation space using following oborem conditions. immediately suggest algorithm minimize upper bound respect either wasserstein order minimize error estimating individual treatment effect. leads algorithm below. within-sample task estimate units sample outcome treatment observed. corresponds common scenario cohort selected changed. task non-trivial never observe unit. out-of-sample setting goal estimate units observed outcomes. corresponds case patient arrives goal select best possible treatment. within-sample error computed training validation sets out-of-sample error test set. simulated outcome ihdp hill compiled dataset causal effect estimation based infant health development program covariates come randomized experiment studying effects specialist home visfuture cognitive test scores. treatment groups made imbalanced removing biased subset treated population. dataset comprises units covariates measuring aspects children mothers. simulated outcome implemented setting npci package following hill noiseless outcome compute true effect. report estimated pehe loss \u0001pehe absolute error average treatment effect \u0001ate m)|. results experiments ihdp presented table average realizations outcomes train/validation/test splits. investigate effects increasing imbalance original treatment groups constructing biased subsamples ihdp dataset. logistic-regression propensity score model form estimates conditional treatment probability. then repeatedly probability remove remaining control observation closest probability algorithm counterfactual regression integral probability metrics input factual sample scaling parameter loss function representation network initial weights outcome network initial weights function family ipm. sample mini-batch calculate gradient term ipmg}tij calculate gradients empirical loss obtain step size scalar matrix standard neural methods e.g. adam check convergence criterion balancing regularization task estimating ate. implemented feed-forward neural network fully-connected exponential-linear layers representation hypothesis. layer sizes layers used jobs representation hypothesis used ihdp. model trained using adam overview figure layers corresponding hypothesis regularized small weight decay. continuous data mean squared loss binary data log-loss. theory immediately apply log-loss curious model performs compare method ordinary least squares treatment feature separate regressors treatment k-nearest neighbor targeted maximum likelihood doubly robust method bayesian additive regression trees random forests causal forests well balancing linear regression balancing neural network johansson classiﬁcation tasks substitute logistic regression ols. choosing hyperparameters estimating pehe nontrivial; detail selection procedure applied methods subsection supplement. study lalonde widely used benchmark causal inference community treatment training outcomes income employment status training. dataset combines randomized study based national supported work program observational data form larger dataset presence randomized subgroup gives estimate ground truth causal effect. study includes covariates education well previous earnings. construct binary classiﬁcation task called jobs goal predict unemployment using feature dehejia wahba following smith todd lalonde experimental sample psid comparison group subjects unemployed study. average train/validation/test splits ratios figure policy risk jobs function treatment inclusion rate. lower better. subjects included treatment order estimated treatment effect given various methods. wass similar omitted avoid clutter. i∈c∩e control group. report error \u0001att |att cannot evaluate \u0001pehe dataset since ground truth ite. instead order evaluate quality estimation measure call policy risk. policy risk deﬁned average loss value treating according policy implied estimator. case model policy treat treat otherwise. policy risk rpol estimate randomized trial subset jobs ˆrpol ﬁgure risk function treatment threshold aligned proportion treated table risk begin noting indeed imbalance confers advantage using regularization term theoretical results indicate e.g. results wass tarnet ihdp table also figure even harder case increased imbalance treated control relative gain using method remains signiﬁcant. jobs smaller gain using penalties ihdp. believe case because minimizing bound observational data accounting bias evaluating predictions randomized subset treatment groups distributed identically. ihdp non-linear estimators signiﬁcantly better linear ones terms individual effect jobs dataset straightforward logistic regression remarkably well estimating att. however linear model ascribe uniform policy case treat everyone. nuanced policies offered non-linear methods achieve lower policy risk case causal forests cfr. emphasizes fact estimating average effect individual effect require different models. speciﬁcally smoothing many units yield good estimate might signiﬁcantly hurt estimation. k-nearest neighbors good within-sample results jobs evaluation performed randomized component suffers heavily generalizing sample expected. paper give meaningful intuitive error bound problem estimating individual treatment effect. bound relates estimation classic machine learning problem learning ﬁnite samples along methods measuring distributional distances ﬁnite samples. bound lends naturally creation learning algorithms; focus using neural nets representations hypotheses. apply theoryguided approach synthetic real-world tasks showing every case method matches outperforms state-of-the-art. important open questions theoretical considerations choosing weight best derive conﬁdence intervals model’s predictions integrate work complicated causal models hidden confounding instrumental variables. wish thank aahlad manas assistance experiments. also thank jennifer hill marco cuturi esteban tabak sanjong misra fruitful conversations stefan wager help code causal forests. supported career award belloni alexandre chernozhukov victor hansen christian. inference treatment effects selection among highdimensional controls. review economic studies ben-david shai blitzer john crammer koby pereira fernando analysis representations domain adaptation. advances neural information processing systems ben-david shai blitzer john crammer koby kulesza alex pereira fernando vaughan jennifer wortman. theory learning different domains. machine learning bengio yoshua courville aaron vincent pierre. representation learning review perspectives. pattern analysis machine intelligence ieee transactions chernozhukov victor chetverikov denis demirer mert duﬂo esther hansen christian double machine learnarxiv preprint treatment causal parameters. arxiv. maathuis marloes colombo diego kalisch markus b¨uhlmann peter. predicting causal effects large-scale systems observational data. nature methods funk michele jonsson westreich daniel wiesen chris st¨urmer brookhart alan davidian marie. doubly robust estimation causal effects. american journal epidemiology mooij joris peters jonas janzing dominik zscheischler jakob sch¨olkopf bernhard. distinguishing cause effect using observational data methods benchmarks. journal machine learning research ganin yaroslav ustinova evgeniya ajakan hana germain pascal larochelle hugo laviolette franc¸ois marchand mario lempitsky victor. domain-adversarial training neural networks. journal machine learning research http//jmlr.org/papers/ v/-.html. gretton arthur smola alex huang jiayuan schmittfull marcel borgwardt karsten sch¨olkopf bernhard. covariate shift kernel mean matching. dataset shift machine learning hartford jason lewis greg leyton-brown kevin taddy matt. counterfactual prediction deep instrumental variables networks. arxiv preprint arxiv. hoyer patrik janzing dominik mooij joris peters jonas sch¨olkopf bernhard. nonlinear causal discovery additive noise models. advances neural information processing systems johansson fredrik shalit sontag david. learning representations counterfactual inference. proceedings international conference machine learning shpitser ilya pearl judea. identiﬁcation conditional interventional distributions. proceedings twenty-second conference uncertainty artiﬁcial intelligence press sriperumbudur bharath fukumizu kenji gretton arthur sch¨olkopf bernhard lanckriet gert empirical estimation integral probability metrics. electronic journal statistics swaminathan adith joachims thorsten. batch learning logged bandit feedback counterfactual risk minimization. journal machine learning research taddy matt gardner matt chen liyun draper david. nonparametric bayesian analysis heterogenous treatment effects digital experimentation. journal business economic statistics triantaﬁllou soﬁa tsamardinos ioannis. constraint-based causal discovery multiple interventions overlapping journal machine learning research variable sets. ﬁrst deﬁne necessary distributions prove simple results them. assume joint distribution function recall assume consistency assume observe deﬁnition treatment effect unit equality assume independent conditioned equality follows consistency assumption. finally last equation composed entirely observable quantities estimated data since assume deﬁnition denote respectively treatment control distributions. representation function. assume differentiable. assumption representation function one-toone. without loss generality assume image deﬁne inverse deﬁnition representation function distribution deﬁned distribution induced deﬁne treatment control distributions induced one-to-one distribution obtained standard change variables formula using determinant jacobian case mapping spaces different dimensions. lemma notation distribution marginal probability treatment. treated distribution. control distribution. representation function mapping inverse function mapping distribution induced loss function expected loss unit treatment expected factual counterfactual loss expected treatment effect unit \u0001pehe expected error estimating individual treatment effect function ipmg integral probability metric distance induced function family distributions equality change variable formula. proof identical loss function e.g. absolute loss squared loss. deﬁnition representation function. hypothesis deﬁned representation space expected loss unit treatment pair proof immediate noting deﬁnitions losses. deﬁnition function family consisting functions pair distributions deﬁne integral probability metric ipmg deﬁnes pseudo-metric space probability functions sufﬁciently large function families ipmg proper metric examples sufﬁciently large functions families includes bounded continuous functions -lipschitz functions unit norm functions universal reproducing norm hilbert space. latter give rise wasserstein maximum mean discrepancy metrics respectively note function families three mentioned above absolute value omitted deﬁnition evaluate expectations expectations general unavailable since require evaluate treatment outcomes control control outcomes treated. therefore upper bound unknowable quantities taking supremum function family includes upper bound ignores details outcome amounts measuring distance distributions samples from control treated distribution. note randomized trial indeed straightforipm crucial condition lemma function subsections look speciﬁc function families evaluate inclusion condition entail particular derive speciﬁc bounds deﬁnition deﬁne state prove important technical lemma section. lemma invertible representation inverse. deﬁned deﬁnition family functions denote ipmg integral probability metric induced hypothesis. assume exists constant function theorem one-to-one representaφ deﬁned tion function inverse deﬁnition family functions denote ipmg integral probability metric induced hypothesis. loss assume exists constant functions have \u0001pehe upper bound terms standard generalization error treated control distributions separately. note cases might different sample sizes treated control show ﬁnite sample bounds generalization errors. also note upper bound easily adapted case absolute loss pehe case upper bound theorem factor instead stated above standard deviation replaced mean absolute deviation. proof straightforward simply applies triangle inequality inequality give speciﬁc upper bounds constant theorem using function families family -lipschitz functions family -norm reproducing kernel hilbert space functions. different assumptions distribution representation hypothesis family -lipschitz functions function lipschitz constant differentiable sufﬁcient condition k-lipschitz constant simplicity’s sake assume throughout subsection true labeling functions densities loss differentiable. however assumption could relaxed mere lipschitzness assumption. assumption exists constant assumption entails potential outcomes change smoothly function covariates assumption loss function differentiable exists constant additionally exists constant deﬁnition family lipschitz functions obtain so-called -wasserstein distance distributions denote wass. well known wass indeed metric distributions deﬁnition jacobian matrix point i.e. matrix partial derivatives σmax σmin denote respectively largest smallest singular values matrix deﬁne supx∈x σmax immediate result deﬁnition call representation function note non-constant representation function jacobian-normalized simple scalar multiplication. lemma assume jacobian-normalized representation inverse. lipschitz constant bounded assumption deﬁnition proof. inverse exists assumption one-to-one. jacobian matrix evaluated similarly jacobian matrix evaluated note since identity function therefore inequality matrix norm inequality equality inequality assumption norms gradient w.r.t inequality deﬁnition assumption jacobian-normalized noting singular values necessarily non-negative. lemma conditions lemma assume gradients bounded bounded gradient norm loss bounded gradient norm jacobian-normalized. lipschitz constant upper bounded control measures aspect complexity true underlying functions wish approximate. terms depend choice loss function size space term comes assumption hypothesis norm note smaller reducing bound might force factual loss term larger since small implies less ﬂexible finally consider term assumption normalized rather natural expect certain scale representation. furthermore show fact wasserstein distance positively homogeneous respect representation therefore lemma indeed assume normalized. speciﬁc choice jacobian-normalized scaling yields opinion interpretable result terms inverse condition number twice-differentiable minimized linear orthogonal transformation lemma wasserstein distance positive homogeneous scalar transformations underlying space. probability density functions deﬁned mapping distributions induced then functions unit ball rkhs hxhr reproducing kernel hilbert space corresponding kernels hilbert space mapping similarly recall major condition lemma function space s.t. lemma marginal probability treatment assume one-to-one jacobian-normalized representation function. lipschitz constant functions lipschitz constant loss function assumption hypothesis lipschitz constant then deviation yt|x. assumption exists conditional standard deviation functions yt|x assume assumption invertible representation function inverse. assume exists bounded linear operator further assume hilbert-schmidt norm γφhs bounded space well known result product )−h)·)−h) lies tensor product space equal hr⊗hr norm function general hilbert space version fact vector matrix frobenius norm square standard euclidean norm. therefore similar result norm function overall leads conclude using equation have using t)hr⊗hr norms given triangle inequality. inequality hilbert space inequality deﬁnition operator norm. equality norm adjoint operator equal norm original operator abused notation mean norm operators vice-versa. finally inequality assumptions lemma’s premise norm lemma marginal probability treatment assume assume distribution conditioned follows assumptions constant one-to-one representation function obeys assumption corresponding operator operator norm functions obey assumption bounded hilbert space norm hypothesis assume exist assume deﬁned respect here kernel matrix corresponding metric euclidean distance e−λφ−φ sizes control treatment groups. minimize entire objective frameworks commonly used training neural networks box. general computing wasserstein distance involves solving linear program prohibitively expensive many practical applications. cuturi showed approximation based entropic regularization obtained sinkhornknopp matrix scaling algorithm orders magnitude faster speed. dubbed sinkhorn distances approximation computed using ﬁxed-point iteration involving repeated multiplication kernel matrix algorithm cuturi framework. algorithm overview compute gradient algorithm computing disregarding gradient amounts minimizing upper bound sinkhorn transport. advanced ideas stochastic optimization distance recently proposed aude might used future work. framework agnostic parameterization experiments focus case neural network. convenience implementation represent ﬁxed-point iterations sinkhorn algorithm standard methods hyperparameter selection cross-validation generally applicable estimating pehe loss since potential outcome observed realworld data observed outcome nearest neighbor opposite treatment group surrogate counterfactual outcome. deﬁne nearestneighbor approximation pehe loss \u0001pehenn figure t-sne visualizations balanced representations ihdp learned algorithms wass. note nearest-neighbor like quality wasserstein distance results strip-like representation whereas linear results ball-like shape regions overlap small.", "year": 2016}