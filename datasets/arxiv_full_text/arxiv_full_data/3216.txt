{"title": "Tensor Canonical Correlation Analysis for Multi-view Dimension Reduction", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Canonical correlation analysis (CCA) has proven an effective tool for two-view dimension reduction due to its profound theoretical foundation and success in practical applications. In respect of multi-view learning, however, it is limited by its capability of only handling data represented by two-view features, while in many real-world applications, the number of views is frequently many more. Although the ad hoc way of simultaneously exploring all possible pairs of features can numerically deal with multi-view data, it ignores the high order statistics (correlation information) which can only be discovered by simultaneously exploring all features.  Therefore, in this work, we develop tensor CCA (TCCA) which straightforwardly yet naturally generalizes CCA to handle the data of an arbitrary number of views by analyzing the covariance tensor of the different views. TCCA aims to directly maximize the canonical correlation of multiple (more than two) views. Crucially, we prove that the multi-view canonical correlation maximization problem is equivalent to finding the best rank-1 approximation of the data covariance tensor, which can be solved efficiently using the well-known alternating least squares (ALS) algorithm. As a consequence, the high order correlation information contained in the different views is explored and thus a more reliable common subspace shared by all features can be obtained. In addition, a non-linear extension of TCCA is presented. Experiments on various challenge tasks, including large scale biometric structure prediction, internet advertisement classification and web image annotation, demonstrate the effectiveness of the proposed method.", "text": "canonical correlation analysis proven eﬀective tool two-view dimension reduction profound theoretical foundation success practical applications. respect multi-view learning however limited capability handling data represented two-view features many real-world applications number views frequently many more. although simultaneously exploring possible pairs features numerically deal multiview data ignores high order statistics discovered simultaneously exploring features. therefore work develop tensor straightforwardly naturally generalizes handle data arbitrary number views analyzing covariance tensor diﬀerent views. tcca aims directly maximize canonical correlation multiple views. crucially prove multi-view canonical correlation maximization problem equivalent ﬁnding best rank- approximation data covariance tensor solved eﬃciently using well-known alternating least squares algorithm. consequence high order correlation information contained diﬀerent views explored thus reliable common subspace shared features obtained. addition non-linear extension tcca presented. experiments various challenge tasks including large scale biometric structure prediction internet advertisement classiﬁcation image annotation demonstrate eﬀectiveness proposed method. features utilized many real-world data mining tasks frequently high dimension extracted multiple views example page content hyperlink represented bag-ofwords usually used page classiﬁcation blum mitchell foster common combine global local descriptors image annotation chua guillaumin applications features dimensions several hundred thousand. figure tensor motivation. pairwise correlation explored traditional extensions much information obtained simultaneously examining views explored proposed tcca. multi-view dimension reduction foster seeks low-dimensional common subspace compactly represent heterogeneous data data examples associated multiple high-dimensional features. often beneﬁts subsequent learning process signiﬁcantly curseof-dimensionality alleviated computation-al eﬃciency improved canonical correlation analysis designed inspect linear relationship sets variables hardoon bach jordan formally introduced multi-view dimension reduction method foster authors prove labeled instance complexity eﬀectively reduced certain weak assumptions. addition widely used multi-view classiﬁcation farquhar regression kakade foster clustering blaschko lampert chaudhuri etc. theoretically bach jordan bach jordan interpreted probabilistically latent variable model thus able involved larger probabilistic model. spite profound theoretical foundation practical success multi-view learning handle data represented two-view features. features utilized many real-world applications however usually extracted views. example diﬀerent kinds color texture shape features popular used visual analysis-based tasks image annotation video retrieval. typical approach generalizing several views maximize pairwise correlations diﬀerent views v´ıa main drawback strategy statistics pairs features explored high-order statistics obtained simultaneously examining features ignored. tackle problem develop tensor generalize handle arbitrary number views straightforward natural way. particular tcca aims directly maximize correlation canonical variables views achieved analyzing high-order covariance tensor data views. prove maximizing correlation equivalent approximating covariance tensor rank- tensor optimal least square sense. approximation investigated literature eﬃcient alternating least square algorithm adopted optimization kroonenberg leeuw lathauwer comon respect traditional pairwise correlation maximization statistics fig. illustrative example much correlation information encoded common subspace shared features multi-view dimension reduction thus hopefully better performance achieved. furthermore extend proposed tcca non-linear case useful feature dimensions high limited instances available. perform extensive experiments variety challenge tasks including large scale biometric structure prediction internet advertisement classiﬁcation image annotation. compare proposed method traditional foster multi-view extension v´ıa well representative unsupervised multi-view dimension reduction approaches long results conﬁrm eﬀectiveness proposed tcca. article organized follows. summarize closely related works section brief introduction traditional multi-view extension presented section section includes description formulation analysis proposed tcca well non-linear extension kernel tcca multi-view dimension reduction. extensive experiments presented section paper concluded section dimension reduction technique machine learning. goal dimension reduction dimensional representation high dimensional data feature selection feature transformation main approaches dimension reduction. former aims select subset variables original latter transforms data space fewer dimensions. dimension reduction performed either unsupervised laplacian eigenmaps belkin niyogi semi-supervised benabdeslem hindawi supervised setting diﬀered amount labeled information utilized. another research line multi-view learning attracted much attention recently. multi-view refer multiple feature representations object spatial viewpoints vision graphics applications generally classify multi-view learning algorithms three families weighted view combination lanckriet mcfee lanckriet multiview dimension reduction hardoon white view agreement exploration blum mitchell kumar multi-view dimension reduction focuses removing irrelevant redundant information benabdeslem hindawi reducing feature dimension data consists multiple views leveraging dependencies coherence complementarity views. diﬀerent views often assumed conditionally independent thus latent representation shared views obtained exploiting conditional independence structure multi-view data foster long white chen example canonical correlation analysis employed multi-view dimension reduction foster exploit underlying conditional independence redundancy assumption multi-view learning. general unsupervised learning method presented long multi-view data consensus representation learned ﬁrst applying dimension reduction technique view combining results matrix factorization. structured sparsity jenatton enforced among diﬀerent views learning lowdimension consensus representation allow information shared across subsets features adaptively. contrast unsupervised multi-view dimension reduction similarity/dissimilarity pairwise constraints utilized semi-supervised multi-view dimension reduction. chen supervising information also incorporated learned latent shared subspace largemargin latent markov network. methods local optimal subspace usually obtained. therefore white white proposed convex formulation learning shared subspace multiple sources. learned subspace conditional independence constraints enforced. canonical correlation analysis originally proposed hotelling ﬁnds bases random variables coordinates variable pairs projected bases maximally correlated hardoon much success achieved applying pattern recognition data mining. example svm-k proposed farquhar two-view classiﬁcation. combines kernel support vector machine single optimization problem authors prove rademacher complexity svm-k signiﬁcantly lower individual svms. kakade foster kakade foster presented multi-view regression algorithm regularized norm derived applying unlabeled data. authors show intrinsic dimension regression problem induced norm characterized correlation coeﬃcients obtained cca. conditionally uncorrelated assumption simple eﬃcient subspace learning algorithm based proposed chaudhuri multi-view clustering. algorithm shown work well much weaker separation conditions previous clustering methods. addition applications dozens developments concentrate inspecting relationship sets tensors rather vectors. example classical extended choi d-cca directly analyzes images without reshaping vectors. extensions local d-cca wang sparse d-cca multilinear considering high-order tensors studied share multiple modes cipolla cipolla presented architectures tensor correlation maximization applying canonical transformation non-shared modes. features good balance ﬂexibility descriptive power obtained. method also termed tensor quite diﬀerent approach proposed paper. main diﬀerence lies latter focuses analyzing high-order tensor data sets objective analyze high-order statistics among multiple vector data sets closely related works methods concerned maximum variance kettenring adaptive algorithm termed cca-ls v´ıa based least square regression. cca-maxvar algorithm performed weighted combination canonical variables views approximate latent common representation. approach requires costly singular value decomposition optimization cannot trained adaptive fashion. avoid drawbacks v´ıa reformulated ccamaxvar coupled regression problems seeks minimize distance pair canonical variables. reformulation proved equivalent original cca-maxvar formulation much eﬃcient learned adaptively. nevertheless still disadvantage cca-ls cca-maxvar namely pairwise correlations exploited high order correlations views ignored. developed following tensor framework rectify shortcoming. figure system diagram multi-view dimension reduction method proposed tcca. firstly diﬀerent kinds features extracted represent available instances diﬀerent views. covariance tensor calculated obtained representations discover correlation information views. approximating covariance tensor rank- tensors obtain transformation matrix p’th view. maps original dimensional common subspace ﬁnal representation concatenation orthogonal constraint imposed diﬀerent solutions obtained using iterative algorithm based regression v´ıa here vector canonical variables projected using i’th canonical vector p’th view. contrast cca-maxvar kettenring cca-ls v´ıa pairwise correlations considered propose tensor multi-view dimension reduction exploiting high-order tensor correlation views. diagram multi-view dimension reduction method using proposed tcca shown fig. diﬀerent kinds features color histogram wavelet texture local sift features ﬁrst extracted represent here intuitive illustration without loss generality. diﬀerent sets features used calculate data covariance tensor subsequently decomposed weighted rank- tensors i.e. original high dimensional features dimensional common subspace. projected features concatenated ﬁnal representation instances. details technique given mode-p matricization denoted matrix obtained mapping ﬁbers associated p’th dimension rows aligning corresponding multiplication manipulated matrix multiplication storing tensors metricized form i.e. speciﬁcally series p-mode product expressed series kronecker products given solution obtained using alternating least square algorithm kroonenberg leeuw comon algorithms high-order power method lathauwer tensor power method allen also applied optimization empirical ﬁndings indicate algorithm performs best experiments. two-view perform recursive maximization correlation linear combinations however cannot expect diﬀerent linear combinations uncorrelated other rank orthogonality constraints cannot imposed since rank- decomposition orthogonal decomposition high-order tensors cannot satisﬁed simultaneously lathauwer non-linear feature space. develop kernel tensor extends proposed tcca non-linear case. ktcca aims non-linear projections ﬁrst projecting data higher dimensional space induced feature mapping similar tcca problem equivalent ﬁnding best rank- approximation solution found using algorithm. recursively maximizing correlation obtain projected data p’th view complexity analysis time space complexities proposed tcca model closely related size tensor straightforwardly space complexity tensor calculated oﬄine common speculate time complexity ktcca determined tensor size space time complexities respectively. means ktcca capable scaled problems high feature dimensions small number instances. section empirically validate eﬀectiveness proposed tcca biometric structure prediction advertisement classiﬁcation problem following foster well challenging image annotation task chua following experiments random choices labeled instances used. twenty percent test data used validation means parameters corresponding best performance validation used testing. evaluation criterion classiﬁcation accuracy. ﬁrst sets experiments regularized least squares base learner following foster given labeled instances {}nl positive trade-oﬀ parameter according foster constant feature appended instance include bias term image annotation k-nearest-neighbor classiﬁer utilized candidate speciﬁcally compare using single view feature achieves best performance rls/knn-based classiﬁcation. concatenating normalized features views long vector performing common representation diﬀerent views. formulation regularization term added control model complexity parameter biometric structure prediction advertisement classiﬁcation according foster parameter tuned {i|i image annotation. implementation details found foster diﬀerent views subsets views. subset achieves best performance termed combine results subsets average predicted scores rls-based classiﬁcation adopt majority voting strategy knn. combination approach termed dataset used experiments secstr benchmark dataset evaluating semisupervised systems chapelle task associated dataset predict secondary structure given amino acid protein based sequence window centered around amino acid chapelle secstr dataset large-scale contains instances. randomly select instances labeled samples. also unlabeled instances observe performance three cca-based methods respect diﬀerent amounts unlabeled data. following foster provided data used common subspace cca-based methods. performance evaluated transductive setting unlabeled samples instances. ssmvd naturally transductive since learn low-dimensional representation given data directly projection matrix learned data. therefore methods cannot handle large datasets experiments conducted instances. particular needs solve eigenperformance diﬀerent methods best dimensions summarized table results observe that concatenation strategy comparable slightly better strategy using best single view features learning common subspace compared multi-view dimension reduction methods signiﬁcantly better baselines dimensionalities properly according accuracy validation dataset. particular superior although subset views utilized former; accuracy three cca-based methods increases increasing number unlabeled data. combining results diﬀerent subsets better cca-ls superior performance best dimension comparable. number unlabeled data ssmvd comparable cca-ls respectively; performance tcca decease signiﬁcantly cca-ls number dimensions high. main reason algorithm used tcca seeks maximize canonical correlations factors simultaneously greedily orthogonal decomposition components allen main variance tends explained uniformly factors ﬁrst several factors. also reason oscillations tcca; proposed tcca signiﬁcantly outperforms methods dimensionalities. demonstrates high order correlation information features well discovered exploring kind information much better experiments conducted dataset well-known machine learning repository. task predict whether given hyperlink advertisement. instances dataset. randomly choose instances labeled training samples instances except validation utilized unlabeled samples common subspace. performance evaluated transductive setting unlabeled samples. features described kushmerick omit attributes missing values height image. remained attributes represented binary features indicate presence/absence corresponding terms. cca-ls tcca divide features three views follows view- features based terms images caption text. dimensions; view- features based terms current site. dimensions; view- features based terms anchor url. dimensions. fig. shows classiﬁcation accuracy compared methods accuracies best dimensions summarized table contrast observations last experiments that accuracy concatenation strategy best single view almost same. performance relatively worse since feature dimension experiments high over-ﬁtting occurs given limited number labeled samples; performance ssmvd ﬁrst increase decrease sharply increasing number dimension cca-based methods much steady; improvement tcca compared cca-based methods great last experiments. need samples approximate true underlying high order correlation compared traditional pairwise correlation since variables estimated high order statistics. unlabeled instances utilized experiments much fewer thus high order correlation information well explored. cca-ls comparable reason. verify eﬀectiveness proposed algorithm natural image dataset nus-wide chua dataset contains images experiments conduct subset consists images belonging mammal concepts bear horse tiger whale zebra. randomly split images training images test images. distinguishing concepts challenging since many similar other e.g. dataset choose three types visual feature namely visual words based sift lowe descriptors color auto-correlogram wavelet texture represent image chua annotation performance compared methods shown fig. table seen results that general performance improves increased number labeled instances; cca-ls comparable best performance cca-ls usually higher; performance poor large ssmvd much steady superior cca-ls sometimes; accuracies cca-ls ﬁrst increase decrease increasing number dimension results proposed tcca satisfactory even though large; accuracy tcca signiﬁcantly better methods dimensionalities. evaluate non-linear extension proposed tcca image annotation task. discussed section non-linear extension able handle small sample size problem feature dimensions high possibly inﬁnite. thus randomly choose small samples animal subset. perform non-linear classiﬁcation construct kernel kind feature. kernel deﬁned using single view kernel achieves best performance knn-based classiﬁcation. averaging normalized kernels views performing knn-based classiﬁcation. kcca hardoon using kcca formulation presented hardoon {i|i setup kcca kcca similar experimental results shown fig. table compared results fig. that although small number unlabeled samples utilized performance better since separability improved non-linear projection implemented kernel trick shawetaylor cristianini simple view combination strategy outperforms best single view kernel signiﬁcantly comparable kcca kcca slightly better kcca proposed ktcca achieves best performance dimensionalities. memory ddr-ram. results diﬀerent datasets shown fig. results observe that costs proposed tcca higher cca-based methods general. decomposition performed algorithm kroonenberg leeuw comon could result satisfactory accuracy eﬃcient; tcca much eﬃcient ssmvd feature dimensions high number instances large demonstrates superiority tcca compared existed unsupervised multi-view dimension reduction methods large sample size problems. standard cannot deal multi-view data typical multi-view extensions ignore high order statistics among feature views. resolve problem presented tensor discover statistics analyzing covariance tensor views. experimental validation variety application tasks conclude that ﬁnding common subspace views using cca-based strategy often better simply concatenating features especially feature dimension high; examining statistics require unlabeled data utilized often leads better performance; exploring high order statistics proposed tcca outperforms methods especially dimension common subspace high. compared traditional multi-view extensions main disadvantage proposed tcca high computational cost. tcca cost lies tensor decomposition point paper. future devote eﬃcient tensor decomposition methods could speed tcca introduce parallel computing technique utilizing accelerate tensor decomposition.", "year": 2015}