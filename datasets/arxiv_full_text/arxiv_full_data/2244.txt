{"title": "Manifold Regularization for Kernelized LSTD", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Policy evaluation or value function or Q-function approximation is a key procedure in reinforcement learning (RL). It is a necessary component of policy iteration and can be used for variance reduction in policy gradient methods. Therefore its quality has a significant impact on most RL algorithms. Motivated by manifold regularized learning, we propose a novel kernelized policy evaluation method that takes advantage of the intrinsic geometry of the state space learned from data, in order to achieve better sample efficiency and higher accuracy in Q-function approximation. Applying the proposed method in the Least-Squares Policy Iteration (LSPI) framework, we observe superior performance compared to widely used parametric basis functions on two standard benchmarks in terms of policy quality.", "text": "abstract policy evaluation value function q-function approximation procedure reinforcement learning necessary component policy iteration used variance reduction policy gradient methods. therefore quality signiﬁcant impact algorithms. motivated manifold regularized learning propose novel kernelized policy evaluation method takes advantage intrinsic geometry state space learned data order achieve better sample efﬁciency higher accuracy q-function approximation. applying proposed method least-squares policy iteration framework observe superior performance compared widely used parametric basis functions standard benchmarks terms policy quality. consider discrete-time inﬁnite horizon discounted markov decision process states admissible actions discount factor reward function transition probability density solving means ﬁnding policy maximizes accumulated discounted rewards states actions distribution induced applying policy policy evaluation component policy iteration policy gradient approaches objective ﬁnding q-function associated stationary deterministic policy ﬁxed point bellman operator solution bellman equation state space large inﬁnite solving exactly becomes intractable. least-squares temporal difference widely used simulation-based algorithm approximately solving projected bellman equation linear representation value function order apply lstd policy iteration lagoudakis parr proposed q-function extension showed resulting policy iteration algorithm successfully applied control problems. work study lstd-based approximate policy evaluation methods beneﬁt manifold regularized learning intuition q-function smooth manifold states necessary ambient euclidean space. manifold structure naturally arises many robotics tasks constraints state space. example contact e.g. foot ground hand object restricts feasible states along manifold union manifolds. examples include cases state variables belong special euclidean group encode poses obstacle avoidance settings geodesic paths state vectors naturally better motivated geometrically infeasible straight-line paths ambient space. provide brief introduction manifold regularized learning kernelized lstd approach proposed method detailed experimental results presented finally conclude paper related future work. manifold regularization previously studied semi-supervised learning datadependent regularization exploits geometry input space thus achieving better generalization error. given labeled dataset laplacian regularized least-squares method ﬁnds function reproducing kernel hilbert space data proper complexity generalization data matrices choices k·km) scalars regularization parameters. natural choice rx∈m k∇mf support assumed k·km compact manifold gradient along unknown learning settings k·km estimated empirically optimization problem becomes graph laplacian note problem still convex since graph laplacian positive semideﬁnite multiplicity eigenvalue number connected components graph. fact broad family graph regularizers used context includes iterated graph laplacian theoretically better standard laplacian diffusion kernel graphs. farahmand introduce regularization extension kernelized lstd termed regularized lstd featuring better control complexity function approximator regularization mitigating burden selecting basis functions kernel machinery. reg-lstd formulated adding regularization terms steps nested minimization problem equivalent original lstd informally speaking projection enforced close projection kernel selected tensor product kernel furthermore kronecker delta function admissible action ﬁnite. approach combines beneﬁts manifold regularized learning incorporates geometric structure kernelized lstd regularization offers better control function complexity eases feature selection. concretely besides regularization term controls complexity ambient space augment objective function manifold regularization term enforces smooth manifold supports particular manifold regularization term chosen large penalty imposed varying fast along manifold. empirically estimated manifold regularization term optimization problem becomes present experimental results standard benchmarks two-room navigation cartpole balancing. reg-lstd manifold regularization compared reg-lstd without manifold regularization lstd three commonly used basis function construction mechanisms polynomial radial basis functions laplacian eigenmap terms quality policy produced least-squares policy iteration kernel used experiments expδ hyperparameter combinatorial laplacian adjacency matrix computed ǫ-neighborhood weights table two-room navigation. quality policies computed lspi-based methods different basis functions measured number states whose corresponding actions optimal varying number samples averaged runs. maximum number iterations lspi best performance among lspi iterations coarsely tuned hyperparameters reported. particular polynomial degree ranges discretization per-dimension ranges table cart-pole. quality policies computed lspi-based methods different basis functions measured average number steps termination trials maximum length trials numbers average runs lspi iteration number hyperparameter tuning two-room navigation. space discrete grid world admissible actions stepping four cardinal directions i.e. right down left. dynamics stochastic action succeeds probability movement blocked obstacle border otherwise leaves agent location. goal navigate diagonally opposite corner room single-cell doorway connecting rooms. reward goal location otherwise discount factor data collected beforehand uniformly sampling states actions used throughout lspi iterations. seen table laplacian eigenmap exploits intrinsic state geometry outperforms parametric basis functions polynomial rbf. mr-lstd method propose achieves best performance. cart-pole balancing task balance pole upright applying force cart it’s attached agent constantly receives reward trial ends reward it’s away upright posture. contrary two-room navigation task state space continuous consisting angle angular velocity pole. admissible actions ﬁnite pushing cart left right. discount factor data collected random episodes i.e. starting perturbed upright posture applying uniformly sampled actions. results reported table shows reg-lstd achieves signiﬁcantly better performance parametric basis functions performance even improved manifold regularization. closest work paper recently introduced utilized manifold regularization learning state representation unsupervised learning adopting learned representation policy iteration. contrast work naturally blend manifold regularization policy evaluation possibly provable performance guarantee also work constructing basis functions directly geometry e.g. laplacian methods geodesic gaussian kernels furthermore different regularization mechanisms lstd proposed including regularization feature selection nested penalization avoid overﬁtting propose manifold regularization kernelized lstd approach order exploit intrinsic geometry state space better sample efﬁciency q-function approximation demonstrate superior performance standard benchmarks. future work directions include accelerating structured random matrices kernel machinery graph sketching graph regularizer construction scale large datasets rich observations e.g. images providing theoretical justiﬁcation combining manifold regularization deep neural nets policy evaluation e.g. policy iteration algorithms learning datadependent kernel capturing geometry makes easier derive algorithms extension continuous action spaces constructing kernels policy improvement tractable", "year": 2017}