{"title": "Deep Value Networks Learn to Evaluate and Iteratively Refine Structured  Outputs", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "We approach structured output prediction by optimizing a deep value network (DVN) to precisely estimate the task loss on different output configurations for a given input. Once the model is trained, we perform inference by gradient descent on the continuous relaxations of the output variables to find outputs with promising scores from the value network. When applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar estimating the intersection over union between the input and ground truth masks. For multi-label classification, the DVN's objective is to correctly predict the F1 score for any potential label configuration. The DVN framework achieves the state-of-the-art results on multi-label prediction and image segmentation benchmarks.", "text": "approach structured output prediction optimizing deep value network precisely estimate task loss different output conﬁgurations given input. model trained perform inference gradient descent continuous relaxations output variables outputs promising scores value network. applied image segmentation value network takes image segmentation mask inputs predicts scalar estimating intersection union input ground truth masks. multi-label classiﬁcation dvn’s objective correctly predict score potential label conﬁguration. framework achieves state-of-the-art results multi-label prediction image segmentation benchmarks. structured output prediction fundamental problem machine learning entails learning mapping input objects complex multivariate output structures. because structured outputs live high-dimensional combinatorial space needs design factored prediction models expressive also computationally tractable learning inference. computational considerations large body previous work tsochantaridis focused relatively weak graphical models pairwise small clique potentials. models capable learning complex correlations among random variables making suitable tasks requiring *work done internship google brain. z¨urich gifs.com google brain mountain view usa. correspondence michael gygli <gyglivision.ee.ethz.ch> mohammad norouzi <mnorouzigoogle.com>. expressive family energy-based models studied lecun belanger mccallum exploits neural network score different joint conﬁgurations inputs outputs. network trained simply resorts gradient-based inference mechanism energy outputs. despite recent developments optimizing parameters deep energy-based models remains challenging limiting applicability. moving beyond large margin training used previous work paper presents simpler effective objective inspired value based reinforcement learning training energy-based models. intuition learning critique different output conﬁgurations easier learning directly come optimal predictions. accordingly build deep value network takes input corresponding output structure inputs predicts scalar score evaluating quality conﬁguration correspondence input exploit loss function compares output ground truth label teach evaluate different output conﬁgurations. goal distill knowledge loss function weights value network inference absence labeled output still rely value judgments neural compare outputs. enable effective iterative reﬁnement structured outputs gradient ascent score similar belanger mccallum relax discrete output variables live continuous space. moreover extend domain loss functions loss applies continuous variable outputs. example multi-label classiﬁcation instead enforcing output dimension binary generalize notion score apply continuous predictions. image segmentation similar generalization intersection union. then train many output examples encouraging network predict precise loss scores almost output conﬁguration. figure illustrates gradient based inference process optimized image segmentation. figure segmentation results weizmann horses test samples. gradient based inference method iteratively reﬁnes segmentation masks maximize predicted scores deep value network. starting black mask step predictions converge within steps yielding output segmentation. https//goo.gl/olufh animated results. paper presents novel training objective deep structured output prediction inspired value-based reinforcement learning algorithms precisely evaluate quality input-output pair. assess effectiveness proposed algorithm multi-label classiﬁcation based text data image segmentation. obtain state-of-the-art results cases despite differences domains loss functions. even given small number input-output pairs able build powerful structure prediction models. example weizmann horses dataset without form pre-training able optimize million network parameters training images multiple crops. deep value network setup outperforms methods pre-trained large datasets imagenet methods operate larger inputs. source code based tensorflow available https//github.com/gyglim/dvn. structured output prediction entails learning mapping input objects multivariate discrete outputs given training dataset input-output pairs y∗)}n truth output structures high-dimensional space often infeasible measures quality mapping loss function evaluates distance different output structures. given loss function quality mapping measured empirical loss loss take arbitrary form often nondifferentiable. multi-label classiﬁcation common loss negative score image segmentation typical loss negative intersection union structured output prediction methods learn mapping inputs outputs score function evaluates different input-output conﬁgurations based linear function joint input-output features empirical loss amenable numerical optimization argmax discontinuous. structural formulations introduce margin violation variable training pair deﬁne continuous upper bound empirical loss. upper bound loss previous work deﬁnes surrogate objective empirical loss summing bound different training examples plus regularizer. surrogate objective convex makes optimization convenient. paper inspired structural formulation above give convexity objective obtain expressive models using multi-layer neural networks. speciﬁcally generalize formulation three ways non-linear score function denoted fuses together jointly learns features. gradient descend iterative reﬁnement outputs approximately best optimize score function regression objective predicted scores closely approximate negative loss values deep value network non-linear function trying evaluate value output conﬁguration accurately. structural svm’s objective score surface vary long violate margin constraints contrast restrict score surface much penalizing whenever overunderestimates loss values. seems beneﬁcial neural network ﬂexibility adding suitable constraints help regularization. call model deep value network emphasize importance notion value shaping ideas architecture thought example structured prediction energy network similar inference strategy. belanger mccallum rely structural surrogate objective train spens whereas inspired value based reinforcement learning learn accurate estimate values empirically outperforms large margin spens multi-label classiﬁcation using similar neural network architecture. propose deep value network architecture denoted evaluate joint conﬁguration input corresponding output neural network. specifically deep value network takes input jointly several layers followed non-linearities predicts scalar evaluates quality output compatibility assume training access oracle value function quantiﬁes quality oracle value function assigns optimal values input-output pairs given ground truth labels training goal optimize parameters value network denoted mimic behavior oracle value function much possible. example oracle value functions image segmentation multi-label classiﬁcation include metrics deﬁned denotes number dimension active denotes number dimensions least active. assuming learned suitable value network attains every input-output pairs order infer prediction input valued highly value network needs argmaxy described below. straightforward approximate inference algorithms based graph-cut loopy belief propagation easily applicable. instead advocate using simple gradient descent optimizer inference. facilitate that relax structured output variables live real-valued space. example instead using make inference algorithm work during training make sure value estimates optimized along inference trajectory. alternatively make input convex neural networks denotes operator projects predicted outputs back feasible solutions remains simplest case operator projects dimensions smaller zero back zero dimensions larger one. ﬁnal gradient step simply round become discrete. empirically trained generated tend become nearly binary themselves. train using oracle value function ﬁrst needs extend domain applies continuous output y’s. scores simply extend notions intersection union using element-wise operators training objective aims minimizing discrepancy training triplets denoted v∗}n much like q-learning training evolves time make experience replay buffer. section discuss several strategies generate training tuples experiments evaluate strategies terms empirical loss given dataset training tuples appropriate loss regress values. speciﬁcally since scores used cross-entropy loss oracle values values. such neural network sigmoid non-linearity predict number loss takes form exact form loss signiﬁcant impact performance loss functions used e.g. high level overview training shown algorithm simplicity show case using queue batch size training tuple comprises input output corresponding oracle value i.e. training tuples generated signiﬁcantly impacts performance structured prediction algorithm. particular important tuples chosen provide good coverage space possible outputs result large learning signal. exist several ways generate training tuples including elaborate methods below present comparison performance section ablation experiments suggest combining examples gradient based inference adversarial tuples works best. ground truth. setup simply ground truth outputs training provide positive examples. inference. scenario generate samples running gradient based inference algorithm along training. procedure useful helps learning good value estimate output hypotheses generated along inference trajectory test time. speed training parallel inference using slightly older neural network weights accumulate inferred examples queue. random samples. approach sample solution proportional exponentiated oracle value i.e. sampled probability exp{v∗/τ} controls concentration samples vicinity ground truth. recover ground truth samples above. follow sample exponentiated value distribution using stratiﬁed sampling group according values. approach provides good coverage space possible solutions. adversarial tuples. maximize cross-entropy loss used train value network generate adversarial tuples using gradient based optimizer adversarial tuples outputs network overunderestimates oracle values most. strategy ﬁnds difﬁcult tuples provide useful learning signal ensuring value network minimum level accuracy across outputs surge recent interest using neural networks structured prediction structured prediction energy network inspired part identical architecture. importantly motivation learning objective spens dvns distinct spens rely max-margin surrogate objective whereas directly regress energy input-output pair corresponding loss. unlike spens consider multi-label classiﬁcation problems allowed train deep convolutional network successfully address complex image segmentation problems. concurrent work explored another improving training spens directly backpropagating error gradient-based inference process. requires expensive gradient computation unrolling computation graph number inference gradient steps. contrast training algorithm much efﬁcient requiring back-propagation value network once. recent work applied expressive neural networks structured prediction achieve impressive results machine translation image audio synthesis autoregressive models impose order output variables predict outputs variable time formulating locally normalized probabilistic model. training often efﬁcient limitation models inference complexity grows linearly number output dimensions; acceptable high-dimensional output structures. contrast inference method efﬁcient output dimensions updated parallel. approach inspired part success previous work value-based reinforcement learning q-learning overview). main idea learn estimate future reward optimal behavior policy point time. recent algorithms neural network function approximator model estimate action values adopt similar ideas structured output prediction task loss optimal value estimate. unlike gradient based inference algorithm optimal solutions test time. gradient based inference sometimes called deep dreaming impressive artwork inﬂuential designing deep dreaming style transfer methods iteratively reﬁne input neural optimize prespeciﬁed objective. methods often pre-trained network deﬁne notion perceptual loss contrast train task speciﬁc value network learn characteristics task speciﬁc loss function learn network’s weights scratch. image segmentation problem computer vision canonical example structured prediction. many segmentation approaches based convolutional neural networks proposed deep neural network make per-pixel prediction thereby modeling pairs pixels conditionally independent given input. diminish conditional independence problem recent techniques propose model dependencies among output labels reﬁne initial cnn-based coarse segmentation. different ways incorporate pairwise dependencies within segmentation mask obtain expressive models proposed methods perform joint inference segmentation mask dimensions graph-cut message passing loopy belief propagation name variants. methods incorporate higher order potentials crfs model global shape priors restricted boltzmann machines methods learn iteratively reﬁne initial prediction cnns coarse segmentation mask contrast paper presents framework training score function gradient based inference algorithm mind training. deep value network applies generic structured prediction tasks opposed methods above exploit complex combinatorial structures special constraints submodularity design inference algorithms. rather expressive energy models simplest conceivable inference algorithm gradient descent. evaluate proposed deep value networks tasks multi-label classiﬁcation binary image segmentation -class face segmentation task. section investigates sampling mechanisms training section visualizes learned models. start evaluating method task predicting tags text inputs. standard benchmarks multi-label classiﬁcation namely bibtex bookmarks introduced task multiple labels possible example correct number known. given structure label space methods modeling label correlations often outperform models independent label predictions. compare standard baselines including per-label logistic regression two-layer neural network cross entropy loss well spens prlr state-of-the-art datasets. allow direct comparison spens adopt architecture paper. architecture combines local predictions non-linear linear figure deep value network feed-forward convolutional architecture used segmentation. network takes image segmentation mask input predicts scalar evaluating compatibility input pairs. strategies possible obtain full resolution segmentation investigate section comparison also implemented fully convolutional network baseline using convolutional layers value network explicitly stated masks averaged crops model test compare model weizmann horses segmentation task table tune hyperparameters model validation best hyper-parameters found ﬁne-tune combination training validation sets. report mean image well whole test commonly done literature. clear approach outperforms previous methods signiﬁcant margin metrics. model shows strong segmentation results without relying externally trained features weights value network learned scratch crops training images. even though number examples small dataset observe overﬁtting during training attribute able generate large segmentation masks training. so-called global network scores label conﬁguration non-linear function independent eqs. local prediction global networks hidden layers softplus non-linerarities. follow experimental protocol report scores test split results summarized table seen table method outperforms logistic regression baselines large margin. also signiﬁcantly improves spen despite using pre-training. spen hand relies pre-training feature network logistic loss obtain good results. results even outperform encouraging method speciﬁc classiﬁcation encourages sparse low-rank predictions whereas technique dataset speciﬁc regularizers. weizmann horses dataset dataset commonly used evaluating image segmentation algorithms dataset consists images left oriented horses binary segmentation masks. follow evaluate segmentation results dimensions. satisfactory segmentation horses requires learning strong shape priors complex high level reasoning especially resolution pixels small parts legs often barely visible image. follow experimentation protocol report results test split. simple architecture consisting convolutional fully connected layers learning rate apply dropout ﬁrst fully connected layer keeping probability determined validation set. empirically found work best stratiﬁed sampling. training data augmentation purposes randomly crop image similar test time various conﬁguration inference ground truth inference stratiﬁed sampling inference adversarial mask averaging joint inference mask avg. non-binary joint inf. non-binary mask averaging joint inference table shows quantitative results. performs reasonably well outperformed state methods dataset. attribute three reasons. pre-training direct optimization per-pixel prediction methods input resolution properties dataset. contrast horses faces thin parts exhibit limited deformations. thus feed forward method used produces coarser smooth predictions sufﬁcient obtain good results. indeed also observed negligible improvement reﬁning predictions conditional random fields restricted boltzmann machines despite this model able learn prior shape align image evidence cases. failure cases include failing recognize subtle rare parts mustaches given small size difﬁculties correctly labeling blond hair. figure shows qualitative results segmentation method dataset. section analyze different conﬁgurations method. already mentioned generating appropriate training data method learning good value networks. compare main approaches inference figure qualitative results weizmann dataset. comparison previous works able learn strong shape prior thus correctly detect horse shapes including legs. previous methods often misled objects contrast thus generating inferior masks. references implementation correctly segmenting legs ensuring segmentation masks single connected component indeed masks produced correspond much reasonable horse shapes opposed methods seem capable learning complex shape models effectively grounding visual evidence. also note comparison table prior methods using larger inputs also outperformed dvns. labeled faces wild dataset proposed face recognition contains images. subset faces later annotated segmentation labels provided superpixel basis consist classes face hair background. dataset test application approach multiclass segmentation. train validation test splits method predicts labels pixels follow pixel figure visualization learned horse shapes weizmann dataset. left right mean mask training mask generated providing mean horse image training outputs generated model given mean horse image plus gaussian noise input. visualize model learned inference algorithm mean image weizmann dataset optionally perturb mean image adding gaussian noise. masks obtained procedure shown figure segmentation masks found value network mean images resemble side-view horse uncertainty head positions. parts amount variation dataset. even though noisy images contain horses value network hallucinates proper horse silhouettes model trained paper presents framework structured output prediction learning deep value network predicts quality different output hypotheses given input. learns predict value based both input output implicitly learns prior output variables takes advantage joint modelling inputs outputs. visualizing prior image segmentation indeed model learns realistic shape priors. furthermore rather learning model optimizing surrogate loss using dvns allows directly train network accurately predict desired performance metric even non-differentiable. apply method several standard datasets multi-label classiﬁcation image segmentation. experiments show dvns apply different structured prediction problems achieving state-of-the-art results pre-training. future work plan improve scalability computational efﬁciency algorithm inducing input features computed solely going computed once. gradient based inference improve injecting noise gradient estimate similar hamiltonian monte carlo sampling. finally explore better ways initialize inference process. ground truth inference stratiﬁed sampling inference adversarial training. experiments conducted weizmann dataset described above. table portion reports results different approaches training dataset. seen including adversarial training works best followed stratiﬁed sampling. methods help explore space segmentation masks vicinity ground truth masks better opposed including ground truth masks. adding adversarial examples works better stratiﬁed sampling adversarial examples masks model least accurate. thus masks provide useful gradient information help improve model. also investigate ways model averaging averaging segmentation masks multiple crops leads improved performance. masks averaged na¨ıvely result becomes blurry making difﬁcult obtain ﬁnal segmentation. instead joint inference updates complete segmentation mask step using gradients individual crops. procedure leads clean near-binary segmentation masks. manifested performance using foreground conﬁdence joint inference leads somewhat improved segmentation results even after binarization particular using fewer crops. references abadi mart´ın agarwal ashish barham paul brevdo eugene chen zhifeng citro craig corrado greg davis andy dean jeffrey devin matthieu ghemawat sanjay goodfellow harp andrew irving geoffrey isard michael yangqing jozefowicz rafal kaiser lukasz kudlur manjunath levenberg josh man´e monga rajat moore sherry murray derek olah chris schuster mike shlens jonathon steiner benoit sutskever ilya talwar kunal tucker paul vanhoucke vincent vasudevan vijay vi´egas fernanda vinyals oriol warden pete wattenberg martin wicke martin yuan zheng xiaoqiang. tensorflow large-scale machine learning heterogeneous systems http//tensorflow. org/. software available tensorﬂow.org. chen liang-chieh papandreou iasonas murphy kevin yuille alan deeplab semantic image segmentation deep convolutional nets atrous convolution fully connected crfs. arxiv. chen liang-chieh papandreou george kokkinos iasonas murphy kevin yuille alan semantic image segmentation deep convolutional nets fully connected crfs. arxiv. huang gary ramesh manu berg tamara learned-miller erik. labeled faces wild database studying face recognition unconstrained environments. technical report technical report university massachusetts amherst victoria singh sameer luheng taskar zettlemoyer luke. multi-label learning posterior regularization. nips workshop modern machine learning natural language processing norouzi mohammad bengio samy chen zhifeng jaitly navdeep schuster mike yonghui schuurmans dale. reward augmented maximum likelihood neural structured prediction. nips szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. iclr oord a¨aron dieleman sander heiga simonyan karen vinyals oriol graves alex kalchbrenner senior andrew kavukcuoglu koray. wavenet generative model audio. arxiv. nguyen dosovitskiy alexey yosinski jason brox thomas clune jeff. synthesizing preferred inputs neurons neural networks deep generator networks. arxiv. zheng shuai jayasumana sadeep romera-paredes bernardino vineet vibhav zhizhong dalong huang chang torr philip conditional random ﬁelds recurrent neural networks. cvpr", "year": 2017}