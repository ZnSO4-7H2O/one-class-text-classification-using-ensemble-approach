{"title": "Modeling Human Categorization of Natural Images Using Deep Feature  Representations", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Over the last few decades, psychologists have developed sophisticated formal models of human categorization using simple artificial stimuli. In this paper, we use modern machine learning methods to extend this work into the realm of naturalistic stimuli, enabling human categorization to be studied over the complex visual domain in which it evolved and developed. We show that representations derived from a convolutional neural network can be used to model behavior over a database of >300,000 human natural image classifications, and find that a group of models based on these representations perform well, near the reliability of human judgments. Interestingly, this group includes both exemplar and prototype models, contrasting with the dominance of exemplar models in previous work. We are able to improve the performance of the remaining models by preprocessing neural network representations to more closely capture human similarity judgments.", "text": "last decades psychologists developed sophisticated formal models human categorization using simple artiﬁcial stimuli. paper modern machine learning methods extend work realm naturalistic stimuli enabling human categorization studied complex visual domain evolved developed. show representations derived convolutional neural network used model behavior database human natural image classiﬁcations group models based representations perform well near reliability human judgments. interestingly group includes exemplar prototype models contrasting dominance exemplar models previous work. able improve performance remaining models preprocessing neural network representations closely capture human similarity judgments. problem categorization—how intelligent agent group stimuli discrete concepts—is intriguing valuable target psychological research extends many inﬂuential themes western classical thought clear interpretations multiple levels analysis likely fundamental understanding human minds advancing artiﬁcial ones previous categorization research development many successes—in particular high-precision statistical models human behavior. literature human categorization data often accounted respect either category summaries abstractions stored examples memory seemingly disparate models uniﬁed mathematically strategies density estimation interpretation enables interpolation them notably mixture density estimators fully extrapolating probabilistic re-framing categorization allows explain rational choice among estimators using bayesian nonparametric methods tying complexity strategy availability data work insightful theoretically productive know little relates complex visual world meant describe derives almost exclusively laboratory experiments using highly-controlled simpliﬁed perceptual stimuli solutions similarity judgments human categorization abilities contrast emerge contact natural world problems poses. category divisions result best understood context central challenge extend existing theory account behavior domains. recent work begun take challenge however fundamental problem remains ﬁnding appropriate psychological representations large numbers varied naturalistic stimuli. figure stimuli previous canonical studies categorization. shows representative stimulus bottom representations stimuli used inputs categorization models. modiﬁed mckinley nosofsky nosofsky j¨uttner rentschler palmeri nosofsky developments machine learning suggest means solve problem. tackling natural-image classiﬁcation engineering perspective computer scientists achieved human-level accuracy using deep neural networks loosely inspired structure human brain. networks learn representations used optimize classiﬁcation large sets natural images hence provide source representations complex naturalistic stimulus structure used input psychological models categorization. unclear whether models resemble human categorization feature learning representations learn nevertheless apparently relevant information humans judge stimulus similarity shown provide reasonable basis approximating human representations psychological experiments study show representations extracted convolutional neural network used input formal prototype exemplar models categorization paving towards wider nuanced exploration human behavior. moreover outside traditional laboratory setting using representations three layers canonical model human behavior massive dataset crowd-sourced naturalimage category judgments models based representations perform well close reliability human judgments. surprisingly although exemplar model performs best overall several variants prototype model nearly accurate ﬁnding contrasts might expected based previous research highlights importance representational space relative performance categorization models. models making rigid assumptions category structure based representations alone perform less well. however show layers neural network performance improved integrating salient information human behavior alternate pre-transforming representations closely approximate human ones. results demonstrate promising route modern machine learning methods developed tool extend traditional cognitive modeling categorization representative domains. begin brief review categorization models convolutional neural networks. mathematical details found methods section. categorization models seems intuitive categorize novel stimulus based similarity previouslylearned concepts categories. motivates comparison formal models within common framework categorization assignment novel stimulus category based measure similarity feature vectors existing category members specify model summary statistic represents properties categories necessary inputs similarity calculation different strategies. psychological literature canonical strategies developed regarding properties. prototype model category prototype—the average category members—is used comparison becomes central tendency members category exemplar model existing category members used. accordingly represents existing members exemplars category similarity calculation follow shepard exponentially-decreasing function relate distance stimulus feature space similarity. also take additive function vector becomes summation similarities element luce-shepard choice rule determine likelihood single categorization made categories bird convolutional neural networks deep cnns provide rich images enable state-of-the-art performance many core problems machine vision pass pixel-level input data series processing layers either apply convolutional ﬁlter activation nodes previous layer pool subset them. node activations layer form vector representation image deﬁnes sigmoid function around classiﬁcation boundary freely-estimated response-scaling parameter controls slope therefore degree determinism. becomes deterministic reduces random responding. formulated manner multivariate-gaussian classiﬁer exemplar model k-nearest-neighbors classiﬁer distance weighting. label participant gives stimulus takes value acting invert prototype difference distances appropriately. exemplar models differ similarity category calculated. prototype model variants prototype models similarity category taken exponential function negative squared mahalanobis distance stimulus vector category prototype mean—or prototype—and covariance matrix category deﬁne number prototype models using different strategies estimate parameters ground-truth image category resulting linear four quadratic prototype models increasingly abstract eventually input simple parameteric classiﬁer representation vectors layer directly input categorization models described above. beyond ﬂat-object classiﬁcation representations shown best predict brain activity visual cortices human similarity judgments natural images ﬁrst study several variants traditional prototype exemplar categorization models large dataset human categorization decisions natural images using stimulus representations multiple layers cnn. stimuli human decisions accompanying representations investigate based cifar dataset comprises color images categories natural objects. human judgments collected subset categories birds planes particular images chosen based uncertainty sampling method increasing sample value using intermediate models present stimuli least certain participants human behavioral data behavioral dataset consists human categorization decisions made stimulus set—to knowledge largest reported single study date. participants image asked whether bird plane. data originally collected part large project improve crowd-sourcing latency explored psychological context mean number judgments image deep representations extract feature representations stimuli three major layers simpliﬁed version popular alexnet pre-trained cifar dataset overall -class classiﬁcation accuracy using caffe network depicted figure two-dimensional principal component projections representations shown figure colored according human judgments corresponding images. network simple architecture allows easier exploration layers maintaining classiﬁcation accuracy ballpark much larger state-of-theart variants. figure architecture used train cifar image dataset supply three sets representation layer. pixel-valued inputs image series stacked non-linear transformations composed linear transformation non-linearities. ﬁnal softmax layer equivalent simple multivariate-gaussian classiﬁer. figure two-dimensional principal component projections three layers network colored average guess. covariance ellipses overlaid colored category. here deﬁnes -dimensional decision hyperplane parallel midpoint line linking prototypes offset bias term representing difference squared length means method corresponds dropping estimation prototypes category representations instead learning projection line connecting prototypes human consensus space representational space; equally thought learning linear transformation representational space based behavioral data. hyperplane resulting linear model taking mean category representations prototype test following models deﬁne different linear decision boundaries feature space optimization learned model parameters -fold-cross-validation early stopping using adam variant stochastic gradient descent batch size images. fold generated log-likelihood score held-out validation every batches. early-stopping point model trial index average validation log-likelihood score across folds minimized. model conducted grid search adam’s learning rate hyperparameter selecting ﬁnal model parameter based gave lowest cross-validated average-loglikelihood model’s early-stopping point. model comparison models present following three measures performance log-likelihood correlation human response proportions aikake information criterion baseline model output probabilities neural network image give normalized image. models including softmax baseline computed ﬁnal log-likelihood scores generating predictions images stimulus using averaged cross-validated parameters taken early-stopping point described above. ‘ideal’ model split-half reliability applying spearman-brown correction gives indication inter-participant consistency ceiling model performance. this generate random half-splits human judgments half-split contains half human guesses image. split compute correlation halves take mean correlations ﬁnal reliability estimate applying spearman-brown correction compare categorization models ideal model take random splits data compute correlation average half model predictions. average results predicting halves average values splits; values reported correlation. model name identity common variance vector common variance hyperplane hyperplane category pooled variance category variance category scalar variance category vector variance note number model parameters number feature dimensions. tions prototype test following models deﬁne different quadratic decision boundaries feature space category pooled variance empirically-estimated scalar-valued mean category variance terms. also known poole spherical variance; wk’s positive dimensional-scaling parameters called attentional weights must attentional weights serve modify importance dimension distance calculation build exemplar models based them. attention model eliminate calculation; attention model learn data baseline ceiling measures ceiling baseline measures shown table split-half reliability indicates large amount inter-subject variability image classiﬁcation—beyond perhaps would expected laboratory experiments understandable given complex nature small size images inherently-decreased precision crowdsourcing data using uncertainty sampling select stimuli also consider softmax output baseline model. image softmax function takes inner product matrix learned weights rasterized output ﬁnal pooling layer returns probability distribution cifar classes. weights learned based minimizing classiﬁcation loss whole cifar dataset comprises training images categories. thus softmax generous baseline includes many extra parameters learned much larger dataset related task ground-truth image categorization. consistent this achieves log-likelihood score high correlation. categorization models categorization model performance using untransformed representations shown figure numerical scores appendix. terms log-likelihood models consistently outperform baseline hyperplane models linear prototype class without bias term category vector variance model quadratic prototype class exemplar models without attentional general models parameters weights. meaning able alter representations using human behavioral data. although exemplar model attentional weights performs best overall striking simple decision bounds formed prototype models allow perform nearly well three layers. indeed reviewing correlation scores models performing close ceiling provided split-half reliability. prototype models fewer parameters however performed less well consistently baseline. models incorporate information human behavior alter shape decision boundary training instead estimating representations alone. models perform better using abstract lowerdimensional representations higher layers exception exemplar model without attentional weights. again difference largest prototype models fewer parameters. result cognitive implications explained machine learning terms. training learned feature representation allow disambiguate stimuli easily deepest layer meaning features associated different categories become increasingly well-separated depth. greater degree category overlap superﬁcial layers therefore penalizes models directly estimate categorical structure representations. addition increase feature dimensions generalization solutions found training likely worsen ratio stimuli dimensions therefore available information constrain solutions decreases around layer around layer order offer alternate evaluation models takes risk overﬁtting account also present model scores penalize highly-parameterized models. analysis affect model rank deepest layers indicate layer risk overparameterization outweighs beneﬁt log-likelihood models except exemplar without attentional weights compared softmax. design recent work demonstrates human similarity judgments used transform vector representations stimuli closely correspond human ones core strategy techniques learned transformation underlying space increase correlation human similarity scores stimuli measure vector similarity—for example inner product. reasons ﬁrst approach extracts ampliﬁes information stimulus repfigure model results using untransformed representations three layers convolutional neural network image classiﬁer reported likelihood score correlation human response proportions resentations relevant human behavior modeled resulting faithful interpretable conceptual structure. second previous categorization research low-dimensional solutions used representations complex stimuli using transformation techniques complements extends approach retaining information content higher-dimensional spaces doing directly improves quality conceptual structure lower-dimensional projections. second analyses ﬁrst transform representations closely approximate human similarity judgstimuli collected similarity judgments randomlyselected subset birds planes cifarbased categorization stimuli. behavioral data collected similarity judgments between unique pairs stimuli amazon mechanical turk giving total ratings different participants. participants instructed rate similarity four pairs bird plane images scale human behavior perform better models performing best. exemplar models picture complex. exemplar model without attentional weights performs well deepest layers scoring higher baseline. exemplar model attentional weights however performs best superﬁcial layer. comparing results figure interesting pattern emerges. transformation improves performance prototype models fewer parameters—which form decision boundaries based representations alone—in layers category scalar variance model could also included class incorporates coarse information human behavior; model pattern also holds. opposite effect holds top-performing highly-parameterized models including exemplar model attentional weights scores negatively impacted transformation layers. obvious explanation ﬁndings transformation eliminates dimensions important capturing similarity relations addition emphasizing are. effect regularizing basic models especially higher-dimensional feature spaces simultaneously penalizing highlyparameterized ones. longer exploit information eliminated dimensions nonetheless relevant categorization. evidence theory comes inspection transformation weight matrices conjunction solutions human similarity judgments inner-product spaces. example figure clear already formed inner-product space optimize categorization. transformation increases separation stimuli different categories eliminating majority dimensions. analysis goes beyond typical evaluations categorization models several ways. first large collection natural images stimuli enabling study human categorization domain representative environment evolved learned within theorized about. second able state-of-the-art methods computer vision estimate structure stimuli. contrasts previous work small number priori-identiﬁed features paid workers four comparisons. task eight example pairs shown help prevent bias early judgments. amazon workers could repeat task pairs many times wanted. result similarity matrix averaging judgments. transforming representations transform representations follow method introduced peterson uses l-regularized linear regression increase correlation vector inner products average human similarity judgment corresponding images. similarity matrix expressed matrix product feature matrix diagonal weight matrix context categorization models require weights reﬂect linear transformation squared distances; therefore constrained nonnegative. non-negative least squares algorithm scipy python module enforcing regularization augmenting space matrix orthogonal vectors whose length controlled ridge parameter optimal regularization parameter using grid search values -fold crossvalidation. retrain model parameters whole dataset yield ﬁnal diagonal coefﬁcient matrix transformation generate second representations images pre-multiplying representation matrix element-wise square root weight matrix equivalent calculation described above. results transforming representations using linear transformation described above able substantially increase correlation human similarity ratings especially deepest layers improving pair-wise correlation transformation also recovers global structure stimulus organization; figure categorization models results evaluating categorization models transformed representations shown figure numerical scores shown appendix. prototype models general pattern holds across layers models able augment computation representations information figure dendrogram similarity data untransformed transformed representations representative layer solutions similarity stimuli; dendrogram similarities. lines colored cluster cluster membership determined distance entities threshold. manipulated deﬁne differentiate categories consequence limited simple artiﬁcial stimuli. finally offset modelling uncertainty advances introduce using large crowdsourced behavioral datasets ﬁnely assess graded category membership stimuli improve utility representations them. taken together results show using representations derived cnns makes possible apply psychological models categorization complex naturalistic stimuli resultant models make competitive predictions human behavior. approach naturally complements extends related work seeking apply models natural images categories relying low-dimensional general ﬁnding categorization models incorporate representations predict human categorizations natural images well—in particular able augment representations information human categorizations free parameters. however still able information human behavior improve performance less-ﬂexible models transforming representational substrate closely reﬂect properties human counterpart. theoretically interesting indicates enough latent figure model results using transformed representations three layers convolutional neural network image classiﬁer reported likelihood score correlation human response proportions information ﬂexibility ground-truth-trained representations harness related task. further practically encouraging shows successfully draw preexisting representations behavioral datasets model inputs rather procuring individual experiments heavy computational cost. preprocessing machine learning representations manner ﬁeld infancy likely beneﬁts complex transformations taken account along classical considerations relative timing similarity-judgments respect main task working complex naturalistic stimuli reveals potentially nuanced view human categorization. broad consensus decades laboratory studies using simple artiﬁcial stimuli people could learn complex category boundaries kind could captured exemplar model extrapolating results might imagine human categorization thought terms learning complex category boundaries simple feature-based representations. results outline different perspective. representations formed complex within complex representational spaces simple category boundaries seem sufﬁcient capture human behavior. think cases inspire theorize categorization—of children learning categorize furry animals cats dogs say—it seems plausible story developed much realistic stimuli might reasonable alternative. despite attractiveness important limitations analysis. caveat comes source representations images used obtain results generated explicitly trained classify images categories including categories birds planes. trying form representation simple boundary sufﬁcient pick category another. sense representations expected favor prototype models. worthy concern don’t think signiﬁcantly detracts results. first illustrated figure people’s judgments often don’t agree ground truth network trained capturing human performance using representations non-trivial. second regard primary contribution results existence proof representations exist complex natural stimuli allow prototype models perform similarly exemplar models—illustrating complex representations simple boundaries provide reasonable alternative simple representations complex boundaries capturing people reason natural categories. simply don’t representations images lead better performance predicting human behavior. given this important direction future work obtaining representations state-of-the machine learning algorithms applied images including unsupervisedlearning models evaluate impact classiﬁcation training results. categorization traditionally regarded distinct feature learning. however ﬁndings suggest dual processes considered together. thinking humans feature representations likely learned early slow data-driven learning process. given considerations might expect psychological representations reﬂect natural world categorization natural stimuli made efﬁcient simple possible. hand artiﬁcial unlikely stimuli times carve awkward boundaries spaces perhaps underlies success exemplar models point. including feature-learning evaluation human categorization called developing deeper understanding processes interact important next step towards fully characterizing human categorization.", "year": 2017}