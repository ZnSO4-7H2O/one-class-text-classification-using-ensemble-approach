{"title": "$l_{2,p}$ Matrix Norm and Its Application in Feature Selection", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Recently, $l_{2,1}$ matrix norm has been widely applied to many areas such as computer vision, pattern recognition, biological study and etc. As an extension of $l_1$ vector norm, the mixed $l_{2,1}$ matrix norm is often used to find jointly sparse solutions. Moreover, an efficient iterative algorithm has been designed to solve $l_{2,1}$-norm involved minimizations. Actually, computational studies have showed that $l_p$-regularization ($0<p<1$) is sparser than $l_1$-regularization, but the extension to matrix norm has been seldom considered. This paper presents a definition of mixed $l_{2,p}$ $(p\\in (0, 1])$ matrix pseudo norm which is thought as both generalizations of $l_p$ vector norm to matrix and $l_{2,1}$-norm to nonconvex cases $(0<p<1)$. Fortunately, an efficient unified algorithm is proposed to solve the induced $l_{2,p}$-norm $(p\\in (0, 1])$ optimization problems. The convergence can also be uniformly demonstrated for all $p\\in (0, 1]$. Typical $p\\in (0,1]$ are applied to select features in computational biology and the experimental results show that some choices of $0<p<1$ do improve the sparse pattern of using $p=1$.", "text": "recently matrix norm widely applied many areas computer vision pattern recognition biological study etc. extension vector norm mixed matrix norm often used jointly sparse solutions. moreover efﬁcient iterative algorithm designed solve l-norm involved minimizations. actually computational studies showed lp-regularization sparser l-regularization extension matrix norm seldom considered. paper presents deﬁnition mixed matrix pseudo norm thought generalizations vector norm matrix l-norm nonconvex cases fortunately efﬁcient uniﬁed algorithm proposed solve induced lpnorm optimization problems. convergence also uniformly demonstrated typical applied select features computational biology experimental results show choices improve sparse pattern using many ﬁelds computer vision pattern recognition computational biology etc. mixed matrix norm received increasing attention joint sparsity pattern. multi-task feature learning authors proposed similar models l-norm regularization couple feature selection across tasks. approach solve problem proposed known convergence rate. reformulate nonsmooth l-norm regularized optimization smooth convex optimization problems apply nesterov’s method solve them. algorithm analytical computes solution globally converges solution linear time. recently proximal alternating direction method addressed solve l-norm regularized least square problem multi-task feature learning. l-norm involved minimization also successfully employed correlated attribute transfer multi-task graph-guided fusion nonnegative graph embedding moreover authors used spectral regression lnorm constraint evaluate features jointly. group lasso logistic group-lasso constructed l-norm regularization many applications. major challenge l-norm minimization efﬁciently solve nonsmooth optimization problem. authors propose directly iterative algorithm solve robust l-norm minimization loss function regularization. global convergence proved literature. algorithm widely used many applications efﬁcient behavior construction example algorithm modiﬁed unsupervised feature selection semi-supervised learning spatial group sparse coding imagelevel tagging multi-instance learning also employ similar technique. whole models algorithms mentioned constructed convex l-norm framework. actually extensive computational studies showed using lp-norm sparser solution using l-norm. naturally expect lp-norm based minimization better sparsity pattern l-norm. recently similar penalty sparse linear multiple kernel multi-task learning considered induced optimization problems separately solved different algorithms according convex non-convex cases. disadvantage brings computational difﬁculty freely vary paper deﬁne mixed matrix norm present uniﬁed algorithm solve involved lp-norm based minimizations best knowledge ﬁrst algorithm uniformly solve specially mixed convex nonconvex optimization problems. presentation several innovations follows. generalization l−norm regularization nonconvex case. lp-norm neither convex lipschitz continuous induced lp-norm based optimization problem nonconvex non-lipschitz continuous yet. since lp-norm based functions neither convex lipschitz continuous except efﬁciently solving mixed problem much challenging pure l-norm minimization. extend existing work uniﬁed algorithm solving lp-norm optimization problems. general algorithm reduced case uniﬁed algorithm ﬁnds local approximate solution nonconvex lp-norm minimization. fortunately convergence also uniformly proved typical tested lp-norm based objective functions. experiments bioinformatics study provide empirical evidence alternatives constructing sparsity patterns obviously outperforms employ notations usual. matrices written boldface uppercase letters vectors written boldface lowercase letters. example denotes real matrix i−th j−th column respectively. moreover vector norm neither convex lipschitz continuous matrix pseudo norm convex lipschitz continuous yet. properties challenge researchers uniformly solve mixed convex noncovex lp-norm based optimization problems. data distributions practical applications. traditional least square regression solves following optimization problem obtain unknown matrix rd×c theoretically mostly preferred desirable sparsity. practically chosen often computational sake. certain conditions r▽-regularization equivalent r△-regularization. chose intermediate sense problem reduced popular l-norm based minimization proposed non-convex hence algorithm directly applied. know scheme presented uniformly solve specially mixed problem. therefore necessary develop uniﬁed approach efﬁciently solve problem objective problem written thus algorithm generates monotonically decreasing iterations converge point problem since problem convex optimization. full-column rank convergence point {yk} local minimization remark extent algorithm offers alternative solve regularized problems number columns allaml leukemia gene microarray data originally obtained golub et.al. genes containing classes acute lymphocytic leukemia acute mylogenous leukemia data ﬁrstly performed preprocessing data sets standardized zero-mean nomalized standard deviation. demonstrate effect different matrix pseudo norms feature selection typical tested algorithm implement lp-norm based optimization problems. using features classiﬁers individually performed data sets −fold crosses. classiﬁcation errors reported tables experimental procedure indicates four lp-norm based minimizations select different features hence result distinct classiﬁcation performances. parameter matrix norm balances sparsity non-convexity optimization problem closer sparser representation near model almost convex. classiﬁcation error comparisons show non-convex matrix norms provide alternatives l-norm. especially empirically outperforms choosing better sparse pattern various situations. order validate efﬁcient performance uniﬁed algorithm solving nonconvex pseudo norm optimization problems well convex l-norm based minimization employ relative reduction objective function estimate convergence speed. actually convergence behaviors lp-norm case similar. display change respect iterative steps case features experiments four data sets uniformly expected accuracy within around steps. paper kind general matrix norms proposed usually used jointly sparse optimization problems. uniﬁed algorithm designed solve mixed lp-norm based sparse model convergence also uniformly ensured. experiment results gene express data sets validate uniﬁed performance proposed method. meanwhile approach provides choices variety jointly sparse structures.", "year": 2013}