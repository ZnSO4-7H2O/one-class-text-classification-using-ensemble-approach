{"title": "Inverse Reward Design", "tag": ["cs.AI", "cs.LG"], "abstract": "Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.", "text": "autonomous agents optimize reward function give them. don’t know hard design reward function actually captures want. designing reward might think speciﬁc training scenarios make sure reward lead right behavior scenarios. inevitably agents encounter scenarios optimizing reward lead undesired behavior. insight reward functions merely observations designer actually wants interpreted context designed. introduce inverse reward design problem inferring true objective based designed reward training mdp. introduce approximate methods solving problems solution plan risk-averse behavior test mdps. empirical results suggest approach help alleviate negative side effects misspeciﬁed reward functions mitigate reward hacking. robots becoming capable optimizing reward functions. along comes burden making sure specify reward functions correctly. unfortunately notoriously difﬁcult task. consider example figure alice engineer wants build robot we’ll call mobile navigation. wants reliably navigate target location expects primarily encounter grass lawns dirt pathways. trains perception system identify terrain types uses deﬁne reward function incentivizes moving towards target quickly avoiding grass possible. deployed world encounters novel terrain type; dramatic effect we’ll suppose lava. terrain prediction goes haywire out-of-distribution input generates meaningless classiﬁcation which turn produces arbitrary reward evaluation. result might drive demise. failure occurs reward function alice speciﬁed implicitly terrain predictors ends outputting arbitrary values lava different alice intended would actually penalize traversing lava. terminology amodei negative side effect misspeciﬁed reward failure mode reward design leaving important aspects leads poor behavior. examples date back king midas wished everything touched turn gold leaving didn’t mean food family. another failure mode reward hacking happens when e.g. vacuum cleaner ejects collected dust collect even racing boat game loops place collect points instead actually winning race short requiring reward designer anticipate penalize possible misbehavior advance alleviate impact reward misspeciﬁcation? figure illustration negative side effect. alice designs reward function robot navigates gold prefers dirt paths. consider robot might encounter lava real world leaves reward speciﬁcation. robot maximizing proxy reward function drives lava demise. work formalize inverse reward design problem problem inferring true reward function proxy. show help mitigate unintended consequences misspeciﬁed reward functions like negative side effects reward hacking. leverage insight designed reward function merely observation intended reward rather deﬁnition; interpreted context designed. first robot uncertainty reward function instead treating ﬁxed. enables e.g. risk-averse planning scenarios clear right answer help. uncertain true reward however half battle. effective robot must acquire right kind uncertainty i.e. know knows doesn’t. propose ‘correct’ shape uncertainty depends environment reward designed. alice’s case situations tested rob’s learning behavior contain lava. thus lava-avoiding reward would produced behavior alice’s designed reward function environments alice considered. robot knows settings evaluated also know that even though designer speciﬁed lava-agnostic reward might actually meant lava-avoiding reward. reward functions would produce similar behavior training environment treated equally likely regardless designer actually speciﬁed. formalize probabilistic model relates proxy reward true reward following assumption assumption proxy reward functions likely extent lead high true utility behavior training environment. formally assume observed proxy reward function approximate solution reward design problem extracting true reward inverse reward design problem. idea using human behavior observations reward function new. inverse reinforcement learning uses human demonstrations shared autonomy uses human operator control signals preference-based reward learning uses answers comparison queries even human wants observe that even human behavior actually write reward function still treated observation demanding observation model. paper makes three contributions. first deﬁne inverse reward design problem problem inferring true reward function given proxy reward function intended decision problem possible reward functions. second propose solution justify intuitive algorithm treats proxy reward expert demonstrations serve effective approximation. third show inference approach combined risk-averse planning leads algorithms robust misspeciﬁed rewards alleviating negative side effects well reward hacking. build system ‘knows-what-it-knows’ reward evaluations automatically detects avoids distributional shift situations high-dimensional features. approach substantially outperforms baseline literal reward interpretation. deﬁnition markov decision process tuple states. actions. probability distribution next state given previous state action. write reward function maps states rewards ﬁnite solution policy mapping current timestep state distribution actions. optimal policy maximizes expected rewards. represent trajectories. work consider reward functions linear combinations feature vectors thus reward trajectory given weights formalism deﬁnes optimal behavior given reward function. however provides information reward function comes refer without rewards world model. practice system designer needs select reward function encapsulates intended behavior. process reward engineering reward design deﬁnition reward design problem world model. agent model deﬁnes distribution designer believes agent represented policy true reward function robot understand reward evaluations high variance plan avoid states. refer inference problem inverse reward design problem deﬁnition inverse reward design problem deﬁned world model. tuple optimal reward design. singh formalize study problem designing optimal rewards. consider designer faced distribution environments class reward functions give agent ﬁtness function. observe that case bounded agents optimal select proxy reward distinct ﬁtness function. sorg subsequent work studied computational problem selecting optimal proxy reward. work consider alternative situation system designer bounded agent. case proxy reward function distinct ﬁtness function true utility function terminology system designers make mistakes. formalizes problem determining true utility function given observed proxy reward function. enables design agents robust misspeciﬁcations reward function. inverse reinforcement learning. inverse reinforcement learning agent observes demonstrations optimal behavior infers reward function optimized. similar problem approaches infer unobserved reward function. difference observation observes behavior directly observes reward function. assuming observed reward incentivizes behavior approximately optimal respect true reward. section show ideas used approximate ird. ultimately consider complementary strategies value alignment approaches allow designers users communicate preferences goals. pragmatics. pragmatic interpretation language interpretation phrase utterance context alternatives example utterance some apples often interpreted mean apples although literally implied. because context typically assume speaker meant apples would simply recent models pragmatic language interpretation levels bayesian reasoning lowest level literal listener interprets language according shared literal deﬁnition words utterances. then speaker selects words order convey particular meaning literal listener. model pragmatic inference consider probable meaning given utterance speaker. think model pragmatic reward interpretation speaker pragmatic interpretation language directly analogous reward designer ird. solve problems formalizing assumption idea proxy reward functions likely extent incentivize high utility behavior training mdp. give probabilistic model invert probability model compute distribution designer’s model probability robot select trajectory recall maximum entropy trajectory distribution ziebart i.e. designer models robot approximately optimal maximize expected true value i.e. controlling close optimal assume person formal statement assumption pulled expectation goal invert sample primary difﬁculty entails need know normalized probability preferring dirt grass generates gray trajectory. middle testing lava proxy penalize lava optimizing makes agent straight negative side effect agent avoids treats proxy observation context training makes realize cannot trust weight lava. right testing cells sensor indicators longer correlate look like grass sensor target other. proxy puts weight ﬁrst literal agent goes cells agent knows can’t trust distinction goes target sensors agree sample approximate normalizing constant. approach inspired methods approximate bayesian computation samples ﬁnite weights {wi} approximate integral equation found empirically helped include candidate sample sum. leads normalizing constant vector feature counts realized optimizing respectively. bayesian inverse reinforcement learning. inference normalizing constant serves calibration purpose computes good behavior produced proxy rewards would respect true reward. reward functions increase reward trajectories preferred inference. creates invariance linear shifts feature encoding. change shifting features vector posterior would remain same. achieve similar calibration maintain property directly integrating possible trajectories figure challenge domain latent rewards. terrain type induces different distribution high-dimensional features designer never builds indicator lava agent still needs avoid test mdps. shows equation invariant constant shifts feature function. argument applies equation choice normalizing constant approximates posterior problem posterior maximum entropy result intuitive interpretation. proxy determines average feature counts hypothetical dataset expert demonstrations determines effective size dataset. agent solves computes corresponding feature expectations agent pretends like demonstrations features counts runs irl. robot believes human good reward design demonstrations pretends gotten person. fact reducing proxy approximates surprising main point proxy reward behavior merely statement behavior good training environment. evaluated approaches model scenario figure call lavaland. system designer alice programming mobile robot rob. model gridworld movement four cardinal direction four terrain types target grass dirt lava. true objective encodes target quickly stay grass avoid lava. alice designs proxy performs well training contain lava. then measure rob’s performance test contain lava. results show combining risk-averse planning creates incentives avoid unforeseen scenarios. experiment four variations environment proof-of-concept conditions reward misspeciﬁed agent direct access feature indicators different categories challenge conditions right features latent; reward designer build indicator lava reasoning observation space using risk-averse planning agent still avoids lava. domains contain feature indicators four categories grass dirt target lava. side effects lavaland. alice expects encounter types terrain grass dirt target considers training figure provides encode trade-off path length time spent grass. training contains lava introduced deployed. agent treats proxy reward literally might lava test mdp. however agent runs know can’t trust weight lava indicator since weights would produce behavior training reward hacking lavaland. reward hacking refers generally reward functions gamed tricked. model within lavaland features correlated training domain uncorrelated testing environment. features three sensor three another sensor. training environment features sensors correct indicators state’s terrain category test time correlation gets broken lava looks like target category second sensor grass category ﬁrst sensor. akin racing game winning game points correlated reward design time test environments might contain loopholes maximizing points without winning. want agents hedge bets winning points lavaland sensors. agent treats proxy reward function literally might cells closer. contrast agent runs know reward function weights ﬁrst sensor likely proxy. risk averse planning makes target sensors agree previous examples allow explore reward hacking negative side effects isolated experiment unrealistic assume existence feature indicator unknown unplanned-for terrain. investigate misspeciﬁed objectives realistic setting shift terrain type latent inducing observations model terrain category determines mean variance multivariate gaussian distribution observed features. figure shows depiction scenario. designer mind proxy reward dirt target grass forgets lava might exist. consider realistic ways designer might actually specify proxy reward function based terrain types robot access directly observations collect samples training terrain types train reward predictor; classiﬁer features build classiﬁer classify terrain dirt grass target deﬁne proxy output. note domain allows negative side effects reward hacking. negative side effects occur feature distribution lava different feature distribution three safe categories proxy reward trained three safe categories. thus testing evaluation lava cells arbitrary maximizing proxy reward likely lead agent lava. reward hacking occurs features correlated safe categories uncorrelated lava category. lavaland parameters. deﬁned distribution layouts likelihood function prefers maps neighboring grid cells same. mixed likelihood quadratic cost deviating target ratio grid cells ensure similar levels lava feature testing mdps. training dirt grass. testing lava dirt grass. proof-of-concept experiments selected proxy reward function uniformly random. latent rewards picked proxy reward function evaluated target dirt grass. deﬁne proxy observations sampled examples grass dirt target linear regression. classiﬁer features simply used target rewards weights classiﬁed features. used dimensions feature vectors. selected trajectories risk-averse trajectory optimization. details planning method approach rationale selecting found supplementary material. dvs. measured fraction runs encountered lava cell test dependent measure. tells proportion trajectories robot gets ’tricked’ misspeciﬁed reward function; grid cell never seen conservative robot plan avoid manipulate factors literal-optimizer z-approx. literal-optimizer true robot interprets proxy reward literally false otherwise. z-approx varies approximation technique used compute posterior. varies across levels described section sample approximate normalizing constant normalizing constant maximum entropy results. figure compares approaches. left alleviates negative side effects reward hacking important inference method generalizes across different consequences misspeciﬁed rewards. figure shows example behaviors. figure results experiment comparing proposed method baseline directly plans proxy reward function. solving inverse reward design problem able create generic incentives avoid unseen novel states. realistic latent reward setting agent avoids lava cells despite designer forgetting penalize despite even indicator lava latent space reward functions would implicitly penalize lava likely actually speciﬁed risk-averse planning avoids also distinction observations classiﬁer features. ﬁrst essentially matches proof-of-concept results latter much difﬁcult across methods. proxy performs worse grid cell classiﬁed evaluated relatively good chance least lava cells misclassiﬁed target. performs worse behaviors considered inference plan already classiﬁed terrain non-linear transformation features. inference must determine good linear reward function match behavior discover corresponding uncertainty proxy linear function observations ﬁrst considerably easier. summary. work motivated introduced inverse reward design problem approach mitigate risk misspeciﬁed objectives. introduced observation model identiﬁed challenging inference problem entails gave several simple approximation schemes. finally showed solution inverse reward design problem avoid side effects reward hacking navigation problem. showed able avoid issues reliably simple problems features binary indicators terrain type. although result encouraging real problems won’t convenient access binary indicators matters. thus challenge evaluation domain gave robot access high-dimensional observation space. reward designer speciﬁed reward based observation space forgets penalize rare catastrophic terrain. inference still enabled robot understand rewards would implicitly penalize catastrophic terrain also likely. limitations future work. gives robot posterior distribution reward functions much work remains understanding best leverage posterior. risk-averse planning work sometimes limitation robot avoid things like lava also avoids potentially good things like giant gold. anticipate leveraging posterior follow-up queries reward designer addressing misspeciﬁed objectives. another limitation stems complexity environments reward functions considered here. approaches used work rely explicitly solving planning problem bottleneck inference. future work plan explore different agent models plan approximately leverage e.g. meta-learning scale complex environments. another limitation linear reward functions. cannot expect perform well unless prior places weights true reward function. e.g. encoded terrain types values lavaland unlikely reward function hypothesis space represents true reward well. finally work considers relatively simple error model designer. encodes implicit assumptions nature likelihood errors future work plan investigate sophisticated error models allow systematic biased errors designer perform human subject studies empirically evaluate models. overall excited implications short term also contribution general study value alignment problem.", "year": 2017}