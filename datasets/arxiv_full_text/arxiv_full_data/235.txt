{"title": "Rectified Factor Networks", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "We propose rectified factor networks (RFNs) to efficiently construct very sparse, non-linear, high-dimensional representations of the input. RFN models identify rare and small events in the input, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure. RFN learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means. We proof convergence and correctness of the RFN learning algorithm. On benchmarks, RFNs are compared to other unsupervised methods like autoencoders, RBMs, factor analysis, ICA, and PCA. In contrast to previous sparse coding methods, RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error. We test RFNs as pretraining technique for deep networks on different vision datasets, where RFNs were superior to RBMs and autoencoders. On gene expression data from two pharmaceutical drug discovery studies, RFNs detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods.", "text": "propose rectiﬁed factor networks efﬁciently construct sparse non-linear high-dimensional representations input. models identify rare small events input interference code units small reconstruction error explain data covariance structure. learning generalized alternating minimization algorithm derived posterior regularization method enforces non-negative normalized posterior means. proof convergence correctness learning algorithm. benchmarks rfns compared unsupervised methods like autoencoders rbms factor analysis pca. contrast previous sparse coding methods rfns yield sparser codes capture data’s covariance structure precisely signiﬁcantly smaller reconstruction error. test rfns pretraining technique deep networks different vision datasets rfns superior rbms autoencoders. gene expression data pharmaceutical drug discovery studies rfns detected small rare gene modules revealed highly relevant biological insights missed unsupervised methods. success deep learning large part based advanced efﬁcient input representations representations sparse hierarchical. sparse representations input general obtained rectiﬁed linear units dropout advantage sparse representations dependencies coding units easy model interpret. importantly distinct concepts much less likely interfere sparse representations. using sparse representations similarities samples often break co-occurrences features samples. bioinformatics sparse codes excelled biclustering gene expression data ﬁnding sharing patterns humans neanderthals representations learned relus sparse also non-negative. non-negative representations code degree absence events objects input. vast majority events supposed absent code degree absence would introduce high level random ﬂuctuations. also non-linear input representations stack models constructing hierarchical representations. finally representations supposed large number coding units allow coding rare small events input. rare events observed samples like seldom side effects drug design rare genotypes genetics small customer groups e-commerce. small events affect input components like pathways genes biology relevant mutations oncology pattern products e-commerce. summary goal construct input representations sparse non-negative non-linear many code units model structures input data current unsupervised deep learning approaches like autoencoders restricted boltzmann machines model speciﬁc structures data. hand generative models explain structures data codes cannot enforced sparse non-negative. input representation generative model posterior’s mean median mode depends data. therefore sparseness non-negativity cannot guaranteed independent data. example generative models rectiﬁed priors like rectiﬁed factor analysis zero posterior probability negative values therefore means positive sparse sparse priors guarantee sparse posteriors seen experiments factor analysis laplacian jeffrey’s prior factors address data dependence code employ posterior regularization method method separates model characteristics data dependent characteristics enforced constraints model’s posterior. representations feasible many code units massive datasets therefore computational complexity generating code essential approach. non-gaussian priors computation posterior mean input requires either numerically solve integral iteratively update variational parameters contrast gaussian priors posterior mean product input matrix independent input. still posterior regularization method leads quadratic constrained optimization problem e-step below). speed computation solve quadratic problem perform gradient step. allow stochastic gradients fast implementations also m-step gradient step. e-step m-step modiﬁcations posterior regularization method result generalized alternating minimization algorithm show algorithm used learning converges correct. correctness means codes non-negative sparse reconstruction error explain covariance structure data. goal construct representations input sparse non-negative non-linear many code units model structures input. structures input identiﬁed generative model model assumptions determine input structures explain model. want model covariance structure input therefore choose maximum likelihood factor analysis model. constraints input representation enforced posterior regularization method non-negative constraints lead sparse non-linear codes normalization constraints scale signal part hidden unit. normalizing constraints avoid generative models explain away rare small signals noise. explaining away becomes serious problem models many coding units since capacities utilized. normalizing ensures hidden units used cost coding also random spurious signals. spurious true signals must separated subsequent step either supervised techniques evaluating coding units additional data domain experts. generative model hidden units data deﬁned prior likelihood full model distribution expressed model’s posterior evidence representation input posterior’s mean median mode. posterior regularization method introduces variational distribution family approximates posterior choose constrain posterior means non-negative normalized. full model distribution contains model assumptions thereby deﬁnes structures data modeled. contains data dependent constraints posterior therefore code. data posterior regularization method maximizes objective kullback-leibler distance. maximizing achieves goals simultaneously extracting desired structures information data imposed generative model ensuring desired code properties factor analysis model extracts covariance structure data. prior hidden units noise visible units independent. model parameters weight matrix rm×l noise covariance matrix rm×m. assume diagonal explain correlations input components hidden units correlated noise. factor analysis model depicted fig. given mean-centered data posterior gaussian mean vector covariance matrix rectiﬁed factor network consists single stacked factor analysis model constraints posterior. incorporate posterior constraints factor analysis model posterior regularization method maximizes objective given like expectation-maximization algorithm posterior regularization method alternates e-step m-step. minimizing ﬁrst respect leads constrained optimization problem. gaussian distributions solution andσp quadratic problem component-wise. constraint non-convex quadratic optimization problem number hidden units complex solved iteration. therefore perform step gradient projection algorithm performs ﬁrst gradient step projects result feasible set. start step projected newton method gradient projection algorithm thereafter scaled gradient projection algorithm reduced matrix methods fail decrease objective generalized reduced method solves equality constraint variable inserts objective ensuring convex constraints. alternatively rosen’s gradient projection method improvement methods guarantee decrease e-step objective. since projection fast projected newton projected gradient update fast too. projected newton step requires steps deﬁned theorem projected gradient step requires steps scaled gradient projection step requires steps. complexity iteration contrast quadratic program solver typically requires variables steps minimum exemplify values benchmark datasets mnist cifar speedup projected newton projected gradient contrast quadratic solver gives speedup ratios mnist cifar. speedup ratios show efﬁcient e-step updates essential learning. furthermore computers restrictions limited quadratic program solvers problems m-step decreases expected reconstruction error complexity objective e-step nlm); projected newton projected gradient scaled gradient projection estep m-step overall complexity projected newton gradient allow stochastic gradients fast implementation dropout regularization. newton step derived supplementary gives details too. also e-step learning performs gradient step using projected newton gradient projection methods. projection methods require euclidean projection posterior means onto non-convex feasible projected newton method projected gradient method scaled gradient projection algorithm reduced matrix \u0001-active consists reduced matrix hessian \u0001-active columns rows ﬁxed unit vectors resulting algorithm posterior regularization method gradient based em-step leading generalized alternating minimization algorithm learning algorithm given alg. dropout regularization included e-step randomly setting code units zero predeﬁned dropout rate convergence correctness learning convergence learning. theorem states alg. converges maximum theorem rectiﬁed factor network learning algorithm given alg. generalized alternating minimization algorithm converges solution maximizes objective proof. present sketch proof given detail supplement. convergence show alg. algorithm convergences according proposition alg. ensures decrease m-step objective convex update leads minimum objective. convexity objective guarantees decrease m-step minimum. alg. ensures decrease e-step objective using gradient projection methods. requirements convergence also fulﬁlled. proposition based zangwill’s generalized convergence theorem thus updates algorithm viewed point-to-set mappings therefore numerical precision choice methods e-step implementations covered proof. correctness learning. goal algorithm explain data covariance structure. expected approximation error deﬁned line alg. theorem states algorithm correct explains data captures covariance structure good possible. theorem ﬁxed point alg. minimizes given ridge regression using trace norm matrices states left hand side quadratic trace norm positive semi-deﬁnite matrix trace bounds frobenius norm thus covariance approximated quadratic error according diagonal exactly modeled. since minimization expected reconstruction error based quality reconstruction depends correlation ensure maximal information i-projection posterior onto family rectiﬁed normalized gaussian distributions. rfns unsupervised methods. assess performance rectiﬁed factor networks unsupervised methods data representation. compare rectiﬁed factor networks rfnn rfns without normalization denoising autoencoders relus restricted boltzmann machines gaussian visible units fasp factor analysis jeffrey’s prior hidden units sparser laplace prior falap factor analysis laplace prior hidden units independent component analysis fastica sparse factor analysis laplace prior parameters standard factor analysis principal component analysis. number components ﬁxed method. generated nine different benchmark datasets dataset consists instances. instance samples features resulting matrix. matrices biclusters implanted bicluster pattern particular features found particular samples like pathway activated samples. optimal representation code biclusters present sample. datasets different noise levels different bicluster sizes. large biclusters samples features small biclusters samples features. pattern’s signal strength particular sample randomly chosen according gaussian finally matrix zero-mean gaussian background noise added standard deviation datasets characterized background noise number large biclusters number small biclusters evaluated methods according sparseness components input reconstruction error code covariance reconstruction error generative models. rfns sparseness percentage components exactly others methods percentage components absolute value smaller reconstruction error squared errors across samples. covariance reconstruction error frobenius norm difference model data covariance. supplement details data information hyperparameter selection different methods. tab. gives averaged results models coding units. results mean instances consisting instances dataset supplement separately tabulate results conﬁrm different noise levels. falap yield sparse codes since variational parameter push table comparison unsupervised methods upper part contains methods yielded sparse codes. criteria sparseness code reconstruction error difference data model covariance panels give results models coding units. results mean instances instances dataset rfns sparsest code lowest reconstruction error lowest covariance approximation error methods yielded sparse representations figure randomly selected ﬁlters trained image datasets using hidden units. rfns learned stroke local global blob detectors. rfns robust background noise absolute representations threshold variational approximation laplacian gaussian distribution rfns sparsest code lowest reconstruction error lowest covariance approximation error methods yielded sparse representations pretraining deep nets. assess performance rectiﬁed factor networks used pretraining deep networks. stacked rfns obtained ﬁrst training single layer passing resulting representation input training next rfn. deep network architectures pretrained ﬁrst layer stacks rfns giving hidden layer network. classiﬁcation performance deep networks pretrained layers compared support vector machines deep networks pretrained stacking denoising autoencoders stacking regular autoencoders restricted boltzmann machines stacking restricted boltzmann machines benchmark datasets results taken previous publications contain mnist basic bg-rand bg-img rect rect-img convex cifar- norb dataset size training validation test given second column tab. preprocessing performed median centering. model selection based validation performance rfns hyperparameters number units layer dropout rate learning rate ﬁxed default value supervised ﬁne-tuning stochastic gradient descent selected learning rate table results deep networks pretrained rfns models test error rate reported together conﬁdence interval. best performing method given bold well conﬁdence intervals overlap. ﬁrst column gives dataset second size training validation test last column indicates number hidden layers selected deep network. case pretraining signiﬁcantly worse best method still second best. nine experiments pretraining performed best four cases signiﬁcantly best. sdae figure examples small rare events identiﬁed drug design studies missed previous methods. panel ﬁrst gives coding unit rows display expression values genes controls active drugs inactive drugs drugs panel strongly downregulate expression tubulin genes hints genotoxic effect formation micronuclei micronuclei conﬁrmed microscopic analysis drugs panel show transcriptional effect genes negative feedback mapk signaling pathway therefore potential cancer drugs. masking noise number layers fine-tuning stopped based validation performance following test error rates together conﬁdence interval deep network pretraining rfns methods given tab. fig. shows learned ﬁlters. result best performing method given bold well result methods conﬁdence intervals overlap. rfns signiﬁcantly worse best method still second best. nine experiments rfns performed best four cases signiﬁcantly best. rfns drug discovery. using rfns analyzed gene expression datasets projects lead optimization phase pharmaceutical company ﬁrst project aimed ﬁnding novel antipsychotics target pdea. second project oncology study focused compounds inhibiting receptor. projects expression data summarized farms standardized. rfns trained hidden units masking noise learning rate identiﬁed transcriptional modules shown fig. panels illustrate rfns found rare small events input. panel drugs genotoxic downregulating expression small number tubulin genes genotoxic effect stems formation micronuclei since mitotic spindle apparatus impaired. also panel identiﬁed rare small event transcriptional module negative feedback mapk signaling pathway. rare events unexpectedly inactive drugs inhibit receptor. ﬁndings detected unsupervised methods highly relevant supported decision-making projects introduced rectiﬁed factor networks constructing sparse non-linear input representations many coding units generative framework. like factor analysis learning explains data variance model parameters. learning algorithm posterior regularization method enforces non-negative normalized posterior means. shown learning generalized alternating minimization method proved converge correct. rfns sparsest code lowest reconstruction error lowest covariance approximation error methods yielded sparse representations rfns shown improve performance used pretraining deep networks. pharmaceutical drug discovery studies rfns detected small rare gene modules missed unsupervised methods. gene modules highly relevant supported decision-making studies. rfns geared large datasets sparse coding many representational units therefore high potential unsupervised deep learning techniques. schmidhuber. deep learning neural networks overview. neural networks lecun bengio hinton. deep learning. nature nair hinton. rectiﬁed linear units improve restricted boltzmann machines. icml pages srivastava hinton krizhevsky sutskever salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research hochreiter bodenhofer fabia factor analysis bicluster acquisition. bioinformatics haug arora. applied optimal design. wiley sons york ben-tal nemirovski. interior point polynomial time methods linear programming conic quadratic programming semideﬁnite programming chapter pages society industrial applied mathematics lecun f.-j. huang bottou. learning methods generic object recognition invariance pose lighting. proceedings ieee conference computer vision pattern recognition ieee press rectiﬁed factor network. rectiﬁed factor network projection e-step improvement simple projection rectifying simple projection rectifying normalization scaled newton projection scaled projection reduced matrix weight decay dropout supplement contains additional information complementing main manuscript structured follows first rectiﬁed factor network learning algorithm em-step updates weight decay dropout regularization given section section proof learning algorithm generalized alternating minimization algorithm converges solution maximizes objective. correctness algorithm proofed section section describes maximum likelihood factor analysis model model selection em-algorithm. objective maximized described section next rfn’s algorithm gradient descent m-step e-step reported section following sections describe gradient-based me-step respectively. section describe rfns sparseness controlled gaussian prior. additional information selected hyperparameters benchmark methods given section sections describe data generation benchmark datasets report results three different experimental settings namely extracting factors hidden units. finally section describes experiments done assess performance ﬁrst layer pretraining cifar- cifar- three deep convolutional network architectures alexnet deeply supervised networks -convolution-network-innetwork algorithm rectiﬁed factor network learning algorithm. algorithm calls algorithm project posterior probability onto family rectiﬁed normalized variational distributions algorithm guarantees improvement e-step objective pi). projection algorithm relies different projections complicated projection tried simpler failed improve e-step objective. following newton-based gradient projection methods fail decrease e-step objective projection algorithm falls back gradient projection methods. first equality constraints solved inserted objective. thereafter constraints convex gradient projection methods applied. approach called generalized reduced gradient method preferred alternative method. method fails rosen’s gradient projection method used. finally method haug arora used. first consider newton-based projection methods used algorithm algorithm performs simple projection projected newton method learning rate one. projection fast ideally suited performed gpus rfns many coding units. algorithm fast simple projection without normalization even simpler algorithm algorithm generalizes algorithm introducing step sizes step size scales gradient step scales difference projection projection. annealing steps learning rate decay used appropriate update. newton-based update rules work algorithm used. algorithm performs scaled projection reduced hessian matrix instead full hessian computing \u0001-active determined consists reduced matrix hessian algorithm allows regularization parameters weight decay. priors parameters introduced. priors convex functions convergence algorithm still ensured. weight decay algorithm optionally used m-step algorithm coding units regularized dropout. however dropout covered convergence proof algorithm. dropout algorithm applied projection rectifying normalization. methods like mini-batches stochastic gradient methods covered convergence proof algorithm. however shown generalize convergence proof mini-batches shown incremental algorithm. dropout stochastic gradient methods show converge similar mini-batches. theorem rectiﬁed factor network learning algorithm given algorithm generalized alternating minimization algorithm converges solution maximizes objective proof. factor analysis algorithm given section algorithm factor analysis algorithm modiﬁed e-step m-step. e-step modiﬁed constraining variational distribution non-negative means normalizing means across samples. m-step modiﬁed newton direction gradient step. like factor analysis algorithm aims maximizing negative free energy denotes kullback-leibler divergence larger equal zero. e-step algorithm decreases constraints non-negative means normalization. constraint optimization problem algorithm performs gradient descent step newton direction decrease factor analysis minimizes modiﬁcation e-step m-step follows algorithm generalized alternating minimization algorithm according algorithm increases e-step increases m-step important requirements convergence algorithm according theorem increase objective e-step m-step. therefore ﬁrst show decreases showing requirements convergence theorem met. algorithm ensures decrease m-step objective. m-step objective convex according theorem theorem update leads minimum according theorem theorem convexity guarantees update decreases m-step objective except current already minimizers. algorithm ensures decrease e-step objective. e-step decrease algorithm performed algorithm according theorem scaled projection reduced matrix ensures decrease e-step objective rectifying constraints according theorem also gradient projection methods ensure decrease e-step objective rectifying constraints. rectifying constraints normalization feasible convex equality constraints. optimize problems generalized reduced gradient method solves equality constraint variable inserts objective. problem gives solution resulting convex constraints. scaled projection gradient projection methods applied. rectifying normalizing constraints also rosen’s haug arora’s gradient projection method ensures decrease e-step objective since applied non-convex problems. parameter compact ensured bounding family variational distributions compact ensured continuous continuous differentiable functions constraints bounds variational parameters determined bounds parameters data density models continuous parameters ensured gaussian models e-step unique maximizer ensured convex continuous continuous differentiable function minimized together compact feasible e-step increases objective maximizer ensured shown above m-step unique maximizer ensured minimizing convex continuous continuous differentiable function model parameter convex feasible maximum global maximum since proposition based zangwill’s generalized convergence theorem updates algorithm viewed point-to-set mappings therefore numerical precision choice methods e-step implementations covered proof. m-step unique maximizer required proof theorem theorem however obtain alternative proof exchanging variational distribution parameters exchanging e-step m-step. theorem analog theorem e-step m-step conditions exchanged derived zangwill’s generalized convergence theorem resulting model procedure local maximum objective given model family family variational distributions. solution minimizes kl-distance family full variational distributions full model family. full means observed hidden variables taken account variational distributions probability observations desired family deﬁned probability distributions assign probability observation. case family variational distributions desired family since distributions excluded constraints. therefore solution optimization guarantee stationary points likelihood means maximize likelihood minimize since minimization reconstruction error based quality reconstruction covariance explanation depends correlation larger correlation lower reconstruction error better explanation data covariance. ensure maximal information i-projection posterior onto family rectiﬁed normalized gaussian distributions. reconstruction error given mean values using trace norm matrices states left hand side quadratic trace norm positive semi-deﬁnite matrix trace bounds frobenius norm furthermore states left hand side equation zero diagonal entries. therfore follows therefore model corresponding ﬁxed point explains empirical matrix second moments noise part signal part like factor analysis data variance explained model parameters model includes observations noise factors factor loading matrix rm×l noise covariance matrix rm×m. typically assume diagonal matrix explain data covariance signal noise. data variance explained signal part noise part parameters model model assumption follows given noise random variable want derive likelihood data model likelihood model produced data. denote expectation data including prior distribution factors noise distribution. obtain ﬁrst moments variance observations gaussian distributed since distribution product gaussian densities divided normalizing constant. therefore marginal distribution denotes absolute value determinant matrix. maximize likelihood difﬁcult since closed form maximum exists. therefore typically expectation maximization algorithm used maximize likelihood. algorithm variational distribution required estimates factors given observations. consider single data vector posterior also gaussian mean covariance matrix denotes kullback-leibler divergence larger zero. objective maximized order maximize likelihood. estep maximizes respect variational distribution therefore e-step minimizes standard unconstrained e-step variational distribution equal posterior i.e. therefore divergence goal sparse non-negative representation input extracts structure input. sparse non-negative representation desired code events objects caused input. assume events objects caused input therefore sparseness. furthermore want code degree absence events objects. vast majority events objects supposed absent code degree absence would introduce high level random ﬂuctuations. extracting structures input therefore generative models explicitly model input structures. example factor analysis models covariance structure data. however generative model cannot enforce sparse non-negative representation input. input representation generative model posterior’s mean median mode. generative models rectiﬁed priors lead rectiﬁed posteriors. however posteriors sparse means yield sparse codes example rectiﬁed factor analysis rectiﬁes gaussian priors selects models using variational bayesian learning procedure yield posteriors sparse means generative model hidden units data deﬁned prior likelihood posterior supplies input representation model posterior’s mean median mode. however posterior depends data therefore sparseness non-negativity means cannot guaranteed independent data. problem coding input generative models data-dependency posterior means. therefore posterior regularization method posterior regularization framework separates model characteristics data dependent characteristics like likelihood posterior constraints. posterior regularization incorporates datadependent characteristics constraints model posteriors given observed data difﬁcult encode model parameters bayesian priors. generative model prior likelihood full model distribution written model posterior hidden variables evidence likelihood data produced model. model family parametrization determines structures extracted data. typically model parameters enter likelihood adjusted observed data. posterior regularization method family allowed posterior distributions introduced. deﬁned expectations constraint features. case posterior means non-negative. distributions called variational distributions full variational distribution distribution unknown distribution observations determined world data generation process. distribution approximated samples drawn world namely training samples. contains model assumptions like structures used model data contains data dependent characteristics including data dependent constraints posterior. goal achieve obtain desired structure extracted data desired code properties. however general achieve identity therefore want minimize distance distributions. kullback-leibler divergence measure distance distributions. therefore objective minimizing divergence extracts desired structure data increasing likelihood enforces desired code properties thus code derived desired properties extracts desired input data structures. approximate divergence approximating expectation empirical mean samples drawn last term neither depends model therefore neglect following often abbreviate write since hidden variable based observation similarly often write instead even often instead ﬁrst line negative objective posterior constraint method third line negative without term objective framework maximized. maximizing increases ﬁnds proper input representation small model likelihood thus data representation extracts structures data imposed generative model ensuring desired code properties variational framework variational distribution called negative free energy physical term used since variational methods introduced quantum physics richard feynman hidden variables considered ﬁctive causes explanations environmental ﬂuctuations obtain classical algorithm. algorithm maximizes lower bound log-likelihood seen ﬁrst line ensures e-step instead algorithm generalized alternating minimization algorithm allow gradient descent m-step e-step. representation input generative model vector mean values posterior likely hidden variables produced observed data. modify e-step enforce variational distributions lead sparse codes zero values components mean vector. sparse codes many components mean vector zero obtained enforcing non-negative means. rectiﬁcation analog rectiﬁed linear units neural networks enabled sparse codes neural networks. therefore variational distributions restricted stem family non-negative constraints means. impose constraints posterior known posterior constraint method posterior constraint method maximizes objective e-step m-step. posterior constraint method computationally infeasible approach since assume large number hidden units. models many hidden units maximization e-step would take much time. posterior constraint method support fast implementations gpus stochastic gradients want allow order mini-batches dropout regularization. therefore perform gradient descent step e-step m-step. unfortunately convergence proofs algorithm longer valid. however show algorithm generalized alternating minimization method. gunawardana byrne showed converges point-to-set mappings allow extended e-step m-steps without unique iterates. therefore theorem holds different implementations different hardware different precisions algorithm consideration. method converge ensure objective increases e-step m-step. constrained family variational distributions posterior full distribution derived model family. model family parametrized family. models support density models depend parameter density models continuous parameters. convergence requires furthermore e-step m-step must unique maximizers increase objective maximum point. learning rules e-step m-step closed maps continuous functions. objective e-step strict convex parameters variational distributions simultaneously quadratic mean vectors constraints imposed. objective m-step convex parameters objective quadratic loading matrix rectifying only guarantee unique global maximizers convex compact sets family desired distributions possible parameters. convex optimization problem global maximum. rectifying normalizing family desired distributions convex equality constraints introduced normalization. however guarantee local unique maximizers. support density models depend parameter density models continuous parameters e-step unique maximizer e-step increases objective maximizer m-step unique maximizer m-step increases objective maximizer. resulting model procedure local maximum objective given model family family variational distributions. solution minimizes kl-distance family full variational distributions full model family. full means observed hidden variables taken account variational distributions probability observations desired family deﬁned probability distributions assign probability observation. case family variational distributions desired family since distributions excluded constraints. therefore solution optimization guarantee stationary points likelihood means maximize likelihood minimize kl-distance variational distributions model. expected approximation error sample estimate therefore hessian positive deﬁnite values like small values order guarantee positive deﬁnite hessian precisely approximation minmization representation data vector model variational mean vector order obtain sparse codes want non-negative enforce non-negative mean values constraints optimize projected newton methods gradient projection methods. non-negative constraints correspond rectifying neural network ﬁeld. therefore construct sparse codes analogy rectiﬁed linear units used neural networks. constrain variational distributions family normal distributions non-negative mean components. consequently introduce non-negative rectifying constraints however generative models many coding units face problem. tend explain away small rare signals noise. many coding units model selection algorithms prefer models coding units variation therefore removed model. coding units hardly contribute explain observations. likelihood larger small rare signals explained noise likelihood coding units explain signals. coding units without variance kept default values maximal contribution likelihood. used coding deviate maximal values sample. accumulation deviations decrease likelihood increased explaining small rare signals. models problem become severe since models several tens thousands coding units. avoid explaining away problem enforce selected models coding units equal level. keeping variation noise-free coding unit across training one. consequently introduce normalization constraint coding unit derive methods increase objective e-step rectifying constraints rectifying normalization constraints. methods ensure reduce objective e-step guarantee convergence theory. resulting model procedure local maximum objective given model family family variational distributions. solution minimizes kl-distance family full variational distributions full model family. full means observed hidden variables taken account. mean vector rectifying normalizing constraints introduce non-negative constraints equality constraints. minimization respect gives constraint minimization problem minimize respect moment care constraints introduced non-negativity normalization. quadratic form enter terms enter. therefore separately minimize minimization respect require decrease objective perform gradient projection projected newton step. base algorithms euclidean least distance projections. projected onto convex sets projections increase distances. euclidean projection onto feasible denoted takes nearest point feasible set. rectifying constraints projection onto convex feasible given solution convex optimization problem following theorem shows update projection deﬁned optimization problem theorem solution optimization problem deﬁnes euclidean least distance projection requires lagrangian larger equal zero conditions follows therefore constraints primal problem require follows conditions follows conditions lead therefore solution problem also consider normalizing constraints minimize kl-divergences simultaneously. normalizing constraints connect single optimization problems sample e-step obtain minimization problem constraints deﬁne convex feasible set. solve equality constraints variable insert objective called generalized reduced gradient method solving reduced problem methods constraint optimization ensure convex feasible set. methods solve original problem require improvement objective feasible value. reduced problem perform step gradient projection method. gradient projection methods. also original problem gradient projection methods used. gradient projection method generalized rosen non-linear constraints later improved gradient projection algorithm rosen works non-convex feasible sets. idea linearize nonlinear constraints solve problem. subsequently restoration move brings solution back constraint boundaries. decrease objective perform gradient projection projected newton step step generalized reduced method. base algorithms euclidean least distance projections. projected onto convex sets projections increase distances. euclidean projection onto feasible denoted simultaneously takes nearest points {µi} feasible set. rectifying normalizing constraints projection onto non-convex feasible leads optimization problem following theorem shows updates form projection deﬁned optimization problem theorem least positive solution optimization problem deﬁnes euclidean least distance projection inserted expression since mixed second order derivatives zero hessian lagrangian diagonal negative entries. therefore strict negative deﬁnite. thus second order necessary conditions cannot fulﬁlled. minimum border point constraints. non-positive optimization problem deﬁnes plane normal vector positive orthant corresponding equality constraint deﬁnes hypersphere. minimization means plane containing solution parallel original plane close origin possible. move plane parallel origin positive orthant ﬁrst intersection hypersphere solution minimization problem. positive follows s=sjµsj positive otherwise negative terms left hand side. particular second order necessary conditions always fulﬁlled positive. follows conditions follows conditions therefore deﬁne projected gradient descent gradient projection algorithm performs ﬁrst gradient step projects result feasible set. projection onto feasible denoted takes nearest point feasible feasible must convex however later introduce gradient projection methods non-convex feasible sets. improved methods ﬁnding appropriate line search methods proposed search special version gradient projection method generalized reduced method method able solve optimization problem equality constraints. gradient projection method generalized rosen non-linear constraints gradient projection algorithm rosen also used region convex. idea linearize nonlinear constraints solve problem. subsequently restoration move brings solution back constraint boundaries. rosen’s gradient projection method improved methods guarantee increase objective e-step non-convex feasible sets except case already reached maximum. algorithms non-convex feasible sets give local maximum. also algorithm local maximum. search direction unconstrained problem rotated orthogonal direction decrease inactive directions constrained problem. escape possible problem \u0001-active introduced contains columns rows hessian index \u0001-active ﬁxed sorting indices \u0001-active together form block sub-identity matrix. hessian \u0001-active columns rows replaced unit vectors. following theorem lemma theorem states objective decreases using reduced hessian projected newton method convex feasible sets. denotes reduced matrix according projected newton method scaled gradient projection algorithm. convex feasible sets guarantee level already increase objective e-step. gradient projection algorithm. particular include generalized reduced method rosen’s gradient projection method. step guarantee increase objective e-step even non-convex feasible sets also complex methods constraint optimization. step ensures improvement using rectifying constraints according theory projected newton methods step ensures improvement using rectifying constraints normalizing constraints known methods constraint optimization. sufﬁcient increase objective e-step decreases divergence. however always avoid accumulation points outside solution set. likelihood gaussian since afﬁne transformation gaussian random variable gaussian random variable convolution gaussians gaussian too. thus gaussian gaussian. prior moments posterior derived gaussian conditioning likelihood prior gaussian distributed. conditional distribution random variables follow gaussian distribution gaussian performance rectiﬁed factor networks unsupervised methods data representation compared with rectiﬁed factor networks rfnn rfns without normalization denoising autoencoders rectiﬁed linear units restricted boltzmann machines gaussian visible units hidden binary units fasp factor analysis jeffrey’s prior hidden units sparser laplace prior falap factor analysis laplace prior hidden units independent component analysis fastica sparse factor analysis laplace prior parameters standard factor analysis principal component analysis. number components ﬁxed method. used hyperparameters listed tab. {learning rate=. iterations=} {learning rate=. iterations=} {corruption level=. learning rate=e- iterations=} {learning rate=. iterations=} {iterations=} {iterations=} {laplace weight decay factor=e- iterations=} generated nine different benchmark data sets data consists instances averaging results. instance consists samples features resulting data matrix. data matrices structures implanted biclusters bicluster pattern consisting particular number features found particular number samples. size bicluster given number features form pattern number samples pattern found. data sets different noise levels different bicluster sizes. considered large small bicluster sizes large biclusters samples features small biclusters samples features. signal strength pattern sample randomly chosen according gaussian finally data matrix background noise added noise distributed according zero-mean gaussian standard deviation data sets described tab. remaining components spanning outer product vectors drawn settoandtorespectively. resultsreportedherearethemeantogetherwiththestandarddeviationofinstances.themaximalvalueinthetableandthemaximalstandarddeviationwas errorandthedifferencebetweentheempiricalandthemodelcovariancematrix.thelowerrightcolumnblockgivestheaveragesperandco. analysisfactoranalysisandprincipalcomponentanalysisonninedatasets.criteriaaresparsenessofthecodingunitsreconstruction machinesfactoranalysiswithaverysparsepriorfactoranalysiswithalaplacepriorindependentcomponentanalysissparsefactor tablescomparisonforfactors/hiddenunitsextractedbyrfnrfnwithoutnormalizationdenoisingautoencoderrestrictedboltzmann settoandtorespectively. resultsreportedherearethemeantogetherwiththestandarddeviationofinstances.themaximalvalueinthetableandthemaximalstandarddeviationwas errorandthedifferencebetweentheempiricalandthemodelcovariancematrix.thelowerrightcolumnblockgivestheaveragesperandco. analysisfactoranalysisandprincipalcomponentanalysisonninedatasets.criteriaaresparsenessofthecodingunitsreconstruction machinesfactoranalysiswithaverysparsepriorfactoranalysiswithalaplacepriorindependentcomponentanalysissparsefactor tablescomparisonforfactors/hiddenunitsextractedbyrfnrfnwithoutnormalizationdenoisingautoencoderrestrictedboltzmann maximalvalueinthetableandthemaximalstandarddeviationwassettoandtorespectively. thelowerrightcolumnblockgivestheaveragesperandco.resultsreportedherearethemeantogetherwiththestandarddeviationofinstances.the errorandthedifferencebetweentheempiricalandthemodelcovariancematrix.thelowerrightcolumnblockgivestheaveragesperandco. analysisfactoranalysisandprincipalcomponentanalysisonninedatasets.criteriaaresparsenessofthecodingunitsreconstruction machinesfactoranalysiswithaverysparsepriorfactoranalysiswithalaplacepriorindependentcomponentanalysissparsefactor tablescomparisonforfactors/hiddenunitsextractedbyrfnrfnwithoutnormalizationdenoisingautoencoderrestrictedboltzmann settoandtorespectively. resultsreportedherearethemeantogetherwiththestandarddeviationofinstances.themaximalvalueinthetableandthemaximalstandarddeviationwas errorandthedifferencebetweentheempiricalandthemodelcovariancematrix.thelowerrightcolumnblockgivestheaveragesperandco. analysisfactoranalysisandprincipalcomponentanalysisonninedatasets.criteriaaresparsenessofthecodingunitsreconstruction machinesfactoranalysiswithaverysparsepriorfactoranalysiswithalaplacepriorindependentcomponentanalysissparsefactor tablescomparisonforfactors/hiddenunitsextractedbyrfnrfnwithoutnormalizationdenoisingautoencoderrestrictedboltzmann settoandtorespectively. resultsreportedherearethemeantogetherwiththestandarddeviationofinstances.themaximalvalueinthetableandthemaximalstandarddeviationwas errorandthedifferencebetweentheempiricalandthemodelcovariancematrix.thelowerrightcolumnblockgivestheaveragesperandco. analysisfactoranalysisandprincipalcomponentanalysisonninedatasets.criteriaaresparsenessofthecodingunitsreconstruction machinesfactoranalysiswithaverysparsepriorfactoranalysiswithalaplacepriorindependentcomponentanalysissparsefactor tablescomparisonforfactors/hiddenunitsextractedbyrfnrfnwithoutnormalizationdenoisingautoencoderrestrictedboltzmann deviationwassettoandtorespectively. erandco.resultsreportedherearethemeantogetherwiththestandarddeviationofinstances.themaximalvalueinthetableandthemaximalstandard reconstructionerrorandthedifferencebetweentheempiricalandthemodelcovariancematrix.thelowerrightcolumnblockgivestheaveragesp factoranalysisfactoranalysisandprincipalcomponentanalysisonninedatasets.criteriaaresparsenessofthefactorsreportedin% machinesfactoranalysiswithaverysparsepriorfactoranalysiswithalaplacepriorindependentcomponentanalysissparse tablescomparisonforfactors/hiddenunitsextractedbyrfnrfnwithoutnormalizationdenoisingautoencoderrestrictedboltzmann assess performance ﬁrst layer pretraining cifar- cifar- three deep convolutional network architectures alexnet deeply supervised networks -convolution-network-in-network cifar datasets contain rgb-color images divided train test sets split categories. datasets preprocessed global contrast normalization whitening additionally datasets augmented padding images four zero pixels borders. data augmentation beginning every epoch images training distorted random translation random ﬂipping horizontal vertical directions. alexnet neither preprocessed augmented datasets. inspired network network approach constructed -convolution-network-innetwork architecture convolutional layers followed max-pooling layer multilayer perceptron convolutional layer. relus used convolutional layers dropout regularization. weight initialization learning rates learning policies used strategy alexnet networks trained using mini-batches size c-nin alexnet respectively. pretraining randomly extracted patches training data construct ﬁlters c-nin alexnet. ﬁlters constitute ﬁrst convolutional layer network trained using default setting. assessing improvement rfns repeated training randomly initialized weights ﬁrst layer. results presented tab. comparison lower panel table reports performance currently performing networks network network maxout networks deepcnin cases pretraining rfns decreases test error rate. table upper panel shows results convolutional deep networks ﬁrst layer pretrained ﬁrst layer randomly initialized ﬁrst column gives network architecture namely alexnet deeply supervised networks -convolutionnetwork-in-network test error rates reported currently best performing networks network network maxout networks deepcnin reported lower panel. cases pretraining rfns decreased test error rate. schmidhuber. deep learning neural networks overview. neural networks lecun bengio hinton. deep learning. nature nair hinton. rectiﬁed linear units improve restricted boltzmann machines. icml pages srivastava hinton krizhevsky sutskever salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research hochreiter bodenhofer fabia factor analysis bicluster acquisition. bioinformatics haug arora. applied optimal design. wiley sons york ben-tal nemirovski. interior point polynomial time methods linear programming conic quadratic programming semideﬁnite programming chapter pages society industrial applied mathematics lecun f.-j. huang bottou. learning methods generic object recognition invariance pose lighting. proceedings ieee conference computer vision pattern recognition ieee press ciresan meier schmidhuber. multi-column deep neural networks image classiﬁcation. ieee conference computer vision pattern recognition cvpr long preprint arxiv.v krizhevsky sutskever hinton. imagenet classiﬁcation deep convolutional neural networks. pereira burges bottou weinberger editors advances neural information processing systems pages curran associates inc. graca ganchev taskar pereira. posterior parameter sparsity latent variable models. bengio schuurmans lafferty williams culotta editors advances neural information processing systems volume pages graca ganchev taskar. expectation maximization posterior constraints. j.c. platt koller singer s.t. roweis editors advances neural information processing systems volume pages feynman. statistical mechanics. benjamin reading friston. free energy principle biological systems. entropy convergence properties algorithm. annals statistics seraﬁni zanghirati zanni. gradient projection methods quadratic programs applications training support vector machines. optimization methods software dhillon. projected quasi-newton approach nonnegative least squares problem. technical report tr-- department computer sciences university texas austin", "year": 2015}