{"title": "Time Series Classification from Scratch with Deep Neural Networks: A  Strong Baseline", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics.", "text": "vector space model reduce time complexity improve performance ensembling models difference window size. ﬁnal classiﬁcation performed one-nearest-neighbor classiﬁer. ensemble based approaches combine different classiﬁers together achieve higher accuracy. different ensemble paradigms integrate various feature sets classiﬁers. elastic ensemble combines classiﬁers based elastic distance measures weighted ensemble scheme. shapelet ensemble produces classiﬁers shapelet transform conjunction heterogeneous ensemble. collective transform-based ensembles ensemble different classiﬁers based features extracted time frequency domains. approaches need heavy crafting data preprocessing feature engineering. recently effort spent exploit deep neural network especially convolutional neural networks end-to-end time series classiﬁcation. multi-channel proposed multivariate time series classiﬁcation. ﬁlters applied single channel features ﬂattened across channels input fully connected layer. authors applied sliding windows enhance data. evaluate approach multivariate time series datasets published benchmark comparison. author proposed multi-scale approach univariate time series classiﬁcation. sampling skip sampling sliding windows used preprocessing data manually prepare multiscale settings. although approach claims state-of-theart performance time series datasets heavy preprocessing efforts large hyperparameters make complicated deploy. proposed window slicing method data augmentation seems ad-hoc. provide standard baseline exploit deep neural networks end-to-end time series classiﬁcation without crafting feature engineering data preprocessing. deep multilayer perceptrons fully convolutional networks residual networks evaluated benchmark datasets benchmarks. pure end-to-end training time series data resnet achieve comparable better performance cote mcnn. global average pooling convolutional model enables exploitation abstract—we propose simple strong baseline time series classiﬁcation scratch deep neural networks. proposed baseline models pure end-to-end without heavy preprocessing data feature crafting. proposed fully convolutional network achieves premium performance state-of-the-art approaches exploration deep neural networks resnet structure also competitive. global average pooling convolutional model enables exploitation class activation contributing region data speciﬁc labels. models provides simple choice real world application good starting point future research. overall analysis provided discuss generalization capability models learned features network structures classiﬁcation semantics. time series data ubiquitous. human activities nature produces time series everyday everywhere like weather readings ﬁnancial recordings physiological signals industrial observations. simplest type time series data univariate time series provides reasonably good starting point study temporal signals. representation learning classiﬁcation research found many potential application ﬁelds like ﬁnance industry health care. however learning representations classifying time series still attracting much attention. earliest baseline distance-based methods work directly time series pre-deﬁned similarity measures euclidean distance dynamic time warping perform classiﬁcation. combination k-nearestneighbors classiﬁer known efﬁcient approach golden standard last decade. feature-based methods suppose extract features able represent global/local time series patterns. commonly features quantized form bag-ofwords given classiﬁers feature-based approaches mostly differ extracted features. name recent benchmarks bag-of-features framework extracts interval features different scales interval form instance time series forms bag. supervised codebook built random forest classifying time series. bag-of-sfa-symbols proposes distance based histograms symbolic fourier approximation words. extension bossvs method combines boss model plain baselines basic stacking three fullyconnected layers. fully-connected layers neurons following design rules using dropout layer’s input improve generalization capability non-linearity fulﬁlled rectiﬁed linear unit activation function prevent saturation gradient network deep. network ends softmax layer. basic layer block formalized shown compelling quality efﬁciency semantic segmentation images output pixel classiﬁer corresponding receptive ﬁeld networks thus trained pixel-to-pixel given category-wise semantic segmentation annotation. problem settings performed feature extractor. ﬁnal output still comes softmax layer. basic block convolutional layer followed batch normalization layer relu activation layer. convolution operation fulﬁlled three kernels sizes without striding. basic convolution block architecture mostly distinguished seminal decades utilization relu dropout. relu helps stack networks deeper dropout largely prevent co-adaption neurons help model generalizes well especially small datasets. however network deep neuron hibernate relu totally halve negative part. leaky relu might help three layers relu provide fundamental baselines. dropout rates convolution operator. build ﬁnal networks stacking three convolution blocks ﬁlter sizes block. unlike mcnn mc-cnn exclude pooling operation. strategy also adopted resnet prevent overﬁtting. batch normalization applied speed convergence speed help improve generalization. convolution blocks features global average pooling layer instead fully table shows results comprehensive comparison eight best benchmark methods. report test error rate best model trained minimum crossentropy loss number dataset achieved best performance. literature also report ranks ranking-based statistics evaluate performance make comparison also provide average rankings. however neither number best-performed dataset ranking based statistics unbiased measurement compare performance. number best-performed dataset focuses performance highly skewed. ranking based statistics highly sensitive model pools. better than comparative measurement also skewed input models might arbitrarily changed. evaluation measures wipe factor number classes. propose simple evaluation measure mean per-class error evaluate classiﬁcation performance speciﬁc models multiple datasets. given model {mi} dataset pool {dk} number class label {ck} corresponding error rate {ek} refers dataset denotes model. intuition behind mpce simple expected error rate single class across datasets. considering number classes mpce robust baseline criterion. paired t-test identiﬁes differences mpce signiﬁcant across different models. select seven existing best methods claim stateof-the-art results published within recent three years time series based bag-offeatures elastic ensemble bag-of-sfa-symbols vector space shapelet ensemble model ﬂat-cote multi-scale note cote ensemble model combines weighted votes different classiﬁers. bossvs ensemble multiple boss models different window length. nn-dtw also included simple standard baseline. training deploying complexity models small like nn-dtw resnet extends neural networks deep structures adding shortcut connection residual block enable gradient directly bottom layers. achieves state-of-the-art performance object detection vision related tasks explore resnet structure since really interested deep neural networks perform time series data. obviously resnet overﬁts training data much easier datasets comparatively small lack enough variants learn complex structures deep networks still good practice import much deeper model analyze pros cons. number ﬁlters ﬁnal resnet stacks three residual blocks followed global average pooling layer softmax layer. setting simply reuses structures certainly better structures problem given structures adequate provide qualiﬁed demonstration baseline test proposed neural networks subset time series repository includes distinct time series datasets compare benchmarks. dataset split training testing default. preprocessing experiment z-normalization training test split mean standard deviation training part dataset. trained adadelta learning rate resnet trained adam learning rate loss function tested model categorical cross entropy. choose best model achieves lowest training loss report performance test set. training setting tends give overﬁtted conﬁguration likely generalize poorly test proposed networks generalize quite well. unlike benchmarks experiment excludes hyperparameter tuning cross validation provide unbiased baseline. settings also largely reduce adiac beef chlorinecon cincecgtorso coffee cricketx crickety cricketz diatomsizer ecgfivedays faceall facefour facesucr words gunpoint haptics inlineskate italypower lightning lightning mallat medicalimages motestrain noninvthorax noninvthorax oliveoil osuleaf sonyaiborobot sonyaiborobotii starlightcurves swedishleaf symbols syntheticcontrol trace twoleadecg twopatterns uwavex uwavey uwavez wafer wordsynonyms yoga table provide four metrics fully evaluate different approaches. indicates best performance three metrics ﬁrst sight resnet also competitive mpce score rankings. authors proposed validate effectiveness models wilcoxon signed-rank test error rates. instead choose wilcoxon rank-sum test deal conditions among error rates correction p-values case quite different results reported except mpce score reported last row. best worse mpce score respectively. resnet ranks among models little worse cote. paired t-test mean score performed tell difference mpce signiﬁcant interestingly found difference mpce among cote mcnn boss resnet signiﬁcant. approaches clustered best group. analogously rest approaches grouped figure show examples cams output using approach. discriminative regions time series right classes highlighted. also highlight differences cams different labels. contributing regions different categories different. ’cbf’ dataset label determined mostly region sharp drop occurs. sequences label signature pattern sharp rise followed smoothly trending. label neural network address attention long plateau occurs around middle. similar analysis also applied contributing region ’starlightcurve’ dataset. however label label quite similar shapes contributing label focus less smooth trends drop label attract uniform attention signal much smoother. provides natural contributing region data speciﬁc labels. enables classiﬁcation-trained convolutional networks learn localize without extra effort. class activation maps also allow visualize predicted class scores given time series highlighting discriminative subsequences detected convolutional networks. also provide possible explanation convolutional networks work setting classiﬁcation. neural networks strong universal approximator known overﬁt easily large number parameters. experiments overﬁtting expected signiﬁcant since time series data small validation/test settings choose model lowest training loss test. however models generalize quite well given training accuracy almost dropout improves generalization capability large margin. family convolutional networks batch normalization known help improve training speed generalization. another important reason replace fully-connected layer global average pooling layer softmax layer greatly reduces amount parameters. thus starting basic network structures without data transformation ensemble three best group boss cote ensemble based models. mcnn exploit convolutional networks requires heavy preprocessing data transformation downsampling window slicing. proposed resnet able classify time series scratch achieves premium performance. compared resnet tends overﬁt data much easier still clustered ﬁrst group without signiﬁcant difference four best models. also note proposed three-layer achieves comparable results nn-dtw without signiﬁcant difference. recent advances relu dropout work quite well experiments help gain similar performance previous baseline. given time series represent activation ﬁlter last convolutional layer temporal location ﬁlter output following global average pooling indicate weight ﬁnal softmax function output ﬁlter class input ﬁnal softmax function hence directly indicates importance activation temporal location leading classiﬁcation sequence time series class output last convolutional layer input still identify contributing regions relevant particular category simply upsampling class activation length input time series. fig. class activation mapping technique allows classiﬁcation-trained classify time series localize class-speciﬁc regions single forward-pass. plots give examples contributing regions ground truth label data ’cbf’ ’starlightcurve’ dataset. number indicates likelihood corresponding label. another nuance results that deep neural networks work potentially quite well small dataset expand generalization recent advances network structures technical tricks. choose gasf provides intuitive interpret multi-scale correlation space. encodes cosine summation points striding step main diagonal special case contains original values. figure provides visual demonstration ﬁlters three tested models. weights second last layer similar clear structures little degradation occurring. weights ﬁrst layer generally higher values following layers. ﬁlters resnet similar. convolution extracts local features temporal axis essentially like weighted moving average enhances several receptive ﬁelds nonlinear transformations relu. sliding ﬁlters consider dependencies among different time intervals frequencies. ﬁlters learned deeper layers similar preceding layers. suggests local patterns across multiple convolutional layers seemingly homogeneous. visualization classiﬁcation performance indicates effectiveness convolution. exploration deep architecture interesting informative. resnet model layers still holds premium performance. factors impact performance resnet. shortcut connections gradients directly bottom layers resnet largely improve interpretability model learn highly complex patterns data. meanwhile much deeper models tend overﬁt much easier requiring effort regularizing model improve generalization ability. batch normalization global average pooling largely improved performance test data still tend overﬁt patterns dataset comparably complex catch. result test performance resnet good fcn. data larger complex encourage exploration resnet structure since likely good trade-off strong interpretability generalization. benchmark approaches time series classiﬁcation could categorized three groups distance based feature based neural neural network based. combination distance feature based approaches also commonly explored improve performance. curious classiﬁcation behavior different models semantics different models evaluated based scores. choose reduce dimension simple linear transformation able preserves large pairwise distances. figure distance three baseline models benchmarks comparatively large. indicates feature classiﬁcation criterion learned models good complement models. natural resnet quite close other. embedding isolated single category meaning classiﬁcation behavior quite different approaches. inspires synthesis feature learned convolutional networks deep-and-wide model might also improve performance. heavy preprocessing data feature crafting. achieves premium performance state-of-theart approaches. exploration much deeper neural networks resnet structure also gets competitive performance experiment settings. global average pooling convolutional model enables exploitation class activation contributing region data speciﬁc labels. simple found identical nn-dtw previous golden baseline. overall analysis provided discuss generalization models learned features network structures classiﬁcation semantics. rather ranking based criterion mpce proposed unbiased measurement evaluate performance multiple models multiple datasets. many research focus time series classiﬁcation recent effort lying deep learning approach related tasks. baseline simple protocol small complexity building deploying provides default choice real world application good starting point future research. bagnall lines hills bostrom time-series classiﬁcation collective transformation-based ensembles ieee cote transactions knowledge data engineering vol. h.-t. cheng harmsen shaked chandra aradhye anderson corrado chai ispir wide deep learning recommender systems proceedings workshop deep learning recommender systems. zhou khosla lapedriza oliva torralba learning deep features discriminative localization arxiv preprint arxiv.", "year": 2016}