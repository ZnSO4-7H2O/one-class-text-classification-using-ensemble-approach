{"title": "On the Computability of Solomonoff Induction and Knowledge-Seeking", "tag": ["cs.AI", "cs.LG"], "abstract": "Solomonoff induction is held as a gold standard for learning, but it is known to be incomputable. We quantify its incomputability by placing various flavors of Solomonoff's prior M in the arithmetical hierarchy. We also derive computability bounds for knowledge-seeking agents, and give a limit-computable weakly asymptotically optimal reinforcement learning agent.", "text": "abstract. solomonoﬀ induction held gold standard learning known incomputable. quantify incomputability placing various ﬂavors solomonoﬀ’s prior arithmetical hierarchy. also derive computability bounds knowledge-seeking agents give limit-computable weakly asymptotically optimal reinforcement learning agent. keywords solomonoﬀ induction exploration knowledge-seeking agents general reinforcement learning asymptotic optimality computability complexity arithmetical hierarchy universal turing machine aixi bayesexp. solomonoﬀ’s theory learning commonly called solomonoﬀ induction arguably solves induction problem data drawn computable measure solomonoﬀ induction converge correct belief hypothesis moreover convergence extremely fast sense expected number prediction errors compared number errors made informed predictor knows reinforcement learning agent repeatedly takes actions receives observations rewards. goal maximize cumulative reward. solomonoﬀ’s ideas extended reinforcement learning leading bayesian agent aixi however aixi’s trade-oﬀ exploration exploitation includes insuﬃcient exploration prior’s bias universal agent aixi achieve asymptotic optimality extra exploration resort orseau’s knowledge-seeking agents. instead rewards knowledge-seeking agents maximize entropy gain expected information gain agents explorers asymptotically learn environment perfectly reinforcement learning agent weakly asymptotically optimal value policy converges optimal value ces`aro mean weak asymptotic optimality stands currently known nontrivial objective notion optimality general reinforcement learners lattimore deﬁnes agent bayesexp grafting knowledge-seeking component aixi shows bayesexp weakly asymptotically optimal agent class stochastically computable environments purpose models solomonoﬀ induction aixi knowledgeseeking agents answer question solve learning theory. answers useless cannot approximated practice i.e. regular turing machine. therefore posit ideal model must least limit computable established aixi limit computable restricted ε-optimal policies placed various versions aixi ainu aimu arithmetical hierarchy. paper investigate computability solomonoﬀ induction knowledge-seeking. universal prior lower semicomputable hence conditional limit computable. semimeasure assigns positive probability observed string ﬁnite length. circumvented normalizing solomonoﬀ’s normalization mnorm preserves ratio limit computable. remove contribution programs compute ﬁnite strings semimeasure normalized norm multiplication constant. show norm limit computable. results computability solomonoﬀ induction stated table proved section section show ﬁnite horizons entropy-seeking information-seeking agent -computable limit-computable ε-optimal policies. weakly asymptotically optimal agent bayesexp relies optimal policies generally limit computable section give weakly asymptotically optimal agent based bayesexp limit computable. list notation found page even call formula right hand side n-formula. shown bounded quantiﬁers duplicate quantiﬁers type without changing classiﬁcation class co-recursively enumerable sets n-hard) n-hard many-one reducible i.e. computable function hierarchy subsets natural numbers known arithmetical hierarchy. ﬁnite strings alphabet inﬁnite strings alphabet union. empty given string denote length |x|. string length denote ﬁrst characters ﬁrst characters notation stresses inﬁnite string. write preﬁx i.e. y|x|. limit-computable sets highest level arithmetical hierarchy approached regular turing machine. limit-computable sets necessarily need form halting oracle. table deﬁnition lower/upper semicomputable limit-computable functions terms arithmetical hierarchy. similarly measure mixture measure since case normalization multiplication constant leading normalized measure mixture norm. using solomonoﬀ prior sequence prediction need compute conditional probability ﬁnite strings ﬁnite strings quotient well-deﬁned. -computable would imply norm -computable since norm measure contradiction. since conditional probability least complexity analogously mnorm norm since measures. norm cycles time step agent chooses action receives percept consisting observation real-valued reward cycle repeats history element denote interaction cycle denote history length policy function mapping history action taken seeing history. assume ﬁnite. environment stochastic assumed semicomputable. accordance aixi literature model environments lower semicomputable chronological conditional semimeasures class name conditional semimeasures specify conditional probabilities; environment joint probability distribution actions percepts. care computability environment purposes chronological conditional semimeasures behave like semimeasures. equivalently solomonoﬀ prior deﬁned mixture lower semicomputable semimeasures using lower semicomputable universal prior generalize representation chronological conditional semimeasures lower semicomputable universal prior given reference machine according universal prior gives rise universal mixture convex combination lscccss discuss variants knowledge-seeking agents entropy-seeking agents information-seeking agents entropy-seeking agent maximizes shannon entropy gain informationseeking agent maximizes expected bayesian information gain universal mixture quantities expressed value function. section ﬁnite lifetime knowledge-seeking agent maximizes entropy/information received including time step assume function computable. seeking value function substituted. deﬁnition optimal policy). optimal value function deﬁned supπ policy optimal histories policy ε-optimal histories entropy-seeking agent work well stochastic environments gets distracted noise environment rather trying distinguish environments moreover unnormalized knowledge-seeking agents fail seek knowledge deterministic semimeasures following example demonstrates. transitions labeled action/percept/probability. return percept deterministically nothing action distinguishes environments. prior mixture entropy-seeking value function hence action preferred entropyseeking agent. taking action yields percept hence nothing learned environment. solomonoﬀ’s prior extremely good learning prior bayesian agent learns value policy asymptotically however generally learn result counterfactual actions take. knowledge-seeking agents learn environment eﬀectively focus exploration. entropy-seeking agent information-seeking agent strongly asymptotically optimal class deterministic computable environments value policy converges optimal value sense discount functions admit weakly asymptotically optimal policies necessary condition eﬀective horizon grows sublinearly satisﬁed geometric discounting harmonic power discounting condition also suﬃcient lattimore deﬁnes weakly asymptotically optimal agent called bayesexp bayesexp alternates phases exploration phases exploitation optimal information-seeking value larger bayesexp starts exploration phase otherwise starts exploitation phase. exploration phase bayesexp follows optimal information-seeking policy steps. exploitation phase bayesexp follows ξ-optimal reward-seeking policy step -hard optimal knowledge-seeking policies proved therefore know bayesexp limit computable expect however approximate using ε-optimal policies preserving weak asymptotic optimality. proof. analogously theorem lower semicomputable hence optimal reward-seeking value function limit computable hence lemma limit-computable −t-optimal rewardseeking policy universal mixture theorem limit-computable \u0001t/-optimal information-seeking policies lifetime deﬁne policy analogously bayesexp instead optimal policies proceed analogously proof consists three parts. first shown value ξ-optimal reward-seeking policy converges optimal value exploitation time steps norm limit computable table page result implies approximate mnorm prediction measure mixture norm. cases normalized priors advantages. illustrated example unnormalized priors make entropy-seeking agent mistake entropy gained probability assigned ﬁnite strings knowledge. method tries tackle reinforcement learning problem balance exploration exploitation. aixi strikes balance bayesian way. however lead enough exploration agent cares present future—hence investment form exploration discouraged. counteract this knowledgeseeking component agent. section discussed variants knowledge-seeking agents entropy-seekers information-seekers showed ε-optimal knowledge-seeking agents limit computable optimal knowledge-seeking agents goal ﬁnding perfect reinforcement learning agent limit computable. weakly asymptotically optimal agents considered suitable candidate since currently known general reinforcement learning agents optimal objective sense discussed lattimore’s bayesexp relies solomonoﬀ induction learn environment knowledge-seeking component extra exploration. results culminated limit-computable weakly asymptotically optimal agent based lattimore’s bayesexp. sense goal achieved.", "year": 2015}