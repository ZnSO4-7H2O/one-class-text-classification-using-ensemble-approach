{"title": "Learning what to share between loosely related tasks", "tag": ["stat.ML", "cs.AI", "cs.CL", "cs.LG", "cs.NE"], "abstract": "Multi-task learning is motivated by the observation that humans bring to bear what they know about related problems when solving new ones. Similarly, deep neural networks can profit from related tasks by sharing parameters with other networks. However, humans do not consciously decide to transfer knowledge between tasks. In Natural Language Processing (NLP), it is hard to predict if sharing will lead to improvements, particularly if tasks are only loosely related. To overcome this, we introduce Sluice Networks, a general framework for multi-task learning where trainable parameters control the amount of sharing. Our framework generalizes previous proposals in enabling sharing of all combinations of subspaces, layers, and skip connections. We perform experiments on three task pairs, and across seven different domains, using data from OntoNotes 5.0, and achieve up to 15% average error reductions over common approaches to multi-task learning. We show that a) label entropy is predictive of gains in sluice networks, confirming findings for hard parameter sharing and b) while sluice networks easily fit noise, they are robust across domains in practice.", "text": "multi-task learning motivated observation humans bring bear know related problems solving ones. similarly deep neural networks proﬁt related tasks sharing parameters networks. however humans consciously decide transfer knowledge between tasks. natural language processing hard predict sharing lead improvements particularly tasks loosely related. overcome this introduce sluice networks general framework multi-task learning trainable parameters control amount sharing. framework generalizes previous proposals enabling sharing combinations subspaces layers skip connections. perform experiments three task pairs across seven different domains using data ontonotes achieve average error reductions common approaches multi-task learning. show label entropy predictive gains sluice networks conﬁrming ﬁndings hard parameter sharing sluice networks easily noise robust across domains practice. existing theory mainly provides guarantees multitask learning homogeneous tasks pure regression classiﬁcation tasks guarantees however hold heterogeneous tasks multi-task learning often applied share common input variables compensate lack theory case loosely related tasks researchers started explore multi-task learning experimental point view correlating performance gains task properties achieve better understanding models proﬁt auxiliary tasks works shed partial light effectiveness particular approaches multi-task learning remains hard predict parts networks beneﬁt sharing extent limited understanding multi-task learning also practical problem hundreds potential sharing structures exhaustive exploration search space multi-task learning speciﬁc problems infeasible. existing work uses sparsity share task predictors applied homogeneous tasks. previous work multi-task learning heterogeneous tasks considers couple architectures sharing contrast present framework uniﬁes different approaches introducing trainable parameters components differentiate multi-task learning approaches. build recent work trying learn split merged networks well work trying learn best combine private shared subspaces layer associating elements input sequence case english words vector representations word character embeddings. sequences vectors passed respective inner recurrent layers. layer divided subspaces e.g. allow network learn task-speciﬁc shared representations beneﬁcial. output inner layer network passed second layer well second layer network trafﬁc information mediated parameters second layer network receives weighted combination output inner layers. subspaces different weights. importantly weights trainable allow model learn whether share whether restrict sharing shared subspace etc. finally weighted combination outputs outer recurrent layers well weighted outputs inner layers mediated parameters reﬂect mixture representations various depths network. sluice networks capacity learn layers subspaces shared well layers network learned best representations input sequences. matrix regularization cast learning share matrix regularization problem following assume different tasks loosely related potentially non-overlapping datasets task associated deep neural network layers assume deep networks hyper-parameters outset. loosely related tasks task better modeled hidden layer; another share many parameters others mostly rely task-speciﬁc representations. architecture ﬂexible enough learn allocating appropriate subspaces mediating weights starting union priori task networks. figure sluice network main task auxiliary task consists shared input layer task-speciﬁc output layers three hidden layers task partitioned subspaces. parameters control subspaces shared main auxiliary task parameters control layer outputs used prediction. contributions architecture empirically justiﬁed deals dirtiness loosely related tasks. goes signiﬁcantly beyond previous work learning layers share parts layers share well using skip connections learn mixture model architecture’s outer layer. show generalization various multi-task learning algorithms hard parameter sharing supervision cross-stitch networks well transfer learning algorithms frustratingly easy domain adaptation moreover study task properties predict gains properties correlate learning certain types sharing well inductive bias resulting architecture. introduce novel architecture multi-task learning refer sluice network sketched figure case tasks. network learns share parameters deep recurrent neural networks recurrent networks could easily replaced multi-layered perceptrons convolutional neural networks applications. networks share embedding bination ﬁrst subspace task input layer task concatenation subspace outputs different weights correspond different matrix regularizers including several ones proposed previously multi-task learning. review section observe α-values obtain hard parameter sharing equivalent heavy l-regularizer. adding inductive bias naturally also inductive bias sluice networks partially constraining regularizer adding learned penalty. inspired work shared-space component analysis penalty enforce division labor discourage redundancy shared task-speciﬁc subspaces. networks theoretically learn separation explicit constraint empirically leads better results enables sluice networks take better advantage subspace-speciﬁc α-values. modeled orthogonality constraint layerwise subspaces model loss functions cross-entropy functions labels task note sluice networks restricted tasks loss functions could also applied jointly learn regression classiﬁcation tasks. weights determine importance different tasks training. experiments weight tasks. explicitly inductive bias model regularizer describe equation however model also implicitly learns regularization multi-task learning mediated weights weights used learn parameters mixture functions detailed following. learning matrix regularizers explain updating parameters lead different matrix regularizers. matrix consists rows number tasks. length number parameters. subvectors correspond parameters network layer layer consists subspaces parameters gmk. recall architecture partly motivated observation loosely related tasks certain features speciﬁc layers shared many layers subspaces remain task-speciﬁc want learn share inducing models different tasks. simplicity ignore subspaces ﬁrst assume tasks outputs hakt hbkt k-th layer time step task respectively interact misra refer crossstitch units omitting simplicity output layers learning mixtures many tasks implicit hierarchy informs interaction. rather predeﬁning enable model learn hierarchical relations associating different tasks different layers beneﬁcial learning. inspired advances residual learning employ skip-connections layer controlled using parameters. layer acts mixture model returning mixture expert predictions complexity model adds minimal number additional parameters compared single-task models architecture. experiments parameters task networks. such scale linearly number layers quadratically number tasks subspaces parameters scale linearly number tasks number layers. sluice network tasks layers task subspaces layer thus obtain additional parameters parameters. training sluice networks much slower training hard parameter sharing networks increase training time. equivalent mean-constrained -regularizer weighted losses smaller loss penalty always highest parameters shared. group lasso group lasso regularizer ||gig|| weighted norms groups often used enforce subspace sharing architecture learns group lasso subspaces αba-values outer layer α-values shared block communication networks. frustratingly easy domain adaptation approach domain adaptation relies shared private space task domain encoded sluice networks setting αabαba-weights associated setting αab-weights associated αba-weights associated αaa. note daumé discusses three subspaces. obtain space share half second subspaces across networks. supervision søgaard goldberg propose model inner layers deep recurrent works shared. obtained using heavy mean-constrained regularization ﬁrst layer e.g. auxiliary task ﬁrst layer parameter cross-stitch networks misra introduce cross-stitch networks values control layers convolutional neural networks. model corresponds setting αvalues associated identical letting β-value associated outer layer experiments include hard parameter sharing supervision cross-stitch networks baselines. report results group lasso frustratingly easy domain adaptation consistently inferior development data margin. data want experiment multiple loosely related tasks also study performance across domains make sure architecture prone overﬁtting. testbed experiments therefore choose ontonotes dataset high inter-annotator agreement also enables analyze generalization ability models across different tasks domains. ontonotes dataset provides data annotated array tasks across different languages domains. present experiments english portions datasets show statistics table tasks multi-task learning task usually considered main task tasks used auxiliary tasks improve performance main task. main tasks chunking named entity recognition simpliﬁed version semantic role labeling identify headwords pair part-of-speech tagging auxiliary task following example annotations task found table building block model. bilstm consists layers hidden dimension every time step model receives input concatenation -dimensional embedding word character-level embedding produced bi-lstm -dimensional character embeddings. word character embeddings randomly initialized. output layer dimensionality initialize parameters bias towards source subspace direction initialize parameters bias towards last layer. found effective apply orthogonality constraint weights associated lstm inputs. training evaluation train models stochastic gradient descent initial learning rate learning rate decay. training randomly uniformly sample data task. perform early stopping patience based main task hyperparameter optimization in-domain development data newswire domain. hyperparameters comparison models across domains. train models domain evaluate in-domain test well test sets domains evaluate out-of-domain generalization ability. table accuracy scores in-domain out-of-domain test sets chunking tagging auxiliary task different target domains baselines sluice networks. out-of-domain results target domain averages across remaining source domains. average error reduction single-task performance in-domain; out-of-domain. in-domain error reduction hard parameter sharing ﬁrst layer; iii) model based hard parameter sharing cross-stitch networks compare complete sluice network subspace constraints learned parameters. implement models dynet make code available httpanonymized.com. assumption particularly helps generalize data whose distribution differs seen training. thus ﬁrst investigate well sluice networks perform in-domain out-of-domain test data compared state-of-the-art multi-task learning models evaluating models chunking tagging auxiliary task. results show results in-domain out-ofdomain tests sets table average sluice networks signiﬁcantly outperform model architectures in-domain out-of-domain data. single task models hard parameter sharing achieve lowest results. single task learning comparatively useful indomain setting distribution test data training. out-of-domain data hard parameter sharing performs better demonstrating regularizing effect improves model’s generalization ability. models consistently outperformed supervision slightly better hard parameter sharing out-of-domain setting. crossstitch networks provide another signiﬁcant performance improvement. finally sluice networks perform best domains except telephone conversation domain outperformed cross-stitch networks. performance boost particularly signiﬁcant out-of-domain setting sluice networks point performance compared hard parameter sharing almost compared strongest baseline average demonstrating sluice networks particularly useful help model generalize better. summary shows proposed model learning parts multi-task models share small additional parameters learn achieve signiﬁcant consistent improvements strong baseline methods. table test accuracy scores different target domains source domain named entity recognition simpliﬁed semantic role labeling tagging auxiliary task baselines sluice networks. in-domain. out-of-domain. performance across tasks compare sluice nets across different combinations main auxiliary tasks. particular evaluate tagging auxiliary task simpliﬁed semantic role labeling tagging auxiliary task. show results table sluice networks outperform comparison models tasks in-domain test data successfully generalize out-of-domain test data average. yield best performance domains domains semantic role labeling. learning employs tasks typically pairing main task auxiliary task. existing studies also evaluate pair-wise interactions tasks. model aware learns multiple stand-alone tasks uses task-speciﬁc architecture model applied task combination. sluice network jointly learn four tasks newswire domain show results comparing baseline models table discussion four tasks low-level tagging simpliﬁed tasks ones beneﬁt hard parameter sharing. consistent results table previous work results highlight hard parameter sharing sufﬁcient effective multi-task learning semantic tasks. rather require task-speciﬁc layers used transform shared low-level representation form able capture ﬁne-grained task-speciﬁc knowledge. table ablation analysis. accuracy scores out-of-domain test sets chunking tagging auxiliary task different target domains different conﬁgurations sluice networks. scores target domain averaged across remaining source domains. sluice networks able outperform single task models tasks except chunking all-tasks setting. generally stand-alone tasks little explored best choices hyperparameters sampling ratio start stop training task whether freeze continue training already learned parameters remain discovered. task properties performance bingel søgaard correlate meta-characteristics task pairs gains hard parameter sharing across large task pairs. inspired study correlate various meta-characteristics error reductions values sluice networks well hard parameter sharing. importantly multi-task learning gains also sluice networks higher less training data sluice networks learn share variance training data generally values inner layers correlate highly meta-characteristics values outer layers. ablation analysis different types sharing important others. order analyze this perform ablation analysis table investigate impact parameters; parameters; iii) division subspaces orthogonality penalty. also evaluate whether concatenation outputs layer reasonable alternative mixture model. overall learnable parameters preferable constant parameters. learned parameters marginally outperform skip-connections hard parameter sharing setting skipconnections competitive learned values learned setting. addition modeling subspaces explicitly helps almost domains. knowledge ﬁrst time subspaces within individual lstm layers shown beneﬁcial. able effectively partition lstm weights opens research inducing structured neural network representations encode task-speciﬁc priors. finally concatenation layer outputs viable form share information across layers also demonstrated recent models densenet analysis values figure presents ﬁnal weights sluice networks chunking trained newswire training data. low-level simpliﬁed sharing inner layers line chunking also rely outer layer information shared complex target tasks vice versa. analysis values inspecting values all-tasks sluice table tasks place little emphasis ﬁrst layer prefer aggregate representations different later layers model semantic ability noise sluice networks learn disregard sharing completely expect good single-task networks random noise potentially even better. verify computing learning curve random relabelings sentences annotated syntactic chunking brackets well gold standard pos-annotated sentences. figure shows hard parameter sharing learning faster smoother loss surface multi-task learning good regularizer conﬁrming ﬁndings sluice network even better ﬁtting noise single-task models. believe ability noise practical settings informative rademacher complexities argue result suggests introducing additional inductive bias beneﬁcial. context deep neural networks multi-task learning often done hard soft parameter sharing hidden layers. hard parameter sharing introduced caruana there hidden layers shared tasks projected output layers speciﬁc different tasks. multi-task learning approach easy implement peng dredze apply variation hard parameter sharing multi-domain multi-task sequence tagging shared layer domainspeciﬁc projection layers. yang also hard parameter sharing jointly learn different sequence-tagging tasks across languages. also word character embeddings share character embeddings model. martínez alonso plank explore similar set-up sharing limited initial layer. three papers amount sharing networks ﬁxed advance. soft parameter sharing hand task separate parameters separate hidden layers architecture loss outer layer regularized current distance models. example loss regularized distance main auxiliary models. regularization schemes used multi-task learning include group lasso trace norm selective sharing kumar daumé maurer enable selective sharing allowing task predictors select sparse parameter bases homogeneous tasks. several authors discussed parts model share heterogeneous tasks. søgaard goldberg perform experiments hidden layers share context hard parameter sharing deep recurrent neural networks sequence tagging. show low-level tasks i.e. easy natural language processing tasks typically used preprocessing part-of-speech tagging named entity recognition supervised lower layers used auxiliary tasks. another line work looks separating learned space private shared space explicitly capture difference between task-speciﬁc cross-task features. enforce behavior constraints enforced prevent models duplicating information. bousmalis shared private encoders contrast work mentioned above limit predeﬁned sharing model learn parts network share using latent variables weights learned end-to-end fashion. work related also look learning share multi-task learning. however consider small class architectures learnable sluice networks. speciﬁcally restrict learning split architectures. architectures n-layer networks share innerk layers learn mechanism similar α-values. work seen generalization including in-depth analysis augmented works. introduced sluice networks framework learning share multi-task learning using trainable parameters. approach generalization recent work goes well beyond enabling network learn selective sharing layers subspaces skip connections. experiments task pairs ontonotes show average error reduction hard parameter sharing increase training time. provide analysis ability sluice networks noise well properties predictive gains sluice networks seeing effect size correlates highly label entropy conﬁrming previous ﬁndings hard parameter sharing", "year": 2017}