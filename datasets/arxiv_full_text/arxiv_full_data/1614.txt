{"title": "Unanimous Prediction for 100% Precision with Application to Learning  Semantic Mappings", "tag": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.6"], "abstract": "Can we train a system that, on any new input, either says \"don't know\" or makes a prediction that is guaranteed to be correct? We answer the question in the affirmative provided our model family is well-specified. Specifically, we introduce the unanimity principle: only predict when all models consistent with the training data predict the same output. We operationalize this principle for semantic parsing, the task of mapping utterances to logical forms. We develop a simple, efficient method that reasons over the infinite set of all consistent models by only checking two of the models. We prove that our method obtains 100% precision even with a modest amount of training data from a possibly adversarial distribution. Empirically, we demonstrate the effectiveness of our approach on the standard GeoQuery dataset.", "text": "train system that input either says don’t know makes prediction guaranteed correct? answer question afﬁrmative provided model family wellspeciﬁed. speciﬁcally introduce unanimity principle predict models consistent training data predict output. operationalize principle semantic parsing task mapping utterances logical forms. develop simple efﬁcient method reasons inﬁnite consistent models checking models. prove method obtains precision even modest amount training data possibly adversarial distribution. empirically demonstrate effectiveness approach standard geoquery dataset. user asks system many painkillers take? better system don’t know rather making costly incorrect prediction. system learned data uncertainty pervades must manage uncertainty properly achieve precision requirement. particularly challenging since training inputs might representative test inputs limited data covariate shift adversarial ﬁltering unforgiving setting still train system guaranteed either abstain make correct prediction? figure given training examples compute mappings consistent training examples. input mappings unanimously predict output return output; else return don’t know. building reliable question answering systems natural language interfaces. goal learn semantic mapping examples utterancelogical form pairs generally assume input source atoms output target atoms consider learning mappings decompose according multiset s∈xm {oh} {areaoh}). main challenge individual training example tell source atoms target atoms. system sure something seen small number possibly non-representative examples? approach based call unanimity principle model family contains true mapping inputs outputs. subset mappings consistent semantic parser requires modeling context dependence words logical form structure joining predicates. framework handles cases different choice source target atoms mappings training data. unanimously predict output test input return output; else return don’t know unanimity principle provides robustness particular input distribution tolerate even adversaries provided training outputs still mostly correct. operationalize unanimity principle need able efﬁciently reason predictions consistent mappings represent mapping matrix number times target atom shows occurrence source atom input. show unanimous prediction performed solving integer linear programs. linear programming relaxation show checking unanimity done efﬁciently without optimization rather checking predictions random mappings still guaranteeing precision probability relax linear program linear system gives geometric view unanimity predict input expressed linear combination training inputs. example suppose given training data consisting cities iowa cities ohio area iowa compute area ohio analogy reasoning patterns fall complex linear combinations. handle noisy data asking unanimity additional slack variables. also show linear algebraic formulation enables extensions learning denotations active learning paraphrasing validate methods section artiﬁcial data generated adversarial distribution noise show unanimous prediction obtains precision whereas point estimates fail. geoquery standard semantic parsing dataset model assumptions violated still obtain precision. able reach recall recovering predicates full logical forms. represent input source atoms output target atoms. simplest case source atoms words target atoms predicates—see figure example. assume true mapping source atom target atoms note also source atom target atoms multiple target atoms {parent parent}). extend source atoms multiset s∈xm∗. course know must trainestimate examples input-output pairs assume noise section shows deal noise. goal output mapping maps input either target atoms don’t know. precision whenever don’t know. chief difﬁculty source atoms target atoms unaligned. could infer alignment show unnecessary obtaining precision. finding element deﬁned corresponds solving integer linear program np-hard worst case though exist relatively effective off-the-shelf solvers gurobi. however solution enough. check whether input safe need check whether mappings predict output insight check whether solving ilps. recall want know output vector different different this pick random vector consider scalar projection ﬁrst maximizes scalar second minimizes ilps return value probability conclude mappings thus following proposition formalizes this proposition input. random vector. minm∈c maxm∈c probability proof. output exists def= words obtains precision. furthermore obtains best possible recall given model family subject precision since least possible outputs generated consistent mappings cannot safely guess them. solve learning problem laid previous section recast problem linear algebraic terms. number source atom types. first represent ns-dimensional vector counts; example vector figure goal points relative interior polytope deﬁned inequalities shown right. inequalities always active. therefore -dimensional polytope. solution results point chosen randomly ball radius r×nt nonzero. probability zero space orthogonal -dimensional space drawn nt-dimensional space. therefore probability xmv. without loss generality linear programming proposition requires solving non-trivial ilps input test time. natural step relax integer constraint solve instead. consistent mappings larger safe inputs smaller therefore predict still maintain precision although recall could lower. show exploit convexity avoid solving test time all. basic idea choose mappings randomly enough whether equivalent unanimity clp. could sample uniformly costly. instead show less random choice sufﬁces. formalized follows proposition ﬁnite test inputs. dimension clp. mapping sampled proper density d-dimensional ball lying centered vec. then probability implies flp. proof. prove contrapositive. clp. without loss generality assume agree i-th component note inner product vec. since convex projection onto must one-dimensional polytope. projection would -dimensional polytope orthogonal vec. since sampled proper density d-dimensional ball probability here lower bound slack inequality scales polytope positive exactly optimum solution. importantly constraint always active solutions optimal solution deﬁne submatrix containing rows consist remaining rows gives p∗/α∗ lies relative interior obtain deﬁne radius def= aj)−. columns proof. space write linear combination coefﬁcients unique output safe exists recall element decomposed orthogonal column basis null space means space intuitively proposition says stitching inputs together adding subtracting existing training examples gives exactly relaxed safe fls. note relaxations increases consistent mappings contravariant effect shrinking safe therefore using relaxation still preserves precision. deﬁnition null space. non-active implies ensures algorithm summarizes overall procedure training time solve single draw random vector obtain satisfying proposition test time simply apply scales linearly number source atoms input. linear system obtain additional intuition unanimity principle relax removing non-negativity constraint results linear system. deﬁne relaxed consistent mappings solutions linear system relaxed safe accordingly note afﬁne subspace expressed arbitrary solution basis null space arbitrary matrix. figure presents linear system four training examples. rare case full column rank left inverse exists exactly consistent mapping true require this. let’s explore linear algebraic structure intuitively know area problem. ohio maps area ohio maps conclude area maps area subtracting second example ﬁrst. following proposition formalizes generalizes intuition characterizing relaxed safe constraint assume adversary made nmistakes additions deletions target atoms across examples course know examples tainted. still guarantee precision? answer formulation simply replace exact match condition weaker nmistakes result still techniques section readily apply. note nmistakes increases candidate mappings grows means safe shrinks. unfortunately procedure degenerate linear programs. constraint tight also satisﬁes constraint matrix small enough norm. means consistent mappings full-dimensional certainly unanimous input. another strategy remove examples dataset could potentially noisy. training example i-th example. i-th example resulting safe remove procedure produces noiseless dataset apply noiseless linear program linear system previous sections. artiﬁcial data generated true mapping source atoms target atoms source atom maps target atoms. created training examples test examples length every input source atoms divided clusters input contains source atoms cluster. figure shows results comparison point estimation. recall unanimity principle reasons entire consistent mappings allows robust changes input distribution e.g. training attacks alternative consider computing point estimate minimizes point estimate minimizing average loss implicitly assumes i.i.d. examples. generate output input compute round coordinate closest integer. obtain precision-recall tradeoff threshold target atoms interval contains integer integer; otherwise report don’t know input compare unanimous prediction point estimation randomly generate subsampled datasets consisting fraction training examples. sweep across figure obtain curve. select distribution results maximum/minimum difference respectively. shown always precision obtain less precision full curve. adversary hurt recall unanimous prediction. noise. stated section algorithm ability guarantee precision even adversary modify outputs. increase number predicate additions/deletions figure shows precision remains recall naturally decreases response less conﬁdent figure algorithm always obtains precision different amounts training examples different relaxations existence noise adversarial input distributions. reconstructing logical form. deﬁne target atoms include information predicates enables reconstruct logical forms predicates. variable-free functional logical forms target atom predicate conjoined argument order table shows different choices target atoms. test time search possible compatible ways combining target atoms logical forms. exactly return logical form abstain otherwise. call predicate combination compatible appears training set. null word sentence collapsed traverse predicates. deal noise minimized real-valued mappings removed example non-zero residual. perform experiments using linear system relaxation. training takes seconds. figure shows precision recall function number training examples. obtain recall predicates test examples. unique compatible combining target atoms logical form results recall logical forms. though modeling assumptions incorrect real data still able precision training examples. interestingly linear system helps model geoquery dataset better linear program exists predicate alle geoquery every sentence unless utevaluate approach standard geoquery dataset contains utterances corresponding logical forms. utterances questions related geography what river runs states. handling context. words polysemous largest predicates river largest city word largest maps longest biggest respectively. therefore instead using words source atoms table different choices target atoms shows predicates shows predicates conjoined argument position. sufﬁcient simply recovering predicates whereas allows logical form reconstruction. construct integer linear program follows training example adds constraint output exactly candidate output. i-th example form matrix rki×nt candidate outputs. formally want πiti. entire active learning side beneﬁt linear system relaxation suggests active learning procedure. setting given inputs want choose inputs obtain output for. proposition states linear system formulation safe inputs exactly space therefore input already space affect all. algolot work semantic parsing tackles geoquery dataset state-of-the-art precision recall however none methods guarantee precision perform feature engineering numbers quite comparable. practice could unanimous prediction approach conjunction others example could classic semantic parser simply certify examples correct approach. critical applications could approach ﬁrst-pass ﬁlter fall back humans abstentions. learning denotations assumed input-output pairs. means annotating sentences logical forms expensive. motivated previous work learn question-answer pairs provides weaker supervision example area ohio also code chatﬁeld. true output could either area zipcode. section show handle form weak supervision asking unanimity additional selection variables. formally training examples consists candidate outputs case unknowns mapping along selection vector speciﬁes outputs equal xim. implement unanimity prinfigure shows recall choose examples linearly independent comparison choose examples randomly. active learning scheme requires half many labeled examples passive scheme reach recall. takes rank examples obtain recall labeled examples. course precision systems paraphrasing another side beneﬁt linear system relaxation easily partition safe subsets utterances paraphrases other. utterances paraphrase logical form e.g. texas’s capital capital texas. given sentence goal paraphrases fls. explained section represent input linear combination coefﬁcients want guaranteed output represent coefﬁcients outputs thus respectively. thus interested words null space basis null space write therefore paraphrases work motivated semantic parsing task last decade much work semantic parsing mostly focusing learning weaker supervision scaling beyond small databases applying semantic parsing tasks howidea computing consistent hypotheses appears classic theory version spaces binary classiﬁcation extended structured settings version space used context unanimity principle explore novel linear algebraic structure. safe inputs appears literature complement disagreement region notion active learning whereas support unanimous prediction. classic work learning classiﬁers abstain work however focuses classiﬁcation setting whereas considered structured output settings another difference operate adversarial setting leaning unanimity principle. another avenue providing user conﬁdence probabilistic calibration explored recently structured prediction however methods guarantee precision training test input. summary presented unanimity principle guaranteeing precision. task learning semantic mappings leveraged linear algebraic structure problem make unanimous prediction efﬁcient. view work ﬁrst step learning reliable semantic parsers. natural next step explore framework additional modeling improvements—especially dealing context structure noise.", "year": 2016}