{"title": "Hilbert Space Embeddings of POMDPs", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "A nonparametric approach for policy learning for POMDPs is proposed. The approach represents distributions over the states, observations, and actions as embeddings in feature spaces, which are reproducing kernel Hilbert spaces. Distributions over states given the observations are obtained by applying the kernel Bayes' rule to these distribution embeddings. Policies and value functions are defined on the feature space over states, which leads to a feature space expression for the Bellman equation. Value iteration may then be used to estimate the optimal value function and associated policy. Experimental results confirm that the correct policy is learned using the feature space representation.", "text": "nonparametric approach policy learning pomdps proposed. approach represents distributions states observations actions embeddings feature spaces reproducing kernel hilbert spaces. distributions states given observations obtained applying kernel bayes’ rule distribution embeddings. policies value functions deﬁned feature space states leads feature space expression bellman equation. value iteration used estimate optimal value function associated policy. experimental results conﬁrm correct policy learned using feature space representation. partially observable markov decision processes general models sequential control problems partially observable environments agent executes action uncertainty reward delivery state transitions vary depending state action. objective optimal policy maximizes value function deﬁned beliefs determined reward function. value expected discounted rewards optimal policy optimal value function computed solving bellman equation. solutions bellman equation generally diﬃcult obtain number approaches employed. include tractable parametric models sys); methods take advantage piece-wise linear convex property perseus hsvi last approaches fact value functions computed ﬁnite-step value iteration algorithms pwlc beliefs methods drawbacks however parametric models cause bias errors oversimplify monte carlo sampling methods computationally costly pwlc-based methods require observations actions discrete. nonparametric approach based series recent works probability distributions represented embeddings reproducing kernel hilbert spaces probability distributions embedded points rkhss expectations rkhs functions inner products emobtained beddings using kernel suﬃciently rich rkhs every probability distribution unique embedding distance embeddings metric distributions inference methods make embeddings including hidden markov modproposed belief propagation recently embeddings used performing value iteration optimal policy learning markov decision processes kernel bayes’ rule proposed updates prior distribution embedding feature space operations obtain posterior embedding. since computation scales pomdps training sample size state observation dimensionality. convergence distribution embedding rate independent dimension underlying space paper organized follows. introduce notations pomdps next section review recent kernel methods probability distributions section present kernel pomdps section bellman equations feature spaces empirical expression value iteration algorithms shown. experiments follow online planning algorithm. agent executes action setting true state known partial information observed according agent transitions next state receiving reward making observation agent executes next action equations determining policy based nonparametric model deﬁned appropriate rkhss. probability distributions required algorithm represented embeddings rkhss including beliefs states transition models observation models predictive distributions subsequent states given actions observations current beliefs. embeddings updated sequentially based actions observations. likewise value functions deﬁned feature representations states policies rkhs representations states actions leads expression bellman equation feature space. feature representation bellman equations deﬁne value iteration algorithm pomdps directly estimates expected immediate rewards expected values posterior beliefs feature space. earlier work cited above expectations represented inner products respective state observation feature spaces eﬃciently computed using kernel trick. original bellman operator contractive isotonic properties i.e. original value iteration guaranteed converge monotonically resulting kernel bellman operator not. said properties enforced following simple correction proposed approaches classical pomdp literature initializing enhancing eﬃciency applied including ways initial values state feature space pruning methods action edges. since kpomdps nonparametric embeddings distributions used algorithm learned training samples. note training must access samples hidden state although test phase observations necessary. setting reasonable cases hidden states relatively costly obtain might possible observe initially learning system dynamics would access value iteration phase learning optimal policy. kernel pomdps expectations appearing computed nonparametrically without explicitly estimating distributions transition observation models described section section matching probabilistic reinforcement learning results presented section nonparametric kernel-based counterparts following section. bayes’ rule becomes kernel bayes’ rule empirical counterpart leads updates bellman equations take form claims bellman operator becomes kernel bellman operator value initializations lead initializations present section provide overview reproduc. embeddings represented mean hence also referred mean embeddings. also operators kernel bayes’ rule mean embeddings updated using conditional embedding operators; particular allows obtain posterior embeddings given prior embeddings feature spaces. rkhs associated bounded measurable positive deﬁnite kernel domain ⟨··⟩hx corresponding given rkhs element ex∼p coincides unique element satisfying f⟩hx ex∼p means expectation function computed inner product embedding without rkhss associated respectively. random variable taking values tional density functions {p|x deﬁne family embeddings according mapping characterized conditional embedding operator since posterior distribution also written conditional distribution embedding posterior expressed conditional embedding operator prior distribution density random variable distribution corresponding density embedding posterior given expressed corresponding conditional embedding operator rkhss state action observation rkhss associated bounded measurable positive deﬁnite kernels respectively. ⟨··⟩hs ⟨··⟩ha ⟨··⟩ho denote respective inner sample version training samples according pomdp note included used state samples computed inner products rkhss figure search tree using kernel bellman equations. beliefs represented n-dimensional weight vectors samples. expected immediate reward given linear combination link discounted expected value next beliefs given linear combination samples associated observation links ˜on}. observation links expanded respect ﬁnite instead kernel bayes’ rule applied starting initial values kernel value iter; always give nonnegative since vectors bellman operator guaranteed isotonic contractive although empirically used value iteration algorithm. corrected isotonic contractive properties approximating weight vectors probability given samples embeddings ˆµo′|a;s ˆµa;o vectors respectively bellman equations represented terms weight vectors. therefore belief predictive distributions represented implemented online planning algorithm pomdps using kernel bellman equations example kpomdp dynamics shown figure agent computes initial belief initial observation corresponds belief estimate without prior. agent makes decision kernel value iteration ﬁnite horizon current belief weights agent updates tive weights obtaining reward prediction fails case small training samples reset current belief weights estimated initial belief weights. make online planning algorithms eﬃcient respectively. exact policy computed agent complete knowledge pomdp environment. histogram policy computed running classical value iteration algorithm transition observation models estimated histograms using samples algorithm. since histogram policy requires samples combinations states actions estimate transition models samples drawn uniform prior test kpomdp histograms exactly conditions. note need prior samples. used qmdp initial values pruned action links based qmdp values action link pruned qmdp value lower current estimated value. kbr-controller learned exact policy number training samples acmax{wi;} max{wi;} weight vectors cording corresponding kernel bellman operator using probability vectors guaranteed isotonic contractive. proof given supplementary material. complexity respect number samples action matrix ls|o;a computed training phase. update rule complexity observation intotal pair costs compared bayes’ whereas classical value iteration costs od). complexity comput;o′ reduced rank approximations gram matrices hallway problems respectively. averaged discounted rewards agent test experiments plotted number training samples. experiments experiment consists steps. limited experiments though problems. histogram methods sometimes showed diﬀerent results depending training data gave similar results average. also implemented simulator swing-up cartbalancing system. system consists cart mass running track freely swinging pendulum mass attached cart rod. state system angle angular velocity pendulum however agent observes angle. agent apply horizontal force action cart chosen system nonlinear. states continuous time discretized steps objective balance pendulum inverted position. training samples collected applying uniform random actions uniformly random states reward function variances uniform distributions figure shows visualization kpomdp dynamics inverted pendulum results rightmost ﬁgure plots averaged result earned rewards learned policies function training samples. episode length i.e. maximum total rewards result averaged experiments planning depth initial value function reward function kernel parameters meddist/ meddist/ meddist median inter-sample distance. kernel actions identity. compared kpomdp histogram policies environment discretized. introduced pomdps feature spaces beliefs states represented distribution embeddings feature spaces updated kernel bayes’ rule. bellman equations value functions policies expressed functions feature representation. proposed policy learning strategy value iteration kernel framework isotonic contraction properties kernel bellman operator enforced simple correction. value initialization action edge pruning implemented kernel pomdps following approach distributional pomdps qmdp. experiments conﬁrm controller learned feature space converges optimum policy. approach serves ﬁrst step towards powerful kernel-based algorithms pomdps. i.e. belief embedding weights identifying belief embedding ˆµs. true state system also marked black point plot. four colors indicate diﬀerent combinations signs states middle ﬁgure plots shows initial belief estimate given initial observation estimated algorithm since uncertain initial point positive weights spread direction axis. left right ﬁgures plots correspond updated weights belief embeddings depending executed actions observing observation angular velocity well estimated kpomdps. rightmost ﬁgure shows averaged result total rewards obtained learned policies training samples increased. description text. small regularization parameter avoid instability since empirical estimate include negative weights. estimate consistent smoothness assumptions converges inﬁnite sample limit. since always normalize weight vectors probability vectors subsection however simpler computationally eﬃcient estimate small regularization parameter. though consistency combination normalization theoretically proven experiments show good empirical results. follows give algorithm estimate used normalized weights.", "year": 2012}