{"title": "Improving the Improved Training of Wasserstein GANs: A Consistency Term  and Its Dual Effect", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Despite being impactful on a variety of problems and applications, the generative adversarial nets (GANs) are remarkably difficult to train. This issue is formally analyzed by \\cite{arjovsky2017towards}, who also propose an alternative direction to avoid the caveats in the minmax two-player training of GANs. The corresponding algorithm, called Wasserstein GAN (WGAN), hinges on the 1-Lipschitz continuity of the discriminator. In this paper, we propose a novel approach to enforcing the Lipschitz continuity in the training procedure of WGANs. Our approach seamlessly connects WGAN with one of the recent semi-supervised learning methods. As a result, it gives rise to not only better photo-realistic samples than the previous methods but also state-of-the-art semi-supervised learning results. In particular, our approach gives rise to the inception score of more than 5.0 with only 1,000 CIFAR-10 images and is the first that exceeds the accuracy of 90% on the CIFAR-10 dataset using only 4,000 labeled images, to the best of our knowledge.", "text": "xiang wei∗ boqing gong∗ zixia liqiang wang department computer science university central florida orlando school software engineering beijing jiaotong university beijing china tencent bellevue yqweixiangknights.ucf.edu boqinggooutlook.com zixiaknights.ucf.edu luweibjtu.edu.cn lwangcs.ucf.edu despite impactful variety problems applications generative adversarial nets remarkably difﬁcult train. issue formally analyzed arjovsky bottou also propose alternative direction avoid caveats minmax two-player training gans. corresponding algorithm called wasserstein hinges -lipschitz continuity discriminator. paper propose novel approach enforcing lipschitz continuity training procedure wgans. approach seamlessly connects wgan recent semi-supervised learning methods. result gives rise better photo-realistic samples previous methods also state-of-the-art semi-supervised learning results. particular approach gives rise inception score cifar- images ﬁrst exceeds accuracy cifar- dataset using labeled images best knowledge. witnessed great surge interests deep generative networks recent years central idea therein feed random vector neural network take output desired sample. sampling procedure efﬁcient without need markov chains. order train deep generative network broad categories methods proposed. ﬁrst stochastic variational inference optimize lower bound data likelihood. samples proxy minimize distribution divergence model real two-player game maximum mean discrepancy f-divergence recent wasserstein distance doubt generative adversarial networks among biggest impact thus variety problems applications gans learn generative network playing twoplayer game generator auxiliary discriminator network. generator difference deep generative models sense translates random vector desired sample impossible calculate sample likelihood instead discriminator serves evaluate quality generated samples checking difﬁcult differentiate real data points. however remarkably difﬁcult train gans without good heuristics generalize across different network architectures application domains. training dynamics often unstable generated samples could collapse limited modes. issues formally analyzed arjovsky bottou also propose alternative direction avoid caveats minmax two-player training gans. corresponding algorithm namely wasserstein shows superior performance gans also nice correlation sample quality value function gans lack. wgan aims learn generator network random vector wasserstein distance minimized resulting distribution generated samples real distribution underlying observed data points {x}; words ming wasserstein distance shown sensible cost function learning distributions supported low-dimensional manifolds popular distribution divergences distances example jensen-shannon divergence implicitly employed gans -lipschitz functions. analogous gans still call discriminator although actually real-valued function classiﬁer all. arjovsky specify family functions neural networks weight clipping enforce lipschitz continuity. however authors note networks’ capacities become limited weight clipping could gradient vanishing problems training. improved training wgan. gulrajani give concrete examples illustrate perils weight clipping propose alternative imposing lipschitz continuity. particular introduce gradient penalty term noting differentiable discriminator -lipschitz norm gradients everywhere potential caveats. unlike weight clipping however means penalize everywhere using term ﬁnite number training iterations. result gradient penalty term examined all. particular consider observed data points underlying manifold supports real distribution beginning training stage generated sample enforced generative model becomes close enough real can. light pros cons propose improve improved training wgan additionally laying lipschitz continuity condition manifold real data moreover instead focusing particular data point time devise regularization pair data points drawn near manifold following basic deﬁnition -lipschitz continuity. particular perturb real data point twice lipschitz constant bound difference discriminator’s responses perturbed data points figure illustrates main idea. gradient penalty gp|x often fails check continuity region near real data around discriminator function freely violate lipschitz continuity. alleviate issue explicitly checking continuity condition using perturbed version near observed real data point paper make following contributions. propose alternative mechanism enforcing lipschitz continuity family discriminators resorting basic deﬁnition lipschitz continuity. effectively improves gradient penalty method gives rise generators photo-realistic samples higher inception scores approach data efﬁcient terms less prone overﬁtting even small training sets. observe obvious overﬁtting phenomena even model trained images cifar- approach seamlessly integrated gans competitive semi-supervised training technique thanks inject noise real data points. results able report state-of-the-art results generative model inception score cifar- semi-supervised learning results cifar- using labeled images especially signiﬁcantly better existing gan-based semi-supervised learning results best knowledge. ﬁrstly review deﬁnition lipschitz continuity discuss regularize training wgan. arrive approach seamlessly integrated semi-supervised learning method bringing best worlds report better semi-supervised learning results existing ganbased methods. remarks. face snag i.e. impractical substitute possibilities pairs pairs regions input check arguably fairly safe limit scope manifold supports real data distribution surrounding regions mainly reasons. first keep gradient penalty term improve proposed consistency term overall approach. former enforces continuity points sampled real generated points latter complement former focusing region around real data manifold instead. second distribution generative model virtually desired close possible notation different reﬂect fact continuity checked sparsely ﬁnite data points practice. perturbing real data. ﬁrst version tried directly gaussian noise real data point resulting pair however noted arjovsky found samples generator become blurry gaussian noise used training. also tested dropout noise applied input found resulting mnist samples there. algorithm proposed ct-gan training generative neural net. experiments conducted default values iter require batch size. weights. learning rate. iter number iterations. iter training iterations success comes perturb hidden layers discriminator using dropout opposed input dropout rate small perturbed discriminator’s output considered output clean discriminator response virtual data point thus denote discriminator output applying dropout hidden layers. manner second virtual point around applying dropout hidden layers discriminator denote corresponding output. note that however becomes impossible compute distance virtual data points. work assume bounded constant absorb constant accordingly tune experiments take account unknown constant; best results obtained consistency report results paper. output discriminator given input apply dropout hidden layers discriminator. envision equivalent passing virtual data point clean discriminator. slightly improves performance controlling second-to-last layer discriminator i.e. d−)) above. consistent regularization ct|xx enforces lipschitz continuity data manifold surrounding regions effectively complementing improving gradient penalty gp|x algorithm shows complete algorithm learning wgan paper. hyperparameters borrow gulrajani experiments matter dataset. another hyper-parameter stated previously taking values gives rise results experiments. following modify output layer discriminator output neurons number classes interest neuron reserved contrasting generated samples real data using wasserstein distance wgan. -way softmax activation function last layer. ﬁrst three terms last consistency regularization calculated apply dropout discriminator. last term essentially leads temporal self-ensembling scheme beneﬁt semi-supervised learning. please insightful discussions mnist dataset provides handwritten digits total often left testing purpose. following train wgan fair comparison training examples semi-supervised learning experiments reveal labels setup data augmentation used. please appendix network architectures generator discriminator respectively. qualitative results. figure shows generated samples improved training wgan gradient penalty consistency regularization respectively generator iterations. clear approach gives rise realistic samples gp-wgan. contrasts samples foreground background general sharper gp-wgan. overﬁtting. approach less prone overﬁtting. demonstrate point show convergence curves discriminator’s value functions gp-wgan ct-gan figure curves evaluated training blue ones test set. results test become saturated pretty early gp-wgan consistently decrease costs training test sets. observation also holds cifar- dataset semi-supervised learning results. compare semi-supervised learning results several competitive methods table approach among best mnist dataset. cifar- cifar- contains natural images size test networks generative model small resnet former images train model whole training learn resnet. qualitative results. figure contrasts samples generated gp-wgan generator small cnn. figure shows results larger-scale resnet. results photo-realistic. additionally also draw histograms discriminator’s weights figure train using gp-wgan ct-gan respectively. interesting controls weights within smaller symmetric range gp-wgan partially explaining approach less prone overﬁtting. comparison inception scores. finally compare approach gp-wgan whole training unsupervised supervised generative-purpose task using resnet. model selection ﬁrst samples compute inception scores choose best model ﬁnally report test score another samples. experiment follows previous setup comparison results tables conclude proposed model achieves highest inception score cifar dataset best knowledge. generated samples shown figure small based generator inception scores gp-wgan ct-gan respectively. figure generated samples resnet model generated samples unsupervised model generated samples supervised model. column corresponds class cifar- dataset. semi-supervised learning. semi-supervised learning approach follow standard training/test split dataset labels training. regular data augmentation ﬂipping images horizontally randomly translating images within pixels used paper report semi-supervised learning results table mean standard errors obtained running experiments rounds. comparing several competitive methods able achieve state-of-the-art results. notably ct-gan outperfroms based methods large margin. please appendix network architectures appendix ablation study algorithm. paper present consistency term derived lipschitz inequality boosts performance gans model. proposed term demonstrated efﬁcient manner ease over-ﬁtting problem data amount limited. experiments show model obtains state-of-the-art accuracy inception score cifar- dataset semisupervised learning task learning generative models. acknowledgements. work partially supported iis- iis- n---. b.g. l.w. would also like thank adobe research nvidia amazon respectively gift donations. deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee emily denton soumith chintala fergus deep generative image models using laplacian pyramid adversarial networks. advances neural information processing systems vincent dumoulin ishmael belghazi poole alex lamb martin arjovsky olivier mastropietro aaron courville. adversarially learned inference. arxiv preprint arxiv. gintare karolina dziugaite daniel zoubin ghahramani. training generative neural networks maximum mean discrepancy optimization. arxiv preprint arxiv. goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems diederik kingma shakir mohamed danilo jimenez rezende welling. semi-supervised learning deep generative models. advances neural information processing systems salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems fisher seff yinda zhang shuran song thomas funkhouser jianxiong xiao. lsun construction large-scale image dataset using deep learning humans loop. arxiv preprint arxiv. table table detail network architectures used classiﬁcationpurpose ct-gan classiﬁers widely used ones semi-supervised networks except apply weight-norm rather batch-norm. generators follow network structures gp-wgan lower dimensional noise input generator cifar- order reproduce complicated images instead shift focus training classiﬁer. gaussian noise relu softplus batch norm gaussian noise relu softplus batch norm gaussian noise relu sigmoid weight norm gaussian noise relu gaussian noise relu gaussian noise softmax experiments generative models fair comparison results existing ones keep network structure hyper-parameters improved training wgan except three dropout layers hidden layers shown tables ablation study approach semi-supervised learning thanks dual effect proposed consistency term able connect temporal ensembling laine aila method ssl. superior results thus beneﬁt them veriﬁed ablation study detailed table remove approach almost reduces fact settings except extra regularization second-to-last layer. error still signiﬁcantly larger overall method. norm gradient. experiments although gp-wgan applied lipschitz constraint form gradient penalty input sampled real data point generated actual effect norm gradient good ct-gan model real data points illustrated figure empirically verify fact figure shows norms gradients discriminator respect real data points. closer norms better -lipschitz continuity preserved. figure demonstrates consistency regularization able improve gp-wgan deﬁnition lipschitz continuity. additionally check much -lipschitz continuity satisﬁed according vanilla deﬁnition figures plot respectively different iterations training using cifar- images. figure actual used train generative model. figure drawn follows. every iterations randomly pick real examples split subsets size. compute d)/d pairs ﬁrst subset second. maximum d)/d plotted figure ct-gan curve converges certain value much faster gp-wgan. -lipschitz continuity better maintained ct-gan gp-wgan whole course training procedure manifold real data. another experiment gp-wgan adding dropout layers used method networks gp-wgan denoted gp-wgan+dropout. help understand contribution approach preventing overﬁtting problem regularization merely dropout? experiment done removing term value function training wgan using cifar- images keep dropout layers. inception score gp-wgan+dropout generated samples shown figure also plot curve training procedure gp-wgan+dropout contrast curve ct-gan figure clear gp-wgan+dropout outperforms gp-wgan method outperforms gp-wgan large margin. finally plot convergence curves discriminators’ negative cost function learned gpwgan gp-wgan+dropout ct-gan figure dropout able reduce overﬁtting gp-wgan effective ct-gan. refer readers code github details setup. experiment imagenet conducted using images size generator iterations inception score proposed ct-gan whereas gp-wgan’s .±.. addition inception score comparison gp-wgan ct-gan generator iteration shown figure observe inception score proposed ct-gan becomes higher gp-wgan’s early generator iterations. finally figure shows samples generated gp-wgan ct-gan respectively.", "year": 2018}