{"title": "Dilated Recurrent Neural Networks", "tag": ["cs.AI", "cs.LG"], "abstract": "Learning with recurrent neural networks (RNNs) on long sequences is a notoriously difficult task. There are three major challenges: 1) complex dependencies, 2) vanishing and exploding gradients, and 3) efficient parallelization. In this paper, we introduce a simple yet effective RNN connection structure, the DilatedRNN, which simultaneously tackles all of these challenges. The proposed architecture is characterized by multi-resolution dilated recurrent skip connections and can be combined flexibly with diverse RNN cells. Moreover, the DilatedRNN reduces the number of parameters needed and enhances training efficiency significantly, while matching state-of-the-art performance (even with standard RNN cells) in tasks involving very long-term dependencies. To provide a theory-based quantification of the architecture's advantages, we introduce a memory capacity measure, the mean recurrent length, which is more suitable for RNNs with long skip connections than existing measures. We rigorously prove the advantages of the DilatedRNN over other recurrent neural architectures. The code for our method is publicly available at https://github.com/code-terminator/DilatedRNN", "text": "learning recurrent neural networks long sequences notoriously difﬁcult task. three major challenges complex dependencies vanishing exploding gradients efﬁcient parallelization. paper introduce simple effective connection structure dilatedrnn simultaneously tackles challenges. proposed architecture characterized multi-resolution dilated recurrent skip connections combined ﬂexibly diverse cells. moreover dilatedrnn reduces number parameters needed enhances training efﬁciency signiﬁcantly matching state-of-the-art performance tasks involving long-term dependencies. provide theory-based quantiﬁcation architecture’s advantages introduce memory capacity measure mean recurrent length suitable rnns long skip connections existing measures. rigorously prove advantages dilatedrnn recurrent neural architectures. code method publicly available. recurrent neural networks shown remarkable performance many sequential learning problems. however long sequence learning rnns remains challenging problem following reasons ﬁrst memorizing extremely long-term dependencies maintaining midshort-term memory difﬁcult; second training rnns using back-propagationthrough-time impeded vanishing exploding gradients; lastly forwardback-propagation performed sequential manner makes training time-consuming. many attempts made overcome difﬁculties using specialized neural structures cells optimization techniques. long short-term memory gated recurrent units powerfully model complex data dependencies. recent attempts focused multi-timescale designs including clockwork rnns phased lstm hierarchical multi-scale rnns etc. problem vanishing exploding gradients mitigated lstm memory gates; partial solutions include gradient clipping orthogonal unitary weight optimization skip connections across multiple timestamps efﬁcient sequential training wavenet abandoned structures proposing instead dilated causal convolutional neural network architecture provides signiﬁcant advantages working directly audio waveforms. however length dependencies captured dilated limited kernel size whereas rnn’s autoregressive modeling theory capture potentially inﬁnitely figure single-layer recurrent skip connections. single-layer dilated recurrent skip connections. computation structure equivalent second graph reduces sequence length four times. long dependencies small number parameters. recently proposed learningbased rnns ability jump seeing timestamps worth data; although authors showed modiﬁed lstm jumping provides six-fold speed increase efﬁciency gain mainly testing phase. paper introduce dilatedrnn neural connection architecture analogous dilated recurrent setting. approach provides simple useful solution tries alleviate challenges simultaneously. dilatedrnn multi-layer cell-independent architecture characterized multi-resolution dilated recurrent skip connections. main contributions work follows. introduce dilated recurrent skip connection building block proposed architecture. alleviate gradient problems extend range temporal dependencies like conventional recurrent skip connections dilated version require fewer parameters signiﬁcantly enhance computational efﬁciency. stack multiple dilated recurrent layers hierarchical dilations construct dilatedrnn learns temporal dependencies different scales different layers. present mean recurrent length neural memory capacity measure reveals performance difference previously developed recurrent skip-connections dilated version. also verify optimality exponentially increasing dilation distribution used proposed architecture. worth mentioning that recent proposed dilated lstm viewed special case model contains dilated recurrent layer ﬁxed dilation. main purpose model reduce temporal resolution time-sensitive tasks. thus dilated lstm general solution modeling multiple temporal resolutions. empirically validate dilatedrnn multiple settings variety sequential learning tasks including long-term memorization pixel-by-pixel classiﬁcation handwritten digits character-level language modeling speaker identiﬁcation audio waveforms. dilatedrnn improves signiﬁcantly performance regular lstm fewer parameters. many studies shown vanilla cells perform poorly learning tasks. however within proposed structure even vanilla cells outperform sophisticated designs match state-of-the-art. believe dilatedrnn provides simple generic approach learning long sequences. main ingredients dilatedrnn dilated recurrent skip connection exponentially increasing dilation; discussed following subsections respectively. referred skip length dilation layer input layer time denotes cell output operations e.g. vanilla cell lstm etc. skip connections allow information travel along fewer edges. difference dilated figure example three-layer dilatedrnn dilation example two-layer dilatedrnn dilation ﬁrst layer. case extra embedding connections required compensate missing data dependencies. regular skip connection dependency removed dilated skip connection. left middle graphs ﬁgure illustrate differences architectures dilation skip length removed middle graph. reduces number parameters. importantly computational efﬁciency parallel implementation greatly improved parallelizing operations that regular would impossible. middle right graphs ﬁgure illustrate idea example. input subsequences given four different colors. four cell chains computed parallel feeding four subsequences regular shown right ﬁgure output obtained interweaving among four output chains. degree parallelization increased times. extract complex data dependencies stack dilated recurrent layers construct dilatedrnn. similar settings introduced wavenet dilation increases exponentially across layers. denote dilation l-th layer. then left side ﬁgure depicts example dilatedrnn hand stacking multiple dilated recurrent layers increases model capacity. hand exponentially increasing dilation brings beneﬁts. first makes different layers focus different temporal resolutions. second reduces average length paths nodes different timestamps improves ability rnns extract long-term dependencies prevents vanishing exploding gradients. formal proof statement given section improve overall computational efﬁciency generalization standard dilatedrnn also proposed. dilation generalized dilatedrnn start formally called starting dilation. compensate missing dependencies shorter -by-m convolutional layer appended ﬁnal layer. right side ﬁgure illustrates example i.e. dilations start two. without edges would edges connecting nodes even time stamps. discussed section computational efﬁciency increased breaking input sequence downsampled subsequences feeding l-layer regular dilatedrnn shared weights. facilitate theoretical analysis apply cyclic graph notation introduced deﬁnition cyclic graph representation structure directed multi-graph edge labeled origin node destination node number time steps edge travels. node labeled time index node modulo period graph node index. must contain least directed cycle. along edges directed cycle summation must zero. deﬁne length shortest path input node time output node time measure memory capacity proposed essentially looks period graph. reasonable period small. however period large entire distribution makes difference span concrete example suppose architecture period implemented using equation skip length network rapidly memorizes dependence inputs time outputs time shorter dependencies much harder learn. motivated this proposed following additional measure memory capacity. deﬁnition cycle mean recurrent length mean recurrent length studies average dilation across different time spans within cycle. architecture good memory capacity generally small recurrent length time spans. otherwise network selectively memorize information time spans. also take maximum punish networks good length starting times well memorize information originating speciﬁc times. proposed mean recurrent length interesting reciprocal relation short-term memory measure proposed mean recurrent length emphasizes long-term memory capacity suitable intended applications. this ready illustrate memory advantage dilatedrnn consider architectures. proposed dilatedrnn structure layers regular d-layer skip connections skip connections skip connections strict superset dilatedrnn accomplishes exactly dilatedrnn twice number trainable parameters suppose give every layer largest possible skip graph period every layer regular skip setting. apparent advantage turns disadvantage time spans suffer increased path lengths therefore grows logarithmically much smaller regular skip rnn. implies information past average travels along much fewer edges thus undergoes less attenuation. derivation given appendix supplementary materials. advantage dilatedrnn lies memory capacity also number parameters achieves memory capacity. quantify analysis following measure introduced. deﬁnition denote card{·} cardinality. represented number recurrent edges node deﬁned ideally would want network large recurrent skips maintaining small number recurrent weights. easy show dilatedrnn rnns regular skip connections dilatedrnn half recurrent complexity regular skip removal direct recurrent edge. following theorem states dilatedrnn able achieve best memory capacity among class connection structures thus among parameter efﬁcient architectures. since dilatedrnn motivated dilated useful compare memory capacities. although cyclic graph mean recurrent length number recurrent edges node designed recurrent structures happen applicable dilated well. what’s more easily shown that compared dilatedrnn number layers dilation rate layer dilated exactly number recurrent edges node slightly smaller mean recurrent length. hence architectures model complexity looks like dilated slightly better memory capacity. however mean recurrent length measures memory capacity within cycle. going beyond cycle already shown recurrent length grows linearly number cycles structures including dilatedrnn whereas dilated receptive ﬁeld size always ﬁnite example dilation rate layers dilated receptive ﬁeld size cycles. hand memory dilatedrnn beyond cycles particularly sophisticated units like lstm. hence memory capacity advantage dilatedrnn dilated obvious. clockwork also utilizes exponentially decreasing temporal resolutions strengthen memory capacity different way. clockwork controls update rate hidden node whereas dilatedrnn updates nodes time step controls data dependency. result memory capacity clockwork undesirably time-dependent output times e.g. exponentials output node short shortest paths connecting input nodes past matching case dilatedrnn; times recurrent paths much longer. since mean recurrent length penalizes time-variant memory capacity taking worst case absolute times dilatedrnn much better mean recurrent length clockwork number groups matching number layers dilatedrnn. section evaluate performance dilatedrnn four different tasks include long-term memorization pixel-by-pixel mnist classiﬁcation character-level language modeling penn treebank speaker identiﬁcation waveforms vctk also investigate effect dilation performance computational efﬁciency. unless speciﬁed otherwise models implemented tensorﬂow default nonlinearities rmsprop optimizer learning rate decay rate weight matrices initialized standard normal distribution. batch size furthermore experiments apply sequence classiﬁcation setting output layer adds sequence. results reported trained models achieve best validation loss. unless stated otherwise tricks gradient clipping learning rate annealing recurrent weight dropout recurrent batch norm layer norm etc. applied. tasks sequence level classiﬁcation tasks therefore gridding problem irrelevant. degridded module needed. three cells vanilla lstm cells combined dilatedrnn refer dilated vanilla dilated lstm dilated respectively. common baselines figure results copy memory problem dilatedrnn converges quickly perfect solutions. except rnns dilated skip connections methods unable improve random guesses. task tests ability recurrent models memorizing long-term information. follow similar setup input sequence length ﬁrst values randomly generated integers next values last values ﬁrst signals model needs start output ﬁrst values inputs. different settings average cross-entropy loss measured last timestamps. therefore random guess yields expected average cross entropy dilatedrnn uses layers hidden state size dilation starts last hidden layer. single-layer baselines hidden units. multi-layer baselines number layers hidden state size dilatedrnn skip vanilla layers skip length layer matches maximum dilation dilatedrnn. convergence curves settings shown ﬁgure settings dilatedrnn vanilla cells converges good optimum training iterations whereas dilated lstm converge slower. might lstm cells much complex vanilla unit. except proposed models models unable better random guess including skip vanilla. results suggest proposed structure simple renovation useful problems requiring long memory. sequential classiﬁcation mnist digits commonly used test performance rnns. ﬁrst implement settings. ﬁrst setting called unpermuted setting follow setups serializing image sequence. second setting called permuted setting rearranges input sequence ﬁxed permutations. training validation testing sets default ones tensorﬂow. hyperparameters results reported table addition baselines already described also implement dilated cnn. however receptive ﬁelds size nine-layer dilated insufﬁcient cover sequence length therefore added layer dilated enlarges receptive ﬁeld size also forms slight advantage dilated dilatedrnn structures. unpermuted setting dilated achieves best evaluation accuracy however performance improvements dilated lstm singlemulti-layer ones marginal might task simple. further observe signiﬁcant performance differences stack vanilla skip vanilla consistent ﬁndings rnns better model long-term dependencies achieves good results recurrent skip connections added. nevertheless dilated vanilla another signiﬁcant performance gain skip vanilla consistent argument section dilatedrnn much balanced memory wide range time periods rnns regular skips. performance dilated dominated dilated lstm even latter fewer parameters former figure results noisy mnist task models without skip connections fail. dilatedrnn signiﬁcant outperforms regular recurrent skips on-pars dilated cnn. permuted setting almost performances lower. however dilatedrnn models maintain high evaluation accuracies. particular dilated vanilla outperforms previous rnn-based state-of-the-art zoneout comparable number parameters. achieves test accuracy parameters. note previous state-of-the-art utilizes recurrent batch normalization. version without much lower performance compared dilated models. believe consistently high performance dilatedrnn across different permutations hierarchical multi-resolution dilations. addition dilated able achieve best performance accordance claim section dilated slightly shorter mean recurrent length dilatedrnn architectures sequence length fall within receptive ﬁeld size. however note achieved adding additional layer expand receptive ﬁeld size compared counterparts. useful information lies outside receptive ﬁeld dilated might fail completely. addition settings propose challenging task called noisy mnist unpermuted pixel sequences uniform random noise length results setups shown ﬁgure dilated recurrent models skip layers hidden units layer. number skips layer skip dilated layers layers respectively. expands receptive ﬁeld size dilated entire input sequence. number ﬁlters layer worth mentioning that case -layer dilated instead produce random guesses. output node sees last input samples contain informative data. reported models hyperparameters shown ﬁrst three table found none models without skip connections able learn. although skip vanilla remains learning performance drops compared ﬁrst unpermuted setup. contrary dilatedrnn dilated models obtain almost performances before. also worth mentioning three experiments dilatedrnn models able achieve comparable results extremely small number parameters. investigate task predicting next character penn treebank dataset follow data splitting rule sequence length commonly used previous studies. corpus contains million words small prone over-ﬁtting. therefore model regularization methods shown effective validation test performances. unlike many existing approaches apply regularization dropout input layer. instead focus investigating regularization effect dilated structure itself. results shown table although zoneout layernorm hm-lstm hypernetowrks outperform dilatedrnn models apply batch layer normalizations regularization. best knowledge dilated achieves best result among models similar sizes without layer normalizations. also dilated models outperform regular counterparts vanilla lstm without increasing model complexity. also perform speaker identiﬁcation task using corpus vctk learning audio models directly waveform poses difﬁcult challenge recurrent models vastly long-term dependency. recently cldnn family models managed match surpass mel-frequency features several speech problems using waveform. however cldnns coarsen temporal granularity pooling ﬁrst-layer output feeding subsequent layers solve memory challenge. instead dilatedrnn directly works waveform without pooling considered difﬁcult. achieve feasible training time adopt efﬁcient generalization dilatedrnn proposed equation mentioned before dilations start model equivalent multiple shared-weight networks working partial inputs predictions made fusing information using -by-m convolutional layer. baseline model follows setting various resolutions dilation starting baseline share-weight networks subnetwork works subsampled sequences. fusion layer used obtain ﬁnal prediction. since regular baselines failed converge also implemented mfcc-based models task setting reference. -dimensional log-mel frequency features computed window shift. inputs mfcc models length match input duration waveform-based models. mfcc feature natural advantages information loss operating subsequences; shorter sequence length. nevertheless dilated models operating directly waveform still offer competitive performance subsection ﬁrst investigate relationship performance number dilations. compare dilatedrnn models different numbers layers noisy mnist task. models vanilla cells hidden state size number dilations starts one. ﬁgure observe classiﬁcation accuracy rate convergence increases models become deeper. recall maximum skip exponential number layers. thus deeper model larger maximum skip mean recurrent length. second consider maintaining large maximum skip smaller number layers increasing dilation bottom layer dilatedrnn first construct nine-layer dilatedrnn model vanilla cells. number dilations starts hidden state size architecture referred starts ﬁgure then remove bottom hidden layers one-by-one construct seven models. last created model three layers number dilations starts figure demonstrates wall time evaluation accuracy training iterations noisy mnist dataset. training time reduces roughly every dropped layer although testing performance decreases dilation start effect marginal small notably model dilation starting able train within minutes using single nvidia maintaining test accuracy. experiments dilatedrnn provide strong evidence simple multi-timescale architectural choice reliably improve ability recurrent models learn long-term dependency problems different domains. found dilatedrnn trains faster requires less hyperparameter tuning needs fewer parameters achieve state-of-the-arts. complement experimental results provided theoretical analysis showing advantages dilatedrnn proved optimality meaningful architectural measure rnns. references martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. david krueger tegan maharaj jános kramár mohammad pezeshki nicolas ballas rosemary anirudh goyal yoshua bengio hugo larochelle aaron courville zoneout regularizing rnns randomly preserving hidden activations. arxiv preprint arxiv. tara sainath weiss andrew senior kevin wilson oriol vinyals. learning speech frontend waveform cldnns. sixteenth annual conference international speech communication association aäron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu. wavenet generative model audio. corr abs/. alexander sasha vezhnevets simon osindero schaul nicolas heess jaderberg david silver koray kavukcuoglu. feudal networks hierarchical reinforcement learning. arxiv preprint arxiv. scott wisdom thomas powers john hershey jonathan roux atlas. full-capacity unitary recurrent neural networks. advances neural information processing systems pages saizheng zhang yuhuai tong zhouhan roland memisevic ruslan salakhutdinov yoshua bengio. architectural complexity measures recurrent neural networks. advances neural information processing systems pages appendix gives detailed derivation conclusions section consider architectures. proposed dilatedrnn structure layers. regular d-layer skip edges length shown ﬁgure regular skip obvious grows linearly within cycle. minimum aggregate length edges travel time. minimum aggregate length edges travel layers ﬁxed. problem ﬁnding reduced ﬁnding reformulated change-making problem given banknotes valued amount denote number banknote make amount total number banknotes used minimized. formally since dilations si’s multiples other simple greedy algorithm sufﬁces shortest path spanning time steps. ﬁrst largest skip edge n/sd times rest dilations residuals. process analogous converting binary representation. hence optimal solution equation appendix provides proof theorem analogy change-making problem theorem reformulated optimal denomination problem involves ﬁnding banknote denominations average number banknotes making change values ranging i.e. minimized. optimal denomination problem remains open problem mathematics solutions readily available candidate denominations conﬁned satisfying equation shown proof adapted proof theorem proof. first easy show architecture minimizes must dilation rate within layer paths consist combinations recurrent edges different dilations optimal shortest paths must lie; architectures depend maximum equation effect. conﬁned candidate problem reduced ﬁnding minimized. apply equation average number recurrent edge usage. hence minimizing reduced minimizing since dilations si’s multiples other simple greedy algorithm sufﬁces shortest path spanning time steps. ﬁrst largest skip edge n/sd times rest skip lengths residuals. therefore time spans ranging histogram uses recurrent edge length time span distributed uniformly across si+/si time. formally total uses recurrent skips length", "year": 2017}