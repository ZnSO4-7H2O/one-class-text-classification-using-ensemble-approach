{"title": "Optimal transport maps for distribution preserving operations on latent  spaces of Generative Models", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Generative models such as Variational Auto Encoders (VAEs) and Generative Adversarial Networks (GANs) are typically trained for a fixed prior distribution in the latent space, such as uniform or Gaussian. After a trained model is obtained, one can sample the Generator in various forms for exploration and understanding, such as interpolating between two samples, sampling in the vicinity of a sample or exploring differences between a pair of samples applied to a third sample. In this paper, we show that the latent space operations used in the literature so far induce a distribution mismatch between the resulting outputs and the prior distribution the model was trained on. To address this, we propose to use distribution matching transport maps to ensure that such latent space operations preserve the prior distribution, while minimally modifying the original operation. Our experimental results validate that the proposed operations give higher quality samples compared to the original operations.", "text": "generative models variational auto encoders generative adversarial networks typically trained ﬁxed prior distribution latent space uniform gaussian. trained model obtained sample generator various forms exploration understanding interpolating samples sampling vicinity sample exploring differences pair samples applied third sample. paper show latent space operations used literature induce distribution mismatch resulting outputs prior distribution model trained address this propose distribution matching transport maps ensure latent space operations preserve prior distribution minimally modifying original operation. experimental results validate proposed operations give higher quality samples compared original operations. generative models variational autoencoders generative adversarial networks emerged popular techniques unsupervised learning intractable distributions. framework generative adversarial networks generative model obtained jointly training generator discriminator adversarial manner. discriminator trained classify synthetic samples real ones whereas generator trained samples drawn ﬁxed prior distribution synthetic examples fool discriminator. variational autoencoders also trained ﬁxed prior distribution done loss autoencoder minimizes variational lower bound data likelihood. vaes gans using data trained generator supposed latent samples ﬁxed prior distribution output samples distribution data. order understand visualize learned model common practice literature generative models explore output behaves various arithmetic operations latent samples paper show operations typically used linear interpolation spherical interpolation vicinity sampling vector arithmetic cause distribution mismatch latent prior distribution results operations. problematic since generator trained ﬁxed prior expects inputs statistics consistent distribution. show this somewhat paradoxically also problem support resulting distribution within support uniformly distributed prior whose points equal likelihood training. figure show examples distribution mismatches induced previous interpolation schemes using uniform prior dimensions. matched interpolation avoids minimal modiﬁcation linear trajectory traversing space points along path distributed identically prior. address this propose distribution matching transport maps obtain analogous latent space operations preserve prior distribution latent space minimally changing original operation. figure showcase proposed technique gives interpolation operator avoids distribution mismatch interpolating samples uniform distribution. points matched trajectories obtained minimal deviations points linear trajectory. literature dozens papers sample operations explore learned models. bengio linear interpolation neighbors latent space study well deep shallow representations disentangle latent space contractive auto encoders seminal paper goodfellow authors linear interpolation latent samples visualize transition outputs trained mnist. dosovitskiy linearly interpolate latent codes auto encoder trained synthetic chair dataset. radford also linearly interpolate samples evaluate quality learned representation. furthermore motivated semantic word vectors mikolov explore using vector arithmetic samples change semantics adding smile generated face. reed linear interpolation explore proposed model operates jointly visual textual domain. brock combine gans vaes neural photo editor using masked interpolations edit embedded photo latent space. numerous works performing operations samples ignored problem distribution mismatch presented figure kingma welling makhzani sidestep problem visualizing models performing operations latent samples instead restrict latent space uniformly sample percentiles distribution grid. samples statistics consistent prior distribution. however approach scale higher dimensions whereas latent spaces used literature hundreds dimensions. table examples interesting sample operations need adapted want distribution result match prior distribution. prior gaussian proposed matched operation simpliﬁes proper re-scaling factor additive operations. related work white experimentally observe distribution mismatch distance origin points drawn uniform gaussian distribution points obtained linear interpolation propose so-called spherical linear interpolation reduce mismatch obtaining higher quality interpolated samples. however proposed approach theoretical guarantees. work propose generic method fully preserve desired prior distribution using sample operations. approach works follows given ‘desired’ operation linear interpolation since distribution match prior distribution search warping distribution order modiﬁcation faithful possible original operation optimal transform maps minimal modiﬁcation recovers prior distribution implicit models gans vaes data drawn unknown random variable learn generator respect ﬁxed prior distribution approximates model trained sample feeding latent samples bring attention operations latent samples z··· i.e. mappings give examples operations table since inputs operations random variables output also random variable typically perform operations realized samples analysis done underlying random variable treatment typically used analyze statistics random variables sample mean sample variance test statistics. table show example operations commonly used literature. discussed introduction operations provide valuable insight trained generator changes creates related samples source samples. common operation linear interpolation view operation since general impossible distribution means distribution mismatch inevitable using linear interpolation. similar analysis reveals operations table figure distribution squared norm midpoints prior distributions dimensions components uniform components gaussian linear interpolation proposed matched interpolation spherical interpolation proposed white linear spherical interpolation introduce distribution mismatch whereas proposed matched interpolation preserves prior distribution priors. curse dimensionality empirically observed white mismatch signiﬁcant high dimensions. illustrate figure plot distribution squared norm midpoint linear interpolation compared prior distribution distributions dramatically different almost common support. appendix expand analysis show happens prior distributions i.i.d. entries order address distribution mismatch propose simple intuitive strategy constructing distribution preserving operators optimal transport strategy cost function step could e.g. euclidean distance used measure faithful modiﬁed operator original operator finding gives minimal modiﬁcation challenging fortunately well studied problem optimal transport theory. refer modiﬁed operation matched version respect cost prior distribution completeness introduce concept optimal transport theory simpliﬁed setting i.e. assuming probability distributions euclidean space skipping measure theoretical formalism. refer villani santambrogio thorough formal treatment optimal transport. problem step ﬁrst posed monge formally stated problem problem given probability distributions domains respectively cost function want minimize however problem remained unsolved relaxed problem studied kantorovich problem problem given probability distributions domains respectively cost function want minimize difference relax deterministic relationship joint probability distribution marginals case problem minimization might empty since guaranteed exists mapping contrast problem always construct joint density marginals trivial construction independent i.e. pxpy. note given joint density view conditioned ﬁxed stochastic function since given ﬁxed speciﬁc function value instead random variable depends density case view problem relaxation problem allowed stochastic mapping. choose cost function problems coincide analytical solution least efﬁcient numerical solution. particular note operators table pointwise points i.i.d. components result also i.i.d. components. fortunately mild constraints scalar problems known solution theorem convex suppose cost takes form given continuous source distribution target distribution ﬁnite optimal transport cost mapping uniform uniform combining theorems obtain concrete realization strategy outlined above. choose cost admits theorem operation pointwise need compute monotone transport i.i.d components distribution need compute component distribution result operation cdfs obtain component-wise modiﬁcation i.e. figure show monotone transport linear interpolation various values detailed calculations examples various operations given appendix uniform gaussian priors. gaussian case particularly simple resulting transport additive operations linear transformation scalar multiplication summarized third column table validate correctness matched operators obtained above numerically simulate distributions examples well prior distributions typically used literature. priors interpolations figure sample million pairs points dimension uniform prior estimate numerically midpoint distribution linear interpolation proposed matched interpolation spherical interpolation white reassuring matched interpolation gives midpoints identically distributed prior. contrast linear interpolation condenses towards origin forming pyramid-shaped distribution since spherical interpolation white follows great circle varying radius points resulting distribution hole circling around origin priors. figure -point interpolation example shows linear slerp transport matched interpolation bottom respectively. icon dataset lsun outputs produced dcgan using uniform prior distribution whereas celeba model uses gaussian prior. output resolution pixels. distribution squared norm midpoints. dramatic difference vector lengths prior midpoints linear interpolation minimal overlap. also show spherical interpolation white matching ﬁrst moment otherwise also induces distribution mismatch. contrast matched interpolation fully preserves prior distribution perfectly aligns. note setting commonly used literature. section present concrete examples differences generator output dependent exact sample operation used traverse latent space generative model. generator output latent samples produced linear interpolation slerp white proposed matched interpolation compared. please refer table overview operators used section. setup used dcgan generative models trained lsun bedrooms celeba icon dataset qualitatively evaluate. lsun model trained different output resolutions providing pixel pixel output images models lsun icon dataset trained uniform latent prior distribution celeba gaussian prior used. dimensionality latent space lsun celeba model trained icon model. furthermore improved wasserstein gradient penalty trained cifar- pixels -dimensional gaussian prior produce inception scores presented section figure -point interpolation sampled points dcgan trained lsun using uniform prior. interpolation shown using linear slerp distribution matched interpolation. -point interpolation begin classic example -point interpolation figure shows three examples dataset interpolation points latent space. example ﬁrst done linear interpolation slerp ﬁnally matched interpolation. figure appendix show densely sampled examples. immediately obvious figures linear interpolation produces inferior results generally blurry less saturated less detailed output images. slerp matched interpolation slightly different however visually obvious superior. differences various interpolation methods celeba much subtle point virtually indistinguishable viewed side-by-side. inconsistency figure -point interpolation sampled points dcgan trained icon dataset using uniform prior. interpolation shown using linear slerp distribution matched interpolation. midpoint interpolation cases point interpolation methods diverge most midpoint interpolation thus provide interpolation midpoints figures direct comparison. figure random walk lsun celeba. random walks consist succession steps random directions calculated sequence directions using vicinity sampling upper rows proposed matched vicinity sampling lower rows. dataset model prior inception score inception score midpoints interpolation operations -point linear -point slerp -point matched -point linear -point slerp -point matched highlights apparent loss detail increasing prevalence artifacts towards midpoint linear version compared slerp compared matched interpolation. vicinity sampling furthermore provide examples vicinity sampling figures analogous previous observations output linear operator lacks deﬁnition sharpness saturation compared spherical matched operators. random walk interesting property matched vicinity sampling obtain random walk latent space applying repeatedly start point drawn prior obtain point sampling single point vicinity using ﬁxed ’step size’ show example walk figure using result repeated application vicinity sampling operation divergence prior distribution non-matched case becomes stronger step resulting completely unrecognizable output images lsun icon models. even celeba model differences minimal before quite apparent experiment. random walk thus perfectly illustrates need respecting prior distribution performing operation latent space adverse effects cumulate repeated application operators comply prior distribution. quantitatively conﬁrm observations previous section using inception score. table compare inception score trained models score sampling midpoints -point -point interpolations described above reporting mean standard deviation samples well relative change original model scores signiﬁcant. compared original scores trained models matched operations statistically indistinguishable linear interpolation gives signiﬁcantly lower score settings observed quality visually slerp heuristic gives similar scores matched operations. shown common latent space operations used generative models induce distribution mismatch prior distribution models trained for. problem mostly ignored literature partially belief problem uniform priors. however statistical experimental analysis shows problem real operations used producing signiﬁcantly lower quality samples compared inputs. address distribution mismatch propose optimal transport minimally modify operations fully preserve prior distribution. give analytical formulas resulting operations various examples easily implemented. matched operators give signiﬁcantly higher quality samples compared originals potential become standard tools evaluating exploring generative models. references yoshua bengio gr´egoire mesnil yann dauphin salah rifai. better mixing deep representations. proceedings international conference machine learning alexey dosovitskiy jost tobias springenberg thomas brox. learning generate chairs convolutional neural networks. proceedings ieee conference computer vision pattern recognition goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems salah rifai pascal vincent xavier muller xavier glorot yoshua bengio. contractive autoencoders explicit invariance feature extraction. proceedings international conference machine learning salimans goodfellow wojciech zaremba vicki cheung alec radford chen. improved techniques training gans. advances neural information processing systems fisher seff yinda zhang shuran song thomas funkhouser jianxiong xiao. lsun construction large-scale image dataset using deep learning humans loop. arxiv preprint arxiv. note analysis seen rigorous version observation made white experimentally show signiﬁcant difference average norm midpoint linear interpolation points prior uniform gaussian distributions. suppose latent space prior i.i.d entries case look squared norm distribution. thus assuming large enough close convergence approximate distribution particular implies almost points relatively thin spherical shell since mean grows whereas standard deviation grows suppose mapping non-decreasing maps continuous distribution distribution i.e. example uniform linear interpolation suppose uniform components uniform. case denote linear interpolation points component distribution pyt. symmetry assume since py−t. obtain convolution i.e. first note compute example gaussian linear interpolation vicinity sampling analogies suppose components case compute linear interpolation before since gaussians gaussian easy proper scaling factor adjust variance back monotone transport adjusting vicinity sampling operation figure -point interpolation sampled points dcgan trained celeba gaussian prior. interpolation shown using linear slerp distribution matched interpolation. figure -point interpolation detail example shows linear slerp transport matched interpolation bottom respectively points taken path produced dcgan using uniform prior distribution.", "year": 2017}