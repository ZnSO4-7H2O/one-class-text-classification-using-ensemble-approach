{"title": "Image Question Answering using Convolutional Neural Network with Dynamic  Parameter Prediction", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "We tackle image question answering (ImageQA) problem by learning a convolutional neural network (CNN) with a dynamic parameter layer whose weights are determined adaptively based on questions. For the adaptive parameter prediction, we employ a separate parameter prediction network, which consists of gated recurrent unit (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights as its output. However, it is challenging to construct a parameter prediction network for a large number of parameters in the fully-connected dynamic parameter layer of the CNN. We reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer. The proposed network---joint network with the CNN for ImageQA and the parameter prediction network---is trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU. The proposed algorithm illustrates the state-of-the-art performance on all available public ImageQA benchmarks.", "text": "figure sample images questions dataset question requires different type and/or level understanding corresponding input image correct answers. image question answering aims solve holistic scene understanding problem proposing task unifying various recognition problems. imageqa task automatically answering questions input image illustrated figure critical challenge problem different questions require different types levels understanding image correct answers. example answer question like weather? need perform classiﬁcation multiple choices related weather decide question like picture taken during day? reason performance single recognition task also capability select proper task important solve imageqa problem. imageqa problem short history computer vision machine learning community already exist several approaches among methods simple deep learning based approaches perform classiﬁcation combination features extracted image question currently demonstrate state-oftackle image question answering problem learning convolutional neural network dynamic parameter layer whose weights determined adaptively based questions. adaptive parameter prediction employ separate parameter prediction network consists gated recurrent unit taking question input fully-connected layer generating candidate weights output. however challenging construct parameter prediction network large number parameters fully-connected dynamic parameter layer cnn. reduce complexity problem incorporating hashing technique candidate weights given parameter prediction network selected using predeﬁned hash function determine individual weights dynamic parameter layer. proposed network—joint network imageqa parameter prediction network— trained end-to-end back-propagation weights initialized using pre-trained gru. proposed algorithm illustrates state-of-the-art performance available public imageqa benchmarks. ultimate goals computer vision holistic scene understanding requires system capture various kinds information objects actions events scene atmosphere relations many different levels semantics. although signiﬁcant progress various recognition tasks made recent years works focus solving relatively simple recognition problems controlled settings dataset consists concepts similar level understanding less efforts made solving various recognition problems simultaneously complex realistic even though crucial step toward holistic scene understanding. the-art accuracy public benchmarks approaches extract image features using convolutional neural network bag-of-words obtain feature descriptors question. interpreted method answer given co-occurrence particular combination features extracted image question. contrary existing approaches deﬁne different recognition task depending question. realize idea propose deep dynamic parameter layer whose weights determined adaptively based questions. claim single deep architecture take care various tasks allowing adaptive weight assignment dynamic parameter layer. adaptive parameter prediction employ parameter prediction network consists gated recurrent units taking question input fully-connected layer generating candidate weights dynamic parameter layer. entire network including imageqa parameter prediction network trained end-to-end back-propagation weights initialized using pre-trained gru. main contributions work summarized below predict large number weights dynamic parameter layer effectively efﬁciently apply hashing trick reduces number parameters signiﬁcantly little impact network capacity. ﬁne-tune pre-trained large-scale text corpus improve generalization performance network. pre-training large corpus natural deal small number training data attempted knowledge. ﬁrst work report results currently available benchmark datasets daquar coco-qa vqa. algorithm achieves state-of-the-art performance three datasets. rest paper organized follows. ﬁrst review related work section section describe overview algorithm architecture network respectively. discuss detailed procedure train proposed network section experimental results demonstrated section learning except malinowski fritz propose bayesian framework exploits recent advances computer vision natural language processing. specifically employs semantic image segmentation symbolic question reasoning solve imageqa problem. however method depends pre-deﬁned predicates makes difﬁcult represent complex models required understand input images. deep learning based approaches demonstrate competitive performances imageqa approaches based deep learning commonly cnns extract features image different strategies handle question sentences. algorithms employ embedding joint features based image question however learning softmax classiﬁer simple joint features—concatenation cnn-based image features continuous bag-of-words representation question—performs better lstm-based embedding coco-qa dataset. another line research utilize cnns feature extraction image question combine features approach demonstrates impressive performance enhancement daquar dataset allowing ﬁne-tuning whole parameters. prediction weight parameters deep neural networks explored context zeroshot learning. perform classiﬁcation unseen classes trains multi-layer perceptron predict binary classiﬁer class-speciﬁc description text. however method directly applicable imageqa since ﬁnding solutions based combination question answer complex problem discussed imageqa involves signiﬁcantly larger candidate answers requires much parameters binary classiﬁcation case. recently parameter reduction technique based hashing trick proposed chen large neural network limited memory budget. however applying technique dynamic prediction parameters deep neural networks attempted knowledge. although imageqa requires different types levels image understanding existing approaches pose problem classiﬁcation task. however believe difﬁcult solve imageqa using single deep neural network ﬁxed parameters. many cnn-based recognition problems well-known ﬁne-tune layers adaptation tasks. addition figure overall architecture proposed dynamic parameter prediction network composed classiﬁcation network parameter prediction network. weights dynamic parameter layer mapped hashing trick candidate weights obtained parameter prediction network. networks designed solve tasks jointly constructing multiple branches connected common architecture. work hope solve heterogeneous recognition tasks using single adapting weights dynamic parameter layer. since task deﬁned question imageqa weights layer determined depending question sentence. addition hashing trick employed predict large number weights dynamic parameter layer avoid parameter explosion. problem formulation imageqa systems predict best answer given image question conventional approaches typically construct joint feature vector based inputs solve classiﬁcation problem imageqa using following equation network. classiﬁcation network cnn. fully-connected layers dynamic parameter layer weights layer determined adaptively parameter prediction network. parameter prediction network cells fully-connected layer. takes question input generates realvalued vector corresponds candidate weights dynamic parameter layer classiﬁcation network. given image question algorithm estimates weights dynamic parameter layer hashing candidate weights obtained parameter prediction network. then feeds input image classiﬁcation network obtain ﬁnal answer. details proposed network discussed following subsections. classiﬁcation network constructed based -layer pre-trained imagenet remove last layer network attach three fullyconnected layers. second last fully-connected layer network dynamic parameter layer whose weights determined parameter prediction network last fully-connected layer classiﬁcation layer whose output dimensionality equal number possible answers. probability answer computed applying softmax function output vector ﬁnal layer. dynamic parameter layer second last fully-connected layer instead classiﬁcation layer because involves smallest number parameters. number parameters classiﬁcation layer increases proportion number possible answers predicting weights classiﬁcation layer good option general imageqa problems terms scalability. choice dynamic parameter layer interpreted follows. ﬁxing classiﬁcation layer adapting immediately preceding layer obtain task-independent semantic embedding possible answers representation input embedded answer space solve imageqa problem. therefore relationships answers globally learned recognition tasks help solve ones involving unseen classes especially multiple choice questions. example exact ground-truth word similar words shown training time network still predict close answers based globally learned answer embedding. even though could also exploit beneﬁt answer embedding based relations among answers deﬁne loss function leave future work. parameter prediction network denotes bias rm×n denotes matrix constructed dynamically using parameter prediction network given input question. words weight matrix corresponding layer parametrized function input question parameter prediction network composed cells followed fully-connected layer produces candidate weights used construction weight matrix dynamic parameter layer within classiﬁcation network. similar lstm designed model dependency multiple time scales. illustrated figure dependency captured adaptively updating hidden states gate units. however contrary lstm maintains separate memory cell explicitly directly updates hidden states reset gate update gate. detailed procedure update described below. words question number words question. time step given embedded vector word encoder updates hidden state time denoted using following equations respectively denote reset update gates time candidate activation time addition indicates element-wise multiplication operator sigmoid function. note coefﬁcient matrices related learned training algorithm. applying encoder question sentence series cells obtain ﬁnal embedding vector question sentence. output parameter prediction network weight matrix fully-connected layer parameter prediction network. note even though employ parameter prediction network since pre-trained network sentence embedding— skip-thought vector model based form neural networks e.g. fully-connected convolutional neural network used construct parameter prediction network. weights dynamic parameter layers determined based learned model parameter prediction network given question. straightforward approach obtain weights generate whole matrix using parameter prediction network. however size matrix large network overﬁtted easily given limited number training examples. addition since need quadratically parameters fully-connected layer parameter prediction network increase dimensionality output desirable predict full weight matrix using network. therefore preferable construct based small number candidate weights using hashing trick. single output value parameter prediction network shared multiple connections dynamic parameter layer derivatives respect shared weights need accumulated compute derivative respect element output parameter prediction network follows denotes indicator function. gradients preceding layers classiﬁcation parameter prediction networks computed standard backpropagation algorithm. using pre-trained although encoders based recurrent neural networks lstm demonstrate impressive performance sentence embedding beneﬁts imageqa task marginal comparison bag-of-words model reasons fact lack language data imageqa dataset. contrary tasks large-scale training corpora even largest imageqa dataset contains relatively small amount language data; example contains questions total. note model trained using corpus sentences. deal deﬁciency linguistic information imageqa problem transfer information acquired large language corpus ﬁne-tuning pre-trained embedding network. initialize skipthought vector model trained book-collection corpus containing sentences note skip-thought vector model trained unsupervised manner predicting surrounding sentences embedded sentences. task requires understand context pre-trained model produces generic sentence embedding difﬁcult trained limited number training examples. ﬁne-tuning initialized generic sentence embedding model imageqa obtain representations questions generalized better. fine-tuning eter candidate weight vector shared multiple elements done applying predeﬁned hash function converts location index simple hashing trick reduce number parameters maintaining accuracy network {+−} another hash function independent function useful remove bias hashed inner product implementation hash function adopt open-source implementation xxhash. believe reasonable reduce number free parameters based hashing technique many redundant parameters deep neural networks network parametrized using smaller candidate weights. instead training huge number parameters without constraint would advantageous practically allow multiple elements weight matrix share value. also demonstrated number free parameter reduced substantially little loss network performance section discusses error back-propagation algorithm proposed network introduces techniques adopted enhance performance network. training error back-propagation proposed network trained end-to-end minimize error ground-truths estimated answers. error back-propagated chain rule classiﬁcation network parameter prediction network jointly trained ﬁrst-order optimization method. denote loss function. partial derivatives respect element input output dynamic parameter layer given respectively questions coco-qa automatically generated image descriptions coco dataset using constituency parser simple question-answer generation rules. questions dataset typically long explicitly classiﬁed types depending generation rules object questions number questions color questions location questions. answers one-words questions training questions testing. similar coco-qa also constructed coco question associated multiple answers annotated different people. dataset contains largest number questions training validation testing testing data splited test-dev test-standard testchallenge test-reserve question provided answers take consensus annotators account. answers single words answers exceed three words. daquar coco-qa employ classiﬁcation accuracy relaxed version based word similarity wups uses thresholded wu-palmer similarity based wordnet taxonomy compute similarity words. predicted answer groundtruth answer example wups given dataset provides open-ended task multiplechoice task evaluation. open-ended task answer word phrase answer chosen candidate answers multiple-choice task. cases answers evaluated accuracy reﬂecting human consensus. predicted answer target answer example accuracy given denotes indicator function. words predicted answer regarded correct least three annotators agree score depends number agreements predicted answer correct. problem. observe gradients dynamic parameter layer noisy since weights predicted parameter prediction network. hence straightforward approach ﬁne-tune typically fails improve performance employ slightly different technique ﬁne-tuning sidestep observed problem. update parameters network using datasets except part transferred -layer beginning start update weights subnetwork validation accuracy saturated. training details training question sentences normalized lower cases preprocessed simple tokenization technique normalize answers lower cases regard whole answer single multiple words separate class. network trained end-to-end back-propagation. adam used optimization initial learning rate clip gradient handle gradient explosion recurrent structure training terminated progress validation accuracy epochs. optimizing dynamic parameter layer straightforward since distribution outputs dynamic parameter layer likely change signiﬁcantly batch. therefore apply batch-normalization output activations layer alleviate problem. addition observe tends converge fast overﬁt data easily training continues without restriction. stop ﬁne-tuning network start overﬁt continue train parts network; strategy improves performance practice. evaluate proposed network public imageqa benchmark datasets daquar cocoqa collected question-answer pairs existing image datasets answers single words short phrases. daquar based nyudv dataset originally designed indoor segmentation using rgbd images. daquar provides benchmarks distinguished number classes amount data; daquar-all consists questions training testing respectively includes categories answer. daquar-reduced includes answer categories training testing questions. test three independent datasets coco-qa daquar ﬁrst present results dataset table proposed dynamic parameter prediction network outperforms existing methods nontrivially. performed controlled experiments analyze contribution individual components proposed algorithm—dynamic parameter prediction pre-trained ﬁne-tuning trained additional models concat rand-gru cnn-fixed. cnn-fixed useful impact ﬁne-tuning since identical dppnet except weights ﬁxed. rand-gru model without pre-training weights word embedding model initialized randomly. ﬁne-tune either. concat basic model predicts answers using fully-connected layers combination features. obviously employ components parameter prediction pre-trained ﬁne-tuning. results controlled experiment also illustrated table concat already outperforms lstm integrating instead lstm batch normalization. rand-gru achieves better accuracy employing dynamic parameter prediction additionally. interesting improvement comes yes/no questions involve various kinds tasks since easy many different aspects input image binary classiﬁcation. cnn-fixed improves accuracy adding pre-training ﬁnal model dppnet achieves state-of-the-art performance dataset large margins illustrated table algorithm outperforms existing approaches consistently benchmarks. table single answer multiple answers denote subsets questions divided number ground-truth answers. also numbers second rows wups thresholds. understand parameter prediction network understand questions present several representative questions ﬁne-tuning descending order based cosine similarities query question table retrieved sentences frequently determined common subjective objective words ﬁne-tuning rely tasks solved ﬁne-tuning. qualitative results proposed algorithm presented figure general proposed network successful handle various types questions need different levels semantic understanding. figure shows network able adapt recognition tasks depending questions. however often fails questions asking number occurrences since questions involve difﬁcult tasks learn image level annotations. hand proposed network effective answers question different images fairly well illustrated figure refer project website comprehensive qualitative results. figure sample images questions dataset question requires different type and/or level understanding corresponding input image correct answer. answers blue correct answers incorrect. incorrect answers ground-truth answers provided within parentheses. parameter prediction network. classiﬁcation network dynamic parameter layer enables classiﬁcation network adaptively determine weights parameter prediction network. predicting entries weight matrix infeasible large dimensionality relieved limitation using parameter hashing weight sharing. effectiveness proposed architecture supported experimental results showing state-of-the-art performances three different datasets. note proposed method achieved outstanding performance even without complex recognition processes referencing objects. believe proposed algorithm extended integrating attention model solve difﬁcult problems.", "year": 2015}