{"title": "Properties of Bethe Free Energies and Message Passing in Gaussian Models", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We address the problem of computing approximate marginals in Gaussian probabilistic models by using mean field and fractional Bethe approximations. We define the Gaussian fractional Bethe free energy in terms of the moment parameters of the approximate marginals, derive a lower and an upper bound on the fractional Bethe free energy and establish a necessary condition for the lower bound to be bounded from below. It turns out that the condition is identical to the pairwise normalizability condition, which is known to be a sufficient condition for the convergence of the message passing algorithm. We show that stable fixed points of the Gaussian message passing algorithm are local minima of the Gaussian Bethe free energy. By a counterexample, we disprove the conjecture stating that the unboundedness of the free energy implies the divergence of the message passing algorithm.", "text": "address problem computing approximate marginals gaussian probabilistic models using mean ﬁeld fractional bethe approximations. deﬁne gaussian fractional bethe free energy terms moment parameters approximate marginals derive lower upper bound fractional bethe free energy establish necessary condition lower bound bounded below. turns condition identical pairwise normalizability condition known suﬃcient condition convergence message passing algorithm. show stable ﬁxed points gaussian message passing algorithm local minima gaussian bethe free energy. counterexample disprove conjecture stating unboundedness free energy implies divergence message passing algorithm. major tasks probabilistic inference calculating marginal posterior probabilities variables given observations. case gaussian models computational complexity computing marginals might scale cubically number variables models discrete variables often leads intractable computations. computations made faster tractable using approximate inference methods like mean ﬁeld approximation bethe-type approximation methods developed discrete probabilistic graphical models applicable gaussian models well. however important diﬀerences behavior discrete gaussian cases. example discrete models error function bethe approximation—called bethe free energy—is bounded gaussian models might always case understanding properties bethe free energy gaussian models might also help understand properties energy function conditional gaussian models. conditional gaussian hybrid graphical models switching kalman ﬁlters combine discrete gaussian variables. approximate inference models carried expectation propagation viewed generalization bethe approximation marginal consistency constraints approximate marginals replaced expectation constraints order understand properties bethe free energy hybrid models good understanding special cases discrete gaussian models needed. properties bethe free energy discrete models studied extensively last decade well understood properties gaussian bethe free energy studied much less. message passing algorithm well established method ﬁnding stationary points bethe free energy works locally updating approximate marginals successfully applied discrete gaussian models gaussian message passing simplest case free-energy based message passing algorithm models continuous variables therefore important understand behavior. gaussian message passing many practical applications like distributed averaging peer-to-peer rating linear detection regression generally problems involve solving large sparse linear systems approximating marginal variances large sparse gaussian systems typically encountered distributed computing settings. applications reader referred work bickson references therein. finding suﬃcient conditions convergence message passing gaussian models successfully addressed many authors. using computation tree approach weiss freeman proved message passing converges whenever precision matrix—inverse covariance—of probability distribution diagonally dominant. help analogy message passing walk–sum analysis derived stronger condition pairwise normalizability. diﬀerent approach taken welling directly minimized bethe free energy regard parameters approximate marginals conjecturing gaussian message passing converges free energy bounded below. experiments showed message passing direct minimization either converge solution fail converge. adopt similar approach instead analyzing properties gaussian message passing algorithm using approaches like weiss freeman malioutov choose study properties gaussian bethe free energy stationary points. help draw conclusions existence local minima possible stable ﬁxed points message passing converge. paper structured follows. section introduce gaussian markov random ﬁelds message passing algorithm. section deﬁne gaussian fractional bethe free energies parameterized moment parameters approximate marginals derive boundedness conditions them. sections based authors earlier work section analyze stability properties gaussian message passing algorithm using similar line argument watanabe probability density also deﬁned terms undirected probabilistic graphical model commonly known gaussian markov random ﬁeld since interactions variables pairwise associate variables nodes undirected graph edges graph stand non-zero oﬀ-diagonal elements proxy using notation introduced above density written partitioning corresponding factors. practice however factors might given problem hand well computed summing parameters computing partitioning respectively. without loss generality since results paper easily re-formulated general rescaling variables takahashi equations alternative option calculate marginal means approximate marginal variances gaussian message passing algorithm probabilistic graphical model associated representation gaussian message passing algorithm gaussian variant message passing algorithm dynamical programming algorithm introduced compute marginal densities discrete probabilistic models pairwise interactions tree-structured graphs however turned running loops graphs cycles yields good approximations marginal distributions weiss freeman showed gaussian message passing algorithm converging computes exact mean parameters thus also used solving linear systems message passing works updating passing directed messages along edges graph which case algorithm converges used compute marginal probability distributions. gaussian discrete algorithms functional form exception summation integration operators message µi←j updated according update steps iterated convergence. corresponding qijs common damping yield ﬁnal approximation replace µnew practice helps dampen possible periodic paths keeps properties ﬁxed points unchanged. figure illustrates incoming outgoing messages nodes associated variables quite signiﬁcant diﬀerence discrete gaussian message passing replacement operator integral operator. ﬁnite sums always exist integral become inﬁnite. problem remedied technically canonical parameterization keeps algorithm running lead non-normalizable approximate marginals thus break-down algorithm. ﬁnding stationary points so-called bethe free energy error function measuring diﬀerence speciﬁc family distributions detailed next section. shown heskes later diﬀerent watanabe fukumizu stable ﬁxed points message passing algorithm local minima corresponding bethe free energy. paper show holds gaussian models well. interest properties gaussian bethe free energy corresponding gaussian message passing algorithm motivated mainly implications general models inference algorithms like non-gaussian models expectation propagation respectively. reason compare speed method accuracy approximation mentioned exact linear algebraic methods. mentioned introduction approach take similar welling study properties gaussian bethe free energy parameterized terms moment parameters approximate marginals. following introduce mean ﬁeld bethe approximation gaussian models. readers familiar subject continue section popular method approximate marginals approximating distribution form makes marginals easy identify example factorizes treelike form. common quantity measure diﬀerence probability family densities possessing form makes marginals easy identify words problem approximate distribution independent variables. approximation type called mean ﬁeld approximation deﬁning easily mean ﬁeld approximation underestimates variances. mean ﬁeld approximation computes solution means exact variances computed interactions variables namely matrix diagonal thus giving poor estimates variances. order improve estimates variances choose approximating distributions able capture dependencies variables veriﬁed distribution dependencies form tree graph written form form together constraints qijs called bethe approximation. denote family functions choosing qiqj easily check thus non-empty. assuming approximate stationary conditions lagrangian corresponding fractional bethe free energy marginal consistency normalization constraints derive iterative algorithm corresponding lagrange multipliers consistency constraints similarly approximate marginals computed according shown one-to-one correspondence stationary points bethe free energy ﬁxed points message passing algorithm later section link stable ﬁxed points local minima mentioned introduction case gaussian models message passing algorithm always converge. reason appears approximate marginals indeﬁnite negative deﬁnite covariance matrices. welling pointed unboundedness bethe free energy. showed fractional bethe free energy interpolates mean ﬁeld bethe approximation. bethe free energy case αijs tend mutual information variables highly penalized therefore enforces solutions close mean ﬁeld solution. also showed fractional message passing algorithm derived interpreted pearl’s message passing algorithm diﬀerence instead computing local marginals— like pearl’s algorithm—one computes local αij–marginals. local αij–marginals correspond true local marginals local mean ﬁeld approximations resulting algorithm called fractional message passing algorithm message updates deﬁned power expectation propagation minka approximate inference method uses local approximations α–divergences. case gaussian models power expectation propagation—with fully factorized approximating distribution—leads message passing algorithm derived appropriate constraints. starting idea creating upper bound partition function exponential distributions wainwright derived form message passing works well practice however ways local minima fractional free energies like direct minimization w.r.t. parameterization approximate marginals latter method slower likely converge. following analyze bethe free energy expressed terms moment parameters approximate marginals qij. later section analyze stability conditions fractional message passing algorithm expressing conditions term moment parameters approximate marginals show stable ﬁxed points fractional gaussian message passing local minima fractional bethe free energy. section analyze parametric form show fractional gaussian bethe free energy non-increasing function letting tend inﬁnity obtain lower bound free energies. turns condition lower bound bounded pairwise normalizability condition work malioutov constraints parameterization. slight abuse notation matrix formed diagonal elements oﬀ-diagonal elements denoted vector means vector variances substituting gets achieved q−h. check second order derivative regard non-negative ﬁrst order derivative vivj. since variables independent conclude convex vij. independence follows proof since interaction parameters term depending bounded positive deﬁniteness simply neglect term analyzing boundedness write detail lower bound fractional bethe free energies form latter bounded follows bounded well. according lemma boundedness implies fractional bethe free energies bounded below. statement assumed gaussian network connected undirected. according perron-frobenius theory non-negative matrices simple maximal eigenvalue λmax elements eigenvector umax corresponding positive. take fractional bethe free energy tumax large values analyze behavior simpliﬁes term dominates logarithmic ones result limit independent choice tends whenever λmax statement λmax direction quadratic term log. α—and thus fα—depends condition shown malioutov condition λmax equivalent condition pairwise normalizability. therefore pairwise normalizability suﬃcient condition message passing algorithm converge also necessary condition fractional gaussian bethe free energies bounded. using lemma show suitably chosen always exists constrained fractional free energy possesses local minimum example case models adjacency matrix corresponding k–regular graph equal interaction weights maximal eigenvalue λmax eigenvector corresponding eigenvalue model symmetric verifying stationary point conditions turns choice exists local minimum also lies direction show model pairwise normalizable critical fractional figure visualizing critical parameters symmetric k-regular gaussian model node –regular gaussian model varying plots right panel correspond constrained bethe free energies node –regular gaussian model varying here rvalid supremum model valid positive deﬁnite. example disproves conjecture welling even bethe free energy bounded below possess ﬁnite local minimum message passing minimization algorithms converge. section turn attention towards properties message passing algorithm gaussian models. following similar line argument watanabe fukumizu show stable ﬁxed points message passing algorithm correspond local minima bethe free energy. moment parameterization introduced previous sections. proceed following make linear expansion message passing iteration ﬁxed point express linear expansion terms moment parameters corresponding ﬁxed point ﬁnally connect properties latter properties hessian bethe free energy using matrix determinant lemma. form equation implies messages µi←j univariate gaussian functions thus express terms scalar parameters ηijxi τijj τijs irrelevant constants. expressed terms damped message passing algorithm translates parameters section assumption approximate marginals might normalizable message passing iteration stays well deﬁned unless zero denominator rhs. rarely happens practice. however common message passing converges intermediate steps approximate marginals normalizable. often remedied choosing appropriate damping parameter iteration λijs independent ηijs iteration ηijs linear ηij. interesting neither constrained bethe free energy message passing algorithm depend sign rij. relevant compute means—when —and signs correlations following analyze stability message passing iteration ﬁxed points stationary points lagrangian corresponding constrained minimization gaussian bethe free energy. reiterate denote number non-zeros since parallel message update given equations rewritten terms matrix-vector multiplications element element operations vectors computational complexity update also scales roughly nnzeros goal connect stability properties message passing algorithm properties bethe free energy. therefore express stability properties terms moment parameters approximate marginals. leads normalizable approximate marginals identify local covariance parameters deﬁned section without enforcing marginal matching constraints approximate local covariances fully determined form leaves moment parameters computed message passing algorithm. r|e| deﬁned ˆvij vij/ written since singular graph k-regular—see property section appendix details—for rest cases continuous bijective mapping moment parameters canonical parameters lead normalizable approximate marginals. forms free energies message passing algorithms diﬀerent gaussian discrete case stability conditions similar forms. allow results watanabe fukumizu next section show implications condition properties hessian free energy. shows boundedness existence local minima case unbounded plays signiﬁcant role convergence gaussian message passing. illustrate section fractional message passing algorithm converges converges messages corresponds local minimum fractional free energy. also implies mean parameters local approximate marginals exact note observations section property appendix together property imply always range values fractional free energy possesses local minimum fractional message passing converge. local stability condition independent damping parameter therefore alter local stability properties makes iteration slower numerically stable dampen possible periodic trajectories message passing algorithm. fractional parameter characterizes inference process seen example previous sections choosing smaller create local minima. particular case somewhat similar property message passing updates well. r|e| messages lead normalizable approximate marginals. characterized model parameters reiterate elements local variances continuous bijective mapping r|e| given unless k-regular. allows study stability properties terms moment parameters vector local correlations. using gershgorin’s theorem eigenvalue α−diag)m α−diag)m method. newton method started diﬀerent initial points. experienced algorithm converge high values explained plots figure high values initial point might convergence region local minimum. fractional message passing algorithm used types initialization symmetric partitioning diagonal elements. initial messages approximate marginals normalizable ﬁrst step iteration. experienced behavior similar described welling standard message passing namely fractional message passing direct minimization either converge fail converge. experiments combination theorem minimum bethe free energy. standard message passing fails converge decrease search stationary point—preferably local minimum—of corresponding fractional free energy. seen results right panels figure model longer pairwise normalizable local minimum unbounded global minimum viewed natural continuation global minimum pairwise normalizable models. explains quality approximation local minimum models pairwise normalizable still comparable global minimum models pairwise normalizable. provide tight upper lower bounds gaussian fractional bethe free energies. turns pairwise normalizability suﬃcient condition message passing algorithm converge also necessary condition gaussian fractional bethe free energies bounded below. model pairwise normalizable lower bound bounded direct minimization message passing converging. experiments converged minimum. suggests pairwise normalizable case fractional bethe free energies possess unique global minimum. model pairwise normalizable none fractional bethe free energies bounded below. however always range values fractional free energy possesses local minimum direct minimization fractional message passing converge. thus decreasing towards zero gets closer mean ﬁeld energy ﬁnite local minimum appear experienced suitable range αs\u0001s initial values fractional gaussian message passing made converge. mentioned section αijs correspond using local divergences applying power expectation propagation fully factorized approximating distribution. seeger reports expectation propagation converge applying power expectation propagation helps achieve convergence. case problem addressed paper behavior explained observation small make ﬁnite local minima likely occur thus prevents covariance matrices becoming indeﬁnite even positive deﬁnite. although common reason using numerical robustness also implies ﬁnding saddle point α-fractional free energy. might interesting investigate whether reason convergence likely case gaussian fractional message passing. wainwright propose convexify bethe free energy discrete models choosing αijs suﬃciently large fractional bethe free energy unique global minimum. strategy appears fail gaussian models. convexiﬁcation makes possibly useful ﬁnite local minima disappear leaving unbounded global minimum. case general hybrid models convexiﬁcation still unclear. example section disproves conjecture work welling even bethe free energy bounded below possess ﬁnite local minimum message passing minimization algorithms converge. shown stable ﬁxed points gaussian fractional message passing algorithms local minima fractional bethe free energy. although existence local minimum guarantee convergence message passing algorithm practice experienced existence local minimum implies convergence. based results hypothesize pairwise normalizability hold gaussian bethe free energy gaussian message passing algorithm types behavior gaussian bethe free energy possesses unique ﬁnite local minimum optimization methods converge starting from mean ﬁeld solution /qii; gaussian message passing corresponding unique stable ﬁxed point converge suitable starting point suﬃcient damping remains open question. believe properties free energies k-regular symmetric models critical values easily computed give good insight properties free energies general gaussian models. would like thank jason johnson sharing ideas properties message passing algorithm k-regular models. would also like thank anonymous reviewers valuable comments earlier versions manuscript. research reported paper supported vici grant netherlands organization scientiﬁc research form implies always choose proper subset positive quadrant words properties domain since interior closed follows bounded converges uniformly w.r.t. implies exists closed bounded continuous latter conditions imply extrema local minimum interior", "year": 2014}