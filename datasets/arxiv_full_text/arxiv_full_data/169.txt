{"title": "Overcoming catastrophic forgetting with hard attention to the task", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.", "text": "catastrophic forgetting occurs neural network loses information learned previous task training subsequent tasks. problem remains hurdle artiﬁcial intelligence systems sequential learning capabilities. paper propose task-based hard attention mechanism preserves previous tasks’ information without affecting current task’s learning. hard attention mask learned concurrently every task stochastic gradient descent previous masks exploited condition learning. show proposed mechanism effective reducing catastrophic forgetting cutting current rates also show robust different hyperparameter choices offers number monitoring capabilities. approach features possibility control stability compactness learned knowledge believe makes also attractive online learning network compression applications. renewed interest neural networks problems re-emerge specially solution still open. case so-called catastrophic forgetting catastrophic interference problem essence catastrophic forgetting corresponds tendency neural network forget learned upon learning different information. instance network ﬁrst trained convergence task trained second task forgets perform ﬁrst task. telef´onica research barcelona spain universitat polit`ecnica catalunya barcelona spain universitat pompeu fabra barcelona spain. correspondence joan serr`a <joan.serratelefonica.com>. able seamlessly remember different tasks learn sequentially following lifelong learning paradigm apart biologically plausible many practical situations require sequential learning system instance unattainable robot retrain scratch underlying model upon encountering object/task. accumulating large number objects/tasks corresponding information performing concurrent multitask learning scale costly. storing previous information using retrain model among earliest attempts overcome catastrophic forgetting; strategy named rehearsal memory modules context subject research today however efﬁciency capacity constrains memory-free approaches also introduced starting termed pseudorehearsal approach found success transfer learning situations needs maintain certain accuracy source task learning target task within pseudo-rehearsal category could also consider recent approaches substitute memory module generative network besides difﬁculty training generative network sequence tasks certain types data rehearsal pseudo-rehearsal approaches imply form concurrent learning re-process ‘old’ instances learning task. popular strategy overcome catastrophic forgetting reduce representational overlap done output intermediate also input levels clean soft manner so-called structural regularization either present loss function separate merging step strategies seeks prevent major changes weights important previous tasks. dedicating speciﬁc sub-parts network task another reducing representational overlap main tradepaper propose task-based hard attention mechanism maintains information previous tasks without affecting learning task. concurrently learning task also learn almost-binary attention vectors gated task embeddings using backpropagation minibatch stochastic gradient descent attention vectors previous tasks used deﬁne mask constrain updates network’s weights current tasks. since masks almost binary portion weights remains static rest adapt task. call approach hard attention task evaluate context image classiﬁcation using believe high-standard evaluation protocol consider random sequences publicly-available data sets representing different tasks compare dozen recent competitive approaches. show favorable results different experimental setups cutting current rates also show robustness respect hyperparameters illustrate number monitoring capabilities. make code experiments publicly-available. primary observation drives proposed approach task deﬁnition pragmatically identiﬁer crucial operation network. consider task discriminating bird images. training network learn intermediate features. second task discriminate brown black animals using data network learn features much overlap ﬁrst ones. thus training data tasks important difference task description identiﬁer. intention learn task identiﬁer condition every layer later exploit learned conditioning prevent forgetting previous tasks. condition current task employ layer-wise attention mechanism given output units layer element-wise multiply however important difference common attention gate function positive scaling parameter. sigmoid gate experiments note gating mechanisms could used. layers operate equally except last layer binary hard-coded. operation layer equivalent multi-head output routinely employed context catastrophic forgetting idea behind gating mechanism form hard possibly binary attention masks which inhibitory synapses thus activate deactivate output units every layer. similar pathnet dynamically create destroy paths across layers later preserved learning task. however unlike pathnet paths based modules single units. therefore need pre-assign module size maximum number modules task. given network architecture learns automatically dimensions individual-unit paths ultimately affect individual layer weights. furthermore instead learning paths separate stage using genetic algorithms learns together rest network using backpropagation sgd. preserve information learned previous tasks upon learning task condition gradients according cumulative attention previous tasks. obtain cumulative attention vector learning task obtaining using element-wise maximum all-zero vector preserves attention values units important previous tasks allowing condition training future tasks. unit indices correspond output input layers respectively. words expand vectors match dimensions gradient tensor layer perform element-wise minimum subtraction multiplication compute attention input data consists complex signals like images audio. however case data consisted separate independent features could also consider output layer apply methodology. note that create masks prevent large updates weights important previous tasks. similar approach packnet made public development hat. packnet heuristic selection retraining binary mask found later applied freeze corresponding network weights. regard differs packnet three important aspects. firstly mask unit-based weight-based masks automatically derived those. therefore also stores maintains lightweight structure. secondly mask learned instead heuristicallyrule-driven. therefore need pre-assign compression ratios determine parameter importance post-training step. thirdly mask necessarily binary allowing intermediate values useful want reuse weights learning tasks expense forgetting want work online mode forgetting oldest tasks remember ones. train embeddings backpropagation prefer differentiable function. construct pseudo-step function allows gradient sigmoid positive scaling parameter scaling introduced control polarization ‘hardness’ pseudo-step function resulting output strategy anneal training inducing gradient smax testing using smax approximates unit step function. notice latter start training epoch network units equally active progressively polarize within epoch. batch index total number batches epoch. hyperparameter smax controls stability learned tasks words plasticity network’s units. smax close gating mechanism operates like regular sigmoid function without particularly enforcing binarization provides plasticity units model able forget previous tasks backpropagation stage alternatively smax larger number gating mechanism starts operating unit step function. provides stability regard previously learned tasks preventing changes corresponding weights backpropagation stage. preliminary analysis empirically observed embeddings changing much magnitude gradient weak weights. investigation realized major part problem introduced annealing scheme illustrate effect annealing scheme gradients across active range standard sigmoid perform annealing obtain cumulative gradient epoch bell-like shape spans whole sigmoid range contrastingly smax obtain much larger magnitude fig. annealed much lower range sense thought compressibility constant affecting compactness learned models higher lower number active attention values sparse resulting network globally tasks adapt best compression individual task. regularization promote network sparsity context catastrophic forgetting also considered yoon dynamically expandable networks introduced developing hat. plain regularization combined considerable heuristics l-transfer thresholding measure semantic drift applied network weights so-called selective retraining phase. attention-weighted regularization attention values independent part single training phase approach. instead considering network weights focuses unit attention. compare proposed approach conceptually closest works appeared concurrently development hat. general overview related work done sec. qualitative comparison three related strategies done along sec. quantitative comparison approaches done sec. appendix elastic weight consolidation synaptic intelligence approaches ‘soft’ structural regularization term loss function order discourage changes weights important previous tasks. uses ‘hard’ structural regularization loss function gradient magnitudes explicitly. measures weights’ importance network training compute weights’ importance concurrently network training. speciﬁc formulation learns attention masks. incremental moment matching evolution performing separate model-merging step learning task. essence idea embedding gradient compensation remove effects annealed sigmoid artiﬁcially impose desired range magnitude motivated previous paragraph. divide gradient derivative annealed sigmoid multiply desired compensation constrain numerical stability clamp |set remain within active range standard sigmoid case however limits. constant regions pseudo-step function. notice also that minimum never equal important realize hard attention values directly deterli ‘active’ mine units dedicated task therefore order model capacity reserved future tasks promote sparsity attention l−}. regularvectors ization term loss function takes account cumulative attention vectors task {a<t two-task transfer learning setups accuracy measured source target tasks however limitations setups. firstly performing permutations mnist data suggested favor certain approaches yielding misleading results context catastrophic forgetting secondly using mnist data representative modern computer vision tasks particularly challenging thirdly incrementally adding classes groups classes implies assumption data comes joint distribution unrealistic real-world setting. finally evaluating catastrophic forgetting tasks biases conclusions towards transfer learning setups prevents analysis truly sequential learning tasks. paper consider aforementioned mnist cifar setups nonetheless primarily evaluate sequence multiple tasks formed different classiﬁcation data sets obtain generic estimate weigh number tasks uniformly randomize order. training task compute accuracies testing sets tasks repeat times sequential train/test procedure different seed numbers also used rest randomizations initializations compare different task accuracies order obtain general measurement amount forgetting introduce forgetting ratio aτ≤t accuracy measured task sequentially learning task accuracy random frequency-based classiﬁer solely trained task aτ≤t accuracy measured task jointly learning tasks multitask fashion. note correspond performances close ones random multitask classiﬁers respectively. report single number learning tasks take average data consider common image classiﬁcation data sets adapt them necessary input size pixels. number classes goes training sizes test sizes task ranessentially mnist data contains many values close allow easier identiﬁcation important units weights which permuted easily frozen without overlapping ones tasks assigning column width task. employ so-called adapters reuse knowledge previous columns/tasks leading progressive increase number weights assigned future tasks. instead blindly pre-assigning column widths learns ‘widths’ layer together network weights adapts difﬁculty current task. pathnet also pre-assigns amount network capacity task contrast pnns avoids network columns adapters. uses evolutionary approach learn paths constant number so-called modules interconnect themselves. maintain population solutions entirely trains backpropagation rely constant modules. together pnns pathnet packnet also employs binary mask constrain network. however constrain based columns layer modules network weights. therefore allows potentially better network’s capacity. packnet based heuristic weight pruning pre-assigned pruning ratios. also focuses network weights uses unit-based masks constrain those also results lightweight structure. avoids absolute pre-assigned pruning ratio although uses compressibility parameter inﬂuence compactness learned models. another difference previous three approaches purely binary masks. instead stability parameter smax controls degree binarization. dynamically expandable networks also assign network capacity depending task hand. however separate stage called selective retraining. complex mixture heuristics hyperparameters used identify drifting units duplicated retrained another stage. regularization l-transfer employed condition learning together corresponding regularization constants additional thresholds. strives simplicity restricting number hyperparameters straightforward conceptual interpretation. instead plain regularization network weights employs attention-weighted regularization attention masks. attention masks lightweight structure plugged without need introducing important changes pre-existing network. experiments setups common setups evaluate catastrophic forgetting classiﬁcation context based permutations mnist data label splits mnist data incrementally learning classes cifar data sets facescrub fashionmnist notmnist mnist svhn trafﬁcsigns details data refer appendix baselines consider reference approaches plus recent competitive ones standard dropout freezing layers except last learning without forgetting less-forgetting learning pathnet pnns. best hyperparameter combination approach perform grid search using task sequence determined single seed. compute forgetting ratio also aforementioned random multitask classiﬁers. network unless stated otherwise employ alexnet-like architecture convolutional layers ﬁlters kernel sizes respectively plus fullyconnected layers units each. rectiﬁed linear units activations max-pooling convolutional layers. also dropout ﬁrst layers rest. fully-connected layer softmax output used ﬁnal layer together categorical cross entropy loss. layers randomly initialized xavier uniform initialization except embedding layers gaussian distribution unless stated otherwise code uses pytorch’s defaults version adapt base architecture baseline approaches match number parameters training train models backpropagation plain using learning rate decaying factor improvement validation loss consecutive epochs. stop training reach learning rate lower iterated epochs batch size methods task sequence data split batch shufﬂe weight initialization given seed. results ﬁrst look average forgetting ratio learning task ﬁrst thing note considered baselines perform better references. case lfl. observe still competitive two-task setup designed however performance rapidly degrades indicating approach difﬁculties extending beyond transfer learning setup. extremely sensitive conﬁguration hyperparameter point good value seed turns choice another seed. hence poor average performance seeds. highest standard deviations obtained pathnet suggests high sensitivity respect hyperparameters initializations data sets. another thing note approaches perform similarly slightly better sgd-f reference. believe different nature tasks’ data consideration tasks complicates choice mixing hyperparameter. best performing baselines pathnet pnn. pathnet present contrasting behaviors. both construction never forget; therefore important difference learning capability. pathnet starts correctly learning ﬁrst task progressively exhibits difﬁculties contrastingly pnns exhibits difﬁculty ﬁrst tasks becomes better increases. contrasting behaviors approaches allocate network capacity. mentioned cannot dynamically therefore need pre-assign number network weights task. tasks network capacity pre-assignment increasingly harms performance baselines lowering corresponding curves fig. move results. first observe consistently performs better considered baselines case obtains average forgetting ratio best baseline case obtains best baseline implies reduction forgetting notice standard deviation lower ones obtained majority baselines denotes certain stability respect different task sequences data sets data splits network initializations. given slightly increasing tendency could speculate would score however empirical analyzes suggest case particular observe gradual lowering pathnet curves increasing sequences addition observe pathnet obtaining worse performances case incremental class setup general none baseline methods consistently outperforms rest across setups situation observe hat. broaden strength results additionally experiment three common alternative setups. first consider incremental class learning scenario similar lopez-paz ranzato using class subsets cifar cifar data. setup best baseline scores next consider permuted mnist sequence tasks setup best result could literature scores finally also consider split mnist task setup best result literature corresponds conceptor-aided backpropagation approach scores detail setups results found appendix machine learning algorithm important assess sensitivity respect hyperparameters. stability parameter smax compressibility parameter smax provides plasticity units capacity adaptation network easily forget learned. high smax prevents forgetting network difﬁculties adapting tasks. allows almost network’s capacity given task potentially spending much current task. high forces learn compact model expense reaching accuracy original network could reached. empirically found good operation ranges smax variation within ranges results reasonable performance unless stated otherwise smax interesting note hard attention mechanism introduced sec. offers number possibilities monitor behavior models. instance computing conditioning mask hard attention vectors assess weights obtain high attention value binarize compute estimate instantaneous network capacity usage also inform amount active weights layer task another facet monitor weight reuse across tasks. similar procedure comparing conditioning masks tasks asses percentage weights task later reused task task comparing numbers compression rates used packnet generally uses much compact model. comparing speciﬁc mnist cifar tasks observe compresses respectively. interestingly contrast majority network pruning approaches learns prune network weights backpropagation time network weights themselves. introduce hard attention mechanism that focusing task embedding able protect information previous tasks learning tasks. hard attention mechanism lightweight sense adds small fraction weights base network trained together main model negligible overhead using backpropagation vanilla sgd. demonstrate effectiveness approach control catastrophic forgetting image classiﬁcation context running series experiments multiple data sets state-of-the-art approaches. hyperparameters intuitively refer stability compactness learned knowledge whose tuning demonstrate crucial obtaining good performance. addition offers possibility monitor used network capacity across tasks layers unit reuse across tasks compressibility model trained given task. hope approach also useful online learning network compression contexts hard attention mechanism presented also applicability beyond catastrophic forgetting problem. assess network’s weights important prune irrelevant ones compress network deployment low-resource devices time-constrained environments want focus compression task higher value used catastrophic forgetting start positive random initialization embeddings former promote compression latter ensure start learning model putting attention weights ﬁrst epochs empirically found using yields reasonable trade-off accuracy compression single task that compress network sizes original size depending kemelmacher-shlizerman seitz miller brossard megaface benchmark million faces recognition scale. proc. ieee conf. computer vision pattern recognition kirkpatrick pascanu rabinowitz veness desjardins rusu milan quan ramalho grabska-barwinska hassabis clopath kumaran hadsell overcoming catastrophic forgetting neural networks. proc. national academy sciences krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural networks. pereira burges bottou weinberger advances neural information processing systems volume french using semi-distributed representations overcome catastrophic forgetting connectionist netproc. annual conf. cognitive works. science society goodfellow mizra courville bengio empirical investigation catastrophic forgetting gradient-based neural networks. proc. int. conf. learning representations dally deep compression compressing deep neural networks pruning trained quantization huffman coding. proc. int. conf. learning representations s.-w. j.-h. j.-w. zhang b.-t. overcoming catastrophic forgetting incremental moment matching. guyon luxburg bengio wallach fergus vishwanathan garnett advances neural information processing systems volume curran associates inc. lopez-paz ranzato gradient episodic memory continuum learning. guyon luxburg bengio wallach fergus vishwanathan garnett advances neural information processing systems volume curran associates inc. netzer wang coates bissacco reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning paszke gross chintala chanan yang devito desmaison antiga lerer automatic differentiation pytorch. nips workshop future gradient-based machine learning software techniques shin continual learning deep generative replay. guyon luxburg bengio wallach fergus vishwanathan garnett advances neural information processing systems volume curran associates inc. sprechmann jayakumar pritzel puigdom`enech uria vinyals hassabis pascanu blundell memory-based parameter adaptation. proc. int. conf. learning representations srivastava masci kazerounian gomez schmidhuber compete compute. burges bottou welling ghahramani weinberger advances neural information processing systems volume curran associates inc. stallkamp schlipsing salmen igel german trafﬁc sign recognition benchmark multi-class classiﬁcation competition. proc. int. joint conf. neural networks data sets used experiments summarized table mnist data comprises monochromatic images handwritten digits. fashion-mnist comprises gray-scale images size zalando’s articles. german trafﬁc sign data contains trafﬁc sign images. used version data udacity self-driving github repository. notmnist data comprises glyphs extracted publicly available fonts making similar data mnist; need resize images. svhn data comprises digits cropped house numbers google street view images. facescrub data widely used face recognition tasks images listed original data hosted anymore corresponding internet domains version data stored megaface challenge website select ﬁrst people appearances. cifar cifar data sets contain color images match image input shape required experiments images corresponding data sets need resized padded zeros addition data sets comprising monochromatic images replicate image across channels. note perform sort data augmentation; adapt inputs. provide necessary code perform adaptations links listed above. https//github.com/zalandoresearch/fashion-mnist https//github.com/georgesung/traffic_sign_classification_german code attached ﬁle. http//megaface.cs.washington.edu/participate/challenge.html code attached ﬁle. table wall-clock training time measured single nvidia pascal titan total epoch batch batch processing time measured forward pass forward backward pass additional experiment complement evaluation consider incremental cifar setup following similar approach lopez-paz ranzato divide cifar cifar data sets consecutive-class subsets tasks presented random order according seed. take groups classes cifar classes cifar yielding total tasks. decide take groups classes order similar number training instances task. rest procedure main paper. important results summarized there. complete numbers depicted fig. reported table common experiment proposed srivastava later employed evaluate catastrophic forgetting goodfellow consists taking random permutations pixels mnist data tasks. typically average accuracy sequentially training mnist permutations reported. match different number table accuracy permuted mnist task taking average training tasks. exception generative replay approach whose performance assessed tasks. superscripts indicate results reported nguyen jaeger asterisk parameter count indicates approach presents additional structure included parameter count parameters used literature consider small medium large network based two-layer fully-connected architecture zenke hidden units respectively. large network dropout probabilities kirkpatrick smax small network smax medium large networks. results available table another popular experiment split mnist data tasks report average accuracy learning other. follow splitting data using labels tasks running experiment times. also match base network architecture used train epochs results reported table preliminary experiments observed dropout could increase accuracy percentage. however keep conﬁguration cited reference ﬁnally section want mention number alternatives experimented development hat. purpose section report formal results inform reader potential different choices implementing variations give intuition outcome choices. realized embedding weights changing much gradients small compared rest network introduced annealing initially tackled issue using different learning rate embeddings. that empirically found factors times original learning rate able tackle issue leading performances almost good ﬁnal ones reported main paper. however different learning rate introduced additional parameter could conceptually relate catastrophic forgetting could tricky tune generic setting. also studied adaptive optimizer adagrad adam embedding weights. idea adaptive optimizer would able automatically introduce appropriate scaling factor. found option effectively learning suitable values however performance worse constant-factor boost explained above. noticeably introducing adaptive optimizer also introduces number hyperparameters type optimizer another learning rate weight decays etc. rationale ﬁrst expression starts sigmoid equivalent straight line degrees then increasing linearly increases angle towards degrees second expression parametric evolution ﬁrst one. annealing schedules feature maximum inﬁnite yielding true step function inference time. therefore obtain truly binary attention vectors forgetting. addition ﬁrst expression able remove smax hyperparameter. nonetheless found ﬁrst expression perform worse solution proposed main paper. introduction second expression improved situation results still good ones main paper tuning tricky. deﬁnes ‘valid’ range input gate. gate yields much simpler formulation gradient compensation described main paper. however implies need could considered hyperparameter. also implies embedding values away step transition point receive proportionally similar gradient ones close values close treated equal ones still undecided test alternative gate quantitatively. accumulating attention across tasks soon dismissed ﬁnal max-based formula. previous equation could interesting online learning scenarios limited model capacity together embedding initialization experiments using uniform initialization embeddings instead gaussian also experimented idea behind alternative initializations that sufﬁciently large smax almost start value effect distributing attention units time beginning training. using values yielded competitive results worse ones using intuition uniform initialization like better purely compressive approach used last experiment main paper. results small percentage lower ones attention-weighted regularization main paper. also exchanged previous regularization l-based regularization mentioned main paper attention mask used input good strategy general image classiﬁcation problem ﬁrst-layer convolutional ﬁlters particular. however input consists independent isolated features think putting hard attention input kind supervised feature selection process. performed number experiments using fully-connected layers mnist data above introduced additional hard attention vectors directly multiplied input network. results suggested could potentially viable option feature selection data compression writing ﬁrst version paper realized idea binary mask affects given unit could potentially traced back inhibitory synapses mcculloch pitts idea inhibitory synapses quite unconventional rarely seen today best knowledge speciﬁc learning inputs speciﬁc function proposed. weight-based binary masks implicitly explicitly used many catastrophic forgetting approaches least rusu fernando mallya lazebnik nguyen yoon different learns unit-based hard attention vectors possible binary values.", "year": 2018}