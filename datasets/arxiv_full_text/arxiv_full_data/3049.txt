{"title": "Ensembles of Multiple Models and Architectures for Robust Brain Tumour  Segmentation", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "Deep learning approaches such as convolutional neural nets have consistently outperformed previous methods on challenging tasks such as dense, semantic segmentation. However, the various proposed networks perform differently, with behaviour largely influenced by architectural choices and training settings. This paper explores Ensembles of Multiple Models and Architectures (EMMA) for robust performance through aggregation of predictions from a wide range of methods. The approach reduces the influence of the meta-parameters of individual models and the risk of overfitting the configuration to a particular database. EMMA can be seen as an unbiased, generic deep learning model which is shown to yield excellent performance, winning the first position in the BRATS 2017 competition among 50+ participating teams.", "text": "abstract. deep learning approaches convolutional neural nets consistently outperformed previous methods challenging tasks dense semantic segmentation. however various proposed networks perform diﬀerently behaviour largely inﬂuenced architectural choices training settings. paper explores ensembles multiple models architectures robust performance aggregation predictions wide range methods. approach reduces inﬂuence meta-parameters individual models risk overﬁtting conﬁguration particular database. emma seen unbiased generic deep learning model shown yield excellent performance winning ﬁrst position brats competition among participating teams. brain tumours among fatal types cancer tumours originally develop brain gliomas frequent arise glioma cells depending aggressiveness broadly categorized high grade gliomas high grade gliomas develop rapidly aggressively forming abnormal vessels often necrotic core accompanied surrounding oedema swelling malignant high mortality average survival rate less years even treatment grade gliomas benign malignant grow slower recur evolve thus treatment warranted. treatment patients undergo radiotherapy chemotherapy surgery firstly diagnosis monitoring tumour’s progression treatment planning afterwards assessing eﬀect treatment various neuroimaging protocols employed. magnetic resonance imaging widely used clinical routine research studies. facilitates tumour analysis allowing estimation extent location investigation subcomponents however requires accurate delineation tumour proves challenging complex structure appearance nature images multiple sequences need consulted parallel informed judgement. factors make manual delineation time-consuming subject interintra-rater variability fig. left right flair; manual annotation brats’ subject yellow depicts oedema surrounding tumour core; conﬁdence predicting oedema trained cross-entropy loss. although overall performance similar training loss alters cnn’s behaviour tends output highly conﬁdent predictions even false. automatic segmentation systems providing objective scalable solution. representative early works atlas-based outlier detection method joint segmentation-registration framework often guided tumour growth model past years rapid developments machine learning methods random forests among successful recently convolutional neural networks gained popularity exhibiting promising results segmentation brain tumours variety architectures proposed presenting diﬀerent strengths weaknesses. additionally networks vast number meta parameters. multiple conﬁguration choices system inﬂuence performance also behaviour instance diﬀerent models perform better diﬀerent types pre-processing. consequently investigating behaviour given task ﬁndings biased. finally conﬁguration highly optimized given database over-ﬁt generalise data tasks. work push towards constructing reliable objective deep learning model. bring together variety architectures conﬁgured trained diverse ways order introduce high variance them. combining them construct ensemble multiple models architectures averaging away variance modelconﬁguration-speciﬁc behaviours. approach leads system robust unpredictable failures independent components enables objective analysis generic deep learning model unbiased behaviour introduces perspective ensembling objectiveness. contrast common ensembles single model trained small variations initial seeds renders ensemble biased main architectural choices. ﬁrst milestone endeavour evaluated emma brain tumour segmentation challenge method ﬁrst position ﬁnal testing stage among competing teams. indicates reliability approach paves analysis. feedforward neural networks shown capable approximating function thus models zero bias possible systematic error. however panacea. left unregularized overﬁt noise training data leads mistakes called generalise. coupled stochasticity optimization process multiple local minima leads unpredictable inconsistent errors diﬀerent instances. constitutes models high variance. regularization reduces variance increases bias expressed bias/variance dilemma regularization explicit weight decay prevents networks learning rare noisy patterns implicit local connectivity kernels however allow model learn patterns larger receptive ﬁeld. architectural conﬁguration choices thus introduce bias altering behaviour network. route address bias/variance dilemma ensembling. combining multiple models ensembling seeks create higher performing model variance. popular combination rule averaging sensitive inconsistent errors singletons commonly instances network trained diﬀerent initial weights multiple ﬁnal local minima ensembled majority correcting irregular errors. intuitively inconsistent errors averaged out. lack consistent failures interpreted statistical independency. thus methods de-correlating instances developed. popular bagging commonly used random forests. uses bootstrap sampling learn less correlated instances diﬀerent subsets data. works often discuss ensembling means increasing performance. approached high variance scope unreliability. discussed ensembling type n-version programming advocates reliability redundancy. producing n-versions program versions fail independently majority voting behave reliable system. formalize intuitive requirements reliability target function covered ensemble majority correct. turn advocates diversity independence overall quality components. biomedical applications reliability-critical high variance would deter neural networks. reason investigate robustness diverse ensembles. diverting works introduce another perspective ensembling creating objective conﬁguration-invariant model facilitate objective analysis. variety architectures shown promising results recent literature. regarding architectures commonly diﬀer depth number ﬁlters process multi-scale context among others. architectural choices fig. ensemble diverse networks emma averages bias infused individual model conﬁgurations approximate reliably true posterior robust suboptimal conﬁgurations. posteriors left obtained multiple perceptrons trained classify clusters centred example diﬀerent losses regularizations noise training labels. ensemble provides reliable estimates. bias model behaviour. instance models large receptive ﬁelds show improved localisation capabilities less sensitive texture models emphasizing local information. strategies handle class imbalance another performance relevant parameter. common strategies training class-weighted sampling class-weighted cross entropy. analysed methods strongly inﬂuence sensitivity model class. furthermore choice loss function impacts results. example observed networks trained optimize intersection union dice similar losses tend give worse conﬁdence estimations trained cross entropy finally setting hyper-parameters optimization strongly aﬀect performance. often observed practitioners choice optimizer conﬁguration instance learning rate schedule make diﬀerence good segmentation. distance computed points given training data represents choice meta-parameters. commonly neglected although conditions learnt estimator. take account instead deﬁne stochastic variable space meta-parameter conﬁgurations corresponding prior order marginalizes away eﬀects note case single model conﬁgured derived above setting dirac prior thus ensemble relaxes pre-existing neglected strong prior. biasing eﬀect marginalized out. exposed limitations agree requirements ensembling mentioned sec. need restrict subspace area relatively high quality models need cover relatively small number models thus diversity key. remainder section describe main properties models used construct collection emma cover various contemporary architectures conﬁgured trained diﬀerent settings. model description ﬁrst architecture employ deepmedic originally presented fully multi-scale designed focus eﬃcient processing images. this employs parallel pathways take input down-sampled context avoiding convolve large volumes full resolution remain computationally cheap. although originally developed segmentating brain lesions found promising diverse tasks segmentation placenta making good component robust ensemble. include deepmedic models emma. ﬁrst residual version previously employed brats depicted fig. second wider variant double number feature maps layer. training details models trained extracting multi-scale image segments probability centred healthy tissue probability tumour proposed wider variant trained larger inputs width scales respectively. trained cross-entropy loss meta-parameters adopted original conﬁguration. model description integrate three fcns emma. schematic ﬁrst architecture depicted fig. second constructed larger replacing convolutional layer residual block convolutions. third also residual-based less down-sampling step. layers batch normalisation relus zero-padding. training details draw training patches width ﬁrst voxels residual-based fcns equal probability label. trained using adam. ﬁrst trained optimize loss dice used similarly two. model description employ versions u-net architecture ensemble. main elements ﬁrst architecture depicted fig. version follow strategy suggested reduce model complexity skip connections implemented summations signals up-sampling part network instead concatenation originally used. second architecture similar concatenates skip connections uses patches sampled within brain equal probability centred around voxel four labels. trained minimizing cross entropy adadelta adam respectively diﬀerent optimization regularization augmentation meta-parameters. models trained completely separately. testing time model segments individually unseen image outputs class-conﬁdence maps. models ensembled emma according this ensemble’s conﬁdence maps class created calculating voxel average conﬁdence individual models voxel belong class. ﬁnal segmentation made emma performed assigning voxel class highest conﬁdence. original implementation deepmedic used corresponding models along default meta-parameters publicly available https// biomedia.doc.ic.ac.uk/software/deepmedic/. fcns implemented using dltk deep learning library focus medical imaging applications allowed quick implementation experimentation finally adaptation unet released https// gitlab.com/eferrante. system evaluated data brain tumour segmentation challenge training consists cases high grade glioma cases grade glioma manual segmentations provided. segmentations include following tumour tissue labels necrotic core enhancing tumour oedema enhancing core. label used. validation consists cases grade revealed. reference segmentations validation hidden evaluation carried online system allows multiple submissions. testing phase competition test cases provided teams teams hours window single submission system. evaluation predicted labels merged diﬀerent sets whole tumour core enhancing tumour subject four sequences available flair contrast enhanced datasets pre-processed organisers provided skull-stripped registered common space resampled isotropic resolution. dimensions volume experimented three diﬀerent versions intensity normalisation preprocessing z-score normalisation modality case individually mean stdev brain intensities. bias ﬁeld correction followed bias ﬁeld correction followed piece-wise linear normalisation followed preliminary comparisons inconclusive. instead chose average away normalisation’s eﬀect emma. three instances network trained data processed diﬀerent normalisation. applied correspondingly processed images inference results averaged emma provide results emma achieved validation testing brats’ challenge table system competition achieving overall best performance testing phase based dice score haussdorf distance. also show results achieved validation teams ranked next positions testing stage. testing-phase metrics available methods. note emma achieves similar levels performance validation test sets even though latter contains data diﬀerent sources indicating robustness method. comparison competing methods good validation manage retain levels testing set. emphasizes importance research towards robust reliable systems. table performance emma validation test sets brats system achieved segmentation performance testing stage competition. comparison show performance validation teams ranked next position. performance teams testing stage available fig. flair manual annotation case training along automatic segmentation preliminary version emma consisting models. green arrows point inconsistent mistakes individual model corrected ensembling arrow shows consistent mistake. neural networks proven potent imperfect estimators often making unpredictable errors. biomedical applications reliability-critical however. reason ﬁrst concentrate improving robustness. towards goal introduced emma ensemble widely varying cnns. combining heterogeneous collection networks construct model insensitive independent failures components thus generalises well also introduced perspective ensembling objectiveness. marginalizing ensembling biased behaviour introduced conﬁguration choices emma model objective analysis. even though individual networks straight-forward architectures optimized task emma ﬁrst position ﬁnal testing stage brats competition among teams indicating strong generalisation. robust suboptimal conﬁgurations components emma oﬀer re-usability diﬀerent tasks explore future. emma could also useful unbiased investigation factors sensitivity cnns diﬀerent sources domain shift strongly aﬀecting large-scale studies estimating amount training data required task. finally emma’s uncertainty could serve objective measure type patients tumours challenging learn. work supported epsrc partially funded framework programme european commission supported presidents scholarship imperial college london. beneﬁciary research fund postdoctoral grant. supported microsoft research scholarship programme epsrc centre doctoral training high performance embedded distributed systems gratefully acknowledge support nvidia donation gpus research.", "year": 2017}