{"title": "Domain and Function: A Dual-Space Model of Semantic Relations and  Compositions", "tag": ["cs.CL", "cs.AI", "cs.LG", "H.3.1; I.2.6; I.2.7"], "abstract": "Given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). Likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). It seems that these two tasks, recognizing relations and compositions, are closely connected. However, up to now, the best models for relations are significantly different from the best models for compositions. In this paper, we introduce a dual-space model that unifies these two tasks. This model matches the performance of the best previous models for relations and compositions. The dual-space model consists of a space for measuring domain similarity and a space for measuring function similarity. Carpenter and wood share the same domain, the domain of carpentry. Mason and stone share the same domain, the domain of masonry. Carpenter and mason share the same function, the function of artisans. Wood and stone share the same function, the function of materials. In the composition dog house, kennel has some domain overlap with both dog and house (the domains of pets and buildings). The function of kennel is similar to the function of house (the function of shelters). By combining domain and function similarities in various ways, we can model relations, compositions, and other aspects of semantics.", "text": "given appropriate representations semantic relations carpenter wood mason stone suitable algorithm able recognize relations highly similar likewise representations house kennel algorithm able recognize semantic composition house house highly similar kennel seems tasks recognizing relations compositions closely connected. however best models relations signiﬁcantly diﬀerent best models compositions. paper introduce dual-space model uniﬁes tasks. model matches performance best previous models relations compositions. dual-space model consists space measuring domain similarity space measuring function similarity. carpenter wood share domain domain carpentry. mason stone share domain domain masonry. carpenter mason share function function artisans. wood stone share function function materials. composition house kennel domain overlap house function kennel similar function house combining domain function similarities various ways model relations compositions aspects semantics. distributional hypothesis words occur similar contexts tend similar meanings many vector space models semantics word–context matrix represent distribution words contexts capturing intuition behind distributional hypothesis vsms achieved impressive results level individual words clear extend level phrases sentences beyond. example know represent house vectors represent house approach representing house treat unit handle individual words. call holistic noncompositional approach representing phrases. holistic approach suitable phrases scale vocabulary individual words two-word phrases threeword phrases even large corpus text possible phrases never appear corpus. people continually inventing phrases able understand phrases although never heard before; able infer meaning phrase composition meanings component words. scaling problem could viewed issue data sparsity better think problem linguistic creativity master natural language algorithms must able represent phrases composing representations individual words. cannot treat n-grams treat unigrams hand holistic approach ideal idiomatic expressions meaning cannot inferred component words. creativity novelty natural language require take compositional approach majority n-grams encounter. suppose vector representations house. compose representations represent house strategy represent house average vectors house simple proposal actually works limited degree however boat house house boat would represented average vector diﬀerent meanings. composition averaging deal order sensitivity phrase meaning. landauer estimates meaning english text comes word choice remaining comes word order. similar issues arise representation semantic relations. given vectors carpenter wood represent semantic relations carpenter wood treat carpenter wood unit search paraphrases relations carpenter wood large corpus could phrases carpenter wood carpenter used wood wood carpenter. variation holistic approach enable recognize semantic relations carpenter wood highly similar relations mason stone. however holistic approach semantic relations suﬀers data sparsity linguistic creativity problems holistic approach semantic composition. could represent relation carpenter wood averaging vectors. might enable recognize carpenter wood mason stone would incorrectly suggest carpenter wood stone mason. problem order sensitivity arises semantic relations arose semantic composition. many ideas proposed composing vectors pad´o point problems common several proposals. first often adaptive capacity represent variety possible syntactic relations phrase. example phrase horse draws horse subject verb draws whereas object verb phrase draws horse. composition vectors horse draws must able adapt variety syntactic contexts order properly model given phrases. second single vector weak handle long phrase sentence document. single vector encode ﬁxed amount structural information dimensionality ﬁxed upper limit sentence length hence amount structure encoded ﬁxed dimensionality allow information scalability. simple averaging vectors lacks adaptive capacity treats kinds composition way; ﬂexibility represent diﬀerent modes composition. good model must capacity adapt diﬀerent situations. information scalability means size semantic representations grow proportion amount information representing. size representation ﬁxed eventually information loss. hand size representations grow exponentially. case problem information scalability arises approaches multiple vectors single vector. example represent house adding vectors house information loss. increase number vectors mapped single vector eventually reach point single vector longer contain information multiple vectors. problem avoided multiple vectors single vector. suppose k-dimensional vector ﬂoating point elements bits each. vector hold bits information. even allow grow ﬁxed eventually information loss. vector space model semantics vectors resistance noise. perturb vector noise threshold signiﬁcant change meaning represents. therefore think vector hypersphere radius rather point. also bounds range values elements vector. ﬁnite number hyperspheres radius packed bounded k-dimensional space according information theory ﬁnite messages need bits encode message. likewise ﬁnite vectors vector represents bits information. therefore information capacity single vector bounded k-dimensional space limited bits. past work suggests recognizing relations compositions closely connected tasks goal research uniﬁed model handle compositions relations also resolving issues linguistic creativity order sensitivity adaptive capacity information scalability. considerations dual-space model consisting domain space measuring domain similarity function space measuring function similarity analogy relatively high domain similarity relatively high domain similarity hand relatively high function similarity relatively high function similarity combining domain function similarity appropriate ways models vectors normalized unit length elements must within range element outside range length vector greater one. general ﬂoating point representations minimum maximum values. semantic composition appropriate combine similarities depend syntax composition. let’s focus noun-modiﬁer composition example. noun-modiﬁer phrase head noun modiﬁed adjective noun suppose word synonymous functional role noun-modiﬁer phrase determined head noun relatively high degree function similarity high degree domain similarity combining domain function similarity recognize brain doctor synonymous neurologist. brieﬂy proposal compose similarity measures instead composing vectors. apply various mathematical functions combine cosine similarity measures instead applying functions directly vectors. addresses information loss problem preserve vectors individual component words. since diﬀerent spaces also ﬂexibility address problem adaptive capacity. model compositional resolves linguistic creativity problem. deal order sensitivity combining similarity measures ways recognize eﬀects word order. might argued present model semantic composition compare words form phrases order derive measure similarity phrases. example section derive measure similarity phrases environment secretary defence minister actually provide representation phrase environment secretary. hand past work problem semantic composition yields representation composite phrase environment secretary diﬀerent union representations component words environment secretary. argument based assumption goal semantic composition create single general-purpose stand-alone representation phrase composite distinct union representations component words. assumption necessary approach assumption. believe assumption held back progress problem semantic composition. argue present model semantic composition composition similarities composition vectors. vectors represent individual words similarities inherently represent relations things. composing vectors yield stand-alone representation phrase composing similarities necessarily yields linking structure connects phrase phrases. similarity composition result stand-alone representation phrase practical applications require stand-alone representations. whatever practical tasks performed stand-alone representations phrases believe performed equally well similarity composition. discuss issue depth section next section surveys related work modeling semantic composition semantic relations. section describes build domain function space. test hypothesis value separate spaces also create mono space merger domain function spaces. present four sets experiments dual-space model section evaluate dual-space approach multiple-choice analogy questions multiple-choice nounmodiﬁer composition questions derived wordnet phrase similarity rating problems similarity versus association problems discuss experimental results section section considers theoretical questions dual-space model. limitations model examined section section concludes. paper assumes familiarity vector space models semantics. overview semantic vsms papers handbook latent semantic analysis review mitchell lapata’s paper survey turney pantel examine related work semantic composition relations. introduction mentioned four problems semantic models yield four desiderata semantic model adaptive capacity phrases model ﬂexibility represent diﬀerent kinds syntactic relations. word pairs model ﬂexibility handle variety tasks measuring degree relational similarity pairs versus measuring degree phrasal similarity pairs information scalability phrases model scale neither loss information exponential growth representation size number component words phrases increases. n-ary semantic relations model scale neither loss information exponential growth representation size number terms relations increases. phrase noun-modiﬁer phrase assume vectors represent component words earliest proposals semantic composition represent vector average using cosine measure vector similarity taking average vectors adding vectors a+b. vector addition works relatively well practice although lacks order sensitivity adaptive capacity information scalability. regarding order sensitivity adaptive capacity mitchell lapata suggest using weights αa+βb tuning weights diﬀerent values diﬀerent syntactic relations. experiments weighted addition performed better unweighted addition. kintsch proposes variation additive composition words given vocabulary neighbours chosen manner attempts address order sensitivity adaptive capacity still problem information scalability ﬁxed dimensionality. utsumi presents similar model diﬀerent selecting neighbours. mitchell lapata found simple additive model peformed better additive model included neighbours. mitchell lapata suggest element-wise multiplication composition operation like vector addition element-wise multiplication suﬀers lack order sensitivity adaptive capacity information scalability. nonetheless experimental evaluation seven compositional models noncompositional models element-wise multiplication best performance another approach tensor product composition outer product outer product vectors elements matrix outer product three vectors third-order tensor. results information scalability problem representations grow exponentially large phrases grow longer. furthermore outer product perform well element-wise multiplication mitchell lapata’s experiments. recent work tensor products attempted address issue information scalability. circular convolution similar outer product outer product matrix compressed back vector avoids information explosion results information loss. circular convolution performed poorly mitchell lapata’s experiments. baroni zamparelli guevara suggest another model composition adjective-noun phrases. core strategy share holistic vectors train compositional model. partial least squares regression learn linear model maps vectors component nouns adjectives linear approximations holistic vectors phrases. linguistic creativity problem avoided linear model needs holistic vectors training; need holistic vectors plausible adjective-noun phrases. given phrase training data linear model predicts holistic vector phrase given ways avoid exponential growth; example third-order tensor rank three modes compactly encoded three component vectors. kolda bader discuss compact tensor representations. application semantic composition measuring similarity phrases kernel methods applied closely related task identifying paraphrases emphasis kernel methods syntactic similarity rather semantic similarity. neural network models combined vector space models task language modeling impressive results. goal language model estimate probability phrase decide several phrases likely. vsms improve probability estimates language model measuring similarity words phrases smoothing probabilities groups similar words. however language model words considered similar degree exchanged without altering probability given phrase without regard whether exchange alters meaning phrase. like function similarity measures degree words similar functional roles language models missing anything like domain similarity. pad´o present model similar parts vector space measuring similarity model selectional preferences. vector space similar domain space model selectional preferences plays role similar function space. individual word represented triple consisting word’s vector selectional preferences inverse selectional preferences phrase represented pair triples b′i. triple modiﬁed form triple represents individual word modiﬁcations adjust representation model meaning altered relation phrase likewise triple modiﬁed form triple represents takes account aﬀects transformed represent inﬂuence meaning vector transformed vector vector represents typical words consistent selectional preferences vector composition pad´o element-wise multiplication composition intention make like typical vector would expected phrase likewise pad´o’s model related models address linguistic creativity order sensitivity adaptive capacity information scalability suitable measuring similarity semantic relations. consider analogy traﬃc street water riverbed. represent traﬃc street represent water riverbed. transformation reinforces connection traﬃc street water riverbed help recognize relational similarity traﬃc street water riverbed. course models designed relational similarity surprising. however goal uniﬁed model handle compositions relations. semantic relations make general observations order sensitivity. word pairs simr measure degree similarity relations good analogy simr relatively high value. general good model relational similarity respect following equalities inequalities example given carpenter wood mason stone make good analogy follows equation wood carpenter stone mason make equally good analogy. also according equation mason stone carpenter wood make good analogy. hand suggested equation carpenter wood analogous stone mason. likewise indicated equation poor analogy assert carpenter stone mason wood. rosario hearst present algorithm classifying word pairs according semantic relations. lexical hierarchy word pairs feature vectors. classiﬁcation scheme implicitly tell something similarity. word pairs semantic relation class implicitly relationally similar word pairs diﬀerent classes. consider relational similarity implied rosario hearst’s algorithm problem order sensitivity equation violated. simh measure degree hierarchical similarity words simh relatively high share common hypernym relatively close given lexical hierarchy. essence intuition behind rosario hearst’s algorithm simh simh high simr also high. simh simh high enough assigned relation class. example consider analogy mason stone carpenter wood. common hypernym mason carpenter artisan; simh high. common hypernym stone wood material; hence simh high. seems good analogy indeed characterized high values simh simh. however symmetry simh leads problem. simh high simh must also high implies simr high. incorrectly conclude mason wood carpenter stone later work classifying semantic relations used diﬀerent algorithms underlying intuition hierarchical similarity similar intuition here since similarity function space closely related simf function similarity measured cosine vectors function space. simd domain similarity measured cosine vectors domain space. like past researchers look high values simf simf indicators simr high also look high values simd simd. continuing previous example conclude mason wood carpenter stone wood belong domain masonry stone belong domain carpentry. determiner hearst showed patterns form kind used infer hypernym pair–pattern matrix rows word pairs columns various patterns. turney littman bigham shnayder demonstrated pair–pattern used measure relational similarity. suppose pair-pattern matrix word pair corresponds vector corresponds approach measure relational similarity simr cosine ﬁrst patterns pair–pattern matrices generated hand later work used automatically generated patterns. authors used variations technique models suﬀer linguistic creativity problem. models noncompositional cannot scale handle huge number possible pairs. even largest corpus cannot contain pairs human speaker might daily conversation. turney attempted handle linguistic creativity problem within holistic model using synonyms. example corpus contain traﬃc street within certain window text perhaps might contain traﬃc road. contain water riverbed perhaps water channel. however best partial solution. turney’s algorithm required nine days process multiple choice analogy questions. using dual-space model without specifying advance word pairs might face answer questions seconds compositional models scale better holistic models. mangalath presented model semantic relations represents word pairs vectors abstract relational categories hyponymy meronymy taxonomy degree. approach construct kind second-order vector space elements vectors degrees similarity calculated cosines ﬁrst-order word–context matrix. instance carpenter wood represented second-order vector composed cosines calculated ﬁrst-order vectors. second-order vector value element corresponding meronymy would cosine ﬁrst-order vectors vector would ﬁrst-order vectors carpenter wood. vector would several vectors words related meronymy part whole component portion contains constituent segment. cosine would indicate degree carpenter wood related meronymy. mangalath al.’s model suﬀers information scalability order sensitivity problems. information loss takes place ﬁrst-order vectors summed also high-dimensional ﬁrst-order space reduced ten-dimensional second-order space. order sensitivity problem second-order vectors violate equation pairs represented second-order vector. natural proposal represent word pair would represent phrase whatever compositional model phrases could also applied word pairs. however problems compositional model order sensitivity information scalability carry word pairs. example represent violate equation section describe three vector space models. three spaces consist word– context matrices rows correspond words columns correspond contexts words occur. diﬀerences among three spaces kinds contexts. domain space uses nouns context function space uses verb-based patterns context mono space merger domain function contexts. mono space created order test hypothesis useful separate domain function spaces; mono space serves baseline. building three spaces involves series steps. three main steps substeps. ﬁrst last steps three spaces; diﬀerences spaces result diﬀerences second step. input corpus step collection pages gathered university websites webcrawler. corpus contains approximately words comes gigabytes plain text. facilitate ﬁnding term frequencies sample phrases indexed corpus wumpus search engine rows matrices selected terms wordnet lexicon. found selecting terms wordnet resulted subjectively higher quality simply selecting terms high corpus frequencies. step extract unique words phrases index.sense wordnet skipping n-grams contain numbers n-gram corpus frequencies querying wumpus n-gram. n-grams frequency least least characters candidate rows step selected n-gram query wumpus maximum phrases step phrases limited window words left n-gram words right total window size words. opennlp tokenize part-of-speech phrases tagged phrases come gigabytes. step generate contextual patterns part-of-speech tagged phrases. diﬀerent kinds patterns created three diﬀerent kinds spaces. details step given following subsections. phrase yield several patterns. three spaces rows maximum phrases several patterns phrase. result millions distinct patterns ﬁlter patterns steps select patterns shared largest number rows. given large number patterns ram. work limited linux sort command designed eﬃciently sort ﬁles large ram. make distinct patterns generated row. concatenate ﬁles corpus collected charles clarke university waterloo. wumpus available http//www.wumpus-search.org/. wordnet available http//wordnet.princeton.edu/. limit phrases n-gram required make wumpus tolerable amount time. finding phrases time-consuming step construction spaces. solid-state drive speed step. rows alphabetically sort patterns concatenated ﬁle. sorted identical patterns adjacent makes easy count number occurrences pattern. counting second sort operation yields ranked list patterns select possible candidate rows step might match patterns step rows would zeros matrix remove step finally output sparse frequency matrix rows columns. i-th corresponds n-gram j-th column corresponds contextual pattern value element number phrases containing generate pattern step svdlibc calculate singular value decomposition format output sparse matrix step chosen meet requirements svdlibc. step apply positive pointwise mutual information sparse frequency matrix variation pointwise mutual information values less zero replaced zero matrix results ppmi applied matrix number rows columns frequency matrix value element deﬁned follows deﬁnition estimated probability word occurs context estimated probability word estimated probability context statistically independent pi∗p∗j thus pmiij zero product pi∗p∗j would expect occurs pure random chance. hand interesting semantic relation expect larger would indepedent; hence pi∗p∗j thus pmiij positive. word unrelated context pmiij negative. ppmi designed give high value interesting semantic relation otherwise value zero indicating occurrence uninformative. finally step apply svdlibc decomposes product three matrices uσvt column orthonormal form diagonal matrix singular values rank also rank diagonal matrix formed singular values matrices produced selecting corresponding columns matrix ukσkvt matrix rank best approximates original matrix sense minimizes approximation errors. ukσkvt minimizes matrices rank denotes frobenius norm ﬁnal output three matrices form truncated ukσkvt domain space step tagged phrase generates contextual patterns. contextual patterns simply ﬁrst noun left given n-gram ﬁrst noun right since window size words side n-gram usually nouns sides n-gram. nouns either common nouns proper nouns. opennlp uses penn treebank tags include several diﬀerent categories noun tags. noun tags begin capital simply extract ﬁrst words left right n-gram tags begin extracted nouns converted lower case. noun appears sides n-gram contextual pattern generated. extracted patterns always unigrams; noun compound component noun closest n-gram extracted. table shows examples n-gram boat. note window words count punctuation number tokens window greater number words window. table vector n-gram boat frequency matrix nonzero values columns lake summer step drop rows zero left equal ppmi nonzero values matrix density table shows contextual patterns ﬁrst columns last columns count column table gives number rows generate pattern last patterns begin counts ties broken alphabetical order. building/vbg permit/nn anyway/rb we/prp should/md have/vb a/dt boat/nn next/jj summer/nn with/in skiing/nn and/cc tubing/nn paraphernalia/nns table contextual patterns ﬁrst last columns domain space. abbreviation chartered life underwriter terms condyle round bump bone forms joint another bone conocer spanish verb know sense acquainted person. concept function space function role word characterized syntactic context relates verbs occur near narrow window function space domain space based intuition proximity verb important determining functional role given word. distant verb less likely characterize function word. generate relatively complex patterns function space capture syntactic patterns connect given word nearby verbs. step tagged phrase generates contextual patterns. given tagged phrase ﬁrst step window tokens given n-gram tokens remaining tokens left n-gram punctuation punctuation everything left punctuation removed. remaining tokens right n-gram punctuation punctuation everything right punctuation removed. let’s call remaining tagged phrase truncated tagged phrase. simplify part-of-speech tags reducing ﬁrst character example various verb tags reduced truncated tagged phrase contains generates zero contextual patterns. phrase contains generate types contextual patterns general patterns speciﬁc patterns. general patterns verbs tags removed tokens reduced naked tags speciﬁc patterns verbs modals prepositions tags removed tokens reduced naked tags. general speciﬁc patterns left trim leading naked tags. right trim trailing naked tags. replace remaining naked tags sequence tags likely compound noun reduce sequence single given truncated tagged phrase patterns general pattern speciﬁc pattern. either patterns tokens left right sides make patterns duplicating splitting pattern point patterns verb drop thus three speciﬁc patterns three general patterns given truncated tagged phrase. speciﬁc general patterns same generated. contextual patterns function space complex patterns domain space. motivation greater complexity observation mere proximity enough determine functional roles although seems suﬃcient determining domains. example consider verb gives. word occurs near gives could subject direct object indirect object verb. determine functional role need know case applies. syntactic context connects gives provides information. contextual pattern gives implies subject gives implies object likely direct object gives suggests indirect object. modals prepositions supply information functional role context given verb. verb gives appears diﬀerent contextual patterns many vectors function space matrix correspond verbs. might seem surprising characterize function verb syntactic relation verbs consider example verb run. vector ppmi matrix function space nonzero values; characterized diﬀerent contextual patterns. note appearing contextual pattern diﬀerent nonzero value contextual pattern. character string word appears diﬀerent contextual patterns vector word nonzero values contextual patterns mono space simply merger domain space function space. step take union domain space columns function space columns resulting total columns. step total rows. mono matrix ppmi nonzero values yielding density values mono frequency matrix equal corresponding values domain function matrices. rows mono space matrix corresponding rows function space matrix. rows corresponding values zeros table summarizes three matrices. following four sets experiments three matrices cases; generate diﬀerent matrices experiments. three four sets experiments involve datasets used past researchers. made special eﬀort ensure words three datasets corresponding rows three matrices. intention three matrices adequate handle applications without special customization. cosine angle vectors inner product vectors normalized unit length. cosine ranges vectors point opposite directions point direction vectors orthogonal cosine zero. frequency vectors necessarily cannot negative elements cosine cannot negative weighting smoothing often introduce negative elements. ppmi weighting yield negative elements truncated generate negative elements even input matrix negative values. semantic similarity terms given cosine corresponding rows ukσp need set. parameter controls number latent factors parameter adjusts weights factors raising corresponding singular values power parameter well-known literature less familiar. suggested caron following experiments explore range values suppose take word list words descending order cosines using ukσp calculate cosines. high list cosines nearest neighbours decrease slowly. decrease quickly. high results broad fuzzy neighbourhood yields sharp crisp neighbourhood. parameter controls sharpness similarity measure. reduce running time svdlibc limit number singular values usually results less singular values. example domain space singular values. long greater experiment range values without rerunning svdlibc. generate ukσp simply deleting columns smallest singular values. experiments vary increments vary increments give weight factors smaller singular values; factors larger singular values weight. caron observes researchers either either ukσk. simf function similarity measured cosine vectors function space. simd domain similarity measured cosine vectors domain space. similarity measure combines simd simf four parameters tune domain space function space. space feasible explore combinations parameter values spaces combinations values. make search tractable initialize parameters middle ranges alternate tuning simd holding simf ﬁxed tuning simf holding simd ﬁxed. stop search improvement performance training data. almost cases local optimum found pass; tuned parameters once improvement tune second time. thus typically evaluate parameter values could standard numerical optimization algorithm tune four parameters algorithm takes advantage background knowledge optimization task. know small variations parameters make small changes performance need make ﬁne-grained search know simd simf relatively independent optimize separately. rows matrices based terms wordnet index.sense ﬁle. nouns singular forms verbs stem forms. calculate ﬁrst look exact matches terms correspond rows given matrix exact match found corresponding vector matrix. otherwise look alternate forms terms using validforms function wordnetquerydata perl interface wordnet. automatically converts plural nouns singular forms verbs stem forms. none alternate forms exact match matrix term zero vector length approach semantic relations compositions combine similarities simd simf various ways depending task hand syntax phrase hand. general want combined similarity high component similarities high want values component similarities balanced. achieve balance geometric mean combine similarities instead arithmetic mean. geometric mean suitable negative numbers cosine negative cases; hence deﬁne geometric mean zero component similarities negative successful approaches composition element-wise multiplication approach makes sense elements vectors negative. elements positive relatively large values reinforce other resulting large value makes intuitive sense. highly negative highly positive although intuition says highly negative. mitchell lapata designed word–context matrices ensure vectors negative elements. typically half positive half negative. element-wise multiplication baseline following experiments. fair baseline cannot simply apply element-wise multiplication vectors ukσp solution would ppmi matrix negative elements would allow element-wise multiplication take advantage smoothing eﬀect svd. solution vectors ukσkvt although ppmi matrix sparse ukσp vectors correspond terms vectors beneﬁt smoothing truncated elements almost positive. negative elements zero. apply element-wise multiplication vectors multiply vkσp− resulting vector c′vkσp− compared vectors matrix ukσp another deal element-wise multiplication would nonnegative matrix factorization instead svd. found implementation scales matrix sizes past experiments smaller matrices similar performance. section presents four sets experiments. ﬁrst experiments presents dualspace model semantic relations evaluates model multiple choice analogy questions sat. second presents model semantic composition evaluates multiple choice questions constructed wordnet. third applies dual-space model phrase similarity dataset mitchell lapata ﬁnal uses three classes word pairs chiarello test hypothesis dual-space model domain space function space capture intuitive concepts association similarity. evaluate dual-space model applied task measuring similarity semantic relations. multiple-choice analogy questions college entrance exam table gives example questions. task select choice word pair analogous stem word pair. intent measure function similarity across pairs. domain similarity inside pairs measured whereas domain similarity across pairs given sim. relational similarity simr simply function similarity subject constraint domain similarity inside pairs must less domain similarity across pairs sim. figure conveys main ideas behind equations want high function similarities measured sim. also prefer relatively high domain similarities contrast relatively domain similarities using example table lulling person trust analogous cajoling person compliance since functional role lull similar functional role cajole functional role trust similar functional role compliance captured sim. constraint implies domain similarities lull trust cajole compliance greater equal domain similarities lull compliance cajole trust. analogy mapping knowledge source domain target domain source domain mapped target domain play role source domain plays target domain. theory behind sim. source domain target recently came across rectangular structure lepage shin-ichi’s paper morphological analogy although algorithm task diﬀer considerably algorithm task lepage shin-ichi independently discovered underlying structure analogical reasoning. domain internal domain similarity internal domain similarity less cross-domain similarities. motivates constraint sim. deﬁnition natural expression gentner’s theory analogy. recall four equations introduced section repeat equations inspection show deﬁnition relational similarity equation satisﬁes requirements equations understood considering figure equation tells rotate figure vertical axis without altering network similarities symmetry ﬁgure. equation tells rotate figure horizontal axis without altering network similarities. another break symmetry equation satisﬁed would similarity measure inherently asymmetric skew divergence. equation symmetry broken natural considering domain function similarity apply analogies need introduce inherently asymmetric measure. also note symmetries equations desirable; wish break symmetries. would reasonable include simd simd decided leave out. seems function similarities simf simf high values good analogy might cause simd simd relatively high even though cross domains. people observe certain kind abstract function similarity frequently function similarity might become popular topic discussion could result high domain similarity. example carpenter wood analogous mason stone. domain carpenter wood carpentry domain mason stone masonry. functional role carpenter similar functional role mason artisans. although carpenter mason belong diﬀerent domains high degree abstract function similarity result discussions mention together discussions specialized trades skilled manual labour construction industry workplace injuries. words high function similarity words cause rise domain similarity. therefore include simd simd sim. choices question relational similarity zero skip question. ten-fold cross-validation parameters questions. parameter values selected nine folds parameters determined questions answered seconds. equation correctly answers questions skips questions incorrectly answers questions achieving accuracy comparison average score senior highschool students applying universities wiki lists many past results questions. table shows results time writing. table dual-space refers dualspace model using equation four past results achieved accuracy higher. four used holistic approaches hence able address issue linguistic creativity. best previous algorithm attains accuracy diﬀerence statistically signiﬁcant conﬁdence level according fisher’s exact test. majority algorithms table unsupervised dual-space pairclass bagpack limited supervision. pairclass bagpack answer given question learning binary classiﬁcation model speciﬁc given question. training given question consists positive training example stem pair question randomly selected pairs negative training examples. induced binary classiﬁer used assign probabilities choices probable choice guess. dual-space uses training tune four numerical parameters. three algorithms best described weakly supervised. sensitive dual-space model values parameters perform exhaustive grid searches coarse wide grid another narrow grid. point grids evaluate dual-space model using whole questions. narrow grid search centred parameter values selected nine folds previous experiment searches evaluate values parameter yielding total parameter settings. table shows values explored grid searches table presents minimum maximum average standard deviation accuracy searches. accuracy attained heuristic search ten-fold cross-validation near best accuracy grid search using whole questions evidence heuristic search eﬀective. accuracy coarse search varies demonstrates importance tuning parameters. hand accuracy search spans narrower range lower standard deviation suggests dualspace model overly sensitive relatively small variations parameter values; parameters reasonably stable. since domain space based nouns function space based verbs interesting know performance dual-space model varies diﬀerent parts speech. answer this manually labeled questions part-of-speech labels. labels single pair ambiguous labels become unambiguous context whole question. example lull trust could noun verb context table must verb noun. table splits results various parts speech. none diﬀerences table statistically signiﬁcant conﬁdence level according fisher’s exact test. larger varied questions needed determine part speech aﬀects dual-space model. parts speech nounnoun nounadjective adjectivenoun nounverb verbnoun adjectiveadjective verbadjective adjectiveverb verbverb verbadverb adverbverb verify hypothesis reformulated questions would test function domain comprehension. method ﬁrst expand choice pair including stem pair resulting full explicit analogy expanded choice generate another choice table shows reformulation table symmetry must assign similarity ten-choice test evaluates function domain similarities. lulltrustbalkfortitude lullfortitudebalktrust lullloyaltybetraytrust lulltrustbetrayloyalty lullcompliancecajoletrust lulltrustcajolecompliance lulldestinationhindertrust lulltrusthinderdestination lulltrustsoothepassion lullpassionsoothetrust lulltrustcajolecompliance task expanded ten-choice questions original ﬁve-choice questions select best analogy. solution table solution table except stem pair explicit table signﬁcant change distractors added choices. answer ten-choice questions selecting choice maximizes simr. ten-choice reformulated test simr attains accuracy whereas alone achieves diﬀerence statistically signiﬁcant conﬁdence level according fisher’s exact test. stringent test supports claim function similarity insuﬃcient itself. test value separate spaces single space simd simf equation model still four parameters tune matrix used similarities. best result accuracy ten-question reformulated test using function space simd simf signiﬁcantly accuracy dual-space model simd based domain space simf based function space dual-space model accurate modiﬁed models. signiﬁcant column indicates whether accuracy modiﬁed model signiﬁcantly less original dual-space model diﬃcult ten-choice questions clearly show value distinct spaces. algorithm dual-space modiﬁed dual-space modiﬁed dual-space modiﬁed dual-space modiﬁed dual-space modiﬁed dual-space modiﬁed dual-space dual-space modiﬁed dual-space modiﬁed dual-space modiﬁed dual-space modiﬁed dual-space modiﬁed dual-space modiﬁed dual-space accuracy signiﬁcant questions matrix simd matrix simf function space function space mono space domain space function space mono space domain space function space function space mono space domain space function space mono space domain space ﬁve-choice ﬁve-choice ﬁve-choice mono space ﬁve-choice ﬁve-choice ﬁve-choice ﬁve-choice ten-choice ten-choice ten-choice mono space ten-choice ten-choice ten-choice ten-choice table accuracy original ﬁve-choice questions reformulated ten-choice questions. modiﬁed models intentionally wrong matrix simd simf modiﬁed models show accuracy decreases space used. dual-space model performs well current state-of-the-art holistic model addresses issue linguistic creativity. results reformulated questions support claim value separate spaces. mentioned section task classifying word pairs according semantic relations closely connected problem measuring relational similarity. turney applied measure relational similarity relation classiﬁcation using cosine similarity measure nearness nearest neighbour supervised learning algorithm. dualspace model also suitable relation classiﬁcation nearest neighbour algorithm. second experiments apply dual-space model noun-modiﬁer compositions. given vectors house kennel would like able recognize house kennel synonymous. compare dual-space model holistic approach vector addition element-wise multiplication. approaches evaluated using multiple-choice questions automatically generated wordnet using wordnetquerydata perl interface wordnet. table gives example noun-modiﬁer questions. questions stem bigram choices unigrams. choice correct answer modiﬁer head noun. choice synonym hypernym modiﬁer synonym hypernym head noun. synonyms hypernyms found noun randomly chosen. last choices randomly selected nouns. choices either nouns adjectives choices must nouns. stem bigram choice unigrams must corresponding rows function space stem bigram must noun sense wordnet solution unigram must member synset ﬁrst noun sense stem bigram cannot simply hyphenation concatenation stem bigram. requirements result total seven-choice questions randomly split training testing. questions deliberately designed diﬃcult. particular approaches strongly attracted choices furthermore attempt ensure stem bigrams compositional; idiomatic expressions compositional approach could possibly right. want bias questions imposing theories distinguishing compositions idioms construction. represent noun-modiﬁer bigram represent unigram answer multiple-choice questions selecting unigram maximizes compositional similarity simc deﬁned follows thinking behind high domain similarity modiﬁer head noun furthermore function bigram determined head noun head noun high function similarity constraints tends high values sim. seems plausible humans constraints like this reason house cannot mean thing house extra word house would serve purpose; would meaningless noise. seven choices noun-modiﬁer question compositional similarity zero skip question. training best parameter settings testing equation correctly answers questions skips questions incorrectly answers yielding accuracy mitchell lapata compared many diﬀerent approaches semantic composition experiments considered task paper chosen compare smaller number approaches larger number tasks. include element-wise multiplication experiments approach best performance mitchell lapata’s experiments. vector spite constraints still worthwhile include head noun modiﬁer distractors multiple-choice questions enables experimentally evaluate impact distractors various algorithms constraints removed also future users dataset avoid distractors without explicit constraints. addition included historical importance simplicity. although mitchell lapata found weighted addition better unweighted addition include weighted addition experiments perform well element-wise multiplication mitchell lapata’s experiments. include holistic model noncompositional baseline. table dual-space refers dual-space model using equation holistic model represented corresponding vector given space. recall section that step rows matrices correspond n-grams wordnet greater one. thus example house corresponding vector three spaces. holistic model simply uses vector representation house. element-wise multiplication represented using equation vector addition model represented vectors normalized unit length added. four models constraints four models training data parameter tuning. diﬀerence dual-space model best variation elementwise multiplication statistically signiﬁcant conﬁdence level according fisher’s exact test. however diﬀerence dual-space model best variation vector addition signiﬁcant. three spaces holistic model signiﬁcantly better models inability address issue linguistic creativity major limitation. multiplechoice questions used experiments intentionally constructed requirement stem bigram must corresponding function space done could holistic model baseline; however gives misleading impression holistic model serious competitor compositional approaches. design table shows holistic model achieve ideal conditions. mitchell lapata’s dataset used experiments section illustrates limitations holistic model. dataset consists distinct pairs bigrams composed distinct bigrams. bigrams occur wordnet. pairs bigrams contain bigrams occur wordnet. given matrices holistic approach would reduced random guessing pairs mitchell lapata’s dataset. might argued failure holistic approach mitchell lapata’s dataset decision base rows matrices terms wordnet. however suppose attempt build holistic model frequent bigrams. -gram corpus includes list bigrams appeared times terabyte text total bigrams. using compositional approach matrices represent majority bigrams. hand holistic approach would require matrix rows considerably beyond current state art. possibility build matrix holistic approach needed given input n-grams instead building large static multipurpose matrix. problems idea. first slow. turney used approach analogy questions required nine days whereas dual-space model process questions seconds given static multipurpose matrix. second requires large corpus corpus size must grow exponentially length phrases. longer phrases rare larger corpora needed gather suﬃcient data model phrases. larger corpora also result longer processing times. given application wise predeﬁned list bigrams holistic representations would wise expect list suﬃcient cover bigrams would seen practice. creativity human language requires compositional models although holistic model included baseline experiments competitor models; supplement models. alone dropping constraints accuracy drops signﬁcantly however models beneﬁt greatly constraints. table take best variation model table look happens constraints dropped. holistic model models idiomatic bigrams testing questions. successful approaches determining whether multiword expression compositional noncompositional compare holistic vector representation compositional vector representation however approach suitable here want assume entirely idiomatic bigrams; instead would like estimate much idiomatic bigrams. wordnet contains clues indicators bigram might less compositional bigrams clue whether wordnet gloss bigram contains either head noun modiﬁer. example gloss house outbuilding serves shelter contains modiﬁer dog. suggests house compositional. classiﬁed testing questions head modiﬁer neither four classes approximately equally distributed testing questions match ﬁrst characters allow cases like brain surgeon gloss someone surgery nervous system bigram classiﬁed both ﬁrst characters surgeon match ﬁrst characters surgery. table shows accuracy models varies four classes questions. three compositional models neither class signiﬁcantly less accurate three classes diﬀerence signiﬁcant holistic model. three compositional models neither class less accurate classes. supports view signiﬁcant fraction wrong answers compositional models noncompositional bigrams. another clue compositionality wordnet whether head noun hypernym bigram. example surgeon hypernym brain surgeon. classiﬁed testing questions hyper testing questions hyper not. table gives accuracy models classes. table general pattern table three compositional models signiﬁcantly lower accuracy class decreases signiﬁcant diﬀerence holistic model. note vector addition element-wise multiplication lack order sensitivity equation sensitive order simc simc. impact reformulating noun-modiﬁer questions test order-sensitivity. first expand choice unigram including stem bigram resulting explicit comparison expanded choice generate another choice increases number choices seven fourteen. symmetry vector addition element-wise multiplication must assign similarity table compares dual-space model element-wise multiplication vector addition using reformulated fourteen-choice noun-modiﬁer questions. holistic model included table rows matrices reversed bigrams stricter test dual-space model signiﬁcantly accurate element-wise multiplication vector addition dual-space model perform well fourteen-choice questions need simd simf. drop simd equation ignoring modiﬁer paying attention head noun. accuracy drops drop simf equation equation becomes symmetrical similarity assigned reformulated fourteen-choice noun-modiﬁer questions dual-space signiﬁcantly better element-wise multiplication vector addition. original seven-choice questions diﬀerence large questions test order. unlike element-wise multiplication vector addition dual-space model addresses issue order sensitivity. unlike holistic model dual-space addresses issue linguistic creativity. subsection apply dual-space model measuring similarity phrases using mitchell lapata’s dataset human similarity ratings pairs phrases. dataset includes three types phrases adjective-noun noun-noun verb-object. pairs type pair phrases rated human subjects. ratings point scale signiﬁes lowest degree similarity signiﬁes highest degree. table gives examples. equation based instructions human participants imply function domain similarity must high phrase pair high similarity rating. figure illustrates reasoning behind equation. want high domain function similarities corresponding components phrases coincidence modiﬁed dual-space models accuracy fourteenchoice questions. although aggregate accuracy same individual questions models typically select diﬀerent choices. certain circumstance particular case large number great majority evidence cost environment secretary defence minister action programme development plan city centre research work lift hand raise head satisfy demand emphasise need like people increase number mitchell lapata divided dataset development evaluation development ratings phrase pair evaluation ratings phrase pair. development evaluation sets contain phrase pairs judgments diﬀerent participants. thus rated phrase pairs development ratings evaluation set. challenging evaluation divide dataset phrase pairs rather participants. development phrase pairs ratings evaluation phrase pairs ratings each. three phrase types randomly select phrase pairs development mitchell lapata spearman’s rank correlation coeﬃcient evaluate performance various vector composition algorithms task emulating human similarity ratings. given phrase type phrase pairs divided groups pairs each. group evaluation people gave similarity ratings pairs given group. group pairs given diﬀerent group people. score algorithm given phrase type average three values three groups. people rating pairs group ratings. human ratings represented vector numbers. algorithm generates rating pair group yielding numbers. make algorithm’s ratings comparable human ratings algorithm’s ratings duplicated times yielding vector numbers. spearman’s calculated vectors ratings. phrase types values ratings value ratings. believe evaluation method underestimates performance algorithms. combining ratings diﬀerent people vector numbers allow correlation adapt diﬀerent biases. person gives consistently ratings another person gives consistently high ratings people ranking ranking matches algorithm’s ranking algorithm high score. fair evaluation score algorithm calculating value human participant given phrase type calculate average values participants. given phrase type phrase pairs divided groups pairs each. development randomly select phrase pairs groups leaves phrase pairs groups evaluation human participant’s ratings represented vector numbers. algorithm’s ratings also represented vector numbers. value calculated vectors numbers input. given phrase type algorithm’s score average values phrase types values ratings value ratings. table compares dual-space model vector addition element-wise multiplication. development tune parameters three approaches. vector addition represented represented similarity given cosine vectors. element-wise multiplication uses equation represent dual-space model uses equation average correlation dual-space model signiﬁcantly average correlation vector addition using function space element-wise multiplication mono space also signiﬁcantly vector addition using function space comment leave-one-out correlation subjects domain function space mono space domain space function space mono space domain space function space diﬀerence dual-space model element-wise multiplication mono space signﬁcant. average correlation algorithm based values calculate statistical signiﬁcance using paired t-test signiﬁcance level based pairs values. mitchell lapata’s dataset test order sensitivity. given phrase pair test order sensitivity adding pair assume pairs would given rating human participants. table show happens transformation applied examples table save space give examples participant number certain circumstance particular case certain circumstance case particular large number great majority large number majority great evidence cost evidence cost table gives results expanded dataset. stringent dataset dual-space model performs signiﬁcantly better vector addition vector multiplication. unlike element-wise multiplication vector addition dualspace model addresses issue order sensitivity. manually inspected pairs automatically rated found rating reasonable cases although cases could disputed. example original noun-noun pair charge interest rate generates pair charge rate interest original verb-object pair produce eﬀect achieve result generates pair produce eﬀect result achieve. seems natural tendency correct comment leave-one-out correlation subjects domain function space mono space domain space function space mono space domain space function space incorrectly ordered pairs minds assign higher ratings deserve. predict human ratings pairs would vary greatly depending instructions given human raters. instructions emphasized importance word order pairs would ratings. prediction supported results semeval task instructions raters emphasized importance word order wrongly ordered pairs received ratings. dataset test order sensitivity vector addition performs slightly better dual-space model. dataset tests order sensitivity dual-space model surpasses vector addition element-wise multiplication large margin. chiarello created dataset word pairs labeled similar-only associated-only similar+associated table shows examples dataset. labeled pairs created cognitive psychology experiments human subjects. experiments found evidence processing associated words engages left right hemispheres brain ways diﬀerent processing similar words. seems fundamental neurological diﬀerence types semantic relatedness. hypothesize similarity domain space simd measure degree words associated similarity function space simf measure degree words similar. test hypothesis deﬁne similar-only simso associated-only simao similar+associated simsa follows class label similar-only similar-only similar-only similar-only associated-only associated-only associated-only associated-only similar+associated similar+associated similar+associated similar+associated experiments three preceding subsections three sets parameter settings dual-space model. table shows parameter values. eﬀect three sets parameter setttings give three variations similarity measures simso simao simsa. evaluate three variations well correspond labels chiarello al.’s dataset. given similarity measure simso sort word pairs descending order similarities look pairs many desired label; case simso would like majority label similar-only. table shows percentage pairs desired labels three variations three similarity measures. note random guessing would yield since three classes pairs size. three sets parameter settings table displays high density desired labels tops sorted lists. density slowly decreases move lists. evidence three similarity measures capturing three classes chiarello another test hypothesis three similarity measures create feature vectors three elements word pair. word pair represented feature vector hsimso simao simsai. supervised learning ten-fold cross-validation classify feature vectors three classes chiarello learning algorithm logistic regression implemented weka. results summarized table results lend support hypothesis similarity domain space simd measure degree words associated similarity function space simf measure degree words similar. table similar-only seems sensitive parameter settings associatedsimilar+associated. hypothesize function similarity diﬃcult measure domain similarity. note construction function space complex construction domain space intuitively seems easier identify domain thing identify functional role. gentner’s work suggests children master domain similarity become competent function similarity. section used multiple-choice analogy questions evaluate dual-space model relational similarity simr. diﬀerence performance dual-space model best past result using holistic model statistically signiﬁcant. experiments reformulated version questions designed test order sensitivity supported hypothesis domain function space required. function space sensitive order merging spaces causes signiﬁcant drop performance. section automatically generated multiple-choice noun-modiﬁer composition questions wordnet evaluate dual-space model noun-modiﬁer compositional similarity simc. diﬀerence performance dual-space model state-of-the-art element-wise multiplication model statistically signiﬁcant. best performance obtained holistic model model address issue linguistic creativity. experiments suggest signiﬁcant fraction holistic model models noncompositional phrases. limitation element-wise multiplication model lack sensitivity order. experiments reformulated version questions designed test order sensitivitiy demonstrated statistically signiﬁcant advantage dual-space model element-wise multiplication vector addition models. section used mitchell lapata’s dataset pairs phrases evaluate dual-space model phrasal similarity simp. reformulated version dataset modiﬁed test order sensitivitiy showed statistically signiﬁcant advantage dual-space model element-wise multiplication vector addition models. section used chiarello al.’s dataset word pairs labeled similar-only associated-only similar+associated test hypothesis similarity domain space simd measure degree words associated similarity function space simf measure degree words similar. experimental results support hypothesis. interesting chiarello argue fundamental neurological diﬀerence people process kinds semantic relatedness. experiments support claim dual-space model address issues linguistic creativity order sensitivity adaptive capacity. furthermore dual-space model provides uniﬁed approach semantic relations semantic composition. results section suggest function similarity correspond kind taxonomical similarity often associated lexicons wordnet word pairs table labeled similar-only kinds words typically share common hypernym taxonomy. example tablebed share hypernym furniture. believe correct necessarily imply lexiconbased similarity measures would better corpus-based approach used here. various similarities section arguably relational similarity simr makes function similarity. itself function similarity achieves questions however best performance achieved questions using wordnet diﬀerence statistically signiﬁcant conﬁdence level based fisher’s exact test. consider analogy traﬃc street water riverbed. questions involves analogy traﬃc street stem pair water riverbed correct choice. simr function similarity make correct choice. recognize traﬃc water high degree function similarity; fact similarity used hydrodynamic models traﬃc however must climb wordnet hierachy entity shared hypernym traﬃc water. believe manually generated lexicon capture functional similarity discovered large corpus. dual-space model phrase stand-alone general-purpose representation composite phrase apart representations component words. composite meaning constructed context given task. example task measure similarity relation house relation bird nest compose meanings house task measure similarity phrase house word kennel compose meanings house another task measure similarity phrase house phrase canine shelter compose meanings house third composition construction explicitly ties together things compared depends nature comparison desired task performed. hypothesize single stand-alone task-independent representation constructed suitable purposes. noted introduction composition vectors result stand-alone representation phrase composing similarities necessarily yields linking structure connects phrase phrases. linking structures seen figures intuitively seems important part understand phrase connecting phrases. part understanding house connection kennel. dictionaries make kinds connections explicit. perspective idea explicit linking structure seems natural given making connnections among words phrases essential aspect meaning understanding. subsection present general scheme ties together various similarities deﬁned section scheme includes similarities chunks text arbitrary size. scheme encompasses phrasal similarity relational similarity compositional similarity. chunk text word. represent semantics matrices. vector vector domain space represents domain semantics word vector vector function space represents function semantics word keep notation simple parameters domain space function space implicit. assume vectors normalized unit length. note size representation scales linearly number words hence information scalability. large values inevitably duplicate words representation could easily compressed sublinear size without loss information. words input composition function cosines operate directly vectors contrast much work discussed section composition operation shifted representations similarity measure exact speciﬁcation depends task hand. sentences envision structure determined syntactic structures sentences. seen instance equation hci. case based cosines constraints expressed terms cosines simd simd similar analyses apply similarities sections similarities also instances equation although representations sizes linear functions numbers phrases size composition equation quadratic function numbers phrases however speciﬁc instances general equation less quadratic size possible limit growth function words option would treat words. would represented vectors similarities would calculated function domain spaces. another possibility would function words hints guide construction composition function function words would correspond vectors; instead would contribute determining linking structure connects given chunks text. ﬁrst option appears elegant choice options made empirically. section manually constructed functions combined similarity measures using intuition background knowledge. manual construction scale task comparing arbitrarily chosen sentences. however good reasons believing construction composition functions automated. turney presents algorithm solving analogical mapping problems analogy solar system rutherford-bohr model atom. given list terms solar system domain {planet attracts revolves gravity solar system mass} list terms atomic domain {revolves atom attracts electromagnetism nucleus charge electron} automatically generate one-to-one mapping domain other {solar system atom nucleus planet electron mass charge attracts attracts revolves revolves gravity electromagnetism}. twenty analogical mapping problems attains accuracy compared average human accuracy algorithm scores quality candidate analogical mapping composing similarities mapped terms. composition function addition individual component similarities holistic relational similarities. algorithm searches space possible mappings mapping maximizes composite similarity measure. analogical mapping treated argmax problem argument maximized mapping function. eﬀect output algorithm automically generated composition similarities. mapping structures found algorithm essentially linking structures figures believe variation turney’s algorithm could used automatically compose similarities dual-space model; example possible identify paraphrases using automatic similarity composition. proposal search composition maximizes composite similarity subject various constraints turney points analogical mapping could used align words sentences experimentally evaluate suggestion. recent work shown argmax problems solved eﬃciently eﬀectively framed monotone submodular function maximization problems. believe automatic composition similarities naturally framework would result highly scalable algorithms semantic composition. regarding information scalability dual-space model suﬀer information loss sizes representations grow lengths phrases grow. growth might quadratic exponential. questions automate composition similarities impact computational complexity scaling longer phrases evidence questions tractable. area future work experiment longer phrases sentences discussed section interesting topic research parsing might used constrain automatic search similarity composition functions. focused spaces domain function seems likely model spaces would yield better performance. currently experimenting quad-space model includes domain function quality manner spaces. preliminary results quad-space promising. quad-space seems related pustejovsky’s four-part qualia structure. another issue avoided morphology. discussed section used validforms function wordnetquerydata perl interface wordnet morphological variations words base forms. implies that example singular noun plural form semantic representation. certainly simpliﬁcation sophisticated model would diﬀerent representations diﬀerent morphological forms word. paper treated holistic model dual-space model competitors certain cases idiomatic expressions holistic approach required. likewise holistic approach limited inability handle linguistic creativity. considerations suggest holistic dual-space models must integrated. another topic future work. number design decisions made construction domain function space especially conversion phrases contextual patterns decisions guided intuitions. expect exploration experimental evaluation design space fruitful area future research. construction function space speciﬁc english. generalize readily indo-european languages languages present challenge. another topic future research. another question formal logic textual entailment integrated approach. dual-space model seems suitable recognizing paraphrases obvious handle entailment. generally focused various kinds similarity scale phrases sentences encounter truth falsity. g¨ardenfors argues spatial models bridge low-level connectionist models high-level symbolic models. claims spatial models best questions similarity symbolic models best questions truth. know join kinds models. goal research develop model uniﬁes semantic relations compositions also addressing linguistic creativity order sensitivity adaptive capacity information scalability. believe dual-space model achieves goal although certainly room improvement research. many kinds word–context matrices based various notions context; sahlgren gives good overview types context explored past work. novelty dual-space model includes distinct complementary word–context matrices work together synergistically. distinct spaces distinct similarity measures combined many diﬀerent ways. multiple similarity measures similarity composition becomes viable alternative vector composition. example instead multiplying vectors multiply similarities simsa simf results suggest fruitful look problems semantics. thanks george foster yair neuman david jurgens reviewers jair helpful comments earlier version paper. thanks charles clarke corpus used build three spaces stefan b¨uttcher wumpus creators wordnet making lexicon available developers opennlp doug rohde svdlibc mitchell mirella lapata sharing data answering questions evaluation methodology christine chiarello curt burgess lorie richards alma pollock making data available jason rennie wordnetquerydata perl interface wordnet developers perl data language.", "year": 2013}