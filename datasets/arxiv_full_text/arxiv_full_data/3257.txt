{"title": "Expanding the Family of Grassmannian Kernels: An Embedding Perspective", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Modeling videos and image-sets as linear subspaces has proven beneficial for many visual recognition tasks. However, it also incurs challenges arising from the fact that linear subspaces do not obey Euclidean geometry, but lie on a special type of Riemannian manifolds known as Grassmannian. To leverage the techniques developed for Euclidean spaces (e.g, support vector machines) with subspaces, several recent studies have proposed to embed the Grassmannian into a Hilbert space by making use of a positive definite kernel. Unfortunately, only two Grassmannian kernels are known, none of which -as we will show- is universal, which limits their ability to approximate a target function arbitrarily well. Here, we introduce several positive definite Grassmannian kernels, including universal ones, and demonstrate their superiority over previously-known kernels in various tasks, such as classification, clustering, sparse coding and hashing.", "text": "abstract. modeling videos image-sets linear subspaces proven beneﬁcial many visual recognition tasks. however also incurs challenges arising fact linear subspaces obey euclidean geometry special type riemannian manifolds known grassmannian. leverage techniques developed euclidean spaces subspaces several recent studies proposed embed grassmannian hilbert space making positive deﬁnite kernel. unfortunately grassmannian kernels known none showuniversal limits ability approximate target function arbitrarily well. here introduce several positive deﬁnite grassmannian kernels including universal ones demonstrate superiority previously-known kernels various tasks classiﬁcation clustering sparse coding hashing. paper introduces positive deﬁnite kernels embed grassmannians hilbert spaces familiar euclidean structure. nowadays linear subspaces core representation many visual recognition techniques. example several state-of-the-art video image-set matching methods model visual data subspaces linear subspaces also proven powerful representation many computer vision applications chromatic noise ﬁltering domain adaptation despite success linear subspaces suﬀer drawback cannot analyzed using euclidean geometry. indeed subspaces special type riemannian manifolds grassmann manifold nonlinear structure. consequence popular techniques developed euclidean spaces apply. recently problem addressed embedding grassmannian hilbert space. achieved either tangent space approximation manifold exploiting positive deﬁnite kernel function embed manifold reproducing nicta funded australian government represented department broadband communications digital economy well australian research council centre excellence program. kernel hilbert space either case existing euclidean technique applied embedded data since hilbert spaces obey euclidean geometry. recent studies however report superior results rkhs embedding ﬂattening manifold using tangent spaces intuitively attributed fact tangent space ﬁrst order approximation true geometry manifold whereas higher-dimensional rkhs capacity better capturing nonlinearity manifold. rkhs embeddings therefore seem preferable applicability limited fact positive deﬁnite grassmannian kernels known. indeed literature kernels introduced embed grassmannians rkhs binet-cauchy kernel projection kernel former homogeneous second order polynomial kernel latter linear kernel. simple polynomial kernels limited ability closely approximate arbitrary functions. contrast universal kernels provide much better generalization power paper introduce positive deﬁnite grassmannian kernels which among others includes universal grassmannian kernels. start perspective embeddings binet-cauchy projection kernels derived pl¨ucker embedding projection embedding. embeddings yield distance functions. exploit properties distances conjunction several theorems analyzing positive deﬁniteness kernels derive grassmannian kernels summarized table experimental evaluation demonstrates beneﬁts grassmannian kernels classiﬁcation clustering sparse coding hashing. results show kernels outperform binet-cauchy projection ones gender gesture recognition pose categorization mouse behavior analysis. section ﬁrst review notions geometry grassmannians brieﬂy discuss existing positive deﬁnite kernels properties. throughout paper bold capital letters denote matrices bold lower-case letters denote column vectors identity matrix. indicates frobenius norm matrix trace. space p-dimensional linear subspaces euclidean space riemannian manifold known grassmannian note special case grassmann manifold becomes projective space consists lines passing origin. point grassmann riemannian manifold points connected smooth curves. distance points deﬁned length shortest curve connecting manifold. shortest curve length called geodesic geodesic distance respectively. grassmannian geodesic distance points given point grassmannian subspace spanned columns full rank matrix therefore denoted span. slight abuse notation call grassmannian point whenever represents basis subspace. words ﬁrst principal angle smallest angle unit vectors ﬁrst second subspaces. cosines principal angles correspond singular values addition geodesic distance several metrics employed measure similarity grassmannian points section discuss metrics grassmannian. mentioned earlier popular analyze problems deﬁned grassmannian embed manifold hilbert space using valid grassmannian kernel. formally deﬁne grassmannian kernels deﬁnition nonempty set. symmetric function positive deﬁnite kernel deﬁnition function grassmannian kernel well-deﬁned context function well-deﬁned invariant choice basis i.e. denotes special orthogonal group. widely used kernel arguably gaussian radial basis function kernel. therefore tempting deﬁne radial basis grassmannian kernel replacing euclidean distance geodesic distance. unfortunately although symmetric well-deﬁned function nevertheless grassmannian kernels i.e. binet-cauchy kernel projection kernel proposed embed grassmann manifolds rkhs. binet-cauchy projection kernels deﬁned successfully employed transform problems grassmannians hilbert spaces resulting hilbert spaces received comparatively little attention. section bridge study spaces explicitly computed. discuss embeddings deﬁne hilbert spaces namely pl¨ucker embedding projection embedding. embeddings respective properties turn help devise grassmannian kernels. hence pl¨ucker embedding -dimensional space spanned closer look coordinates embedded subspace reveals indeed minors possible submatrices shown hold proposition pl¨ucker coordinates minors matrix obtained taking rows possible ones. remark space induced pl¨ucker able exploit pl¨ucker embedding design kernels need needs invariant speciﬁc realization point still corresponds point furthermore would also like inner product eﬃcient evaluate thus avoiding need explicitly compute high-dimensional embedding. overwhelming hence explicitly computing embedding impractical. achieve goals rely following deﬁnition theorem deﬁnition given matrix matrix whose elements minors order arranged lexicographic order called compound denoted invariant speciﬁc realization subspace. simply veriﬁed permuting columns change subspace change sign det. note sign issue also observed wolf however problem circumvented considering second-order polynomial kernel projection embedding diﬀeomorphism grassmann manifold onto idempotent symmetric matrices rank i.e. one-to-one continuous diﬀerentiable mapping continuous diﬀerentiable inverse space induced embedding smooth compact submanifold dimension since symmetric matrix natural choice inner product inner product shown invariant discussion section deﬁned seen correspond homogeneous second order polynomial kernel space induced pl¨ucker embedding linear kernel space induced projection embedding respectively. section show inner products deﬁned section pl¨ucker projection embeddings actually exploited derive many grassmannian kernels including universal kernels conditionally positive deﬁnite kernels. following denote k·bc kernels derived pl¨ucker embedding projection embedding respectively. given inner product deﬁnes valid linear kernel straightforward create kernels consider higher degree polynomials. polynomial kernels known therefore readily deﬁne polynomial kernels grassmannian although often used practice polynomial kernels known universal crucial impact representation power speciﬁc task. indeed representer theorem that given training data kernel function learned algorithm expressed importantly universal kernels property able approximate target function arbitrarily well given suﬃciently many training samples. therefore generalize suﬃciently well certain problems. following develop several universal grassmannian kernels. make negative deﬁnite kernels relation ones. ﬁrst formally deﬁne negative deﬁnite kernels. deﬁnition nonempty set. symmetric function negative deﬁnite kernel state important theorem establishes relation kernels. theorem probability measure laplace transform i.e. half line e−tsdµ then positive deﬁnite problem designing kernel grassmannian cast ﬁnding appropriate probability measure below show lets reformulate popular kernels euclidean space grassmannian kernels. laplace kernels. laplace kernel another widely used euclidean kernel deﬁned expx obtain laplace kernels grassmannian make following theorem kernels. theorem negative deﬁnite satisﬁes another important class kernels so-called conditionally positive deﬁnite kernels formally deﬁnition nonempty set. symmetric function conditionally positive deﬁnite kernel relations kernels ones studied berg sch¨olkopf among others. introducing kernels grassmannian state important property kernels. property relaxes requirement kernels certain types kernel algorithms. kernel algorithm translation invariant independent position origin. example svms maximizing margin separating hyperplane classes independent position origin. result seamlessly kernel instead kernel svms. introduce kernels grassmannians rely following proposition proposition cpd. kernels derived section summarized table note given linear pl¨ucker projection kernels i.e. klinbc det| klinp possible obtain polynomial gaussian extensions standard kernel construction rules however approach lets derive many kernels principled manner e.g. exploiting diﬀerent measures theorem nonetheless here conﬁned deriving kernels corresponding popular ones euclidean space leave study additional kernels future work. section compare kernels baseline kernels using three diﬀerent kernel-based algorithms grassmannians kernel kernel k-means kernelized locality sensitive hashing experiments unless stated otherwise obtained kernel parameters cross-validation. ﬁrst demonstrate beneﬁts kernels binary classiﬁcation problem grassmannian using grassmannian graph-embedding discriminant analysis proposed consider task gender recognition gait used dataset-b casia gait database comprises individuals gait subject captured viewpoints. every video represented gait energy image size proven eﬀective gender recognition experiment used videos captured normal clothes created subspace order geis corresponding diﬀerent viewpoints. resulted points randomly selected individuals training used remaining individuals testing. table report average accuracies random partitions. note classiﬁer kernels derived pl¨ucker embedding outperform highest accuracy obtained binomial kernel. similarly projection kernels outperform polynomial kernel achieves overall highest accuracy ggda case klogp kernels also outperform previously-known ones. second experiment evaluate performance kernels task clustering grassmannian using kernel k-means. used cmupie face dataset contains images subjects diﬀerent poses diﬀerent illuminations image computed spatial pyramid histograms concatenated form dimensional descriptor. subject collected images acquired pose diﬀerent illuminations image represented linear subspace order resulted total grassmannian points used samples pose compute kernel parameters. goal cluster together image sets representing pose. evaluate quality clusters report clustering accuracy normalized mutual information measures amount statistical information shared random variables representing cluster distribution underlying class distribution data points. results given table that exception klogp kernels embedding outperform respective baseline binet-cauchy kernels maximum accuracy reached kernel. overall maximum accuracy achieved projection-based binomial kernel. also evaluated intrinsic k-means algorithm algorithm achieved accuracy score furthermore intrinsic k-means required perform clustering machine using matlab. machine runtimes kernel k-means using krbc kbip respectively. clearly demonstrates beneﬁts rkhs embedding tackle clustering problems grassmannian. finally utilized kernelized locality-sensitive hashing perform recognition videos mice behavior dataset basic idea klsh search projection rkhs low-dimensional hamming space sample encoded b-bit vector called hash key. approximate nearest-neighbor query found eﬃciently time sublinear number training samples. mice dataset contains behaviors several mice different coating colors sizes genders video estimated background extract region containing mouse frame. regions resized video represented order subspace thus yielding points randomly chose videos training used remaining videos testing. report average recognition accuracy random partitions. fig. depicts recognition accuracies baseline kernels function number bits pl¨ucker embedding kernels kernel reaches hash size hash size projection-based heat kernel outperforms thus reaches overall highest accuracy dictionary atom query vector sparse codes. practice used training sample atom dictionary. note that shown depends kernel values computed dictionary atoms well query point dictionary. classiﬁcation performed assigning label dictionary element strongest response query. keck dataset comprises body gestures static dynamic backgrounds dataset contains videos static scenes ones dynamic environments. following experimental protocol used ﬁrst extracted region interest around gesture resized pixels. represented video subspace order thus yielding points note kernels outperform baselines static dynamic settings. maximum accuracy obtained static scenario kbip dynamic experiments state-of-the-art solution using product manifolds achieves respectively. introduced positive deﬁnite kernels embed grassmannian hilbert spaces familiar euclidean structure. includes among others universal grassmannian kernels ability approximate general functions. experiments demonstrated superiority kernels previously-known grassmannian kernels i.e. binet-cauchy kernel projection kernel important keep mind however choosing right kernel data hand remains open problem. future intend study searching best probability measure theroem could give partial answer question. here prove theorem section i.e. equivalence scale length given curve binet-cauchy distance derived pl¨ucker embedding geodesic distance proof theorem follows several steps. start deﬁnition curve length intrinsic metric. without assumption diﬀerentiability metric space. curve continuous function joins starting point point deﬁnition length curve supremum possible", "year": 2014}