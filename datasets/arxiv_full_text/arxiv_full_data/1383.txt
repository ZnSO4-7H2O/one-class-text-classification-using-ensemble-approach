{"title": "Wide Residual Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks", "text": "deep residual networks shown able scale thousands layers still improving performance. however fraction percent improved accuracy costs nearly doubling number layers training deep residual networks problem diminishing feature reuse makes networks slow train. tackle problems paper conduct detailed experimental study architecture resnet blocks based propose novel architecture decrease depth increase width residual networks. call resulting network structures wide residual networks show superior commonly used thin deep counterparts. example demonstrate even simple -layer-deep wide residual network outperforms accuracy efﬁciency previous deep residual networks including thousand-layerdeep networks achieving state-of-the-art results cifar svhn coco signiﬁcant improvements imagenet. code models available https //github.com/szagoruyko/wide-residual-networks. convolutional neural networks seen gradual increase number layers last years starting alexnet inception residual networks corresponding improvements many image recognition tasks. superiority deep networks spotted several works recent years however training deep neural networks several difﬁculties including exploding/vanishing gradients degradation. various techniques suggested enable training deeper neural networks well-designed initialization strategies better optimizers skip connections knowledge transfer layer-wise training latest residual networks large success winning imagenet coco competition achieving state-of-the-art several benchmarks including object classiﬁcation imagenet cifar object detection segmentation pascal coco. compared inception architectures show better generalization meaning features utilized transfer learning better efﬁciency. also follow-up work showed residual links speed convergence deep networks recent follow-up work explored order activations residual networks presenting identity mappings residual blocks improving training deep networks. successful training deep networks also shown possible highway networks copyright document resides authors. distributed unchanged freely print electronic forms. architecture proposed prior residual networks. essential difference residual highway networks latter residual links gated weights gates learned. therefore point study residual networks focused mainly order activations inside resnet block depth residual networks. work attempt conduct experimental study goes beyond points. goal explore much richer network architectures resnet blocks thoroughly examine several different aspects besides order activations affect performance. explain below exploration architectures interesting ﬁndings great practical importance concerning residual networks. width depth residual networks. problem shallow deep networks discussion long time machine learning pointers circuit complexity theory literature showing shallow circuits require exponentially components deeper circuits. authors residual networks tried make thin possible favor increasing depth less parameters even introduced «bottleneck» block makes resnet blocks even thinner. note however residual block identity mapping allows train deep networks time weakness residual networks. gradient ﬂows network nothing force residual block weights avoid learning anything training possible either blocks learn useful representations many blocks share little information small contribution ﬁnal goal. problem formulated diminishing feature reuse authors tried address problem idea randomly disabling residual blocks training. method viewed special case dropout residual block identity scalar weight dropout applied. effectiveness approach proves hypothesis above. motivated observation work builds tries answer question wide deep residual networks address problem training. context show widening resnet blocks provides much effective improving performance residual networks compared increasing depth. particular present wider deep residual networks signiﬁcantly improve times less layers times faster. call resulting network architectures wide residual networks. instance wide -layer deep network accuracy -layer thin deep network comparable number parameters although several times faster train. type experiments thus seem indicate main power deep residual networks residual blocks effect depth supplementary. note train even better wide residual networks twice many parameters suggests improve performance increasing depth thin networks needs thousands layers case. dropout resnet blocks. dropout ﬁrst introduced adopted many successful architectures etc. mostly applied layers large number parameters prevent feature coadaptation overﬁtting. mainly substituted batch normalization introduced technique reduce internal covariate shift neural network activations normalizing speciﬁc distribution. also works regularizer authors experimentally showed network batch normalization achieves better accuracy network dropout. case widening residual blocks results increase number parameters studied effect dropout regularize training prevent overﬁtting. previously dropout residual networks studied dropout inserted identity part block authors showed negative effects that. instead argue dropout inserted convolutional layers. experimental results wide residual networks show leads consistent gains yielding even state-of-theart results last show proposed resnet architectures achieve state-of-the-art results several datasets dramatically improving accuracy speed residual networks. table structure wide residual networks. network width determined factor original architecture equivalent groups convolutions shown brackets number blocks group downsampling performed ﬁrst layers groups conv conv. final classiﬁcation layer omitted clearance. particular example shown network uses resnet block type compared original architecture order batch normalization activation convolution residual block changed conv-bn-relu bn-reluconv. latter shown train faster achieve better results don’t consider original version. furthermore so-called «bottleneck» blocks initially used make blocks less computationally expensive increase number layers. want study effect widening «bottleneck» used make networks thinner don’t consider focusing instead «basic» residual architecture. small ﬁlters shown effective several works including consider using ﬁlters larger also introduce factors deepening factor widening factor number convolutions block multiplies number features convolutional layers thus baseline «basic» block corresponds figures show schematic examples «basic» «basic-wide» blocks respectively. general structure residual networks illustrated table consists initial convolutional layer conv followed groups residual blocks conv conv conv followed average pooling ﬁnal classiﬁcation layer. size conv ﬁxed experiments introduced widening factor scales width residual blocks three groups conv- want study effect representational power residual block perform test several modiﬁcations «basic» architecture detailed following subsections. denote residual block structure list kernel sizes convolutional layers block. example denotes residual block convolutional layers note that consider «bottleneck» blocks explained earlier number feature planes always kept across block. would like answer question important convolutional layers «basic» residual architecture substituted less computationally expensive layer even combination convolutional layers e.g. increase decrease representational power block. thus experiment following combinations similar effective network-in-network architecture) original «basic» block extra layer dimensionality convolutions «straightened» bottleneck network alternating convolutions everywhere similar idea previous block network-in-network style block also experiment block deepening factor affects performance. comparison done among networks number parameters case need build networks different ensuring network complexity kept roughly constant. means instance decrease whenever increases. addition modiﬁcations experiment widening factor block. number parameters increases linearly number parameters computational complexity quadratic however computationally effective widen layers thousands small kernels much efﬁcient parallel computations large tensors interested optimal ratio. argument wider residual networks would almost architectures residual networks including successful inception much wider compared example residual networks wrn-- wrn-- similar width depth number parameters architectures. refer original residual networks «thin» networks «wide». rest paper following notation wrn-n-k denotes residual network total number convolutional layers widening factor also applicable append block type e.g. wrn---b. widening increases number parameters would like study ways regularization. residual networks already batch normalization provides regularization effect however requires heavy data augmentation would like avoid it’s always possible. dropout layer residual block convolutions shown relu perturb batch normalization next residual block prevent overﬁtting. deep residual networks help deal diminishing feature reuse problem enforcing learning different residual blocks. experiments chose well-known cifar- cifar- svhn imagenet image classiﬁcation datasets. cifar- cifar- datasets consist color images drawn classes split train test images. data augmentation horizontal ﬂips take random crops image padded pixels side ﬁlling missing pixels reﬂections original image. don’t heavy data augmentation proposed svhn dataset google’s street view house numbers images contains digit images coming signiﬁcantly harder real world problem. experiments svhn don’t image preprocessing except dividing images provide range input. experiments except imagenet based architecture pre-activation residual blocks baseline. imagenet using pre-activation networks less layers make signiﬁcant difference decide original resnet architecture case. unless mentioned otherwise cifar follow image preprocessing whitening. however cifar experiments instead simple mean/std normalization directly compare resnet related works make type preprocessing. following describe ﬁndings w.r.t. different resnet block architectures also analyze performance proposed wide residual networks. note experiments related «type convolutions block» «number convolutions block» reduced depth compared order speed training. start reporting results using trained networks different block types used wrn-- blocks blocks convolution. keep number parameters comparable trained networks less layers wrn---b wrn--b. provide results including test accuracy median runs time training epoch table block turned best little margin close accuracy less parameters less layers. faster others small margin. based above blocks comparable number parameters turned give less results. fact hereafter restrict attention wrns convolutions also consistent methods. next proceed experiments related varying deepening factor show indicative results table case took wrn-- convolutions trained several networks different deepening factor number parameters number convolutional layers. noticed turned best whereas worst performance. speculate probably increased difﬁculty optimization result decreased number residual connections last cases. furthermore turned quite worse. conclusion optimal terms number convolutions block. reason remaining experiments consider wide residual networks block type increase widening parameter decrease total number layers. optimal ratio experimented depth results presented table seen networks layers consistent gains width increased times. hand keeping ﬁxed widening factor varying depth consistent improvement however increase depth accuracy decreases show additional results table compare thin wide residual networks. observed wide wrn-- compares favorably thin resnet- achieves better accuracy cifar- cifar-. interesting networks comparable number parameters suggesting depth regularization effects compared width level. show benchmarks wrn-- times faster train evidently depth width ratio original thin residual networks optimal. also wide wrn-- outperforms thin resnet- cifar- cifar- times less layers note result resnet- obtained batch size whereas batch size experiments wide wrn-- respectively times parameters resnet- outperform signiﬁcant margin. table test error different methods cifar- cifar- moderate data augmentation mean/std normalzation. don’t dropout results. second column widening factor. results shown minibatch size parenthesis. results obtained computing median runs. general observed cifar mean/std preprocessing allows training wider deeper networks better accuracy achieved cifar- using wrn- parameters giving total improvement resnet- doesn’t seem regularization effect high depth residual networks wide networks number parameters thin ones learn better representations. furthermore wide networks successfully learn times larger number parameters thin ones would require doubling depth thin networks making infeasibly expensive train. trained networks dropout inserted residual block convolutions datasets. used cross-validation determine dropout probability values cifar svhn. also didn’t increase number training epochs compared baseline networks without dropout. dropout decreases test error cifar- cifar- correnspondingly wrn-- gives improvements resnets well knowledge ﬁrst result approach error cifar- even outperforming methods heavy data augmentation. slight drop accuracy wrn-- cifar- speculate relatively small number parameters. notice disturbing effect residual network training ﬁrst learning rate drop loss validation error suddenly start oscillate high values next learning rate drop. found caused weight decay however making lower leads signiﬁcant drop accuracy. interestingly dropout partially removes effect cases ﬁgures regularization effect. evidence found training curves ﬁgure loss without dropout drops values. results presented table observe signiﬁcant improvements using dropout thin wide networks. thin -layer deep network even outperforms thin -layer deep network stochastic depth additionally trained wrn-- dropout svhn achieves svhn best published result knowledge. without dropout achieves overall despite arguments combining batch normalization dropout shows effective techique regularization thin wide networks. used improve results widening also complementary imagenet ﬁrst experiment non-bottleneck resnet- resnet- trying gradually increase width results shown table increasing width gradually increases accuracy networks networks comparable number parameters achieve similar results despite different depth. althouth networks large number parameters outperfomed bottleneck networks probably either bottleneck architecture simply better suited imagenet classiﬁcation task complex task needs deeper network. test this took resnet- tried make wider increasing inner layer width. widening factor resulting wrn---bottleneck outperforms resnet times less layers signiﬁcantly faster. wrn---bottleneck sergey zagoruyko nikos komodakis wide residual networks slightly worse almost faster best-performing pre-activation resnet- althouth slightly parameters general that unlike cifar imagenet networks need width depth achieve accuracy. however clear unnecessary residual networks layers computational reasons. table ilsvrc- validation error non-bottleneck resnets various widening factors. networks comparable number parameters achieve similar accuracy despite times less layers. also used wrn-- participate coco object detection challenge using combination multipathnet locnet despite layers model achieves state-of-the-art single model performance outperforming even resnet- inception-v-based models. table best performance various datasets single results. coco model based wrn-- uses vgg--based attractionet proposals locnet-style localization part. knowledge best published results cifar- cifar- svhn coco computations much optimal wide networks many times efﬁcient thin ones benchmarks show. cudnn titan measure forward+backward update times minibatch size several networks results ﬁgure show best cifar wide wrn-- times faster thin resnet-. furthermore wide wrn-- approximately accuracy resnet- times faster. figure time forward+backward update minibatch size wide thin networks. numbers beside bars indicate test error cifar- time test time proportional fraction benchmarks. note instance wide wrn-- times faster thin resnet approximately accuracy. experiments nesterov momentum cross-entropy loss. initial learning rate weight decay dampening momentum minibatch size cifar learning rate dropped epochs train total epochs. svhn initial learning rate drop epochs training total epochs. implementation based torch reduce memory footprints networks. imagenet experiments used fb.resnet.torch implementation code models available https//github.com/szagoruyko/wide-residual-networks. presented study width residual networks well dropout residual architectures. based study proposed wide residual network architecture provides state-of-the-art results several commonly used benchmark datasets well signiﬁcant improvements imagenet. demonstrate wide networks layers signiﬁcantly outperform -layer deep networks cifar well -layer outperform -layer imagenet thus showing main power residual networks residual blocks extreme depth claimed earlier. also wide residual networks several times faster train. think intriguing ﬁndings help advances research deep neural networks. thank startup company visionlabs eugenio culurciello giving access clusters without imagenet experiments wouldn’t possible. also thank adam lerer gross helpful discussions. work supported project fp-ict robospect.", "year": 2016}