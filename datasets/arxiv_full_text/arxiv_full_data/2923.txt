{"title": "Choosing Smartly: Adaptive Multimodal Fusion for Object Detection in  Changing Environments", "tag": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Object detection is an essential task for autonomous robots operating in dynamic and changing environments. A robot should be able to detect objects in the presence of sensor noise that can be induced by changing lighting conditions for cameras and false depth readings for range sensors, especially RGB-D cameras. To tackle these challenges, we propose a novel adaptive fusion approach for object detection that learns weighting the predictions of different sensor modalities in an online manner. Our approach is based on a mixture of convolutional neural network (CNN) experts and incorporates multiple modalities including appearance, depth and motion. We test our method in extensive robot experiments, in which we detect people in a combined indoor and outdoor scenario from RGB-D data, and we demonstrate that our method can adapt to harsh lighting changes and severe camera motion blur. Furthermore, we present a new RGB-D dataset for people detection in mixed in- and outdoor environments, recorded with a mobile robot.", "text": "fig. vision-based detection fails dark environments motion blur performs good outdoor scenarios larger distances depth images usually noisy. approach combines best worlds adaptive fusion manner. although method applicable arbitrary number object classes work report results two-class problem human detection rgb-d domain. first experimental results show increased performance method compared fusion approaches reported publicly available rgb-d people unihall dataset second evaluate approach challenging detection scenario. recorded rgb-d sequences people mobile robot captured abrupt changes lighting conditions indoors outdoors.. comparison previously recorded datasets mobile robots sequences task autonomous robots operating dynamic changing environments. robot able detect objects presence sensor noise induced changing lighting conditions cameras false depth readings range sensors especially rgb-d cameras. tackle challenges propose novel adaptive fusion approach object detection learns weighting predictions different sensor modalities online manner. approach based mixture convolutional neural network experts incorporates multiple modalities including appearance depth motion. test method extensive robot experiments detect people combined indoor outdoor scenario rgb-d data demonstrate method adapt harsh lighting changes severe camera motion blur. furthermore present rgb-d dataset people detection mixed inoutdoor environments recorded mobile robot. autonomous robots operating complex environments equipped different sensors perceive surroundings. make entire sensor information context object detection task perception system needs adaptively fuse data different sensor modalities. sensor fusion challenging object detection sensor noise typically depends substantially conditions environment even might changing. example scenario changing environment would robot equipped rgb-d sensor operate both dark indoor bright outdoor scenario different times different weather conditions case autonomous cars. goal equip perception system robots capability autonomously adapt current conditions without human intervention. example dark indoor scenario would expect depth information rgb-d device reliable visual appearance. additionally depth stream would informative sunny outdoor scenario objects away robot. paper demonstrate prior information learned data without hand-crafted features. therefore best combine different modalities robust object detection main question tackle. make following contributions ∗these authors contributed equally. authors department computer science university freiburg germany. {meeso eitel burgard}informatik.uni-freiburg.de. work partly supported european commission erc-agpe--lifenav. recent discussion benenson conclude improved detection performance shown driven design better features also complemented additional data image context motion. demonstrate quality features learned convolutional neural network hosang reported improved results pedestrian detection using pre-trained off-theshelf cnns. recently angelova presented convolutional network architecture processes larger areas input image detects multiple pedestrians simultaneously leading signiﬁcant speedup test time. however none approaches make multiple modalities. iii. mixture deep networks architecture detection approach based mixture deep network experts fused additional network denote gating network. overall architecture illustrated fig. extension adaptive mixture experts method differs modelling gating network. takes input feature representations extracted higher level hierarchy expert network instead using pixel input presented original method. gating network decides based input weight outputs expert produce ﬁnal classiﬁer output. training examples denotes sequence matrices describe different input modalities along gating network output encoding class labels deﬁned dimensional vector one-hot encoding. combine classiﬁer outputs experts modality gating denote feature produced last pooling layer expert cnn. described three-dimensional array size resulting concatenated feature hconcat experts array size ﬂattened representation hconcat denoted one-dimensional array size therefore gating functions depend input solely representation signiﬁcantly minimizes input dimension gating network. maps pixel input outputs fused classiﬁer output written show robot moving poorly illuminated indoor environments followed bright outdoor scenes within short period time. throughout paper talking changing dynamic environment mean underlying conditions environment changing. examples underlying conditions sensor fusion robust object detection changing environments core many robotics applications. past large fraction work focused task context human also general object detection. yebes combined features appearance depth data conjunction detector recognize objects road scenes. features merged channel fusion manner modelling d-aware descriptor. comparison method approach aims combine features later stage. enzweiler introduced mixture experts approach people detection using three input modalities namely appearance motion depth. comparison approach weighting experts constant therefore adaptive. premebida trained late fusion manually designed features perform detector fusion depth modalities. work prior information learn weights fusion although would possible method manually designed fusion features. spinello proposed hierarchical mixture experts approach output individual detectors weighted based missing information sensor modalities. compared approach weighting function directly learned feature representation input data. recently late-fusion network architectures shown successful vision-tasks multimodal pedestrian detection rgb-d object recognition rgb-d object detection therefore test similar late-fusion network architectures baselines comparison approach. work also related area multimodal people detection emphasis mobile platforms equipped rgb-d sensors. jafari proposed multimodal approach combines depth-based close distance upper body detector appearance-based full body detector based groundhog features. detections modalities multi-hypothesis ekf-based tracking module. comparison ﬁltering time approach performs fusion per-frame basis. comparisons refer reader work spinello munaro linder fig. propose mixture deep network experts sensor fusion improves classiﬁcation several experts trained different input modalities additional gating network. fusion learned supervised gating network trained feature representation expert cnns. input sample gating network assigns adaptive weighting value expert produce ﬁnal classiﬁcation output. improvement sensor fusion report results cifarnet architecture single experts mixture experts network. second experiment show improvements cifarnet baseline achieved replacing network inception architecture. googlenet-xxs network layers original network ﬁrst softmax classiﬁer training networks fast r-cnn including region interest pooling layer multi-task loss bounding regression. framework wrapped around modiﬁed version caffe library experts trained using standard parameters ﬁrst stage apply dropout regularizer fully-connected layers. further networks modality ﬁne-tuned using pretrained models available caffe library whereas experts domains trained scratch. second training phase gating network trained additional validation set. optimize weights gating network using keep weights individual experts ﬁxed setting learning rate layers zero. although weights experts changed apply modiﬁcation expert layers training gating network. expert layers apply dropout special case data augmentation order improve performance gating network. generate region interest proposals networks implemented dense multiscale sliding window approach. throughout paper different input modalities feedforward networks. mainly combination following modalities depth optical optical computation opencv denotes probability selecting softmax expert output last inner product layer gating network denoted softmax function deﬁned softmax exp. train combined gating network architecture using cross-entropy loss deﬁned train model using stage approach ﬁrst stage train individual expert networks end-to-end manner using stochastic gradient descent paper several expert architectures standard three-layer convolutional neural network baseline evolved deep network based google inception architecture three-layer convolutional neural network small network designed solve cifar classiﬁcation problem depicted cifarnet. also proposed hosang good baseline people detection. section iv-b consider downsampled google inception architecture depicted googlenet-xxs. ﬁrst experiment section iv-a cifarnet baseline already outperforms previous reported approaches. order leave room three single networks trained three input modalities namely depth optical ﬂow. depth images pre-processed using colorization methodology presented previous work table shows trained depth data performs better comparison network trained modality matches previous results dataset hypothesize reason illumination changes contrast found images. besides lead noticeable amount corruption computing optical images. consequently optical underperforms compared experts also limited detect moving people only. show table depth cifarnet outperforms based approaches. evaluating detection best network achieves relative improvement graph based segmentation combination proposed choi report improvement previous approaches evaluating model fusion approaches compare detection performance different fusion approaches combining depth motion depth motion naive fusing individual detectors averaging classiﬁer outputs seen table approach looses points respect methods. involved method late fusion approach fully-connected layer trained last pooling layers network streams. formally concatenate last layer feature responses feed additional two-layer fully-connected fusion stream parameters ﬁrst fullyconnected layer outputs followed second layer output ends softmax classiﬁer. follow stage training procedure described earlier. results given table show late fusion approach substantially improves detector performance combining three modalities. combined depth optical late fusion approach underperforms conclude suitable modalities highly differ feature space. architecture gating network proposed mode consists fullyconnected layers rectiﬁed linear units size three. input gating network combined output last pooling layer expert. proposed mode approach yields best performance fusion scenarios. combining three modalities achieve addition report relative improvement hge’s evaluate approach publicly available rgbpeople unihall dataset provided spinello dataset contains frames people passing three vertically mounted kinect cameras university hallway. evaluation metrics compute average precision equal error rate deﬁne equal error rate point precision-recall curve precision recall values equal. adopting noreward-no-penalty policy count true positives false positives detection matches annotation partially occluded person. training randomly select frames three kinect cameras extract positive samples annotated candidates show fully visible people. evaluation remaining frames three kinects. hyperparameters learning procedure evaluate trained models training choosing best performing model evaluation test set. moreover dataset provide pre-deﬁned train/test split. therefore created random train/test splits train evaluate detector. obtained standard deviation showing small inﬂuence splits chosen. experiments report results using intersection union different evaluation metrics used literature. unless otherwise speciﬁed evaluation area overlap predicted bounding boxes annotated ground truth box. single expert performance compare reported approaches rgb-d people dataset features baseline architecture trained colorized depth data. report performance next experiment conducted challenging people detection scenario recorded kinect camera mounted mobile robot. several abrupt harsh lighting changes combined severe motion blur provide fig. timeline gating weights test sequence. gating assignment weight switches depth modalities depending current environment example frame people hard detect scene dark. accordingly assigned gating weight grgb respective depth expert’s gating weight higher gdepth grgb people better visible depth image. challenges sensor modalities depth fig. particularity recorded sequences robot driving indoor outdoor environments single take. recorded annotated total rgb-d frames collected robot frame rate camera calibrated using approach wiedemeyer recorded midday recorded dusk. test sequence contains frames robot drives dark indoor scene relatively dark street scene fig. evaluation metrics compute average precision evaluation positive detections. quantitative results report performance several single modality networks namely depth show improved results fusing modalities table iii. further compare cifarnet architecture evolved googlenet-xxs architecture. single modality networks perform reasonably well test set. depthbased googletnet-xxs network outperforms upper body detector jafari gating network architecture googlenet-xxs consists convolution layer output size kernel size followed fullyconnected layers size combined outputs last convolutional layers expert networks serve input gating network. novel fusion approach improves performance comparison late fusion approach. best model combination depth googlenet-xxs fused gating network also evaluated switching expert gating network predicted deﬁning instead weighting experts found underperform. comparison trained multi-channel network channels input end-to-end manner scratch. network slightly underperforms comparison late fusion approach using googlenet architecture. mode method also outperforms naive averaging classiﬁer outputs. qualitative results analyzing output detections obtained rgb-d based mode detector full test sequence gain several interesting insights. fig. shows frame average gating weights experts. plot shows average gating weights correlate reasonably well respective environment. bright indoor scenario modality chosen often whereas dark environments gating network tends weight depth network output higher visionbased one. outdoors mainly relies especially pedestrian distance almost visible depth image seen fig. also found depth network perform better blurred images therefore ﬁnal frames test sequence gating weights vary frames. here outputs mostly chosen although abrupt camera motion gating network switches higher frequency. fig. shows gating weights several exemplary frames supporting observation gating network tends switch changing underlying conditions lighting conditions false depth readings ranges motion blur. paper considered problem multimodal adaptive sensor fusion object detection changing environments. proposed novel mixture deep network experts approach automatically learns adaptive strategy weighting several domain-based classiﬁers input data. extensive experiments demonstrated multimodal method outperforms vision-based detection baseline fusion techniques. moreover show improved detection performance sequence recorded mobile robot containing abrupt lighting changes severe motion blur. finally system outperforms previously reported approaches depth-based people detection publicly available rgb-d people inoutdoor people dataset. eitel springenberg spinello riedmiller burgard multimodal deep learning robust rgb-d object recognition proc. ieee/rsj int. conf. intelligent robots systems hosseini jafari mitzel leibe real-time rgb-d based people detection tracking mobile robots head-worn cameras proc. ieee int. conf. robotics automation shelhamer donahue karayev long girshick guadarrama darrell caffe convolutional architecture fast feature embedding proc. int. conf. multimedia linder girrbach arras towards robust people tracking framework service robots crowded dynamic environments ieee/rsj int. conf. intelligent robots systems wagner fischer herman behnke multispectral pedestrian detection using deep fusion convolutional neural networks european symp. artiﬁcial neural networks", "year": 2017}