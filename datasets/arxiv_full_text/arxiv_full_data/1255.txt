{"title": "BMXNet: An Open-Source Binary Neural Network Implementation Based on  MXNet", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Binary Neural Networks (BNNs) can drastically reduce memory size and accesses by applying bit-wise operations instead of standard arithmetic operations. Therefore it could significantly improve the efficiency and lower the energy consumption at runtime, which enables the application of state-of-the-art deep learning models on low power devices. BMXNet is an open-source BNN library based on MXNet, which supports both XNOR-Networks and Quantized Neural Networks. The developed BNN layers can be seamlessly applied with other standard library components and work in both GPU and CPU mode. BMXNet is maintained and developed by the multimedia research group at Hasso Plattner Institute and released under Apache license. Extensive experiments validate the efficiency and effectiveness of our implementation. The BMXNet library, several sample projects, and a collection of pre-trained binary deep models are available for download at https://github.com/hpi-xnor", "text": "abstract binary neural networks drastically reduce memory size accesses applying bit-wise operations instead standard arithmetic operations. erefore could signicantly improve eciency lower energy consumption runtime enables application state-of-the-art deep learning models power devices. bmxnet open-source library based mxnet supports xnor-networks antized neural networks. developed layers seamlessly applied standard library components work mode. bmxnet maintained developed multimedia research group hasso planer institute released apache license. extensive experiments validate eciency eectiveness implementation. bmxnet library several sample projects collection pre-trained binary deep models available download hps//github.com/hpi-xnor concepts soware engineering→ soware libraries repositories; computer systems organization→ neural networks; computing methodologies computer vision; reference format haojin yang martin fritzsche christian bartz christoph meinel. bmxnet open-source binary neural network implementation based mxnet. proceedings conference washington july pages. ./nnnnnnn.nnnnnnn introduction recent years deep learning technologies achieved excellent performance many breakthroughs academia industry. however state-of-the-art deep models computational expensive consume large storage space. deep learning also strongly demanded numerous applications areas mobile platforms wearable devices autonomous robots devices. eciently apply deep models power several approaches introduce usage bnns. bnns capability decreasing memory consumption computational complexity neural network. achieved hand storing weights typically stored oating point values binary values binarizing oating point values sign function either storing several single integer. computational complexity hand reduced using xnor popcount performing matrix multiplications used convolutional fully connected layers. publicly available implementations store weights binarized form xnor popcount performing matrix multiplications convolutional fully connected layers. deep learning library tensorow tries decrease memory consumption computational complexity deep neural networks quantizing oating point weights inputs integers. together minimum maximum value weight/input matrix less memory usage also decreased computational complexity achieved operations need performed values rather values. bmxnet stores weights convolutional fully connected layers binarized format enables store weights single oat/integer less memory. training inference binarize input binary convolution fully connected layer weights binarized perform matrix multiplication using bit-wise operations implementation also prepared networks store weights inputs arbitrary widths proposed zhou deep learning library mxnet serves base code. mxnet high performance modular deep learning library wrien c++. mxnet provides bindings popular programming languages like python scala used wide range researchers companies. framework bmxnet provides activation convolution fully connected layers support quantization binarization input data weights. layers designed drop-in replacements corresponding mxnet variants called qactivation python example usage framework comparison mxnet shown listing binary layers last layer network conrmed experiments showing greatly decreases accuracy. standard block structure bmxnet conducted qactivation-qconv/qfc-batchnorm-pooling shown listing listing binary lenet data symbol variable first conv layer conv convolution tanh activation pool pooling batchnorm second conv layer conv convolution batchnorm tanh activation pool pooling first fullc layer flatten flatten symbol fullyconnected batchnorm tanh activation second fullc fullyconnected softmax loss lenet softmaxoutput return lenet data symbol variable first conv layer conv convolution tanh activation pool pooling batchnorm second conv layer mx.sym.qactivation conv mx.sym.qconvolution batchnorm pool pooling first fullc layer flatten flatten mx.symbol.qactivation mx.symbol.qfullyconnected batchnorm tanh activation second fullc fullyconnected softmax loss lenet softmaxoutput return lenet antization quantization widths ranging available experiments training prediction using precision weights inputs. quantized data still stored default values standard mxnet product operations applied. binarization extreme case quantizing wide values binarization. working binarized weights input data allows highly performant matrix multiplications utilizing instructions xnor popcount. product xnor popcount. fully connected convolutional layers heavily rely products matrices turn require massive oating point operations. modern cpus optimized types operations. especially real time applications embedded less powerful devices optimizations improve performance reduce memory footprint lower power consumption calculate product binary matrices multiplication operation required. element-wise multiplication summation column approximated combining xnor operation counting number bits result population count approximate multiplication addition times matrix elements processor instructions cpus times elements armv processors. enabled hardware support xnor popcount operations. translate directly single assembly command. population count instruction available cpus supporting sse. architecture included neon instruction set. unoptimized gemm implementation utilizing instructions shown listing compiler intrinsic builtin popcount supported clang compilers translates machine instruction supported hardware. binary word packed data type storing matrix elements represented single bit. implemented several optimized versions xnor gemm kernel. leverage processor cache hierarchies blocking packing data unrolling techniques openmp parallelization. training. carefully designed binarized layers exactly match output built-in layers mxnet limiting discrete values enables massively parallel training support utilizing cudnn high performance clusters. trained model used less powerful devices forward pass prediction calculate product xnor popcount operations instead multiplication addition. possible values performing xnor popcount range matrix multiplication step size whereas normal product matrices limited discrete values range step size enable supported training modify training process. calculation product result back range match xnor product equation figure speedup comparison based naive gemm method varying kernel size convolution layer. input channel size batch size lter number respectively. current deep neural network implementations fully connected convolution layers implemented using gemm. according evaluation result processing time cae-alexnet model spent layers. thus conducted experiments measure eciency dierent gemm methods. measurements performed within convolution layer parameters follows lter number= kernel size=× batch size= matrix sizes kernelw kernelh inputchannelsize respectively. figure shows evaluation results. colored columns denote processing time milliseconds across varying input channel size; xnor xnor denote xnor gemm operator bit; xnor denotes xnor gemm accelerated using openmp parallel programming library; binarize input xnor accumulated processing time input data binarization. results determine xnor achieved acceleration comparison cblas naive gemm kernel respectively. accumulating binarization time input data still achieved acceleration compared cblas method. following strategy applied always avoid binarization convolution layer last fully connected layer. table depicts classication test accuracy binary well full precision models trained mnist cifar-. table shows size binary models signicantly reduced accuracy still competitive. table demonstrates validation accuracy binary figure speedup comparison based naive gemm method varying lter number convolution layer. input channel size kernel size batch size respectively. model converter. training network bmxnet weights stored variables. also case networks trained width bit. provide model converter reads binary trained model packs weights qconvolution qfullyconnected layers. conversion storage runtime memory used weight. resnet- network full precision weights size .mb. conversion model converter achieves compression resulting size gemm computation. order demonstrate applicability developed sample applications image classication android well using binarized resnet- model. source code documentation pre-trained models sample projects published github renzo andri lukas cavigelli davide rossi luca benini. yodann ultra-low power convolutional neural network accelerator based binary weights. vlsi ieee computer society annual symposium ieee tianqi chen yutian naiyan wang minjie wang tianjun xiao bing chiyuan zhang zheng zhang. mxnet flexible ecient machine learning library heterogeneous distributed systems. corr abs/. mahieu courbariaux yoshua bengio jean-pierre david. binaryconnect training deep neural networks binary weights propagations. advances neural information processing systems deng dong socher l.-j. fei-fei. imagenet large-scale hierarchical image database. cvpr. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision paern recognition. itay hubara mahieu courbariaux daniel soudry el-yaniv yoshua bengio. binarized neural networks. advances neural information processing systems google inc. tensorflow large-scale machine learning heterogeneous systems. hp//tensorow.org/ soware available tensorow.org. yangqing jia. learning semantic image representations large scale. yangqing evan shelhamer donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell. convolutional architecture fast feature embedding. arxiv preprint arxiv. mohammad rastegari vicente ordonez joseph redmon farhadi. xnor-net imagenet classication using binary convolutional neural networks. computer vision eccv partially-binarized full precision models trained imagenet. resnet implementation mxnet consists resunit stages thus also report results partially-binarized model specic full precision stages. partially-binarized model full precision stage shows great accuracy improvement minor model size increase compared fully binarized model. example applications python scripts bmxnet repository contains python scripts train validate binarized neural networks. script hpi/examples/ binary mnist/mnist cnn.py train binary lenet mnist data set. train network cifar imagenet data python script based resnet- architecture. find hpi/examples/ binary-imagenetk/train cifar/train .py. information example invocation corresponding readme.md conclusion introduced bmxnet open-source binary neural network implementation c/c++ based mxnet. evaluation results show model size saving much ecient xnor", "year": 2017}