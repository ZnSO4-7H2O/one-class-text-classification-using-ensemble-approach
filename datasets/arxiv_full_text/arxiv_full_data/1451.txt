{"title": "Cutting-off Redundant Repeating Generations for Neural Abstractive  Summarization", "tag": ["cs.CL", "cs.AI", "stat.ML"], "abstract": "This paper tackles the reduction of redundant repeating generation that is often observed in RNN-based encoder-decoder models. Our basic idea is to jointly estimate the upper-bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows significant improvement over a strong RNN-based encoder-decoder baseline and achieved its best results on an abstractive summarization benchmark.", "text": "paper tackles reduction redundant repeating generation often observed rnn-based encoder-decoder models. basic idea jointly estimate upper-bound frequency target vocabulary encoder control output words based estimation decoder. method shows signiﬁcant improvement strong rnn-based encoder-decoder baseline achieved best results abstractive summarization benchmark. rnn-based encoder-decoder approach recently providing signiﬁcant progress various natural language generation tasks i.e. machine translation sutskever abstractive summarization rush since scheme approach interpreted conditional language model suitable tasks. however potential weakness sometimes repeatedly generates phrase issue discussed neural literature part coverage problem repeating generation behavior become severe tasks short task duc- typical example requires generation summary pre-deﬁned limited output space words bytes. thus repeated output consumes precious limited output space. unfortunately coverage approach cannot directly applied tasks since require optimally salient ideas input lossy compression manner thus summary length hardly depends input length; task mainly loss-less generation nearly one-to-one correspondence input output nallapati background paper tackles issue proposes method overcome tasks. basic idea method jointly estimate upper-bound frequency target vocabulary occur summary encoding process exploit estimation control output words decoding step. refer additional component wordfrequency estimation sub-model. sub-model explicitly manages many times word generated might generated future decoding process. thus expect decisively prohibit excessive generation. finally evaluate effectiveness method well-studied benchmark data provided rush rush evaluated chopra nallapati kikuchi takase ayana gulcehre baseline proposal rnn-based encdec model attention mechanism luong model already used strong baseline tasks chopra kikuchi well literature. speciﬁcally case study employ -layer bidirectional lstm encoder -layer lstm decoder global attention bahdanau omit detailed review descriptions space limitations. following necessary parts explaining proposed method. list generated words hidden states process decoder triplet info decoding process push initial triplet priority queue prepare queue store complete sentences repeat repeat input output sequences respectively one-hot vectors correspond i-th word input j-th word output. denote vocabulary output. simpliﬁcation paper uses following four notation rules encdec approach. deﬁne required information shown figure decoding process following triplet cumulative log-likelihood step ˆyj− output word sequence generated step ˆyj− hidden states calculating j-th decoding process. then function calcll line written follows moreover line )matrix number complete sentences -element represents likelihood m-th word namely calculated using k-th candidate step. line function maketriplet constructs triplets based information index then line function selecttopk selects top-k candidates union generated triplets current step {hz}k triplets complete sentences finally function sepcomp line divides triplets distinct sets whether complete senelements tences complete sentences namely algorithm stops according evaluation line incorporate separated components improve frequency ﬁtting. purpose distinguish whether target words occur regardless frequency. thus interpreted gate function resembles estimating fertility coverage switch probability copy mechanism gulcehre ideas originated gated recurrent networks lstm hochreiter schmidhuber chung then much focus model frequency equal larger separation expected since inﬂuence m-th word never selected step thus interpretation directly manages upper-bound frequency target word occur current future decoding time steps. result decoding method never generates words exceed estimation thus expect reduce redundant repeating generation. figure shows detailed procedure calculating features input given encoder estimate frequency. contrast expect lines work kind voting positive negative directions since needs occurrence information frequency. example take large positive negative values certain input word strong inﬂuence occurring occurring speciﬁc target word output. idea borrowed max-pooling layer goodfellow given training data vector representation true frequency target words given input +∞}. clearly obtained counting words corresponding output. deﬁne loss function ψwfe estimating represents overall parameters. form ψwfe closely related used support vector regression smola sch¨olkopf allow estimation take value range penalty case select since elements integer. remaining positive negative sides denotes margin every integer. select penalize larger distant error i.e. since obtain upper-bound estimation penalize underestimation true frequency investigated effectiveness method experiments ﬁrst performed rush rush data consist approximately million training validation test data respectively. generally test data randomly extracted test data section used evaluation. additionally duc- evaluation data also evaluated identical models trained gigaword data. strictly followed instructions evaluation setting used previous studies fair comparison. table summarizes model conﬁguration parameter estimation setting experiments. table shows results baseline encdec proposed encdec+wfe. note duc- data evaluated recall-based rouge scores gigaword data evaluated f-score-based rouge respectively. validity conﬁrmation encdec baseline also performed opennmt tool. results gigaword data were rouge rouge- rouge-l respectively almost similar results implementation. supports baseline worked well strong baseline. clearly encdec+wfe signiﬁcantly outperformed strong encdec baseline wide margin rouge scores. thus conclude sub-model positive impact gain performance since performance table lists current system results. method encdec+wfe successfully achieved current best scores evaluations. result also supports effectiveness incorporating sub-model. ayana previously provided best results. note model structure nearly identical baseline. contrary trained model sequence-wise minimum risk estimation trained models experiments standard log-likelihood maximization. essentially complements method. expect further improve performance applying training since recent progress suggested leveraging sequence-wise optimization technique improving performance wiseman rush shen leave future work. figure shows actual generation examples. based motivation speciﬁcally selected redundant repeating output occurred baseline encdec. clear encdec+wfe successfully reduced them. observation offers evidence effectiveness method quality. evaluate sub-model alone table shows confusion matrix frequency estimation. quantized ⌊ˆa+.⌋ derived margin ψwfe. unfortunately result looks well. seems exist enough room improve estimation. however emphasize already enough power improve overall quality shown table figure expect gain overall performance improving performance sub-model. paper discussed behavior redundant repeating generation often observed neural encdec approaches. proposed method reducing redundancy incorporating submodel directly estimates manages frequency target vocabulary output. experiments benchmark data showed effectiveness method encdec+wfe improving automatic evaluation performance reducing actual redundancy. method suitable lossy compression tasks image caption generation tasks.", "year": 2016}