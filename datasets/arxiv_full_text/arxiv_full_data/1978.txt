{"title": "Learning to relate images: Mapping units, complex cells and simultaneous  eigenspaces", "tag": ["cs.CV", "cs.AI", "nlin.AO", "stat.ML"], "abstract": "A fundamental operation in many vision tasks, including motion understanding, stereopsis, visual odometry, or invariant recognition, is establishing correspondences between images or between images and data from other modalities. We present an analysis of the role that multiplicative interactions play in learning such correspondences, and we show how learning and inferring relationships between images can be viewed as detecting rotations in the eigenspaces shared among a set of orthogonal matrices. We review a variety of recent multiplicative sparse coding methods in light of this observation. We also review how the squaring operation performed by energy models and by models of complex cells can be thought of as a way to implement multiplicative interactions. This suggests that the main utility of including complex cells in computational models of vision may be that they can encode relations not invariances.", "text": "fundamental operation many vision tasks including motion understanding stereopsis visual odometry invariant recognition establishing correspondences images images data modalities. present analysis role multiplicative interactions play learning correspondences show learning inferring relationships images viewed detecting rotations eigenspaces shared among orthogonal matrices. review variety recent multiplicative sparse coding methods light observation. also review squaring operation performed energy models models complex cells thought implement multiplicative interactions. suggests main utility including complex cells computational models vision encode relations invariances. correspondence arguably ubiquitous computational primitive vision tracking amounts establishing correspondences frames; stereo vision diﬀerent views scene; optical images; invariant recognition images invariant descriptions memory; odometry images motion information; action recognition frames; etc. many tasks relationship images content single image carries relevant information. representing structures within single image contours also considered instance correspondence problem namely areas pixels within image. fact correspondence common operation across vision suggests task representing relations importance image correspondence action understanding nicely illustrated heider simmel’s video geometric objects engaged various social activities dynamic mappings illustration mapping units shown figure three variables shown ﬁgure interact multiplicatively result variable thought dynamically modulating connections variables model likewise value variable thought depending product variables contrast common feature learning models like restricted boltzmann machines autoencoder networks many others based bi-partite networks involve three-way multiplicative interactions. models independent hidden variables interact independent observable variables value variable depends weighted product variables. closely related models mapping units energy models thought emulate multiplicative interactions computing squares. shall show mapping units energy models viewed ways learn detect rotations shared invariant subspaces commuting matrices. analysis help understand action recognition methods seem proﬁt squaring non-linearities predicts squaring cross-products helpful general applications involve representing original intent video goes beyond making case correspondences). single frame depicts rather meaningless geometric objects conveys almost information content movie. understand movie understanding motions actions thus decoding relationships frames. shortly mapping units introduced energy models received attention. energy models closely related cross-correlation models which turn type multiplicative interaction model. energy models used model motion stereo vision energy model computational unit relates images summing squared responses typically linear projections input data. operation shown encode translations independently content early approaches building applying energy cross-correlation models based entirely hand-wiring practically models gabor ﬁlters linear receptive ﬁelds whose responses squared summed. focus gabor features somewhat biased analysis energy models focus fourier-spectrum main object interest shall discuss section fourier-components arise special case transformation class namely translation many analyses apply generally types transformation. gabor-based energy models also applied monocularly. case encode features independently fourier-phase input. result responses invariant small translations well contrast variations input. part reason energy models popular models complex cells known show similar invariance properties shortly energy cross-correlation models emerged attention learning invariances higher-order neural networks neural networks trained polynomial basis expansions inputs higher-order neural networks composed units compute sums products. units sometimes referred sigma-pi-units time discussed multiplicative interactions make possible build distributed representations symbolic data. kohonen introduced adaptive subspace self-organizing computes sums squared ﬁlter responses represent data. like energy model assom based idea squared responses invariant various properties inputs. contrast early energy models assom trained data. inspired assom introduced independent subspace analysis puts idea context conventional sparse coding models. extensions work topographic sums computed separate shared groups squared ﬁlter responses. figure symbolic representation mapping unit triangle symbolizes multiplicative interactions three variables value three variables function product others. used global models trained whole images rather using local receptive ﬁelds. contrast recent approaches learning multiplicative interactions training typically involved ﬁlling two-dimensional grid data shows types variability purpose bi-linear models untangle degrees freedom data. recent work make distinction purpose multiplicative hidden variables merely capture multiple ways images related. example show multiplicative interactions make possible model multitude relationships frames natural videos. also show allow model general classes relations images. earlier multiplicative interaction model also related bi-linear models routing-circuit multiplicative interactions also used model structure within static images thought modeling higher-order relations particular pair-wise products pixel intensities recently showed multiplicative interactions class-label feature vector viewed invariant classiﬁer class represented manifold allowable transformations. work viewed modern version model introduced term mapping units main diﬀerence models trained large datasets. brieﬂy review standard feature learning models section discuss relational feature learning section discuss extensions relational models relate complex cells energy models section practically standard feature learning models represented graphical model like shown figure model bi-partite network connects unobserved latent variables observable variables k=...k figure considered unobserved infer separately training case along model parameters training. graphical model shown ﬁgure represents dependencies components parameterized deﬁne model learning algorithm. large variety models learning algorithms parameterized ﬁgure including principal components mixture models k-means clustering restricted boltzmann machines principle used feature learning method recent quantitative comparison). hidden variables extract useful structure images capacity needs constrained. simplest form constraining dimensionality smaller dimensionality images. learning case amounts performing dimensionality reduction. become obvious recently useful applications over-complete representation constrain capacity latent variables instead forcing hidden unit activities sparse. figure follows symbolize fact capacity-constrained kept mind capacity constrained ways. common operations model training inference given image compute generation invent latent vector compute conﬁnes values reside ﬁxed interval. model well-known auto-encoder depicted figure learning amounts minimizing reconstruction error respect practice common enforce order reduce number parameters consistency sparse coding models. penalty term encourages sparsity latent variables. alternatively train auto-encoders de-noise corrupted version inputs achieved simply feeding corrupted inputs training turns auto-encoders de-noising auto-encoders show properties similar sparse coding methods inference like standard auto-encoder simple feed-forward mapping. showing inference again amounts linear mapping plus non-linearity. learning amounts maximizing average log-probability training data. since derivatives respect parameters tractable common approximate gibbs sampling order approximate them. leads hebbian-like learning rule known contrastive divergence training model parameters typically referred features ﬁlters practically methods yield gabor-like features trained natural images. advantage non-linear models rbm’s autoencoders stacking makes possible learn feature hierarchies practice common bias terms inference generation wjkzk parameter shall refrain adding bias terms avoid clutter noting that alternatively think homogeneous coordinates containing extra constant -dimension. computationally demanding. important local features make possible deal images diﬀerent size deal occlusions local object variations. given trained model common ways perform invariant recognition test images bag-of-features crop patches around interest points compute latent representation patch collapse representations obtain single vector zimage classify zimage using standard classiﬁer. several variations scheme including using extra clustering-step collapsing features using histogram-similarity place euclidean distance collapsed representation. convolutional crop patches image along regular grid; compute patch; concatenate descriptors large vector zimage; classify zimage using standard classiﬁer. also combinations schemes local features yield highly competitive performance object recognition tasks next section discuss recent approaches extending feature learning encode relations between opposed content within images. naive approach modeling relations images would perform sparse coding concatenation. hidden unit model would receive input projections image. detect particular transformation receptive ﬁelds would need deﬁned receptive ﬁeld modiﬁed transformation hidden unit supposed detect. input hidden unit receives tend high image pairs showing transformation. however input equally dependent images themselves. reason hidden variables akin logical or-gates accumulate evidence discussion). straightforward build content-independent detector allow multiplicative interactions variables. particular consider outer product one-dimensional binary images shown figure every component matrix constitutes evidence exactly type transformafigure diagonal contains evidence identity transformation. secondary diagonals contain evidence shifts. hidden unit pools diagonals detect transformations. hidden unit computes products. tion components like and-gates detect coincidences. since component equal corresponding pixels equal hidden unit pools multiple components much less likely receive spurious activity depends image content rather transformation. note pooling components amounts computing correlation output image transformed version input image. true real-valued data. figure shows alternative illustrations type model sub-ﬁgure shows hidden variable blend slice w··k parameter tensor. slice matrix connecting input pixel output-pixel. think matrix performing linear regression space stacked gray-value intensities known commonly warp. thus model whole thought deﬁning factorial mixture warps. alternatively input pixel thought blending slice wi·· parameter tensor. thus think model standard sparse coding model output image whose parameters modulated input image. turns model predictive conditional sparse coding model cases hidden variables take roles dynamic mapping units encode relationship content images. unit model gate connections variables model. shall refer type model gated sparse coding synonymously cross-correlation model. like standard sparse coding model needs include biases practice. model parameters thus consists three-way parameters wijk well single-node parameters could also include higher-order-biases like connect groups variables common like before shall drop bias terms follows order avoid clutter. simple graphical model gated sparse coding models tri-partite. standard sparse coding model bi-partite. inference performed almost standard sparse coding model whenever three groups variables observed. shows inference amounts computing output-component quadratic form deﬁned weight tensor w·j·. considering either ﬁxed also think inference simple linear function like standard sparse coding model. property typical models bi-linear dependencies despite similarity standard sparse coding model meaning inference diﬀers standard sparse coding meaning here transformation takes matrix function commonly represent vectorized images linear function warp. note representation linear function factorial. hidden variables make possible compose warp additively constituting components much like factorial sparse coding model makes possible compose image independent components. like standard sparse coding model useful applications assign number input quantifying well represented model. number useful calibrated typically achieved using probabilistic model. contrast simple sparse coding model training probabilistic gated sparse coding model slightly complicated dependencies conditioned discuss issue detail next section. particular note gated model like sparse coding model whose input vectorized outer-product standard learning criteria squared error obviously appropriate. recall think inputs modulating parameters. modulation case-dependent. learning therefore viewed sparse coding case-dependent weights. cost data-case contributes diﬀerentiating respect wijk standard sparse coding model. particular model still linear wrt. parameters. predictive learning therefore possible gradient-based optimization similar standard feature learning avoid iterative inference possible adapt various sparse coding variants like auto-encoders rbms conditional case. example obtain gated boltzmann machine changing energy function three-way energy note normalization only consistent goal deﬁning predictive model. possible deﬁne joint model makes training diﬃcult like standard training involves relational samples drawn conditional sampling another example turn auto-encoder relational auto-encoder deﬁning encoder decoder parameters linear functions learning essentially standard auto-encoder modeling particular model still directed acyclic graph simple back-propagation train model. figure illustration. forces parameters able transform directions give performance similar symmetrically trained fully probabilistic models like auto-encoder model trained gradient based optimization. apply task modeling second-order within-image features features encode pair-wise products pixel intensities. show achieved optimizing joint distribution using image input output contrast suggest hybrid monte carlo train joint. figure shows example gated boltzmann machine applied translations. model trained images showing random dots output image copy input image shifted random direction. center column plots figure visualizes inferred transformation vector ﬁeld. vectorﬁeld produced inferring transformation given image pair computing transformation inferred hiddens ﬁnding input-pixel output-position strongly connected rightcolumns plots show inferred transformation applied images analogy computing output-image given input image inferred transformation figure shows example transformations split-screen translations translations independent half bottom half image. illustrates model decompose transformations factorial constituting transformations. following discuss close relationship gated sparse coding models energy models. ﬁrst describe parameter factorization makes possible pre-process input images thereby reduce number parameters. number gating parameters roughly cubic number pixels assume number constituting transformations number pixels. easily highly over-complete hiddens. suggest reducing number factorizing parameter tensor three matrices component wijk given three-way inner product figure inferring motion direction test data. coherent motion across whole image. factorial motion independent diﬀerent image regions. plots meaning columns follows random test images random test images inferred ﬂow-ﬁeld test-image inferred output obtain similar expression energy gated boltzmann machine. shows factorization viewed ﬁlter matching inference group variables projected onto linear basis functions subsequently multiplied illustrated figure important note factorization reduces parameters projecting data onto lower-dimensional space computing multiplicative interactions claim found frequently literature. fact frequently chosen larger and/or factorization reduces number parameters restricting three-way connectivity. learning amounts ﬁnding basis functions deal restriction optimally. using factorization amounts allowing factor engage single multiplicative interaction. gated sparse coding models subjected factorization. training similar training unfactored model using chain rule diﬀerentiating example factored gated auto-encoder described virtually factored models introduced restriction single multiplicative interactions open research question degree less restrictive show empirically training factored model leads ﬁlter-pairs optimally represent transformation classes fourier-components translations polar variant fourier-components rotations. figures show examples ﬁlters learned translations aﬃne transformations split-screen translations independent bottom half image natural video. training ﬁlters rows bottom right used data-sets described model described ﬁlters resemble receptive ﬁelds found various cells visual cortex obtain split-screen ﬁlters generated data-set split-screen translations trained model described section provide analysis sheds light onto ﬁlters take form. energy models alternative approach modeling image motion disparities deployed monocularly too. main application energy models detection small translational motion image pairs. makes suitable biologically plausible mechanisms local motion estimation binocular disparity estimation. energy models detect motion projecting images onto phase-shifted gabor functions responses across images added squared. squared spatio-temporal responses yields response energy model. rationale behind energy model that since within-image gabor ﬁlter pair thought localized spatio-temporal fourier component squared components yields estimate spectral energy dependent phase thus large degree dependent content input images. ﬁlters within image need sine/cosine pairs commonly referred quadrature. detector local shift built using energy models tuned diﬀerent frequencies. turn energy responses estimate local translation example pick model strongest response pooling stable estimate suggest learning energy-like models data extending sparse coding model elementwise squaring operation followed linear pooling layer. contrast original energy model exactly ﬁlters pool over pooling weights learned along basis functions instead ﬁxed figure shows illustration type model applied image pair. ﬁgure shows type model viewed two-layer network hidden layer uses elementwise squaring nonlinearity. figure input ﬁlters learned various types transformation. top-left translation top-right rotation bottom-left split-screen translation bottom-right natural videos. ﬁgure next page corresponding output ﬁlters. ﬁlters orthogonal avoid degenerate solutions like training standard model approach known independent subspace analysis shall refer hidden layer nodes factors analogy hidden layer factored gbm. factored gated boltzmann machines shown yield state-of-the-art performance various motion recognition tasks figure output ﬁlters learned various types transformation. top-left translation top-right rotation bottom-left split-screen translation bottom-right natural videos. ﬁgure previous page corresponding input ﬁlters. learning energy models concatenation inputs closely related learning gated sparse coding models. wx·f denote weights connecting part concatenated input factor quadratic terms hidden unit activities gated sparse coding model. shall discuss detail below quadratic terms signiﬁcant eﬀect meaning hidden units. therefore also thought implement mapping units encode relations. years variety tricks recipes emerged simplify stabilize speed learning presence multiplicative interactions. approach used practically everyone ﬁeld normalize output ﬁlter matrices learning ﬁlter wx·f wy·f grow slowly maintain roughly length learning progresses. common achieve maintain running average average norm ﬁlters learning re-normalize ﬁlter norm every learning update. furthermore common connect top-level hidden units locally factors rather using full connectivity. theoretical discussion next section provides intuition local connectivity helps speed learning. slightly complicated approach hidden units populate virtual grid low-dimensional space connect hidden units factors neighboring hidden units connected overlapping sets factors. approach popular mainly context learning energy models finally common train models using image patches centered contrast normalized usually also whitened. show hidden variables learn detect subspace-rotations trained transformed image pairs. section showed transformation codes represent linear transformations shall restrict attention following transformations orthogonal identity matrix. words linear transformations pixel-space also known warp. note practically relevant spatial transformations like translation rotation local shifts expressed approximately orthogonal warp orthogonal transformations subsume particular permutations important fact orthogonal matrices eigen-decomposition complex eigenvalues absolute value multiplying complex number absolute value amounts performing rotation complex plane illustrated figure eigenspace associated also referred invariant subspace applying orthogonal warp thus equivalent projecting image onto ﬁlter pairs performing rotation within invariant subspace projecting back image-space. words decompose orthogonal transformation independent -dimensional rotations. well-known examples translations d-translation matrix contains ones along secondary diagonals zero elsewhere. eigenvectors matrix fourier-components rotation invariant subspace amounts phase-shift corresponding fourierfeature. leaves norm projections onto fourier-components constant well known property translation. interesting note imaginary real parts eigenvectors translation matrix correspond sine cosine features respectively reﬂecting fact fourier components naturally come pairs. commonly referred quadrature pairs literature. special case gabor features importance quadrature pairs allow detect translations independently local figure training gated sparse coding models equivalent detecting rotation angles invariant subspaces associated transformations amounts detecting multiple applications angle learning videos content images however property eigenvectors come pairs speciﬁc translations. shared transformations represented orthogonal matrix composed -dimensional rotations. term generalized quadrature pair refer eigen-features transformations. central observation analysis eigenspaces shared among transformations. eigenspaces shared transformations diﬀer angles rotation within eigenspaces. shared eigenspaces allow represent multiple transformations single features. example shared eigenspace fourier-basis shared among translations. wellknown observation follows fact circulant matrices size fourier-basis eigen-basis eigenspaces shared many transformation translation. obvious generalization local translations considered constituting transformations natural videos. another less obvious generalization spatial rotation. formally matrices share eigenvectors commute joint eigenspace. result extract particular transformation given image pair recovering angles rotation projections onto eigenspaces. consider real complex parts eigen-feature coordinates projection onto invariant subspace associated given trigonometric identity. equivalent computing inner product normalized projections words estimate angle rotation projections need product ﬁlter responses. note however normalizing projection amounts dividing squared ﬁlter responses operation highly unstable projection close zero. unfortunately case whenever images almost orthogonal invariant subspace. this turn means rotation angle cannot recovered given image image close axis rotation. view subspace-generalization well-known aperture problem beyond translation orthogonal transformations. normalization would ignore problem provide illusion recovered angle even aperture problem makes detection transformation component impossible. next section discuss overcome problem rephrasing problem detection task. features data contrast normalized projections depend well image pair represents given subspace rotation. value turn depend transformation content images thus output detector factors both presence transformation ability discern fact depends image content makes suboptimal representation transformation. however note conservative detector takes large value input image pair compatible transformation. therefore deﬁne content-independent representation pooling multiple detectors represent transformation respond diﬀerent images. note computing involves summing subspace dimensions also form pooling thus encoding subspace rotations requires types pooling. band-diagonal within-subspace pooling matrix appropriate across-subspace pooling matrix. furthermore following conditions need images contrast-normalized exists corresponding takes form expuf words ﬁlter pairs related rotations only. takes exactly form inference gated sparse coding model absorb within-subspace pooling matrix learning amounts identifying subspaces pooling matrix training multiview feature learning model thought performing multiple simultaneous diagonalizations transformations. data-set contains transformation class learning involves partitioning orthogonal warps commutative subsets simultaneously diagonalizing subset. note that practice complex ﬁlters represented learning two-dimensional subspaces form ﬁlter pairs. uncommon albeit possible learn actually complexvalued features practice. diagonalizing single transformation would amount performing kind canonical correlations analysis learning multi-view feature learning model thought performing multiple canonical correlation analyzes tied features. similarly modeling within-image structure setting would amount learning mixture tied weights. neural networks used implement linear transformation result training multi-view feature learning model simultaneous diagonalization linear transformation. interesting note condition implies ﬁlters normalized length. imposing norm constraint common approach stabilizing learning also common apply sigmoid non-linearity computing mapping unit activities output hidden variable interpreted probability. pooling multiple subspaces addition providing content-independent representations also help deal edge eﬀects noise well fact learned transformations exactly orthogonal. equivalent four quadratic terms. four quadratic terms equal squared norms projections onto invariant subspace. thus like norm projections contribute information discernibility transformations. makes energy response conservative cross-correlation response however peak response still attained images reside within detector’s invariant subspace projections rotated detectors preferred angle pooling multiple rotation detectors obtain equivalent energy response shows energy models applied concatenation images well-suited modeling transformations too. energy models cross-correlation models applied images. gated sparse coding modiﬁed contain cross-terms ones deemed relevant alternatively energy mechanism compute square concatenation images place close relation energy models gated sparse coding makes possible implement other. figure shows example ﬁlters energy model trained concatenated frames videos showing moving random dots. trained gated auto-encoder factors mapping units given concatenation frames. filters constrained -frame input shows random dots moving constant speed. speed direction vary across movies. since gated auto-encoder cross-correlation model multiplies sets ﬁlter responses same eﬀectively computes square thus implements energy model. absence within-image structure ﬁlters learn represent across-image correlations. thus predicted energy model turn implements cross-correlation model. figure depicts separately sets ﬁlters corresponding time-frames. shows model learns spatio-temporal fourier features selective speed frequency orientation. suggests squaring non-linearities example transfer function feed-forward network useful general tasks relations play role recognition tasks involve motion stereo. long term computing squares and/or cross-products could help reduce requirement large handengineered pipelines currently used solving correspondence problems tasks like depth inference. typically involve keypoint extraction descriptor extraction matching outlier-removal learning based system using complex cells able replace parts pipeline single homogeneous model trained data. also help explain visual cortex perform large variety tasks using single homogeneous module trained single type learning mechanism. interestingly invariant object recognition viewed correspondence problem goal match input observation invariant templates memory. discuss variation gated sparse coding model considered approach invariant recognition modeling mappings take images class labels. input model image output orthogonal encoding class label prediction amounts marginalizing figure implementing cross-correlation model energy model crosscorrelation model. sequence ﬁlters learned concatenation frames moving random dots. possible mappings. graphical model also equivalent class-conditional manifolds probability distributions inference feed-forward. model effectively transforms input canonical pose matched template represents object canonical pose. help explain similarity general ﬁlters allow invariant recognition allow selective recognition transformations. show swirly features similar rotation features figures emerge learning perform rotationally invariant recognition. showed similar features emerge feed-forward recognition models contain squaring non-linearities. common object recognition systems somewhat unrealistic trained recognize single static views objects. real biological systems movies objects constantly move around change relative pose. interesting note model computes squares cross-products could automatically learn associate object identity structure articulated motion simply trained multiple concatenated frames. using multiplicative interactions also related analogy making argued analogy making heart many cognitive phenomena interesting question degree analogy-making module could useful building block models higher-level cognitive capabilities. since gated sparse coding energy models trained standard even hebbian-like learning analogy-making require uncommon unusual machinery besides multiplicative interactions. squaring approximated using non-linearities discussion). possible research question type approximations computing squares cross-products advantageous computationally and/or plausible biologically. course squares could simulated using layer feed-forward network sigmoid activations however abundance matching correspondence tasks vision provide inductive bias favor genuine multiplicative interactions squares. another research question degree deviating exactly commuting transformations exactly orthogonal matrices hampers ability learn something useful. existing experiments suggest robustness quantitative analysis. conceivable could pre-process data-points related orthogonal matrices order make amenable energy cross-correlation model. interestingly seems this would transforming data high-dimensional sparse.", "year": 2011}