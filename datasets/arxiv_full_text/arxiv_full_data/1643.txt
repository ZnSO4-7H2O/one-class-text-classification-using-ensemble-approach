{"title": "Discovering Discrete Latent Topics with Neural Variational Inference", "tag": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "abstract": "Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.", "text": "topic models widely explored probabilistic generative models documents. traditional inference methods sought closedform derivations updating models however expressiveness models grows difﬁculty performing fast accurate inference parameters. paper presents alternative neural approaches topic modelling providing parameterisable distributions topics permit training backpropagation framework neural variational inference. addition help stick-breaking construction propose recurrent network able discover notionally unbounded number topics analogous bayesian non-parametric topic models. experimental results song lyrics newsgroups reuters news datasets demonstrate effectiveness efﬁciency neural topic models. probabilistic models inducing latent topics documents great success stories unsupervised learning. starting latent semantic analysis models uncovering underlying semantic structure document collection widely applied data mining text processing information retrieval. probabilistic topic models hdps provide robust scalable theoretically sound foundation document modelling introducing latent variables token topic assignment. either monte carlo variational techniques however topic models grown expressive order capture topic dependencies exploit conditional information inference methods become increasingly complex. especially apparent non-conjugate models deep neural networks excellent function approximators shown great potential learning complicated non-linear distributions unsupervised models. neural variational inference approximates posterior generative model variational distribution parameterised neural network. allows generative model variational network jointly trained backpropagation. models continuous latent variables associated particular distributions gaussians exist reparameterisations distribution permitting unbiased low-variance estimates gradients respect parameters inference network. models discrete latent variables montecarlo estimates gradient must employed. recently algorithms reinforce used effectively decrease variance improve learning work propose evaluate range topic models parameterised neural networks trained variational inference. introduce three different neural structures constructing topic distributions gaussian softmax distribution gaussian stick breaking distribution recurrent stick breaking process conditioned draw multivariate gaussian distribution. gaussian softmax topic model constructs ﬁnite topic distribution softmax function applied projection gaussian random vector. gaussian stick breaking model also constructs discrete distribution gaussian draw time employing stick breaking construction provide bias towards sparse topic distributions. finally recurrent stick breaking process employs recurrent neural network conditioned gaussian draw progressively break stick yielding neural analog dirichlet process topic model compared equation parameterise latent variable neural network conditioned draw gaussian distribution. carry neural variational inference construct inference network approximate posterior functions implemented multilayer perceptrons using gaussian prior distribution able employ re-parameterisation trick build unbiased low-variance gradient estimator variational distribution. without conjugacy updates parameters still derived directly easily variational lower bound. defer discussion inference process next section. introduce several alternative neural networks transform gaussian sample topic proportions deep learning energy-based function generally used construct probability distributions pass gaussian random vector softmax function parameterise multinomial document topic distributions. thus ggsm deﬁned neural topic models combine merits neural networks traditional probabilistic topic models. trained efﬁciently backpropagation scaled large data sets easily conditioned available contextual information. further probabilistic graphical models interpretable explicitly represent dependencies amongst random variables. previous neural document models neural variational document model belief networks document model neural auto-regressive document model replicated softmax explicitly modelled latent topics. evaluations range data sets compare models previously proposed neural document models traditional probabilistic topic models demonstrating robustness effectiveness. probabilistic topic models latent variables topic proportion document topic assignment observed word respectively. order facilitate efﬁcient inference dirichlet distribution employed prior generate parameters multinomial distribution document. conjugate prior allows tractable computation posterior distribution latent variables’ values. alternatives explored log-normal topic distributions extra approximation required closed form derivations. generative process represents topic distribution words given topic assignment number tokens document drawn another dirichlet distribution consider model parameter. hyper-parameter dirichlet prior total number words document marginal likelihood document collection random variables deﬁne breaks unit stick. case following khan transform modelling multinomial probability parameters modelling logits binomial probability parameters using gaussian latent variables. speciﬁcally conditioned gaussian sample breaking proportions generated applying sigmoid func rh×k−. starting tion sigmoid remainθk length taken probability category instance assume generated breaks remaining stick model proceeds break stick remaining stick broken hence different values alk= stick breaking construction illustrated figure distribution ggsb produces sequence binomial logits used break stick sequentially. frnn decomposed equivalent stick breaking function used gaussian stick breaking construction. here able dynamically produce logits break stick inﬁnitum. expressive power model sequences unbounded length still bounded parametric model’s capacity topic modelling adequate model countably inﬁnite topics amongst documents truncation-free fashion. assume ﬁnite number topics topic distribution words given topic assignment multi. introduce topic vectors rk×h word vectors generate topic distributions words therefore rk×v collection simplexes achieved computing semantic similarity topics words. following notation introduced section prior distribution deﬁned projection network generates document. here gaussian softmax ggsm gaussian stick breaking ggsb recurrent stick breaking grsb constructions ﬁxed length rnnsb. derive variational lower bound document log-likelihood according equation variational distribution approximating true posterior following framework neural variational inference introduce inference network conditioned observed document generate variational parameters estimate lower bound sampling practise reparameterise sample variational distribution q|d) term equation easily integrated gaussian kl-divergence. note that parameterisation network parameters shared across addition given sampled latent documents. variable integrated recurrent neural topic models models topic vectors rk×h predeﬁned computing topic distribution words construction model unbounded number topics however addition rnnsb generates topic proportions document must introduce another neural network rnntopic produce topics r∞×h dynamically avoid need truncate variational inference. comparison ﬁnite neural topic models topic vectors rk×h unbounded neural topic models topics r∞×h dynamically generated rnntopic order topics corresponds order states rnnsb. generation follows represents word vectors topic generated rnntopic figure illustrates neural structure rnntopic. unbounded topic models introduce truncationfree neural variational inference method enables model dynamically decide number active topics. assume current active number topics rnntopic generates ri×h step stick-breaking process variational lower bound document corresponds topic distribution words order dynamically increase number topics model proposes break stick split topic. case rnntopic proceeds next state generates topic rnnsb generates extra break stick. firstly compute likelihood increase brought topic across documents then employ acceptance hyper-parameter decide whether generate topic. previous proposed topic contributes generation words increase active number topics otherwise keep current unchanged. thus controls rate model generates topics. practise increase lower bound computed mini-batches model able generate topics current epoch ﬁnished. details algorithm described algorithm topic models documents modelled mixture topics word associated single topic latent variable. miao proposed neural variational document model implemented variational auto-encoder similar neural structure models. major difference nvdm employs softmax decoder generate words document conditioned unnormalised vector construction replace generative distribution distribution remove softmax function reduces variant nvdm model srivastava sutton interpret decoder weighted product experts topic model model topics explicitly. refer models directly assign topics words document models. also convert constructions document models employing softmax decoder. include models experimental evaluation section. topic models extensively studied variety applications document modelling information retrieval. beyond signiﬁcant extensions sought capture topic correlations model temporal dependencies discover unbounded number topics topic models extended capture extra context information time authorship class labels extensions often require carefully tailored graphical models associated inference algorithms capture desired context. neural models provide generic extendable option number works sought leverage these replicated softmax auto-regressive document model sigmoid belief document model variational auto-encoder document model topicrnn model however neural works explicitly capture topic assignments. recent work srivastava sutton also employs neural variational inference train topic models closely related work. model follows original formulation keeping dirichlet-multinomial parameterisation applies laplace approximation allow gradient back-propagate variational distribution. contrast models directly parameterise multinomial distribution neural networks jointly learn model variational parameters inference. nalisnick smyth proposes reparametertable perplexities document models test datasets. table compares results ﬁxed dimension latent variable achieved neural document models product experts neural variational document model hidden dimension constructing neural topic models benchmarks apply neural variational inference dropout applied output parameterising isotropic gaussian distribution. grid search carried learning rate batch size achieving held-out perplexity. recurrent stick breaking construction layer lstm cell constructing recurrent neural network. ﬁnite topic models maximum number topics models trained adam sample used neural variational inference. follow optimisation strategy miao alternately updating model parameters inference network. alleviate redundant topics issue also apply topic diversity regularisation carrying neural variational inference. details found appendix perplexity main metric assessing generalisation ability generative models. variational lower bound estimate document perplexity exp) following miao table presents test document perplexities topic models three datasets. amongst ﬁnite topic models gaussian softmax construction achieves lowest perplexity cases models signiﬁcantly better benchmark nvlda models. amongst selection unbounded topic models compare truncation-free model applies dynamically increase active topics gaussian stick breaking recurrent stick breaking constructions online variational neural variational inference models. lower section shows results unbounded topic models including truncation-free online topic model isation approach continuous latent variables beta prior enables neural variational inference dirichlet process. however taylor expansion required approximate divergence multiple draws kumaraswamy variational distribution. case easily apply gaussian reparametersation trick draw gaussian distribution. perform experimental evaluation employing three datasets song lyrics newsgroups reuters rcv-v news. ofﬁcial lyrics collection million song dataset training testing datapoints respectively. newsgroups corpus divided training testing documents rcv-v corpus larger collection training test cases reuters newswire stories. employ original vocabulary provided datasets processed stemming ﬁltering stopwords taking frequent words vocabularies. next evaluate neural network parameterisations document models implicit topic distribution introduced section table compares proposed neural document models benchmarks. according experimental results generalisation abilities models improved switching implicit topic distribution performance also signiﬁcantly better nvdm prodlda. hypothesise effect models needing infer topic-word assignments makes optimisation much easier. interestingly model performs better newsgroups topic settings. possibly fact apply linear transformations generate hidden variable breaking proportions gaussian draw applies recurrent neural networks produce sequence induces dependencies helps escape local minima. worth noting recurrent neural network uses parameters models. mentioned section variant nvdm applies topic word vectors construct topic distribution words instead directly modelling multinomial distribution softmax function simpliﬁes optimisation. necessary model explicit topic distribution words using implicit topic distribution lead better generalisation. demonstrate effectiveness stickbreaking construction figure presents average probability topic estimating posterior probability document newsgroups. number topics large enough dataset. figure shows topics higher probability evenly distributed. figure higher probability ones placed front small tail topics sparsity inducing property stick-breaking construction topics tail less likely sampled. also advantage stick-breaking construction apply rsb-tf non-parameteric topic model since model activates topics according knowledge learned data becomes less sensitive hyperparameter controlling initial number topics. figure shows impact test perplexity neural topic models maximum number topics increased. performance model gets worse maximum number topics exceeds stable even though number topics outstrips model requires. addition figure shows convergence process truncationfree model newsgroups. different initial number topics rsbtf dynamically increases number active topics achieve better variational lower bound. training perplexity keeps decreasing rsb-tf activates topics. numbers active topics stabilise convergence point approaching hence non-parametric model rsb-tf sensitive initial number active topics. addition since quality discovered topics directly reﬂected perplexity evaluate topic observed coherence normalised point-wise mutual information table shows topic observed coherence achieved ﬁnite neural topic models. according results appear signiﬁcant difference topic coherence amongst neural topic models. observe npmi scores former topics stick breaking order figure test perplexities neural topic models varying maximum number topics newsgroups dataset. truncation-free dynamically increases active topics dashed line represent test perplexity reference ﬁgure. figure convergence behavior truncation-free model different initial active topics newsgroups. dash lines represent corresponding active topics. plausible stickhigher latter ones. breaking construction implicitly assumes order topics former topics obtain sufﬁcient gradients update topic distributions. likewise present results obtained neural document models implicit topic distributions. though topic probability distribution words exist could rank words positiveness connections words dimension latent variable. interestingly performance document models signiﬁcantly better topic model counterparts topic coherence. results rsb-tf presented fact number active topics dynamic makes models directly comparable others. demonstrate quality topics produce t-sne projection estimated topic proportions document figure figure t-sne projection estimated topic proportions document newsgroups. vectors learned model topics color represents group different groups dataset. paper introduced family neural topic models using gaussian softmax gaussian stickbreaking recurrent stick-breaking constructions parameterising latent multinomial topic distributions document. help stick-breaking construction able build neural topic models exhibit similar sparse topic distributions found traditional dirichlet-multinomial models. exploiting ability recurrent neural networks model sequences unbounded length present truncation-free variational inference method allows number topics dynamically increase. evaluation results show neural models achieve state-of-the-art performance range standard document corpora. bertin-mahieux thierry ellis daniel p.w. whitman brian lamere paul. million song dataset. proceedings international conference music information retrieval khan mohammad emtiyaz mohamed shakir marlin benjamin murphy kevin stick-breaking likelihood categorical data analysis latent gaussian models. proceedings aistats rosen-zvi michal grifﬁths thomas steyvers mark smyth padhraic. author-topic model authors proceedings conference documents. uncertainty artiﬁcial intelligence auai press wang xuerui mccallum andrew. topics time non-markov continuous-time model topical trends. proceedings sigkdd international conference knowledge discovery data mining issue exists probabilistic neural topic models redundant topics. neural models straightforward regularises distance topic vectors order diversify topics. following apply topic diversity regularisation carrying neural variational inference. cosine distance measure distance topics ||ti||·||tj|| mean angle pairs arccos variance following topic diversity regularisation variational objective hyper-parameter regularisation empirically though practise diversity regularisation provide signiﬁcant improvement perplexity helps reduce topic redundancy easily applied topic vectors instead simplex full vocabulary.", "year": 2017}