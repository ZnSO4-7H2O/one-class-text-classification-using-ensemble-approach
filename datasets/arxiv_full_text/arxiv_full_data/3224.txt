{"title": "Bethe Learning of Conditional Random Fields via MAP Decoding", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Many machine learning tasks can be formulated in terms of predicting structured outputs. In frameworks such as the structured support vector machine (SVM-Struct) and the structured perceptron, discriminative functions are learned by iteratively applying efficient maximum a posteriori (MAP) decoding. However, maximum likelihood estimation (MLE) of probabilistic models over these same structured spaces requires computing partition functions, which is generally intractable. This paper presents a method for learning discrete exponential family models using the Bethe approximation to the MLE. Remarkably, this problem also reduces to iterative (MAP) decoding. This connection emerges by combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on a convex dual objective which circumvents the intractable partition function. The result is a new single loop algorithm MLE-Struct, which is substantially more efficient than previous double-loop methods for approximate maximum likelihood estimation. Our algorithm outperforms existing methods in experiments involving image segmentation, matching problems from vision, and a new dataset of university roommate assignments.", "text": "many machine learning tasks formulated terms predicting structured outputs. frameworks structured support vector machine structured perceptron discriminative functions learned iteratively applying efﬁcient maximum posteriori decoding. however maximum likelihood estimation probabilistic models structured spaces requires computing partition functions generally intractable. paper presents method learning discrete exponential family models using bethe approximation mle. remarkably problem also reduces iterative decoding. connection emerges combining bethe approximation frankwolfe algorithm convex dual objective circumvents intractable partition function. result single loop algorithm mle-struct substantially efﬁcient previous double-loop methods approximate maximum likelihood estimation. algorithm outperforms existing methods experiments involving image segmentation matching problems vision dataset university roommate assignments. learning parameters markov random ﬁeld conditional random ﬁeld ubiquitous problem machine learning related ﬁelds. often parameters learned regularized maximum likelihood estimation prediction performed maximum a-posteriori marginal inference log-likelihood concave principle maximized gradient ascent. however requires repeatedly computing gradients log-partition function general intractable. circumvent difﬁculty using surrogates log-partition function approximating partition function using sampling alternatively avoid likelihoods entirely methods structured perceptron structured support vector machines rely solver methods often quite accurate typically faster approximate since relaxations thereof performed quickly using sophisticated combinatorial solvers. using solvers black boxes mapbased training methods also offer users attractive abstraction learning problem optimization algorithm. hand remains primary goal many practitioners since yield superior predictive accuracy offers parameter values increased interpretability statistical properties supports testtime marginal inference. work introduce mle-struct novel approximate algorithm also requires access solver. combine bethe-style convex free energies frank-wolfe method naive application approximate would perform approximate marginal inference using repeated calls experiments sontag jaakkola marginals perform single gradient step parameters. double-loop algorithm requires signiﬁcant number solver calls especially accurate answers required. approach achieves fast learning avoiding costly double loop structure. first employ generic reweighted entropy approximation technique yields convex bethemodels include potts ising models well log-linear distributions matchings given observations corresponding feature vectors form would like learn maximizing log-likelihood observations plus quadratic regularizer. central challenge maximum likelihood estimation computing partition function sample iteration. work approximate partition function order make learning problem tractable. begin bethe free energy standard approximation so-called gibbs free energy motivated ideas statistical physics. approximation generalized include different counting numbers result alternative entropy approximations focus restricted counting numbers result family convex reweighted free energies. style surrogate likelihoods underlying undirected graphical model. then construct constrained convex dual problem approximate maximum likelihood objective. demonstrate approximate dual problem minimized efﬁciently using linear subproblems solved part algorithm formulated separate approximate exact inference tasks training example. finally introduce technique accelerate line search subroutine precomputing certain data-dependent terms. also perform test-time marginal inference using procedure sontag jaakkola therefore train test time interact underlying problem structure using routine. allows design fast approximate learning prediction algorithms wide variety settings efﬁcient approximate/exact solvers exist bipartite/general matching b-matching problems pairwise binary graphical models planar ising models external ﬁeld among others. apply method learn pairwise binary crfs distributions matchings bipartite general graphs. method provides good predictive performance often solving approximate problem signiﬁcantly faster fewer numerical instabilities approximate methods. also apply method dataset housing preferences roommate assignments university students predict good freshmen roommate assignments. consider conditional random ﬁelds where addition samples discrete space also observe feature vectors case conditional probability sample form graph divided smaller subgraphs partition function efﬁciently computed exactly approximately. results combined approximate true partition function. subproblems typically much smaller procedure quite fast inaccurate pieces small bipartite matchings obtain unbiased noisy gradient log-likelihood utilizing perfect sampler algorithm huber petterson approach ranking graph matching problems limited themselves vertices observation required samples. domke proposed performing using small ﬁxed number iterations part procedure estimate gradient. however converging quickly resulting procedure fail converge. vishwanathan proposed improving convergence outer loop using accelerated gradient methods. methods rely double loop. setting recovers typical bethe free energy approximation. reweighting parameters always chosen approximate free energy convex example treereweighted belief propagation algorithm chooses reweighting parameters correspond edge appearance probabilities collection spanning trees. consider convex dual reformulation applies convex free energies yields fast learning algorithm. ﬁrst note concave variables maximized convex variables minimized variables constrained compact domain. thus invoke sion’s minimax theorem reverse operators. next analytically solve optimal terms ﬁxed setting gradient respect equal zero yields heinemann globerson investigated unregularized likelihoods form mrfs demonstrated convexity bethe free energy guarantees empirical marginals satisfy moment matching condition empirical marginals minimize bethe free energy maximizes approximate log-likelihood. moment matching necessarily achieved general mrfs reweighted approximation convex wainwright investigated learning pairwise binary graphical models. observed parameters learned robust addition data learned robustness convex free energy approximations learning made theoretically precise compute partition function iterative procedure solving necessarily requires double-loop algorithm expensive large datasets. existing work bethe learning sought design efﬁcient approximate learning algorithms. sutton mccallum proposed piecewise training scheme whereby convex free energies obtained using reweighting techniques above whenever graph tree standard bethe free energy convex exact. principle similar argument made convex approximation partition function though focus convex bethe-style approximations work. objective dual maximum entropy problem recent work approximate focused different families entropy approximations ganapathi also followed maximum entropy approach approximate problem. proposed approximating entropy objective using bethe entropy approximation speciﬁcally avoided convex entropy approximations. unfortunately results non-convex optimization problem general authors concaveconvex procedure. recent work approximated problem using convex free energies consider maximum entropy approach since constraints separable across training examples decouples independent linear programs solved parallel. depending speciﬁc application purely combinatorial methods reweighted message-passing algorithms provide faster space efﬁcient alternatives generic solvers. note could solved projected gradient algorithms efﬁciently since projection onto local marginal polytopes tractable. despite ability perform parallel still prohibitive large sample sizes. convex optimization problems separable constraint spaces lacostejulien propose block-coordinate algorithm bcfw procedure performs iteration randomly selected leaves remaining coordinates untouched. bcfw requires less work iteration asymptotic rate convergence remains standard algorithm block coordinate approach known outperform svmstruct problem. technical details concerning convergence problem including methods bound convergence rate found appendix bcfw versions algorithm mle-struct described algorithm .line search accelerated precomputing quadratic terms discussed many applications computing marginals useful test time well learning. fortunately perform marginal inference thus maintaining ability interact underlying model solver. speciﬁcally approximate reweighted bethe marginals obtained minimizing respect convex problem suitable technique ﬁrst used sontag jaakkola using generic solver map. appendix provide experiments crfs deﬁned bipartite matchings demonstrating favorable accuracy speed fw-based inference versus algorithm huang jebara instance perturb-and-map framework designed speciﬁcally matchings outperforms perturb-and-map terms accuracy convergence speed. minimize objective since bethe entropy convex matchings compare purely terms speed. preferable regimes except apply mle-struct framework variety exponential family models deﬁned different combinatorial structures including grid crfs image segmentation bipartite matchings vision applications general perfect matchings university roommate assignment problem. crfs inference intractable efﬁciently solve relaxation equivalent inference local polytope qpbo means estimated pseudomarginals globally consistent procedure still yield accurate predictions matching problem efﬁcient max-ﬂow solvers obtain exact solutions case estimated pseudomarginals globally consistent. appendix details data sources feature extraction machine setup. adjacency matrix perfect matching weighted adjacency matrix partition function. entry function edgewise features. formulation relaxed distributions matchings allowing correspond adjacency matrix matching. bipartite partition function permanent matrix edge weights thus p-hard compute although partition function computed given accuracy using fully polynomial randomized approximation scheme algorithms impractical graphs signiﬁcant size. practice unknown must learned data. learn generative model estimating directly conditional model ﬁrst assuming linear combination feature maps learning weights. concreteness suppose features feature matrix model parameters theorem proven appendix inclusion implies also convex marginal polytope. generalizes earlier known results convexity bipartite perfect matchings convexity bethe entropy availability high-quality maximum-weight matching solvers algorithm well-suited approximate task. derivation speciﬁc form matchings technique making associated line search particularly efﬁcient precomputing certain data-dependent terms found appendix begin synthetic experiment using ﬂexibility mle-struct analyze accuracy various entropy approximations matchings. sample bipartite matchings distribution explore choices weight matrix high regime off-diagonals diagonals regime off-diagonal on-diagonal. problems small enough compute exact partition functions gradients ryser’s algorithm hence perform exact gradient descent. also evaluate true likelihood estimates. algorithm called result bethe estimator. addition setting problem guarantees concave entropy approximation upper bound partition function also algorithm setting denote result estimator. figure displays average regularized log-likelihood estimator higher better exact curve upper bound. high regimes bethe estimator superior estimator. reweighted entropies chosen known perform poorly estimators true partition function compared belief propagation. interestingly although objective values estimates different case regime estimation methods produce likelihood. figure approximate assignment produced learned model. edges incorrectly matched. middle pseudomarginals correctly predicted edge. correct edge high probability others probability bottom pseudomarginals wrongly predicted edge. edges nontrivial probability model forced pick picked wrong one. high problem average true regularized log-likelihood evaluated exact well parameters maximize bethe approximate log-likelihoods. higher better exact curve upper bound. data nearly probable bethe estimator exact mle. high problem optimal values regularized true approximate log-likelihoods bethe approximations. true likelihood always bounded approximations. estimator regularized bethe estimator. plot quantities figure also obtain upper lower bounds using inference compute since already available upon convergence algorithm appendix shows results. apply bipartite matching model graph matching problem arising computer vision house hotel image sequences. follow setup caetano data consist frames house separate frames hotel rotated ﬁxed angle predecessor. frame hand-labeled landmark points. consider pairs images ﬁxed number frames apart divide training validation testing sets following splits caetano measure average hamming error predicted matching ground truth. compare algorithm linear+learning method caetano parameters linear model using features algorithm hinge loss objective. results experiments reweighting parameters described figure subsequence chose regularization parameter cross-validation. methods perform comparably method slightly better houses method caetano slightly better hotels. also compared performance algorithm different reweighting parameters figure shows results house data various choices observed little difference test error varies conﬁrmed synthetic well real data. result tune different data/problem setups. figure illustrates advantage learning probabilistic model discriminative model pseudomarginals indicate model’s conﬁdence prediction. many cases algorithm made wrong prediction edges incident speciﬁc node relatively high pseudomarginal probabilities. cases errors completely unfounded. similar image parts matched albeit incorrectly. algorithm permits fast simple approximate problems previously quite difﬁcult. namely found using standard approaches compute marginals standard double-loop approach unstable problem often failed converge taking several gradient steps. later sections juxtapose algorithm alternative approximate approaches. many undergraduate institutions assign ﬁrst-year students roommates based questionnaire responses allow returning students pick roommates subsequent years. therefore observed roommate matchings returning students train model students’ preferences. model used administration assign ﬁrst-year students roommates along with. obtained anonymized dataset campus housing room assignments questionnaire responses undergraduate students major institution years used data train data test. prune students live campus housing assigned single rooms remaining students thus assigned roommate questionnaire data consists binary features ordinal features levels each. pair students questionnaire question created feature absolute differences many interaction features consisted indicator feature possible pair answers questionnaire questions. simplicity assumed symmetric interactions. student pair weighted score matching linear combination features. model using mle-struct. table lists largest coordinates distance features ordinal ordinal questionnaire responses. here negative values indicate closer agreement required. smoking personality sleeping habits require strongest agreement. results details feature appendix effectively performing multiclass classiﬁcation classes features expect high accuracy terms hamming error. instead consider use-case model reject roommate assignments. evaluate this learned form cost matrix features test year entries cost matrix scores binary classiﬁer. plot curves figure demonstrate gains random guesses constant baseline distance features interactions. particular algorithm dominates false-positive regime graph. competitors also evaluated structured perceptron structured using decoder even extensive parameter tuning generalize well test data obtained test aucs worse constant baseline. next study binary image segmentation problem weizmann horses dataset formulate pairwise binary model variable pixel indicating whether part horse. used features resized images obtained domke kept split training testing images. results grid crfs approximately nodes. initial experiments tried naive double-loop method subgradient descent using belief propagation compute subgradients log-partition function. method slow unstable even small real problems. instead compared algorithm method proposed domke solves using small ﬁxed number iterations approximate partition function thereby gradient approximate likelihood. outer loop runs l-bfgs convergence. limiting number inner iterations algorithm’s efﬁciency burdens user another tuning parameter. figure compare variants algorithm three variants algorithm domke terms objective value test error time. methods optimize objective—we optimize dual optimizes primal. parameterization matches setting published result. mle-struct curves result applying block-coordinate version algorithm obtained fastest convergence without using linesearch. mle-struct-wavg uses algorithm evaluates test error using weighted average iterates described lacoste-julien curve averaged iterates substantially smoother mlestruct curve quickly attains test error. domkex curves result running algorithm inner loop iterations. method guaranteed converge global optimum ﬁnite practitioner must algorithm sequence increasing values conﬁrm convergence correct value. contrast algorithm requires single early iterations algorithm achieves lowest objective value test error. algorithm attains test error even objective value relatively optimal. phenomenon result dual formulation iteratively move minimize objective value compute optimal linear function initially inaccurate contributing large objective value much lower dimensional errors cancel computing resulting good predictions nevertheless. contrast method domke iteratively moves value computes optimal value using trw. prior convergence enable better training data expense accurate estimation effect readily apparent visualizing predicted marginals test figure domke method takes minutes complete iteration. parameters iteration nowhere near evidenced ﬁrst early portion objective value plot mean hamming loss sample marginal estimates point used local intensity data light regions classiﬁed horse dark regions classiﬁed horse. contrast time bcfw method already iterations made passes training data essentially recovers correct segmentation mean hamming loss algorithms produce comparable visualizations though normalized hamming error sample domke gets takes domke converge according internal criteria attaining ﬁnal test error image. maximum likelihood estimation discrete exponential family models replacing gibbs free energy convex free energy approximation leads concaveconvex saddle point problem. shown adding quadratic regularizer enables closed-form maximization leaving single convex minimization problem solved efﬁciently using frank-wolfe algorithm. scale large datasets using block-coordinate frank-wolfe rapidly achieve test error solving dual objective. method accomplishes approximate using simple wrapper around black-box solver. previously practitioners either employed expensive double-loop procedures abandoned resorting structured svms perceptrons. method competitive max-margin map-based estimation methods terms prediction error faster competing methods minimizing test error simple implement. future work extend method combinatorial problems incorporate structure learning regularization handle latent variable models single minimax problem. references bayati borgs chayes zecchina belief propagation weighted b-matchings arbitrary graphs relation linear programs integer solutions. siam discrete math. borenstein ullman class-speciﬁc top-down jerrum sinclair vigoda polynomial-time approximation algorithm permanent matrix nonnegative entries. kolmogorov blossom implementation minimum cost perfect matching algorithm. math. prog. comp. following material used provide details techniques employed paper. part presents details experimental setup additional results. part discuss convergence properties problems. part prove convexity bethe free energy general matchings. part derives instance algorithm matchings part derives linear crfs. then present full derivation dual objective line search objective learning graph matchings. presented terms matrix-valued terms facilitates easy implementation since solvers operate matrices. matchings likelihood lower bounds true likelihood bethe likelihood upper bounds true likelihood parameter values. thus obtain two-sided bounds likelihood trw) likelihood figure displays results. approximate likelihoods computed using inference using procedure described sec. exact likelihood computed using ryser’s algorithm feasible small problem. timing experiments performed dedicated core .ghz intel xeon machine physical running ubuntu matlab computations restricted single core experiments time. algorithms implemented matlab interfacing combinatorial solvers written c++. downloaded code authors’ websites implemented matlab extensions respectively. obtained original experiment scripts correspondence author. proﬁle item description weight generally at... generally wake at... rising sophomore cleanliness smoking sleeping habits overnight guests personality usual study hours study location study audio/visual single-sex ﬂoor single-sex ﬂoor allow brownstone dataset obtained major university three year period anonymized dataset consists roommate assignments pairs students three years. addition students required complete brief housing survey asked preferences terms cleanliness sleeping schedule habits personality study preferences etc. questionnaire data consists binary features ordinal features levels each. pair students questionnaire question created feature absolute differences several interaction indicator features possible pair answers questionnaire questions. simplicity assumed symmetric interactions. student pair weighted score matching linear combination features. learned weights distance features relative rankings described figure using log-linear model weights interpreted log-odds ratio unit increase absolute distance qualitative observations results. first single-sex ﬂoor rising sophomore allow brownstone received relatively weights indicating perhaps data noisy respect survey responses. second personality smoking bedtime among strongest predictors successful match cleanliness study hours study location among least important. comparing bcfw algorithm structured employed publicly-available code authors. tried regularization parameter lambda values range best-performing conﬁguration achieved hamming error outperforms random guessing signiﬁcantly underperforms model trained mle. curvature stronger notion function’s geometry lipschitz parameter since afﬁne-invariant like entire frank-wolfe algorithm curvature differentiable function given objective function curvature heavily inﬂuenced entropy term curvature quadratic piece simply constant depends φ’s. unfortunately curvature unbounded approaches integer points local polytope entropy approximation becomes arbitrarily steep. therefore worst-case convergence rate problem unbounded. order obtain bounded curvature could strengthen constraints marginal polytope requiring ﬁxed constant tends zero tends towards hence long optimal pseudomarginals strictly inside modiﬁed always guaranteed linear rate convergence optimum. pseudomarginals produced often strictly inside constraints typically issue practice. order appropriate could example bounds pseudomarginals proposed mooij kappen bounds obtained running bp/rbp ﬁxed number iterations. appropriate selected interval contains bounds mooij kappen adding additional constraints expensive necessary practice. pseudomarginals steepness becomes unmanageable components close iterates algorithm never close boundary effective curvature term reasonable. course optimal pseudomarginals reweighted approximation close boundary large neighborhood solution expect fast convergence. rough estimate distance boundary entropy pseudomarginals experience shown algorithm converges faster true pseudomarginal distribution higher entropy. appendix argue bethe free energy matching problem convex general graphs. convexity bethe approximation bipartite matching problem investigated experimentally huang jebara proven vontobel argument holds choice reweighting parameters simplicity argue case general case similar theorem vontobel entropy polytope approximations formulated follows. appendix describe conditional random ﬁeld perfect matchings formulate approximate learning problem context describe linesearch procedure used part algorithm. general conditional random ﬁeld features arbitrary functions produce model whose solution maximum-weight perfect matching require features linear since denotes presence absence edge coefﬁcient ought depend data items therefore feature k’th feature linear function coefﬁcients given applying single function every pair rows features total. write observation thus replace regularizer. note n-vector reweighting parameter entries using variational formulation free energy write maximum bethe likelihood problem minimax problem analytically reduce convex program linear constraints. begin second line justiﬁed minimizations separable operators commute. cost must minimize larger product space later problem. last line follows sion’s minimax theorem minimization domain compact convex objective convex minimization variable concave maximization variable theorem requires compact domain remain unconstrained. thus concave function attains maximum stationary point e.g. λ−g. moreover strictly concave maximum unique. plugging simplifying procedure requires changes switching complete bipartite graphs general graphs. equations steps hold replace biadjacency feature matrices adjacency features permutation matrices matrices representing perfect matchings. technical caveats. first general graphs need able allow zero. occur either edge neighbors possible perfect matching linked. cases simply clamp zero. similarly edges occur every perfect matching need discover a-priori clamp one. second unlike bipartite matching initialization non-trivial since neighbors different every cannot choose integral local marginal polytope initial point since curvature inﬁnite there. instead every edge graph matching contains edge matching solving series matching problems. average matchings obtain initial feasible point. work conditional random ﬁeld labels graph standard overcomplete parameterization. indicator vector state node indicator matrix state edge also treat vector convenient. denote element matrix vector parentheses. node feature vector edge feature vector. implicitly feature vectors derived applying function input vector refer elements vector learn linear node edge parameters replace parameterized surrogate likelihood interpolates bethe approximations. variational formulation local polytope note bethe approximation convex setting grid mrfs edge probability appearing spanning tree edge. ynyn µnn) )/µnµn)] mutual information variables ynyn µnn) µnn) singleton pairwise entropies. used identity implicitly used pairwise marginalization constraints using mutual information identity gradients valid local polytope—a fact important remember optimizing. eliminated matrix reveals objective quadratic form gram matrices obtained vertically stacking obtained vertically stacking emem matrices obtained matrices whose )’th entries given whose )’th entry given objective quadratic simplify signs later. write objective block-coordinate frank-wolfe update gradient perform linesearch without computing full inner products denote step size denote step direction sample submatrices containing rows sample precisely number rows nonzero. suppose move point figure compare frank-wolfe bipartite perfect matching perturb-and-map algorithm using code obtained authors. plot distance approximate marginals exact marginals computed brute force v.s. number calls maximum-matching solver. bipartite graph nodes side i.i.d. edge weights distributed unif inverse temperatures algorithm large number calls range temperatures order identify affect temperature algorithms’ errors results aggregated random graphs. overall bethe approximation provided substantially accurate perturb-and-map converges quickly. however suggests changes temperature affect algorithms’ approximation accuracies differently. proposed algorithm bipartite perfect matching belief propagation algorithm huang jebara minimize objective polytope focus speed-accuracy trade-offs algorithms. table ’rand-n‘ refers random complete bipartite graphs nodes side i.i.d. edge weights unif. ‘lda-’ experiment aligns topics different runs gibbs sampler topic model topics. results averages graphs. algorithms range termination tolerances order obtain various speed-accuracy points. then range distances true bethe marginals compute time necessary achieve speciﬁed error. table presents ratio computation time gives good accuracy quickly faster). expected algorithm faster slow converge within tight error tolerances. speed-accuracy trade affects ﬁrst order methods still advantageous many cases including large-scale applications optimizing beyond statistical error problem pointless. huang jebara approximating permanent belief propagation. arxiv preprint arxiv. jaggi revisiting frank-wolfe projection-free sparse convex optimization. icml swersky zemel efﬁcient feature learning using perturb-and-map. nips workshop perturba-", "year": 2015}