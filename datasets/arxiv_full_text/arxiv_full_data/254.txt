{"title": "Acquiring Common Sense Spatial Knowledge through Implicit Spatial  Templates", "tag": ["cs.AI", "cs.CL", "cs.CV", "stat.ML"], "abstract": "Spatial understanding is a fundamental problem with wide-reaching real-world applications. The representation of spatial knowledge is often modeled with spatial templates, i.e., regions of acceptability of two objects under an explicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with prior work that restricts spatial templates to explicit spatial prepositions (e.g., \"glass on table\"), here we extend this concept to implicit spatial language, i.e., those relationships (generally actions) for which the spatial arrangement of the objects is only implicitly implied (e.g., \"man riding horse\"). In contrast with explicit relationships, predicting spatial arrangements from implicit spatial language requires significant common sense spatial understanding. Here, we introduce the task of predicting spatial templates for two objects under a relationship, which can be seen as a spatial question-answering task with a (2D) continuous output (\"where is the man w.r.t. a horse when the man is walking the horse?\"). We present two simple neural-based models that leverage annotated images and structured text to learn this task. The good performance of these models reveals that spatial locations are to a large extent predictable from implicit spatial language. Crucially, the models attain similar performance in a challenging generalized setting, where the object-relation-object combinations (e.g.,\"man walking dog\") have never been seen before. Next, we go one step further by presenting the models with unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging word embeddings enables the models to output accurate spatial predictions, proving that the models acquire solid common sense spatial knowledge allowing for such generalization.", "text": "computational cognitive models often handle spatial representations spatial templates regions acceptability objects explicit spatial preposition below left contrary previous work conceives spatial templates explicit spatial language extend concept implicit spatial language i.e. relationships— generally actions—that explicitly deﬁne relative spatial conﬁguration objects implicitly words implicit spatial templates capture common sense spatial knowledge humans possess explicit language utterances. predicting spatial templates implicit relationships notably challenging explicit relationships. firstly whereas tens explicit spatial prepositions exist thousands actions entailing thus drastic increase sparsity combinations. secondly complexity task radically increases implicit language. precisely explicit spatial prepositions highly deterministic spatial arrangements unequivocally implies object relatively lower object) actions generally not. e.g. relative spatial conﬁguration object clearly distinct action same. contrarily relationships jumping highly informative spatial template i.e. object lower position object. hence unlike explicit relationships predicting spatial layouts implicit spatial language requires spatial common sense knowledge objects actions interaction suggests need learning compose triplet whole instead learning template relationship. spatial understanding fundamental problem widereaching real-world applications. representation spatial knowledge often modeled spatial templates i.e. regions acceptability objects explicit spatial relationship contrast prior work restricts spatial templates explicit spatial prepositions extend concept implicit spatial language i.e. relationships spatial arrangement objects implicitly implied contrast explicit relationships predicting spatial arrangements implicit spatial language requires signiﬁcant common sense spatial understanding. here introduce task predicting spatial templates objects relationship seen spatial question-answering task continuous output present simple neural-based models leverage annotated images structured text learn task. good performance models reveals spatial locations large extent predictable implicit spatial language. crucially models attain similar performance challenging generalized setting object-relation-object combinations never seen before. next step presenting models unseen objects scenario show leveraging word embeddings enables models output accurate spatial predictions proving models acquire solid common sense spatial knowledge allowing generalization. provide machines common sense major long term goals artiﬁcial intelligence. common sense knowledge regards knowledge humans acquired lifetime experiences. crucial language understanding content needed correct understanding expressed explicitly resides mind communicator audience. addition humans rely common sense knowledge performing variety tasks including interpreting images navigation reasoning name few. representing understanding spatial knowledge fact imperative agent navigates physical world. task predicting relative spatial locations objects given structured text input introduce simple neural-based models trained annotated images successfully address challenges implicit spatial language discussed above. quantitative evaluation reveals spatial templates reliably predicted implicit spatial language—as accurately explicit spatial language. also show models generalize well templates unseen combinations e.g. predicting without exposed scene before tackling thus challenge sparsity. furthermore leveraging word embeddings models correctly generalize spatial templates unseen words e.g. predicting without ever seen elephant before. since word embeddings capture attributes objects reasonably expect embeddings informative spatial behavior objects i.e. likelihood exhibiting certain spatial patterns respect objects. instance without ever seen boots sandals model correctly predicts template inferring that since boots similar sandals must worn location person’s body. hence model leverages acquired common sense spatial knowledge generalize unseen objects. furthermore provide both qualitative visualization predictions analysis learned weights network provide insight spatial connotations words revealing fundamental differences implicit explicit spatial language. rest paper organized follows. sect. review related research. sect. ﬁrst introduce task predicting spatial templates present simple neural models. then sect. describe experimental setup. sect. present discuss results. finally sect. summarize contributions article. spatial processing drawn signiﬁcant attention cognitive artiﬁcial intelligence communities specifically spatial understanding essential tasks involving text-to-scene conversion robots’ understanding natural language commands robot navigation. spatial templates. earlier approaches predominantly considered rule-based spatial representations contrast malinowski fritz propose learning-based pooling approach retrieve images given queries form learn parameters spatial template explicit spatial preposition computes soft spatial objects relationship. e.g. object left referent object obtains high score left template right template. contrary them consider implicit spatial language instead explicit. additionally build spatial template relationship build template combination allowing template determined interaction/composition subject relationship object instead relationship alone. additionally model malinowski fritz output spatial arrangements objects perform predictions generalized relationships. leveraging spatial knowledge tasks. shown knowledge spatial structure images improves task image captioning authors manually annotate images geometric relationships objects show rule-based caption generation system beneﬁts knowledge. contrast work interest lies predicting spatial arrangements objects text instead generating text given images. furthermore employ small domain actions goal learn frequent spatial conﬁgurations generalize unseen rare objects/actions spatial knowledge also improved object recognition authors mine texts labeled images obtain spatial knowledge form object co-occurrences relative positions. knowledge represented graph random walk algorithm graph results ranking possible object labellings. contrarily representations spatial knowledge neural network based primarily used object recognition furthermore predict spatial templates. common sense spatial knowledge. yatskar ordonez farhadi propose model extract common sense facts annotated images textual descriptions using co-occurrence statistics among point-wise mutual information facts include spatial relationships result symbolic representation common sense knowledge form relations objects logically entail relational facts. method also extracts common sense knowledge images text predicts spatial templates. parikh leverage common sense visual knowledge tasks ﬁll-in-the-blank visual paraphrasing. compute likelihood scene identify likely answer multiple-choice textual scene descriptions. contrast focus solely spatial information—and assuring correctness spatial predictions—rather scene understanding. image generation. although models generate images text exist focus quite distant producing spatially sensible images generally meant generate single object rather placing relative objects. proposed models build mapping input output task propose simple neural models architecture identical input representation layers differ output loss function. input representation layers. embedding layer maps three input words respective d-dimensional vectors wsws wrwr wowo r|vs| r|vr| r|vo| one-hot encodings rd×|vs| rd×|vr| rd×|vo| embedding matrices |vs||vr||vo| vocabulary sizes. layer represents objects relationships continuous features enabling thus introduce external knowledge unseen objects features. embeddings concatenated subject center size vector inputted stack hidden layers compose joint hidden representation models used spatial common sense input methods rely good spatial priors. additionally existing methods require spatial information present data lack capacity extrapolate/generalize. thus paper focus ﬁrst showing generalizing spatial arrangements unseen objects object-relation-object combinations possible second ensuring predicted templates accurate performing several quantitative qualitative evaluations. proposed task learn spatial templates propose task predicting relative spatial arrangement objects relationship given structured text input form —henceforth abbreviated denote coordinates center object’s half height object’s employ similar notation subject model predicdicting object’s location size given structured text input location size subject deﬁnes supervised task size location bounding boxes images serve ground truth. task interpreted spatial question-answering structured questions allows evaluating answer quantitatively space. hence goal answer common sense spatial questions crucially notice knowing subject’s coordinates requirement generating templates since inputting arbitrary coordinates still enables visualizing relative object locations. additionally input subject’s size order provide reference size model. however without input model still learns predict average size object. model output rm×m number pixels side element-wise sigmoid heatmap pixel activations ˆyij indicating probability pixel belongs object predictions evaluated true rm×m binary ˆyij models conceptually different different capabilities. model outputs crisp pointwise predictions model diffuse spatial templates location object variability e.g. kite easily move around. notice contrast convolutional neural networks approach make image pixels yielding model fully specialized spatial knowledge. employ -fold cross-validation setting. data randomly split disjoint parts employed testing training repeating folds. reported results averages folds. visual genome data visual genome dataset source annotated images. visual genome consists images containing human-annotated instances bounding boxes subject object ﬁlter instances containing explicit spatial prepositions preserving instances implicit spatial relationships. keep combinations word embeddings available whole triplet ﬁltering instances preserved yielding unique implicit relationships unique objects left instances explicit language yield instances unique explicit spatial prepositions unique objects. evaluation sets consider following subsets visual genome data evaluate performance. data simply unﬁltered instances visual genome data contains substantial proportion meaningless irrelevant instances. generalized triplets pick random combinations among frequent implicit combinations visual genome. yields instances etc. generalized words randomly choose objects among frequent objects visual genome take instances contain words. example since apple list e.g. kept. notice combination generalized word automatically generalized triplet too. also evaluated list generalized relationships obtaining similar results. additionally tested extra lists generalized objects yielded consistent results. enforcing generalization conditions experiments combinations sets removed training data prevent model seeing them. even without imposing generalization conditions reported results always unseen instances—yet combinations seen training sets contain exclusively implicit spatial language although analogous version explicit spatial prepositions also considered experiments. data pre-processing coordinates bounding boxes images normalized width height image. thus additionally notice distinction left right arbitrary regarding semantics image mirrored image preserves entirely meaning—while vertically inverted image not. example child walking horse meaningfully either side horse child riding horse cannot either horse. hence free model arbitrariness mirror image object left hand side subject. leaves object always right-hand side subject. notice mirroring aimed properly measuring performance impose constraint evaluation metrics model directly outputs object coordinates outputs heatmaps. however enable evaluating model regression/classiﬁcation metrics taking point maximum activation since employ image input thus cannot leverage pixels predict object’s location. avoid confusion term ‘zero-shot’ classiﬁcation denote unseen words/triplets generalized. although settings resemblances differ that ours unseen categories inputs classiﬁcation targets. notice settings must necessarily semantic knowledge zero-shot class hand otherwise task clearly infeasible. consider control method outputs random normal predictions equal dimension-wise mean standard deviation training targets. test statistical signiﬁcance friedman rank test post nemenyi tests results folds. indicate asterisk tables method signiﬁcantly better rest within model evaluation data table shows methods perform well considering amount noise present data. especially noteworthy ﬁnding relative locations predicted implicit spatial language approximately accurately explicit spatial language. interestingly unlike metrics clearly higher explicit implicit language suggests implicit templates exhibit ﬂexibility object’s location. hence blurrier predictions good choice applications e.g. computing soft images templates perform image retrieval. also observe models embeddings tend perform better differences generally signiﬁcant rnd. generalized evaluations table shows models perform well generalized triplets remarkably closely performance without imposing generalization conditions again performs slightly better signiﬁcantly better notably good performance generalized triplets evidences model rely external knowledge predict unseen combinations. ability generalizing frequent combinations rare/unseen ones especially valuable given sparsity implicit combinations impossibility learning data. coefﬁcient determination model predictions used evaluate goodness model regression. best score worst arbitrarily negative. constant prediction would obtain score above/below classiﬁcation. semantic distinction vertical horizontal axes mind consider problem classifying above/below relative locations. model predicts object center above/below subject center actually occurs must match. report macro-averaged pixel accuracy. pixels equivalent binary pixel accuracy. however good measure here class comprises average pixels. thus constant prediction zeros everywhere would obtain accuracy. hence consider macro-averaged pixel accuracy a.k.a. mean report best miou across full range decision thresholds. model hyperparameters implementation experiments implemented python keras deep learning framework models model hyperparameters ﬁrst selected fold cross-validation setting report results splits. models trained epochs batches size rmsprop optimizer using learning rate hidden layers relu units. models sensitive parameter variations. parameters embeddings backpropagated although choice little effect results. employ pixel grid output model. consider evaluation sets sect. following variations models subindex denotes model employs glove embeddings model embeddings randomly drawn dimension-wise normal distribution mean standard deviation equal glove embeddings preserving original dimensionality third type employs one-hot vectors additionally table results generalized triplets generalized words tables right show results sets without imposing generalization conditions i.e. allowing combinations/words training. qualitative evaluation ensure model predictions meaningful interpretable validate quantitative results qualitative evaluation. notice plots fig. generalized both able infer size location object notably well unseen triplets regardless embedding type however unseen words models leverage word embeddings tend perform better aligning quantitative results above. remarkably pixemb regemb output accurate predictions generalized words e.g. predicting correctly size elephant relative even though model never seen elephant before. noticeably models learn compose triplets distinguishing instance carrying surfboard riding surfboard playing frisbee holding frisbee etc. mapping language visualization provides another interesting property. traditional language processing systems translate spatial information qualitative symbolic representations capture spatial knowledge limited symbolic vocabulary used qualitative spatial reasoning research shows translation language qualitative spatial symbolic representations difﬁcult obtaining rather measures recognition performance even language utterance accompanied visual data here shown translate language utterances visualizations quantitative space complementing thus existing symbolic models. weights learn model without hidden layers resulting embedding layer followed linear output layer woutu bout using one-hot encodings concatenation layer becomes size |vs| |vr| |vo| e.g. wearing one-hot index relationships’ vocabulary index concatenation layer |vs| product woutu -dimensional vector i-th component product i-th wout vector thus component |vs|+j i-th wout gives inﬂuence j-th relationship notice objects kite headband tend subject large negative weight objects tend below e.g. sandals hoof large positive weight i.e. large positive inﬂuence object’s y-coordinate. implicit relations kicking riding strong predictors object’s figure model predictions generalized words triplets model embedding types indicated legend left. subject’s location given model predicts object intensity corresponds predicted probability. generalized words underlined. overall paper provides insight fundamental differences implicit explicit spatial language extends concept spatial templates implicit spatial language understanding requires common sense spatial knowledge objects actions. deﬁne task predicting relative spatial arrangements objects relationship present embedding-based neural models attain promising performance proving spatial templates accurately predicted implicit spatial language. remarkably models generalize well predicting correctly unseen object-relationshipobject combinations. furthermore acquired common sense spatial knowledge—aided word embeddings— allows model correctly predict templates unseen words. finally show weights model provide great insight spatial connotations words. ﬁrst limitation approach fully supervised setting models trained using images detected ground truth objects parsed text—which aims keeping design clean ﬁrst study implicit spatial templates. notice however methods automatically parse images text exist. future work implementing approach weakly supervised setting. second limitation spatial treatment actual world. worth noting however models setting trivially generalize appropriate data available. y-coordinate ﬁnding pulled weak suggesting spatial template rather depends composition subject object. fact weights implicit relations ﬂying riding comparable explicit relations above atop behaving similarly explicit language. notice even less informative explicit relations weights least order magnitude larger least informative implicit relations. figure evidences explicit relationships generally larger weights implicit relationships. alto-", "year": 2017}