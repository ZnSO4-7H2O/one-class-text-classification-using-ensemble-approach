{"title": "Causal Inference by Stochastic Complexity", "tag": ["cs.LG", "cs.AI"], "abstract": "The algorithmic Markov condition states that the most likely causal direction between two random variables X and Y can be identified as that direction with the lowest Kolmogorov complexity. Due to the halting problem, however, this notion is not computable.  We hence propose to do causal inference by stochastic complexity. That is, we propose to approximate Kolmogorov complexity via the Minimum Description Length (MDL) principle, using a score that is mini-max optimal with regard to the model class under consideration. This means that even in an adversarial setting, such as when the true distribution is not in this class, we still obtain the optimal encoding for the data relative to the class.  We instantiate this framework, which we call CISC, for pairs of univariate discrete variables, using the class of multinomial distributions. Experiments show that CISC is highly accurate on synthetic, benchmark, as well as real-world data, outperforming the state of the art by a margin, and scales extremely well with regard to sample and domain sizes.", "text": "kailash budhathoki planck institute informatics saarland university germany jilles vreeken planck institute informatics saarland university germany algorithmic markov condition states likely causal direction random variables identied direction lowest kolmogorov complexity. halting problem however notion computable. hence propose causal inference stochastic complexity. propose approximate kolmogorov complexity minimum description length principle using score mini-max optimal regard model class consideration. means even adversarial seing true distribution class still obtain optimal encoding data relative class. instantiate framework call cisc pairs univariate discrete variables using class multinomial distributions. experiments show cisc highly accurate synthetic benchmark well real-world data outperforming state margin scales extremely well regard sample domain sizes. introduction causal inference observational data—that identifying cause eect data collected carefully controlled randomised trials—is fundamental problem business science particularly interesting seing tell cause eect pair random variables given data joint distribution. identify likely causal direction. recent years number important ideas proposed allow accurate causal inference based properties joint distribution. ideas include additive noise model assume eect function cause additive noise independent cause algorithmic markov condition based kolmogorov complexity. loosely speaking idea causes shortest description joint distribution given separate descriptions distributions less dependent however kolmogorov permission make digital hard copies part work personal classroom granted without provided copies made distributed prot commercial advantage copies bear notice full citation page. copyrights third-party components work must honored. uses contact owner/author. copyright held owner/author. manuscript submied paper time dene causal inference rule based algorithmic markov condition using stochastic complexity. particular approximate kolmogorov complexity minimum description length principle using score mini-max optimal regard model class consideration. means even true data generating distribution reside model class consideration still obtain optimal encoding data relative best unlike kolmogorov complexity stochastic complexity computable. show strength approach instantiating pairs univariate discrete data using class multinomials. class stochastic complexity computable remarkably eciently score linear-time computational complexity. rough experiments show method cisc causal inference stochastic complexity performs well practice. strength mini-max property shows consider synthetic data vary data generating process—cisc outperforms state margin including out-of-model distributions geometric hypergeometric poisson. t¨ubingen benchmark data univariate pairs cisc signicantly outperforms existing proposals discrete data accuracy pairs certain about overall accuracy comparable state causal inference continuous-valued data. last least perform three case studies show cisc indeed infers sensible causal directions real-world data. dene causal indicator pairs discrete variables based stochastic complexity show eciently compute provide extensive experimental results synthetic benchmark real-world data make implementation used data available paper structured usual. introduce notation give preliminaries sec. give brief primer causal inference kolmogorov complexity sec. present cisc practical instantiation based stochastic complexity score sec. related work discussed sec. evaluate cisc empirically sec. round discussion sec. conclude sec. kolmogorov complexity kolmogorov complexity nite binary string length shortest binary program universal turing machine generates halts formally simply succinct algorithmic description kolmogorov complexity length ultimate lossless compression. conditional kolmogorov complexity length shortest binary program generates halts given input. amount algorithmic information contained shortest binary program dening analogously. intuitively number bits saved description shortest description already known. algorithmic information symmetric i.e. denotes equality additive constant therefore also called algorithmic mutual information strings algorithmically independent algorithmic mutual information i.e. purpose also need kolmogorov complexity distribution. kolmogorov complexity probability distribution length shortest program outputs precision input formally causal inference complexity given correlated variables interested inferring causal relationship. particular want infer whether causes whether causes correlated. assume causal suciency. confounding variable i.e. hidden common cause indicate causes base causal inference method following postulate postulate marginal distribution cause conditional distribution eect given cause independent contains information vice versa since correspond independent mechanisms nature. postulate provides foundation many successful causal inference frameworks designed pair variables think conditional mechanism transforms x-values y-values i.e. generates eect cause postulate justied dealing mechanism nature care input provide case). independence hold direction contain information inherit properties creates asymmetry cause eect. insightful consider following example amount radiation solar cell causes power generation cell ect). aect actions moving solar cell shady place varying angle aect likewise change pect cause) actions using ecient cells. however hard actions change pect) without aecting pect) vice versa. notion independence however abstract. accordingly dierent formalisations proposed. janzing dene independence terms information geometry. chan formulate independence terms distance correlation marginal conditional empirical distribution. janzing sch¨olkopf formalise independence using algorithmic information theory postulate algorithmic independence since algorithmic formulation captures types dependencies sound theoretical foundation arguably beer mathematical formalisation postulate using algorithmic information theory arrive following postulate. postulate equivalent saying factorizing joint distribution lead simpler terms kolmogorov complexity models factorizing following theorem hence consequence algorithmic independence input mechanism. words perform causal inference simply identifying direction factorization joint distribution lowest kolmogorov complexity. although inference rule sound theoretical foundations problem remains kolmogorov complexity computable widely known halting problem. practice therefore need other computable notions independence information. instance approximate kolmogorov complexity lossless compression generally minimum description length principle provides statistically sound computable means approximating kolmogorov complexity causal inference compression section discuss stochastic complexity used practical causal inference. gradually move towards goal starting covering basics along way. minimum description length principle minimum description length principle practical version kolmogorov complexity. instead possible programs considers programs know generate halt. lossless compressors. theory programs referred models. principle root two-part decomposition kolmogorov complexity roughly described follows given models data best model minimises length bits description model length bits description data encoded model intuitively represents compressible part data represents noise data. called two-part crude mdl. crude practice dene model class description methods well models consideration dene probability distributions optimal prex code given shannon entropy probability mass density function according denition however tricky vary encoding other introducing arbitrariness process. manuscript submied rened version overcomes arbitrariness encoding together. unlike crude rened encodes model class resulting single one-part code one-part code length also called stochastic complexity respect code designed exists model minimal also minimal. codes property also called universal codes. exist various types universal codes. although coding schemes dierent across codes resulting code lengths almost work consider universal code particular. stochastic complexity i.i.d. sample observed outcomes outcome element space observations parameter space. model class family probability distributions consisting dierent distributions produced varying parameters formally model class dened encode data optimally respect model class code corresponding distribution induced maximum likelihood estimate data given model class since distribution assigns shorter code length i.e. higher likelihood data distributions model class. normalized maximum likelihood distribution dened n-fold cartesian product indicating possible datasets size domain data dened continuous sample space summation symbol equation replaced integral. data pnml assigns probability diers highest achievable probability within model class maximum likelihood constant factor words distribution mini-max optimal universal model respect model class distribution represents behaviour distributions model class worst-case data generating distribution expectation even true data generating distribution reside model class consideration pnml still gives optimal encoding data relative properties important relevant modelling real-world problems. cases know true data generating distribution. cases ideally would want encode data best possible close optimal true distribution. distribution provides theoretically sound means that. term parametric complexity model class indicates well random data. stochastic complexity data model class gives shortest description data relative hence richer closer kolmogorov complexity. intuitively also amount information bits data relative model class. moreover evident formulation stochastic complexity data relative model class depends data model class particular models specied. causal inference stochastic complexity unless stated otherwise write stochastic complexity data relative model class corresponds complexity distribution data relative means stochastic complexity approximation kolmogorov complexity such provides general computable theoretically sound foundation causal inference based algorithmic information theory. ease notation wherever clear context write infer causal direction look total stochastic complexity directions vice versa. total stochastic complexity approximating given describing describing given easier terms stochastic complexity vice versa infer likely cause around infer likely cause ways describing same remain undecided. refer framework cisc stands causal inference stochastic complexity. causal inference using stochastic complexity number powerful properties. first unlike kolmogorov complexity stochastic complexity computable. second inference rule generic sense restricted data type distribution—we constrained model class consideration multinomial stochastic complexity consider discrete random variable values. furthermore assume data multinomially distributed. space observations multinomial model class dened maximum likelihood parameters multinomial distribution given number times outcome seen distribution induced maximum likelihood parameters model class given computational complexity compute counts going data once. however computing normalizing hence parametric complexity exponential number manuscript submied however approximate normalising nite oating-point precision sub-linear time respect data size given precomputed counts precisely computational complexity oating-point precision digits. experiments computing conditional complexity discussed compute stochastic complexity data model class. purpose also need compute conditional stochastic complexity vice versa. stochastic complexity conditioned conditional stochastic complexity possible values computational complexity compute compute conditional stochastic complexity compute hence computational complexity conditional stochastic complexity likewise altogether computational complexity cisc related work inferring causal direction observational data challenging task lack controlled randomised experiments. however also aracted quite aention years causal inference frameworks built continuous real-valued data. constraint-based approaches like conditional independence test widely used causal inference frameworks. however require least three observed random variables. erefore cannot distinguish factorization joint distribution direction i.e. recent years several methods proposed exploit sophisticated properties joint distribution. linear trace method infers linear causal relations form structure matrix maps cause eect using linear trace condition. kernelized trace method infer non-linear causal relations requires causal relation deterministic functional invertible. contrast make assumptions causal relation variables. frameworks causal inference additive noise models anms assume eect function cause additive noise independent cause. causal inference done nding direction admits model. years many frameworks causal inference real-valued data proposed using anms algorithmic information theory provides sound general theoretical foundation causal inference idea causes shortest description joint distribution given separate descriptions however kolmogorov complexity computable practical instantiations require computable notions independence. instance information-geometric approach denes independence orthogonality information space. cure denes independence terms accuracy estimations using algorithmic information theory vreeken proposes causal framework based relative conditional complexity instantiates cumulative entropy infer causal direction continuous real-valued data. budhathoki vreeken propose decision tree based approach causal inference univariate multivariate binary data. methods consider either continuous real-valued binary data. causal inference discrete data received much less aention. peters extend additive noise models discrete data. however regression ideal modelling categorical variables relies dependence measure choice aects outcome. chan dene independence terms distance correlation empirical distributions infer causal direction categorical data. such look possible space observed samples hence overts. contrast look possible space observed samples. moreover provide general computable theory causal inference applicable type data. particular directly approximate kolmogorov complexity using score mini-max optimal regard model class consideration. computational complexity instantiation cisc linear sample size regardless domain variables. experiments consider comparison. experiments implemented cisc python provide source code research purposes along used datasets synthetic dataset generator. experiments executed single-threaded intel xeon machine memory running linux. consider synthetic benchmark real-world data. particular note cisc parameter-free. compare cisc discrete regression particular signicance level independence test threshold synthetic data evaluate cisc data known ground truth consider synthetic data. generating non-trivial synthetic data identiable causal direction surprisingly dicult though. generate synthetic cause-eect pairs ground truth using additive noise model generate cause generate eect using model given note even though generate data following joint distribution might admit additive noise model reverse direction. erefore cases true direction might also equally plausible hence full accuracy might achievable cases. however happens trivial instances accuracy model class sample dierent models hence dierent cause-eect pairs. model sample points i.e. figure compare accuracy cisc various model classes. cisc either outperforms good methods case. certainly proves generality cisc. although compute stochastic complexity multinomial model class still able perform good model classes. optimality property distribution even though true data generating distribution inside model class consideration distribution still gives optimal encoding relative works well cases. decision rate next investigate accuracy cisc fraction decisions cisc forced make. model class sample dierent cause-eect pairs. cause-eect pair sample points. sort pairs absolute score dierence directions i.e. |sx→y −sy→x descending order. compute accuracy top-k% pairs. decision rate fraction cause-eect pairs consider. alternatively also fraction cause-eect pairs whose |sx→y sy→x greater figure show decision rate versus accuracy dierent model classes. cisc highly accurate high decision rate cases. cisc highly accurate cause-eect pairs absolute score dierence high methods decisive. hand doesn’t perform well cases. seing relatively good performance family uniform distributions. results indicate increase threshold hence decision rate higher accuracy. scalability next empirically investigate scalability cisc. first examine runtime regard sample size. domain size cause-eect pairs i.e. given sample size sample uniformly randomly |x|. likewise next sample size vary domain size |y|. observe cisc nish within seconds whole range. iteratively searches entire domain shows non-linear runtime behaviour respect domain size. benchmark data next evaluate cisc benchmark cause-eect pairs known ground truth particular take univariate cause-eect pairs. exist discretization strategy provably preserves causal relationship variables. since cause-eect pair dierent domain using discretization strategy manuscript submied figure compare accuracy cisc various decision rate together condence interval random coin look pairs cisc infers correct direction roughly pairs. consider pairs cisc decisive—with high value accurate pairs accurate pairs on-par top-performing causal inference frameworks continuous real-valued data hand results insignicant almost every decision rate. nine measurements consider length diameter height length diameter height abalone measured millimetres dierent values respectively whereas abalone nominal following peters regard data discrete consider ground truth causes size abalone around. cisc infers correct direction three cases. evaluation evaluation dataset available machine learning repository. rows derived hierarchical decision model. contains evaluation buying purpose based characteristics car. consider estimated safety evaluation car. safety feature takes nominal value evaluation feature also takes nominal value regard ground truth safety causes decision buying vice versa. cisc identies correct direction. aributes consider three education occupation income domain education aribute consists dropout associates bachelors doctorate hs-graduate masters prof-school. occupation admin armed-force blue-collar white-collar service sales professional other-occupation possible values. lastly income aribute values intuitively education causes income vice versa regard ground truth. similarly occupation causes income regard ground truth. cisc pairs observe pairs cisc infers causal direction correctly. discussion experiments show cisc works well practice. cisc reliably identies true causal direction regardless data distribution. remarkably fast. benchmark data it’s performance comparable state-of-the-art causal inference frameworks continuous real-valued data. moreover qualitative case studies show results sensible. work give general framework causal inference based solid foundations information theory. apply framework practice compute stochastic complexity relative model class. richer model class beer solution. although computing stochastic complexity involves looking possible datasets theoretically still computable exist ecient algorithms certain model classes. proposed framework lays clear computable foundation algorithmic causal inference principle postulated janzing sch¨olkopf although results show strength proposed framework cisc particular many possibilities improve. instantiated framework using multinomial stochastic complexity discrete data. cisc performs relatively well even cases data sampled multinomial model class. optimality property multinomial distribution even true data generating distribution inside model class consideration distribution still gives optimal encoding data relative would engaging future work instantiate framework types data model classes aspect study would ecient algorithms computing stochastic complexity model classes. dene conditional stochastic complexity stochastic complexities conditioned look local stochastic complexities parts relative value perhaps compute conditional stochastic complexity globally relative would also interesting explore factorized normalized maximum likelihood models instantiate framework multivariate data infer causal relationship variables assume confounding variable would interesting framework additionally discover confounding variables. rough idea factorizing joint complexity presence confounding variable leads smallest stochastic complexity compared factorizing another avenue future work would framework causal discovery. proposed framework infers causal relationship given variables would interesting explore framework employed discover causal models directly data. conclusion considered causal inference observational data. proposed general computable framework information-theoretic causal inference optimality guarantees. particular proposed perform causal inference stochastic complexity. illustrate strength this proposed cisc pairs univariate discrete variables using stochastic complexity class multinomial distributions. extensive evaluation synthetic benchmark real-world data showed cisc highly accurate outperforming state margin scales extremely well regard sample domain sizes. acknowledgments kailash budhathoki supported international planck research school computer science. authors supported cluster excellence multimodal computing interaction within excellence initiative german federal government. references kailash budhathoki jilles vreeken. causal inference compression. icdm. ieee chen zhang chan. nonlinear causal discovery high dimensional data kernelized trace method. icdm. ieee peter gr¨unwald. minimum description length principle. press. peter gr¨unwald paul vit´anyi. algorithmic information eory. corr abs/. hoyer janzing mooij peters sch¨olkopf. nonlinear causal discovery additive noise models. nips. janzing hoyer sch¨olkopf. telling cause eect based high-dimensional observations. icml. jmlr dominik janzing joris mooij zhang lemeire jakob zscheischler povilas daniuˇsis bastian steudel bernhard sch¨olkopf. janzing sch¨olkopf. causal inference using algorithmic markov condition. ieee janzing steudel. justifying additive noise model-based causal discovery algorithmic information eory. osid a.n. kolmogorov. approaches antitative denition information. problemy peredachi informatsii vit´anyi. introduction kolmogorov complexity applications. springer. furui laiwan chan. causal inference discrete data estimating distance correlations. neur. comp. tommi mononen petri myllym¨aki. computing multinomial stochastic complexity sub-linear time. pgm. joris mooij jonas peters dominik janzing jakob zscheischler bernhard sch¨olkopf. distinguishing cause eect using observational data methods benchmarks. jmlr mooij stegle janzing zhang sch¨olkopf. probabilistic latent variable models distinguishing cause eect. nips. curran myung daniel navarro mark model selection normalized maximum likelihood. math. psych. judea pearl. causality models reasoning inference. cambridge university press york usa. judea pearl. causality models reasoning inference cambridge university press york usa. peters janzing sch¨olkopf. identifying cause eect discrete data using additive noise models. aistats. jmlr peters mooij janzing sch¨olkopf. causal discovery continuous additive noise models. jmlr jorma rissanen. modeling shortest data description. automatica jorma rissanen. strong optimality normalized models universal codes information data. ieee roos silander kontkanen myllym¨aki. bayesian network structure learning using factorized universal models. proc. sgouritsa janzing hennig sch¨olkopf. inference cause eect unsupervised inverse regression. jmlr shohei shimizu patrik hoyer aapo hyv¨arinen kerminen. linear non-gaussian acyclic model causal discovery. jmlr shtarkov. universal sequential coding single messages. problems information transmission spirtes glymour scheines. causation prediction search. press. n.k. vereshchagin p.m.b. vitanyi. kolmogorov’s structure functions model selection. ieee zhang aapo hyv¨arinen. identiability post-nonlinear causal model. uai. auau press", "year": 2017}