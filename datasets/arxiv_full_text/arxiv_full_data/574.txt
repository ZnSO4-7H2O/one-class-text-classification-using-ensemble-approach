{"title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.", "text": "ankit kumar peter ondruska mohit iyyer james bradbury ishaan gulrajani victor zhong romain paulus richard socher ﬁrstnamemetamind.io metamind palo alto figure example inputs questions together answers generated dynamic memory network trained corresponding task. sequence modeling tasks answer mechanism triggered input word instead end. propose dynamic memory network neural network based framework general question answering tasks trained using input-question-answer triplets. generally solve sequence tagging tasks classiﬁcation problems sequence-to-sequence tasks question answering tasks require transitive reasoning. ﬁrst computes representation inputs question. question representation triggers iterative attention process searches inputs retrieves relevant facts. memory module reasons retrieved facts provides vector representation relevant information answer module generates answer. tasks natural language processing cast question answering problems language input. introduce dynamic memory network neural network architecture processes input sequences questions forms episodic memories generates relevant answers. questions trigger iterative attention process allows model condition attention inputs result previous iterations. results reasoned hierarchical recurrent sequence model generate answers. trained end-to-end obtains state-of-the-art results several types tasks datasets question answering text classiﬁcation sentiment analysis sequence modeling part-of-speech tagging training different tasks relies exclusively trained word vector representations input-question-answer triplets. question answering complex natural language processing task requires understanding meaning text ability reason relevant facts. most tasks natural language processing cast question answering problem high level tasks like machine translation sequence modeling tasks like named entity recognition part-of-speech tagging classiﬁcation problems like sentiment analysis give overview modules make dmn. examine module detail give intuitions formulation. high-level illustration shown fig. input module input module encodes text inputs task distributed vector representations. paper focus natural language related problems. cases input sentence long story movie review news article several wikipedia articles. question module like input module question module encodes question task distributed vector representation. example case question answering question sentence author ﬁrst ﬂy?. representation episodic memory module forms basis initial state upon episodic memory module iterates. episodic memory module given collection input representations episodic memory module chooses parts inputs focus attention mechanism. produces memory vector representation taking account question well previous memory. iteration provides module newly relevant information input. words module ability retrieve information form input representations thought irrelevant previous iterations. answer module answer module generates answer ﬁnal memory vector memory module. natural language processing problems input sequence words encode input sequence recurrent neural network word embeddings given inputs recurrent network. time step network updates hidden state ht−) embedding matrix word index word input sequence. cases input sequence single sentence input module outputs hidden states recurrent network. cases input sequence list sentences concatenate sentences long list word tokens inserting sentence end-of-sentence token. hidden states end-of-sentence tokens ﬁnal representations input module. subsequent sections denote output input module sequence fact representations whereby denotes element output sequence figure overview modules. communication indicated arrows uses vector representations. questions trigger gates allow vectors certain inputs given episodic memory module. ﬁnal state episodic memory input answer module. input module. note case input single sentence number output representations equal number words sentence. case input list sentences equal number sentences. choice recurrent network experiments gated recurrent network also explored complex lstm performed similarly computationally expensive. work much better standard tanh postulate main strength comes gates allow model suffer less vanishing gradient problem assume time step input hidden state internal mechanics deﬁned similar input sequence question also commonly given sequence words natural language processing problems. before encode question recurrent neural network. given question figure real example input list sentences attention gates triggered speciﬁc question babi tasks gate values shown corresponding vectors. gates change search inputs. draw connections gates close zero. note second iteration wrongly placed weight sentence makes intuitive sense sentence another place john been. words hidden states question encoder time given qt−) represents word embedding matrix previous section represents word index word question. share word embedding matrix across input module question module. unlike input module question module produces output ﬁnal hidden state recurrent network encoder qtq. episodic memory module iterates representations outputted input module updating internal episodic memory. general form episodic memory module comprised attention mechanism well recurrent network updates memory. during iteration attention mechanism attends fact representations taking consideration question representation previous memory produce episode episode used alongside previous memories update episodic memory initial state initialized question vector itself tasks beneﬁcial episodic memory module take multiple passes input. passes ﬁnal memory given answer module. need multiple episodes iterative nature module allows attend different inputs pass. also allows type transitive inference since ﬁrst pass uncover need retrieve additional facts. instance example fig. asked football? ﬁrst iteration model ought attend sentence question asks football. model sees john relevant reason second iteration retrieve john was. similarly second pass help sentiment analysis show experiments section below. attention mechanism work gating function attention mechanism. pass mechanism takes input candidate fact previous memory question compute gate scoring function takes input feature produces scalar score. ﬁrst deﬁne large feature vector captures variety similarities input memory question vectors datasets facebook’s babi dataset specify facts important given question. cases attention mechanism function trained supervised fashion standard crossentropy cost function. memory update mechanism compute episode pass employ modiﬁed sequence inputs weighted gates episode vector given answer module ﬁnal state gru. equation update hidden states time equation compute episode respectively criteria stopping episodic memory module also signal stop iterating inputs. achieve this append special end-of-passes representation input stop iterative attention process representation chosen gate function. datasets without explicit supervision maximum number iterations. whole module end-to-end differentiable. employ another whose initial state initialized last memory timestep takes input question last hidden state well previously predicted output yt−. concatenate last generated word question vector input time step. output trained cross-entropy error classiﬁcation correct sequence appended special end-of-sequence token. training cast supervised classiﬁcation problem minimize cross-entropy error answer sequence. datasets gate supervision babi cross-entropy error gates overall cost. because modules communicate vector representations various types differentiable deep neural networks gates entire model trained backpropagation gradient descent. given many shoulders paper standing many applications model applied impossible related ﬁelds justice. deep learning several deep learning models applied many different tasks nlp. instance recursive neural networks used parsing sentiment analysis paraphrase detection question answering logical inference among tasks. however lack memory question modules single model cannot solve many varied tasks tasks require transitive reasoning multiple sentences. ancommonly used model chain-structured recurrent neural network kind employ above. recurrent neural networks successfully used language modeling speech recognition sentence generation images also relevant sequence-to-sequence model used machine translation sutskever model uses extremely large deep lstms encode sentence language decode sentence another language. sequence-to-sequence model special case without question without episodic memory. instead maps input sequence directly answer sequence. attention memory second line work relevant dmns attention memory deep learning. attention mechanisms generally useful improve image classiﬁcation automatic image captioning machine translation neural turing machines memory solve algorithmic problems list sorting work recent months weston memory networks focuses adding memory component natural language question answering. input response component generalization output feature components functional overlap episodic memory. however memory network canapplied variety tasks since processes sentences independently sequence model. requires n-gram vector features well separate feature captures whether sentence came another one. various neural memory attention architectures recently proposed algorithmic problems caption generation images visual question answering problems datasets contrast employs neural sequence models input representation attention response mechanisms thereby naturally capturing position temporality. result directly applicable broader range applications without feature engineering. compare directly memory networks babi dataset applications general model apply several problems. compare what best knowledge current state-of-the-art method task. many different approaches question answering build large knowledge bases open information extraction systems neural networks dependency trees others sentences approaches exist. systems produce right answer often unclear access facts cannot reason never seen type question phenomenon. dataset hundred questions answers require complex reasoning. hence solved models learn purely examples. synthetic datasets problems often solved easily manual feature engineering disentangle failure modes models understand necessary capabilities. useful analyzing models attempt learn everything rely external features like coreference parsing logical rules etc. model. another related model andreas combines neural logical reasoning question answering knowledge bases visual question answering. sentiment analysis useful classiﬁcation task recently stanford sentiment treebank become standard benchmark dataset. reports previous state-of-the-art result based convolutional neural network uses multiple word vector representations. previous best model part-of-speech tagging wall street journal section penn tree bank sogaard used semisupervised nearest neighbor approach. also directly compare paragraph vectors neuroscience episodic memory humans stores speciﬁc experiences spatial temporal context. instance might contain ﬁrst memory somebody ﬂying hang glider. eichenbaum cohen argued episodic memories represent form relationship hippocampus responsible general relational learning interestingly also appears hippocampus active transitive inference disruption hippocampus impairs ability episodic memory module inspired ﬁndings. retrieves speciﬁc temporal states related triggered question. furthermore found module able transitive inference simple facts babi dataset. module also similarities temporal context model bayesian extensions developed analyze human behavior word recall experiments. include experiments question answering part-ofspeech tagging sentiment analysis. model trained independently problem architecture remains except answer module input fact subsampling answer module described section triggered either token. datasets used either ofﬁcial train development test splits development deﬁned used training development. hyperparameter tuning model selection done development set. trained backpropagation adam employ regularization dropout word embeddings. word vectors pre-trained using glove single supporting fact supporting facts three supporting facts argument relations three argument relations yes/no questions counting lists/sets simple negation indeﬁnite knowledge basic coreference conjunction compound coreference time reasoning basic deduction basic induction positional reasoning size reasoning path finding agent’s motivations facebook babi dataset synthetic dataset testing model’s ability retrieve facts reason them. task tests different skill question answering model ought have coreference resolution deduction induction. showing ability exists sufﬁcient conclude model would also exhibit real world text data. however necessary condition. training babi dataset uses following objective function αece βece standard cross-entropy cost hyperparameters. practice begin training later switch keeping described section input module outputs fact representations taking encoder hidden states time steps corresponding end-of-sentence tokens. gate supervision aims select sentence pass; thus also experimented modifying simple softmax instead gru. here compute t)ct value softmax. dcnn kalchbrenner pvec mikolov. cnn-mc drnn irsoy cardie ct-lstm list results table worse memory network refer memnn tasks tasks long input sequences. suspect recurrent input sequence model trouble modeling long inputs. memnn suffer problem views sentence separately. power episodic memory module evident tasks signiﬁcantly outperforms memnn. tasks require model iteratively retrieve facts store representation slowly incorporates relevant information input sequence. models poorly tasks though memnn better. suspect memnn using n-gram vectors sequence position features. stanford sentiment treebank popular dataset sentiment classiﬁcation. provides phrase-level ﬁne-grained labels comes train/development/test split. present results formats ﬁne-grained root prediction full sentences test classiﬁed either negative negative neutral positive positive binary root prediction non-neutral full sentences test classiﬁed either positive negative. train model full sentences well subsample phrase-level labels every epoch. during evaluation model evaluated full sentences binary classiﬁcation neutral phrases removed dataset. achieves state-ofthe-art accuracy binary classiﬁcation task well ﬁne-grained classiﬁcation task. part-of-speech tagging traditionally modeled sequence tagging problem every word sentence classiﬁed part-of-speech class evaluate standard wall street journal dataset standard splits sections training development test sets since word level tagging task memories classiﬁed time step corresponding word. described detail section discussion sequence modeling. compare results achieves state-of-the-art accuracy single model reaching development accuracy ensembling development models gets test accuracies achieving slightly higher state-of-the-art main novelty architecture episodic memory module. hence analyze important episodic memory module tasks particular number passes input affect accuracy. table shows accuracies subset babi tasks well stanford sentiment treebank. note several hard reasoning tasks multiple passes inputs crucial achieving high performance. sentiment differences smaller. however passes outperform single pass zero passes. latter case episodic memory outputs passed directly input module answer module. note that especially complicated examples often correctly classiﬁed passes many examples sentiment contain simple sentiment words negation misleading expressions. hence need complicated architecture small. true tagging. here differences accuracy less different numbers passes. table effectiveness episodic memory module across tasks. shows ﬁnal accuracy term percentages different maximum limit number passes episodic memory module take. note -pass network essential reduces output attention module. apart quantitative analysis also show qualitatively happens attention multiple passes. present speciﬁc examples experiments illustrate iterative nature episodic memory module enables model focus relevant parts input. instance table shows example focuses pass three-iteration scan question babi dataset. also evaluate episodic memory module sentiment analysis. given performs well iteration iterations study test examples one-iteration incorrect twoepisode correct. looking sentences fig. make following observations attention two-iteration generally much focused compared oneiteration dmn. believe fact fewer iterations input hidden states input module encoder capture content adjacent time steps. hence attention mechanism cannot focus time steps. instead needs pass necessary information answer module single pass. second iteration two-iteration attention becomes signiﬁcantly focused relevant words less attention paid strong sentiment words lose sentiment context. exempliﬁed sentence fig. includes positive word best. ﬁrst iteration word best dominates attention scores however context best described clear relevance diminished lukewarm becomes important. facts yesterday julie traveled school. yesterday marie went cinema. morning julie traveled kitchen. bill went back cinema yesterday. mary went bedroom morning. julie went back bedroom afternoon. figure attention weights sentiment examples labeled correctly episodes. y-axis shows episode number. sentence demonstrates case ability iterate allows sharply focus relevant words. figure sentence demonstrate cases initially positive words lost importance entire sentence context became clear either contrastive conjunction modiﬁed action best described. perform multiple passes data beneﬁcial. provides signiﬁcant beneﬁts harder babi tasks require reasoning several pieces information transitive reasoning. increasing number passes also slightly improves performance sentiment analysis though difference signiﬁcant. attempt iterations sentiment analysis model struggles overﬁtting three passes. model potentially general architecture variety applications including classiﬁcation question answering sequence modeling. single architecture ﬁrst step towards single joint model multiple problems. trained end-to-end albeit complex objective function. future work explore additional tasks larger multi-task models multimodal inputs questions. merrienboer gulcehre bahdanau bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation. emnlp passos kumar mccallum lexicon infused phrase embeddings named entity resolution. conference computational natural language learning. association computational linguistics june", "year": 2015}