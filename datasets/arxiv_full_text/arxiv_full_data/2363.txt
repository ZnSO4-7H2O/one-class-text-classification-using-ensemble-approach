{"title": "Comparing Bayesian Network Classifiers", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper, we empirically evaluate algorithms for learning four types of Bayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BN augmented Naive-Bayes and general BNs, where the latter two are learned using two variants of a conditional-independence (CI) based BN-learning algorithm. Experimental results show the obtained classifiers, learned using the CI based algorithms, are competitive with (or superior to) the best known classifiers, based on both Bayesian networks and other formalisms; and that the computational time for learning and using these classifiers is relatively small. Moreover, these results also suggest a way to learn yet more effective classifiers; we demonstrate empirically that this new algorithm does work as expected. Collectively, these results argue that BN classifiers deserve more attention in machine learning and data mining communities.", "text": "paper empirically algorithms network classifiers augmented nai've-bayes nai've-bayes learned using variants conditional-independence learning results show obtained learned using based algorithms superior bayesian networks formalisms; computational learning using classifiers relatively small. moreover suggest learn effective classifiers; algorithm collectively classifiers learning paper provides standard approaches networks effective nai've-bayes augmented nai've-bayes general describes learning effectiveness. section experimental problems. learning results learning active research topic data mining. past decades developed classifiers. neural-network powerful representation inference uncertainty discovery kind assumes attributes given classification node probability collectively represents nodes represents general compute conditional values assigned used classifier distribution attributes. datasets attributes. later idea markov boundary node nina markov boundary nodes \"shields\" node outside boundary. boundaries parents children. markov blanket natural feature markov blanket ways view suggesting particular structure encodes joint distribution attributes. best best fits data leads scoring-based learning algorithms maximizes entropy scoring function cooper herskovits second structure conditional nodes according suggests identifying among nodes. using statistical chi-squared find conditional attributes construct based algorithms procedure process efficient. assumption other. although independence assumption obviously outperformed large number datasets strongly heckerman compare general learning often certain advantages methods terms modeling distribution. friedman show theoretically methods result poor general scoring-based classifiers function reach conclusion moreover analysis. often less efficient langley sage forward selection find subset good subset attributes construct bayesian john best-first accuracy algorithm either decision bayesian performs feature joining well feature selection classifier. kononenko' algorithm attributes independence attributes groups. friedman studied allows tree-like structures dependencies learn gbns bans. empirically compared classifiers using eight datasets repository evaluated. cbli algorithm information dataset) efficiency tree construction three-phase essentially thickening verifies sufficient guaranteed underlying glymour correctness proof complexity please refer classification learning learns tree structure information tests conditioned classification similar nayve-bayes classification figure default experiments used four learning algorithms learn four classifiers export bayesian files. learned using default threshold powerconstructor. nodes much mutual information considered section naive-bayes learning algorithms threshold. data sets \"nursery\" classifier cases actually nai\"ve-bayes missing links bayes sometimes reveals independent naive-bayes however using classifiers weak dependencies among classification weak dependencies accuracy also measured running time classifierÂ­ learning time classifier learning classifiers learning time less minutes. five times classifier slower learning took minutes learn \"adult\" data. given training attributes learning \"dna\" data take long time might small \"mushroom\" data gives prediction dataset features. small evidence appropriate. uses harsh. another learned naive classifiers. obtained complex probably advantage threshold. setting ban) outperform dataset wrapper algorithm threshold datasets. another possible providing forbidden help learners find better structure. might also jearn classifiers without using prior node ordering specifying paper empirically four classifiers. ban) learned using variants algorithm results wrapper function. results around demonstrated wrapper classifier classifiers. given theoretical empirical based methods data sets friedman singh provan reported believe mutual information classifier based methods. note addition information feature", "year": 2013}