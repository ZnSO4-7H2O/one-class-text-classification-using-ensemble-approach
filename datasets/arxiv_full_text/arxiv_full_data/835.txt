{"title": "Coloring black boxes: visualization of neural network decisions", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "Neural networks are commonly regarded as black boxes performing incomprehensible functions. For classification problems networks provide maps from high dimensional feature space to K-dimensional image space. Images of training vector are projected on polygon vertices, providing visualization of network function. Such visualization may show the dynamics of learning, allow for comparison of different networks, display training vectors around which potential problems may arise, show differences due to regularization and optimization procedures, investigate stability of network classification under perturbation of original vectors, and place new data sample in relation to training data, allowing for estimation of confidence in classification of a given sample. An illustrative example for the three-class Wine data and five-class Satimage data is described. The visualization method proposed here is applicable to any black box system that provides continuous outputs.", "text": "abstract—neural networks commonly regarded black boxes performing incomprehensible functions. classiﬁcation problems networks provide maps high dimensional feature space k-dimensional image space. images training vector projected polygon vertices providing visualization network function. visualization show dynamics learning allow comparison different networks display training vectors around potential problems arise show differences regularization optimization procedures investigate stability network classiﬁcation perturbation original vectors place data sample relation training data allowing estimation conﬁdence classiﬁcation given sample. illustrative example three-class wine data ﬁve-class satimage data described. visualization method proposed applicable black system provides continuous outputs. common opinion neural networks black boxes used safety-critical applications. understanding network decisions found network converted logical rules understanding always comes price. network function approximated decision borders provided neural networks severely distorted since feature space partitioned hypercuboids ellipsoids alternative convert neural network simpliﬁed structure performing logical functions. since neural networks universal approximators regularization leads low-complexity models perform quite well providing estimation posterior probabilities approximation logical rules always distorts mapping found network. although data classiﬁcation accuracy obtained optimized logical rules higher accuracy obtained neural networks seems artifact quantization outputs information typical neural network? estimation overall classiﬁcation accuracy mean square error sometimes estimation classiﬁcation probability. quality networks compared looking receiver operator characteristics curves measures global; distinguish easy difﬁcult cases. overall classiﬁcation accuracy good estimator accuracy particular problem hand since errors conﬁned distant localized region feature space. multilayer perceptron networks provide outputs close making overconﬁdent predictions. difference networks make errors time predicting wrong answer probability close networks make wrong answers probability slightly higher correct answer. regularization improve generalization since stochastic learning algorithms create networks identical accuracy quite different weights biases network ﬁnally choosen? network hidding strange behavior lead completely wrong results data? visualization mappings performed neural networks certainly widen range applicability. since feature spaces highly dimensional faithful presentation mapping learned neural network possible. interesting information contained perceived similarities training data samples. classiﬁcation problems categories similarities displayed scatterogram k-dimensional space. next section linear projection method introduced projecting network outputs vertices polygon. section three presents detailed case study using networks class wine dataset examples -class satimage dataset. last section discussion remarks usefulness development visualization methods given. since color makes much easier understand ﬁgures reader advised view version paper assume k-class problem training vector neural network outputs given. come either single network networks single output specialize discrimination vectors single class. target output typical classiﬁcation problem zero outputs output corresponds class input vector belongs requirement many cases artiﬁcial. output classes form continuum rather small integer numbers leading fuzzy degree membership replacing crisp labeling. outputs treated estimation degree membership caes estimation similarity vector vectors class. network realizations outputs estimations posterior probabilities given network vector since probabilities number unit interval points within hexagon corners corresponding binary values. opposite corners hexagon inverted bits points corresponding vectors weakly exciting output approach center along line points overlapping region class three approach center along line. chemical analysis wines grown region italy derived three different cultivars sufﬁcient recognize source wine. analysis determined quantities including alcohol content color intensity content chemical compounds. data stored irvine repository machine learning problems details found. number data samples classes respectively data rather small. possible separate classes perfectly using network hidden neurons. classes designated markers. independent outputs reduced networks outputs k-dimensional images inputs created non-linear function network learned. vectors different classes images created neural networks make errors separable clusters otherwise clusters overlap. visualization network decisions possible kdimensional space presenting images training vectors. network outputs independent desired answers fall corners square coordinates. images vectors belong overlapping regions close vertex vectors recognized close vertex. vectors decision borders classiﬁed correctly scatterogram images clustering around corners. images vectors close decision borders fall closer middle square. vectors different classes distinguished using different markers. comparing scatterograms different networks immediately show signiﬁcant differences despite similar accuracies. position image vector relation images training vectors shown scatterogram allows evaluation reliability classiﬁcation. similar representation possible larger number classes projection three dimensions needed. although linear projections loose information sophisticated projections could devised simple approach presented already quite useful. hypercube corners correspond binary labels correspond corners regular polygon dimensions. coordinates polygon vertex corresponding point vertex corresponding point calculated transformation found setting linear equations equations projections unit vectors polygon vertices equations projection point polygon center coordinates projection several interesting features. center triangle corresponds points dimensions. cases three outputs fall there well cases three outputs since outputs assumed networks trained scaled conjugate gradient procedure single hidden layer network. networks used -dimensional vectors -dimensions project result -dimensions using method introduced previous section. using scatterograms training data created following issues addressed three hidden neurons used numerical experiments here. since network initialized small values weights biases ﬁrst training epoch output values concentrated around ﬁrst series pictures shows network performance iterations. since time network trained different solution obtained extreme cases selected trials best network worst network vectors still correctly handled easily identiﬁed. lower left corner class vectors clustered. lower left part ﬁgure already well separated classes although hyperplane separating class vectors corner) still close class vectors. clear vectors images close corner. training shift decision border class vectors away class vectors. stochastic training algorithm changes network parameters along quite different trajectories parameter space creating learning different networks evident left right subﬁgures fig. initializations convergence fast emerging separation vectors different classes simple remedies applied re-initializing network decreasing network parameters make sigmoidal functions less saturated perturbing weights adding random numbers. fig. suggests another possibility present input vectors correspond images near middle line since network response closer therefore gradients relatively large learning proceed faster scatterogram becomes like left side fig. ﬁnal solutions look similar although network weights signiﬁcantly differ. size network weights reﬂected concentration vector images around corners; training images training vectors cluster almost exactly polygon’s corners indicating binary target values classes achieved. number errors good indicator quality solutions networks used create fig. plots made errors training data test results second network signiﬁcantly worse since data vectors close isolated class vectors lead several errors. large number errors result problems convergence wine data networks collapse images vectors cluster evidently becoming trapped local minimum corresponding majority classiﬁer. case repeating network training several times lead better solution. problem also underﬁtting data case repeating calculation help. classiﬁcation problems underﬁtting manifests inability network create appropriate decision borders. images training vectors scatterograms clustered around polygon vertices. fig. images created networks hidden neuron shown corresponding quite good solution errors only rather poor solution errors. cases images class appear triangle corner images classes appear somewhere middle triangle showing inability network proper solution. hand networks complex overﬁtting data. training network hidden neurons done randomly selected data results displayed data. although errors made training partition images several test vectors appear near center triangle corresponding vectors network recognize indicating network generalize well. conﬁrmed adding noise original data fig. small images original data vectors slightly perturbed gaussian distributed random vectors unit variance multiplied lines center triangle vertices show perturbed vectors regions feature space sigmoidal functions network small values. convergence images training vectors collapse single point showing network overconﬁdent images vectors classiﬁed wrongly mapped wrong vertices polygon. networks behave weights become large creating almost step-like functions correspond sharp decision borders. decision borders brittle lead poor generalization network. perturbing training vectors adding noise show effect clearly scatterograms lines connecting vertices polygon’s center appear right plot fig. right plot fig. fact adding noise input data equivalent regularization procedure making solutions robust increasing classiﬁcation margins. wide margin solutions manifested images training vectors concentrated near polygon vertices collapsed single point. network overconﬁdent i.e. errors closer center polygon close midpoints lines connecting polygon’s vertices. shown fig. network hidden units able perfectly separate training data. without regularization images training vectors generated network collapse three vertices triangle images fig. network solution gaussian functions; right ﬁgure network slightly perturbed input vectors. bottom comparison solutions inputs perturbed strong noise perturbed vectors line joining vertices centers indicating vectors region sigmoidal function large value gaussian regularization prior added error function scaled small hyperparameter partially removes effect making corners blurred removing images perturbed vectors center although images training vectors still close triangle vertices. increasing regularization hyperparameter makes network much less conﬁdent shows realistic predictions samples wines class happen rather similar samples class class similar samples class. large regularization hyperparameter network start make errors even images almost perturbed vectors concentrated around correct corners triangle. thus visualization useful select best network proper regularization. networks similar making number errors identical confusion matrices still signiﬁcantly differ areas feature space. wine example vectors classes quite close decision surface vectors class close decision surface. although cases errors made network preferred costs mixing different classes equivalent. demonstrated fig. adding variance noise perturb original data. different gradient optimization procedures also converge different networks. differences visible even better network used instead mlp. gaussian functions network also ﬁnds solution single error. images training vectors mapping network much less localized perturbed vectors much closer unperturbed vectors mlps. nonlinearities introduced network signiﬁcantly smaller network therefore solution robust. perturbing original vectors noise large variance elicit unexpected behavior network network small regularization number hidden units makes less errors places many perturbed vectors close vertices corresponding wrong classes images vectors mapped show close vectors decision borders images obtained mapping show also similarities vectors feature spaces. easy problems well separated clusters regularization provides quite robust solutions. hidden neurons strong regularization creates images vectors classes clustered vertices pentagon. network mapping quite robust even adding noise variance network behavior quite predictable indicating strange kinks hiding black box. arms extending vertices vertices simply indicate feature space vectors corresponding images belong clusters relatively close together. satimage data originally contained images types soil landsat satellite multi-spectral scanner. neighborhoods central pixels different spectra provided feature vector last mixed soil class removed make small ﬁgures legible leaving classes training samples. hidden nodes regularization coefﬁcient trained data providing good separation data points errors mixing class vectors. stable solution? point class selected noise points generated placing gaussian variance added providing additional points display feature space areas reliability classiﬁcation high noise points staying within cluster triangles circles crosses. many points generated near vectors squares diamonds class region none network outputs strong value additional vectors line corner representing wrong class center indicating output value signiﬁcantly greater zero. images vectors appear center wrong cluster showing network still conﬁdent predictions sharp decision borders close data points. recognizing existence regions obviously important safety critical applications. neural networks used various ways data visualization. activity hidden neurons networks displayed directly. self-organized-maps competitive learning algorithms neural principal independent component analysis algorithms autoassociative feedforward networks neuroscale algorithms aimed using neural algorithms reduce dimensionality data display visualization method presented rather different since neural networks modiﬁed used display multidimensional data directly rather projection method introduced elucidate network function. method applicable black classiﬁcation system outputs estimation class memberships. although linear projection cannot show details higher dimensional data distribution contains useful information. classes images data vectors appear square corners coresponding uniquely classiﬁed cases unknown case cases overlapping regions. detailed information images training data vectors mapped neural networks used show dynamics learning compare different network solution inspecting regions input space potential problems arise evaluate effects regularization investigate stability network classiﬁcation perturbation original vectors place data relation known data vectors allowing estimation conﬁdence classiﬁcation given vector. best network solutions overconﬁdent show large clusters points around vertices polygon without overlaps clusters vectors close center projection. type visualization also combined receiver operator characteristic curves show detection rates given false alarm rate samples images close polygon vertices correspond high probability assigned classiﬁer. leaving data vectors speciﬁed detection rate leave images close polygon vertices. moving higher detection rates number errors observed roughly inversely proportional slope curve. scatterograms carry information showing type errors made allowing quick identiﬁcation data vectors. common practice selecting largest network output value class indicator leads optimal decision borders well separated images scatterograms; accurate decision boundaries image space selected. number options remains investigated including applications visualization dynamic data. reason scatterogram images known data always displayed part neural network output. although visualization open black completely least adds color elucidate function. acknowledgement. grateful jankowski discussions network function visualization particular idea polygon corners projections. initial version matlab software used simulations presented paper developed orlowski part thesis. paper presented international joint conference neural networks blake c.j. merz repository machine learning databases http//www.ics.uci.edu/∼mlearn/mlrepository.html. university california irvine dept. information computer science viinsualization multidimensional data. thesis dept formatics nicolaus copernicus university available http//www.phys.uni.torun.pl/kmk/publications.html", "year": 2018}