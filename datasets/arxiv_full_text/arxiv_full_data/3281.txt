{"title": "Optimizing affinity-based binary hashing using auxiliary coordinates", "tag": ["cs.LG", "cs.CV", "math.OC", "stat.ML"], "abstract": "In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as a corrected, iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.", "text": "supervised binary hashing wants learn function maps high-dimensional feature vector vector binary codes application fast image retrieval. typically results diﬃcult optimization problem nonconvex nonsmooth discrete variables involved. much work simply relaxed problem training solving continuous optimization truncating codes posteriori. gives reasonable results quite suboptimal. recent work tried optimize objective directly binary codes achieved better results hash function still learned posteriori remains suboptimal. propose general framework learning hash functions using aﬃnity-based loss functions uses auxiliary coordinates. closes loop optimizes jointly hash functions binary codes gradually match other. resulting algorithm seen corrected iterated version procedure optimizing ﬁrst codes learning hash function. compared this optimization guaranteed obtain better hash functions much slower demonstrated experimentally various supervised datasets. addition framework facilitates design optimization algorithms arbitrary types loss hash functions. information retrieval arises several applications obviously search. example image retrieval user interested ﬁnding similar images query image. computationally essentially involves deﬁning high-dimensional feature space relevant image represented vector ﬁnding closest points vector query image according suitable distance example features sift gist euclidean distance purpose. finding nearest neighbors dataset images vector dimension slow since exact algorithms essentially time space practice approximated successful binary hashing here given high-dimensional vector hash function maps b-bit vector nearest neighbor search done binary space. costs time space orders magnitude faster typically crucially operations binary vectors fast hardware support entire dataset memory rather slow memory disk. disadvantage results inexact since neighbors binary space identical neighbors original space. however approximation error controlled using suﬃciently many bits learning good hash function. topic much work recent years. general approach consists deﬁning supervised objective small value good hash functions minimizing ideally objective function minimal neighbors given image original binary spaces. practically information retrieval often evaluated using precision recall. however ideal objective cannot easily optimized hash functions uses approximate objectives instead. many objectives proposed literature. focus aﬃnity-based loss functions directly preserve original similarities binary space. speciﬁcally consider objective functions form high-dimensional dataset feature vectors minh means minimizing parameters hash function loss function compares codes images ground-truth value measures aﬃnity original space images often restricted subset image pairs keep runtime low. examples objective functions include models developed dimension reduction spectral laplacian eigenmaps locally linear embedding nonlinear elastic embedding t-sne well objective functions designed speciﬁcally binary hashing supervised hashing kernels binary reconstructive embeddings semi-supervised sequential projection learning hashing hash function continuous function input parameters could simply apply chain rule compute derivatives parameters objective function apply nonlinear optimization method gradient descent. would guaranteed converge optimum mild conditions would global objective convex local otherwise hence optimally learning function would principle doable although would still slow objective quite nonlinear involve many terms. binary hashing optimization much diﬃcult addition previous issues hash function must output binary values hence problem generally nonconvex also nonsmooth. view this much work sidestepped issue settled simple suboptimal solution. first deﬁnes objective function directly b-dimensional codes image optimizes assuming continuous codes then binarizes codes image. finally learns hash function given codes. optimizing aﬃnity-based loss function done using spectral methods nonlinear optimization described above. binarizing codes done diﬀerent ways simply rounding using zero threshold optimally ﬁnding threshold rotating continuous codes thresholding introduces less error finally learning hash function output bits considered binary classiﬁcation problem resulting classiﬁers collectively give desired hash function solved using various machine learning techniques. several works used approach produce reasonable hash functions order better needs take account optimization fact codes constrained binary. implies attempting directly discrete optimization aﬃnity-based loss function binary codes. daunting task since usually np-complete problem binary variables altogether practical applications could make number large millions beyond. recent works applied alternating optimization this optimizes usually small subset binary variables given ﬁxed values remaining ones result competitive precision/recall compared state-of-the-art. still slow future work likely improve provides option learn better binary codes. three-step suboptimal approach mentioned works manage join ﬁrst steps hence learn binary codes. then learns hash function given binary codes. better? indeed paper show elements problem incorporated single algorithm optimizes jointly them. hence initializing binary codes previous approach algorithm guaranteed achieve lower error learn better hash functions. fact framework seen iterated corrected version two-step approach learn binary codes given current hash function learn hash functions given codes iterate achieve principled recently proposed method auxiliary coordinates optimizing nested systems i.e. consisting composition functions processing stages. introduces variables constraints cause decoupling stages resulting mentioned alternation learning hash function learning binary codes. section reviews aﬃnity-based loss functions section describes mac-based proposed framework section evaluates several supervised datasets using linear nonlinear hash functions section discusses implications work. related work although construct hash functions without training data focus methods learn hash function given training since perform better emphasis optimization. learning unsupervised attempts preserve distances original space supervised addition attempts preserve label similarity. many objective functions proposed achieve focus aﬃnity-based ones. create aﬃnity matrix subset training points based distances labels combine loss function methods optimize directly hash function. example binary reconstructive embeddings alternating optimization weights hash functions. supervised hashing kernels learns hash functions sequentially considering diﬀerence inner product codes corresponding element aﬃnity matrix. although many approaches exist common theme apply greedy approach ﬁrst ﬁnds codes using aﬃnity-based loss function hash functions codes found relaxing problem binarizing solution approximately solving binary codes using form alternating optimization two-step hashing using relaxation ways dimensionality reduction literature developed number objective functions form low-dimensional projection high-dimensional data point free real-valued parameter. neighborhood information encoded values representative example elastic embedding form ﬁrst term tries project true neighbors close together second repels non-neighbors’ projections other. laplacian eigenmaps locally linear embedding result replacing second term constraint ﬁxes scale results eigenproblem rather nonlinear optimization also produces distorted embeddings. objectives exist t-sne separate functions pairs points. optimizing nonlinear embeddings quite challenging much progress done recently although models developed produce continuous projections successfully used binary hashing truncating codes using two-step approach similar dissimilar. binary reconstructive embeddings xmk. exponential variant uses proposed uses exp. approach applied loss functions though mostly focus loss simplicity. variables binary call optimization problems binary embeddings analogy traditional continuous embeddings dimension reduction. optimization loss diﬃcult thresholded hash function appears argument loss function recently proposed method auxiliary coordinates meta-algorithm construct optimization algorithms nested functions. proceeds stages. first introduce variables equality constraints problem goal unnesting function. achieve introducing binary vector point. transforms original unconstrained problem following constrained problem seen equivalent eliminating recognize objective function embedding form loss function except free parameters fact constrained deterministic outputs hash function second solve constrained problem using penalty method quadratic-penalty augmented lagrangian discuss former simplicity. solve following minimization problem progressively increasing constraints eventually satisﬁed optimize binary codes given images). seen regularized binary embedding projections encouraged close hash function outputs here diﬀerent approaches modiﬁcations. similar two-step approach except latter learns codes isolation rather given current hash function iterating two-step approach would change nothing optimize loss precisely corresponds optimizing practice start small value increase slowly optimizing equality constraints satisﬁed i.e. possible prove step algorithm make changes since constraints satisﬁed. gives reliable stopping criterion easy check algorithm stop ﬁnite number iterations also possible prove path minimizers continuous penalty parameter fact discrete changes happening ﬁnite number values based practical experience found following approach leads good schedules little eﬀort. exponential schedules form µαi− user parameters initial multiplier choose exponential schedules typically algorithm makes progress beginning important track good minimum there. upper value past changes occur reached exponential schedule ﬁnite number iterations stopping criterion detect that. multiplier value small computationally convenient. small algorithm take many iterations even change algorithm reach quickly stopping point without time better minimum. initial estimate trying values changes initial value start occur. overall computational time required estimate comparable running extra iterations algorithm. finally practice form early stopping order improve generalization. small validation evaluate precision achieved hash function along optimization. precision decreases previous step ignore step skip next value besides helping avoid overﬁtting saves computation avoid extra optimization steps. since validation small provides noisy estimate generalization ability current iterate occasionally leads skipping valid value. problem next value close skipped likely work. point optimization reach overﬁtting region precision stops increasing algorithm skip remaining values stops. summary using validation procedure guarantees precision greater equal initial thus resulting better hash function. binary vector hence one-bit hash functions parallel concatenate b-bit hash function. binary classiﬁcation problem using number misclassiﬁed patterns loss. allows regular classiﬁer even simpler surrogate loss since also enforce constraints eventually example optimizing margin plus slack using high penalty misclassiﬁed patterns. discuss classiﬁers experiments. although technique signiﬁcantly simpliﬁed original problem step still complex. involves ﬁnding binary codes given hash function np-complete problem binary variables. fortunately recent works proposed practical approaches problem based alternating optimization quadratic surrogate method graphcut method cases would correspond ﬁrst step two-step hashing quadratic surrogate graphcut method starting point apply alternating optimization points given remaining bits ﬁxed points solve optimization approximately. describe next method. start describing method original form give modiﬁcation make work step objective solution using quadratic surrogate method based fact loss function depends hamming distance binary variables equivalently written quadratic function binary variables since case every term write ﬁrst term binary quadratic problem. consider second term well. optimization written loss function deﬁned point vector containing binary codes point except binary code point generated hash function show replaced binary quadratic function hi)t vector length terms minimization quadratic binary variables. still np-complete problem approximate relaxing continuous quadratic program binarizing solution. general matrix positive deﬁnite relaxed convex need initialization. construct initialization converting binary binary eigenproblem solve problem spectral relaxation constraints relaxed solution problem eigenvector corresponding smallest eigenvalue truncated eigenvector initialization minimizing relaxed boundconstrained noted above step np-complete problem general cannot expect global optimum. even possible approximate solution could increase objective previous iteration’s occurs simply skip update order guarantee decrease monotonically avoid oscillating around minimum. solution using graphcut algorithm optimize minimize general np-complete problem bits form quadratic function binary variables. apply graphcut algorithm proposed fasthash algorithm proceeds follows. first assign data points diﬀerent possibly overlapping groups then minimize objective function binary codes block binary codes ﬁxed proceed next block etc. speciﬁcally optimize bits unlike quadratic surrogate method using graphcut algorithm alternating optimization blocks deﬁning submodular functions guaranteed lower equal objective value initial therefore decrease monotonically tested framework several combinations loss function hash function number bits datasets comparing several state-of-the-art hashing methods report representative subset show ﬂexibility approach. esplh loss functions. test quadratic surrogate graphcut methods step mac. hash functions linear svms kernel svms following labeled datasets cifar contains images classes. gist features image. images training test. inﬁnite mnist generated using elastic deformations original mnist handwritten digit dataset images training test classes. represent image vector pixels. computational cost aﬃnity-based methods previous work used training sets limited thousand points train hash functions subset points training report precision recall searching test query entire dataset report precision precision/recall test queries using ground truth training points label. precision curves retrieved contains nearest neighbors query point hamming space. report precision diﬀerent values test robustness diﬀerent algorithms. precision/recall curves retrieved contains points inside hamming distance query point. curves show precision recall diﬀerent hamming distances report zero precision neighbor inside hamming distance query. happens time large small. precision/recall curves precision drops signiﬁcantly small large values small values happens query points retrieve neighbor. large values happens number retrieved points becomes large. main comparison point quadratic surrogate graphcut methods denote section quad respectively regardless hash function resulting codes. correspondingly denote version macquad maccut respectively. following schedule penalty parameter algorithm initialize i.e. result quad cut. starting multiply iteration experiments show algorithm indeed ﬁnds hash functions signiﬁcantly consistently lower objective function value rounding two-step approaches outperforms state-of-the-art algorithms diﬀerent datasets maccut beating macquad time. improvement precision makes using well worth relatively small extra runtime minimal additional implementation eﬀort requires. plots vertical arrows indicate improvement maccut macquad quad. goal paper introduce aﬃnity-based loss hash function describe generic framework construct algorithms optimize given combination thereof. illustrate eﬀectiveness cifar dataset diﬀerent sizes retrieved neighbor sets using bits. optimize aﬃnity-based loss functions esplh) hash functions cases algorithm achieves better hash function terms loss precision/recall. compare ways optimizing loss function quad macquad maccut. train kernel radial basis functions centers given random subset training points apply linear output. computationally fast constant gram matrix. using hash function kernel trained libsvm gave similar results much slower support vectors change labels change. bandwidth average euclidean distance ﬁrst points. neighbors chosen random diﬀerent label respectively. fig. shows loss function methods iterations algorithm well precision recall. clear maccut macquad reduce loss function quad respectively well original algorithm cases type hash function number bits hence applying always beneﬁcial. reducing loss nearly always translates better precision recall gain maccut /macquad /quad signiﬁcant often comparable gain obtained changing linear kernel hash function within algorithm. usually outperforms quad correspondingly maccut outperforms macquad. interestingly macquad maccut similar even though started diﬀerently. suggests crucial methods step although still prefer usually produces somewhat better optima. finally shows maccut results using esplh loss. settings ﬁrst experiment. before maccut outperforms loss function precision/recall using either linear kernel svm. two-step approaches starting point free binary codes obtained minimizing loss codes without output particular hash function. minimizing without constraints resulting free codes achieve good precision/recall independently whether hash function actually produce codes. constraining codes realizable speciﬁc family hash functions means loss larger free codes. diﬃcult hash function produce free codes? fig. plots loss function free codes two-step codes codes maccut linear kernel hash functions experiment clear free codes loss kernel function produce even farther linear function produce. relatively smooth functions cannot represent presumably complex structure free codes. could improved using ﬂexible hash function could better approximate free codes ﬂexible function would likely generalize well require fast hash functions fast retrieval anyway. given linear kernel hash functions two-step optimization hash function directly free codes. guaranteed best hash function terms original problem indeed produces pretty suboptimal function. contrast gradually optimizes codes hash function eventually match ﬁnds better hash function original problem np-complete). fig. illustrates conceptually. shows space possible binary codes contours codes produced linear hash functions feasible +}b×n linear two-step codes project free codes onto feasible codes optimal hash function runtime iteration -point training sets bits neighbors laptop maccut macquad. stop iterations. iteration comparable single quad since step dominates computation. iterations ﬁrst faster warm-started. fig. shows results cifar inﬁnite mnist. create aﬃnities methods using dataset labels before similar neighbors dissimilar neighbors. compare macquad maccut two-step hashing fasthash hashing kernels iterative quantization binary reconstructive embeddings self-taught hashing macquad maccut quad loss function results show maccut generally outperform methods often large margin nearly situations particular maccut macquad ones beat long uses suﬃciently many bits. figure comparison binary hashing methods cifar inﬁnite mnist using linear hash function using bits. rows panel show precision retrieved points range precision/recall diﬀerent hamming distances. two-step approaches algorithm aﬃnity-based loss functions two-step approach two-step hashing fasthash signiﬁcant advance ﬁnding good codes binary hashing also causes maladjustment codes hash function since codes learned without knowledge hash function would them. ignoring interaction loss hash function limits quality results. example linear hash function harder time nonlinear learning codes. algorithm tradeoﬀ enforced gradually step regularization term ﬁnds best codes according loss function makes sure close realizable current hash function. experiments demonstrate signiﬁcant consistent gains achieved loss function value precision/recall image retrieval two-step approach. similar well-known situation arises feature selection classiﬁcation best combination classiﬁer features result jointly minimizing classiﬁcation error respect classiﬁer features rather ﬁrst selecting features according criterion using learn particular classiﬁer point method auxiliary coordinates algorithmically decouples elements make binary hashing model hash function loss function. elements combination produce function maps input patterns binary codes represent neighborhood input space play distinct roles. hash function role input patterns binary codes. loss function role assign binary codes input patterns order preserve neighborhood relations regardless easy mapping produce binary codes. itself loss function would produce nonparametric hash function training form table pairs. however hash function loss function cannot independently objective function depends both. optimal combination hash loss diﬃcult obtain nonlinear discrete nature objective. several previous optimization attempts binary hashing ﬁrst codes optimize loss hash function them thus imposing strict suboptimal separation loss function hash function. elements decoupled within iteration still optimizing correct objective step hash function involve loss step codes involve hash function iterated. connection steps occurs auxiliary coordinates binary codes themselves. penalty regularizes loss optimal codes progressively closer hash function given class achieve. best type hash function use? answer unique depends application-speciﬁc factors quality codes produced time compute codes high-dimensional data ease implementation within given hardware architecture software libraries etc. framework facilitates considerably choice training diﬀerent types hash functions simply involves reusing existing classiﬁcation algorithm within step changes step. terms runtime resulting algorithm much slower two-step approach; comparable iterating latter times. besides since iterations except ﬁrst warm-started average cost iteration lower two-step approach. finally note method auxiliary coordinates used also learn out-of-sample mapping continuous embedding elastic embedding t-sne —rather learn hash functions discrete embedding case binary hashing. resulting algorithm optimizes out-of-sample mapping auxiliary coordinates alternating steps. step optimizes out-of-sample mapping projects highdimensional points continuous latent space given auxiliary coordinates regression problem binary hashing classiﬁcation problem step optimizes auxiliary coordinates given mapping regularized continuous embedding problem. steps solved using existing algorithms. particular solving step done eﬃciently large datasets using -body methods eﬃcient optimization techniques binary hashing step combinatorial optimization present challenging solve. however continuous embeddings must drive penalty parameter inﬁnity constraints satisﬁed solution follows continuous path binary hashing solution follows discretized piecewise path terminates ﬁnite value binary autoencoder aﬃnity-based loss trained method auxiliary coordinates also applied context binary hashing diﬀerent objective function binary autoencoder hash function encoder decoder. aﬃnity-based loss function algorithm alternates ﬁtting hash function given codes optimizing codes. however binary autoencoder optimization codes decouples every data point important computational advantage step rather solve large optimization problem binary variables solve small optimization problems variables much faster easier solve parallelize. also objective require neighborhood information scales linearly dataset. computing aﬃnity values even ﬁnding pairs neighbors ﬁrst place computationally costly. reasons scale training larger datasets aﬃnity-based loss functions. objective function disadvantage less directly related goals desirable information retrieval point view precision recall. neighborhood relations indirectly preserved autoencoders whose direct reconstruct inputs thus learn data manifold aﬃnity-based loss functions form allow user specify complex neighborhood relations example based class labels signiﬁcantly diﬀer actual distances image feature space. still ﬁnding eﬃcient scalable optimization methods binary embeddings able handle larger numbers training neighbor points would improve quality loss function. important topic future research. proposed general framework optimizing binary hashing using aﬃnity-based loss functions. improves previous two-step approaches based learning binary codes ﬁrst learning hash function. instead optimizes jointly binary codes hash function alternation binary codes eventually match hash function resulting better local optimum aﬃnitybased loss. possible introducing auxiliary variables conditionally decouple codes hash function gradually enforcing corresponding constraints. framework makes easy design optimization algorithm choice loss function hash function simply reuses existing software optimizes isolation. resulting algorithm much slower suboptimal two-step approach—it comparable iterating latter times—and well worth improvement precision/recall. step hash function essentially solved problem using classiﬁer since learned accurate scalable using machine learning techniques. diﬃcult timeconsuming part approach optimization binary codes np-complete involves many binary variables terms objective. although techniques exist produce practical results designing algorithms reliably good local optima scale large training sets important topic future research. another direction future work involves learning sophisticated hash functions beyond mapping image features onto output binary codes using simple classiﬁers svms. possible optimization hash function parameters conﬁned step takes form supervised classiﬁcation problem apply array techniques machine learning computer vision. example possible learn image features work better hashing standard features sift learn transformations input binary codes invariant translation rotation alignment. although aﬃnity-based hashing intended work supervised datasets also used unsupervised ones approach applies well. siftm dataset contains training high-resolution color images test images represented sift features. experiments conclusions generally supervised datasets small diﬀerences settings experiments. order construct aﬃnity-based objective function deﬁne neighbors follows. point training nearest neighbors positive neighbors points chosen randomly among remaining points negative neighbors. report precision precision/recall test queries using ground truth nearest neighbors unsupervised datasets training points label supervised datasets. fig. shows results using eslph loss functions respectively diﬀerent sizes retrieved neighbor sets using bits. supervised datasets clear algorithm ﬁnds better optima maccut generally better macquad. fig. shows case using quad outperforms correspondingly macquad outperforms maccut although results close particularly precision recall. fig. shows results comparing binary hashing methods. methods trained subset points. consider types methods. ﬁrst type create pseudolabels point apply supervised methods cifar pseudolabels training point obtained declaring similar points true nearest neighbors dissimilar points random subset points among remaining points. second type purely unsupervised methods thresholded iterative quantization binary autoencoder spectral hashing anchorgraph hashing spherical hashing results general agreement conclusions main paper. comparison using code utilization fig. shows results eﬀective number bits beﬀ. measure code utilization hash function introduced carreira-perpi˜n´an raziperchikolaei deﬁned entropy code distribution. given codes training consider samples distribution possible codes. entropy distribution measured bits test set. although code utilization correlates extent precision/recall ranking diﬀerent methods large guarantee good hash function indeed tpca typically achieves largest beﬀ; discussion carreira-perpi˜n´an raziperchikolaei however large indicate better available codes advantage precision/recall depend user parameters compare binary hashing methods single number particularly useful compare methods optimizing objective function. mind compare maccut macquad quad pairs methods optimize objective function. figure code utilization eﬀective number bits diﬀerent hashing algorithms using bits siftm dataset. plots correspond codes obtained algorithms ﬁgure solid lines training dashed lines test set. diagonal-horizontal black dotted lines give upper bound algorithm training test sets", "year": 2015}