{"title": "Predictive Business Process Monitoring with LSTM Neural Networks", "tag": ["stat.AP", "cs.DB", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Predictive business process monitoring methods exploit logs of completed cases of a process in order to make predictions about running cases thereof. Existing methods in this space are tailor-made for specific prediction tasks. Moreover, their relative accuracy is highly sensitive to the dataset at hand, thus requiring users to engage in trial-and-error and tuning when applying them in a specific setting. This paper investigates Long Short-Term Memory (LSTM) neural networks as an approach to build consistently accurate models for a wide range of predictive process monitoring tasks. First, we show that LSTMs outperform existing techniques to predict the next event of a running case and its timestamp. Next, we show how to use models for predicting the next task in order to predict the full continuation of a running case. Finally, we apply the same approach to predict the remaining time, and show that this approach outperforms existing tailor-made methods.", "text": "abstract. predictive business process monitoring methods exploit logs completed cases process order make predictions running cases thereof. existing methods space tailor-made speciﬁc prediction tasks. moreover relative accuracy highly sensitive dataset hand thus requiring users engage trial-anderror tuning applying speciﬁc setting. paper investigates long short-term memory neural networks approach build consistently accurate models wide range predictive process monitoring tasks. first show lstms outperform existing techniques predict next event running case timestamp. next show models predicting next task order predict full continuation running case. finally apply approach predict remaining time show approach outperforms existing tailor-made methods. predictive business process monitoring techniques concerned predicting evolution running cases business process based models extracted historical event logs. range techniques proposed variety prediction tasks predicting next activity predicting future path running case predicting remaining cycle time predicting deadline violations predicting fulﬁllment property upon completion predictions generated techniques range applications. example predicting next activity predicting sequence future activities case provide valuable input planning resource allocation. meanwhile predictions remaining execution time used prioritize process instances order fulﬁll service-level objectives existing predictive process monitoring approaches tailor-made speciﬁc prediction tasks readily generalizable. moreover relative accuracy varies signiﬁcantly depending input dataset point time prediction made. technique outperform another given prediction point under-perform another prediction point earlier prediction point cases multiple techniques need combined considerable tuning required order achieve consistent accuracy. recurrent neural networks long short-term memory architectures shown deliver consistently high accuracy several sequence modeling application domains e.g. natural language processing speech recognition recently evermann applied lstms predictive process monitoring speciﬁcally predict next activity case. inspired results paper investigates following questions lstms applied broad range predictive process monitoring problems how? lstms achieve consistently high accuracy across range prediction tasks event logs prediction points? address questions paper puts forward lstm architectures predicting next activity running case timestamp; continuation case completion; remaining cycle time. outlined lstm architectures empirically compared tailor-made approaches respect accuracy diﬀerent prediction points using four real-life event logs. paper structured follows. section discusses related work. section introduces foundational concepts notation. section describes technique predict next activity case timestamp compares tailor-made baselines. section extends previous technique predict continuation running case. section shows latter method used predict remaining time case compares tailor-made approaches. section concludes paper outlines future work directions. section discusses existing approaches predictive process monitoring three prediction tasks time-related predictions predictions outcome case predictions continuation case and/or characteristics thereof. range research proposals addressed problem predicting delays deadline violations business processes. pika propose technique predicting deadline violations. metzger present techniques predicting late show events freight transportation process. senderovich apply queue mining techniques predict delays case executions. another body work focuses predicting remaining cycle time running cases. dongen predict remaining time using nonparametric regression models based case variables aalst propose remaining time prediction method constructing transition system event using sequence abstractions. rogge-solti weske stochastic petri nets predict remaining time process taking account elapsed time since last observed event. folino develop ad-hoc clustering approach predict remaining time overtime faults. paper show prediction remaining cycle time approached special case prediction process continuation. speciﬁcally approach proven generally provide better accuracy goal approaches category predict cases undesirable state. maggi propose framework predict outcome case based sequence activities executed given case values data attributes last executed activity case. latter framework constructs classiﬁer on-the-ﬂy based historical cases similar trace running case. approaches construct collection classiﬁers oﬄine. example construct classiﬁer every possible prediction point meanwhile apply clustering techniques group together similar preﬁxes historical traces construct classiﬁer cluster. approaches require extract feature vector preﬁx ongoing trace. leoni propose framework classiﬁes possible approaches extract feature vectors. breuker probabilistic ﬁnite automaton tackle next-activity prediction problem evermann lstms. using latter approach baseline propose lstm architecture solves nextactivity prediction problem higher accuracy generalized prediction problems. lakshmanan markov chains estimate probability future execution given task running case. meanwhile spoel address ambitious problem predicting entire continuation case using shortest path algorithm causality graph. polato reﬁne approach mining annotated transition system event annotating edges transition probabilities. paper take latter approach baseline show lstms improve providing higher generalizability. given denotes sequences sequence length empty sequence concatenation sequences preﬁx length sequence suﬃx. example sequence event universe i.e. possible event identiﬁers time domain. assume events characterized various properties e.g. event timestamp corresponds activity performed particular resource etc. impose speciﬁc properties however given focus paper assume properties given trace property often need compute sequence consisting value property event trace. lift function maps event value property apply sequences events neural network consists layer inputs units layer outputs units multiple layers in-between referred hidden units. outputs input units form inputs units ﬁrst hidden layer outputs units hidden layer form input subsequent hidden layer. outputs last hidden layer form input output layer. output unit function weighted inputs. weights weighted performed unit learned gradient-based optimization training data consists example inputs desired outputs example inputs. recurrent neural networks special type neural networks connections neurons form directed cycle. rnns unfolded shown figure step unfolding referred time step input time step rnns take arbitrary length sequence input providing feature representation element sequence time step. hidden state time step contains information extracted time steps hidden state updated information input time step vectors weights inputs hidden state respectively. function known activation function usually either hyperbolic tangent logistic function often +exp neural network referred sigmoid function sigmoid literature sigmoid function often represented letter fully write sigmoid avoid confusion traces. output step long short-term memory model special recurrent neural network architecture powerful modeling capabilities long-term dependencies. main distinction regular lstm latter complex memory cell replacing value state result function weighted average lstm state accessed written cleared controlling gates respectively information input accumulated memory cell activated. additionally past memory cell status forgotten activated. information propagated output based activation output gate combined lstm model described following formulas start predicting next activity case timestamp learning activity prediction function functions preﬁx length transform event feature vector vectors lstm inputs build feature vector follows. start features represent type consistent ordering activities index |a|} indicate position activity one-hot encoding assigns value feature number index value features. three time-based features one-hot encoding feature vector. ﬁrst time-based feature event time previous event feature allows lstm learn dependencies time diﬀerences diﬀerent points process. many activities performed oﬃce hours therefore time feature contains time within contains time within week added learn lstm last event observed occurred working working week time next event expected longer. time step one-hot encoding activity event time step later. however case case ends time case event predict. therefore extra element output one-hot-encoding vector value case ends second target output equal feature next time step i.e. target time diﬀerence next current event. however knowing timestamp current event calculate timestamp following event. optimize weights neural network adam learning algorithm cross entropy ground truth one-hot encoding next event predicted one-hot encoding next event well mean absolute error ground truth time next event predicted time next event minimized. time prediction function lstms done using several architectures. firstly train separate models using input features time step represented figure secondly learned jointly single lstm model generates outputs multi-task learning setting usage lstms multi-task learning setting shown improve performance individual tasks jointly learning multiple natural language processing tasks including part-of-speech tagging named entity recognition sentence classiﬁcation hybrid option architecture figures architecture number shared lstm layers tasks followed number layers specialize either prediction next activity prediction time next event shown figure noted activity prediction function implemented technique python scripts using recurrent neural network library keras experiments performed single nvidia tesla experiments took seconds training iteration depending neural network architecture. execution time make prediction order milliseconds. section describe motivate metrics datasets baseline methods used evaluation predictions next activities timestamps next events. best knowledge existing technique predict next activity timestamp. therefore utilize baseline method activity prediction diﬀerent timestamp prediction. well-known error metrics regression tasks mean absolute error root mean square error time diﬀerences events tend highly varying values diﬀerent orders magnitude. evaluate predictions using rmse would sensitive errors outlier data points time events large. remaining cycle time prediction method proposed aalst naturally adjusted predict time next event. build transition system event using either sequence abstraction instead annotate transition system states average time next event. approach baseline predict timestamp next event. evaluate performance predicting next activity timestamp datasets. chronologically ordered ﬁrst traces training data evaluate activity time predictions remaining traces. evaluate next activity timestamp prediction helpdesk dataset contains events ticketing management process help desk italian software company. process consists activities cases start insertion ticket ticketing management system. case ends issue resolved ticket closed. contains around cases events. bpi’ subprocess dataset event originates business process intelligence challenge contains data application procedure ﬁnancial products large ﬁnancial institution. process consists three subprocesses tracks state application tracks states work items associated application third tracks state oﬀer. context predicting coming events timestamps interested events performed automatically. thus narrow evaluation work items subprocess contains events manually executed. further ﬁlter retain events type complete. existing techniques next activity prediction described section evaluated event identical preprocessing enabling comparison. table shows performance various lstm architectures helpdesk bpi’ subprocess logs terms predicted time accuracy predicting next event. speciﬁc preﬁx sizes chosen represent short medium long traces log. thus bpi’ contains longer traces preﬁx sizes evaluated higher log. table reports average performance preﬁxes three preﬁx sizes reported three preceding columns. number shared layers represents number layers contribute time activity prediction. rows numbers shared layers correspond architecture figure prediction time activities performed separate models. number shared layers equal number layers neural network contains specialized layers corresponding architecture figure table also shows results predicting time next event using adjusted method aalst comparison. lstm architectures outperform baseline approach preﬁxes well averaged preﬁxes datasets. further observed performance gain best lstm model best baseline model much larger short preﬁx long preﬁx. best performance obtained next activity prediction preﬁxes classiﬁcation accuracy helpdesk log. bpi’ best accuracy higher accuracy reported breuker accuracy reported evermann fact results obtained lstm consistently higher approaches. even though evermann also rely lstm approach several diﬀerences likely cause performance gap. first uses technique called embedding create feature descriptions events instead features described above. embeddings automatically transform activity useful large dimensional continuous feature vector. approach shown work really well ﬁeld natural language processing number distinct words predicted large process mining event logs number distinct activities event often order hundreds much less useful feature vector learned automatically. second uses two-layer architecture neurons layer explore variants. found performance decrease increasing number neurons makes likely performance neuron model decrease overﬁtting. third last explanation performance diﬀerence multi-task learning showed slightly improves prediction performance next activity. even though performance diﬀerences three lstm architectures small logs observe best performances lstm model terms time prediction next activity prediction either obtained completely shared architecture figure hybrid architecture figure experimented decreasing number neurons layer increasing architectures shared layer found results decreasing performance tasks. likely neurons resulted underﬁtting models neurons resulted overﬁtting models. also experimented traditional rnns layer architectures found perform signiﬁcantly worse lstms time activity prediction. using functions repeatedly allows make longer-term predictions predict ahead single time step. refer activity time next event prediction functions predict whole continuation running case functions given trace preﬁx evaluate performance calculating distance predicted continuation actual continuation πa). many sequence distance metrics exist levenshtein distance well-known ones. levenshtein distance deﬁned minimum number insertion deletion substitution operations needed transform sequence other. actual next events consider minor error since often relevant order parallel activities executed. however levenshtein distance would assign cost prediction transforming predicted sequence ground truth sequence would require deletion insertion operation. evaluation measure better reﬂects prediction quality damerau-levenstein distance adds swapping operation operations used levenshtein distance. damerau-levenshtein rable results traces variable length normalize damerau-levenshtein distance maximum length ground truth suﬃx length predicted suﬃx subtract normalized damerau-levenshtein distance obtain damerau-levenshtein similarity best knowledge recent method predict arbitrary number events ahead polato authors ﬁrst extract transition system learn machine learning model transition system state predict next activity. evaluate predictions ﬁxed number events ahead interested continuation case end. redid experiments prom plugin obtain performance predicted full case continuation. lstm experiments two-layer architecture shared layer neurons layer showed good performance terms next activity prediction predicting time next event previous experiment addition previously introduced environmental permit dataset environmental permitting process dutch municipality. case refers permit application. contains cases events event types. almost every case follows unique path making suﬃx prediction challenging. table summarizes results suﬃx prediction log. seen lstm outperforms baseline logs. even though improves baseline performance bpi’ given contains activities. inspection found contains many sequences events activity occurrences identical events uncommon. found lstms problems dealing characteristic causing predict overly long sequences activity resulting predicted suﬃxes much longer ground truth suﬃxes. hence also evaluated suﬃx prediction modiﬁed version bpi’ removed repeated occurrences event keeping ﬁrst occurrence. however notice mild improvement unmodiﬁed log. remaining cycle time prediction time prediction function predicts timestamps events running case still come. since last predicted timestamp prediction generated timestamp case easy used predicting remaining cycle time running case. given unﬁnished case contains predicted timestamps next events contains predicted time therefore estimated remaining cycle time obtained architecture suﬃx prediction experiments. predict evaluate remaining time passed event starting preﬁx size remaining cycle time prediction methods aalst dongen baseline methods. figure shows mean absolute error preﬁx size four logs seen lstm consistently outperforms baselines helpdesk log. exception bpi’ lstm performs worse baselines short preﬁxes. caused problem lstms predicting next event many repeated events described section problem causes lstm predict suﬃxes long compared ground truth thereby also overestimating remaining cycle time. lstm outperform baseline modiﬁed version bpi’ kept ﬁrst occurrence repeated event sequence. note remove last event case even repeated event would change ground truth remaining cycle time preﬁx. foremost contribution paper technique predict next activity running case timestamp using lstm neural networks. showed technique outperforms existing baselines real-life data sets. additionally found predicting next activity timestamp single model yields higher accuracy predicting using separate models. showed basic technique generalized address predictive process monitoring problems predicting entire continuation running case predicting remaining cycle time. empirically showed generalized lstm-based technique outperforms tailor-made approaches problems. also identiﬁed limitation lstm models dealing traces multiple occurrences activity case model predicts overly long sequences event. addressing latter limitation direction future work. proposed technique extended prediction tasks prediction aggregate performance indicators case outcomes. latter task approached classiﬁcation problem wherein neuron output layer predicts probability corresponding outcome. another avenue future work extend feature vectors additional case event attributes finally plan extend multi-task learning approach predict attributes next activity besides timestamp. reproducibility. source code supplementary material required reproduce experiments reported paper found http// verenich.github.io/processsequenceprediction.", "year": 2016}