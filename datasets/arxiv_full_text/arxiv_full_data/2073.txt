{"title": "Asymptotic Model Selection for Directed Networks with Hidden Variables", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We extend the Bayesian Information Criterion (BIC), an asymptotic approximation for the marginal likelihood, to Bayesian networks with hidden variables. This approximation can be used to select models given large samples of data. The standard BIC as well as our extension punishes the complexity of a model according to the dimension of its parameters. We argue that the dimension of a Bayesian network with hidden variables is the rank of the Jacobian matrix of the transformation between the parameters of the network and the parameters of the observable variables. We compute the dimensions of several networks including the naive Bayes model with a hidden root node.", "text": "extend bayesian information criterion asymptotic approximation marginal likelihood bayesian networks hidden variables. approximation used select models given large samples data. standard well extension punishes complexity model according dimension parameters. argue dimension bayesian network hidden variables rank jacobian matrix transformation parameters network parameters observable variables. compute dimensions several networks including naive bayes model hidden root node. learning bayesian networks data extends applicability situations data easily obtained expert knowledge expensive. consequently subject much research recent years buntine researchers pursued types approaches learning bayesian networks uses independence tests direct search among valid models another uses score search best scored network—a procedure known model selection. scores based exact bayesian computations developed cooper herskovits spiegelhalter buntine heckerman scores based minimum description length developed bacchus suzuki case instance variables bayesian network network structure bayesian network directed acyclic graph node associated random variable parameters associated network structure. stand hypothesis true objective joint distribution encoded network structure then bayesian measure goodness-of-ﬁt network structure problem model selection among bayesian networks hidden variables networks variables whose values observed difﬁcult model selection among networks without hidden variables. first space possible networks becomes inﬁnite second scoring network computationally harder must account possible values missing variables goal develop bayesian scoring approach networks include hidden variables. obtaining score computationally eﬀective conceptually simple allow select model among competing models. approach asymptotic approximation marginal likelihood. asymptotic approximation known bayesian information criteria equivalent rissanen’s minimum description length asymptotic approximation carried bayesian networks herskovits bouckaert hidden variables present. bouckaert shows marginal likelihood data given network structure given sample size data entropy probability distribution obtained projecting frequencies observed cases conditional probability tables bayesian network number parameters reveals qualitative preferences made bayesian approach. first suﬃcient data network structure i-map true distribution likely network structure i-map true distribution. second among network structures i-maps true distribution minimum number parameters likely. derived explicit formula probability network given data letting sample size inﬁnity using dirichlet prior parameters. nonetheless depend selected prior. section laplace’s method rederive without assuming dirichlet prior. derivation standard application asymptotic bayesian analysis. derivation useful gaining intuition hidden-variable case. section provide approximation marginal likelihood bayesian networks hidden variables give heuristic argument approximation using laplace’s method. obtain following equation introduce following notation bayesian network. number states variable variables corresponding parents number states pai. integer index states pai. write denote parents assigned state. θijk denote true probability parameter given θijk also assume θijk addition {θijk| denote parameters associated node given instance parents {θij| denote parameters associated node thus {θi| unambiguous instead tions usually made. first data assumed random sample bayesian network second network structure parameter sets mutually independent parameter sets θiqi assumed mutually independent third node parents distinct networks distribution parameters associated node identical networks fourth case complete. fifth prior distribution parameters associated node dirichlet— αijk interpreted equivalent number cases seen last assumptions made sake convenience. namely parameter distributions before data seen family dirichlet family. geiger heckerman provide characterization dirichlet distribution shows ﬁfth assumption implied ﬁrst three assumptions additional assumption equivalent bayesian networks value parameters network dimension value dimension model interpreted equivalent ways. first number free parameters needed represent parameter space near maximum likelihood value. second rank jacobian matrix transformation parameters network parameters observable variables. case dimension depends value contrast dimension ﬁxed throughout parameter space. section compute dimensions several network structures including naive bayes model hidden class node. section demonstrate scoring function used autoclass sometimes sections describe approach extended gaussian sigmoid networks. sets joint distributions) events equivalent well assumption made explicit hold causal networks arcs opposing directions correspond distinct hypotheses satisfy assumptions heckerman show must cooper–herskovits scoring function joint probability distribution obtained initial prior bayesian network speciﬁed user user’s eﬀective sample size conﬁdence prior network. sterling’s approximation. derivation hinges assumptions global local independence dirichlet prior although show result still holds without assumptions. intuitively large sample size data washes away contribution prior. shall rederive herskovits’ bouckaert’s asymptotic result. technique laplace’s method expand likelihood data around maximum likelihood value approximate peak using multivariate-normal distribution. quires assumptions discussed previous section. thermore derivation assumes prior around maximum likelihood value positive. finally argue next section derivation extended bayesian networks hidden variables. grows inﬁnity peak neighborhood around maximum becomes sharper. consequently ignore prior normal distribution around peak. furthermore assume bayesian networks function known. thus assumptions function veriﬁed. first note block diagonal matrix block corresponds variable particular instance plify notation assume three states. denote θijk ﬁxed. consider cases examine observations denote values obtained process. observation associate indicator functions function gets ﬁrst value case zero otherwise. similarly gets second value case zero otherwise. parameters true joint probability distribution corresponding every value value deﬁnes range curved manifold space deﬁned consider image values small region around manifold resemble euclidean space dimension small region around look like orthogonal coordinates φd}. thus log-likelihood function written function φ—log p—will become maximum likelihood equation single solution hessian positive deﬁnite because increases peak becomes sharper conditions general derivation met. plugging maximum likelihood value correct yields polynomial diagonalizing leading elements ﬁrst lines remain polynomials whereas lines dependent given every value become identically zero. rank falls values roots polynomials diagonalized matrix. roots measure zero. heuristic argument provide error term. image manifold curved might possible local region never become suﬃciently obtain bound error approximate marginal likelihood. conjecture that manifolds corresponding bayesian networks hidden variables local region always suﬃciently ﬂat. researchers shown bounds attainable variety statistical models although arguments researchers directly apply case possible extend methods prove conjecture. argued second term bayesian networks hidden variables rank jacobian matrix transformation parameters network parameters observable variables. section explain compute rank demonstrate approach several examples. theorem suggests random algorithm calculating rank. compute jacobian matrix symbolically equation computation possible since vector polynomials then assign random value diagonalize numeric matrix theorem guarantees that probability resulting rank regular rank every network select—say—ten values determine maximum resulting ranks. experiments none columns correspond diﬀerentiation respect θx|h θx|h θx|¯h θx|¯h respectively. symbolic computation rank matrix carried out; shows regular rank equal dimension matrix—namely nonetheless argued order compute regular rank simply choose random values diagonalize resulting numerical matrix. done naive bayes models binary hidden regular rank found conjecture regular rank rank respectively size full parameter space binary variables. rank greater maximum possible dimension jacobian matrix. fact proven lower bound well. bayes models binary hidden root node redundant parameters. therefore best represent probability distribution representable model network representation explicitly. hidden. assuming variables binary space observables representable parameters number parameters network example could compute rank symbolically. instead used following mathematica code. functions similarly written. jacobian matrix computed command outer three arguments. ﬁrst stands diﬀerentiation operator second functions third variables. interpretation result that around almost every value locally represent hidden structure parameters. contrast encode distribution using network parameters structure must parameters. thus network parameters locally redundant. approximation punishes structure according eﬃcient representation uses parameters according representation given structure requires parameters. interesting note dimension structure three four states states. know predict dimension changes result increasing number hidden states without computing dimension explicitly. nonetheless dimension increase beyond average hidden variable structure obtain another network structure parameters. next command produces diagonalized matrix random point precision decimal digits. precision selected matrix elements equal zero would correctly identiﬁed such. result mathematica program diagonalized matrix non-zero rows rows containing zeros. counts obtained runs program. hence regular rank jacobian matrix probability autoclass clustering algorithm developed cheeseman stutz uses naive bayes model. state hidden root node represents cluster class; observable node represents measurable feature. number classes unknown priori. autoclass computes approximation marginal likelihood naive bayes model given data using increasing values probability reaches peak speciﬁc selected number classes. apply techniques developed paper also need specify parameters observable variables. given joint distribution multivariate-normal multivariate-normal distributions closed marginalization need specify vector means observed variables covariance matrix observed variables. addition need specify transform parameters network observable parameters. transformation means transformation obtain observable covariance matrix accomplished trek-sum rule using trek-sum rule easy show observable parameters sums products network parameters. given mapping observable parameters polynomial function follows thm. rank consider gaussian models. mathematica code similar code section compute dimensions perform computation symbolically. previous experiments none randomly chosen values accidentally reduces rank. hidden variable observed. network parameters conditional variances conditional means linear parameters. marginal distribution observed variables also parameters means variances covariances. nonetheless analysis rank jacobian matrix tells dimension model follows fact model imposes tetrad constraints model three tetrad constraints approximation context simple autoclass models used score bayesian network discrete variables well models call approximation scoring function. number classes two. nonetheless always equal example number parameters regular rank jacobian matrix computed rank using mathematica described previous section. consequently example found far; believe incorrect results obtained rare combinations nonetheless simple modiﬁcation scoring function yields approximation asymptotically converge variables continuous. before bayesian network network structure bayesian network parameters associated network structure. gaussian network joint likelihood multivariate gaussian distribution product local likelihoods. local likelihood linear regression model second example structure described section variables continuous. network parameters conditional means conditional variances linear parameters. marginal distribution observed variables parameters whereas analysis rank jacobian matrix tells dimension model coincides intuition many values variance linear network parameters whereas dimension compute reduction expected could encode dependency variables middle level removing variable layer adding variables producing network parameters.", "year": 2013}