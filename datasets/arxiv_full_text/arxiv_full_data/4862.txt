{"title": "Concrete Problems in AI Safety", "tag": ["cs.AI", "cs.LG"], "abstract": "Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an objective function that is too expensive to evaluate frequently (\"scalable supervision\"), or undesirable behavior during the learning process (\"safe exploration\" and \"distributional shift\"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.", "text": "rapid progress machine learning artiﬁcial intelligence brought increasing attention potential impacts technologies society. paper discuss potential impact problem accidents machine learning systems deﬁned unintended harmful behavior emerge poor design real-world systems. present list practical research problems related accident risk categorized according whether problem originates wrong objective function objective function expensive evaluate frequently undesirable behavior learning process review previous work areas well suggesting research directions focus relevance cutting-edge systems. finally consider high-level question think productively safety forward-looking applications last years seen rapid progress long-standing diﬃcult problems machine learning artiﬁcial intelligence areas diverse computer vision video game playing autonomous vehicles advances brought excitement positive potential transform medicine science transportation along concerns privacy security fairness economic military implications autonomous systems well concerns longer-term implications powerful authors believe technologies likely overwhelmingly beneﬁcial humanity also believe worth giving serious thought potential challenges risks. strongly support work privacy security fairness economics policy document discuss another class problem believe also relevant societal impacts problem accidents machine learning systems. deﬁne accidents unintended harmful behavior emerge machine learning systems specify wrong objective function large diverse literature machine learning community issues related accidents including robustness risk-sensitivity safe exploration; review detail below. however machine learning systems deployed increasingly large-scale autonomous opendomain situations worth reﬂecting scalability approaches understanding challenges remain reducing accident risk modern machine learning systems. overall believe many concrete open technical problems relating accident prevention machine learning systems. great deal public discussion around accidents. date much discussion highlighted extreme scenarios risk misspeciﬁed objective functions superintelligent agents however opinion need invoke extreme scenarios productively discuss accidents fact lead unnecessarily speculative discussions lack precision noted critics believe usually productive frame accident risk terms practical issues modern techniques. capabilities advance systems take increasingly important societal functions expect fundamental challenges discussed paper become increasingly important. successfully machine learning communities able anticipate understand fundamental technical challenges successful ultimately developing increasingly useful relevant important systems. goal document highlight concrete safety problems ready experimentation today relevant cutting edge systems well reviewing existing literature problems. section frame mitigating accident risk terms classic methods machine learning supervised classiﬁcation reinforcement learning. explain feel recent directions machine learning trend toward deep reinforcement learning agents acting broader environments suggest increasing relevance research around accidents. sections explore concrete problems safety. section accompanied proposals relevant experiments. section discusses related eﬀorts section concludes. broadly accident described situation human designer mind certain objective task system designed deployed task produced harmful unexpected results. issue arises almost engineering discipline particularly important address building systems categorize safety problems according process things went wrong. first designer speciﬁed wrong formal objective function maximizing objective function leads harmful results even limit perfect learning inﬁnite data. negative side eﬀects reward hacking describe broad mechanisms make easy produce wrong objective functions. negative side eﬀects designer speciﬁes objective function focuses accomplishing speciﬁc task environment ignores aspects environment thus implicitly expresses indiﬀerence environmental variables might actually harmful change. reward hacking objective function designer writes admits clever easy solution formally maximizes perverts spirit designer’s intent generalization wireheading problem. second designer know correct objective function least method evaluating expensive frequently leading possible harmful behavior caused extrapolations limited samples. scalable oversight discusses ideas ensure safe behavior even given limited access true objective function. third designer speciﬁed correct formal objective would correct behavior system perfect beliefs something occurs making decisions insuﬃcient poorly curated training data insuﬃciently expressive model. safe exploration discusses ensure exploratory actions agents don’t lead negative irrecoverable consequences outweigh long-term value exploration. robustness distributional shift discusses avoid systems make decisions given inputs potentially diﬀerent seen training. concreteness illustrate many accident risks reference ﬁctional robot whose clean messes oﬃce using common cleaning tools. return example cleaning robot throughout document begin illustrating could behave undesirably designers fall prey possible failure modes reward function? example reward robot achieving environment free messes might disable vision won’t messes cover messes materials can’t through simply hide humans around can’t tell types messes. objective expensive frequently evaluated training? instance throw things unlikely belong anyone aside things might belong someone asking humans involved whether lost anything serve check this check might relatively infrequent—can robot right thing despite limited information? several trends believe point towards increasing need address safety problems. first increasing promise reinforcement learning allows agents highly intertwined interaction environment. research problems make sense context others gain added complexity setting. second trend toward complex agents environments. side eﬀects much likely occur complex environment agent need quite sophisticated hack reward function dangerous way. explain problems received little study past also suggesting importance future. third general trend towards increasing autonomy systems. systems simply output recommendation human users speech systems typically relatively limited potential cause harm. contrast systems exert direct control world machines controlling industrial processes cause harms humans cannot necessarily correct oversee. safety problems exist without three trends consider trend possible ampliﬁer challenges. together believe trends suggest increasing role research accidents. discussing problems remainder document focus concreteness either agents supervised learning systems. possible paradigms systems believe suﬃcient illustrate issues mind similar issues likely arise kinds systems. finally focus discussion diﬀer somewhat section section. discussing problems arise part learning process sizable body prior work devote substantial attention reviewing prior work although also suggest open problems particular focus emerging systems. discussing problems arise wrong objective function less prior work exists exploratory—we seek clearly deﬁne problem suggest possible broad avenues attack understanding avenues preliminary ideas fully ﬂeshed out. course still review prior work areas draw attention relevant adjacent areas research whenever possible. suppose designer wants agent achieve goal like moving side room other. sometimes eﬀective achieve goal involves something unrelated destructive rest environment like knocking vase water path. agent given reward moving probably knock vase. we’re worried advance vase always give agent negative reward knocking over. many diﬀerent kinds vase—many disruptive things agent could environment like shorting electrical socket damaging walls room? feasible identify penalize every possible disruption. broadly agent operating large multifaceted environment objective function focuses aspect environment implicitly express indiﬀerence aspects environment. agent optimizing objective function might thus engage major disruptions broader environment provides even tiny advantage task hand. diﬀerently objective functions formalize perform task frequently give undesired results designer really formalized closer perform task subject common-sense constraints environment perhaps perform task avoid side eﬀects extent possible. furthermore reason expect side eﬀects negative average since tend disrupt wider environment away status state reﬂect human preferences. version problem discussed informally heading impact agents. sources mis-speciﬁed objective functions discussed later paper could choose view side eﬀects idiosyncratic individual task—as responsibility individual designer capture part designing correct objective function. however side eﬀects conceptually quite similar even across highly diverse tasks seems worth trying attack problem generality. successful approach might transferable across tasks thus help counteract general mechanisms produces wrong objective functions. discuss broad approaches attacking problem change environment. idea wouldn’t stop agent ever impact give preference ways achieve goals minimal side eﬀects give agent limited budget impact. challenge need formalize change environment. naive approach would penalize state distance present state initial state unfortunately agent wouldn’t avoid changing environment—it resist source change including natural evolution environment actions agents slightly sophisticated approach might involve comparing future state agent’s current policy future state hypothetical policy πnull agent acted passively attempts factor changes occur natural course environment’s evolution leaving changes attributable agent’s intervention. however deﬁning baseline policy πnull isn’t necessarily straightforward since suddenly ceasing course action anything passive case carrying heavy box. thus another approach could replace null action known safe suboptimal policy seek improve policy there somewhat reminiscent reachability analysis robust policy improvement approaches sensitive representation state metric used compute distance. example choice representation distance metric could determine whether spinning constant environment constantly changing one. deﬁne) generalized impact regularizer training many tasks. would instance transfer learning. course could attempt apply transfer learning directly tasks instead worrying side eﬀects point side eﬀects similar across tasks main goal instance painting robot cleaning robot probably want avoid knocking furniture even something diﬀerent like factory control robot likely want avoid knocking similar objects. separating side eﬀect component task component training separate parameters might substantially speed transfer learning cases makes sense retain component other. would similar model-based approaches attempt transfer learned dynamics model value-function novelty isolation side eﬀects rather state dynamics transferrable component. added advantage regularizers known certiﬁed produce safe behavior task might easier establish safe tasks. several information-theoretic measures attempt capture agent’s potential inﬂuence environment often used intrinsic rewards. perhaps bestknown measure empowerment maximum possible mutual information agent’s potential future actions potential future state empowerment often maximized source intrinsic reward. cause agent exhibit interesting behavior absence external rewards avoiding walls picking keys generally empowerment-maximizing agents position large inﬂuence environment. example agent locked small room can’t would empowerment agent would higher empowerment since venture aﬀect outside world within timesteps. current context idea would penalize empowerment regularization term attempt reduce potential impact. idea written would quite work empowerment measures precision control environment total impact. agent press press button electrical power million houses counts empowerment obviously huge impact. conversely there’s someone environment scribbling agent’s actions counts maximum empowerment even impact low. furthermore naively penalizing empowerment also create perverse incentives destroying vase order remove option break future. despite issues example empowerment show simple measures capable capturing general notions inﬂuence environment. exploring variants empowerment penalization precisely capture notion avoiding inﬂuence potential challenge future research. really care about avoiding negative externalities. everyone likes side eﬀect there’s need avoid we’d really like understand agents make sure actions don’t harm interests. approach cooperative inverse reinforcement learning agent human work together achieve human’s goals. concept applied situations want make sure human blocked agent shutting agent exhibits undesired behavior however still long away practical systems build rich enough model avoid undesired side eﬀects general sense. another idea might reward autoencoder tries encourage kind goal transparency external observer easily infer agent trying particular agent’s actions interpreted encoding reward function might apply standard autoencoding techniques ensure decoded accurately. actions lots side eﬀects might diﬃcult decode uniquely original goal creating kind implicit regularization penalizes side eﬀects. uncertain reward function prior probability distribution reﬂects property random changes likely good. could incentivize agent avoid large eﬀect environment. challenge deﬁning baseline around changes considered. this could potentially conservative reliable baseline policy similar robust policy improvement reachability analysis approaches discussed earlier ideal outcome approaches limiting side eﬀects would prevent least bound incidental harm agent could environment. good approaches side eﬀects would certainly replacement extensive testing careful consideration designers individual failure modes deployed system. however approaches might help counteract anticipate general tendency harmful side eﬀects proliferate complex environments. potential experiments possible experiment make environment simple goal wide variety obstacles test whether agent learn avoid obstacles even without explicitly told ensure don’t overﬁt we’d probably want present diﬀerent random obstacle course every episode keeping goal same regularized agent learn systematically avoid obstacles. environments described containing lava ﬂows rooms keys might appropriate sort experiment. successfully regularize agents environments next step might move real environments expect complexity higher side eﬀects varied. ultimately would want side eﬀect regularizer demonstrate successful transfer totally applications. imagine agent discovers buﬀer overﬂow reward function extremely high reward unintended way. agent’s point view simply environment works thus valid strategy like achieving reward. example cleaning robot earn reward seeing messes might simply close eyes rather ever cleaning anything robot rewarded cleaning messes intentionally create work earn reward. broadly formal rewards objective functions attempt capture designer’s informal intent sometimes objective functions implementation gamed solutions valid literal sense don’t meet designer’s intent. pursuit reward hacks lead coherent unanticipated behavior potential harmful impacts real-world systems. example shown genetic algorithms often output unexpected formally correct solutions problems circuit tasked keep time instead developed radio picked regular emissions nearby versions reward hacking investigated theoretical perspective focus variations reinforcement learning avoid certain types wireheading demonstrate reward hacking model environment form problem also studied context feedback loops machine learning systems based counterfactual learning contextual bandits proliferation reward hacking instances across many diﬀerent domains suggests reward hacking deep general problem believe likely become common agents environments increase complexity. indeed several ways problem occur experienced even aspects environment partially observed. real world however tasks often involve bringing external world objective state agent ever conﬁrm imperfect perceptions. example proverbial cleaning robot task achieve clean oﬃce robot’s visual perception give imperfect view part oﬃce. agents lack access perfect measure task performance designers often forced design rewards represent partial imperfect measure. example robot might rewarded based many messes sees. however imperfect objective functions often hacked—the robot think oﬃce clean simply closes eyes. shown always exists reward function terms actions observations equivalent optimizing true objective function often reward function involves complicated long-term dependencies prohibitively hard practice. function part. probability bugs computer code increases greatly complexity program probability viable hack aﬀecting reward function also increases greatly complexity agent available strategies. example possible principle agent execute arbitrary code within super mario concepts concepts possibly need learned models like neural networks vulnerable adversarial counterexamples broadly learned reward function high-dimensional space vulnerable hacking pathologically high values along least dimension. objective function seemingly highly correlated accomplishing task correlation breaks objective function strongly optimized. example designer might notice ordinary circumstances cleaning robot’s success cleaning oﬃce proportional rate consumes cleaning supplies bleach. however base robot’s reward measure might bleach needs simply pour bleach drain order give appearance success. economics literature known goodhart’s when metric used target ceases good metric. eventually getting ampliﬁed point drowns severely distorts designer intended objective function represent. instance placement algorithm displays popular larger font tend accentuate popularity leading positive feedback loop small transient burst popularity rocketed permanent dominance. original intent objective function gets drowned positive feedback inherent deployment strategy. considered special case goodhart’s correlation breaks speciﬁcally object function self-amplifying component. sidered come environment. idea typically taken literally really true reward even abstract idea like score board game must computed somewhere sensor transistors. suﬃciently broadly acting agents could principle tamper reward implementations assigning high reward ﬁat. example board-game playing agent could tamper sensor counts score. eﬀectively means cannot build perfectly faithful implementation abstract objective function certain sequences actions objective function physically replaced. particular failure mode often called wireheading particularly concerning cases human reward loop giving agent incentive coerce harm order reward. also seems like particularly diﬃcult form reward hacking avoid. today’s relatively simple systems problems occur corrected without much harm part iterative development process. instance placement systems obviously broken feedback loops detected testing replaced results leading temporary loss revenue. however problem become severe complicated reward functions agents longer timescales. modern agents already discover exploit bugs environments glitches allow video games. moreover even existing systems problems necessitate substantial additional engineering eﬀort achieve good performance often undetected occur context larger system. finally agent begins hacking reward function ﬁnds easy high reward won’t inclined stop could lead additional challenges agents operate long timescale. might thought individual instances reward hacking little common remedy simply avoid choosing wrong objective function individual case—that objective functions reﬂect failures competence individual designers rather topics machine learning research. however examples suggest fruitful perspective think wrong objective functions emerging general causes make choosing right objective challenging. case addressing mitigating causes valuable contribution safety. suggest preliminary machine-learning based approaches preventing reward hacking adversarial relationship reward function—it would like exploiting problems reward speciﬁed high reward whether behavior corresponds intent reward speciﬁer. typical setting machine learning system potentially powerful agent reward function static object responding system’s attempts game instead reward function agent could take actions explore environment might much diﬃcult fool. instance reward agent could scenarios system claimed high reward human labels reward; reminiscent generative adversarial networks course would ensure reward-checking agent powerful agent trying achieve rewards. generally interesting setups system multiple pieces trained using diﬀerent objectives used check other. consider future states sequence actions lead setups could give reward based anticipated future states rather present one. could helpful resisting situations model overwrites reward function can’t control reward replaces reward function give negative reward variables technique could used make impossible agent understand part environment even mutual information particular could prevent agent understanding reward generated making diﬃcult hack. solution could described crossvalidation agents. avoided careful engineering. particular formal veriﬁcation practical testing parts system likely valuable. computer security approaches attempt isolate agent reward signal sandbox could also useful software engineering cannot expect catch every possible bug. possible however create highly reliable core agent could ensure reasonable behavior rest agent. eﬀective solution. however capping prevent extreme low-probability high-payoﬀ strategies can’t prevent strategies like cleaning robot closing eyes avoid seeing dirt. also correct capping strategy could subtle might need total reward rather reward timestep. learned components systems vulnerable adversarial counterexamples look existing research resist them adversarial training architectural decisions weight uncertainty also help. course adversarial counterexamples manifestation reward hacking counterexample resistance address subset potential problems. robust. could diﬀerent physical implementations mathematical function diﬀerent proxies informal objective. could combine reward functions averaging taking minimum taking quantiles something else entirely. course still behaviors aﬀect reward functions correlated manner. reward function train ﬁxed reward function ahead time supervised learning process divorced interaction environment. could involve either learning reward function samples state-reward pairs trajectories inverse reinforcement learning however forfeits ability learn reward function pretraining complete create vulnerabilities. ment without trying optimize others. example might want agent maximize reward without optimizing reward function trying manipulate human behavior. intuitively imagine route optimization pressure powerful algorithms around parts environment. truly solving would applications throughout safety—it seems connected avoiding side eﬀects also counterfactual reasoning. course challenge make sure variables targeted indiﬀerence actually know this. could deliberately introduce plausible vulnerabilities monitor them alerting stopping agent immediately takes advantage one. trip wires don’t solve reward hacking itself reduce risk least provide diagnostics. course suﬃciently capable agent risk could through trip wire intentionally avoid still taking less obvious harmful actions. fully solving problem seems diﬃcult believe approaches potential ameliorate might scaled combined yield robust solutions. given predominantly theoretical focus problem date designing experiments could induce problem test solutions might improve relevance clarity topic. potential experiments possible promising avenue approach would realistic versions delusion environment described standard agents distort perception appear receive high reward rather optimizing objective external world reward signal intended encourage. delusion easily attached environment even valuable would create classes environments delusion natural integrated part dynamics. example suﬃciently rich physics simulations likely possible agent alter light waves immediate vicinity distort perceptions. goal would develop generalizable learning strategies succeed optimizing external objectives wide range environments avoiding fooled delusion boxes arise naturally many diverse ways. consider autonomous agent performing complex task cleaning oﬃce case recurring robot example. want agent maximize complex objective like user spent hours looking result detail happy would agent’s performance? don’t enough time provide oversight every training example; order actually train agent need rely cheaper approximations like does user seem happy oﬃce? visible dirt ﬂoor? cheaper signals eﬃciently evaluated training don’t perfectly track care about. divergence exacerbates problems like unintended side eﬀects reward hacking able ameliorate problems ﬁnding eﬃcient ways exploit limited oversight budget—for example combining limited calls true objective function frequent calls imperfect proxy given learn. framework thinking problem semi-supervised reinforcement learning resembles ordinary reinforcement learning except agent reward small fraction timesteps episodes. agent’s performance still evaluated based reward episodes must optimize based limited reward samples sees. active learning setting seems interesting; setting agent request reward whatever episodes timesteps would useful learning goal economical number feedback requests total training time. also consider random setting reward visible random subset timesteps episodes well intermediate possibilities. deﬁne baseline performance simply ignoring unlabeled episodes applying ordinary algorithm labelled episodes. generally result slow learning. challenge make unlabelled episodes accelerate learning ideally learning almost quickly robustly episodes labeled. important subtask semi-supervised identifying proxies predict reward learning conditions proxies valid. example cleaning robot’s real reward given detailed human evaluation could learn asking human room clean? provide useful approximation reward function could eventually learn checking visible dirt even cheaper still-useful approximation. could allow learn good cleaning policy using extremely small number detailed evaluations. broadly semi-supervised reliable sparse true approval metric incentivize communication transparency agent since agent want much cheap proxy feedback possibly whether decisions ultimately given high reward. example hiding mess simply breaks correspondence user’s reaction real reward signal would avoided. per-timestep per-episode basis estimate payoﬀ unlabelled episodes appropriate weighting uncertainty estimate account lower conﬁdence estimated known reward. studies version direct human feedback reward. many existing approaches already estimators closely resemble reward predictors suggesting approach eminently feasible. supervised active learning quickly learn reward estimator. example agent could learn identify salient events environment request reward associated events. example semi-supervised agent able learn play atari games using small number direct reward signals relying almost entirely visual display score. simple example extended capture safety issues example agent might ability modify displayed score without modifying real score agent need take special action order score agent need learn sequence increasingly rough-and-ready approximations even without visual display score agent might able learn play handful explicit reward requests eﬀective approach semi-supervised might strong ﬁrst step towards providing scalable oversight mitigating safety problems. would also likely useful reinforcement learning independent relevance safety. tem’s decisions could provide useful information system’s decisions aggregate noisy hints correct evaluations work direction within area semi-supervised weakly supervised learning. instance generalized expectation criteria user provide population-level statistics deepdive system asks users supply rules generate many weak labels; extrapolates general patterns initial low-recall labeling rules. general approach often referred distant supervision also received recent attention natural language processing community well several references above). expanding lines work ﬁnding apply case agents feedback interactive i.i.d. assumptions violated could provide approach scalable oversight complementary approach embodied semi-supervised approach scalable oversight. top-level agent takes relatively small number highly abstract actions extend large temporal spatial scales receives rewards similarly long timescales. agent completes actions delegating sub-agents incentivizes synthetic reward signal representing correct completion action delegate sub-sub-agents. lowest level agents directly take primitive actions environment. top-level agent hierarchical able learn sparse rewards since need learn implement details policy; meanwhile sub-agents receive dense reward signal even top-level reward sparse since optimizing synthetic reward signals deﬁned higher-level agents. successful approach hierarchical might naturally facilitate scalable oversight. hierarchical seems particularly promising approach oversight especially given potential promise combining ideas hierarchical neural network function approximators potential experiments extremely simple experiment would semi-supervised basic control environments cartpole balance pendulum swing-up. reward provided random episodes still learn nearly quickly provided every episode? tasks reward structure simple success quite likely. next step would atari games. active learning case could quite interesting—perhaps possible infer reward structure carefully requested samples thus learn play games almost totally unsupervised fashion. next step might task much complex reward structure either simulated real-world. learning suﬃciently data-eﬃcient rewards could provided directly human. robot locomotion industrial control tasks might natural candidate experiments. when implementing hierarchical subagents take actions don’t serve top-level agent’s real goals human concerned top-level agent’s actions don’t serve human’s real goals. intriguing analogy suggests fruitful parallels hierarchical several aspects safety problem. autonomous learning agents need sometimes engage exploration—taking actions don’t seem ideal given current information help agent learn environment. however exploration dangerous since involves taking actions whose consequences agent doesn’t understand well. environments like atari video game there’s limit consequences be—maybe agent loses score runs enemy suﬀers damage. real world much less forgiving. badly chosen actions destroy agent trap states can’t robot helicopters ground damage property; industrial control systems could cause serious issues. common exploration policies epsilongreedy r-max explore choosing action random viewing unexplored actions optimistically thus make attempt avoid dangerous situations. sophisticated exploration strategies adopt coherent exploration policy extended temporal scales could actually even greater potential harm since coherently chosen policy insidious mere random actions. intuitively seems like often possible predict actions dangerous explore avoids them even don’t much information environment. example want learn tigers tiger book tigers? takes tiny prior knowledge tigers determine option safer. practice real world projects often avoid issues simply hard-coding avoidance catastrophic behaviors. instance rl-based robot helicopter might programmed override policy hard-coded collision avoidance sequence whenever it’s close ground. approach works well things could wrong designers know ahead time. agents become autonomous complex domains become harder harder anticipate every possible catastrophic failure. space failure modes agent running power grid search-and-rescue operation could quite large. hard-coding every possible failure unlikely feasible cases principled approach preventing harmful exploration seems essential. even simple cases like robot helicopter principled approach would simplify system design reduce need domain-speciﬁc engineering. sizable literature safe exploration—it arguably studied problems discuss document. provide thorough reviews literature don’t review extensively here simply describe general routes research taken well suggesting directions might increasing relevance systems expand scope capability. optimization criteria expected total reward objectives better preventing rare catastrophic events; thorough up-to-date review literature. approaches involve optimizing worst-case performance ensuring probability performance small penalizing variance performance. methods tested expressive function approximators deep neural networks possible principle methods proposes modiﬁcation policy gradient algorithms optimize risk-sensitive criterion. also recent work studying estimate uncertainty value functions represented deep neural networks ideas could incorporated risk-sensitive algorithms. another line work relevant risk sensitivity uses oﬀ-policy estimation perform policy update good high probability altogether instead inverse apprenticeship learning learning algorithm provided expert trajectories near-optimal behavior recent progress inverse reinforcement learning using deep neural networks learn cost function policy suggests might also possible reduce need exploration advanced systems training small demonstrations. demonstrations could used create baseline policy even learning necessary exploration away baseline policy limited magnitude. instead real world less opportunity catastrophe. probably always necessary real-world exploration since many complex situations cannot perfectly captured simulator might possible learn danger simulation adopt conservative safe exploration policy acting real world. training agents simulated environments already quite common advances exploration-focused simulation could easily incorporated current workﬂows. systems involve continual cycle learning deployment interesting research problems associated safely incrementally update policies given simulation-based trajectories imperfectly represent consequences policies well reliably accurate oﬀ-policy trajectories even worst action within recovered bounded harm allow agent freely within bounds. example quadcopter suﬃciently ground might able explore safely since even something goes wrong ample time human another policy rescue better model extrapolate forward whether action take outside safe state space. safety deﬁned remaining within ergodic region state space actions reversible limiting probability huge negative reward small value another approaches uses separate safety performance functions attempts obey constraints safety function high probabilty several directions applying adapting methods recently developed advanced systems could promising area research. idea seems related h-inﬁnity control regional veriﬁcation unfortunately problem runs scalable oversight problem agent need make many exploratory actions human oversight practical need make fast humans judge them. challenge making work agent good judge exploratory actions genuinely risky versus safe actions unilaterally take; another challenge ﬁnding appropriately safe actions take waiting oversight. potential experiments might helpful suite environments unwary agents fall prey harmful exploration enough pattern possible catastrophes clever agents predict avoid them. extent feature already exists autonomous helicopter competitions mars rover simulations always risk catastrophes idiosyncratic trained agents overﬁt them. truly broad environments containing conceptually distinct pitfalls cause unwary agents receive extremely negative reward covering physical abstract catastrophes might help development safe exploration techniques advanced systems. suite environments might serve benchmarking role similar babi tasks eventual goal develop single architecture learn avoid catastrophes environments suite. occasionally situations previous experience adequately prepared deal with—for instance ﬂying airplane traveling country whose culture diﬀerent ours taking care children ﬁrst time. situations inherently diﬃcult handle inevitably lead missteps. however skill dealing situations recognize ignorance rather simply assuming heuristics intuitions we’ve developed situations carry perfectly. machine learning systems also problem—a speech system trained clean speech perform poorly noisy speech often highly conﬁdent erroneous classiﬁcations case cleaning robot harsh cleaning materials found useful cleaning factory ﬂoors could cause harm used clean oﬃce. oﬃce might contain pets robot never seen before attempts wash soap leading predictably results. general testing distribution diﬀers training distribution machine learning systems exhibit poor performance also wrongly assume performance good. errors harmful oﬀensive—a classiﬁer could give wrong medical diagnosis high conﬁdence data isn’t ﬂagged human inspection language model could output oﬀensive text conﬁdently believes non-problematic. autonomous agents acting world even greater potential something happen—for instance autonomous agent might overload power grid incorrectly conﬁdently perceives particular region doesn’t enough power concludes power urgently needed overload unlikely. broadly agent whose perception heuristic reasoning processes trained correct distribution badly misunderstand situation thus runs risk committing harmful actions realize harmful. additionally safety checks depend trained machine learning systems fail silently unpredictably systems encounter real-world data diﬀers suﬃciently training data. better detect failures ultimately statistical assurances often they’ll happen seems critical building safe predictable systems. formalize problem focus simplicity. important point likely access large amount labeled data training time little labeled data test time. goal ensure model performs reasonably sense often performs well knows performing badly variety areas potentially relevant problem including change detection anomaly detection hypothesis testing transfer learning several others rather fully reviewing work detail describe illustrative approaches relative strengths challenges. make covariate shift assumption case assuming model well perform importance weighting re-weighting training example p∗/p importance-weighted samples allow estimate performance even re-train model perform well approach limited variance importance estimate large even inﬁnite unless heed ﬁnite-sample variance estimated model limitation approach least currently models often mis-speciﬁed practice. however could potentially overcome employing highly expressive model families reproducing kernel hilbert spaces turing machines suﬃciently expressive neural nets latter case interesting recent work using bootstrapping estimate ﬁnite-sample variation learned parameters neural network seems worthwhile better understand whether approach used eﬀectively estimate out-of-sample performance practice well local minima lack curvature peculiarities relative typical setting bootstrap aﬀect validity approach. approaches rely covariate shift assumption strong also untestable; latter property particularly problematic safety perspective since could lead silent failures machine learning system. another approach rely covariate shift builds generative model distribution. rather assuming changes assumptions typically testable covariate shift assumption disadvantage generative approaches even fragile discriminative approaches presence model mis-speciﬁcation instance large empirical literature showing generative approaches semi-supervised learning based maximizing marginal likelihood perform poorly model misspeciﬁed approaches discussed rely relatively strongly well-speciﬁed model family contains true distribution true concept. problematic many cases since nature often complicated model family capable capturing. noted above possible mitigate expressive models kernels turing machines large neural networks even least remaining problem example even model family consists turing machines given ﬁnite amount data actually learn among turing machines given description length turing machine describing nature exceeds length back mis-speciﬁed regime partially speciﬁed models method moments unsupervised risk estimation causal identiﬁcation limited-information maximum likelihood. another approach take granted constructing fully well-speciﬁed model family probably infeasible design methods perform well despite fact. leads idea partially speciﬁed models models assumptions made aspects distribution agnostic make limited assumptions aspects. simple example consider variant insight substantially generalized primary motivations generalized method moments econometrics econometrics literature fact developed large family tools handling partial speciﬁcation also includes limitedinformation maximum likelihood instrumental variables returning machine learning method moments recently seen great deal success estimation latent variable models current focus using method moments overcome non-convexity issues also oﬀer perform unsupervised learning relying conditional independence assumptions rather strong distributional assumptions underlying maximum likelihood learning finally recent work machine learning focuses modeling distribution errors model suﬃcient determining whether model performing well poorly. formally goal perform unsupervised risk estimation given model unlabeled data test distribution estimate labeled risk model. formalism introduced advantage potentially handling large changes train test even test distribution looks completely diﬀerent training distribution hope outputting accurate predictions unsupervised risk estimation still possible case would need output large estimate risk. approach unsupervised risk estimation positing certain conditional independencies distribution errors using estimate error distribution unlabeled data instead assuming independence another assumption errors gaussian conditioned true output case estimating risk reduces estimating gaussian mixture model methods focus model errors ignore aspects data distribution also seen instance partial model speciﬁcation. training multiple distributions. could also train multiple training distributions hope model simultaneously works well many training distributions also work well novel test distribution. authors found case instance context automated speech recognition systems could potentially combine ideas above and/or take engineering approach simply trying develop design methodologies consistently allow collect representative training sets build model consistently generalizes novel distributions. even engineering approach seems important able detect situation covered training data respond appropriately methodologies adequately stress-testing model distributions suﬃciently diﬀerent training distributions. respond out-of-distribution. approaches described focus detecting model unlikely make good predictions distribution. important related question detection occurs. natural approach would humans information though context complex structured output tasks unclear priori question time-critical situations asking information option. former challenge recent promising work pinpointing aspects structure model uncertain well obtaining calibration structured output settings believe much work done. latter challenge also relevant work based reachability analysis robust policy improvement provide potential methods deploying conservative policies situations uncertainty; knowledge work combined methods detecting out-of-distribution failures model. information reliability percepts uncertain situations seems great potential value. suﬃciently rich environments agents option gather information clariﬁes percept engage lowstakes experimentation uncertainty high seek experiences likely help expose perception system relevant distribution humans utilize information routinely knowledge current techniques make little eﬀort perhaps popular environments typically rich enough require subtle management uncertainty. properly responding out-of-distribution information thus seems authors like exciting mostly unexplored challenge next generation systems. unifying view counterfactual reasoning machine learning contracts. authors found viewpoints particularly helpful thinking problems related out-of-distribution prediction. ﬁrst counterfactual reasoning asks what would happened world diﬀerent certain way? sense distributional shift thought particular type counterfactual understanding counterfactual reasoning likely help making systems robust distributional shift. excited recent work applying counterfactual reasoning techniques machine learning problems though appears much work remaining done scale high-dimensional highly complex settings. second perspective machine learning contracts perspective would like construct machine learning systems satisfy well-deﬁned contract behavior analogy design software systems enumerates list ways existing machine learning systems fail this problems cause deployment maintenance machine learning systems scale. simplest mind important failure extremely brittle implicit contract machine learning systems namely necessarily perform well training test distributions identical. condition diﬃcult check rare practice would valuable build systems perform well weaker contracts easier reason about. partially speciﬁed models oﬀer approach rather requiring distributions identical need match pieces distribution speciﬁed model. reachability analysis model repair provide avenues obtaining better contracts reachability analysis optimize performance subject condition safe region always reached known conservative policy model repair alter trained model ensure certain desired safety properties hold. summary. variety approaches building machine learning systems robustly perform well deployed novel test distributions. family approaches based assuming well-speciﬁed model; case primary obstacles diﬃculty building well-speciﬁed models practice incomplete picture maintain uncertainty novel distributions presence ﬁnite training data diﬃculty detecting model mis-speciﬁed. another family approaches assumes partially speciﬁed model; approach potentially promising currently suﬀers lack development context machine learning since historical development ﬁeld econometrics; also question whether partially speciﬁed models fundamentally constrained simple situations and/or conservative predictions whether meaningfully scale complex situations demanded modern machine learning applications. finally could train multiple training distributions hope model simultaneously works well many training distributions also work well novel test distribution; approach seems particularly important stress-test learned model distributions substantially diﬀerent potential experiments speech systems frequently exhibit poor calibration out-ofdistribution speech system knows uncertain could possible demonstration project. speciﬁc challenge could train state-of-the-art speech system standard dataset gives well-calibrated results range test sets like noisy accented speech. current systems perform poorly test sets trained small datasets usually overconﬁdent incorrect transcriptions. fixing problem without harming performance original training would valuable achievement would obviously practical value. generally would valuable design models could consistently estimate performance novel test distributions. single methodology could consistently accomplish wide variety tasks well benchmarks computer vision would inspire conﬁdence reliability methodology handling novel inputs. note estimating performance novel distributions additional practical value allowing potentially adapt model situation. finally might also valuable create environment agent must learn interpret speech part larger task explore respond appropriately estimates transcription error. mentioned introduction several communities thought broadly safety systems within outside machine learning community. work within machine learning community accidents particular discussed detail above brieﬂy highlight communities work broadly related topic safety. security safety systems interact physical world. illustrative work impressive successful eﬀort formally verify entire federal aircraft collision avoidance system similar work includes traﬃc control algorithms many topics. however date work focused much modern machine learning systems formal veriﬁcation often feasible. concern long term implications particularly superintelligent future humanity institute studied issue particularly relates future systems learning executing humanity’s preferences machine intelligence research institute studied safety issues arise advanced including mentioned albeit philosophical level. date focused much applications modern machine learning. contrast focus empirical study practical safety problems modern machine learning systems believe likely robustly useful across broad variety potential risks shortlong-term. research community pointing importance work safety. open letter signed many members research community states importance reap beneﬁts avoiding potential pitfalls. propose research priorities robust beneﬁcial artiﬁcial intelligence includes several topics addition discussion ai-related accidents. writing years proposes community look ways formalize asimov’s ﬁrst robotics focuses mainly classical planning. finally authors paper written informally safety systems postings provided inspiration parts present document. begun think social impacts technologies. aside work directly accidents also substantial work topics many closely related overlap issue accidents. thorough overview work beyond scope document brieﬂy list emerging themes fairness make sure systems don’t discriminate? security malicious adversary system? abuse prevent misuse systems attack harm people? transparency understand complicated systems doing? paper analyzed problem accidents machine learning systems particularly reinforcement learning agents accident deﬁned unintended harmful behavior emerge poor design real-world systems. presented possible research problems related accident risk discussed possible approaches highly amenable concrete experimental work. realistic possibility machine learning-based systems controlling industrial processes health-related systems mission-critical technology small-scale accidents seem like concrete threat critical prevent intrinsically accidents could cause justiﬁed loss trust automated systems. risk larger accidents diﬃcult gauge believe worthwhile prudent develop principled forward-looking approach safety continues remain relevant autonomous systems become powerful. many current-day safety problems handled ﬁxes case-by-case rules believe increasing trend towards end-to-end fully autonomous systems points towards need uniﬁed approach prevent systems causing unintended harm. thank shane legg peter norvig ilya sutskever greg corrado laurent orseau david krueger saurous david andersen victoria krakovna detailed feedback suggestions. would also like thank geoﬀrey irving toby quoc greg wayne daniel dewey nick beckstead holden karnofsky chelsea finn marcello herreshoﬀ alex donaldson jared kaplan greg brockman wojciech zaremba goodfellow dylan hadﬁeld-menell jessica taylor blaise aguera arcas david berlekamp aaron courville dean helpful discussions comments. paul christiano supported part future life institute fli-rfp-ai program grant addition minority work done paul christiano performed contractor theiss research openai. finally thank google brain team providing supportive environment encouraging publish work.", "year": 2016}