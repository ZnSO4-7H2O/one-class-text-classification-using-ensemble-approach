{"title": "Adversarial Examples: Attacks and Defenses for Deep Learning", "tag": ["cs.LG", "cs.CR", "cs.CV", "stat.ML"], "abstract": "With rapid progress and great successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called \\textit{adversarial examples}. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical scenarios. Therefore, the attacks and defenses on adversarial examples draw great attention.  In this paper, we review recent findings on adversarial examples against deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications and countermeasures for adversarial examples are investigated. We further elaborate on adversarial examples and explore the challenges and the potential solutions.", "text": "despite great successes numerous applications many deep learning empowered applications life crucial raising great concerns ﬁeld safety security. with great power comes great responsibility recent studies deep learning vulnerable well-designed input samples. samples easily fool well-performed deep learning model little perturbations imperceptible humans. szegedy ﬁrst generated small perturbations images image classiﬁcation problem fooled state-ofthe-art deep neural networks high probability misclassiﬁed samples named adversarial examples. large amount deep learning based applications used planned deployed physical world especially safety-critical environments. recent studies show adversarial examples applied real world. thus adversarial examples require addressed carefully. instance adversary construct physical adversarial examples confuse autonomous vehicles manipulating stop sign trafﬁc sign recognition system removing segmentation pedestrians object detection system adversarial commands generated attackers automatic speech recognition models voice controllable system apple siri amazon alexa microsoft cortana deep learning widely regarded black know performs well limited knowledge reason many studies proposed explain interpret deep neural networks inspecting adversarial examples gain insights semantic inner levels neural networks problematic decision boundaries turn helps increase robustness performance neural networks improve interpretability paper investigate summarize approaches generating adversarial examples applications adversarial examples corresponding countermeasures. explore characteristics possible causes adversarial examples. recent advances deep learning revolve around supervised learning especially ﬁeld computer vision task. therefore adversarial examples generated computer vision models. mainly discuss adversarial examples image classiﬁcation/object recognition abstract—with rapid progress great successes wide spectrum applications deep learning applied many safety-critical environments. however deep neural networks recently found vulnerable well-designed input samples called adversarial examples. adversarial examples imperceptible human easily fool deep neural networks testing/deploying stage. vulnerability adversarial examples becomes major risks applying deep neural networks safety-critical scenarios. therefore attacks defenses adversarial examples draw great attention. paper review recent ﬁndings adversarial examples deep neural networks summarize methods generating adversarial examples propose taxonomy methods. taxonomy applications adversarial examples investigated. elaborate countermeasures adversarial examples explore challenges potential solutions. deep neural networks made signiﬁcant progresses wide domain machine learning image classiﬁcation object detection speech recognition language translation voice synthesis online master beat players world. recently alphago zero surpassed previous version without using human knowledge general version alphazero achieved superhuman level within hours cross domains chess shogi deep learning requires less hand engineered features expert knowledge. driven emergence data hardware acceleration intricacy data extracted higher abstract level representation input features constantly increasing number real-world applications systems powered deep learning. companies auto industry testing self-driving cars require plenty deep learning techniques object detection reinforcement learning multimodal deep learning. face recognition system deployed atms method biometric authentication china apple provides face authentication unlock mobile phones behavior-based adversaries attack testing/deploying stage. tamper input data testing stage victim deep learning model trained. neither trained model training dataset modiﬁed. adversaries knowledge trained model allowed modify model common assumption many online machine learning services. attacking training stage another interesting topic studied limitation space include topic paper. since great performance achieved deep learning study attacks deep neural networkbased models. adversarial examples conventional machine learning discussed previous studies adversarial examples deep neural networks proved effective conventional machine learning models adversaries target integrity. integrity presented performance metrics essential deep learning model. although security issues related conﬁdentiality privacy drawn attention deep learning focus attacks degrade performance deep learning models attacks cause increasing number false positives false negatives. notations symbols used paper listed description original input data label class classiﬁcation problem. number classes adversarial example label adversarial class targeted adversarial examples deep learning model parameters deep learning model loss function difference original modiﬁed input data norm gradient hessian second-order derivatives kullback-leibler divergence function instance adversarial examples image classiﬁcation task using trained image classiﬁer published third party user inputs image prediction class label. adversarial images original clean images small perturbations often barely recognizable humans. however perturbations misguide image classiﬁer. user response incorrect image label. given trained deep learning model original input data sample generating adversarial example generally described box-constrained optimization problem denote output label denotes distance data sample. perturbation added optimization problem minimizes perturbation misclassifying prediction constraint input data. many variants optimization problem proposed different scenarios assumptions instance adversaries consider perturbation small enough unnoticeable human. therefore perturbation viewed constraint. optimization objective function becomes distance targeted prediction score original prediction score. variant problem described paper presents following contributions systematically analyze approaches generating adversarial examples taxonomize attack approaches along different axes provide accessible intuitive overview approaches. investigate recent approaches variants generating adversarial examples compare using proposed taxonomy. show examples selected applications ﬁeld reinforcement learning generative modeling face recognition object detection semantic segmentation natural language processing malware detection. countermeasures adversarial examples also discussed. outline main challenges potential future research directions adversarial examples around three main problems transferability adversarial examples existence adversarial examples robustness evaluation deep neural networks. remaining paper organized follows. section introduces background deep learning techniques models datasets. discuss adversarial examples raised conventional machine learning section propose taxonomy approaches generating adversarial examples section elaborate approaches section section discuss applications adversarial examples. corresponding countermeasures investigated section discuss current challenges potential solutions section vii. section viii concludes work. section brieﬂy introduce deep learning techniques which extent used generating adversarial examples. next review adversarial examples conventional compared difference adversarial examples conventional subsection discusses main concepts techniques popular architectures common datasets deep learning. wide breakthrough successes ﬁndings become major targets attacks adversaries usually applied evaluate attack methods. main concepts deep learning deep learning type machine learning makes computers learn experience knowledge without explicit programming extract useful patterns data. conventional machine learning algorithms however difﬁcult extract well-represented features limitations curse dimensionality computational bottleneck requirement domain expert knowledge. deep neural network machine learning algorithm many layers networks. deep learning solves problem representation building multiple simple features represent sophisticated concept. example deep learning-based image classiﬁcation system represents object describing edges fabrics structures hidden layers. increasing number available training data deep learning becomes powerful. deep learning models solve complicated problems complex large models help hardware acceleration computational time. neural network layer composed perceptrons perceptron maps inputs output values simple activation function. function neural network formed chain convolutional neural networks recurrent neural networks widely used neural networks recent neural network architectures. cnns deploy convolution operations hidden layers weight sharing parameter reduction. cnns extract local information grid-like input data. cnns shown incredible successes computer vision tasks image classiﬁcation object detection semantic segmentation rnns neural networks processing sequential input data variable length. rnns produce output time step. hidden neuron time step calculated based input data hidden neurons previous time step. avoid vanishing/exploding gradients rnns longterm dependency long short-term memory gated recurrent unit controllable gates widely used practical applications. generative adversarial network type generative model introduced goodfellow inspired szegedy al.’s work adversarial examples goodfellow found adversarial examples used improve representation deep learning conduct unsupervised learning generated samples using generative network used discriminative network adversary determine generated samples real fake. kind network architectures referred generative adversarial network generates data unsupervised way. used augment data samples provide image superresolution image-to-image translation discriminator networks used supervised learning well. autoencoder another type generative model composed parts encoder fenc decoder fdec. encoder maps input latent representation decoder maps back high-dimensional output encoder decoder deep neural networks. autoencoders trained minimize difference inputs outputs. used model compression/decompression unsupervised learning etc. techniques used deep learning besides cnns rnns many techniques discovered deep learning. recent deep learning models built upon techniques. adversarial examples also leverage techniques. brieﬂy discuss following techniques. dropout simple regularization approach deep neural networks randomly removing hidden neurons training stage. part networks learn training dataset epoch. seen ensemble sub-networks. dropout shown avoid overﬁtting make deep learning models robust generalized. activation functions usually used provide nonlinearity deep neural networks. rectiﬁed linear unit function commonly used activation function outputs zero unit negative max. leaky rectiﬁed linear function parametric rectiﬁed linear unit allow modiﬁed output batch normalization applied normalize hidden units applying activation function among batch samples during training. similarly normalization preprocessing batch normalization rescales hidden units shifts average value. afﬁne transformation recommended batch normalization batch normalization speeds training process many applications especially neural networks deep layers. techniques used conventional machine learning algorithms also effective deep learning. several regularization methods applied training avoid overﬁtting adding regularization term limits weights neurons. early stopping technique applied training stage loss increases improves performance outside training dataset. optimal parameters neurons layer many optimization approaches applied deep learning stochastic gradient descent without momentum adagrad rmsprop adam architectures deep neural networks several deep learning architectures widely used computer vision tasks. lenet alexnet googlenet resnet widely used networks simplest network deepest complex one. alexnet ﬁrst showed deep learning models largely outperformed conventional machine learning algorithms based imagenet challenge future study deep learning. architectures made tremendous breakthroughs imagenet challenge seen milestones image classiﬁcation problem. popular architectures referred baselines attackers usually generate adversarial examples them. general deep learning datasets mnist cifar imagenet three widely used datasets computer vision tasks. mnist dataset handwritten digits recognition cifar dataset imagenet dataset image recognition task cifar consists tiny color images classes. imagenet dataset consists images classes large number images imagenet dataset adversarial approaches evaluated part imagenet dataset. street view house numbers dataset similar mnist dataset consists digits obtained real-world house numbers google street view images. youtubedataset referred dataset gained youtube consisting million images used adversarial examples conventional machine learning models discussed since decades ago. machine learning-based systems handcrafted features main targets spam ﬁlters intrusion detection biometric dalvi ﬁrst discussed adversarial examples formulated problem game adversary classiﬁer sensitive cost attack defense adversarial examples became iterative game. biggio ﬁrst tried gradient-based approach generate adversarial examples linear classiﬁer support vector machine neural network compared deep learning adversarial examples methods allow greater freedom modify data. mnist dataset ﬁrst evaluated attack although human could easily distinguish adversarial digit images. biggio also reviewed several proactive defenses discussed reactive approaches improve security machine learning models barreno presented initial investigation security problems machine learning categorized attacking machine learning system three axes inﬂuence whether attacks poison training data; security violation adversarial examples belong false positive false negative; speciﬁcity attack targeted particular instance wide class. discuss axes deep learning area section iii. barreno compared attacks spambayes spam ﬁlter well defenses study case. however mainly focused binary classiﬁcation problem virus detection system intrusion detection system intrusion prevention system conventional machine learning adversarial examples require knowledge feature selection deep learning usually needs data input. conventional attacking defending methods paid great attention features giving less attention impact humans. target becomes fully automatic machine learning system. inspired papers conventional review recent security issues deep learning area paper. provided comprehensive overview security issues machine learning recent ﬁndings deep learning. established unifying threat model. free lunch theorem introduced tradeoff accuracy robustness. covered wide range security problems. compared work paper focuses adversarial examples detailed discussion recent studies ﬁndings. false positive attacks generate negative sample misclassiﬁed positive malware detection task benign software classiﬁed malware false positive. image classiﬁcation task false positive adversarial image unrecognizable human deep neural networks predict class high conﬁdence. false positive example image classiﬁcation depicted figure false negative attacks generate positive sample misclassiﬁed negative malware detection task false negative condition malware cannot identiﬁed trained model. also called machine learning evasion. error shown adversarial images image recognized human neural networks cannot identify white-box attacks assume adversary knows everything related trained neural network model training data network architectures hyper-parameters numbers layers functions activations network weights etc. many adversarial examples generated calculating network gradients. since deep neural networks tend require input data without handcrafted features deploy end-to-end structure feature selection necessary compared adversarial examples machine learning. black-box attacks assume adversary access trained neural network model. adversary acting standard user knows output model assumption common attacking online machine learning services adversarial example attacks white-box attacks. transferred attack black-box however services transferability adversarial examples proposed papernot elaborate section vii-a. black-box attacks without transferability. targeted attacks misguide deep neural networks speciﬁc class. targeted attacks usually occur multiclass classiﬁcation problem. example adversary fools image classiﬁer predict adversarial examples class. face recognition/biometric system adversary tries disguise face authorized user targeted attacks usually maximize probability targeted adversarial class. non-targeted attacks assign speciﬁc class neural network output. adversarial class output arbitrary except original one. non-targeted attacks easier implement compared targeted attacks since options space redirect output. example adversary makes his/her face misidentiﬁed arbitrary face face recognition system evade detection non-targeted adversarial examples usually generated ways https//aws.amazon.com/machine-learning https//cloud.google.com/products/machine-learning https//bigml.com clarifai https//azure.microsoft.com/en-us/services/machine-learning https//console.bluemix.net/catalog/services/machine-learning https//www.faceplusplus.com running several targeted attacks taking smallest perturbation results; minimizing probability correct class. generating approaches applied targeted non-targeted attacks. binary classiﬁcation targeted attacks equivalent non-targeted attacks. universal attacks create universal perturbation whole dataset. perturbation applied clean input data. current attacks generate adversarial examples individually. however universal perturbations make easier deploy adversary examples real world. adversaries require change perturbation input sample changes. sarial examples. compared one-time attacks iterative attacks usually perform better adversarial examples require interactions victim classiﬁer cost computational time generate them. computational-intensive tasks one-time attacking feasible choice. three commonly used metrics. counts number pixels changed adversarial examples; measures euclidean distance adversarial example original sample; denotes maximum changes pixels adversarial examples. psychometric perceptual adversarial similarity score metric introduced consistent human perception. proposed method called fast gradient value method replaced sign gradient gradient ∇xj. fast gradient value method constraints pixel generate images larger local difference. according one-step attack easy transfer also easy defend applied momentum fgsm generate adversarial examples iterative way. gradient adversarial examples calculated authors increased effectiveness attack introducing momentum improved transferability applying one-step attack ensembling method. method nips targeted non-targeted adversarial attacks competition. found fgsm adversarial training robust white-box attacks black-box attacks gradient masking. proposed attack rand-fgsm added random updating adversarial examples defeat adversarial training previous methods assume adversarial data directly deep neural networks. however many applications people pass data devices kurakin applied adversarial examples physical world extended fast gradient sign method running ﬁner optimization multiple iterations. iteration clipped pixel values avoid large change pixel figure adversarial image generated fast gradient sign method left clean image panda; middle perturbation; right adversarial image classiﬁed gibbon. section illustrate several representative approaches generating adversarial examples. although many approaches defeated countermeasure later studies present methods show adversarial examples generating approaches improved extent current state-of-the-art adversarial examples attacks achieve. existence methods also requires investigation improve robustness deep neural networks. table presents methods along seven axes discussed taxonomy. suitable constant l-bfgs attack calculated approximate values adversarial examples line-searching author showed generated adversarial examples could also generalized different models different training datasets. suggested adversarial examples never/rarely seen examples test datasets. l-bfgs attack used expensive linear search method optimal value time-consuming impractical. goodfellow proposed fast method generating adversarial examples called fast gradient sign method performed step gradient update along direction sign gradient pixel. perturbation expressed magnitude perturbation. thus generated adversarial example calculated perturbation computed simply using backpropagation. figure shows adversarial example imagenet. claimed linear part high dimensional deep neural network could resist adversarial examples although linear behavior speeded training. regularization successfully fooled neural network crafted image taken cellphone camera. also found fast gradient sign method robust phototransformation iterative methods cannot resist phototransformation. found input features made signiﬁcant changes output. small perturbation designed successfully induce large output variations change small portion features could fool neural network. authors deﬁned adversarial saliency maps select feature/pixel crafted iteration. achieved adversarial success rate modifying input features sample. however method runs slow signiﬁcant computational cost. closest distance original input decision boundary adversarial examples overcome non-linearity high dimension performed iterative attack linear approximation. starting afﬁne classiﬁer found minimal perturbation afﬁne classiﬁer distance separating afﬁne hyperplane perturbation afﬁne classiﬁer binary differentiable classiﬁer used iterative method approximate perturbation considering linearized around iteration. minimal perturbation computed result also extended multi-class classiﬁer ﬁnding closest hyperplane. also extended general norm deepfool provides less perturbation compared fgsm jsma. compared jsma deepfool also reduced intensity perturbation instead number selected features. nguyen discovered type attack compositional pattern-producing network-encoded adversarial examples classiﬁed deep neural networks high conﬁdence however unrecognizable human categorize kind attack false positive attack. figure illustrates kind adversarial examples. used evolutionary algorithms algorithm produce adversarial examples. solve multi-class classiﬁcation problem using algorithms applied multidimensional archive phenotypic elites map-elites authors ﬁrst encoded images different methods direct encoding indirect encoding iteration map-elites like general algorithm chose random organism mutated randomly replaced current higher ﬁtness map-elites best individual class. claimed many adversarial images cppn could easily locate important features change output deep neural networks like jsma did. many images evolutionary found similar closely related categories. interestingly cppn fooling images accepted contest acceptance rate. carlini wagner launched targeted attack defeat defensive distillation according study c&w’s attack effective existing adversarial detecting defenses. authors made several modiﬁcations based basic problem figure universal adversarial example fools neural network images. left images original labeled natural images; center image universal perturbation; right images perturbed images wrong labels. adam like algorithms zoo-adam randomly select variable update adversarial examples. experiments showed achieved comparable performance c&w’s attack. leveraging previous method deepfool moosavidezfooli developed universal adversarial attack problem formulated universal perturbation vector satisfying iteration deepfool method minimal sample perturbation input data update perturbation total perturbation loop stop data samples fooled experiments universal perturbation generated using small part data samples instead entire dataset. figure illustrates universal adversarial example fool group images. universal perturbations shown generalized well across popular deep learning architectures introduced variant avoid constraint satisﬁes learning like adam used generate adversarial examples performed iterations generation optimal binary searching. however found gradients scale hard suitable constant iterations gradient search optimal result. reason proposed functions optimal solutions adversarial examples. third three distance measurements perturbation discussed paper attack authors provided three kinds attacks based distance metrics attack attack attack. attack conducted iteratively differentiable. iteration pixels considered trivial generating adversarial examples removed. importance pixels determined gradient distance. iteration stops remaining pixels generate adversarial example. different gradient-based adversarial generating approaches chen proposed zeroth order optimization based attack since attack require gradients directly deployed black-box attack without transferability. inspired authors modiﬁed hinge-like loss function employing gradient estimation gradient hessian need access victim deep learning models. however requires expensive computation query estimate gradients. based stochastic coordinate descent algorithms authors proposed measure distance ass) generate diverse adversarial examples authors deﬁned targeted label class original label cold class. iteration moved toward target class moving away original class. results showed generated adversarial examples comparable fgsm diversity. zhao utilized generative adversarial networks part approach generate adversarial examples images texts made adversarial examples natural human. name approach natural gan. authors ﬁrst trained wgan model dataset generator maps random noise input domain. also trained inverter input data dense inner representations. hence adversarial noise generated minimizing distance inner representations like feature adversary. adversarial examples generated using generator generator inverter built make adversarial examples natural. natural general framework many deep learning ﬁelds. applied natural image classiﬁcation textual entailment machine translation. natural require gradients original neural networks also applied black-box attack. transferability deep neural networks imagenet proposed model-based ensembling attack targeted adversarial examples authors argued compared nontargeted adversarial examples targeted adversarial examples much harder transfer deep models. using modelbased ensembling attack generate transferable adversarial examples attack black-box model. authors generated adversarial examples multiple deep neural networks full knowledge tested black-box model. model-based ensembling attack described following optimization problem require gradients neural networks used non-differential objective functions. evaluated proposed method cifar dataset using three neural networks convolution network network network vgg. results showed images successfully fooled deep neural networks least target class conﬁdence average. sabour performed targeted attack minimizing distance representation internal neural network layers instead output layer refer attack feature adversary. problem described denotes mapping image input output layer. instead ﬁnding minimal perturbation used constraint perturbation. claimed small ﬁxed value good enough human perception. similar used l-bfgs-b solve optimization problem. adversarial images natural closer targeted images internal layers. rozsa proposed hot/cold method multiple adversarial examples every single image input thought small translations rotations allowed long imperceptible. deﬁned metric psychometric perceptual adversarial similarity score measure noticeable similarity humans. hot/cold ignored unnoticeable difference based pixels replaced widely used distance pass. pass includes stages aligning modiﬁed image original image; measuring similarity aligned image original one. homography transform adversarial example original example homography matrix size solved maximizing enhanced correlation coefﬁcient optimization function structural similarity index adopted measure noticeable difference images. leveraged ssim deﬁned measurement regional ssim index rssim weights luminance contrast structure ssim calculated averaging rssim number deep neural networks generation results showed model-based ensembling attack could generate transferable targeted adversarial images enhanced power adversarial examples black-box attacks. results also proved method performs better generating nontargeted adversarial examples previous methods. authors successfully conducted black-box attack clarifai.com using model-based ensembling attack. formal veriﬁcation techniques evaluate robustness neural network even zero-day attacks carlini constructed ground-truth attack provided adversarial examples minimal perturbation ground truth network veriﬁcation always checks whether adversarial example violates property deep neural network. ground-truth attack conducted binary search found adversarial examples within small perturbation hold property network. investigated adversarial examples image classiﬁcation task. section review adversarial examples tasks. mainly focus three questions scenarios adversarial examples applied tasks? generate adversarial examples tasks? whether propose method translate problem image classiﬁcation task solve aforementioned methods? table summarizes applications adversarial examples section. deep neural networks used reinforcement learning training policies input generated adversarial examples deep reinforcement learning policies. since inherent intensive computation reinforcement learning performed fast one-time attack. huang applied fgsm attacks three deep reinforcement learning models deep network trust region policy optimization asynchronous advantage actor-critic similarly added small perturbations input policy calculating gradient cross-entropy loss function ∇xj)). stochastic policy input softmax q-values considered calculate loss function. evaluated adversarial examples four atari games three norm constraints found huang’s attack norm conducted successful attack whitebox attack also black-box attack figure adversarial attacks autoencoders perturbations added input encoder. encoding decoding decoder output adversarial image presenting incorrect class tabacof proposed adversarial examples generative models. adversary autoencoder inject perturbations input encoder generate targeted class decoding. figure depicts targeted adversarial examples autoencoder. adding perturbations input image encoder misguide autoencoder making decoder generating targeted adversarial output image. described scenario apply adversarial examples autoencoder. autoencoders used compress data encoder decompress decoder. example toderici rnn-based autoencoder compress image ledig used superresolve images adversaries leverage autoencoder reconstruct adversarial image adding perturbation input encoder. distance latent encoding representation tabacof chose kl-divergence measure tested attacks mnist svhn dataset found generating adversarial examples autoencoder much harder classiﬁers. even slightly robust deterministic autoencoder. loss function cross-entropy loss function distance original latent vector modiﬁed encoded vector tested vae-gan mnist svhn celeba datasets. experimental results latent attack achieved best result. entire image semantic segmentation targets consist pixels object detection targets consist proposals loss function sums loss targets. instead optimizing loss targets authors performed iterative optimization updated loss targets correctly predicted previous iteration. ﬁnal perturbation sums normalized perturbations iterations. deal large number targets objective detection problem authors used regional proposal network generate possible targets greatly decreases computation targets object detection. also showed capability generating images unrecognizable human deep learning could predict image segmentation task viewed image classiﬁcation task every pixel. since perturbation responsible least pixel segmentation makes space perturbations segmentation much smaller image classiﬁcation generated adversarial examples semantic image segmentation task. however attacks proposed different scenarios. discussed performed non-targeted segmentation. performed targeted segmentation tried removed certain class making deep learning model misguide background classes. generated adversarial examples assigning pixels adversarial class nearest neighbor belongs success rate measured percentage pixels chosen class changed rest classes preserved. presented method generate universal adversarial perturbations semantic image segmentation task. assigned primary objective adversarial examples objects keeping rest segmentation unchanged. metzen deﬁned background classes targeted classes targeted classes classes removed. similar pixels belong targeted classes would assigned nearest background classes deep neural network based face recognition system face detection system widely used high performance. ﬁrst provided design eyeglass frames attack deep neural network based composes blocks layers triplet loss function embedding features. based triplet loss function designed softmaxloss function step implemented adversarial eyeglass frames achieve attack physical world perturbations injected area eyeglass frames. also enhanced printability adversarial images frame adding penalty non-printability score optimized objective. similarly universal perturbation optimize perturbation applied face images. successfully dodged also misguided speciﬁc face high success rate figure illustrates example adversarial eyeglass frames. leveraging approach printability proposed attack algorithm robust physical perturbations modify road sign changed physical road signs kinds attacks overlaying adversarial road sign physical sign; sticking perturbations existing sign. included non-printability score optimization objective improve printability. object detection task proposal object viewed image classiﬁcation task every possible proposal. proposed universal algorithm called dense adversary generation generate adversarial examples object detection semantic segmentation. authors aimed making prediction incorrect figure illustrates adversarial example object detection task. adapted jsma method attack android malware detection model. evaded malware classiﬁer pdfrate hidost modifying pdf. parsed changed object structure using genetic programming. adversarial packed objects. used generate adversarial domain names evade detection domain generation algorithms. proposed based algorithm malgan generate malware examples evade black-box detection. used substitute detector simulate real detector leveraged transferability adversarial examples attack real detector. malgan evaluated programs features. however required knowledge features used model. used large number features cover required feature space portable executable ﬁles. features included header metadata section metadata import&export table metadata etc. also deﬁned several modiﬁcations generate malware evading deep learning detection. solution trained reinforcement learning reward function evasion rate. countermeasures adversarial examples main types defense strategies reactive detect adversarial examples deep neural networks built; proactive make deep neural networks robust adversaries generate adversarial examples. section discuss three kinds reactive countermeasures three kinds proactive countermeasures training classiﬁer robustifying). also discuss ensembling methods prevent adversarial examples. table summarizes countermeasures. figure adversary examples hide pedestrians semantic segmentation task left image original image; middle image segmentation original image predicted dnn; right image segmentation adversarial image predicted dnn. pedestrians. used illc attack solve problem also extended universal perturbation method universal perturbation. results showed existence universal perturbation semantic segmentation task. many tasks natural language processing attacked adversarial examples. people usually generate adversarial examples adding/deleting words sentences. task reading comprehension read paragraphs answer questions paragraphs. generate adversarial examples consistent correct answer confuse human liang added distracting sentences paragraph found models reading comprehension task overstable instead oversensitivity means deep learning models cannot tell subtle critical difference paragraphs. hence proposed kinds methods generate adversarial examples adding grammatical sentences similar question contradictory correct answer adding sentence arbitrary english words successfully fooled models tested stanford question answering dataset adversarial examples also capability transferability cannot improved adversarial training. however adversarial sentences require manpower errors sentences. aimed fool deep learning-based sentiment classiﬁer removing minimum subset words given text. reinforcement learning used approximate subset reward function proposed sentiment label changes otherwise. denotes number removing word reward function also included regularizer make sentence contiguous. deep learning used static behavioralbased malware detection well-generalized zero-day malware generated adversarial malware samples evade deep learning-based malware detection. although approaches proposed conventional machine learning classiﬁers used deep neural networks attacks. introduce attacks section. papernot used network distillation defend deep neural networks adversarial examples network distillation originally designed reduce size deep neural networks transferring knowledge large network small probability classes produced ﬁrst used inputs train second dnn. probability classes extracts knowledge learned ﬁrst dnn. softmax usually used normalize last layer produce studies trained deep neural network-based binary classiﬁers detectors classify input data legitimate input adversarial example metzen created detector adversarial examples auxiliary network original neural network detector simple small neural network predicting binary classiﬁcation i.e. probability input adversarial. safetynet extract binary threshold relu layer’s output features adversarial detector detects adversarial images rbf-svm. authors claimed method hard defeated adversaries even adversaries know detector since difﬁcult adversaries optimal value adversarial examples features safetynet detector. added outlier class original deep learning model. model detects adversarial examples classifying outlier. found measurement maximum mean discrepancy energy distance could successfully distinguish distribution adversarial datasets clean datasets. provided bayesian view detecting adversarial examples. claimed uncertainty adversarial examples higher clean data. hence deployed bayesian neural networks estimate uncertainty input data used uncertainty estimation distinguish adversarial examples clean input data. used probability divergence detectors. showed whitening principal component analysis adversarial examples different coefﬁcients low-ranked components. trained pixelcnn neural network found distribution adversarial examples different clean data. calculated p-value based rank pixelcnn rejected adversarial examples using pvalues. results showed approach could detect fgsm deepfool attack. ﬁrst trained neural networks reverse crossentropy better distinguish adversarial examples clean data latent layers detected adversarial examples using method called kernel density testing stage. reverse cross-entropy made deep neural network predict high conﬁdence true class uniform distribution classes. deep neural network trained clean input close low-dimensional manifold layer softmax. brought great convenience detection adversarial examples. temperature parameter control level knowledge distillation. deep neural networks temperature large output softmax vague small class close rest close schema network distillation repeated several times connects several deep neural networks. network distillation extracted knowledge deep neural networks improve robustness. authors found attacks primarily targeted sensitivity networks proved using high-temperature softmax reduced model sensitivity small perturbations. network distillation tested mnist cifar reduced success rate jsma attack respectively. network distillation also improved generalization neural networks. training adversarial examples countermeasures make neural networks robust. goodfellow huang ﬁrst included adversarial examples training stage. generated adversarial examples every step training inject training set. showed adversarial training improved robustness deep neural networks. adversarial training could provide regularization deep neural networks improve precision well evaluated mnist dataset. comprehensive analysis adversarial training methods imagenet dataset presented used half adversarial examples half origin examples step training. results adversarial training increased robustness neural networks one-step attack would help iterative attacks also suggested adversarial training used regularization avoid overﬁtting small mnist dataset). however carlini wagner summarized adversarial detecting methods showed methods could defend previous attack c&w’s attack slight changes loss function adversarial examples transformed clean data reconstruction. transformation adversarial examples affect prediction deep learning models. rigazio proposed variant denoising autoencoder network penalty called deep contractive autoencoder increase robustness neural networks autoencoder network trained adversarial examples original ones original samples themselves. reconstructed adversarial examples adding gaussian noise encoding autoencoder plan magnet pixeldefend reconstructed adversarial images back training distribution using pixelcnn. pixeldefend changed pixels along channel maximize probability distribution denotes training distribution \u0001def controls changes adversarial examples. pixeldefend also leveraged adversarial detecting adversarial example detected malicious change made adversarial examples katz proposed veriﬁcation method neural networks relu activation function called reluplex used satisﬁability modulo theory solver verify neural networks. authors showed within small perturbation existing adversarial example misclassify neural networks. also proved problem network veriﬁcation np-complete. carlini extended assumption relu function presenting relu relu however reluplex runs slow large computation verifying networks works dnns several hundred nodes proposed potential solutions prioritizing order checking nodes sharing information veriﬁcation. instead checking point individually gopinath proposed deepsafe provide safe regions deep neural network using reluplex. also introduced targeted robustness safe region regarding targeted class. uncertainty adversarial examples bradshaw leveraged bayesian classiﬁers build robust neural networks gaussian processes kernels used provide uncertainty estimation. proposed neural networks called gaussian process hybrid deep neural networks expressed latent variables gaussian distribution parameterized functions mean covariance encoded kernels. showed gpdnns achieved comparable performance general dnns robust adversarial examples. authors claimed gpdnns know don’t know. observed adversarial examples usually went small subset incorrect classes. separated classes sub-classes ensembled result sub-classes voting prevent adversarial examples misclassiﬁed. magnet included detectors reconstructor plan plan detectors used adversarial examples boundary manifold. ﬁrst measured distance input encoded input also probability divergence softmax output input encoded input. adversarial examples expected large distance probability divergence. deal adversarial examples close boundary magnet used reconstructor built neural network based autoencoders. reconstructor adversarial examples legitimate examples. figure illustrates workﬂow defense phases. almost defenses shown effective weak attacks. tend defensive strong unseen attacks. defenses target adversarial examples computer vision task. however development adversarial examples areas defenses areas especially safety-critical systems urgently required. section discuss current challenges potential solutions adversarial examples. although many methods theorems proposed developed recent years fundamental questions need well explained challenges need addressed. reason existence adversarial examples interesting fundamental problems adversaries researchers exploits vulnerability neural networks help defenders resist adversarial examples. discuss following questions section adversarial examples transfer? stop transferability? defenses effective others not? measure strength attack well defense? evaluate robustness deep neural network seen/unseen adversarial examples? transferability common property adversarial examples. szegedy ﬁrst found adversarial examples generated based neural network fool neural networks trained different datasets. papernot found adversarial examples generated based neural network fool neural networks different architectures even classiﬁers trained different machine learning algorithms transferability critical black-box attacks victim deep learning model training dataset accessible. attackers train substitute neural network model generate adversarial examples substitute model. victim vulnerable adversarial examples transferability. defensive view stop transferability adversarial examples defend white-box attackers need access model. deﬁne transferability adversarial examples three dimensions easy hard transfer among neural network architecture trained different data; transfer among different neural network architectures trained task; transfer among deep neural networks different tasks. best knowledge existing solution third dimension many studies examined transferability show ability adversarial examples papernot studied transferability conventional machine learning techniques deep neural networks. found adversarial examples transferred different parameters training investigated transferability targeted nontargeted adversarial examples large models large datasets found non-targeted adversarial examples much transferable targeted ones. observed decision boundary non-targeted adversarial aligns well other. thus proposed model-based ensembling attack create transferable targeted adversarial examples. tramèr found distance model’s decision boundary average larger distance models’ boundaries direction explain existence transferability adversarial examples. tramèr also claimed transferability might inherent property deep neural networks showing counter-example. reason existence adversarial examples still open question. adversarial examples inherent property deep neural networks? adversarial examples achilles’ heel deep neural networks high performance? many papers proposed hypotheses explain existence. suggested adversarial examples probability even never observed data samples testing dataset. deep neural networks fooled covariate shift. training pixelcnn found distribution adversarial examples different clean data. claimed adversarial examples occurred large contiguous space instead randomly scattered. experiments adversarial examples transferring small neural networks formed -dimensional space. suggested adversarial examples results models linear high dimensional manifolds. showed linear case adversarial examples exist decision boundary close manifold training data. contrary believed adversarial examples ﬂexibility classiﬁer certain tasks. also found linearity obvious explanation. showed adversarial examples phenomenon deep neural networks also classiﬁers. blamed adversarial examples sparse discontinuous manifold makes classiﬁer erratic. suggested decision boundaries deep neural networks inherently incorrect detect semantic objects. instead analyzing train robust deep neural network claimed adversarial examples test coverage corner cases. besides adversarial examples image classiﬁcation task shown section adversarial examples generated various applications scenarios. many deployed completely different methods. applications method used image classiﬁcation task. however need propose novel method. current studies adversarial examples mainly focuses image classiﬁcation task. existing paper explains relationship among different applications. exist universal attacking/defending method applied applications? competition attacks defenses adversarial examples becomes arms race defensive method proposed prevent existing attacks later shown vulnerable attacks vice versa defenses showed could defend certain attack later failed slight change attack hence evaluation robustness deep neural network necessary. provided upper bound robustness linear classiﬁer quadratic classiﬁer. following problems robustness evaluation deep neural networks require exploration. methodology evaluation robustness deep neural networks many deep neural networks planned deployed safety-critical settings. defending existing attacks sufﬁcient. zero-day attacks would harmful deep neural networks. methodology evaluating robustness deep neural networks required especially zero-day attacks helps people understand conﬁdence model prediction much rely real world. conducted initial studies evaluation. moreover problem lies performance deep neural network models also conﬁdentiality privacy. benchmark platform attacks defenses attacks defenses described methods without publicly available code mention parameters used methods. brings difﬁculties researchers reproduce solutions provide corresponding attacks/defenses. example carlini tried best best possible defense parameters random initialization. researchers even drew different conclusions different settings experiments. exists benchmark adversaries defenders conduct experiments uniform make fairly clearer comparisons different attacking defending techniques. cleverhans foolbox open-source libraries benchmark vulnerability deep neural networks adversarial images. build great frameworks benchmark attacks. however defensive strategies missing tools. providing dataset adversarial examples generated different methods make easy ﬁnding blind point deep neural networks developing defense strategies. problem also occurs areas deep learning. figure workﬂow benchmark platform attackers defenders attackers defenders update/train strategies training dataset; attackers generate adversarial examples clean data; adversarial examples veriﬁed crowdsourcing whether recognizable human; defenders generate deep neural network defensive strategy; evaluate defensive strategy. various applications robustness evaluation similar existence adversarial examples various applications wide range applications make hard evaluate robustness deep neural network architecture. compare methods generating adversarial example different scenario? universal methodology evaluate robustness scenarios? tackling unsolved problems future direction. paper reviewed recent ﬁndings adversarial examples deep neural networks. investigated existing methods generating adversarial examples. taxonomy adversarial examples proposed. also explored applications countermeasures adversarial examples. paper attempted cover state-of-the-art studies adversarial examples deep learning domain. compared recent work adversarial examples analyzed discussed current challenges potential solutions lying adversarial examples. simonyan zisserman very deep convolutional networks large-scale image recognition arxiv preprint arxiv. redmon farhadi yolo better faster stronger arxiv silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot mastering game deep neural networks tree search nature vol. silver hubert schrittwieser antonoglou guez lanctot sifre kumaran graepel mastering chess shogi self-play general reinforcement learning algorithm arxiv preprint arxiv. dahl stokes deng large-scale malware classiﬁcation using random projections neural networks acoustics speech signal processing ieee international conference saxe berlin deep neural network based malware detection using dimensional binary program features malicious unwanted software international conference dong towards interpretable deep neural networks leveraging adversarial examples proceedings ieee conference computer vision pattern recognition mozaffari-kermani sur-kolay raghunathan systematic poisoning attacks defenses machine learning healthcare ieee journal biomedical health informatics vol. fredrikson ristenpart model inversion attacks exploit conﬁdence information basic countermeasures proceedings sigsac conference computer communications security. abadi goodfellow mcmahan mironov talwar zhang deep learning differential privacy proceedings sigsac conference computer communications security. zhang delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation proceedings ieee international conference computer vision goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets advances neural information processing systems ledig theis huszár caballero cunningham acosta aitken tejani totz wang photo-realistic single image super-resolution using generative adversarial network conference computer vision pattern recognition klambauer unterthiner mayr hochreiter selfnormalizing neural networks arxiv preprint arxiv. ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning szegedy sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolutions proceedings ieee conference computer vision pattern recognition szegedy vanhoucke ioffe shlens wojna rethinking inception architecture computer vision proceedings ieee conference computer vision pattern recognition russakovsky deng krause satheesh huang karpathy khosla bernstein berg fei-fei imagenet large scale visual recognition challenge international journal computer vision vol. biggio fumera roli multiple classiﬁer systems robust classiﬁer design adversarial environments international journal machine learning cybernetics vol. biggio corona maiorca nelson šrndi´c laskov giacinto roli evasion attacks machine learning test time joint european conference machine learning knowledge discovery databases. springer p.-y. chen zhang sharma c.-j. hsieh zeroth order optimization based black-box attacks deep neural networks without training substitute models arxiv preprint arxiv. sharif bhagavatula bauer reiter accessorize crime real stealthy attacks state-of-the-art face recognition proceedings sigsac conference computer communications security. s.-m. moosavi-dezfooli fawzi frossard deepfool simple accurate method fool deep neural networks proceedings ieee conference computer vision pattern recognition nguyen yosinski clune deep neural networks easily fooled high conﬁdence predictions unrecognizable images proceedings ieee conference computer vision pattern recognition s.-m. moosavi-dezfooli fawzi fawzi frossard universal adversarial perturbations proceedings ieee conference computer vision pattern recognition evangelidis psarakis parametric image alignment using enhanced correlation coefﬁcient maximization ieee transactions pattern analysis machine intelligence vol. flynn ward abich poole image quality assessment using ssim noticeable difference paradigm international conference engineering psychology cognitive ergonomics. springer bradshaw matthews ghahramani adversarial examples uncertainty transfer testing robustness gaussian process hybrid deep networks arxiv preprint arxiv. abbasi gagné robustness adversarial examples chen carlini song adversarial example defense ensembles weak defenses strong usenix workshop offensive technologies vancouver usenix association fawzi fawzi frossard analysis classiﬁers’ robustness adversarial perturbations arxiv preprint arxiv. bastani ioannou lampropoulos vytiniotis nori criminisi measuring neural robustness constraints advances neural information processing systems goodfellow papernot mcdaniel feinman faghri matyasko hambardzumyan y.-l. juang kurakin sheatsley garg y.-c. cleverhans adversarial machine learning library arxiv preprint arxiv. rauber brendel bethge foolbox python toolbox benchmark robustness machine learning models arxiv preprint arxiv. available http//arxiv.org/abs/. mnih badia mirza graves lillicrap harley silver kavukcuoglu asynchronous methods deep reinforcement learning international conference machine learning hendrik metzen chaithanya kumar brox fischer universal adversarial perturbations semantic image segmentation proceedings ieee conference computer vision pattern recognition", "year": 2017}