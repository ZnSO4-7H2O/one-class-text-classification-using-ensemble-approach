{"title": "StackGAN++: Realistic Image Synthesis with Stacked Generative  Adversarial Networks", "tag": ["cs.CV", "cs.AI", "stat.ML"], "abstract": "Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) aiming at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of the object based on given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and discriminators in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.", "text": "abstract—although generative adversarial networks shown remarkable success various tasks still face challenges generating high quality images. paper propose stacked generative adversarial networks aiming generating high-resolution photo-realistic images. first propose two-stage generative adversarial network architecture stackgan-v text-to-image synthesis. stage-i sketches primitive shape colors object based given text description yielding low-resolution images. stage-ii takes stage-i results text descriptions inputs generates highresolution images photo-realistic details. second advanced multi-stage generative adversarial network architecture stackganv proposed conditional unconditional generative tasks. stackgan-v consists multiple generators discriminators tree-like structure; images multiple scales corresponding scene generated different branches tree. stackgan-v shows stable training behavior stackgan-v jointly approximating multiple distributions. extensive experiments demonstrate proposed stacked generative adversarial networks signiﬁcantly outperform stateof-the-art methods generating photo-realistic images. index terms—generative models generative adversarial networks multi-stage gans multi-distribution approximation photo-realistic image generation text-to-image synthesis. introduction generative adversarial network generative model proposed goodfellow original setting composed generator discriminator trained competing goals. generator trained generate samples towards true data distribution fool discriminator discriminator optimized distinguish real samples true data distribution fake samples produced generator. recently shown great potential simulating complex data distributions texts images videos despite success models known difﬁcult train. training process usually unstable sensitive choices hyper-parameters. several papers argued instability partially disjoint supports data distribution implied model distribution department electronic engineering chinese university hong kong shatin hong kong. e-mail hsliee.cuhk.edu.hk zhang baidu research beijing china. e-mail zhangshaot wang department electronic engineering chinese university hong kong shatin hong kong. e-mail xgwangee.cuhk.edu.hk huang department computer science engineering metaxas department computer science rutgers problem severe training generate high-resolution images chance rare high-resolution image distribution model distribution share supports high-dimensional space. moreover common failure phenomenon training mode collapse many generated samples contain color texture pattern. order stabilize training process improve sample diversity several methods tried address challenges proposing network architectures introducing heuristic tricks modifying learning objectives previous methods designed approximate single data distribution difﬁculty directly approximating high-resolution image data distribution rare overlap model data distributions highdimensional space previous methods limited generating low-resolution images. work observe that real world data especially natural images modeled different scales view multi-resolution digitized images samplings continuous image signal different sampling rates. henceforth distributions images multiple discrete scales related. apart multiple distributions different scales images coupled without auxiliary conditioning variables viewed conditional distributions unconditional distributions also related distributions. motivated observations argue stably trained generate high resolution images breaking difﬁcult generative task sub-problems progressive goals i.e. propose stacked generative adversarial networks model series lowfirst propose two-stage generative adversarial network stackgan-v generate images text descriptions sketch-reﬁnement process low-resolution images ﬁrst generated stage-i gan. stage-i stack stage-ii generate highresolution images. conditioning stage-i result text again stage-ii learns capture text information omitted stage-i draws details. further propose novel conditioning augmentation technique encourage smoothness latent conditioning manifold allows small random perturbations conditioning manifold increases diversity synthesized images. second propose advanced multi-stage generative adversarial network architecture stackgan-v conditional unconditional generative tasks. stackgan-v multiple generators share parameters tree-like structure. shown figure input network viewed root tree multiscale images generated different branches tree. generator deepest branch ﬁnal goal generating photo-realistic high-resolution images. generators intermediate branches progressive goals generating small large images help accomplish ﬁnal goal. whole network jointly trained approximate different highly related image distributions different branches. positive feedback modeling distribution improve learning others. conditional image generation tasks proposed stackgan-v simultaneously approximates unconditional image-only distribution image distribution conditioned text descriptions. types distributions complementary other. moreover propose color-consistency regularization term guide generators generate coherent samples across different scales. regularization provides additional constraints facilitate multi-distribution approximation especially useful unconditional setting instancewise supervision image input noise vector. summary proposed stacked generative adversarial networks three major contributions. stackgantime generates images resv ﬁrst olution photo-realistic details text descriptions. conditioning augmentation technique proposed stabilize conditional gans’ training also improves diversity generated samples. stackgan-v improves quality generated images stabilizes gans’ training jointly approximating multiple distributions. remainder paper ﬁrst discuss related work preliminaries section section respectively. introduce stackgan-v section stackgan-v section section extensive experiments conducted evaluate proposed methods. finally make conclusions section source code stackganv available https//github.com/hanzhanggit/stackgan source code stackgan-v available related work generative image modeling fundamental problem computer vision. remarkable progress direction emergence deep learning techniques. variational autoencoders formulated problem probabilistic graphical models whose goal maximize lower bound data likelihood. autoregressive models utilized neural networks model pixel space also generated appealing synthetic images. recently generative adversarial networks shown promising performance generating sharper images. training instability makes hard models generate high-resolution images. works proposed stabilize training improve image qualities image generation also studied. methods utilized simple conditioning variables attributes class labels also work conditioned images generate images including photo editing domain transfer super-resolution however super-resolution methods limited details low-resolution images correct large defects proposed stackgan does. recently several methods developed generate images unstructured text. mansimov built aligndraw model learning estimate alignment text generating canvas. reed used conditional pixelcnn generate images using text descriptions object location constraints. nguyen used approximate langevin sampling approach generate images conditioned text. however sampling approach requires inefﬁcient iterative optimization process. conditional reed successfully generated plausible images birds ﬂowers based text descriptions. follow-up work able generate images utilizing additional annotations object part locations. given difﬁculties modeling details natural images many works proposed multiple gans improve sample quality. denton built series gans within laplacian pyramid framework. level pyramid residual image generated conditioned image previous stage added back input image produce input next stage. wang utilized structure style synthesize images indoor scenes. yang factorized image generation foreground background generation layered recursive gans. concurrent work huang added several gans reconstruct multi-level representations pre-trained discriminative model. still unable generate high resolution image photo-realistic details. durugkar used multiple discriminators along generator increase chance generator receiving effective feedback. preliminaries generative adversarial network composed models alternatively trained compete other. generator optimized reproduce true data distribution pdata generating images difﬁcult discriminator differentiate real images. meanwhile optimized distinguish real images synthetic images generated overall training procedure similar two-player min-max game following objective function conditional extension generator discriminator receive additional conditioning variables yielding formulation allows generate images conditioned variables stackgan-v two-stage generative adversarial network generate high-resolution images photo-realistic details propose simple effective two-stage generative adversarial network stackgan-v. shown figure decomposes text-to-image generative process stages. stage-i sketches primitive shape basic colors object conditioned given text description draws background layout random noise vector yielding low-resolution image. stage-ii corrects defects low-resolution image stage-i completes details object reading text description again producing high-resolution photo-realistic image. conditioning augmentation shown figure text description ﬁrst encoded encoder yielding text embedding previous works text embedding nonlinearly transformed generate conditioning latent variables input generator. however latent space text embedding usually high dimensional limited amount data usually causes discontinuity latent data manifold desirable learning generator. mitigate problem introduce conditioning augmentation technique produce additional conditioning variables contrast ﬁxed conditioning text variable randomly sample latent variables independent gaussian distribution mean diagonal covariance matrix functions text embedding proposed conditioning augmentation yields training pairs given small number image-text pairs thus encourages robustness small perturbations along conditioning manifold. enforce smoothness conditioning manifold avoid overﬁtting following regularization term objective generator training kullback-leibler divergence standard gaussian distribution conditioning gaussian distribution. randomness introduced conditioning augmentation beneﬁcial modeling text image translation sentence usually corresponds objects various poses appearances. stage-i instead directly generating high-resolution image conditioned text description simplify task ﬁrst generate low-resolution image stage-i focuses drawing rough shape correct colors object. text embedding given description generated pre-trained encoder paper. gaussian conditioning variables text embedding sampled capture meaning variations. conditioned random variable stage-i trains discriminator generator alternatively maximizing minimizing real image text description true data distribution pdata. noise vector randomly sampled given distribution regularization parameter balances terms experiments. using reparameterization trick introduced learned jointly rest network. model architecture. generator obtain text conditioning variable text embedding ﬁrst fully connected layer generate gaussian distribution sampled gaussian distribution. dimensional conditioning vector computed elementwise multiplication then concatenated dimensional noise vector generate image series up-sampling blocks. fig. architecture proposed stackgan-v. stage-i generator draws low-resolution image sketching rough shape basic colors object given text painting background random noise vector. conditioned stage-i results stage-ii generator corrects defects adds compelling details stage-i results yielding realistic high-resolution image. tensor. meanwhile image series downsampling blocks spatial dimension. then image ﬁlter concatenated along channel dimension text tensor. resulting tensor convolutional layer jointly learn features across image text. finally fully-connected layer node used produce decision score. stage-ii low-resolution images generated stage-i usually lack vivid object parts might contain shape distortions. details text might also omitted ﬁrst stage vital generating photo-realistic images. stageii built upon stage-i results generate highresolution images. conditioned low-resolution images also text embedding correct defects stage-i results. stage-ii completes previously ignored text information generate photo-realistic details. different original formulation random noise used stage assumption randomness already preserved gaussian conditioning variables used stage used stage-i share pre-trained text encoder generating text embedding however stage-i stage-ii conditioning augmentation different fully connected layers generating different means standard deviations. stage-ii learns capture useful model architecture. design stage-ii generator encoder-decoder network residual blocks similar previous stage used generate dimensional text conditioning vector spatially replicated form tensor. meanwhile stage-i result generated stage-i several down-sampling blocks spatial size image features text features concatenated along channel dimension. encoded image features coupled text features several residual blocks designed learn multi-modal representations across image text features. finally series up-sampling layers used generate high-resolution image. generator able help rectify defects input image details generate realistic high-resolution image. discriminator structure similar stagediscriminator extra down-sampling blocks since image size larger stage. explicitly enforce learn better alignment image conditioning text rather using vanilla discriminator adopt matching-aware discriminator proposed reed stages. training discriminator takes real images corresponding text descriptions positive sample pairs whereas negative sample pairs consist groups. ﬁrst real images mismatched text embeddings second synthetic images corresponding text embeddings. implementation details up-sampling blocks consist nearest-neighbor upsampling followed stride convolution. batch normalization relu activation applied every convolution except last one. residual blocks consist stride convolutions batch normalization loss function approximating image distribution generator modiﬁed maximize log) instead minimizing log) mitigate problem gradient vanishing training process discriminators generators alternately optimized till convergence. proposed stackgan-v jointly approximating multiple image distributions increases chance data distributions sharing supports model distributions. adding auxiliary generation tasks intermediate branches provides gradient signals training whole network. instance approximating lowresolution image distribution ﬁrst branch results images basic color structures. generators subsequent branches focus completing details generating higher resolution images. joint conditional unconditional distribution approximation unconditional stackgan-v trained distinguish real images fake ones. handle conditional image generation conventionally images corresponding conditioning variables input discriminator determine whether imagecondition pair matches guides generator approximate conditional image distribution. propose conditional stackgan-v jointly approximates conditional unconditional image distributions. generator conditional stackgan-v converted take conditioning vector input conditioning vector replaces noise vector encourage generators draw images details according conditioning variables. consequently multi-scale samples generated objective function training discriminator conditional stackganv consists terms unconditional loss relu. residual blocks used stackganv models four used models. downsampling blocks consist stride convolutions batch normalization leakyrelu except ﬁrst batch normalization. default training ﬁrst iteratively train stage-i epochs ﬁxing stage-ii gan. iteratively train stage-ii another epochs ﬁxing stage-i gan. networks trained using adam solver batch size initial learning rate learning rate decayed previous value every epochs. source code stackgan-v available https//github.com/hanzhanggit/stackgan implementation details. stackgan-v generative adversarial network discussed above stackgan-v separate networks stage-i stage-ii model low-tohigh resolution image distributions. make framework general paper propose end-to-end network stackgan-v model series multi-scale image distributions. shown figure stackgan-v consists multiple generators discriminators tree-like structure. images low-resolution high-resolution generated different branches tree. branch generator captures image distribution scale discriminator estimates probability sample came training images scale rather generator. generators jointly trained approximate multiple distributions generators discriminators trained alternating fashion. section explore types multi-distributions multi-scale image distributions; joint conditional unconditional image distributions. multi-scale image distributions approximation stackgan-v framework tree-like structure takes noise vector pnoise input multiple generators produce images different scales. pnoise prior distribution usually chosen standard normal distribution. latent variables transformed hidden features layer layer. compute hidden features generator non-linear transformation represents hidden features branch total number branches modeled neural networks order capture information omitted preceding branches noise vector concatenated hidden features inputs calculating based hidden features different layers generators produce samples small-tolarge scales fig. overall framework proposed stackgan-v conditional image synthesis task. vector conditioning variables computed class label text description etc.. numbers channels tensor. empirically default. input vector multi-scale samples generated generators stackgan-v. added loss function generator deﬁned therefore ﬁnal loss training generator deﬁned lci. experimental results indicate color-consistency regularization important unconditional task needed text-to-image synthesis task stronger constraint i.e. instance-wise correspondence images text descriptions. shown figure stackgan-v models designed generate images. input vector ﬁrst transformed ××ng feature tensor number ××ng tensor channels tensor. then gradually transformed ××ng ××ng eventually ××ng tensors different layers network up-sampling blocks. intermediate ××ng ××ng ××ng features used generate images corresponding scales convolutions. conditioning variables unconditional variables also directly intermediate layers network ensure encoded information omitted. discriminators down-sampling blocks convolutions transform input image ××nd tensor eventually sigmoid function used outputting probabilities. datasets residual blocks every generators. adam solver learning rate used models. source code stackgan-v available https//github.com/hanzhanggit/stackgan-v implementation details. unconditional loss determines whether image real fake conditional determines whether image condition match not. accordingly loss function generator converted generator scale therefore jointly approximates unconditional conditional image distributions. ﬁnal loss jointly training generators conditional stackganv computed substituting color-consistency regularization increase image resolution different generators generated images different scales share similar basic structure colors. color-consistency regularization term introduced keep samples generated input different generators consistent color thus improve quality generated images. represent pixel generated image mean covariance pixels given number pixels image. color-consistency regularization term aims minimizing differences different scales encourage consistency deﬁned experiments conduct extensive experiments evaluate proposed methods. section several state-of-the-art methods text-to-image synthesis unconditional image synthesis compared proposed methods. ﬁrst evaluate effectiveness stackganv comparing gawwn gan-intcls text-to-image synthesis. then stackganv compared stackgan-v show advantages limitations. moreover stackgan-v general framework also solve unconditional image synthesis tasks tasks compared several state-ofthe-art methods section several baseline models designed investigate overall design important components stackgan-v. ﬁrst baseline directly train stage-i generating images investigate whether proposed twostage stacked structure conditioning augmentation beneﬁcial. modify stackgan-v generate images investigate whether larger images method result higher image quality. also investigate whether inputting text stages stackgan-v useful. section experiments designed validate important components stackgan-v including designs fewer multi-scale image distributions effect jointly approximating conditional unconditional distributions effectiveness proposed color-consistency regularization. datasets. evaluate conditional stackgan textto-image synthesis oxford- coco datasets. contains bird species images. since birds dataset object-image size ratios less preprocessing step crop images ensure bounding boxes birds greater-than-. object-image size ratios. oxford- contains images ﬂowers different categories. show generalization capability approach challenging dataset coco also utilized evaluation. different oxford coco dataset contains images multiple objects various backgrounds. image coco descriptions descriptions provided every image oxford- datasets. following experimental setup directly training validation sets provided coco meanwhile split oxford- class-disjoint training test sets. unconditional stackgan-v utilizes bedroom church subsets lsun dog-breed cat-breed sub-sets imagenet synthesize different types images. since dataset complex others covers classes imagenet also report quantitative results dataset. statistics datasets presented table difﬁcult evaluate performance generative models choose recently proposed numerical assessment approach inception score quantitative evaluation denotes generated sample label predicted inception model intuition behind metric good models generate diverse meaningful images. therefore divergence marginal distribution conditional distribution large. experiments directly pre-trained inception model coco imagenet datasets. ﬁne-grained datasets oxford- ﬁne-tune inception model them. suggested evaluate metric large number samples model. although inception score shown well correlate human perception visual quality samples cannot generated images well conditioned given text descriptions. therefore also conduct human evaluation. randomly select text descriptions class oxford- test sets. coco dataset text descriptions randomly selected validation set. sentence images generated model. given text descriptions users asked rank results different methods. average ranks human users calculated evaluate compared methods. addition t-sne embedding method visualize large number high-dimensional images two-dimensional map. observe t-sne good tool examine distribution synthesized images identify collapsed modes. comparison state-of-the-art models demonstrate effectiveness proposed method compare state-of-the-art models text-to-image synthesis unconditional image synthesis text-to-image synthesis. ﬁrst compare stackganv state-of-the-art text-to-image methods oxford- coco datasets demonstrate effectiveness. compare stackgan-v stackgan-v show advantages limitations. inception scores average human ranks proposed stackgan models compared methods reported table representative examples compared figure figure figure text-to-image synthesis task stackgan-v model achieves best inception score average human rank three datasets. example compared ganint-cls stackgan-v achieves improvement terms inception score dataset improvement oxford- better average human rank stackganv also indicates proposed method able generate realistic samples conditioned text descriptions. shown figure samples generated ganint-cls reﬂect general shape color birds. results lack vivid parts convincing details cases make neither realistic enough sufﬁciently high resolution. using additional conditioning variables location constraints gawwn obtains better inception score dataset still slightly lower ours. generates higher resolution images details gan-intcls shown figure however mentioned authors gawwn fails generate plausible images conditioned text descriptions comparison stackgan-v ﬁrst time generates images resolution photo-realistic details text descriptions. compared stackgan-v stackgan-v improves inception score dataset. visual comparison results models utilize t-sne algorithm. model large number images generated embedded plane. ﬁrst extract feature generated image using pre-trained inception model. then t-sne algorithm applied embed features plane resulting location image plane. page limits figure shows grid compressed images dataset generated image mapped nearest grid location. shown figure stackgan-v contains collapsed modes stackgan-v generates failurecase images stage-i images around collapsed mode. stackgan-v since generators trained jointly receive regularization thus result stable training behavior less chance mode collapse. observe collapsed nonsensical mode visualization stackgan-v-generated images. stackgan-v advanced stackgan-v many aspects stackgan-v advantage trained stage stage requiring less memory. ness stackgan-v unconditional image generation task comparing dcgan wgan ebganpt lsgan lsun bedroom dataset. shown figure stackgan-v able generate images photo-realistic details. figure also compare samples generated stackgan-v ebgan-pt. shown ﬁgure samples generated methods resolution stackgan-v generates realistic ones qualitative results reported dcgan model trained quantitative comparison using public available source code imagenet dataset. inception score dcgan much lower inception achieved stackganv experiments demonstrate stackgan-v outperforms state-of-the-art methods unconditional image generation. example images generated stackgan-v lsun church imagenet datasets presented figure design stackgan-v. shown ﬁrst four rows table stage-i directly used generate images inception scores decrease signiﬁcantly. performance drop well illustrated results figure shown ﬁrst figure stage-i fails generate plausible samples without using conditioning augmentation although stage-i able generate diverse samples samples realistic samples generated stackgan-v. demonstrates necessity proposed stacked structure. addition decreasing output resolution inception score decreases note images scaled calculating inception score. thus stackgan-v increases image size without adding information inception score would remain samples different resolutions. therefore decrease inception score stackgan-v demonstrates stackgan-v details larger images. stackgan-v text input stage-i inception score decreases indicates processing text descriptions stage-ii helps reﬁne stage-i results. conclusion drawn results stackgan-v models. fig. conditioning augmentation helps stabilize training conditional improves diversity generated samples. without stage-i fails generate plausible samples. although different noise vector used column generated samples collapse input text description. ﬁxing noise vectors methods still able generate birds different poses viewpoints. fig. generated images retrieving nearest training images utilizing stage-ii discriminator stackgan-v extract visual features. distances features calculated nearest-neighbor retrieval. figure illustrates examples stage-i stage-ii images generated stackgan-v. shown ﬁrst figure cases stage-i able draw rough shapes colors objects given text descriptions. however stage-i images usually blurry various defects missing details especially foreground objects. shown second stage-ii generates fig. images generated interpolating sentence embeddings. gradual appearance changes ﬁrst sentence’s meaning second sentence observed. noise vector ﬁxed zeros row. higher resolution images convincing details better reﬂect corresponding text descriptions. cases stage-i generated plausible shapes colors stage-ii completes details. instance column figure satisfactory stage-i result stageii focuses drawing short beak white color described text well details tail legs. examples different degrees details added stage-ii images. many cases stage-ii able correct defects stage-i results processing text description again. example stage-i image column blue crown rather reddish brown crown described text defect corrected stage-ii gan. extreme cases even stage-i fails draw plausible shape stage-ii able generate reasonable objects. also observe stackgan-v ability transfer background stage-i images ﬁne-tune realistic higher resolution stage-ii. importantly stackgan-v achieve good results simply memorizing training samples capturing complex underlying language-image relations. extract visual features generated images training images stage-ii discriminator stackganv. generated image nearest neighbors training retrieved. visually inspecting retrieved images conclude generated images similar characteristics training samples essentially different. conditioning augmentation. also investigate efﬁcacy proposed conditioning augmentation removing stackgan-v inception score decreases figure also shows stage-i generate birds different poses viewpoints text embedding. contrast without using samples generated stage-i collapse nonsensical images unstable training dynamics gans. consequently proposed conditioning augmentation helps stabilize conditional training improves diversity generated samples ability encourage robustness small perturbations along latent manifold. sentence embedding interpolation. demonstrate stackgan-v learns smooth latent data manifold generate images linearly interpolated sentence embeddings shown figure noise vector generated image inferred given text description only. images ﬁrst generated simple sentences made sentences contain simple color descriptions. results show generated images interpolated embeddings accurately reﬂect color changes generate plausible bird shapes. second illustrates samples generated complex sentences contain details bird appearances. generated images change primary color blue change wing color black brown. component analysis stackgan-v section analyze important components proposed stackgan-v. table lists models different settings inception scores test set. figure shows example images generated different baseline models. baseline models built removing changing certain component stackgan-v model. approximating single image distribution inception scores dramatically decrease stackgan-v-g stackgan-v-all inspired also build baseline model multiple discriminators namely stackganv-g. discriminators structure different results show improvement stackgan-v-g. similar comparisons also done unconditional task lsun bedroom dataset. shown figures baseline models fail generate realistic images suffer severe mode collapses. unconditional discriminators conventional ones resulting much lower inception score stackganv. another baseline model color-consistency regularization term. results various datasets show color-consistency regularization signiﬁcant positive effects unconditional image synthesis task. quantitatively removing color-consistency regularization decreases inception score imagenet dataset. demonstrates additional constraint provided color-consistency regularization able facilitate multi-distribution approximation help generators different branches produce coherent samples noise input. worth mentioning need utilize color-consistency regularization text-to-image synthesis task instance-wise constraint images corresponding text descriptions. experimentally adding color-consistency regularization improve inception score dataset. conclusions paper stacked generative adversarial networks stackgan-v stackgan-v proposed decompose difﬁcult problem generating realistic high-resolution images manageable sub-problems. stackganv conditioning augmentation ﬁrst proposed textto-image synthesis novel sketch-reﬁnement process. succeeds generating images resolution photo-realistic details text descriptions. further improve quality generated samples stabilize gans’ training stackgan-v jointly approximates multiple related distributions including multi-scale image distributions joint conditional unconditional image distributions. addition color-consistency regularization proposed facilitate multi-distribution approximation. extensive quantitative qualitative results demonstrate proposed methods signiﬁcantly improve state conditional unconditional image generation tasks. chen duan houthooft schulman sutskever abbeel. infogan interpretable representation learning information maximizing generative adversarial nets. nips table inception scores stackgan-v baseline models test set. means using proposed discriminator jointly approximates conditional unconditional distributions. fig. example images generated without proposed color-consistency regularization stackgan-v imagenet imagenet lsun church datasets. images respectively. ledig theis huszar caballero aitken tejani totz wang shi. photo-realistic single image super-resolution using generative adversarial network. cvpr russakovsky deng krause satheesh huang karpathy khosla bernstein berg feifei. imagenet large scale visual recognition challenge. international journal computer vision yang kannan batra parikh. lr-gan layered recursive generative adversarial networks image generation. iclr zhang song seff xiao. lsun construction large-scale image dataset using deep learning humans loop. arxiv preprint arxiv. information science china agricultural university beijing china m.e. communication information systems beijing university posts telecommunications beijing china currently ph.d. student department computer science rutgers university piscataway current research interests include computer vision deep learning medical image processing. received b.e. agricultural mechanization automatization china agricultural university beijing china m.s. computer science institute computing technology chinese academy science beijing china currently ph.d. student department computer science engineering lehigh university bethlehem current research interests include deep learning computer vision medical image processing. received bachelors degree automation east china university science technology masters doctorate degrees computer science lehigh university pennsylvania respectively. currently department electronic engineering chinese university hong kong. research interests include image analysis computer vision medical degree received zhejiang university tong shanghai university degree january computer interface medical largemachine scale visual learning. ieee. xiaogang wang received degree university science technology china degree chinese university hong kong degree computer science artiﬁcial intelligence laboratory massachusetts institute technology currently associate professor department electronic engineering chinese university hong kong. research interests include computer vision machine learning. xiaolei huang received doctorate degree computer science rutgers university–new brunswick bachelor’s degree computer science tsinghua university currently associate professor computer science engineering department lehigh university bethlehem research interests areas computer vision biomedical image analysis computer graphics machine learning. areas published articles journals tpami media scientiﬁc reports. also regularly contributes research papers conferences cvpr miccai iccv. serves associate editor computer vision image understanding journal. member ieee. dimitris metaxas received degree national technical university athens greece degree university maryland degree university toronto professor computer science department rutgers university. directing computational biomedicine imaging modeling center conducting research toward development formal methods upon computer vision computer graphics medical imaging advance synergistically. fellow ieee.", "year": 2017}