{"title": "A Group Theoretic Perspective on Unsupervised Deep Learning", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Why does Deep Learning work? What representations does it capture? How do higher-order representations emerge? We study these questions from the perspective of group theory, thereby opening a new approach towards a theory of Deep learning.  One factor behind the recent resurgence of the subject is a key algorithmic step called {\\em pretraining}: first search for a good generative model for the input samples, and repeat the process one layer at a time. We show deeper implications of this simple principle, by establishing a connection with the interplay of orbits and stabilizers of group actions. Although the neural networks themselves may not form groups, we show the existence of {\\em shadow} groups whose elements serve as close approximations.  Over the shadow groups, the pre-training step, originally introduced as a mechanism to better initialize a network, becomes equivalent to a search for features with minimal orbits. Intuitively, these features are in a way the {\\em simplest}. Which explains why a deep learning network learns simple features first. Next, we show how the same principle, when repeated in the deeper layers, can capture higher order representations, and why representation complexity increases as the layers get deeper.", "text": "modern incarnation neural networks popularly known deep learning accomplished record-breaking success processing diverse kinds signals vision audio text. parallel strong interest ensued towards constructing theory paper opens group theory based approach towards theoretical understanding particular unsupervised variant. first establish single layer unsupervised pre-training explained light orbit-stabilizer principle sketch principle extended multiple layers. round training layer connected temporary output layer trained learn weights needed reproduce input step executed layer-wise starting ﬁrst hidden layer sequentially moving deeper often referred pre-training hinton salakhutdinov hinton bengio resulting layer called autoencoder. figure shows schematic autoencoder. weight learnt network. subsequently presented input network produce output point output units well weight discarded. alternate characterization autoencoder unit above maps input space itself. moreover learning deﬁnition stabilizer input input signals often decomposable features autoencoder attempts succinct features inputs decomposed into. satisfying pmeans learned conﬁgurations reproduce features. figure illustrates post-training behavior. hidden units learned features then comes back input output must words learning feature equivalent searching transformation stabilizes idea stabilizers invites analogy reminiscent orbit-stabilizer relationship studied theory group actions. suppose group acts moving points around consider points reachable group action. called orbit. subset group elements leave unchanged. subset stabilizer possible deﬁne notion volume group inverse relationship figure post-learning feature stabilized alternate ways decomposing signal simpler features. neurons could potentially learn features bottom row. almost surely simpler ones learned. volumes holds even actually subset example ﬁnite groups product |ox| |sx| order group. inverse relationship volumes orbits stabilizers takes central role connect back many possible ways decompose signals smaller features. figure illustrates point rectangle decomposed l-shaped features straightline edges. experiments date suggest neural network likely learn edges. why? answer this imagine space autoencoders form group. batch learning iterations stops whenever stabilizer found. roughly speaking search markov chain bigger stabilizer earlier hit. group structure implies stabilizer corresponds small orbit. intuition suggests simpler feature smaller orbit. example line-segment generates many fewer possible shapes linear deformations ﬂower-like shape. autoencoder learn simpler features ﬁrst falls line experiments intuition naturally extends many-layer scenario. hidden layer ﬁnding feature stabilizer. beyond ﬁrst level inputs longer inhabit space training samples. simple feature space actually corresponds complex shape space input samples. process repeats number layers increases. effect layer learns edge-like features respect previous layer locally simple representations obtain learned higher-order representation. honglak grosse roger ranganath rajesh andrew convolutional deep belief networks scalable unsupervised learning hierarchical representations. proceedings annual international conference machine learning", "year": 2015}